"{\"abstract\":\"Audio Description (AD) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed ADs, which are temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the MPII Movie Description dataset (MPII-MD) contains a parallel corpus of over 68K sentences and video snippets from 94 HD movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing ADs to scripts, we find that ADs are far more visual and describe precisely what is shown rather than what should happen according to the scripts created prior to movie production.\",\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\",\"url\":\"https://www.semanticscholar.org/author/34721166\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\",\"url\":\"https://www.semanticscholar.org/author/34849128\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\",\"url\":\"https://www.semanticscholar.org/author/1721168\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\",\"url\":\"https://www.semanticscholar.org/author/48920094\"}],\"citationVelocity\":40,\"citations\":[{\"arxivId\":\"1505.04474\",\"authors\":[{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0c5f9f5083b9fca4dcdbc4b122099ac1f630728b\",\"title\":\"Visual Semantic Role Labeling\",\"url\":\"https://www.semanticscholar.org/paper/0c5f9f5083b9fca4dcdbc4b122099ac1f630728b\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9274668\",\"name\":\"Karam M. Abughalieh\"},{\"authorId\":\"2567962\",\"name\":\"S. Alawneh\"}],\"doi\":\"10.1109/ACCESS.2020.2987777\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5cf26e45b4594ea21ec11b6aa449e1978f42c414\",\"title\":\"Predicting Pedestrian Intention to Cross the Road\",\"url\":\"https://www.semanticscholar.org/paper/5cf26e45b4594ea21ec11b6aa449e1978f42c414\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2007.10639\",\"authors\":[{\"authorId\":\"151352107\",\"name\":\"Valentin Gabeur\"},{\"authorId\":\"1491624845\",\"name\":\"Chen Sun\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1007/978-3-030-58548-8_13\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6871f6c5437a747fae75a19962f418d234ce2dc1\",\"title\":\"Multi-modal Transformer for Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6871f6c5437a747fae75a19962f418d234ce2dc1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1704.04689\",\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"119745921\",\"name\":\"Dong Zhang\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":\"10.1109/ICCV.2017.157\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad2339c48ad4ffdd6100310dcbb1fb78e72fac98\",\"title\":\"Video Fill In the Blank Using LR/RL LSTMs with Spatial-Temporal Attentions\",\"url\":\"https://www.semanticscholar.org/paper/ad2339c48ad4ffdd6100310dcbb1fb78e72fac98\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1906849\",\"name\":\"Shiqi Li\"},{\"authorId\":\"1897728715\",\"name\":\"Haipeng Wang\"},{\"authorId\":\"1898046860\",\"name\":\"Shuze Wang\"},{\"authorId\":\"48691862\",\"name\":\"S. Zhang\"}],\"doi\":\"10.1007/s11042-020-09510-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1d38ee5cb65b054f7a99bfe2a14dafc6dc6483e\",\"title\":\"Life detection and non-contact respiratory rate measurement in cluttered environments\",\"url\":\"https://www.semanticscholar.org/paper/f1d38ee5cb65b054f7a99bfe2a14dafc6dc6483e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"1512.02902\",\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2016.501\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"title\":\"MovieQA: Understanding Stories in Movies through Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82564091\",\"name\":\"Ey\\u00fcp \\u00d6zer\"},{\"authorId\":\"1658966195\",\"name\":\"\\u0130lteber Nur Karap\\u0131nar\"},{\"authorId\":\"1658997114\",\"name\":\"Sena Ba\\u015fbu\\u011f\"},{\"authorId\":\"1657299979\",\"name\":\"S\\u00fcmeyye Turan\"},{\"authorId\":\"52219742\",\"name\":\"An\\u0131l Utku\"},{\"authorId\":\"153780569\",\"name\":\"M. Akcayol\"}],\"doi\":\"10.14569/ijacsa.2020.0110365\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f682a618e680e6c488f620a65cf5cf657fc6986b\",\"title\":\"Deep Learning based, a New Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f682a618e680e6c488f620a65cf5cf657fc6986b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153439664\",\"name\":\"Yongjin Kwon\"},{\"authorId\":\"3035146\",\"name\":\"Jinyoung Moon\"},{\"authorId\":\"1938407\",\"name\":\"Kyuchang Kang\"},{\"authorId\":\"2800227\",\"name\":\"Jongyoul Park\"}],\"doi\":\"10.1109/SIP.2015.10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0786069c700bba122950b579ff3e8fda02a4efe9\",\"title\":\"Effective Translation Models for Heterogeneous Schemas of Visual Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/0786069c700bba122950b579ff3e8fda02a4efe9\",\"venue\":\"2015 8th International Conference on Signal Processing, Image Processing and Pattern Recognition (SIP)\",\"year\":2015},{\"arxivId\":\"1707.06029\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"4945045\",\"name\":\"Yeonhwa Kim\"},{\"authorId\":\"143912065\",\"name\":\"Kyung Yoo\"},{\"authorId\":\"2135453\",\"name\":\"S. Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.648\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"title\":\"Supervising Neural Attention Models for Video Captioning by Human Gaze Data\",\"url\":\"https://www.semanticscholar.org/paper/1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"}],\"doi\":\"10.1007/978-3-030-00764-5_8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2036b394a5dff537df48d6db62a0d61491c92046\",\"title\":\"iMakeup: Makeup Instructional Video Dataset for Fine-Grained Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2036b394a5dff537df48d6db62a0d61491c92046\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144343897\",\"name\":\"R. Guo\"},{\"authorId\":\"22771932\",\"name\":\"Shubo Ma\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1007/s11042-018-7118-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef82141c898442d20c5d9e86e0f5d8940ae12c17\",\"title\":\"Image captioning: from structural tetrad to translated sentences\",\"url\":\"https://www.semanticscholar.org/paper/ef82141c898442d20c5d9e86e0f5d8940ae12c17\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9260404\",\"name\":\"Xiaotong Du\"},{\"authorId\":\"46685438\",\"name\":\"J. Yuan\"},{\"authorId\":\"47652550\",\"name\":\"L. Hu\"},{\"authorId\":\"122907636\",\"name\":\"Yuke Dai\"}],\"doi\":\"10.1007/s00371-018-1591-x\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4170882122b559fc39ab3eafd66babe2429ba858\",\"title\":\"Description generation of open-domain videos incorporating multimodal features and bidirectional encoder\",\"url\":\"https://www.semanticscholar.org/paper/4170882122b559fc39ab3eafd66babe2429ba858\",\"venue\":\"The Visual Computer\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"50080046\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/TIP.2019.2916757\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"title\":\"CAM-RNN: Co-Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153554194\",\"name\":\"Dany Joy\"},{\"authorId\":\"80381844\",\"name\":\"Manu P. John\"},{\"authorId\":\"152741307\",\"name\":\"Irene Ann Abraham\"},{\"authorId\":\"1410330428\",\"name\":\"E. Shiju\"},{\"authorId\":\"1410267270\",\"name\":\"Anjali A\"}],\"doi\":\"10.1109/ICCMC.2019.8819638\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"257c7956d0f333058c6f1569c08edd9d10c1be21\",\"title\":\"Automatic Generation of Statutory Warnings Using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/257c7956d0f333058c6f1569c08edd9d10c1be21\",\"venue\":\"2019 3rd International Conference on Computing Methodologies and Communication (ICCMC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ACCESS.2019.2942000\",\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"801827592d18c4e6170d88f8345465de4a8db7ca\",\"title\":\"Video Captioning With Adaptive Attention and Mixed Loss Optimization\",\"url\":\"https://www.semanticscholar.org/paper/801827592d18c4e6170d88f8345465de4a8db7ca\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/2962719\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"title\":\"Semantic Feature Mining for Video Event Understanding\",\"url\":\"https://www.semanticscholar.org/paper/99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"venue\":\"TOMM\",\"year\":2016},{\"arxivId\":\"2005.00343\",\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"48842721\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":\"10.1109/TPAMI.2020.2991965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1badccbe4a3cbf8662b924a97bbeea14fe2f1ac7\",\"title\":\"The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines\",\"url\":\"https://www.semanticscholar.org/paper/1badccbe4a3cbf8662b924a97bbeea14fe2f1ac7\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1609.02284\",\"authors\":[{\"authorId\":\"3029956\",\"name\":\"J. Gao\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1007/978-3-319-54184-6_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d8ab52fa7e145b3a37fd90fd90f1105a277deee\",\"title\":\"Learning Action Concept Trees and Semantic Alignment Networks from Image-Description Data\",\"url\":\"https://www.semanticscholar.org/paper/3d8ab52fa7e145b3a37fd90fd90f1105a277deee\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1707726\",\"name\":\"J. Pustejovsky\"},{\"authorId\":\"144228177\",\"name\":\"Tuan Do\"},{\"authorId\":\"3433384\",\"name\":\"Gitit Kehat\"},{\"authorId\":\"34079649\",\"name\":\"N. Krishnaswamy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5fa1724a79a9f7090c54925f6ac52f1697d6b570\",\"title\":\"The Development of Multimodal Lexical Resources\",\"url\":\"https://www.semanticscholar.org/paper/5fa1724a79a9f7090c54925f6ac52f1697d6b570\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72046972\",\"name\":\"Kataoka Hirokatsu\"},{\"authorId\":\"1393888854\",\"name\":\"Miyashita Yudai\"},{\"authorId\":\"1392031925\",\"name\":\"Yamabe Tomoaki\"},{\"authorId\":\"100877779\",\"name\":\"Shirakabe Soma\"},{\"authorId\":\"52601768\",\"name\":\"Sato Shinichi\"},{\"authorId\":\"1392572194\",\"name\":\"Hoshino Hironori\"},{\"authorId\":\"1392011341\",\"name\":\"Kato Ryo\"},{\"authorId\":\"101524854\",\"name\":\"Abe Kaori\"},{\"authorId\":\"1390054231\",\"name\":\"Imanari Takaaki\"},{\"authorId\":\"1394469677\",\"name\":\"Kobayashi Naomichi\"},{\"authorId\":\"100667175\",\"name\":\"Morita Shinichiro\"},{\"authorId\":\"66767948\",\"name\":\"Nakamura Akio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b35a6e738aadd4a974db2d0815879e37e5d7d66f\",\"title\":\"cvpaper.challenge in CVPR2015 -- A review of CVPR2015\",\"url\":\"https://www.semanticscholar.org/paper/b35a6e738aadd4a974db2d0815879e37e5d7d66f\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394475099\",\"name\":\"Beg\\u00fcm \\u00c7itamak\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"}],\"doi\":\"10.1109/SIU.2019.8806555\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"title\":\"MSVD-Turkish: A Large-Scale Dataset for Video Captioning in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"venue\":\"2019 27th Signal Processing and Communications Applications Conference (SIU)\",\"year\":2019},{\"arxivId\":\"2012.07536\",\"authors\":[{\"authorId\":\"22220222\",\"name\":\"Pinelopi Papalampidi\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"1412841514\",\"name\":\"M. Lapata\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d739bc3eafbac3304be528c706f127dda243dc9c\",\"title\":\"Movie Summarization via Sparse Graph Construction\",\"url\":\"https://www.semanticscholar.org/paper/d739bc3eafbac3304be528c706f127dda243dc9c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1606.04631\",\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"83672162\",\"name\":\"Zi Huang\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967258\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"title\":\"Bidirectional Long-Short Term Memory for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31493920\",\"name\":\"Olli Philippe Lautenbacher\"},{\"authorId\":\"31416760\",\"name\":\"Liisa Tiittula\"},{\"authorId\":\"2539645\",\"name\":\"M. Hirvonen\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"},{\"authorId\":\"1719346\",\"name\":\"M. Kurimo\"}],\"doi\":\"10.18653/v1/W15-2803\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c12575e8465122ec85d70deb737f965e185fe02\",\"title\":\"Towards Reliable Automatic Multimodal Content Analysis\",\"url\":\"https://www.semanticscholar.org/paper/9c12575e8465122ec85d70deb737f965e185fe02\",\"venue\":\"VL@EMNLP\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1109/CVPR.2015.7298966\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"title\":\"Deep correlation for matching images and text\",\"url\":\"https://www.semanticscholar.org/paper/efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1907.06042\",\"authors\":[{\"authorId\":\"2964003\",\"name\":\"C. Lee\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"89c2cc5489d53e7c2082855aed9380694de23e0b\",\"title\":\"Cross-Lingual Transfer Learning for Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/89c2cc5489d53e7c2082855aed9380694de23e0b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1907.12763\",\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"150234800\",\"name\":\"Mattia Soldan\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"145160922\",\"name\":\"Bryan Russell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e12a3e3f3f383f222b5d2007802d7b7944364301\",\"title\":\"Temporal Localization of Moments in Video Collections with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/e12a3e3f3f383f222b5d2007802d7b7944364301\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.01210\",\"authors\":[{\"authorId\":\"1381902419\",\"name\":\"Mihir Prabhudesai\"},{\"authorId\":\"1693704\",\"name\":\"H. F. Tung\"},{\"authorId\":\"40604609\",\"name\":\"Syed Ashar Javed\"},{\"authorId\":\"51039185\",\"name\":\"Maximilian Sieb\"},{\"authorId\":\"34939798\",\"name\":\"Adam W. Harley\"},{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3245f7b672b382d9a122fe1730ae22113d61dbc4\",\"title\":\"Embodied Language Grounding with Implicit 3D Visual Feature Representations\",\"url\":\"https://www.semanticscholar.org/paper/3245f7b672b382d9a122fe1730ae22113d61dbc4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"title\":\"Video representation learning with deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"39855500\",\"name\":\"K. Gupta\"},{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"},{\"authorId\":\"24508084\",\"name\":\"Deshraj Yadav\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/TPAMI.2018.2828437\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c16428a0bae507d2a1785860f07168a807d8e59\",\"title\":\"Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/4c16428a0bae507d2a1785860f07168a807d8e59\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"47775167\",\"name\":\"Teng Li\"},{\"authorId\":\"47906413\",\"name\":\"Y. Wang\"},{\"authorId\":\"1730308\",\"name\":\"X. Wu\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1016/j.cviu.2017.06.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"94a86a758ae2608c00e9690e9951e805755bb1a1\",\"title\":\"Learning explicit video attributes from mid-level representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/94a86a758ae2608c00e9690e9951e805755bb1a1\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"2006.13256\",\"authors\":[{\"authorId\":\"1677780022\",\"name\":\"Dima Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"48842721\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"153155867\",\"name\":\"Jian Ma\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":\"10.5523/bris.2g1n6qdydwa9u22shpxqzp0t8m\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6c1a80daadab9c3bc3fdf183c669070ba7a3fd37\",\"title\":\"Rescaling Egocentric Vision\",\"url\":\"https://www.semanticscholar.org/paper/6c1a80daadab9c3bc3fdf183c669070ba7a3fd37\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52184535\",\"name\":\"Rintaro Yanagi\"},{\"authorId\":\"3470264\",\"name\":\"R. Togo\"},{\"authorId\":\"144392699\",\"name\":\"T. Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ICIP.2019.8803177\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"25a15befd5ee1480cce1d91ff9d3901cc8655463\",\"title\":\"Scene Retrieval for Video Summarization Based on Text-to-Image gan\",\"url\":\"https://www.semanticscholar.org/paper/25a15befd5ee1480cce1d91ff9d3901cc8655463\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"84340382\",\"name\":\"Paul Animbom Ngong\"}],\"doi\":\"10.5195/cinej.2020.265\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1205d7a1fdfc315210a09946f4d6aeeb8a6af041\",\"title\":\"Music and sound in documentary film communication: an exploration of Une Affaire de N\\u00e8gres and Chef!\",\"url\":\"https://www.semanticscholar.org/paper/1205d7a1fdfc315210a09946f4d6aeeb8a6af041\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1007/s00530-018-0598-5\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"title\":\"Multi-guiding long short-term memory for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"venue\":\"Multimedia Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36314727\",\"name\":\"Cole Gleason\"},{\"authorId\":\"48453720\",\"name\":\"A. Pavel\"},{\"authorId\":\"115908215\",\"name\":\"Emma McCamey\"},{\"authorId\":\"9413600\",\"name\":\"Christina Low\"},{\"authorId\":\"144451887\",\"name\":\"Patrick Carrington\"},{\"authorId\":\"37991449\",\"name\":\"Kris M. Kitani\"},{\"authorId\":\"1744846\",\"name\":\"Jeffrey P. Bigham\"}],\"doi\":\"10.1145/3313831.3376728\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"305bacf5a9780718609791a728ff2bbf2dc71da5\",\"title\":\"Twitter A11y: A Browser Extension to Make Twitter Images Accessible\",\"url\":\"https://www.semanticscholar.org/paper/305bacf5a9780718609791a728ff2bbf2dc71da5\",\"venue\":\"CHI\",\"year\":2020},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":\"1604.03249\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1007/978-3-319-50077-5_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"title\":\"Attributes as Semantic Units between Natural Language and Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2007.08751\",\"authors\":[{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"}],\"doi\":\"10.1007/978-3-030-58523-5_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"773e7d33411fc2cdd6829356b7ce8ed34e14cd65\",\"title\":\"Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/773e7d33411fc2cdd6829356b7ce8ed34e14cd65\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1612.06950\",\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aba537944ace733bbab2290cf2814cf7f4e4e275\",\"title\":\"Temporal Tessellation for Video Annotation and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/aba537944ace733bbab2290cf2814cf7f4e4e275\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938407\",\"name\":\"Kyuchang Kang\"},{\"authorId\":\"153439664\",\"name\":\"Yongjin Kwon\"},{\"authorId\":\"3035146\",\"name\":\"Jinyoung Moon\"},{\"authorId\":\"2800227\",\"name\":\"Jongyoul Park\"}],\"doi\":\"10.1109/SIP.2015.12\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8943474b9e7f0cc65266c93246b66de458d3918e\",\"title\":\"Investigation and Review of Embedded Events in Public Video Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8943474b9e7f0cc65266c93246b66de458d3918e\",\"venue\":\"2015 8th International Conference on Signal Processing, Image Processing and Pattern Recognition (SIP)\",\"year\":2015},{\"arxivId\":\"1903.01489\",\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/s11042-018-7040-z\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1344317f255a9d338fb80f276126951b9644f7e3\",\"title\":\"M-VAD names: a dataset for video captioning with naming\",\"url\":\"https://www.semanticscholar.org/paper/1344317f255a9d338fb80f276126951b9644f7e3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":\"1906.03327\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2019.00272\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9311779489e597315488749ee6c386bfa3f3512e\",\"title\":\"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\"url\":\"https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/s11263-017-1033-7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"828ac57f755db989e2886042a85278ae4823297c\",\"title\":\"Uncovering the Temporal Context for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/828ac57f755db989e2886042a85278ae4823297c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"2007.02503\",\"authors\":[{\"authorId\":\"1390538450\",\"name\":\"Xun Yang\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145014675\",\"name\":\"Yixin Cao\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1145/3397271.3401151\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3da2ca65b90e841a586f55ec9df36e3c6f000077\",\"title\":\"Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/3da2ca65b90e841a586f55ec9df36e3c6f000077\",\"venue\":\"SIGIR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/LRA.2018.2793345\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"title\":\"Full-GRU Natural Language Video Description for Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"title\":\"Empirical performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"35084211\",\"name\":\"M. Lewis\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D18-1117\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"title\":\"A Dataset for Telling the Stories of Social Media Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49602133\",\"name\":\"S. Braun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0586116ff3d690755ba35dcb5c17dafab13e291f\",\"title\":\"Finding the Right Words: Investigating Machine-Generated Video Description Quality Using a Corpus-Based Approach\",\"url\":\"https://www.semanticscholar.org/paper/0586116ff3d690755ba35dcb5c17dafab13e291f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1605.08247\",\"authors\":[{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"},{\"authorId\":\"2642022\",\"name\":\"Y. Miyashita\"},{\"authorId\":\"3108668\",\"name\":\"Tomoaki K. Yamabe\"},{\"authorId\":\"3393640\",\"name\":\"Soma Shirakabe\"},{\"authorId\":\"47937389\",\"name\":\"Shin-ichi Sato\"},{\"authorId\":\"29998543\",\"name\":\"H. Hoshino\"},{\"authorId\":\"144802310\",\"name\":\"Ryo Kato\"},{\"authorId\":\"49897653\",\"name\":\"K. Abe\"},{\"authorId\":\"3407486\",\"name\":\"Takaaki Imanari\"},{\"authorId\":\"26851746\",\"name\":\"Naomichi Kobayashi\"},{\"authorId\":\"49133490\",\"name\":\"Shinichiro Morita\"},{\"authorId\":\"2462801\",\"name\":\"A. Nakamura\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"74012279e94f13d44c62f5d5b20dc983b7f5cf84\",\"title\":\"cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey\",\"url\":\"https://www.semanticscholar.org/paper/74012279e94f13d44c62f5d5b20dc983b7f5cf84\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48490580\",\"name\":\"J. Park\"},{\"authorId\":\"35409051\",\"name\":\"Chibon Song\"},{\"authorId\":\"47180565\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/ICIIBMS.2017.8279760\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"title\":\"A study of evaluation metrics and datasets for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"venue\":\"2017 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)\",\"year\":2017},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"39491387\",\"name\":\"J. Zhou\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"}],\"doi\":\"10.1109/TIP.2018.2855422\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd3d94fac6a282414406716040b10c1746634ecd\",\"title\":\"Video Captioning by Adversarial LSTM\",\"url\":\"https://www.semanticscholar.org/paper/fd3d94fac6a282414406716040b10c1746634ecd\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1707.09468\",\"authors\":[{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/D17-1099\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8513e433d5bbf26c2305895128557ed1f795842a\",\"title\":\"Zero-Shot Activity Recognition with Verb Attribute Induction\",\"url\":\"https://www.semanticscholar.org/paper/8513e433d5bbf26c2305895128557ed1f795842a\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"12387007\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":\"10.1007/978-3-030-01225-0_44\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"443b1a20b30f09a2e4d6c47a8a9412a668f615fd\",\"title\":\"Scaling Egocentric Vision: The Dataset\",\"url\":\"https://www.semanticscholar.org/paper/443b1a20b30f09a2e4d6c47a8a9412a668f615fd\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2898850\",\"name\":\"R\\u00e9mi Ronfard\"}],\"doi\":\"10.2312/wiced.20171069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d5c43f7a1c4e2fec310474bb78a2e7aca5aa609\",\"title\":\"Five Challenges for Intelligent Cinematography and Editing\",\"url\":\"https://www.semanticscholar.org/paper/6d5c43f7a1c4e2fec310474bb78a2e7aca5aa609\",\"venue\":\"WICED@Eurographics\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2964003\",\"name\":\"C. Lee\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"},{\"authorId\":\"2208161\",\"name\":\"Szu-Lin Wu\"},{\"authorId\":\"117660020\",\"name\":\"Chi-Liang Liu\"},{\"authorId\":\"145359071\",\"name\":\"Wei Fang\"},{\"authorId\":\"3451540\",\"name\":\"Juei-Yang Hsu\"},{\"authorId\":\"33870107\",\"name\":\"Bo-Hsiang Tseng\"}],\"doi\":\"10.1109/TASLP.2019.2913499\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"76d095f2e000e3f4bb08e55b1941e81cfe699322\",\"title\":\"Machine Comprehension of Spoken Content: TOEFL Listening Test and Spoken SQuAD\",\"url\":\"https://www.semanticscholar.org/paper/76d095f2e000e3f4bb08e55b1941e81cfe699322\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2019},{\"arxivId\":\"1908.07236\",\"authors\":[{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"1389646918\",\"name\":\"Edison Marrese-Taylor\"},{\"authorId\":\"9120887\",\"name\":\"F. Saleh\"},{\"authorId\":\"40124570\",\"name\":\"Hongdong Li\"},{\"authorId\":\"47873182\",\"name\":\"S. Gould\"}],\"doi\":\"10.1109/WACV45572.2020.9093328\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03df26255781ebb71d9430e1b2aaabf8e1af9990\",\"title\":\"Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention\",\"url\":\"https://www.semanticscholar.org/paper/03df26255781ebb71d9430e1b2aaabf8e1af9990\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66957238\",\"name\":\"Youngsaeng Jin\"},{\"authorId\":\"1561609794\",\"name\":\"Junggi Kwak\"},{\"authorId\":\"18016679\",\"name\":\"Younglo Lee\"},{\"authorId\":\"70346423\",\"name\":\"Jeongseop Yun\"},{\"authorId\":\"81950948\",\"name\":\"Hanseok Ko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"85113ab90b59b7365d6b85ba94f907c0f5de3b5b\",\"title\":\"KU-ISPL TRECVID 2018 VTT Model\",\"url\":\"https://www.semanticscholar.org/paper/85113ab90b59b7365d6b85ba94f907c0f5de3b5b\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67159407\",\"name\":\"S. Amirian\"},{\"authorId\":\"1799121\",\"name\":\"K. Rasheed\"},{\"authorId\":\"1784399\",\"name\":\"T. Taha\"},{\"authorId\":\"1712033\",\"name\":\"H. Arabnia\"}],\"doi\":\"10.1109/ACCESS.2020.3042484\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"832aafb4989c24211a8377f82228c31f7a90ef81\",\"title\":\"Automatic Image and Video Caption Generation With Deep Learning: A Concise Review and Algorithmic Overlap\",\"url\":\"https://www.semanticscholar.org/paper/832aafb4989c24211a8377f82228c31f7a90ef81\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"90383477\",\"name\":\"D. Ferreira\"},{\"authorId\":\"8471045\",\"name\":\"Julia Rozanova\"},{\"authorId\":\"2392273\",\"name\":\"K. Dubba\"},{\"authorId\":\"37510526\",\"name\":\"Dell Zhang\"},{\"authorId\":\"145528474\",\"name\":\"Andr\\u00e9 Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a944f4e62eb5744ce74556a4ea0cafeb0f8f44c2\",\"title\":\"On the Evaluation of Intelligence Process Automation\",\"url\":\"https://www.semanticscholar.org/paper/a944f4e62eb5744ce74556a4ea0cafeb0f8f44c2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1808.02280\",\"authors\":[{\"authorId\":\"2964003\",\"name\":\"C. Lee\"},{\"authorId\":\"1947838\",\"name\":\"Shang-Ming Wang\"},{\"authorId\":\"39908989\",\"name\":\"H. Chang\"},{\"authorId\":\"144300094\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/SLT.2018.8639505\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de91454ff9fc82c68359360b661c25f52dd2c0cb\",\"title\":\"ODSQA: Open-Domain Spoken Question Answering Dataset\",\"url\":\"https://www.semanticscholar.org/paper/de91454ff9fc82c68359360b661c25f52dd2c0cb\",\"venue\":\"2018 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/3122865\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"title\":\"Frontiers of Multimedia Research\",\"url\":\"https://www.semanticscholar.org/paper/8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40418780\",\"name\":\"J. Gillick\"},{\"authorId\":\"2168134\",\"name\":\"David Bamman\"}],\"doi\":\"10.18653/v1/W18-1504\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"530a7579ba22b260f086adb7e65078567883f6b2\",\"title\":\"Telling Stories with Soundtracks: An Empirical Analysis of Music in Film\",\"url\":\"https://www.semanticscholar.org/paper/530a7579ba22b260f086adb7e65078567883f6b2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2001.02639\",\"authors\":[{\"authorId\":\"90383477\",\"name\":\"D. Ferreira\"},{\"authorId\":\"8471045\",\"name\":\"Julia Rozanova\"},{\"authorId\":\"2392273\",\"name\":\"K. Dubba\"},{\"authorId\":\"37510526\",\"name\":\"Dell Zhang\"},{\"authorId\":\"145528474\",\"name\":\"Andr\\u00e9 Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"35b4be3c59c226ece9de192de1c9c1ebeea7a618\",\"title\":\"On the Evaluation of Intelligent Process Automation\",\"url\":\"https://www.semanticscholar.org/paper/35b4be3c59c226ece9de192de1c9c1ebeea7a618\",\"venue\":\"AAAI 2019\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TCSVT.2018.2867286\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"title\":\"Dual-Stream Recurrent Neural Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500655884\",\"name\":\"Amulya Shivanath\"},{\"authorId\":\"8079893\",\"name\":\"M. Sumana\"}],\"doi\":\"10.1109/ICCES45898.2019.9002156\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ba891acdd147336163c4ddec847f799d91b040d\",\"title\":\"Extracting Primitive Tasks from Procedural Videos using Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/3ba891acdd147336163c4ddec847f799d91b040d\",\"venue\":\"2019 International Conference on Communication and Electronics Systems (ICCES)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"46172451\",\"name\":\"B. Wang\"},{\"authorId\":\"2248826\",\"name\":\"R. Hong\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TCSVT.2019.2897604\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cb2f25b32344888d644dc3a3e729275a8abee07a\",\"title\":\"Movie Question Answering via Textual Memory and Plot Graph\",\"url\":\"https://www.semanticscholar.org/paper/cb2f25b32344888d644dc3a3e729275a8abee07a\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1808.02559\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"153188107\",\"name\":\"Jongseok Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1007/978-3-030-01234-2_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8befcd91c24038e5c26df0238d26e2311b21719a\",\"title\":\"A Joint Sequence Fusion Model for Video Question Answering and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8befcd91c24038e5c26df0238d26e2311b21719a\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49046603\",\"name\":\"C. Liu\"},{\"authorId\":\"2887672\",\"name\":\"A. Shmilovici\"}],\"doi\":\"10.1007/978-3-030-47124-8_39\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f78ec67e18413fe70e37eee132e7527ffee5ec62\",\"title\":\"Towards Automatic Textual Summarization of Movies\",\"url\":\"https://www.semanticscholar.org/paper/f78ec67e18413fe70e37eee132e7527ffee5ec62\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1016/j.cviu.2017.04.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"title\":\"Hierarchical & multimodal video captioning: Discovering and transferring multimodal knowledge for vision to language\",\"url\":\"https://www.semanticscholar.org/paper/96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"2003.03186\",\"authors\":[{\"authorId\":\"7885575\",\"name\":\"E. Amrani\"},{\"authorId\":\"1397958297\",\"name\":\"R. Ben-Ari\"},{\"authorId\":\"143679933\",\"name\":\"Daniel Rotman\"},{\"authorId\":\"144858358\",\"name\":\"Alex Bronstein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"777873ef6d23c2cdd7dfd6c4834eb56769a25bb1\",\"title\":\"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning\",\"url\":\"https://www.semanticscholar.org/paper/777873ef6d23c2cdd7dfd6c4834eb56769a25bb1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1608.07068\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-46475-6_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"title\":\"Title Generation for User Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1959ba4637739dcc6cc6995e10fd41fd6604713\",\"title\":\"Deep Learning for Semantic Video Understanding by Sourabh Kulhare\",\"url\":\"https://www.semanticscholar.org/paper/d1959ba4637739dcc6cc6995e10fd41fd6604713\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fa2030f41aa0b54965420eb7836c7c32a91b087b\",\"title\":\"Video Content Understanding Using Text\",\"url\":\"https://www.semanticscholar.org/paper/fa2030f41aa0b54965420eb7836c7c32a91b087b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41052836\",\"name\":\"Anya Belz\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"}],\"doi\":\"10.1017/S1351324918000086\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d740802aa22dbc187fe5df39108ba493b34d2839\",\"title\":\"From image to language and back again\",\"url\":\"https://www.semanticscholar.org/paper/d740802aa22dbc187fe5df39108ba493b34d2839\",\"venue\":\"Nat. Lang. Eng.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/ICCV.2017.20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"62e6b749ed5825739aa906021c5e613803d5cbe2\",\"title\":\"Temporal Tessellation: A Unified Approach for Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/62e6b749ed5825739aa906021c5e613803d5cbe2\",\"venue\":\"ICCV\",\"year\":2017},{\"arxivId\":\"1611.08669\",\"authors\":[{\"authorId\":\"145497716\",\"name\":\"A. Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"39855500\",\"name\":\"K. Gupta\"},{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"},{\"authorId\":\"24508084\",\"name\":\"Deshraj Yadav\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPR.2017.121\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2231f44be9a8472a46d8e8a628b4e52b9a8f44e0\",\"title\":\"Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/2231f44be9a8472a46d8e8a628b4e52b9a8f44e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1944274\",\"name\":\"Hongyin Luo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bfe973d679b431276bc1dcb3a019a0272f14ffe9\",\"title\":\"Neural Attentions for Natural Language Understanding and Modeling by Hongyin Luo\",\"url\":\"https://www.semanticscholar.org/paper/bfe973d679b431276bc1dcb3a019a0272f14ffe9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"144245551\",\"name\":\"M. Tang\"},{\"authorId\":\"50561313\",\"name\":\"J. Zhang\"}],\"doi\":\"10.1117/1.JEI.27.2.023027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"title\":\"Deep hierarchical attention network for video description\",\"url\":\"https://www.semanticscholar.org/paper/8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97c881c89166cd4ae5c16050140edbbee417582c\",\"title\":\"Natural Language as a Scaffold for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/97c881c89166cd4ae5c16050140edbbee417582c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"150898549\",\"name\":\"Alessandro Devo\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/TMM.2019.2924598\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61a10a61e95ede1064567be38196a2348af3fb6c\",\"title\":\"The Role of the Input in Natural Language Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61a10a61e95ede1064567be38196a2348af3fb6c\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751591\",\"name\":\"G. Weikum\"},{\"authorId\":\"1727527\",\"name\":\"Johannes Hoffart\"},{\"authorId\":\"1679784\",\"name\":\"Fabian M. Suchanek\"}],\"doi\":\"10.1007/978-3-319-91908-9_13\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f6205c4dcc47c55d878e70f1d660ed54c02d0c1e\",\"title\":\"Knowledge Harvesting: Achievements and Challenges\",\"url\":\"https://www.semanticscholar.org/paper/f6205c4dcc47c55d878e70f1d660ed54c02d0c1e\",\"venue\":\"Computing and Software Science\",\"year\":2019},{\"arxivId\":\"1801.01967\",\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":\"10.1007/978-3-030-01261-8_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab84d00079d0a29e44bdc4c83037dc76b0fbef05\",\"title\":\"Visual Text Correction\",\"url\":\"https://www.semanticscholar.org/paper/ab84d00079d0a29e44bdc4c83037dc76b0fbef05\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"51133784\",\"name\":\"F. Pollastri\"},{\"authorId\":\"1705203\",\"name\":\"C. Grana\"}],\"doi\":\"10.1109/IPAS.2018.8708893\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9184fe68648d1e7fcd1d9d153e3b888096f355b1\",\"title\":\"A Hierarchical Quasi-Recurrent approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9184fe68648d1e7fcd1d9d153e3b888096f355b1\",\"venue\":\"2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS)\",\"year\":2018},{\"arxivId\":\"1809.04938\",\"authors\":[{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"145500855\",\"name\":\"Lei Cui\"},{\"authorId\":\"10780897\",\"name\":\"Damai Dai\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.1609/AAAI.V33I01.33016810\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"09e39f4334417f92bddc20072f43efade9bd5b60\",\"title\":\"LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/09e39f4334417f92bddc20072f43efade9bd5b60\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97753168\",\"name\":\"Omar Shahid\"},{\"authorId\":\"40176277\",\"name\":\"S. Rahman\"},{\"authorId\":\"2011779287\",\"name\":\"Syeda Faiza Ahmed\"},{\"authorId\":\"2013795993\",\"name\":\"Musabbir Ahmed Arrafi\"},{\"authorId\":\"2327468\",\"name\":\"Md Atiqur Rahman Ahad\"}],\"doi\":\"10.20944/preprints202010.0388.v1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b24f17233d4fe56c857f29f2ef16df5e81b51e1\",\"title\":\"Data-Driven Automated Detection of Autism Spectrum Disorder Using Activity Analysis: A Review\",\"url\":\"https://www.semanticscholar.org/paper/3b24f17233d4fe56c857f29f2ef16df5e81b51e1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.05381\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"title\":\"Hybrid Space Learning for Language-based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144117646\",\"name\":\"Jiang Gao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"160930a53915a9d73e63a6fa90c290917e9ccab6\",\"title\":\"Bridging Social Graphs with Character-Centered Story Contexts in Videos\",\"url\":\"https://www.semanticscholar.org/paper/160930a53915a9d73e63a6fa90c290917e9ccab6\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33975342\",\"name\":\"Jiajun Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f31e2d67a479c44a9942b1cd6e4905b6b1972d37\",\"title\":\"Video Understanding : From Video Classification to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f31e2d67a479c44a9942b1cd6e4905b6b1972d37\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":null,\"name\":\"Ning Xu\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"title\":\"Tianjin University and National University of Singapore at TRECVID 2017: Video to Text Description\",\"url\":\"https://www.semanticscholar.org/paper/c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49576171\",\"name\":\"Lingxiao Yang\"},{\"authorId\":\"46335319\",\"name\":\"D. Zhang\"},{\"authorId\":\"48571012\",\"name\":\"L. Zhang\"}],\"doi\":\"10.1609/AAAI.V33I01.33019095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96fa23766fb070dadf33bd615a4d61c4831b3cea\",\"title\":\"Learning a Visual Tracker from a Single Movie without Annotation\",\"url\":\"https://www.semanticscholar.org/paper/96fa23766fb070dadf33bd615a4d61c4831b3cea\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52184535\",\"name\":\"Rintaro Yanagi\"},{\"authorId\":\"3470264\",\"name\":\"R. Togo\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ACCESS.2019.2952676\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4a92e08dd714e70151ed5c97e0f580f8d6ee60f\",\"title\":\"Text-to-Image GAN-Based Scene Retrieval and Re-Ranking Considering Word Importance\",\"url\":\"https://www.semanticscholar.org/paper/b4a92e08dd714e70151ed5c97e0f580f8d6ee60f\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"1931707\",\"name\":\"M. B\\u00e4uml\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1109/CVPR.2015.7298792\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45ca387a4080b6aee610783ed03d19bd1891503f\",\"title\":\"Book2Movie: Aligning video scenes with book chapters\",\"url\":\"https://www.semanticscholar.org/paper/45ca387a4080b6aee610783ed03d19bd1891503f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1506.06724\",\"authors\":[{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/ICCV.2015.11\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e6824e137847be0599bb0032e37042ed2ef5045\",\"title\":\"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books\",\"url\":\"https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1604.01753\",\"authors\":[{\"authorId\":\"34280810\",\"name\":\"Gunnar A. Sigurdsson\"},{\"authorId\":\"2668759\",\"name\":\"G. Varol\"},{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1007/978-3-319-46448-0_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21334d1aac5422da88780f8e24e181bfa15ef0e1\",\"title\":\"Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding\",\"url\":\"https://www.semanticscholar.org/paper/21334d1aac5422da88780f8e24e181bfa15ef0e1\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66801160\",\"name\":\"Ge Yuan-yuan\"},{\"authorId\":\"1441325232\",\"name\":\"Xu Youjiang\"},{\"authorId\":\"1391375173\",\"name\":\"Han Ya-hong\"}],\"doi\":\"10.1007/978-981-10-7299-4_33\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"489baaf7ca4b6ed0ff3cef48a8b078ca09b4c080\",\"title\":\"Video Question Answering Using a Forget Memory Network\",\"url\":\"https://www.semanticscholar.org/paper/489baaf7ca4b6ed0ff3cef48a8b078ca09b4c080\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"2008.09791\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-58589-1_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"title\":\"Identity-Aware Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"title\":\"LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1612.01669\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"3386346\",\"name\":\"Ilchae Jung\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/ICCV.2017.312\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"00672bec9dd0e755f9c375f912c0f1c3918a6b74\",\"title\":\"MarioQA: Answering Questions by Watching Gameplay Videos\",\"url\":\"https://www.semanticscholar.org/paper/00672bec9dd0e755f9c375f912c0f1c3918a6b74\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ec568db83e0921e7d4bf73bd6b9f3dc86e09c38\",\"title\":\"Human Activity Analysis using Multi-modalities and Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ec568db83e0921e7d4bf73bd6b9f3dc86e09c38\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1403616824\",\"name\":\"Ivan Vladimir Meza Ruiz\"},{\"authorId\":\"2684135\",\"name\":\"J. G. Flores\"},{\"authorId\":\"46445814\",\"name\":\"A. Gangemi\"},{\"authorId\":\"144014054\",\"name\":\"L. Pineda\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1aab7ff5ec027eaafe2dd0d12acae43815da39a6\",\"title\":\"Towards narrative generation of spatial experiences in service robots\",\"url\":\"https://www.semanticscholar.org/paper/1aab7ff5ec027eaafe2dd0d12acae43815da39a6\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1707.08559\",\"authors\":[{\"authorId\":\"2667317\",\"name\":\"Cheng-Yang Fu\"},{\"authorId\":\"153231574\",\"name\":\"J. Lee\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"}],\"doi\":\"10.18653/v1/D17-1102\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18aa7b466953189d5bc98e88f2a1ab12182f8b88\",\"title\":\"Video Highlight Prediction Using Audience Chat Reactions\",\"url\":\"https://www.semanticscholar.org/paper/18aa7b466953189d5bc98e88f2a1ab12182f8b88\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.1613/jair.4900\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c162d791b63682d928c09578bd38c3dd61f78c8c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures\",\"url\":\"https://www.semanticscholar.org/paper/c162d791b63682d928c09578bd38c3dd61f78c8c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2016},{\"arxivId\":\"1804.10692\",\"authors\":[{\"authorId\":\"1693704\",\"name\":\"H. F. Tung\"},{\"authorId\":\"34939798\",\"name\":\"Adam W. Harley\"},{\"authorId\":\"9930869\",\"name\":\"Liang-Kang Huang\"},{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"}],\"doi\":\"10.1109/CVPR.2018.00732\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2558c26915e834b60b06be7da1d9db5d0897343\",\"title\":\"Reward Learning from Narrated Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/d2558c26915e834b60b06be7da1d9db5d0897343\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"title\":\"Natural Language Description of Images and Videos\",\"url\":\"https://www.semanticscholar.org/paper/d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47141093\",\"name\":\"Gursimran Singh\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1710980\",\"name\":\"J. Little\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51fadc0803ee53c91eb4fcfb3777720496c9d91f\",\"title\":\"Spatio-temporal Relational Reasoning for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/51fadc0803ee53c91eb4fcfb3777720496c9d91f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"2007.14682\",\"authors\":[{\"authorId\":\"1840585237\",\"name\":\"Philipp Rimle\"},{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"143720818\",\"name\":\"M. Gro\\u00df\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"title\":\"Enriching Video Captions With Contextual Text\",\"url\":\"https://www.semanticscholar.org/paper/f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39796129\",\"name\":\"Hayden Faulkner\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":\"10.1109/DICTA.2017.8227494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"title\":\"TenniSet: A Dataset for Dense Fine-Grained Event Recognition, Localisation and Description\",\"url\":\"https://www.semanticscholar.org/paper/4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"venue\":\"2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2017},{\"arxivId\":\"1706.04261\",\"authors\":[{\"authorId\":\"38962424\",\"name\":\"Raghav Goyal\"},{\"authorId\":\"3127597\",\"name\":\"S. Kahou\"},{\"authorId\":\"1748421\",\"name\":\"Vincent Michalski\"},{\"authorId\":\"7654960\",\"name\":\"Joanna Materzynska\"},{\"authorId\":\"12929417\",\"name\":\"S. Westphal\"},{\"authorId\":\"2233986\",\"name\":\"Heuna Kim\"},{\"authorId\":\"7241984\",\"name\":\"V. Haenel\"},{\"authorId\":\"47544625\",\"name\":\"Ingo Fr\\u00fcnd\"},{\"authorId\":\"19265538\",\"name\":\"Peter Yianilos\"},{\"authorId\":\"1414405239\",\"name\":\"Moritz Mueller-Freitag\"},{\"authorId\":\"143931146\",\"name\":\"F. Hoppe\"},{\"authorId\":\"2020614\",\"name\":\"Christian Thurau\"},{\"authorId\":\"2443288\",\"name\":\"I. Bax\"},{\"authorId\":\"1710604\",\"name\":\"R. Memisevic\"}],\"doi\":\"10.1109/ICCV.2017.622\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b68811a9b5cafe4795a11c1048541750068b7ad0\",\"title\":\"The \\u201cSomething Something\\u201d Video Database for Learning and Evaluating Visual Common Sense\",\"url\":\"https://www.semanticscholar.org/paper/b68811a9b5cafe4795a11c1048541750068b7ad0\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/ICCV.2017.83\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"title\":\"Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2684135\",\"name\":\"J. G. Flores\"},{\"authorId\":\"1403616824\",\"name\":\"Ivan Vladimir Meza Ruiz\"},{\"authorId\":\"4145312\",\"name\":\"Emilie Colin\"},{\"authorId\":\"1794075\",\"name\":\"Claire Gardent\"},{\"authorId\":\"2420171\",\"name\":\"Aldo Gangemi\"},{\"authorId\":\"144014054\",\"name\":\"L. Pineda\"}],\"doi\":\"10.3233/JIFS-169511\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2d9a5feeec8cb93a75a22927cf636b15598500de\",\"title\":\"Robot experience stories: First person generation of robotic task narratives in SitLog\",\"url\":\"https://www.semanticscholar.org/paper/2d9a5feeec8cb93a75a22927cf636b15598500de\",\"venue\":\"J. Intell. Fuzzy Syst.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143750989\",\"name\":\"F. Gao\"},{\"authorId\":\"49982480\",\"name\":\"Yuanyuan Ge\"},{\"authorId\":\"47909272\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/s11042-018-5868-x\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ffcfe47a28929135c2e8905d36f7f8732d217d5d\",\"title\":\"Remember and forget: video and text fusion for video question answering\",\"url\":\"https://www.semanticscholar.org/paper/ffcfe47a28929135c2e8905d36f7f8732d217d5d\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32377007\",\"name\":\"Kyeongmin Rim\"},{\"authorId\":\"72861490\",\"name\":\"Kelley Lynch\"},{\"authorId\":\"1707726\",\"name\":\"J. Pustejovsky\"}],\"doi\":\"10.18653/v1/W19-2512\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8e64748cd47fd1c8ec5690e0e576072888c505b\",\"title\":\"Computational Linguistics Applications for Multimedia Services\",\"url\":\"https://www.semanticscholar.org/paper/f8e64748cd47fd1c8ec5690e0e576072888c505b\",\"venue\":\"LaTeCH@NAACL-HLT\",\"year\":2019},{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"2008.08716\",\"authors\":[{\"authorId\":\"152666334\",\"name\":\"Sudipta Paul\"},{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2efdeaa68e94eeef2831536cdd53f3e3868bbdca\",\"title\":\"Text-based Localization of Moments in a Video Corpus\",\"url\":\"https://www.semanticscholar.org/paper/2efdeaa68e94eeef2831536cdd53f3e3868bbdca\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1606.07373\",\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"1732879\",\"name\":\"Lorenzo Torresani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c69040777e2b0e1d443e22f86e45e527381d79e7\",\"title\":\"ViCom: Benchmark and Methods for Video Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/c69040777e2b0e1d443e22f86e45e527381d79e7\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907260\",\"name\":\"Matthias Gall\\u00e9\"},{\"authorId\":\"25161802\",\"name\":\"Ankuj Arora\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ac1bbf7cc4f1a4992b7961d848c97757eb3aaf0\",\"title\":\"BEAT-o-matic : a baseline for learning behavior expressions from utterances\",\"url\":\"https://www.semanticscholar.org/paper/8ac1bbf7cc4f1a4992b7961d848c97757eb3aaf0\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5781871\",\"name\":\"Jiaqi Su\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"title\":\"Study of Video Captioning Problem\",\"url\":\"https://www.semanticscholar.org/paper/511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1109/TMM.2018.2807588\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3d0aec1410f7e403b7ef8dc99243a9addf07daa5\",\"title\":\"Text2Video: An End-to-end Learning Framework for Expressing Text With Videos\",\"url\":\"https://www.semanticscholar.org/paper/3d0aec1410f7e403b7ef8dc99243a9addf07daa5\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"title\":\"Multi-Modal Deep Learning to Understand Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1809.01696\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.18653/v1/D18-1167\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e7e1313061b0d56364bd2c41f017deb954bb05db\",\"title\":\"TVQA: Localized, Compositional Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e7e1313061b0d56364bd2c41f017deb954bb05db\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52184535\",\"name\":\"Rintaro Yanagi\"},{\"authorId\":\"3470264\",\"name\":\"R. Togo\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/GCCE46687.2019.9015366\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"54dfa3958ca23caa8afbae1dfdec40ec00f8a3e0\",\"title\":\"Scene Retrieval Using Text-to-image GAN-based Visual Similarities and Image-to-text Model-based Textual Similarities\",\"url\":\"https://www.semanticscholar.org/paper/54dfa3958ca23caa8afbae1dfdec40ec00f8a3e0\",\"venue\":\"2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)\",\"year\":2019},{\"arxivId\":\"2001.08034\",\"authors\":[{\"authorId\":\"153060461\",\"name\":\"Darryl Hannan\"},{\"authorId\":\"50658802\",\"name\":\"Akshay Jain\"},{\"authorId\":\"143977265\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.1609/AAAI.V34I05.6294\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6b578fe40abc9ec29424d342a8d676c88a98b921\",\"title\":\"ManyModalQA: Modality Disambiguation and QA over Diverse Inputs\",\"url\":\"https://www.semanticscholar.org/paper/6b578fe40abc9ec29424d342a8d676c88a98b921\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66330846a03dcc10f36b6db9adf3b4d32e7a3127\",\"title\":\"Polylingual Multimodal Learning\",\"url\":\"https://www.semanticscholar.org/paper/66330846a03dcc10f36b6db9adf3b4d32e7a3127\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1605.05440\",\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"8197937\",\"name\":\"K. Ohnishi\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/ICIP.2016.7532983\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"41aa209e9d294d370357434f310d49b2b0baebeb\",\"title\":\"Beyond caption to narrative: Video captioning with multiple sentences\",\"url\":\"https://www.semanticscholar.org/paper/41aa209e9d294d370357434f310d49b2b0baebeb\",\"venue\":\"2016 IEEE International Conference on Image Processing (ICIP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e03b932fa6c87b8a698c86b4b4963bf6e4b45933\",\"title\":\"Matrix ? Why does Cypher betray Morpheus ? How does the movie end ?\",\"url\":\"https://www.semanticscholar.org/paper/e03b932fa6c87b8a698c86b4b4963bf6e4b45933\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1910.11009\",\"authors\":[{\"authorId\":\"47424372\",\"name\":\"Yu Xiong\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":\"10357054\",\"name\":\"L. Guo\"},{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/ICCV.2019.00469\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"87699cff38982712ddb0b2349313077779a5d0ff\",\"title\":\"A Graph-Based Framework to Bridge Movies and Synopses\",\"url\":\"https://www.semanticscholar.org/paper/87699cff38982712ddb0b2349313077779a5d0ff\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28646227\",\"name\":\"Tielin Zhang\"},{\"authorId\":\"144492744\",\"name\":\"Y. Zeng\"},{\"authorId\":\"31184900\",\"name\":\"Ruihan Pan\"},{\"authorId\":\"1811211084\",\"name\":\"Mengting Shi\"},{\"authorId\":\"144863217\",\"name\":\"Enmeng Lu\"}],\"doi\":\"10.1007/s12559-020-09753-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"074a927fb2dc7f481e7cef4e55ad1b6a234df8da\",\"title\":\"Brain-Inspired Active Learning Architecture for Procedural Knowledge Understanding Based on Human-Robot Interaction\",\"url\":\"https://www.semanticscholar.org/paper/074a927fb2dc7f481e7cef4e55ad1b6a234df8da\",\"venue\":\"Cognitive Computation\",\"year\":2020},{\"arxivId\":\"1912.01452\",\"authors\":[{\"authorId\":\"50535497\",\"name\":\"Jiahong Huang\"},{\"authorId\":\"9951160\",\"name\":\"Modar Alfadly\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"1717056\",\"name\":\"M. Worring\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"01aa5668a618fbb376b6ab6608defc074ed355ac\",\"title\":\"Assessing the Robustness of Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/01aa5668a618fbb376b6ab6608defc074ed355ac\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1381902419\",\"name\":\"Mihir Prabhudesai\"},{\"authorId\":\"1693704\",\"name\":\"H. F. Tung\"},{\"authorId\":\"40604609\",\"name\":\"Syed Ashar Javed\"},{\"authorId\":\"51039185\",\"name\":\"Maximilian Sieb\"},{\"authorId\":\"34939798\",\"name\":\"Adam W. Harley\"},{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"}],\"doi\":\"10.1109/CVPR42600.2020.00229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"77e98ae704a70478fb1eea7625f54d103e427a58\",\"title\":\"Embodied Language Grounding With 3D Visual Feature Representations\",\"url\":\"https://www.semanticscholar.org/paper/77e98ae704a70478fb1eea7625f54d103e427a58\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52184535\",\"name\":\"Rintaro Yanagi\"},{\"authorId\":\"3470264\",\"name\":\"R. Togo\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ACCESS.2019.2947409\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"85dac483e80fb6b1b3bfe8598a2df337856258b1\",\"title\":\"Query is GAN: Scene Retrieval With Attentional Text-to-Image Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/85dac483e80fb6b1b3bfe8598a2df337856258b1\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1712.00796\",\"authors\":[{\"authorId\":\"7311172\",\"name\":\"Giorgos Bouritsas\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"2641229\",\"name\":\"A. Zlatintsi\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/CVPR.2018.00516\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e1379e2f0509af074808c1e464ef78fb6abd5ba\",\"title\":\"Multimodal Visual Concept Learning with Weakly Supervised Techniques\",\"url\":\"https://www.semanticscholar.org/paper/9e1379e2f0509af074808c1e464ef78fb6abd5ba\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1502.06648\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-015-0851-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0138ef03aa5cf7bf7d0ce70b3f98876af022ffcd\",\"title\":\"Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data\",\"url\":\"https://www.semanticscholar.org/paper/0138ef03aa5cf7bf7d0ce70b3f98876af022ffcd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1607.03827\",\"authors\":[{\"authorId\":\"3407285\",\"name\":\"Matthias Plappert\"},{\"authorId\":\"2545377\",\"name\":\"Christian Mandery\"},{\"authorId\":\"1722677\",\"name\":\"T. Asfour\"}],\"doi\":\"10.1089/big.2016.0028\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e738d166c72513feaede2e19b4e713fd410cfce8\",\"title\":\"The KIT Motion-Language Dataset\",\"url\":\"https://www.semanticscholar.org/paper/e738d166c72513feaede2e19b4e713fd410cfce8\",\"venue\":\"Big Data\",\"year\":2016},{\"arxivId\":\"1805.05622\",\"authors\":[{\"authorId\":\"115003962\",\"name\":\"Marko Smilevski\"},{\"authorId\":\"117355926\",\"name\":\"Ilija Lalkovski\"},{\"authorId\":\"145798745\",\"name\":\"Gjorgji Madjarov\"}],\"doi\":\"10.1007/978-3-030-00825-3_13\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"07edbcc0acd09abd81d0353cdb48d137ce498f24\",\"title\":\"Stories for Images-in-Sequence by using Visual and Narrative Components\",\"url\":\"https://www.semanticscholar.org/paper/07edbcc0acd09abd81d0353cdb48d137ce498f24\",\"venue\":\"ICT Innovations\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"title\":\"Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74437294\",\"name\":\"Sk. Arif Ahmed\"},{\"authorId\":\"3320759\",\"name\":\"D. P. Dogra\"},{\"authorId\":\"32614479\",\"name\":\"S. Kar\"},{\"authorId\":\"40813600\",\"name\":\"P. Roy\"}],\"doi\":\"10.1007/978-981-10-7590-2_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"522b13ea02d6d62e54180bd13595eb0e40333d48\",\"title\":\"Natural Language Description of Surveillance Events\",\"url\":\"https://www.semanticscholar.org/paper/522b13ea02d6d62e54180bd13595eb0e40333d48\",\"venue\":\"ICITAM\",\"year\":2017},{\"arxivId\":\"1711.08097\",\"authors\":[{\"authorId\":\"8598253\",\"name\":\"Wang-Li Hao\"},{\"authorId\":\"145274329\",\"name\":\"Zhaoxiang Zhang\"},{\"authorId\":\"32561502\",\"name\":\"H. Guan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dde65325dc7600d02983a76bd54693f0050946a4\",\"title\":\"Integrating both Visual and Audio Cues for Enhanced Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/dde65325dc7600d02983a76bd54693f0050946a4\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3449299\",\"name\":\"Atsushi Ushiku\"},{\"authorId\":\"144921213\",\"name\":\"H. Hashimoto\"},{\"authorId\":\"50594656\",\"name\":\"A. Hashimoto\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abd4152773ebb97b90163b9a6bbdf2075e825481\",\"title\":\"Procedural Text Generation from an Execution Video\",\"url\":\"https://www.semanticscholar.org/paper/abd4152773ebb97b90163b9a6bbdf2075e825481\",\"venue\":\"IJCNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/978-3-319-68548-9_36\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a\",\"title\":\"Towards Video Captioning with Naming: A Novel Dataset and a Multi-modal Approach\",\"url\":\"https://www.semanticscholar.org/paper/ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a\",\"venue\":\"ICIAP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":\"10.1145/3009906\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6e60536c847ac25dba4c1c071e0355e5537fe061\",\"title\":\"Computer Vision and Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/6e60536c847ac25dba4c1c071e0355e5537fe061\",\"venue\":\"ACM Comput. Surv.\",\"year\":2017},{\"arxivId\":\"1511.04590\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.5244/C.30.141\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"16aac81ae033f7295d82e5b679400d105170a3e1\",\"title\":\"Oracle Performance for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16aac81ae033f7295d82e5b679400d105170a3e1\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":\"2007.10937\",\"authors\":[{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":\"145984817\",\"name\":\"Yu Xiong\"},{\"authorId\":\"36290866\",\"name\":\"Anyi Rao\"},{\"authorId\":\"1557390077\",\"name\":\"Jiaze Wang\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-58548-8_41\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"0732df185bdfcb9c908ec30bb441252593f58875\",\"title\":\"MovieNet: A Holistic Dataset for Movie Understanding\",\"url\":\"https://www.semanticscholar.org/paper/0732df185bdfcb9c908ec30bb441252593f58875\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9240664\",\"name\":\"A. Arinaldi\"},{\"authorId\":\"2656725\",\"name\":\"M. I. Fanany\"}],\"doi\":\"10.1109/ICOICT.2017.8074679\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74439c25088b5490e1b64cd167cdbe3081f4b2fe\",\"title\":\"Cheating video description based on sequences of gestures\",\"url\":\"https://www.semanticscholar.org/paper/74439c25088b5490e1b64cd167cdbe3081f4b2fe\",\"venue\":\"2017 5th International Conference on Information and Communication Technology (ICoIC7)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"}],\"doi\":\"10.22028/D291-26668\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e9e3afeea446a2ae19e3a8e0678f08b73b0b36b\",\"title\":\"Commonsense knowledge acquisition and applications\",\"url\":\"https://www.semanticscholar.org/paper/5e9e3afeea446a2ae19e3a8e0678f08b73b0b36b\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2596816\",\"name\":\"Leslie N. Smith\"},{\"authorId\":\"152545122\",\"name\":\"D. Bonanno\"},{\"authorId\":\"2886830\",\"name\":\"T. Doster\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"027fbb67a3946d30505a4373b082814b8ce189dc\",\"title\":\"Video Surveillance Autopilot\",\"url\":\"https://www.semanticscholar.org/paper/027fbb67a3946d30505a4373b082814b8ce189dc\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409200612\",\"name\":\"Muhannad A. R. I. Al-Omari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bea7ecf19bd9c823b2b4be05ed0a0ec81d0785b\",\"title\":\"Joint perceptual learning and natural language acquisition for autonomous robots\",\"url\":\"https://www.semanticscholar.org/paper/6bea7ecf19bd9c823b2b4be05ed0a0ec81d0785b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"},{\"authorId\":\"31798873\",\"name\":\"J. Bazin\"}],\"doi\":\"10.1007/978-3-319-46604-0_43\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b21075184eece92f0d4362e38e6199cae131edbc\",\"title\":\"Label-Based Automatic Alignment of Video with Narrative Sentences\",\"url\":\"https://www.semanticscholar.org/paper/b21075184eece92f0d4362e38e6199cae131edbc\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26928334\",\"name\":\"Shivam Duggal\"},{\"authorId\":\"37059411\",\"name\":\"S. Manik\"},{\"authorId\":\"27059378\",\"name\":\"Mohan Ghai\"}],\"doi\":\"10.1145/3163080.3163108\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f781277a40f14ef83485863ba24e937d76d9bb04\",\"title\":\"Amalgamation of Video Description and Multiple Object Localization using single Deep Learning Model\",\"url\":\"https://www.semanticscholar.org/paper/f781277a40f14ef83485863ba24e937d76d9bb04\",\"venue\":\"ICSPS 2017\",\"year\":2017},{\"arxivId\":\"1804.02516\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3448af861bf5d44ce7ab6b25002504815212252e\",\"title\":\"Learning a Text-Video Embedding from Incomplete and Heterogeneous Data\",\"url\":\"https://www.semanticscholar.org/paper/3448af861bf5d44ce7ab6b25002504815212252e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1904.03493\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"46255707\",\"name\":\"Lei Li\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"title\":\"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research\",\"url\":\"https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1907.13487\",\"authors\":[{\"authorId\":\"40457423\",\"name\":\"Y. Liu\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"title\":\"Use What You Have: Video retrieval using representations from collaborative experts\",\"url\":\"https://www.semanticscholar.org/paper/b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":\"1712.06761\",\"authors\":[{\"authorId\":\"2039154\",\"name\":\"Paul Vicol\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"3436589\",\"name\":\"L. Castrej\\u00f3n\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2018.00895\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"523574aca71d8981b4122cce8d132f22391ef26e\",\"title\":\"MovieGraphs: Towards Understanding Human-Centric Situations from Videos\",\"url\":\"https://www.semanticscholar.org/paper/523574aca71d8981b4122cce8d132f22391ef26e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1609.06782\",\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3122865.3122867\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"title\":\"Deep Learning for Video Classification and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"venue\":\"Frontiers of Multimedia Research\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2318546\",\"name\":\"Shiping Wen\"},{\"authorId\":\"9252399\",\"name\":\"Guanghua Ren\"},{\"authorId\":\"48696229\",\"name\":\"Y. Cao\"},{\"authorId\":\"2619052\",\"name\":\"Zhenyuan Guo\"},{\"authorId\":\"145692431\",\"name\":\"Qiang Xiao\"},{\"authorId\":\"145043786\",\"name\":\"Z. Zeng\"},{\"authorId\":\"50592545\",\"name\":\"T. Huang\"}],\"doi\":\"10.1007/978-3-319-92537-0_28\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c1b14ebb101c349e23b4be1021d1b251b7deffd\",\"title\":\"WeiboCluster: An Event-Oriented Sina Weibo Dataset with Estimating Credit\",\"url\":\"https://www.semanticscholar.org/paper/1c1b14ebb101c349e23b4be1021d1b251b7deffd\",\"venue\":\"ISNN\",\"year\":2018},{\"arxivId\":\"1611.07810\",\"authors\":[{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":\"10.1109/CVPR.2017.778\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"title\":\"A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2757212\",\"name\":\"L. Fan\"},{\"authorId\":\"49069534\",\"name\":\"Yixin Chen\"},{\"authorId\":\"145708707\",\"name\":\"P. Wei\"},{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"}],\"doi\":\"10.1109/CVPR.2018.00676\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"037ad2159097dcb7ff29b6c328b859fb64f7078e\",\"title\":\"Inferring Shared Attention in Social Scene Videos\",\"url\":\"https://www.semanticscholar.org/paper/037ad2159097dcb7ff29b6c328b859fb64f7078e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"104510214\",\"name\":\"K. Li\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TETCI.2019.2892755\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"title\":\"Visual to Text: Survey of Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1007/978-3-319-48881-3_11\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d2cda0dbb8b2e83ce3e70d818f78d2add803c661\",\"title\":\"Automatic Video Captioning via Multi-channel Sequential Encoding\",\"url\":\"https://www.semanticscholar.org/paper/d2cda0dbb8b2e83ce3e70d818f78d2add803c661\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-54407-6_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"title\":\"Video Captioning via Sentence Augmentation and Spatio-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"2008.06880\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":null,\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413880\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"title\":\"Poet: Product-oriented Video Captioner for E-commerce\",\"url\":\"https://www.semanticscholar.org/paper/72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46395893\",\"name\":\"Y. Wang\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"39527132\",\"name\":\"Xiaojie Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c53127df6a87a2842f6b64440382890c397e4daf\",\"title\":\"Video description with integrated visual and textual information\",\"url\":\"https://www.semanticscholar.org/paper/c53127df6a87a2842f6b64440382890c397e4daf\",\"venue\":\"China Communications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51906966\",\"name\":\"Muralidhar Pantula\"},{\"authorId\":\"32795089\",\"name\":\"K. Kuppusamy\"}],\"doi\":\"10.1007/s11042-019-7363-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b14c95c7989b786783beec4046857f0b831e6f4\",\"title\":\"AuDIVA: A tool for embedding Audio Descriptions to enhance Video Accessibility for Persons with Visual Impairments\",\"url\":\"https://www.semanticscholar.org/paper/7b14c95c7989b786783beec4046857f0b831e6f4\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"title\":\"Video Captioning and Retrieval Models with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50522234\",\"name\":\"K. Gregory\"},{\"authorId\":\"30849350\",\"name\":\"D. Moore\"},{\"authorId\":null,\"name\":\"Nick Troccoli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"520037bd5b226af846e142f36ea982e63faa966e\",\"title\":\"Visual-Tex : Video Tagging using Frame Captions CS 229 Final Project Report , December 16 , 2016\",\"url\":\"https://www.semanticscholar.org/paper/520037bd5b226af846e142f36ea982e63faa966e\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2898850\",\"name\":\"R\\u00e9mi Ronfard\"},{\"authorId\":\"2898850\",\"name\":\"R\\u00e9mi Ronfard\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf3ca7fa12d224416480b303c3024531b0122a30\",\"title\":\"Five Challenges for Intelligent Cinematography and Editing R\\u00e9mi Ronfard\",\"url\":\"https://www.semanticscholar.org/paper/cf3ca7fa12d224416480b303c3024531b0122a30\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"}],\"doi\":\"10.25781/KAUST-VR909\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5c549fd9172f2d1be80cc77b0de9e90d3416463\",\"title\":\"Efficient Localization of Human Actions and Moments in Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5c549fd9172f2d1be80cc77b0de9e90d3416463\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39356242\",\"name\":\"Andrea Amelio Ravelli\"},{\"authorId\":\"1715983\",\"name\":\"Oier Lopez de Lacalle\"},{\"authorId\":\"1733049\",\"name\":\"Eneko Agirre\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42238855eb05afbc3cd9b68fb673eb8190c1c60f\",\"title\":\"A Comparison of Representation Models in a Non-Conventional Semantic Similarity Scenario\",\"url\":\"https://www.semanticscholar.org/paper/42238855eb05afbc3cd9b68fb673eb8190c1c60f\",\"venue\":\"CLiC-it\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"title\":\"Large-scale video analysis and understanding\",\"url\":\"https://www.semanticscholar.org/paper/6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1707.06436\",\"authors\":[{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"},{\"authorId\":\"3393640\",\"name\":\"Soma Shirakabe\"},{\"authorId\":null,\"name\":\"Yun He\"},{\"authorId\":\"9935341\",\"name\":\"S. Ueta\"},{\"authorId\":\"5014206\",\"name\":\"T. Suzuki\"},{\"authorId\":\"49897653\",\"name\":\"K. Abe\"},{\"authorId\":\"2554424\",\"name\":\"Asako Kanezaki\"},{\"authorId\":\"49133490\",\"name\":\"Shinichiro Morita\"},{\"authorId\":\"22219521\",\"name\":\"Toshiyuki Yabe\"},{\"authorId\":\"50544018\",\"name\":\"Yoshihiro Kanehara\"},{\"authorId\":\"22174281\",\"name\":\"Hiroya Yatsuyanagi\"},{\"authorId\":\"1692565\",\"name\":\"S. Maruyama\"},{\"authorId\":\"10756539\",\"name\":\"Ryousuke Takasawa\"},{\"authorId\":\"3217653\",\"name\":\"Masataka Fuchida\"},{\"authorId\":\"2642022\",\"name\":\"Y. Miyashita\"},{\"authorId\":\"34935749\",\"name\":\"Kazushige Okayasu\"},{\"authorId\":\"20505300\",\"name\":\"Yuta Matsuzaki\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a9ca15aebadad4b7cfa18c5af91c431d6045b86\",\"title\":\"cvpaper.challenge in 2016: Futuristic Computer Vision through 1, 600 Papers Survey\",\"url\":\"https://www.semanticscholar.org/paper/8a9ca15aebadad4b7cfa18c5af91c431d6045b86\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"}],\"doi\":\"10.1145/2903513.2903517\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"968ab65077c4be1c1071120052b2e4b4f3d3c59a\",\"title\":\"\\\"Seeing is believing: the quest for multimodal knowledge\\\" by Gerard de Melo and Niket Tandon, with Martin Vesely as coordinator\",\"url\":\"https://www.semanticscholar.org/paper/968ab65077c4be1c1071120052b2e4b4f3d3c59a\",\"venue\":\"LINK\",\"year\":2016},{\"arxivId\":\"1707.09074\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2017.562\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f6e65e93566b909c04d2d74bc1ade1592e476adf\",\"title\":\"Learning from Video and Text via Large-Scale Discriminative Clustering\",\"url\":\"https://www.semanticscholar.org/paper/f6e65e93566b909c04d2d74bc1ade1592e476adf\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152823345\",\"name\":\"M. Sol\\u00e8r\"},{\"authorId\":\"31798873\",\"name\":\"J. Bazin\"},{\"authorId\":\"47520207\",\"name\":\"O. Wang\"},{\"authorId\":\"153243248\",\"name\":\"A. Krause\"},{\"authorId\":\"1388791172\",\"name\":\"Alexander Sorkine-Hornung\"}],\"doi\":\"10.1007/978-3-319-48881-3_59\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"333aa36e80f1a7fa29cf069d81d4d2e12679bc67\",\"title\":\"Suggesting Sounds for Images from Video Collections\",\"url\":\"https://www.semanticscholar.org/paper/333aa36e80f1a7fa29cf069d81d4d2e12679bc67\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50565167\",\"name\":\"K. Barnard\"}],\"doi\":\"10.2200/S00705ED1V01Y201602COV007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4e69a89a9e322ca78ed3f1140e74aee5ab844f1\",\"title\":\"Computational Methods for Integrating Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c4e69a89a9e322ca78ed3f1140e74aee5ab844f1\",\"venue\":\"Computational Methods for Integrating Vision and Language\",\"year\":2016},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1608.04959\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1145/2964284.2984062\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"41eec260e0980f8fd0af46f0dfc043139087a928\",\"title\":\"Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/41eec260e0980f8fd0af46f0dfc043139087a928\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1506.02059\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1d775083122bb3efa4ea9d235a34276644bfee7\",\"title\":\"Sentence Directed Video Object Codetection\",\"url\":\"https://www.semanticscholar.org/paper/f1d775083122bb3efa4ea9d235a34276644bfee7\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2400138\",\"name\":\"N. Mostafazadeh\"},{\"authorId\":\"144136646\",\"name\":\"X. Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"144354055\",\"name\":\"Elizabeth Clark\"},{\"authorId\":\"144290719\",\"name\":\"D. Elson\"},{\"authorId\":\"2320509\",\"name\":\"Marjan Ghazvininejad\"},{\"authorId\":\"144380001\",\"name\":\"A. Gordon\"},{\"authorId\":null,\"name\":\"Gunhee Kim\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"50556697\",\"name\":\"S. Lukin\"},{\"authorId\":\"144431718\",\"name\":\"J. Magalh\\u00e3es\"},{\"authorId\":\"143945660\",\"name\":\"L. Martin\"},{\"authorId\":null,\"name\":\"Saif Mohammad\"},{\"authorId\":\"2757194\",\"name\":\"Mark O. Riedl\"},{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"804e20b0e07157fb466999c6674c72c39de979e2\",\"title\":\"Event-centric Context Modeling : The Case of Story Comprehension and Story Generation\",\"url\":\"https://www.semanticscholar.org/paper/804e20b0e07157fb466999c6674c72c39de979e2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"title\":\"Trainable performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John R. Smith\"},{\"authorId\":\"5113463\",\"name\":\"D. Joshi\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"},{\"authorId\":\"1716836\",\"name\":\"W. Hsu\"},{\"authorId\":\"33334609\",\"name\":\"Jozef Cota\"}],\"doi\":\"10.1145/3123266.3127906\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ebb463f1b50997ad28624bccf4397986ab7837d\",\"title\":\"Harnessing A.I. for Augmenting Creativity: Application to Movie Trailer Creation\",\"url\":\"https://www.semanticscholar.org/paper/7ebb463f1b50997ad28624bccf4397986ab7837d\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3227595\",\"name\":\"Blaine Rister\"},{\"authorId\":\"36119458\",\"name\":\"D. Lawson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1baa9fdfb27f561be67c52ef55b0e3587f306cff\",\"title\":\"Image Captioning with Attention\",\"url\":\"https://www.semanticscholar.org/paper/1baa9fdfb27f561be67c52ef55b0e3587f306cff\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"title\":\"Story Understanding through Semantic Analysis and Automatic Alignment of Text and Video\",\"url\":\"https://www.semanticscholar.org/paper/39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"title\":\"Image Input OR Video Hierarchical LSTMs with Adaptive Attention ( hLSTMat ) Feature Extraction Generated Captions Losses\",\"url\":\"https://www.semanticscholar.org/paper/e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46958420\",\"name\":\"Tianyi Wang\"},{\"authorId\":\"47539594\",\"name\":\"Jiang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1007/978-3-030-00776-8_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c50c91875767ec7c6391d99d30838d90275a0f1b\",\"title\":\"Collaborative Detection and Caption Network\",\"url\":\"https://www.semanticscholar.org/paper/c50c91875767ec7c6391d99d30838d90275a0f1b\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.06326\",\"authors\":[{\"authorId\":\"2994471\",\"name\":\"X. Zhang\"},{\"authorId\":\"40353974\",\"name\":\"K. Matzen\"},{\"authorId\":\"50393245\",\"name\":\"Vivien L. Nguyen\"},{\"authorId\":\"144210947\",\"name\":\"Dillon Yao\"},{\"authorId\":\"49890547\",\"name\":\"You Zhang\"},{\"authorId\":\"47383180\",\"name\":\"R. Ng\"}],\"doi\":\"10.1145/3306346.3323015\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4150bde5405df091ad07e59e2480fd651758712f\",\"title\":\"Synthetic defocus and look-ahead autofocus for casual videography\",\"url\":\"https://www.semanticscholar.org/paper/4150bde5405df091ad07e59e2480fd651758712f\",\"venue\":\"ACM Trans. Graph.\",\"year\":2019},{\"arxivId\":\"1601.03896\",\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.24963/ijcai.2017/704\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66935500\",\"name\":\"P. Deshmukh\"},{\"authorId\":\"51502239\",\"name\":\"Rani S. Lande\"}],\"doi\":\"10.1109/ICICT48043.2020.9112454\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8ce3c61b083ce5e66e34df3e1113cb603a6a6632\",\"title\":\"Convolutional Neural Network based Review System for Automatic Past Event Search using Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8ce3c61b083ce5e66e34df3e1113cb603a6a6632\",\"venue\":\"2020 International Conference on Inventive Computation Technologies (ICICT)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/ICPR.2016.7900081\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d8eb0ccd00d66b649c6a4c06edb0e34093a2357\",\"title\":\"Automatic video description generation via LSTM with joint two-stream encoding\",\"url\":\"https://www.semanticscholar.org/paper/5d8eb0ccd00d66b649c6a4c06edb0e34093a2357\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"af5b71c042e2bf39e5085ad5b5f1215e129989df\",\"title\":\"Deep Learning for Semantic Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/af5b71c042e2bf39e5085ad5b5f1215e129989df\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9382626\",\"name\":\"M. Amaresh\"},{\"authorId\":\"144902122\",\"name\":\"S. Chitrakala\"}],\"doi\":\"10.1109/ICCSP.2019.8698097\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aad4525b28b18fde9c793ab387ac327802ef71d2\",\"title\":\"Video Captioning using Deep Learning: An Overview of Methods, Datasets and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/aad4525b28b18fde9c793ab387ac327802ef71d2\",\"venue\":\"2019 International Conference on Communication and Signal Processing (ICCSP)\",\"year\":2019},{\"arxivId\":\"1611.09312\",\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"153925540\",\"name\":\"C. Grana\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2017.339\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"title\":\"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1809.06181\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"134724966\",\"name\":\"Shouling Ji\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\"},{\"authorId\":\"98289428\",\"name\":\"G. Yang\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00957\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6a976c123037b138946f6767e1aa0de84b1682d4\",\"title\":\"Dual Encoding for Zero-Example Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6a976c123037b138946f6767e1aa0de84b1682d4\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145488897\",\"name\":\"Hongyin Luo\"},{\"authorId\":\"1754057\",\"name\":\"Mitra Mohtarami\"},{\"authorId\":\"145898106\",\"name\":\"James R. Glass\"},{\"authorId\":\"48180941\",\"name\":\"K. Krishnamurthy\"},{\"authorId\":\"46362798\",\"name\":\"Brigitte Richardson\"}],\"doi\":\"10.21437/interspeech.2019-1736\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a58cd5975fe958e11642edfc63e479c9c066b57d\",\"title\":\"Integrating Video Retrieval and Moment Detection in a Unified Corpus for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a58cd5975fe958e11642edfc63e479c9c066b57d\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1803.00057\",\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":\"10.1109/CVPR.2018.00912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"title\":\"A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1804.02748\",\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"12387007\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc50c9392fd23b6c88915177c6ae904a498aacea\",\"title\":\"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset\",\"url\":\"https://www.semanticscholar.org/paper/fc50c9392fd23b6c88915177c6ae904a498aacea\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2006.13608\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"50144812\",\"name\":\"Z. Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394486.3403325\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d271e93c7566b231e560c48b4cc4942077d762f9\",\"title\":\"Comprehensive Information Integration Modeling Framework for Video Titling\",\"url\":\"https://www.semanticscholar.org/paper/d271e93c7566b231e560c48b4cc4942077d762f9\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"1704.01518\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"39578749\",\"name\":\"Siyu Tang\"},{\"authorId\":\"2390510\",\"name\":\"Seong Joon Oh\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2017.447\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"db2fecc8b1bd175d39687eb471360707a5fddb03\",\"title\":\"Generating Descriptions with Grounded and Co-referenced People\",\"url\":\"https://www.semanticscholar.org/paper/db2fecc8b1bd175d39687eb471360707a5fddb03\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1511.04670\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ed7d774684a1770445c1c53e276011a8364b9e2\",\"title\":\"Uncovering Temporal Context for Video Question and Answering\",\"url\":\"https://www.semanticscholar.org/paper/9ed7d774684a1770445c1c53e276011a8364b9e2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"}],\"doi\":\"10.1145/3390891\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb9fb10f604a196515e48ad90f217d33794f5991\",\"title\":\"Recurrent Attention Network with Reinforced Generator for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/cb9fb10f604a196515e48ad90f217d33794f5991\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"2007.13913\",\"authors\":[{\"authorId\":\"144568152\",\"name\":\"David M. Chan\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ac8179dbdd256e514700b076673b6d5b9083251\",\"title\":\"Active Learning for Video Description With Cluster-Regularized Ensemble Ranking\",\"url\":\"https://www.semanticscholar.org/paper/8ac8179dbdd256e514700b076673b6d5b9083251\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3493606\",\"name\":\"M. Bruni\"},{\"authorId\":\"1789269\",\"name\":\"Tiberio Uricchio\"},{\"authorId\":\"2831602\",\"name\":\"Lorenzo Seidenari\"},{\"authorId\":\"8196487\",\"name\":\"A. D. Bimbo\"}],\"doi\":\"10.1145/2964284.2967301\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e5eedbdf779277b96caf4309aa631ac3e38ae6d\",\"title\":\"Do Textual Descriptions Help Action Recognition?\",\"url\":\"https://www.semanticscholar.org/paper/5e5eedbdf779277b96caf4309aa631ac3e38ae6d\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1707.00836\",\"authors\":[{\"authorId\":\"2593979\",\"name\":\"Kyung-Min Kim\"},{\"authorId\":\"2939188\",\"name\":\"Min-Oh Heo\"},{\"authorId\":\"117172343\",\"name\":\"Seongho Choi\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":\"10.24963/ijcai.2017/280\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e6cc717311c9c3dcf7279bc44e0c25b29650c15\",\"title\":\"DeepStory: Video Story QA by Deep Embedded Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/7e6cc717311c9c3dcf7279bc44e0c25b29650c15\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"1605.02697\",\"authors\":[{\"authorId\":\"145478807\",\"name\":\"Mateusz Malinowski\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"}],\"doi\":\"10.1007/s11263-017-1038-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7ddf708442dad7ed2978658b101c797c7c10220\",\"title\":\"Ask Your Neurons: A Deep Learning Approach to Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f7ddf708442dad7ed2978658b101c797c7c10220\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":\"10.1007/s11263-017-1018-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44855e53801d09763c1fb5f90ab73e5c3758a728\",\"title\":\"Sentence Directed Video Object Codiscovery\",\"url\":\"https://www.semanticscholar.org/paper/44855e53801d09763c1fb5f90ab73e5c3758a728\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"2010.06647\",\"authors\":[{\"authorId\":\"153634296\",\"name\":\"Matthew Hutchinson\"},{\"authorId\":\"74882299\",\"name\":\"Vijay Gadepally\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"65955a106905afb90a2a2fa74e48c6d6d597892f\",\"title\":\"Video Action Understanding: A Tutorial\",\"url\":\"https://www.semanticscholar.org/paper/65955a106905afb90a2a2fa74e48c6d6d597892f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"Li Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"117aae1dc5b3aee679a690f7dab84e9a23add930\",\"title\":\"AGE AND VIDEO CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/117aae1dc5b3aee679a690f7dab84e9a23add930\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7184058\",\"name\":\"Daehun Kim\"},{\"authorId\":\"1565662585\",\"name\":\"Joungmin Beh\"},{\"authorId\":\"152829188\",\"name\":\"Y. Chen\"},{\"authorId\":\"81950948\",\"name\":\"Hanseok Ko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fa160f17a17bbf8bb9ee1aa81b7c2a4975c39885\",\"title\":\"KU-ISPL TRECVID 2017 VTT System\",\"url\":\"https://www.semanticscholar.org/paper/fa160f17a17bbf8bb9ee1aa81b7c2a4975c39885\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3448647\",\"name\":\"A. Matamala\"}],\"doi\":\"10.1075/resla.17001.mat\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8bbffcb218de31c07cd93a6350c0c6491b69f330\",\"title\":\"The VIW project Multimodal corpus linguistics for audio description analysis\",\"url\":\"https://www.semanticscholar.org/paper/8bbffcb218de31c07cd93a6350c0c6491b69f330\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1809.07257\",\"authors\":[{\"authorId\":\"2662002\",\"name\":\"Oliver Nina\"},{\"authorId\":\"47238599\",\"name\":\"W. Garcia\"},{\"authorId\":\"47637016\",\"name\":\"Scott Clouse\"},{\"authorId\":\"1858702\",\"name\":\"A. Yilmaz\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"title\":\"MTLE: A Multitask Learning Encoder of Visual Feature Representations for Video and Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1512.02949\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3bcca85ad84806be6d38d3882f7a6aac0ad90253\",\"title\":\"Video captioning with recurrent networks based on frame- and video-level features and visual content classification\",\"url\":\"https://www.semanticscholar.org/paper/3bcca85ad84806be6d38d3882f7a6aac0ad90253\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"51043468\",\"name\":\"Maksim Bolonkin\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ba0bf9323c2d79300f1a433ff8b4fe0a00ad889\",\"title\":\"VideoMCC: a New Benchmark for Video Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7ba0bf9323c2d79300f1a433ff8b4fe0a00ad889\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2011.11071\",\"authors\":[{\"authorId\":\"2028596941\",\"name\":\"Andreea-Maria Oncescu\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1614034792\",\"name\":\"Yang Liu\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"title\":\"QuerYD: A video dataset with high-quality textual and audio narrations\",\"url\":\"https://www.semanticscholar.org/paper/6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":15184723,\"doi\":\"10.1109/CVPR.2015.7298940\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":30,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721801\",\"name\":\"C. Fellbaum\"}],\"doi\":\"10.2307/417141\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d87ceda3042f781c341ac17109d1e94a717f5f60\",\"title\":\"WordNet : an electronic lexical database\",\"url\":\"https://www.semanticscholar.org/paper/d87ceda3042f781c341ac17109d1e94a717f5f60\",\"venue\":\"\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24492532\",\"name\":\"Jian-xiong Xiao\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1865091\",\"name\":\"Krista A. Ehinger\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2010.5539970\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"908091b4a8757c3b2f7d9cfa2c4f616ee12c5157\",\"title\":\"SUN database: Large-scale scene recognition from abbey to zoo\",\"url\":\"https://www.semanticscholar.org/paper/908091b4a8757c3b2f7d9cfa2c4f616ee12c5157\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Xmedia recode. http://www.xmedia-recode\",\"url\":\"\",\"venue\":\"Xmedia recode. http://www.xmedia-recode\",\"year\":2014},{\"arxivId\":\"1403.6173\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"113090874\",\"name\":\"W. Qiu\"},{\"authorId\":\"33985877\",\"name\":\"Annemarie Friedrich\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-11752-2_15\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"889e723cd6d581e120ee6776b231fdf69707ab50\",\"title\":\"Coherent Multi-sentence Video Description with Variable Level of Detail\",\"url\":\"https://www.semanticscholar.org/paper/889e723cd6d581e120ee6776b231fdf69707ab50\",\"venue\":\"GCPR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Gemulla . Clausie : Clausebased open information extraction\",\"url\":\"\",\"venue\":\"In Proc . WWW\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1875906\",\"name\":\"Luciano Del Corro\"},{\"authorId\":\"1777107\",\"name\":\"Rainer Gemulla\"}],\"doi\":\"10.1145/2488388.2488420\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"406f6caa0b091cd9305d69b58e075765728b3802\",\"title\":\"ClausIE: clause-based open information extraction\",\"url\":\"https://www.semanticscholar.org/paper/406f6caa0b091cd9305d69b58e075765728b3802\",\"venue\":\"WWW '13\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"36794849\",\"name\":\"L. Zhang\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.1109/ICCVW.2011.6130425\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fbadf89b990acedf23e1df03d4869010d2dbc59e\",\"title\":\"Human Focused Video Description\",\"url\":\"https://www.semanticscholar.org/paper/fbadf89b990acedf23e1df03d4869010d2dbc59e\",\"venue\":\"2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/CVPR.2011.5995466\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"169b847e69c35cfd475eb4dcc561a24de11762ca\",\"title\":\"Baby talk: Understanding and generating simple image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/169b847e69c35cfd475eb4dcc561a24de11762ca\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2013.340\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"title\":\"A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching\",\"url\":\"https://www.semanticscholar.org/paper/a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.441\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d721f4d64b8e722222c876f0a0f226ed49476347\",\"title\":\"Action Recognition with Improved Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/d721f4d64b8e722222c876f0a0f226ed49476347\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145592791\",\"name\":\"P. Kuznetsova\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a0d0f6c5a69b264710df0230696f47c5918e2f2\",\"title\":\"Collective Generation of Natural Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2a0d0f6c5a69b264710df0230696f47c5918e2f2\",\"venue\":\"ACL\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"1931707\",\"name\":\"M. B\\u00e4uml\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1109/CVPR.2012.6247986\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"441132dd4ec14991644723c9642ac3a63181753e\",\"title\":\"\\u201cKnock! Knock! Who is it?\\u201d probabilistic person identification in TV-series\",\"url\":\"https://www.semanticscholar.org/paper/441132dd4ec14991644723c9642ac3a63181753e\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"3056091\",\"name\":\"M. Everingham\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/cvprw.2009.5206513\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03d1d0a665e358863ff4de9ee7d78f64edd7e756\",\"title\":\"\\u201cWho are you?\\u201d - Learning person specific classifiers from video\",\"url\":\"https://www.semanticscholar.org/paper/03d1d0a665e358863ff4de9ee7d78f64edd7e756\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fbdbe747c6aa8b35b981d21e475ff1506a1bae66\",\"title\":\"Composing Simple Image Descriptions using Web-scale N-grams\",\"url\":\"https://www.semanticscholar.org/paper/fbdbe747c6aa8b35b981d21e475ff1506a1bae66\",\"venue\":\"CoNLL\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3056091\",\"name\":\"M. Everingham\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.20.92\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4aba56927d7841c0aaedf5c73d42ccfadd75124\",\"title\":\"Hello! My name is... Buffy'' -- Automatic Naming of Characters in TV Video\",\"url\":\"https://www.semanticscholar.org/paper/a4aba56927d7841c0aaedf5c73d42ccfadd75124\",\"venue\":\"BMVC\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"3167681\",\"name\":\"Yoav Artzi\"},{\"authorId\":\"34176020\",\"name\":\"Jesse Dodge\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":\"10.3115/v1/P14-1135\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b82383b09bedb765ab9c5c2153978035aacd830\",\"title\":\"Context-dependent Semantic Parsing for Time Expressions\",\"url\":\"https://www.semanticscholar.org/paper/7b82383b09bedb765ab9c5c2153978035aacd830\",\"venue\":\"ACL\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143790066\",\"name\":\"Dipanjan Das\"},{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"},{\"authorId\":\"144365875\",\"name\":\"Noah A. Smith\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7b9ec476f706db60ed07dc6886227e89e3c533df\",\"title\":\"An Exact Dual Decomposition Algorithm for Shallow Semantic Parsing with Constraints\",\"url\":\"https://www.semanticscholar.org/paper/7b9ec476f706db60ed07dc6886227e89e3c533df\",\"venue\":\"*SEM@NAACL-HLT\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"145297531\",\"name\":\"A. Lai\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1162/tacl_a_00166\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"44040913380206991b1991daf1192942e038fe31\",\"title\":\"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\"url\":\"https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"title\":\"Im2Text: Describing Images Using 1 Million Captioned Photographs\",\"url\":\"https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710748\",\"name\":\"P. Hanckmann\"},{\"authorId\":\"1682184\",\"name\":\"K. Schutte\"},{\"authorId\":\"1909303\",\"name\":\"G. Burghouts\"}],\"doi\":\"10.1007/978-3-642-33863-2_37\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"022c941ea709824129fa13ecbbe131bdea2ceaa5\",\"title\":\"Automated Textual Descriptions for a Wide Range of Video Events with 48 Human Actions\",\"url\":\"https://www.semanticscholar.org/paper/022c941ea709824129fa13ecbbe131bdea2ceaa5\",\"venue\":\"ECCV Workshops\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"24138684\",\"name\":\"Dominikus Wetzel\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"}],\"doi\":\"10.1162/tacl_a_00207\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"title\":\"Grounding Action Descriptions in Videos\",\"url\":\"https://www.semanticscholar.org/paper/21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"title\":\"Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1888731\",\"name\":\"M. Hejrati\"},{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/978-3-642-15561-1_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"title\":\"Every Picture Tells a Story: Generating Sentences from Images\",\"url\":\"https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144574004\",\"name\":\"Z. Zhong\"},{\"authorId\":\"34789794\",\"name\":\"H. Ng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9931b8ea6594b97c7dfca93936a2d95a38167046\",\"title\":\"It Makes Sense: A Wide-Coverage Word Sense Disambiguation System for Free Text\",\"url\":\"https://www.semanticscholar.org/paper/9931b8ea6594b97c7dfca93936a2d95a38167046\",\"venue\":\"ACL\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"40085065\",\"name\":\"Percy Liang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-10590-1_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"title\":\"Linking People in Videos with \\\"Their\\\" Names Using Coreference Resolution\",\"url\":\"https://www.semanticscholar.org/paper/a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2135004\",\"name\":\"K. Schuler\"},{\"authorId\":\"145762466\",\"name\":\"A. Korhonen\"},{\"authorId\":\"1783500\",\"name\":\"S. Brown\"}],\"doi\":\"10.3115/1620950.1620957\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"edf92b92c3bba4a1d79e9b6f47e3e413520dc0e5\",\"title\":\"VerbNet overview, extensions, mappings and applications\",\"url\":\"https://www.semanticscholar.org/paper/edf92b92c3bba4a1d79e9b6f47e3e413520dc0e5\",\"venue\":\"HLT-NAACL\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1749194\",\"name\":\"Collin F. Baker\"},{\"authorId\":\"2912454\",\"name\":\"C. Fillmore\"},{\"authorId\":\"145543490\",\"name\":\"J. Lowe\"}],\"doi\":\"10.3115/980845.980860\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"547f23597f9ec8a93f66cedaa6fbfb73960426b1\",\"title\":\"The Berkeley FrameNet Project\",\"url\":\"https://www.semanticscholar.org/paper/547f23597f9ec8a93f66cedaa6fbfb73960426b1\",\"venue\":\"COLING-ACL\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fad611e35b3731740b4d8b754241e77add5a70b9\",\"title\":\"Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/fad611e35b3731740b4d8b754241e77add5a70b9\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A Barbu\"},{\"authorId\":null,\"name\":\"A Bridge\"},{\"authorId\":null,\"name\":\"Z Burchill\"},{\"authorId\":null,\"name\":\"D Coroian\"},{\"authorId\":null,\"name\":\"S Dickinson\"},{\"authorId\":null,\"name\":\"S Fidler\"},{\"authorId\":null,\"name\":\"A Michaux\"},{\"authorId\":null,\"name\":\"S Mussman\"},{\"authorId\":null,\"name\":\"S Narayanaswamy\"},{\"authorId\":null,\"name\":\"D Salvi\"},{\"authorId\":null,\"name\":\"L Schmidt\"},{\"authorId\":null,\"name\":\"J Shangguan\"},{\"authorId\":null,\"name\":\"J M Siskind\"},{\"authorId\":null,\"name\":\"J Waggoner\"},{\"authorId\":null,\"name\":\"S Wang\"},{\"authorId\":null,\"name\":\"J Wei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yin, and Z. Zhang. Video in sentences out. In UAI\",\"url\":\"\",\"venue\":\"Yin, and Z. Zhang. Video in sentences out. In UAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2135004\",\"name\":\"K. Schuler\"},{\"authorId\":\"145762466\",\"name\":\"A. Korhonen\"},{\"authorId\":\"2980165\",\"name\":\"Neville Ryant\"},{\"authorId\":\"145755155\",\"name\":\"Martha Palmer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c7421135aae4fb46dae513abceb0a320dad48f10\",\"title\":\"Extending VerbNet with Novel Verb Classes\",\"url\":\"https://www.semanticscholar.org/paper/c7421135aae4fb46dae513abceb0a320dad48f10\",\"venue\":\"LREC\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2909813\",\"name\":\"C. C. Tan\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1145/2072298.2072411\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"261ae2f6f426b5efd32ea5b981751d1c63c5309d\",\"title\":\"Towards textually describing complex video contents with audio-visual concept classifiers\",\"url\":\"https://www.semanticscholar.org/paper/261ae2f6f426b5efd32ea5b981751d1c63c5309d\",\"venue\":\"MM '11\",\"year\":2011},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38087946\",\"name\":\"Anthony Fader\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"},{\"authorId\":\"1741101\",\"name\":\"Oren Etzioni\"}],\"doi\":\"10.1145/2623330.2623677\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f86ec155cce6259e5230aaad3b762343757feb1d\",\"title\":\"Open question answering over curated and extracted knowledge bases\",\"url\":\"https://www.semanticscholar.org/paper/f86ec155cce6259e5230aaad3b762343757feb1d\",\"venue\":\"KDD\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2807482\",\"name\":\"Timoth\\u00e9e Cour\"},{\"authorId\":\"35137957\",\"name\":\"C. Jordan\"},{\"authorId\":\"1759302\",\"name\":\"Eleni Miltsakaki\"},{\"authorId\":\"1685978\",\"name\":\"B. Taskar\"}],\"doi\":\"10.1007/978-3-540-88693-8_12\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c980b058f98dc1904ad328c2341a47c31479d076\",\"title\":\"Movie/Script: Alignment and Parsing of Video and Text Transcription\",\"url\":\"https://www.semanticscholar.org/paper/c980b058f98dc1904ad328c2341a47c31479d076\",\"venue\":\"ECCV\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"47680287\",\"name\":\"W. Dong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2009.5206848\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b47265245e8db53a553049dcb27ed3e495fd625\",\"title\":\"ImageNet: A large-scale hierarchical image database\",\"url\":\"https://www.semanticscholar.org/paper/1b47265245e8db53a553049dcb27ed3e495fd625\",\"venue\":\"CVPR 2009\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"144570279\",\"name\":\"Francis R. Bach\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"144189388\",\"name\":\"J. Ponce\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2013.283\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d766bb9364326fbca3d3b40606c6ed5db54f081\",\"title\":\"Finding Actors and Actions in Movies\",\"url\":\"https://www.semanticscholar.org/paper/4d766bb9364326fbca3d3b40606c6ed5db54f081\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1689269\",\"name\":\"C. Liu\"}],\"doi\":\"10.1007/s11263-012-0594-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bbe0819a47a9f3f11dd34bb3ab44a997ef111088\",\"title\":\"Dense Trajectories and Motion Boundary Descriptors for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bbe0819a47a9f3f11dd34bb3ab44a997ef111088\",\"venue\":\"International Journal of Computer Vision\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1162/tacl_a_00177\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ca7d208ff8d81377e0eaa9723820aeae7a7322d\",\"title\":\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\",\"url\":\"https://www.semanticscholar.org/paper/0ca7d208ff8d81377e0eaa9723820aeae7a7322d\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1766254\",\"name\":\"Olivier Duchenne\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"144570279\",\"name\":\"Francis R. Bach\"},{\"authorId\":\"144189388\",\"name\":\"J. Ponce\"}],\"doi\":\"10.1109/ICCV.2009.5459279\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b67efc8f41fcb3458f6ca5482e73bcc929323a09\",\"title\":\"Automatic annotation of human actions in video\",\"url\":\"https://www.semanticscholar.org/paper/b67efc8f41fcb3458f6ca5482e73bcc929323a09\",\"venue\":\"2009 IEEE 12th International Conference on Computer Vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"2577358\",\"name\":\"P. Srinivasan\"},{\"authorId\":\"46865129\",\"name\":\"J. Shi\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1109/cvprw.2009.5206492\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49927656ede0c75af22ca73dcf4abba028839650\",\"title\":\"Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos\",\"url\":\"https://www.semanticscholar.org/paper/49927656ede0c75af22ca73dcf4abba028839650\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"2677488\",\"name\":\"\\u00c0gata Lapedriza\"},{\"authorId\":\"40599257\",\"name\":\"J. Xiao\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9667f8264745b626c6173b1310e2ff0298b09cfc\",\"title\":\"Learning Deep Features for Scene Recognition using Places Database\",\"url\":\"https://www.semanticscholar.org/paper/9667f8264745b626c6173b1310e2ff0298b09cfc\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819004\",\"name\":\"A. Salway\"},{\"authorId\":\"1748440\",\"name\":\"B. Lehane\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"}],\"doi\":\"10.1145/1282280.1282354\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b0a87ad78a970846ad5d8e48ac3af0d1b684ace\",\"title\":\"Associating characters with events in films\",\"url\":\"https://www.semanticscholar.org/paper/4b0a87ad78a970846ad5d8e48ac3af0d1b684ace\",\"venue\":\"CIVR '07\",\"year\":2007},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"3261451\",\"name\":\"Benjamin Rozenfeld\"}],\"doi\":\"10.1109/CVPR.2008.4587756\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f86767732f76f478d5845f2e59f99ba106e9265\",\"title\":\"Learning realistic human actions from movies\",\"url\":\"https://www.semanticscholar.org/paper/0f86767732f76f478d5845f2e59f99ba106e9265\",\"venue\":\"2008 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1750652\",\"name\":\"Jonathan Berant\"},{\"authorId\":\"48244232\",\"name\":\"A. Chou\"},{\"authorId\":\"34765463\",\"name\":\"Roy Frostig\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b29447ba499507a259ae9d8f685d60cc1597d7d3\",\"title\":\"Semantic Parsing on Freebase from Question-Answer Pairs\",\"url\":\"https://www.semanticscholar.org/paper/b29447ba499507a259ae9d8f685d60cc1597d7d3\",\"venue\":\"EMNLP\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/cvprw.2009.5206557\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1fd485daa491c0debcd900b3f6bc141c3883812d\",\"title\":\"Actions in context\",\"url\":\"https://www.semanticscholar.org/paper/1fd485daa491c0debcd900b3f6bc141c3883812d\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":\"1204.2742\",\"authors\":[{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"},{\"authorId\":\"48540451\",\"name\":\"Alexander Bridge\"},{\"authorId\":\"3190146\",\"name\":\"Zachary Burchill\"},{\"authorId\":\"49081881\",\"name\":\"D. Coroian\"},{\"authorId\":\"1779136\",\"name\":\"S. Dickinson\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"38414598\",\"name\":\"A. Michaux\"},{\"authorId\":\"2587937\",\"name\":\"Sam Mussman\"},{\"authorId\":\"38052303\",\"name\":\"S. Narayanaswamy\"},{\"authorId\":\"2968009\",\"name\":\"D. Salvi\"},{\"authorId\":\"50269497\",\"name\":\"Lara Schmidt\"},{\"authorId\":\"2060623\",\"name\":\"Jiangnan Shangguan\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"},{\"authorId\":\"32655613\",\"name\":\"J. Waggoner\"},{\"authorId\":\"30102584\",\"name\":\"S. Wang\"},{\"authorId\":\"2223764\",\"name\":\"Jinlian Wei\"},{\"authorId\":\"1813304\",\"name\":\"Yifan Yin\"},{\"authorId\":\"48806246\",\"name\":\"Zhiqi Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"793c1c908672ea71aef9e1b41a46272aa27598f7\",\"title\":\"Video In Sentences Out\",\"url\":\"https://www.semanticscholar.org/paper/793c1c908672ea71aef9e1b41a46272aa27598f7\",\"venue\":\"UAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Castingwords transcription service. http: //castingwords\",\"url\":\"\",\"venue\":\"Castingwords transcription service. http: //castingwords\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2807482\",\"name\":\"Timoth\\u00e9e Cour\"},{\"authorId\":\"9133363\",\"name\":\"B. Sapp\"},{\"authorId\":\"35137957\",\"name\":\"C. Jordan\"},{\"authorId\":\"1685978\",\"name\":\"B. Taskar\"}],\"doi\":\"10.1109/cvprw.2009.5206667\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df08557d4c3d67718f8e7ce244567b369faabf65\",\"title\":\"Learning from ambiguously labeled images\",\"url\":\"https://www.semanticscholar.org/paper/df08557d4c3d67718f8e7ce244567b369faabf65\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":\"1407.1208\",\"authors\":[{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"3215950\",\"name\":\"R\\u00e9mi Lajugie\"},{\"authorId\":\"144570279\",\"name\":\"Francis R. Bach\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"144189388\",\"name\":\"J. Ponce\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1007/978-3-319-10602-1_41\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16ae6b2ee079d290c6bf8f1ceac0fbc3477292fc\",\"title\":\"Weakly Supervised Action Labeling in Videos under Ordering Constraints\",\"url\":\"https://www.semanticscholar.org/paper/16ae6b2ee079d290c6bf8f1ceac0fbc3477292fc\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49693392\",\"name\":\"A. Kojima\"},{\"authorId\":\"46526487\",\"name\":\"Takeshi Tamura\"},{\"authorId\":\"145950023\",\"name\":\"K. Fukunaga\"}],\"doi\":\"10.1023/A:1020346032608\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d53a97a3dd7760b193c0d9a5293b60feff239059\",\"title\":\"Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions\",\"url\":\"https://www.semanticscholar.org/paper/d53a97a3dd7760b193c0d9a5293b60feff239059\",\"venue\":\"International Journal of Computer Vision\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49604675\",\"name\":\"P. Koehn\"},{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"2529583\",\"name\":\"Wade Shen\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"143832874\",\"name\":\"Ondrej Bojar\"},{\"authorId\":\"46898156\",\"name\":\"B. Cowan\"},{\"authorId\":\"1745899\",\"name\":\"Chris Dyer\"},{\"authorId\":\"152378023\",\"name\":\"Hieu T. Hoang\"},{\"authorId\":\"1983801\",\"name\":\"R. Zens\"},{\"authorId\":\"31542143\",\"name\":\"A. Constantin\"},{\"authorId\":\"6376655\",\"name\":\"E. Herbst\"},{\"authorId\":\"89224350\",\"name\":\"C. C. Moran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"99e8d34817ae10d7304521e89c5fbf908b9d856b\",\"title\":\"Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding\",\"url\":\"https://www.semanticscholar.org/paper/99e8d34817ae10d7304521e89c5fbf908b9d856b\",\"venue\":\"\",\"year\":2006},{\"arxivId\":\"1411.5654\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4af49a1ead3c81cc5d023878cb67c5646dd8a04\",\"title\":\"Learning a Recurrent Visual Representation for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/f4af49a1ead3c81cc5d023878cb67c5646dd8a04\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1755162\",\"name\":\"Philipp Koehn\"},{\"authorId\":\"152378023\",\"name\":\"Hieu T. Hoang\"},{\"authorId\":\"2539211\",\"name\":\"Alexandra Birch\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"46898156\",\"name\":\"B. Cowan\"},{\"authorId\":\"2529583\",\"name\":\"Wade Shen\"},{\"authorId\":\"145046497\",\"name\":\"C. Moran\"},{\"authorId\":\"1983801\",\"name\":\"R. Zens\"},{\"authorId\":\"1745899\",\"name\":\"Chris Dyer\"},{\"authorId\":\"143832874\",\"name\":\"Ondrej Bojar\"},{\"authorId\":\"4733879\",\"name\":\"A. Constantin\"},{\"authorId\":\"6376655\",\"name\":\"E. Herbst\"}],\"doi\":\"10.3115/1557769.1557821\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ee2eab4c298c1824a9fb8799ad8eed21be38d21\",\"title\":\"Moses: Open Source Toolkit for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/4ee2eab4c298c1824a9fb8799ad8eed21be38d21\",\"venue\":\"ACL\",\"year\":2007},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3167681\",\"name\":\"Yoav Artzi\"},{\"authorId\":\"143883142\",\"name\":\"Nicholas FitzGerald\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2335e7ff062b7be074e4c4eb1bee025fdb16d1ff\",\"title\":\"Semantic Parsing with Combinatory Categorial Grammars\",\"url\":\"https://www.semanticscholar.org/paper/2335e7ff062b7be074e4c4eb1bee025fdb16d1ff\",\"venue\":\"ACL\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145468862\",\"name\":\"L. Gagnon\"},{\"authorId\":\"2594476\",\"name\":\"C. Chapdelaine\"},{\"authorId\":\"2346870\",\"name\":\"David Byrns\"},{\"authorId\":\"1786367\",\"name\":\"S. Foucher\"},{\"authorId\":\"49858699\",\"name\":\"Maguelonne H\\u00e9ritier\"},{\"authorId\":\"145581156\",\"name\":\"V. Gupta\"}],\"doi\":\"10.1109/CVPRW.2010.5543575\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39f37f16ae38879cd92346aae994516eb54377fb\",\"title\":\"A computer-vision-assisted system for Videodescription scripting\",\"url\":\"https://www.semanticscholar.org/paper/39f37f16ae38879cd92346aae994516eb54377fb\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2001885\",\"name\":\"Ted Pedersen\"},{\"authorId\":\"145984521\",\"name\":\"Siddharth Patwardhan\"},{\"authorId\":\"3075310\",\"name\":\"Jason Michelizzi\"}],\"doi\":\"10.3115/1614025.1614037\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"495f3405da229b903797472c64d09d83659fdb34\",\"title\":\"WordNet: : Similarity - Measuring the Relatedness of Concepts\",\"url\":\"https://www.semanticscholar.org/paper/495f3405da229b903797472c64d09d83659fdb34\",\"venue\":\"AAAI\",\"year\":2004},{\"arxivId\":\"1407.5035\",\"authors\":[{\"authorId\":\"50196944\",\"name\":\"Judy Hoffman\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2368132\",\"name\":\"E. Tzeng\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"40eac6e6bb54e0da7885989f220f4d3f70249950\",\"title\":\"LSDA: Large Scale Detection through Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/40eac6e6bb54e0da7885989f220f4d3f70249950\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Mao\"},{\"authorId\":null,\"name\":\"W Xu\"},{\"authorId\":null,\"name\":\"Y Yang\"},{\"authorId\":null,\"name\":\"J Wang\"},{\"authorId\":null,\"name\":\"A L Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep captioning with multimodal recurrent neural networks (mrnn ). arXiv:1412\",\"url\":\"\",\"venue\":\"Deep captioning with multimodal recurrent neural networks (mrnn ). arXiv:1412\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152869152\",\"name\":\"J. Lakritz\"},{\"authorId\":\"1819004\",\"name\":\"A. Salway\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f09d505669b9b7b559addc0c15ba046e0ee1f2a\",\"title\":\"The Semi-Automatic Generation of Audio Description from Screenplays\",\"url\":\"https://www.semanticscholar.org/paper/1f09d505669b9b7b559addc0c15ba046e0ee1f2a\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143645506\",\"name\":\"P. Over\"},{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"2974372\",\"name\":\"Brian Antonishek\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"},{\"authorId\":\"1693391\",\"name\":\"G. Qu\\u00e9not\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"760f81db6dabbbd0744595e9fc3d55138e5cb88e\",\"title\":\"TRECVID 2015 - An Overview of the Goals, Tasks, Data, Evaluation Mechanisms and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/760f81db6dabbbd0744595e9fc3d55138e5cb88e\",\"venue\":\"TRECVID\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1403846037\",\"name\":\"J. D\\u00edaz-Cintas\"},{\"authorId\":\"1942070\",\"name\":\"P. Orero\"},{\"authorId\":\"117301477\",\"name\":\"A. Remael\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"968744470285521ecf4d5c6977589b6377185020\",\"title\":\"Media for All: Subtitling for the Deaf, Audio Description, and Sign Language\",\"url\":\"https://www.semanticscholar.org/paper/968744470285521ecf4d5c6977589b6377185020\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"title\":\"Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"venue\":\"COLING\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819004\",\"name\":\"A. Salway\"}],\"doi\":\"10.1163/9789401209564_012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d2b2ee554dc37b510172180f5fc86f4dbdc76397\",\"title\":\"A corpus-based analysis of audio description\",\"url\":\"https://www.semanticscholar.org/paper/d2b2ee554dc37b510172180f5fc86f4dbdc76397\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145592791\",\"name\":\"P. Kuznetsova\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1162/tacl_a_00188\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59927ded86ab4f7253fc32efb351e5a13e746ead\",\"title\":\"TreeTalk: Composition and Compression of Trees for Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/59927ded86ab4f7253fc32efb351e5a13e746ead\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143869248\",\"name\":\"C. Liang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"},{\"authorId\":\"143949499\",\"name\":\"J. Cheng\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/CVPR.2011.5995681\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c089c7d8d1413b54f59fc410d88e215902e51638\",\"title\":\"TVParser: An automatic TV video parsing method\",\"url\":\"https://www.semanticscholar.org/paper/c089c7d8d1413b54f59fc410d88e215902e51638\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"34176020\",\"name\":\"Jesse Dodge\"},{\"authorId\":\"46479604\",\"name\":\"Amit Goyal\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"},{\"authorId\":\"1714215\",\"name\":\"K. Stratos\"},{\"authorId\":\"1682965\",\"name\":\"Xufeng Han\"},{\"authorId\":\"40614240\",\"name\":\"A. Mensch\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"355de7460120ddc1150d9ce3756f9848983f7ff4\",\"title\":\"Midge: Generating Image Descriptions From Computer Vision Detections\",\"url\":\"https://www.semanticscholar.org/paper/355de7460120ddc1150d9ce3756f9848983f7ff4\",\"venue\":\"EACL\",\"year\":2012}],\"title\":\"A dataset for Movie Description\",\"topics\":[{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"Computer vision\",\"topicId\":\"5332\",\"url\":\"https://www.semanticscholar.org/topic/5332\"},{\"topic\":\"Computational linguistics\",\"topicId\":\"26807\",\"url\":\"https://www.semanticscholar.org/topic/26807\"},{\"topic\":\"Parallel text\",\"topicId\":\"16792\",\"url\":\"https://www.semanticscholar.org/topic/16792\"},{\"topic\":\"Temporal logic\",\"topicId\":\"480\",\"url\":\"https://www.semanticscholar.org/topic/480\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Parsing\",\"topicId\":\"1910\",\"url\":\"https://www.semanticscholar.org/topic/1910\"}],\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015}\n"