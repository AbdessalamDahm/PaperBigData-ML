"{\"abstract\":\"Automatically describing an image with a sentence is a long-standing challenge in computer vision and natural language processing. Due to recent progress in object detection, attribute classification, action recognition, etc., there is renewed interest in this area. However, evaluating the quality of descriptions has proven to be challenging. We propose a novel paradigm for evaluating image descriptions that uses human consensus. This paradigm consists of three main parts: a new triplet-based method of collecting human annotations to measure consensus, a new automated metric that captures consensus, and two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences describing each image. Our simple metric captures human judgment of consensus better than existing metrics across sentences generated by various sources. We also evaluate five state-of-the-art image description approaches using this new protocol and provide a benchmark for future comparisons. A version of CIDEr named CIDEr-D is available as a part of MS COCO evaluation server to enable systematic evaluation and benchmarking.\",\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\",\"url\":\"https://www.semanticscholar.org/author/8137017\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\",\"url\":\"https://www.semanticscholar.org/author/1699161\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\",\"url\":\"https://www.semanticscholar.org/author/153432684\"}],\"citationVelocity\":372,\"citations\":[{\"arxivId\":\"1711.06475\",\"authors\":[{\"authorId\":\"3165417\",\"name\":\"J. Wu\"},{\"authorId\":\"145565491\",\"name\":\"He Zheng\"},{\"authorId\":\"143946810\",\"name\":\"Bo Zhao\"},{\"authorId\":\"47003565\",\"name\":\"Yixin Li\"},{\"authorId\":\"50736086\",\"name\":\"Baoming Yan\"},{\"authorId\":\"143978866\",\"name\":\"R. Liang\"},{\"authorId\":\"46314609\",\"name\":\"Wenjia Wang\"},{\"authorId\":\"14547213\",\"name\":\"Shipei Zhou\"},{\"authorId\":\"33344887\",\"name\":\"Guosen Lin\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"36637369\",\"name\":\"Y. Wang\"},{\"authorId\":\"47904050\",\"name\":\"Y. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ff40f0760bd8d3c46d72147b0f5b0d4aee2a24f\",\"title\":\"AI Challenger : A Large-scale Dataset for Going Deeper in Image Understanding\",\"url\":\"https://www.semanticscholar.org/paper/3ff40f0760bd8d3c46d72147b0f5b0d4aee2a24f\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"Jianfeng Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"1890615\",\"name\":\"Yujia Huo\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd14c733736d8d977588d450521a9c18bb65818b\",\"title\":\"UvA-DARE ( Digital Academic Repository ) Early Embedding and Late Reranking for Video\",\"url\":\"https://www.semanticscholar.org/paper/fd14c733736d8d977588d450521a9c18bb65818b\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"}],\"doi\":\"10.3929/ethz-b-000204633\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ef504d7e5c94b9f9b8bfd3d1c44c6aa0d0515f2\",\"title\":\"Interest-Based Video Summarization via Subset Selection\",\"url\":\"https://www.semanticscholar.org/paper/8ef504d7e5c94b9f9b8bfd3d1c44c6aa0d0515f2\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1911.03705\",\"authors\":[{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"153150499\",\"name\":\"Ming Shen\"},{\"authorId\":\"97953933\",\"name\":\"Y. Xing\"},{\"authorId\":\"144032123\",\"name\":\"P. Zhou\"},{\"authorId\":\"145201126\",\"name\":\"X. Ren\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"727c9d3846ebd80a9138d0e6c9e995d9afc1d312\",\"title\":\"CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1912135\",\"name\":\"V. Jindal\"}],\"doi\":\"10.18653/v1/N18-4020\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49ed7d18d67c20c4152ba26cc235b2a8e4aec0d5\",\"title\":\"Generating Image Captions in Arabic Using Root-Word Based Recurrent Neural Networks and Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/49ed7d18d67c20c4152ba26cc235b2a8e4aec0d5\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1802.01958\",\"authors\":[{\"authorId\":\"36053954\",\"name\":\"Philipp Harzig\"},{\"authorId\":\"40090845\",\"name\":\"Stephan Brehm\"},{\"authorId\":\"144739319\",\"name\":\"R. Lienhart\"},{\"authorId\":\"40543160\",\"name\":\"C. Kaiser\"},{\"authorId\":\"26895278\",\"name\":\"Ren\\u00e9 Schallner\"}],\"doi\":\"10.1109/MIPR.2018.00035\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2021d9652bf3ec3921bf9c13a06e1ea51588d54\",\"title\":\"Multimodal Image Captioning for Marketing Analysis\",\"url\":\"https://www.semanticscholar.org/paper/c2021d9652bf3ec3921bf9c13a06e1ea51588d54\",\"venue\":\"2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153030413\",\"name\":\"Xuenan Xu\"},{\"authorId\":\"2451839\",\"name\":\"H. Dinkel\"},{\"authorId\":\"3000684\",\"name\":\"Mengyue Wu\"},{\"authorId\":\"143819768\",\"name\":\"K. Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e0bd03576152ef283d37529fb9dedd1d0b249ee3\",\"title\":\"THE SJTU SUBMISSION FOR DCASE2020 TASK 6: A CRNN-GRU BASED REINFORCEMENT LEARNING APPROACH TO AUDIOCAPTION Technical Report\",\"url\":\"https://www.semanticscholar.org/paper/e0bd03576152ef283d37529fb9dedd1d0b249ee3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.09580\",\"authors\":[{\"authorId\":\"41036094\",\"name\":\"C. Deng\"},{\"authorId\":\"46649124\",\"name\":\"Ning Ding\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1007/978-3-030-58601-0_42\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2320f853059c29ce7e70409fa559074d727da5a2\",\"title\":\"Length-Controllable Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2320f853059c29ce7e70409fa559074d727da5a2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1908.02123\",\"authors\":[{\"authorId\":\"36053954\",\"name\":\"Philipp Harzig\"},{\"authorId\":\"35081710\",\"name\":\"Yan-Ying Chen\"},{\"authorId\":\"144180427\",\"name\":\"F. Chen\"},{\"authorId\":\"1803227\",\"name\":\"R. Lienhart\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"660772ecfba0b3257db8100f4924c789f93b5d5c\",\"title\":\"Addressing Data Bias Problems for Chest X-ray Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/660772ecfba0b3257db8100f4924c789f93b5d5c\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145983457\",\"name\":\"A. Ferreira\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"9750f8a5c956eddf27b4288bea456efef41bef1f\",\"title\":\"Question Generation using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/9750f8a5c956eddf27b4288bea456efef41bef1f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144620591\",\"name\":\"X. Wu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"2523380\",\"name\":\"Qingge Ji\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/CVPR.2018.00714\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"title\":\"Interpretable Video Captioning via Trajectory Structured Localization\",\"url\":\"https://www.semanticscholar.org/paper/f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8194130\",\"name\":\"Qinyu Li\"}],\"doi\":\"10.1145/3303083\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"91aa0eb38446643cd622b060a76043b0ca2d7991\",\"title\":\"Rich Visual and Language Representation with Complementary Semantics for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/91aa0eb38446643cd622b060a76043b0ca2d7991\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50994378\",\"name\":\"Chen Chen\"},{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"35910905\",\"name\":\"E. Koh\"},{\"authorId\":\"39618262\",\"name\":\"Sungchul Kim\"},{\"authorId\":\"152452655\",\"name\":\"S. Cohen\"},{\"authorId\":\"35365476\",\"name\":\"R. Rossi\"}],\"doi\":\"10.1109/WACV45572.2020.9093592\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"40e09c8e31bc859d90ba8ceeb70ef7534bf6b7a5\",\"title\":\"Figure Captioning with Relation Maps for Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/40e09c8e31bc859d90ba8ceeb70ef7534bf6b7a5\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2002.08277\",\"authors\":[{\"authorId\":\"1591146157\",\"name\":\"Yixiao Zhang\"},{\"authorId\":\"2026596\",\"name\":\"Xiaosong Wang\"},{\"authorId\":\"1742135\",\"name\":\"Ziyue Xu\"},{\"authorId\":\"2156559\",\"name\":\"Qihang Yu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"},{\"authorId\":\"3262394\",\"name\":\"Daguang Xu\"}],\"doi\":\"10.1609/AAAI.V34I07.6989\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fa82dfbf469a793d7c98804a63a8acf7d9e4a5f\",\"title\":\"When Radiology Report Generation Meets Knowledge Graph\",\"url\":\"https://www.semanticscholar.org/paper/5fa82dfbf469a793d7c98804a63a8acf7d9e4a5f\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1800818087\",\"name\":\"Rajarshi Biswas\"},{\"authorId\":\"39739949\",\"name\":\"Michael Barz\"},{\"authorId\":\"47492789\",\"name\":\"D. Sonntag\"}],\"doi\":\"10.1007/s13218-020-00679-2\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"947be3f8f6089bd6db7cf6b5bfd45ea0543584b0\",\"title\":\"Towards Explanatory Interactive Image Captioning Using Top-Down and Bottom-Up Features, Beam Search and Re-ranking\",\"url\":\"https://www.semanticscholar.org/paper/947be3f8f6089bd6db7cf6b5bfd45ea0543584b0\",\"venue\":\"KI - K\\u00fcnstliche Intelligenz\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46395893\",\"name\":\"Y. Wang\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"39527132\",\"name\":\"Xiaojie Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c53127df6a87a2842f6b64440382890c397e4daf\",\"title\":\"Video description with integrated visual and textual information\",\"url\":\"https://www.semanticscholar.org/paper/c53127df6a87a2842f6b64440382890c397e4daf\",\"venue\":\"China Communications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1582059215\",\"name\":\"Austin Kershaw\"},{\"authorId\":\"144048413\",\"name\":\"M. Bober\"}],\"doi\":\"10.18653/v1/W19-0603\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"25cf62b795a84896f2d2dc2e4f770ed8edc874a6\",\"title\":\"The Lexical Gap: An Improved Measure of Automated Image Description Quality\",\"url\":\"https://www.semanticscholar.org/paper/25cf62b795a84896f2d2dc2e4f770ed8edc874a6\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2011.14901\",\"authors\":[{\"authorId\":\"32236286\",\"name\":\"Annika Lindh\"},{\"authorId\":\"144394786\",\"name\":\"R. Ross\"},{\"authorId\":\"34967075\",\"name\":\"John D. Kelleher\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"027aba2214f6a199d7be75150e94075f3752e27f\",\"title\":\"Language-Driven Region Pointer Advancement for Controllable Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/027aba2214f6a199d7be75150e94075f3752e27f\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1016/j.cviu.2017.04.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"title\":\"Hierarchical & multimodal video captioning: Discovering and transferring multimodal knowledge for vision to language\",\"url\":\"https://www.semanticscholar.org/paper/96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1708.04686\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"48515099\",\"name\":\"Y. Li\"},{\"authorId\":\"3131569\",\"name\":\"Haoxiang Li\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/ICCV.2017.201\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"00f5bfc2fb760249ba4e9c72b72eea4574068339\",\"title\":\"VQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/00f5bfc2fb760249ba4e9c72b72eea4574068339\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"40894329\",\"name\":\"Stephan Alaniz\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1007/978-3-319-98131-4_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86de19e783d717c5db77f96f707613c27bde2915\",\"title\":\"Generating Post-Hoc Rationales of Deep Visual Classification Decisions\",\"url\":\"https://www.semanticscholar.org/paper/86de19e783d717c5db77f96f707613c27bde2915\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"50172036\",\"name\":\"X. Wu\"},{\"authorId\":\"1993634137\",\"name\":\"Shen Ge\"},{\"authorId\":\"47958349\",\"name\":\"X. Zhang\"},{\"authorId\":\"144934703\",\"name\":\"Wei Fan\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1145/3394171.3414004\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"title\":\"Bridging the Gap between Vision and Language Domains for Improved Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1710.10586\",\"authors\":[{\"authorId\":\"2000709\",\"name\":\"Y. Graham\"},{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"}],\"doi\":\"10.1371/journal.pone.0202789\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cfcb09b8e2d188d09452c65cda4d97605c9467c9\",\"title\":\"Evaluation of automatic video captioning using direct assessment\",\"url\":\"https://www.semanticscholar.org/paper/cfcb09b8e2d188d09452c65cda4d97605c9467c9\",\"venue\":\"PloS one\",\"year\":2018},{\"arxivId\":\"2005.13192\",\"authors\":[{\"authorId\":\"29448826\",\"name\":\"Bingchen Liu\"},{\"authorId\":\"8799238\",\"name\":\"Kunpeng Song\"},{\"authorId\":null,\"name\":\"Yizhe Zhu\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"145159523\",\"name\":\"A. Elgammal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff32b7aab11d30526c6de1d261a381996b2172c8\",\"title\":\"TIME: Text and Image Mutual-Translation Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/ff32b7aab11d30526c6de1d261a381996b2172c8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47767554\",\"name\":\"L. Wu\"},{\"authorId\":\"40444400\",\"name\":\"M. Xu\"},{\"authorId\":\"1783122\",\"name\":\"J. Wang\"},{\"authorId\":\"144242192\",\"name\":\"Stuart Perry\"}],\"doi\":\"10.1109/TMM.2019.2931815\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5b8b83bbb8ea54873be904ff77b1764267b3bd33\",\"title\":\"Recall What You See Continually Using GridLSTM in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5b8b83bbb8ea54873be904ff77b1764267b3bd33\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1708.09666\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"}],\"doi\":\"10.1145/3078971.3079000\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"title\":\"Generating Video Descriptions with Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144663763\",\"name\":\"Q. Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bc2856e70ad3c8fe439dec6cc6a2e03d6e090fb7\",\"title\":\"What value high level concepts in vision to language problems\",\"url\":\"https://www.semanticscholar.org/paper/bc2856e70ad3c8fe439dec6cc6a2e03d6e090fb7\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"1752427\",\"name\":\"Jiansheng Chen\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"47718901\",\"name\":\"Weitao Wan\"},{\"authorId\":\"153447481\",\"name\":\"Youze Xue\"}],\"doi\":\"10.1109/TIP.2020.2969330\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff5a1ec81f327b711a5433e4cd40467215a13f39\",\"title\":\"Image Captioning With End-to-End Attribute Detection and Subsequent Attributes Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ff5a1ec81f327b711a5433e4cd40467215a13f39\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/ICCV.2019.00472\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5abe916562fad8306e3f4e571f83015047f0be1d\",\"title\":\"Robust Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5abe916562fad8306e3f4e571f83015047f0be1d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2010.10839\",\"authors\":[{\"authorId\":\"7878341\",\"name\":\"Wubo Li\"},{\"authorId\":\"46197764\",\"name\":\"D. Jiang\"},{\"authorId\":\"9276071\",\"name\":\"Wei Zou\"},{\"authorId\":\"1898780\",\"name\":\"Xiangang Li\"}],\"doi\":\"10.21437/interspeech.2020-2359\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f5bb5c693a69cc10bc9d4dcc48eb96bae3d0600a\",\"title\":\"TMT: A Transformer-based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/f5bb5c693a69cc10bc9d4dcc48eb96bae3d0600a\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144168152\",\"name\":\"Q. Cai\"},{\"authorId\":\"3409740\",\"name\":\"Ziyu Xue\"},{\"authorId\":\"40569887\",\"name\":\"X. Zhang\"},{\"authorId\":\"1698347\",\"name\":\"Xiaobin Zhu\"},{\"authorId\":\"145259074\",\"name\":\"W. Shao\"},{\"authorId\":null,\"name\":\"Lei Wang\"}],\"doi\":\"10.1007/978-981-10-7299-4_49\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd8c8dfc6ab8ba3c96834b04cd55ce83972de4a4\",\"title\":\"A Novel Framework for Image Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/dd8c8dfc6ab8ba3c96834b04cd55ce83972de4a4\",\"venue\":\"CCCV\",\"year\":2017},{\"arxivId\":\"1909.02097\",\"authors\":[{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"144865339\",\"name\":\"Bo Pang\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/D19-1155\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3b6e6822eabe2f64192a1965c23e38043866319c\",\"title\":\"Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3b6e6822eabe2f64192a1965c23e38043866319c\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"1788029\",\"name\":\"W. Wang\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/ICASSP.2019.8682392\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4dee549ba6f09e59c6d6d13434a2b023d868467a\",\"title\":\"Image Captioning with Two Cascaded Agents\",\"url\":\"https://www.semanticscholar.org/paper/4dee549ba6f09e59c6d6d13434a2b023d868467a\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47748857\",\"name\":\"Wei Zhao\"},{\"authorId\":\"2894465\",\"name\":\"Benyou Wang\"},{\"authorId\":\"145581826\",\"name\":\"Jianbo Ye\"},{\"authorId\":\"144346838\",\"name\":\"Min Yang\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":null,\"name\":\"Yu Qiao\"}],\"doi\":\"10.24963/ijcai.2018/168\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"cbe4eac494fa7bc0bcd958d67393f097e0cb17db\",\"title\":\"A Multi-task Learning Approach for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cbe4eac494fa7bc0bcd958d67393f097e0cb17db\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"16295798\",\"name\":\"J. Hong\"},{\"authorId\":\"1774234\",\"name\":\"H. J. Lee\"},{\"authorId\":\"33785481\",\"name\":\"Yelin Kim\"},{\"authorId\":\"7251290\",\"name\":\"Yong Man Ro\"}],\"doi\":\"10.1007/978-3-030-37734-2_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"392696dd0b7a67a162cc7eadd2ae0812da117a7e\",\"title\":\"Face Tells Detailed Expression: Generating Comprehensive Facial Expression Sentence Through Facial Action Units\",\"url\":\"https://www.semanticscholar.org/paper/392696dd0b7a67a162cc7eadd2ae0812da117a7e\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49933533\",\"name\":\"Jing Yun\"},{\"authorId\":\"49933533\",\"name\":\"Jing Yun\"},{\"authorId\":\"48559698\",\"name\":\"Zhiwei Xu\"},{\"authorId\":\"1807620\",\"name\":\"Guanglai Gao\"}],\"doi\":\"10.1155/2020/9562587\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8a20a9b1345d8919b692f7f7fe919937bf823358\",\"title\":\"Gated Object-Attribute Matching Network for Detailed Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/8a20a9b1345d8919b692f7f7fe919937bf823358\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144515751\",\"name\":\"Abdullah Ali\"},{\"authorId\":\"144844426\",\"name\":\"M. Morris\"},{\"authorId\":\"1796045\",\"name\":\"J. Wobbrock\"}],\"doi\":\"10.1145/3242587.3242621\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"036e637a02972942905fd764773a7db1eaa0bc5a\",\"title\":\"Crowdsourcing Similarity Judgments for Agreement Analysis in End-User Elicitation Studies\",\"url\":\"https://www.semanticscholar.org/paper/036e637a02972942905fd764773a7db1eaa0bc5a\",\"venue\":\"UIST\",\"year\":2018},{\"arxivId\":\"1806.06371\",\"authors\":[{\"authorId\":\"2752573\",\"name\":\"Lisa Beinborn\"},{\"authorId\":\"25080314\",\"name\":\"Teresa Botschen\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b3587363d37dd197b6adbcfa79d49b5486f27d8\",\"title\":\"Multimodal Grounding for Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/1b3587363d37dd197b6adbcfa79d49b5486f27d8\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4942547\",\"name\":\"Zongjian Zhang\"},{\"authorId\":\"145698633\",\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"145093625\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/WACV.2018.00190\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6107d5107e7e0d94a3f0f43b642c4cfd1cba3b79\",\"title\":\"Fine-Grained and Semantic-Guided Visual Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6107d5107e7e0d94a3f0f43b642c4cfd1cba3b79\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30820439\",\"name\":\"Erica L. Meszaros\"},{\"authorId\":\"39679686\",\"name\":\"L. L. Vie\"},{\"authorId\":\"65956766\",\"name\":\"B. Barrows\"},{\"authorId\":\"5115473\",\"name\":\"M. Smith\"},{\"authorId\":\"144911735\",\"name\":\"B. Allen\"}],\"doi\":\"10.2514/6.2019-2204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"38bf2c5a54eec44843093090273bafb7f6004b85\",\"title\":\"Evaluating Communication Modality for Improved Human/Autonomous System Teaming\",\"url\":\"https://www.semanticscholar.org/paper/38bf2c5a54eec44843093090273bafb7f6004b85\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1805.00314\",\"authors\":[{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":\"10.18653/v1/N18-1198\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d3dd33950f4a1be56eb88c0791263b3e3a6deee\",\"title\":\"Object Counts! Bringing Explicit Detections Back into Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7d3dd33950f4a1be56eb88c0791263b3e3a6deee\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665288\",\"name\":\"Jan Zah\\u00e1lka\"},{\"authorId\":\"2815960\",\"name\":\"S. Rudinac\"},{\"authorId\":\"1717056\",\"name\":\"M. Worring\"}],\"doi\":\"10.1145/2733373.2806279\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1051ccbf1eb29f29fac0d32d3650532d5ee5598e\",\"title\":\"Analytic Quality: Evaluation of Performance and Insight in Multimedia Collection Analysis\",\"url\":\"https://www.semanticscholar.org/paper/1051ccbf1eb29f29fac0d32d3650532d5ee5598e\",\"venue\":\"ACM Multimedia\",\"year\":2015},{\"arxivId\":\"2010.05300\",\"authors\":[{\"authorId\":\"39041697\",\"name\":\"Yulin Wang\"},{\"authorId\":\"1648745587\",\"name\":\"Kangchen Lv\"},{\"authorId\":\"1516136186\",\"name\":\"Rui Huang\"},{\"authorId\":\"1760750\",\"name\":\"Shiji Song\"},{\"authorId\":\"46554630\",\"name\":\"L. Yang\"},{\"authorId\":\"143983679\",\"name\":\"Gao Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fa283d4f9c2ed991383c0434ef6043bee0dc8e2\",\"title\":\"Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification\",\"url\":\"https://www.semanticscholar.org/paper/9fa283d4f9c2ed991383c0434ef6043bee0dc8e2\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387548078\",\"name\":\"K. Lin\"},{\"authorId\":\"1738276592\",\"name\":\"Zhuoxin Gan\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.98\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3263b941d0a77bbd2040612ec774ef063ef64c48\",\"title\":\"Semi-Supervised Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3263b941d0a77bbd2040612ec774ef063ef64c48\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40469481\",\"name\":\"T. Nogueira\"},{\"authorId\":\"32535425\",\"name\":\"C. Vinhal\"},{\"authorId\":\"1491425913\",\"name\":\"G\\u00e9lson da Cruz J\\u00fanior\"},{\"authorId\":\"2569769\",\"name\":\"Matheus Rudolfo Diedrich Ullmann\"}],\"doi\":\"10.1007/s11042-020-09539-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"48f17b2b08aebd16e711f5c7ca9e773fe6639dc3\",\"title\":\"Reference-based model using multimodal gated recurrent units for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/48f17b2b08aebd16e711f5c7ca9e773fe6639dc3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"1911.04365\",\"authors\":[{\"authorId\":\"9457831\",\"name\":\"J. He\"},{\"authorId\":\"1405953764\",\"name\":\"Quan-Jie Cao\"},{\"authorId\":\"39844955\",\"name\":\"L. Zhang\"},{\"authorId\":\"93970041\",\"name\":\"Hui Tao\"}],\"doi\":\"10.1109/ACCESS.2020.2982571\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"587e1359d0a54ecf24e2577357abe79d0cd00ab2\",\"title\":\"Conditionally Learn to Pay Attention for Sequential Visual Task\",\"url\":\"https://www.semanticscholar.org/paper/587e1359d0a54ecf24e2577357abe79d0cd00ab2\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1723672\",\"name\":\"X. Zhang\"},{\"authorId\":\"2548483\",\"name\":\"Shengfeng He\"},{\"authorId\":\"1688375\",\"name\":\"Xinhang Song\"},{\"authorId\":\"145402398\",\"name\":\"Pengxu Wei\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"},{\"authorId\":\"1694936\",\"name\":\"Q. Ye\"},{\"authorId\":\"24350293\",\"name\":\"J. Jiao\"},{\"authorId\":\"1726262\",\"name\":\"R. Lau\"}],\"doi\":\"10.1109/ICME.2017.8019525\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4b5bc8ee9632d7ff2f23e92295189dad9522864\",\"title\":\"Keyword-driven image captioning via Context-dependent Bilateral LSTM\",\"url\":\"https://www.semanticscholar.org/paper/e4b5bc8ee9632d7ff2f23e92295189dad9522864\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30771413\",\"name\":\"Brandon Birmingham\"},{\"authorId\":\"35347012\",\"name\":\"Adrian Muscat\"}],\"doi\":\"10.18653/v1/W17-2002\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1dab64439ffb72da80175ba98a47b6f1e0ddd21b\",\"title\":\"The Use of Object Labels and Spatial Prepositions as Keywords in a Web-Retrieval-Based Image Caption Generation System\",\"url\":\"https://www.semanticscholar.org/paper/1dab64439ffb72da80175ba98a47b6f1e0ddd21b\",\"venue\":\"VL@EACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3159346\",\"name\":\"Sebastian Gehrmann\"},{\"authorId\":\"24881182\",\"name\":\"Falcon Z. Dai\"},{\"authorId\":\"40895152\",\"name\":\"Henry Elder\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2f7cce658b3e794b3523facecfd1c5c5527d36b2\",\"title\":\"End-to-End Content and Plan Selection for Natural Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/2f7cce658b3e794b3523facecfd1c5c5527d36b2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98419684\",\"name\":\"Phil Kinghorn\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9549e8e59f5ce436cc4d81ddb859b3295d131c5b\",\"title\":\"Deep learning-based regional image caption generation with refined descriptions\",\"url\":\"https://www.semanticscholar.org/paper/9549e8e59f5ce436cc4d81ddb859b3295d131c5b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40895688\",\"name\":\"Kaylee Burns\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9226675e413bb4b82ad419a10e4ac9ebd6aa7fef\",\"title\":\"Addressing and Understanding Shortcomings in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/9226675e413bb4b82ad419a10e4ac9ebd6aa7fef\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49370397\",\"name\":\"D. Wang\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"}],\"doi\":\"10.1109/ICBK.2017.26\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d22c000c12aaedadcf075736dfc998dea932f06\",\"title\":\"Video Captioning with Semantic Information from the Knowledge Base\",\"url\":\"https://www.semanticscholar.org/paper/4d22c000c12aaedadcf075736dfc998dea932f06\",\"venue\":\"2017 IEEE International Conference on Big Knowledge (ICBK)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471428747\",\"name\":\"Yang Zhen-yu\"},{\"authorId\":\"1471428729\",\"name\":\"Zhang Jiao\"}],\"doi\":\"10.1109/IAEAC47372.2019.8998010\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f57e68cfdc1a7fdde23f69ec9e8a0b39d624ad5c\",\"title\":\"Research on Image Caption Method Based on Mixed Image Features\",\"url\":\"https://www.semanticscholar.org/paper/f57e68cfdc1a7fdde23f69ec9e8a0b39d624ad5c\",\"venue\":\"2019 IEEE 4th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50610439\",\"name\":\"Songtao Ding\"},{\"authorId\":\"2641875\",\"name\":\"Shiru Qu\"},{\"authorId\":\"6962569\",\"name\":\"Yuling Xi\"},{\"authorId\":\"3097765\",\"name\":\"A. K. Sangaiah\"},{\"authorId\":\"31197849\",\"name\":\"Shaohua Wan\"}],\"doi\":\"10.1016/J.PATREC.2019.03.021\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96829c628b8db61c67f3e72dd3f25035bff2d985\",\"title\":\"Image caption generation with high-level image features\",\"url\":\"https://www.semanticscholar.org/paper/96829c628b8db61c67f3e72dd3f25035bff2d985\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11007025\",\"name\":\"Junjiao Tian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"df40d057b584de2cf74123a2ef4274de582d6b03\",\"title\":\"Detailed Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/df40d057b584de2cf74123a2ef4274de582d6b03\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1708.02478\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TNNLS.2018.2851077\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d78c47093fbf3d85225fd502674aba4a29b3987\",\"title\":\"From Deterministic to Generative: Multimodal Stochastic RNNs for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7d78c47093fbf3d85225fd502674aba4a29b3987\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2019},{\"arxivId\":\"1910.05752\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":\"1387518609\",\"name\":\"Jiutong Wei\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"122094546\",\"name\":\"B. Li\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9dab1068a6e9f4696ea3687baf04f9098107c1f7\",\"title\":\"VATEX Captioning Challenge 2019: Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9dab1068a6e9f4696ea3687baf04f9098107c1f7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"39689790\",\"name\":\"L. White\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"2210661\",\"name\":\"S. Shah\"}],\"doi\":\"10.18653/v1/P18-3003\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"954a39752ca5ebc19eb7563c28a5773f70bff452\",\"title\":\"Learning-based Composite Metrics for Improved Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/954a39752ca5ebc19eb7563c28a5773f70bff452\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153185012\",\"name\":\"G. Li\"},{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"143907244\",\"name\":\"Yi Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9b77854443654cec7e2ac5c298f245dbe838c0f5\",\"title\":\"UTS CAI submission at TRECVID 2017 Video to Text Description Task\",\"url\":\"https://www.semanticscholar.org/paper/9b77854443654cec7e2ac5c298f245dbe838c0f5\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67128061\",\"name\":\"J. Rooijmans\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"30aee3132150626432583f5ca6f0d5b6dc5f5fe6\",\"title\":\"Investigating Text-Based Similarity Measures for Automatic Image Description Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/30aee3132150626432583f5ca6f0d5b6dc5f5fe6\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"97799727\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"32074473\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"100887531\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d124bd0f0a6f89880844a7da0e8cb79c9f5cd659\",\"title\":\"Audio-Visual Interpretable and Controllable Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d124bd0f0a6f89880844a7da0e8cb79c9f5cd659\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":\"1911.01857\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2020.03.001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"583222b6a573ad698207a0ebabb06685c4517558\",\"title\":\"Video Captioning with Text-based Dynamic Attention and Step-by-Step Learning\",\"url\":\"https://www.semanticscholar.org/paper/583222b6a573ad698207a0ebabb06685c4517558\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40353639\",\"name\":\"D. Vasile\"},{\"authorId\":\"1690572\",\"name\":\"Thomas Lukasiewicz\"}],\"doi\":\"10.1007/978-3-030-02671-4_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72afe5d25da6dfbb611b2c3dab4efc4b1c8447cf\",\"title\":\"Learning Structured Video Descriptions: Automated Video Knowledge Extraction for Video Understanding Tasks\",\"url\":\"https://www.semanticscholar.org/paper/72afe5d25da6dfbb611b2c3dab4efc4b1c8447cf\",\"venue\":\"OTM Conferences\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32324177\",\"name\":\"C. Wu\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"15862607\",\"name\":\"Xiaoliang Chu\"},{\"authorId\":\"2037988\",\"name\":\"Weichen Sun\"},{\"authorId\":\"144310030\",\"name\":\"F. Su\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"}],\"doi\":\"10.1016/j.neucom.2018.07.029\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fc4a590d1859ba43c1303927c86c64b34e43287\",\"title\":\"Hierarchical attention-based multimodal fusion for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4fc4a590d1859ba43c1303927c86c64b34e43287\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1611.09312\",\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"153925540\",\"name\":\"C. Grana\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2017.339\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"title\":\"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2738095\",\"name\":\"S. Mille\"},{\"authorId\":\"1792458\",\"name\":\"A. Belz\"},{\"authorId\":\"1678747\",\"name\":\"Bernd Bohnet\"},{\"authorId\":\"2000709\",\"name\":\"Y. Graham\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c93ad4b79352cd8b8349147f7c59ed8bb4bfdb2\",\"title\":\"ACL 2018 Multilingual Surface Realisation : Shared Task and Beyond Proceedings of the Workshop July 19 , 2018\",\"url\":\"https://www.semanticscholar.org/paper/1c93ad4b79352cd8b8349147f7c59ed8bb4bfdb2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1811.07662\",\"authors\":[{\"authorId\":\"47833739\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2112160\",\"name\":\"Y. Li\"},{\"authorId\":\"1678689\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00859\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23e943809c131c50dc90c1d308373febc60b9029\",\"title\":\"Intention Oriented Image Captions With Guiding Objects\",\"url\":\"https://www.semanticscholar.org/paper/23e943809c131c50dc90c1d308373febc60b9029\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.02628\",\"authors\":[{\"authorId\":\"65767906\",\"name\":\"Silvio Olivastri\"},{\"authorId\":\"1931660\",\"name\":\"Gurkirt Singh\"},{\"authorId\":\"1754181\",\"name\":\"Fabio Cuzzolin\"}],\"doi\":\"10.1109/ICCVW.2019.00185\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0c1ebaa635f68bb4a09fc59191642f30cfa894c9\",\"title\":\"End-to-End Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0c1ebaa635f68bb4a09fc59191642f30cfa894c9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"2004.10796\",\"authors\":[{\"authorId\":\"4868335\",\"name\":\"J. Park\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1007/978-3-030-58558-7_30\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4aacd623a46adc6a03c925fe3ac007c271c9c6ab\",\"title\":\"VisualCOMET: Reasoning About the Dynamic Context of a Still Image\",\"url\":\"https://www.semanticscholar.org/paper/4aacd623a46adc6a03c925fe3ac007c271c9c6ab\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2022535952\",\"name\":\"Virendra Kumar Meghwal\"},{\"authorId\":\"40204750\",\"name\":\"Namita Mittal\"},{\"authorId\":\"39528728\",\"name\":\"Girdhari Singh\"}],\"doi\":\"10.1007/978-981-15-7031-5_48\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"53af04fbf4703399e135970d35a0eacbe6cd11aa\",\"title\":\"Image Captioning Methodologies Using Deep Learning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/53af04fbf4703399e135970d35a0eacbe6cd11aa\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1909.05424\",\"authors\":[{\"authorId\":\"20132361\",\"name\":\"Changhan Wang\"},{\"authorId\":\"153408223\",\"name\":\"A. Jain\"},{\"authorId\":\"7167137\",\"name\":\"Danlu Chen\"},{\"authorId\":\"3016273\",\"name\":\"Jiatao Gu\"}],\"doi\":\"10.18653/v1/D19-3043\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"37c3f85d0a055de946235729d39787b9ed0e4ca3\",\"title\":\"VizSeq: A Visual Analysis Toolkit for Text Generation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/37c3f85d0a055de946235729d39787b9ed0e4ca3\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1805.09461\",\"authors\":[{\"authorId\":\"2180949\",\"name\":\"Yaser Keneshloo\"},{\"authorId\":\"145531789\",\"name\":\"Tian Shi\"},{\"authorId\":\"1755938\",\"name\":\"Naren Ramakrishnan\"},{\"authorId\":\"144417522\",\"name\":\"C. Reddy\"}],\"doi\":\"10.1109/TNNLS.2019.2929141\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"15a06d8601539b5eb6df5baf6bc4c3bdefb34855\",\"title\":\"Deep Reinforcement Learning for Sequence-to-Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/15a06d8601539b5eb6df5baf6bc4c3bdefb34855\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":\"1911.03047\",\"authors\":[{\"authorId\":\"31497621\",\"name\":\"W. S. Cho\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1675319352\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144628574\",\"name\":\"Chenyan Xiong\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"50468734\",\"name\":\"M. Wang\"},{\"authorId\":\"66648221\",\"name\":\"B. Dolan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ab2777c7f39c5352dd8ab2335c45aafa4194a025\",\"title\":\"Contrastive Multi-document Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/ab2777c7f39c5352dd8ab2335c45aafa4194a025\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2002.00774\",\"authors\":[{\"authorId\":\"153362999\",\"name\":\"Y. Jung\"},{\"authorId\":\"24028009\",\"name\":\"Dahun Kim\"},{\"authorId\":\"2262209\",\"name\":\"S. Woo\"},{\"authorId\":\"97531942\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"153275028\",\"name\":\"Sungjin Kim\"},{\"authorId\":\"98758720\",\"name\":\"I. Kweon\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"def96bfeac699862edaa76e3041e5a5c9f0c8c46\",\"title\":\"Hide-and-Tell: Learning to Bridge Photo Streams for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/def96bfeac699862edaa76e3041e5a5c9f0c8c46\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1703.06029\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/ICCV.2017.323\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"24dc571a49d3431e8cb1f1008f86d5dd5b7a1613\",\"title\":\"Towards Diverse and Natural Image Descriptions via a Conditional GAN\",\"url\":\"https://www.semanticscholar.org/paper/24dc571a49d3431e8cb1f1008f86d5dd5b7a1613\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"2010.03743\",\"authors\":[{\"authorId\":\"13941532\",\"name\":\"Fuxiao Liu\"},{\"authorId\":null,\"name\":\"Yinghan Wang\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e5bcdd0366c3ddd1579159595d1990f515cf93b\",\"title\":\"VisualNews : A Large Multi-source News Image Dataset\",\"url\":\"https://www.semanticscholar.org/paper/7e5bcdd0366c3ddd1579159595d1990f515cf93b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.01096\",\"authors\":[{\"authorId\":\"2562211\",\"name\":\"Xiaoyu Shen\"},{\"authorId\":\"48025720\",\"name\":\"Ernie Chang\"},{\"authorId\":\"1563539653\",\"name\":\"Hui Su\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":\"10.18653/v1/2020.acl-main.641\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b7e85c3d036b23f63c3db628e97948377e44f75\",\"title\":\"Neural Data-to-Text Generation via Jointly Learning the Segmentation and Correspondence\",\"url\":\"https://www.semanticscholar.org/paper/4b7e85c3d036b23f63c3db628e97948377e44f75\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143715692\",\"name\":\"X. Hao\"},{\"authorId\":\"46468475\",\"name\":\"F. Zhou\"},{\"authorId\":\"33899331\",\"name\":\"Xiaoyong Li\"}],\"doi\":\"10.1109/ITNEC48623.2020.9084781\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"653de101370307afc2eba27d4e4c574441eb06da\",\"title\":\"Scene-Edge GRU for Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/653de101370307afc2eba27d4e4c574441eb06da\",\"venue\":\"2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103382547\",\"name\":\"Junzhong Ji\"},{\"authorId\":\"49770078\",\"name\":\"C. Xu\"},{\"authorId\":\"46447188\",\"name\":\"X. Zhang\"},{\"authorId\":\"2692910\",\"name\":\"Boyue Wang\"},{\"authorId\":\"1688375\",\"name\":\"Xinhang Song\"}],\"doi\":\"10.1109/TIP.2020.3004729\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"63632ca4c1a6c16c9b6358624134880c3b23df90\",\"title\":\"Spatio-Temporal Memory Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/63632ca4c1a6c16c9b6358624134880c3b23df90\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143696833\",\"name\":\"Wenjie Cai\"},{\"authorId\":\"31126576\",\"name\":\"Zheng Xiong\"},{\"authorId\":\"2303260\",\"name\":\"Xianfang Sun\"},{\"authorId\":\"1734823\",\"name\":\"Paul L. Rosin\"},{\"authorId\":\"40534966\",\"name\":\"Longcun Jin\"},{\"authorId\":\"9245443\",\"name\":\"Xinyi Peng\"}],\"doi\":\"10.3390/app10010391\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"945ffeb90e538214e3063407db8e094469ec877a\",\"title\":\"Panoptic Segmentation-Based Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/945ffeb90e538214e3063407db8e094469ec877a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1488653711\",\"name\":\"Yanzhi Yi\"},{\"authorId\":\"1486063321\",\"name\":\"Hangyu Deng\"},{\"authorId\":\"153147804\",\"name\":\"Jinglu Hu\"}],\"doi\":\"10.18653/v1/2020.acl-main.93\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0b4d5b7cef06b66182db80803f783d077e3637b6\",\"title\":\"Improving Image Captioning Evaluation by Considering Inter References Variance\",\"url\":\"https://www.semanticscholar.org/paper/0b4d5b7cef06b66182db80803f783d077e3637b6\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1804472\",\"name\":\"H. Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1818920\",\"name\":\"Zijia Lin\"},{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"1783847\",\"name\":\"J. Han\"}],\"doi\":\"10.24963/ijcai.2018/84\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4ae889c38444939ae4312ab38bf7036f6df739f\",\"title\":\"Show, Observe and Tell: Attribute-driven Attention Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b4ae889c38444939ae4312ab38bf7036f6df739f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143904396\",\"name\":\"Xu Jia\"},{\"authorId\":\"2304222\",\"name\":\"E. Gavves\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"1704728\",\"name\":\"T. Tuytelaars\"}],\"doi\":\"10.1109/ICCV.2015.277\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c3640aae13e344ad70a926510221dada626a44de\",\"title\":\"Guiding the Long-Short Term Memory Model for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/c3640aae13e344ad70a926510221dada626a44de\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46448506\",\"name\":\"X. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"25537b45688f90e925bff2bf07f03b3451fcf93b\",\"title\":\"Product Defect Discovery and Summarization from Online User Reviews\",\"url\":\"https://www.semanticscholar.org/paper/25537b45688f90e925bff2bf07f03b3451fcf93b\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145441561\",\"name\":\"W. Cui\"},{\"authorId\":\"49451360\",\"name\":\"Fei Wang\"},{\"authorId\":\"144258396\",\"name\":\"X. He\"},{\"authorId\":\"29090447\",\"name\":\"D. Zhang\"},{\"authorId\":\"122135929\",\"name\":\"Xuxiang Xu\"},{\"authorId\":\"144817588\",\"name\":\"Meng Yao\"},{\"authorId\":\"72682876\",\"name\":\"Z. Wang\"},{\"authorId\":\"1955707\",\"name\":\"Jiejun Huang\"}],\"doi\":\"10.3390/RS11091044\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"75b0a1264b5ba0202ee3e81b89d7e9f0042c3744\",\"title\":\"Multi-Scale Semantic Segmentation and Spatial Relationship Recognition of Remote Sensing Images Based on an Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/75b0a1264b5ba0202ee3e81b89d7e9f0042c3744\",\"venue\":\"Remote. Sens.\",\"year\":2019},{\"arxivId\":\"1912.01119\",\"authors\":[{\"authorId\":\"97634546\",\"name\":\"Khaled Jedoui\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e23a129f49fcd52258c14da28311cce3190d271f\",\"title\":\"Deep Bayesian Active Learning for Multiple Correct Outputs\",\"url\":\"https://www.semanticscholar.org/paper/e23a129f49fcd52258c14da28311cce3190d271f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1511.06432\",\"authors\":[{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"title\":\"Delving Deeper into Convolutional Networks for Learning Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"2009.11436\",\"authors\":[{\"authorId\":\"2145853\",\"name\":\"Daiki Takeuchi\"},{\"authorId\":\"67318326\",\"name\":\"Y. Koizumi\"},{\"authorId\":\"2991962\",\"name\":\"Y. Ohishi\"},{\"authorId\":\"145752315\",\"name\":\"N. Harada\"},{\"authorId\":\"1718803\",\"name\":\"Kunio Kashino\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f02507aeb50f84b5ded81aaa653e2eb5d7e5eec4\",\"title\":\"Effects of Word-frequency based Pre- and Post- Processings for Audio Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f02507aeb50f84b5ded81aaa653e2eb5d7e5eec4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.08315\",\"authors\":[{\"authorId\":\"151195783\",\"name\":\"Ruixiang Tang\"},{\"authorId\":\"3432460\",\"name\":\"Mengnan Du\"},{\"authorId\":\"48513905\",\"name\":\"Yuening Li\"},{\"authorId\":\"47781070\",\"name\":\"Zirui Liu\"},{\"authorId\":\"1490483806\",\"name\":\"X. Hu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0b239fe63de5de0a1e0c08af97a30f79b5a2bdbc\",\"title\":\"Mitigating Gender Bias in Captioning Systems\",\"url\":\"https://www.semanticscholar.org/paper/0b239fe63de5de0a1e0c08af97a30f79b5a2bdbc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2002.10832\",\"authors\":[{\"authorId\":\"90745780\",\"name\":\"Thomas Scialom\"},{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"1502219404\",\"name\":\"Paul-Alexis Dray\"},{\"authorId\":\"1767493\",\"name\":\"Jacopo Staiano\"},{\"authorId\":\"67192547\",\"name\":\"P. Gallinari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6364653539facfdd70837f460b20c62f7ca8a6d\",\"title\":\"What BERT Sees: Cross-Modal Transfer for Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/d6364653539facfdd70837f460b20c62f7ca8a6d\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30300052\",\"name\":\"Takuya Yashima\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"3040648\",\"name\":\"Kentaro Inui\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1007/978-3-319-54193-8_6\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"171042ba12818238e3c0994ff08d71f8c28d4134\",\"title\":\"Learning to Describe E-Commerce Images from Noisy Online Data\",\"url\":\"https://www.semanticscholar.org/paper/171042ba12818238e3c0994ff08d71f8c28d4134\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1843795\",\"name\":\"Lorenzo Baraldi\"},{\"authorId\":\"145518528\",\"name\":\"G. Serra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"96fdc0131dc80ffa6d7b9c526e07f080414c54ec\",\"title\":\"1 Paying More A ention to Saliency : Image Captioning with Saliency and Context A ention\",\"url\":\"https://www.semanticscholar.org/paper/96fdc0131dc80ffa6d7b9c526e07f080414c54ec\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"2272096\",\"name\":\"Guo-Jun Qi\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/WACV.2019.00034\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"449b9189e3058d33f871dfd3b07cc75a717038f7\",\"title\":\"Improving Diversity of Image Captioning Through Variational Autoencoders and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/449b9189e3058d33f871dfd3b07cc75a717038f7\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46299644\",\"name\":\"Jinyoung Oh\"},{\"authorId\":\"1643099832\",\"name\":\"Jeong-Moo Kim\"},{\"authorId\":\"3333853\",\"name\":\"Jeong-Won Cha\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"23e202e6cabafa8c63afb82591933bbcac3f61a9\",\"title\":\"Weighted BLEU: N-Gram weighted BLEU\",\"url\":\"https://www.semanticscholar.org/paper/23e202e6cabafa8c63afb82591933bbcac3f61a9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153804074\",\"name\":\"Heng Quan Liu\"},{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"151118825\",\"name\":\"Mengmeng Jiang\"}],\"doi\":\"10.1117/12.2524235\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"67f717df98ebaa35ac1995f31bc7b678fb8536e0\",\"title\":\"When visual object-context features meet generic and specific semantic priors in image captioning\",\"url\":\"https://www.semanticscholar.org/paper/67f717df98ebaa35ac1995f31bc7b678fb8536e0\",\"venue\":\"International Conference on Graphic and Image Processing\",\"year\":2019},{\"arxivId\":\"1909.02201\",\"authors\":[{\"authorId\":\"40622539\",\"name\":\"Dong-Jin Kim\"},{\"authorId\":\"49331178\",\"name\":\"Jinsoo Choi\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"98758720\",\"name\":\"I. Kweon\"}],\"doi\":\"10.18653/v1/D19-1208\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a87d5654c41f3dfa7252d079045d7094f601f78e\",\"title\":\"Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/a87d5654c41f3dfa7252d079045d7094f601f78e\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1909.03918\",\"authors\":[{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2019.00271\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"05106b86ec45914d1136719d311078182d437872\",\"title\":\"Hierarchy Parsing for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/05106b86ec45914d1136719d311078182d437872\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1606.01393\",\"authors\":[{\"authorId\":\"145399579\",\"name\":\"Karan Sharma\"},{\"authorId\":\"30512587\",\"name\":\"Arun C. S. Kumar\"},{\"authorId\":\"3422895\",\"name\":\"Suchendra M. Bhandarkar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad9b3dc6c0e54070cec79df86458ed38566da1ff\",\"title\":\"Automated Image Captioning for Rapid Prototyping and Resource Constrained Environments\",\"url\":\"https://www.semanticscholar.org/paper/ad9b3dc6c0e54070cec79df86458ed38566da1ff\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1706.09601\",\"authors\":[{\"authorId\":\"50081790\",\"name\":\"L. Zhang\"},{\"authorId\":\"40497013\",\"name\":\"Flood Sung\"},{\"authorId\":\"144238414\",\"name\":\"Feng Liu\"},{\"authorId\":\"145406421\",\"name\":\"T. Xiang\"},{\"authorId\":\"144784813\",\"name\":\"S. Gong\"},{\"authorId\":\"2653152\",\"name\":\"Yongxin Yang\"},{\"authorId\":\"1697755\",\"name\":\"Timothy M. Hospedales\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"58ee208dce1f06724bb443b4cfe0aa30d6cc9d30\",\"title\":\"Actor-Critic Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/58ee208dce1f06724bb443b4cfe0aa30d6cc9d30\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3186804\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"1761970\",\"name\":\"L. Li\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"46948270\",\"name\":\"Ziyi Li\"},{\"authorId\":\"7475375\",\"name\":\"H. Peng\"},{\"authorId\":\"3712008\",\"name\":\"X. Niu\"}],\"doi\":\"10.1016/j.neucom.2018.08.069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"299c4aebcc81dd311f5632f3eec5f8a3bd17a226\",\"title\":\"Image captioning with triple-attention and stack parallel LSTM\",\"url\":\"https://www.semanticscholar.org/paper/299c4aebcc81dd311f5632f3eec5f8a3bd17a226\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1809.04344\",\"authors\":[{\"authorId\":\"51228129\",\"name\":\"Shailza Jolly\"},{\"authorId\":\"3422247\",\"name\":\"Sandro Pezzelle\"},{\"authorId\":\"35660331\",\"name\":\"T. Klein\"},{\"authorId\":\"145279674\",\"name\":\"A. Dengel\"},{\"authorId\":\"1848946\",\"name\":\"Moin Nabi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"610e0bee525a6573932e077f091505f54a5c4ede\",\"title\":\"The Wisdom of MaSSeS: Majority, Subjectivity, and Semantic Similarity in the Evaluation of VQA\",\"url\":\"https://www.semanticscholar.org/paper/610e0bee525a6573932e077f091505f54a5c4ede\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97636424\",\"name\":\"G. Li\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"144303230\",\"name\":\"Ping Liu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/ICCV.2019.00902\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c4530882cfcef1d2b4aa2996f494dfac626b5d9\",\"title\":\"Entangled Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7c4530882cfcef1d2b4aa2996f494dfac626b5d9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152509571\",\"name\":\"Yu Cheng\"},{\"authorId\":\"103615578\",\"name\":\"Y. Shi\"},{\"authorId\":\"48064593\",\"name\":\"Zhiyong Sun\"},{\"authorId\":\"152454596\",\"name\":\"Dezhi Feng\"},{\"authorId\":\"46514887\",\"name\":\"L. Dong\"}],\"doi\":\"10.1109/ROBIO49542.2019.8961478\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d1e4fb11748ff21f5a95d2565de21eddd5e92e\",\"title\":\"A Discrete Event Approach for Scene Generation conditioned on Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/69d1e4fb11748ff21f5a95d2565de21eddd5e92e\",\"venue\":\"2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47666014\",\"name\":\"Hongjun Chen\"},{\"authorId\":\"13032169\",\"name\":\"H. Li\"},{\"authorId\":\"9512265\",\"name\":\"Xueqin Wu\"}],\"doi\":\"10.1145/3380625.3380669\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1a3b4b243afc2616eb2c3e24ec0abad76c2a80f7\",\"title\":\"Research on Feature Extraction and Multimodal Fusion of Video Caption Based on Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1a3b4b243afc2616eb2c3e24ec0abad76c2a80f7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92104112\",\"name\":\"Tangming Chen\"},{\"authorId\":\"9398015\",\"name\":\"Qike Zhao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1007/978-3-030-33982-1_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cf546bfb933bddbd9e7017525d9621405e549b9\",\"title\":\"Boundary Detector Encoder and Decoder with Soft Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1cf546bfb933bddbd9e7017525d9621405e549b9\",\"venue\":\"APWeb/WAIM Workshops\",\"year\":2019},{\"arxivId\":\"1808.03986\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"49596229\",\"name\":\"S. Kumar\"},{\"authorId\":\"50975843\",\"name\":\"Vinod K. Kurmi\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":\"10.18653/v1/D18-1434\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43e846b51e6d3d317c3f34e9fd6af87eacc2314e\",\"title\":\"Multimodal Differential Network for Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/43e846b51e6d3d317c3f34e9fd6af87eacc2314e\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1811.00696\",\"authors\":[{\"authorId\":\"1940556\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"2624978\",\"name\":\"C. Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"2900282\",\"name\":\"W. Wang\"},{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"1700522\",\"name\":\"Guoyin Wang\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"261783ef9c98986652ee11c4df20173edafae826\",\"title\":\"Sequence Generation with Guider Network\",\"url\":\"https://www.semanticscholar.org/paper/261783ef9c98986652ee11c4df20173edafae826\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145116511\",\"name\":\"Jan Deriu\"},{\"authorId\":\"2648584\",\"name\":\"Mark Cieliebak\"}],\"doi\":\"10.18653/v1/W18-6503\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"929314aba74cc935f8dd6d982eb7546e50b004a7\",\"title\":\"Syntactic Manipulation for Generating more Diverse and Interesting Texts\",\"url\":\"https://www.semanticscholar.org/paper/929314aba74cc935f8dd6d982eb7546e50b004a7\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1470727651\",\"name\":\"Ariyo Oluwasanmi\"},{\"authorId\":\"4043033\",\"name\":\"E. Frimpong\"},{\"authorId\":\"32593111\",\"name\":\"Muhammad Umar Aftab\"},{\"authorId\":\"46352756\",\"name\":\"Edward Y. Baagyere\"},{\"authorId\":\"152179239\",\"name\":\"Zhiquang Qin\"},{\"authorId\":\"1470727785\",\"name\":\"Kifayat Ullah\"}],\"doi\":\"10.1109/ACCESS.2019.2957513\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"78396f9e33eaada2a84dc12a59a3deceac05c526\",\"title\":\"Fully Convolutional CaptionNet: Siamese Difference Captioning Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/78396f9e33eaada2a84dc12a59a3deceac05c526\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1809.04960\",\"authors\":[{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"145500855\",\"name\":\"Lei Cui\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dace30835e1debd9e7ee0453cd1fec9b72cba1b9\",\"title\":\"Unsupervised Machine Commenting with Neural Variational Topic Model\",\"url\":\"https://www.semanticscholar.org/paper/dace30835e1debd9e7ee0453cd1fec9b72cba1b9\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1901.09107\",\"authors\":[{\"authorId\":\"2809915\",\"name\":\"H. AlAmri\"},{\"authorId\":\"51002409\",\"name\":\"Vincent Cartillier\"},{\"authorId\":\"50317425\",\"name\":\"Abhishek Das\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"153149395\",\"name\":\"P. Anderson\"},{\"authorId\":\"21472040\",\"name\":\"Irfan Essa\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"}],\"doi\":\"10.1109/CVPR.2019.00774\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"044c56af7005c2013ce24c7199af716319378d7f\",\"title\":\"Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/044c56af7005c2013ce24c7199af716319378d7f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1911.09345\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"1747500\",\"name\":\"A. Mian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"title\":\"Empirical Autopsy of Deep Video Captioning Frameworks\",\"url\":\"https://www.semanticscholar.org/paper/62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.24963/ijcai.2019/726\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"29100728a77fb48d977474652ad90104125e9a07\",\"title\":\"Swell-and-Shrink: Decomposing Image Captioning by Transformation and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/29100728a77fb48d977474652ad90104125e9a07\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50997101\",\"name\":\"A. Mishra\"},{\"authorId\":\"145385391\",\"name\":\"C. Madhurima\"},{\"authorId\":\"50843042\",\"name\":\"S. M. R. Gautham\"},{\"authorId\":\"49657914\",\"name\":\"Jishin James\"},{\"authorId\":\"143754700\",\"name\":\"D. Annapurna\"}],\"doi\":\"10.1109/ICACCI.2018.8554737\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f2ce475add998f2e7c95b7a67b638493c9e5141\",\"title\":\"Environment Descriptor for the Visually Impaired\",\"url\":\"https://www.semanticscholar.org/paper/6f2ce475add998f2e7c95b7a67b638493c9e5141\",\"venue\":\"2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)\",\"year\":2018},{\"arxivId\":\"1909.09953\",\"authors\":[{\"authorId\":\"1863953\",\"name\":\"Kuang-Huei Lee\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"93400474\",\"name\":\"X. Chen\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b488019592d8e0c08e6cd011ae0543a6ac451357\",\"title\":\"Learning Visual Relation Priors for Image-Text Matching and Image Captioning with Neural Scene Graph Generators\",\"url\":\"https://www.semanticscholar.org/paper/b488019592d8e0c08e6cd011ae0543a6ac451357\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1510.04709\",\"authors\":[{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"37922370\",\"name\":\"S. Frank\"},{\"authorId\":\"2955842\",\"name\":\"E. Hasler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5af227e3e3158158163fb0715ae3971be2e1df4\",\"title\":\"Multilingual Image Description with Neural Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/a5af227e3e3158158163fb0715ae3971be2e1df4\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1908.09317\",\"authors\":[{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"145587209\",\"name\":\"N. Navab\"}],\"doi\":\"10.1109/ICCV.2019.00751\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"title\":\"Towards Unsupervised Image Captioning With Shared Multimodal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2003.12633\",\"authors\":[{\"authorId\":\"24339915\",\"name\":\"Davis Gilton\"},{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"145952380\",\"name\":\"R. Willett\"},{\"authorId\":\"46208708\",\"name\":\"G. Shakhnarovich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"title\":\"Detection and Description of Change in Visual Streams\",\"url\":\"https://www.semanticscholar.org/paper/dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"1491236221\",\"name\":\"Meng Gao\"},{\"authorId\":\"1747773\",\"name\":\"T. Zhang\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1109/ICDM.2019.00054\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8f04013efcdf606d65145859f4f9eb6c48908869\",\"title\":\"Exploring Semantic Relationships for Image Captioning without Parallel Data\",\"url\":\"https://www.semanticscholar.org/paper/8f04013efcdf606d65145859f4f9eb6c48908869\",\"venue\":\"2019 IEEE International Conference on Data Mining (ICDM)\",\"year\":2019},{\"arxivId\":\"1606.04621\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"31717541\",\"name\":\"Parker A. Koch\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a861313608ef58ed09ab7c35a589ae22088ff2e9\",\"title\":\"Image Caption Generation with Text-Conditional Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/a861313608ef58ed09ab7c35a589ae22088ff2e9\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48195668\",\"name\":\"Jin Yuan\"},{\"authorId\":\"47059067\",\"name\":\"L. Zhang\"},{\"authorId\":\"2836997\",\"name\":\"Songrui Guo\"},{\"authorId\":\"1406190317\",\"name\":\"Y. Xiao\"},{\"authorId\":\"152985786\",\"name\":\"Z. Li\"}],\"doi\":\"10.1145/3394955\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"title\":\"Image Captioning with a Joint Attention Mechanism by Visual Concept Samples\",\"url\":\"https://www.semanticscholar.org/paper/2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"1807.02202\",\"authors\":[{\"authorId\":\"2719924\",\"name\":\"A. Chaganty\"},{\"authorId\":\"1776721\",\"name\":\"Stephen Mussmann\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"}],\"doi\":\"10.18653/v1/P18-1060\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d0c3adbee8098d47c7c0704e0841fa7daf8d161\",\"title\":\"The price of debiasing automatic metrics in natural language evalaution\",\"url\":\"https://www.semanticscholar.org/paper/9d0c3adbee8098d47c7c0704e0841fa7daf8d161\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1810.05475\",\"authors\":[{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"2370774\",\"name\":\"K. Camilleri\"}],\"doi\":\"10.1007/978-3-030-11018-5_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"17ff28401ba9cc24c6b83256150c86dc49308375\",\"title\":\"Quantifying the amount of visual information used by neural caption generators\",\"url\":\"https://www.semanticscholar.org/paper/17ff28401ba9cc24c6b83256150c86dc49308375\",\"venue\":\"ECCV Workshops\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"31717541\",\"name\":\"Parker A. Koch\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1145/3126686.3126717\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"1e83adb616b8466639a14e78f3d26120be7caf48\",\"title\":\"Watch What You Just Said: Image Captioning with Text-Conditional Attention\",\"url\":\"https://www.semanticscholar.org/paper/1e83adb616b8466639a14e78f3d26120be7caf48\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2012.05153\",\"authors\":[{\"authorId\":\"1476704317\",\"name\":\"Qi Zhu\"},{\"authorId\":\"2429278\",\"name\":\"Chenyu Gao\"},{\"authorId\":\"145498819\",\"name\":\"P. Wang\"},{\"authorId\":\"1720738301\",\"name\":\"Qi Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"952edddbb3438072312756762be1bfde287e1497\",\"title\":\"Simple is not Easy: A Simple Strong Baseline for TextVQA and TextCaps\",\"url\":\"https://www.semanticscholar.org/paper/952edddbb3438072312756762be1bfde287e1497\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1802.07089\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"39772032\",\"name\":\"C. Liu\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":\"10.1609/AAAI.V33I01.33011344\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2105df749e0ece889bf98a671716dac923561062\",\"title\":\"Attentive Tensor Product Learning\",\"url\":\"https://www.semanticscholar.org/paper/2105df749e0ece889bf98a671716dac923561062\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40663500\",\"name\":\"J. Chen\"},{\"authorId\":\"1848462\",\"name\":\"Yarong Han\"},{\"authorId\":\"144530696\",\"name\":\"Li Wan\"},{\"authorId\":\"144025048\",\"name\":\"Xing Zhou\"},{\"authorId\":\"144975798\",\"name\":\"Min Deng\"}],\"doi\":\"10.1080/01431161.2019.1594439\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"baf0c489e3f307187d5f20f10b4e24db37cd1cd1\",\"title\":\"Geospatial relation captioning for high-spatial-resolution images by using an attention-based neural network\",\"url\":\"https://www.semanticscholar.org/paper/baf0c489e3f307187d5f20f10b4e24db37cd1cd1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1904.09317\",\"authors\":[{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.3389/frai.2019.00028\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"78f36f2acb0c88cfe74572933cb52c9cc75a1d50\",\"title\":\"Challenges and Prospects in Vision and Language Research\",\"url\":\"https://www.semanticscholar.org/paper/78f36f2acb0c88cfe74572933cb52c9cc75a1d50\",\"venue\":\"Front. Artif. Intell.\",\"year\":2019},{\"arxivId\":\"1612.01033\",\"authors\":[{\"authorId\":\"3048367\",\"name\":\"M. Pedersoli\"},{\"authorId\":\"144982160\",\"name\":\"T. Lucas\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"34602236\",\"name\":\"Jakob Verbeek\"}],\"doi\":\"10.1109/ICCV.2017.140\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6d86f0e22fed5e065ecf54b273d540b2430f014d\",\"title\":\"Areas of Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6d86f0e22fed5e065ecf54b273d540b2430f014d\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145414749\",\"name\":\"Kun Fu\"},{\"authorId\":\"3068555\",\"name\":\"Jin Li\"},{\"authorId\":\"2512850\",\"name\":\"Junqi Jin\"},{\"authorId\":\"1700883\",\"name\":\"C. Zhang\"}],\"doi\":\"10.1109/TNNLS.2018.2813306\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f5fd76a48726d0cea93aa2201d3ba3d2c5903007\",\"title\":\"Image-Text Surgery: Efficient Concept Learning in Image Captioning by Generating Pseudopairs\",\"url\":\"https://www.semanticscholar.org/paper/f5fd76a48726d0cea93aa2201d3ba3d2c5903007\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2018},{\"arxivId\":\"2004.02077\",\"authors\":[{\"authorId\":\"26688118\",\"name\":\"Mihir Kale\"},{\"authorId\":\"98282509\",\"name\":\"S. Roy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5468fa4da5e548fee0c30c28b60e297b5838f183\",\"title\":\"Machine Translation Pre-training for Data-to-Text Generation - A Case Study in Czech\",\"url\":\"https://www.semanticscholar.org/paper/5468fa4da5e548fee0c30c28b60e297b5838f183\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":\"2001.08730\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"1490642986\",\"name\":\"Shivansh Pate\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":\"10.1109/WACV45572.2020.9093295\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a487065408c44d387aa1cf7836cd58405f945983\",\"title\":\"Robust Explanations for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a487065408c44d387aa1cf7836cd58405f945983\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35661876\",\"name\":\"Dicong Qiu\"},{\"authorId\":\"1904850\",\"name\":\"B. Rothrock\"},{\"authorId\":\"48614648\",\"name\":\"Tanvir Islam\"},{\"authorId\":\"151358685\",\"name\":\"Annie Didier\"},{\"authorId\":\"34933612\",\"name\":\"V. Sun\"},{\"authorId\":\"49484486\",\"name\":\"C. Mattmann\"},{\"authorId\":\"144638956\",\"name\":\"M. Ono\"}],\"doi\":\"10.1016/j.pss.2020.104943\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc6af12144e20f8f8a7fa8cb3cf18dead4cb8826\",\"title\":\"SCOTI: Science Captioning of Terrain Images for data prioritization and local image search\",\"url\":\"https://www.semanticscholar.org/paper/cc6af12144e20f8f8a7fa8cb3cf18dead4cb8826\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.08432\",\"authors\":[{\"authorId\":\"35480010\",\"name\":\"Gencer Sumbul\"},{\"authorId\":\"7641289\",\"name\":\"S. Nayak\"},{\"authorId\":\"100632303\",\"name\":\"B. Demir\"}],\"doi\":\"10.1109/TGRS.2020.3031111\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"269cf8e383efda217405fbef7ad51b139562f105\",\"title\":\"SD-RSIC: Summarization Driven Deep Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/269cf8e383efda217405fbef7ad51b139562f105\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.01166\",\"authors\":[{\"authorId\":\"143725625\",\"name\":\"Hung Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/P19-1564\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"594ad264d6b92afb9d13cb56ad8ffadba94a9f7a\",\"title\":\"Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/594ad264d6b92afb9d13cb56ad8ffadba94a9f7a\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1907.08175\",\"authors\":[{\"authorId\":\"32097919\",\"name\":\"Terrance Devries\"},{\"authorId\":\"144290131\",\"name\":\"A. Romero\"},{\"authorId\":\"2479987\",\"name\":\"L. Pineda\"},{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"3325894\",\"name\":\"M. Drozdzal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ca60ce984392ed3db9dc64ca34ec004416a7250\",\"title\":\"On the Evaluation of Conditional GANs\",\"url\":\"https://www.semanticscholar.org/paper/7ca60ce984392ed3db9dc64ca34ec004416a7250\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.04554\",\"authors\":[{\"authorId\":\"41071249\",\"name\":\"Ece Takmaz\"},{\"authorId\":\"24068173\",\"name\":\"Mario Giulianelli\"},{\"authorId\":\"3422247\",\"name\":\"Sandro Pezzelle\"},{\"authorId\":\"153915609\",\"name\":\"Arabella Sinclair\"},{\"authorId\":\"144151273\",\"name\":\"R. Fern\\u00e1ndez\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.353\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60eedf4faa04dd30f8c5769e0831268133549eb8\",\"title\":\"Refer, Reuse, Reduce: Generating Subsequent References in Visual and Conversational Contexts\",\"url\":\"https://www.semanticscholar.org/paper/60eedf4faa04dd30f8c5769e0831268133549eb8\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1906.01452\",\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474871\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/TPAMI.2019.2920899\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"83a3fe38887880bccc15daa740d8d5041f826d91\",\"title\":\"Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/83a3fe38887880bccc15daa740d8d5041f826d91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49868702\",\"name\":\"Ran Wei\"},{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1016/j.jvcir.2020.102751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"title\":\"Exploiting the local temporal information for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"1711.06354\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"title\":\"Grounded Objects and Interactions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"2004.14667\",\"authors\":[{\"authorId\":\"39420834\",\"name\":\"Hassan Kan\\u00e9\"},{\"authorId\":\"1665851460\",\"name\":\"Muhammed Yusuf Kocyigit\"},{\"authorId\":\"1752691\",\"name\":\"A. Abdalla\"},{\"authorId\":\"1388727777\",\"name\":\"Pelkins Ajanoh\"},{\"authorId\":\"1388727769\",\"name\":\"M. Coulibali\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c858aeb711e0f9e0b9752d887f965096481cc37d\",\"title\":\"NUBIA: NeUral Based Interchangeability Assessor for Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/c858aeb711e0f9e0b9752d887f965096481cc37d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1903.01072\",\"authors\":[{\"authorId\":\"47060391\",\"name\":\"J. Tan\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"},{\"authorId\":\"38900275\",\"name\":\"Joon Huang Chuah\"}],\"doi\":\"10.1109/TMM.2019.2904878\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9d6dbaf44d4437d9a27970bbc65e542706eae49f\",\"title\":\"COMIC: Toward A Compact Image Captioning Model With Attention\",\"url\":\"https://www.semanticscholar.org/paper/9d6dbaf44d4437d9a27970bbc65e542706eae49f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"2003.03305\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"22d733f5d5a995469dc916102f1806253645ae60\",\"title\":\"Captioning Images with Novel Objects via Online Vocabulary Expansion\",\"url\":\"https://www.semanticscholar.org/paper/22d733f5d5a995469dc916102f1806253645ae60\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145143129\",\"name\":\"Sonit Singh\"},{\"authorId\":\"1785901\",\"name\":\"Sarvnaz Karimi\"},{\"authorId\":\"1403196619\",\"name\":\"K. Ho-Shon\"},{\"authorId\":\"1476819159\",\"name\":\"Len Hamey\"}],\"doi\":\"10.1109/DICTA47822.2019.8945819\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e71a487d2a68394a6fd503eb4bf65c7838c6a818\",\"title\":\"From Chest X-Rays to Radiology Reports: A Multimodal Machine Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/e71a487d2a68394a6fd503eb4bf65c7838c6a818\",\"venue\":\"2019 Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150103839\",\"name\":\"F. V. Rijn\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"18172d0bbf0d0543d4477ebdcf22e942da41d044\",\"title\":\"Context-aware multimodal Recurrent Neural Network for automatic image captioning\",\"url\":\"https://www.semanticscholar.org/paper/18172d0bbf0d0543d4477ebdcf22e942da41d044\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1904.05876\",\"authors\":[{\"authorId\":\"38211837\",\"name\":\"Idan Schwartz\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"1918412\",\"name\":\"T. Hazan\"}],\"doi\":\"10.1109/CVPR.2019.01283\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf072e469d82e71f0515f32b686fb084f4f31714\",\"title\":\"A Simple Baseline for Audio-Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/cf072e469d82e71f0515f32b686fb084f4f31714\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1708.01759\",\"authors\":[{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"2848048\",\"name\":\"Jekaterina Novikova\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"41596a97bb0b7c63fd97fa6ea778c7b7cd3ff9d2\",\"title\":\"Referenceless Quality Estimation for Natural Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/41596a97bb0b7c63fd97fa6ea778c7b7cd3ff9d2\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2706292\",\"name\":\"Xianhua Zeng\"},{\"authorId\":\"145117241\",\"name\":\"L. Wen\"},{\"authorId\":\"49167131\",\"name\":\"B. Liu\"},{\"authorId\":\"144244342\",\"name\":\"Xiaojun Qi\"}],\"doi\":\"10.1016/J.NEUCOM.2018.11.114\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0c6212ad31b0c6aecc32f6f5a0d7fa078bcfffea\",\"title\":\"Deep learning for ultrasound image caption generation based on object detection\",\"url\":\"https://www.semanticscholar.org/paper/0c6212ad31b0c6aecc32f6f5a0d7fa078bcfffea\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"2004.02435\",\"authors\":[{\"authorId\":\"2416001\",\"name\":\"Shashank Bujimalla\"},{\"authorId\":\"2536658\",\"name\":\"Mahesh Subedar\"},{\"authorId\":\"1798616\",\"name\":\"O. Tickoo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"81e7e74d0f5f200e575df1908a1b0a5f0500906b\",\"title\":\"B-SCST: Bayesian Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/81e7e74d0f5f200e575df1908a1b0a5f0500906b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.04192\",\"authors\":[{\"authorId\":\"29068663\",\"name\":\"Ruize Wang\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"2193560\",\"name\":\"Piji Li\"},{\"authorId\":\"5006870\",\"name\":\"Haijun Shan\"},{\"authorId\":\"24263694\",\"name\":\"J. Zhang\"},{\"authorId\":\"1409702669\",\"name\":\"Qi Zhang\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c87e0cc7234d9a5fb2a33b8203d7d68c3d85d1d\",\"title\":\"Keep it Consistent: Topic-Aware Storytelling from an Image Stream via Iterative Multi-agent Communication\",\"url\":\"https://www.semanticscholar.org/paper/6c87e0cc7234d9a5fb2a33b8203d7d68c3d85d1d\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11219408\",\"name\":\"Yuxuan Ding\"},{\"authorId\":\"46314444\",\"name\":\"W. Wang\"},{\"authorId\":\"151118825\",\"name\":\"Mengmeng Jiang\"},{\"authorId\":\"48446599\",\"name\":\"H. Liu\"},{\"authorId\":\"30631999\",\"name\":\"Donghu Deng\"},{\"authorId\":\"145673165\",\"name\":\"Wei Wei\"},{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"}],\"doi\":\"10.1007/978-3-030-31726-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6b1c6af412cc94e5cc5dcc11663b9828307aa21e\",\"title\":\"Jointing Cross-Modality Retrieval to Reweight Attributes for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/6b1c6af412cc94e5cc5dcc11663b9828307aa21e\",\"venue\":\"PRCV\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004341822\",\"name\":\"Noorhan K. Fawzy\"},{\"authorId\":\"37370786\",\"name\":\"M. Marey\"},{\"authorId\":\"143987203\",\"name\":\"Mostafa Aref\"}],\"doi\":\"10.1007/978-3-030-58669-0_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18bacea52db19f69bc4933c6ad82eb5d22da281b\",\"title\":\"Video Captioning Using Attention Based Visual Fusion with Bi-temporal Context and Bi-modal Semantic Feature Learning\",\"url\":\"https://www.semanticscholar.org/paper/18bacea52db19f69bc4933c6ad82eb5d22da281b\",\"venue\":\"AISI\",\"year\":2020},{\"arxivId\":\"2011.01694\",\"authors\":[{\"authorId\":\"1805991958\",\"name\":\"Zden\\u011bk Kasner\"},{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"494a86b35c1137f14a5a89e911bdea7d4d288906\",\"title\":\"Data-to-Text Generation with Iterative Text Editing\",\"url\":\"https://www.semanticscholar.org/paper/494a86b35c1137f14a5a89e911bdea7d4d288906\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51162320\",\"name\":\"Imane Allaouzi\"},{\"authorId\":\"40242882\",\"name\":\"M. Ben ahmed\"},{\"authorId\":\"8621376\",\"name\":\"B. Benamrou\"},{\"authorId\":\"3449357\",\"name\":\"M. Ouardouz\"}],\"doi\":\"10.1145/3286606.3286863\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cfaf8e9f269a4d0a0ff2a0ccd07a32caac05072\",\"title\":\"Automatic Caption Generation for Medical Images\",\"url\":\"https://www.semanticscholar.org/paper/1cfaf8e9f269a4d0a0ff2a0ccd07a32caac05072\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1711.08195\",\"authors\":[{\"authorId\":\"29860450\",\"name\":\"Baoyu Jing\"},{\"authorId\":\"40526720\",\"name\":\"Pengtao Xie\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.18653/v1/P18-1240\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d45cc9a3a2fc064eccc0c915dbdf73cce559ce7\",\"title\":\"On the Automatic Generation of Medical Imaging Reports\",\"url\":\"https://www.semanticscholar.org/paper/5d45cc9a3a2fc064eccc0c915dbdf73cce559ce7\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1007/978-981-10-5209-5_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"77991dca4fdc99b6622c55f86ca87429a5b8b308\",\"title\":\"Deep Learning in Natural Language Generation from Images\",\"url\":\"https://www.semanticscholar.org/paper/77991dca4fdc99b6622c55f86ca87429a5b8b308\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4942547\",\"name\":\"Zongjian Zhang\"},{\"authorId\":\"144066903\",\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"145093625\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/TMM.2018.2888822\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2bc7959cf6bf0ae64f1a83f5fce1ac8cb2256ed3\",\"title\":\"High-Quality Image Captioning With Fine-Grained and Semantic-Guided Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/2bc7959cf6bf0ae64f1a83f5fce1ac8cb2256ed3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113001756\",\"name\":\"A. Deshpande\"},{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"24952249\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b2c60061ad32e28eb1e20aff42e062c9160786be\",\"title\":\"Diverse and Controllable Image Captioning with Part-of-Speech Guidance\",\"url\":\"https://www.semanticscholar.org/paper/b2c60061ad32e28eb1e20aff42e062c9160786be\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1783874\",\"name\":\"T. Wang\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"},{\"authorId\":\"144751998\",\"name\":\"C. He\"}],\"doi\":\"10.1007/s11063-019-09979-7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a54a18073b4b4a788e106d540d26817c8c898a63\",\"title\":\"Image Caption with Endogenous\\u2013Exogenous Attention\",\"url\":\"https://www.semanticscholar.org/paper/a54a18073b4b4a788e106d540d26817c8c898a63\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1109/CVPR.2015.7298966\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"title\":\"Deep correlation for matching images and text\",\"url\":\"https://www.semanticscholar.org/paper/efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"65767906\",\"name\":\"Silvio Olivastri\"},{\"authorId\":\"1931660\",\"name\":\"Gurkirt Singh\"},{\"authorId\":\"1754181\",\"name\":\"Fabio Cuzzolin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20f3d85b99b4b595b1c92f60a9b9a126f7384e15\",\"title\":\"An End-to-End Baseline for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20f3d85b99b4b595b1c92f60a9b9a126f7384e15\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134654394\",\"name\":\"Ren C. Luo\"},{\"authorId\":\"34373093\",\"name\":\"Yu-Ting Hsu\"},{\"authorId\":\"123191934\",\"name\":\"Yu-Cheng Wen\"},{\"authorId\":\"151486060\",\"name\":\"Huan-Jun Ye\"}],\"doi\":\"10.1109/ICPHYS.2019.8780171\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f10ee8e986c7e1a3f85c28b3e34eb8d5ffcb14ae\",\"title\":\"Visual Image Caption Generation for Service Robotics and Industrial Applications\",\"url\":\"https://www.semanticscholar.org/paper/f10ee8e986c7e1a3f85c28b3e34eb8d5ffcb14ae\",\"venue\":\"2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"50080046\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/TIP.2019.2916757\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"title\":\"CAM-RNN: Co-Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"1811.00185\",\"authors\":[{\"authorId\":\"50811366\",\"name\":\"Haojie Pan\"},{\"authorId\":\"4645572\",\"name\":\"Junpei Zhou\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"49420788\",\"name\":\"Y. Liu\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"144346838\",\"name\":\"Min Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c682ff7a573082c5b69880c4bf84cdd464b70ae5\",\"title\":\"Dial2Desc: End-to-end Dialogue Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/c682ff7a573082c5b69880c4bf84cdd464b70ae5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150197175\",\"name\":\"L. Zhou\"},{\"authorId\":\"49890476\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"49296155\",\"name\":\"Y. Jiang\"},{\"authorId\":\"145326655\",\"name\":\"Tao Zhang\"},{\"authorId\":\"145631869\",\"name\":\"W. Fan\"}],\"doi\":\"10.1109/TIP.2019.2928144\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"95db18799c539c82379585d25af66fd968ff000c\",\"title\":\"Re-Caption: Saliency-Enhanced Image Captioning Through Two-Phase Learning\",\"url\":\"https://www.semanticscholar.org/paper/95db18799c539c82379585d25af66fd968ff000c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1904.03396\",\"authors\":[{\"authorId\":\"73769459\",\"name\":\"Amit Moryossef\"},{\"authorId\":\"2089067\",\"name\":\"Y. Goldberg\"},{\"authorId\":\"7465342\",\"name\":\"I. Dagan\"}],\"doi\":\"10.18653/v1/N19-1236\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c593f8b5525851c2b453253835ddac6b1104a3b\",\"title\":\"Step-by-Step: Separating Planning from Realization in Neural Data-to-Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/7c593f8b5525851c2b453253835ddac6b1104a3b\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1905.03578\",\"authors\":[{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3422575\",\"name\":\"\\u00d6zg\\u00fcn \\u00c7i\\u00e7ek\"},{\"authorId\":\"47495989\",\"name\":\"S. M. Ali\"},{\"authorId\":\"2454800\",\"name\":\"F. Mahdisoltani\"},{\"authorId\":\"48934984\",\"name\":\"C. Zhang\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b26f0691b4fffa19c035e49ecc0eed97a826a26\",\"title\":\"Learning Representations for Predicting Future Activities\",\"url\":\"https://www.semanticscholar.org/paper/0b26f0691b4fffa19c035e49ecc0eed97a826a26\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.00229\",\"authors\":[{\"authorId\":\"51439692\",\"name\":\"Wanrong Zhu\"},{\"authorId\":\"1526982563\",\"name\":\"Xin Wang\"},{\"authorId\":\"41020222\",\"name\":\"Tsu-Jui Fu\"},{\"authorId\":\"47837986\",\"name\":\"An Yan\"},{\"authorId\":\"145382418\",\"name\":\"P. Narayana\"},{\"authorId\":\"113477341\",\"name\":\"Kazoo Sone\"},{\"authorId\":\"40632403\",\"name\":\"S. Basu\"},{\"authorId\":\"49336516\",\"name\":\"W. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6fe9177f0dd44764b15fe4d42bd96b50dc5fb144\",\"title\":\"Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/6fe9177f0dd44764b15fe4d42bd96b50dc5fb144\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03749\",\"authors\":[{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1109/CVPR42600.2020.01090\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"96485bda4f4118da249cc8a898230281ac8040a7\",\"title\":\"Better Captioning With Sequence-Level Exploration\",\"url\":\"https://www.semanticscholar.org/paper/96485bda4f4118da249cc8a898230281ac8040a7\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"14618116\",\"name\":\"Chongyang Gao\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1016/J.PATREC.2018.07.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab27d39857f613af36eff3fa3796904f474f8cbd\",\"title\":\"Sequence in sequence for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ab27d39857f613af36eff3fa3796904f474f8cbd\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40313071\",\"name\":\"Nermin Samet\"},{\"authorId\":\"2060717\",\"name\":\"Samet Hicsonmez\"},{\"authorId\":\"2446509\",\"name\":\"P. D. Sahin\"},{\"authorId\":\"2132749\",\"name\":\"Emre Akbas\"}],\"doi\":\"10.1109/SIU.2017.7960638\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"1c0d70587340adc412c6e2afd71012d563c1e724\",\"title\":\"Could we create a training set for image captioning using automatic translation?\",\"url\":\"https://www.semanticscholar.org/paper/1c0d70587340adc412c6e2afd71012d563c1e724\",\"venue\":\"2017 25th Signal Processing and Communications Applications Conference (SIU)\",\"year\":2017},{\"arxivId\":\"1803.11439\",\"authors\":[{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"144188763\",\"name\":\"Jian Yao\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2018.00834\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"85e2b2c35b916b1ee4926c155065d01b21c80c60\",\"title\":\"Regularizing RNNs for Caption Generation by Reconstructing the Past with the Present\",\"url\":\"https://www.semanticscholar.org/paper/85e2b2c35b916b1ee4926c155065d01b21c80c60\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"138537379\",\"name\":\"Okan Ulusoy\"},{\"authorId\":\"2802458\",\"name\":\"C. B. Akg\\u00fcl\"},{\"authorId\":\"1771794\",\"name\":\"E. Anarim\"}],\"doi\":\"10.1109/ASYU48272.2019.8946376\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a5bac7b9439c827610c09bc7e5b0e9893663c4a6\",\"title\":\"Improving Image Captioning with Language Modeling Regularizations\",\"url\":\"https://www.semanticscholar.org/paper/a5bac7b9439c827610c09bc7e5b0e9893663c4a6\",\"venue\":\"2019 Innovations in Intelligent Systems and Applications Conference (ASYU)\",\"year\":2019},{\"arxivId\":\"1901.06595\",\"authors\":[{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":\"10.1109/ICCVW.2019.00237\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5a97e9a253a60814f76dab86e8a4ec65fc86e261\",\"title\":\"Evaluating Text-to-Image Matching using Binary Image Selection (BISON)\",\"url\":\"https://www.semanticscholar.org/paper/5a97e9a253a60814f76dab86e8a4ec65fc86e261\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"108124522\",\"name\":\"T. Fujii\"},{\"authorId\":\"1768926\",\"name\":\"Yuichi Sei\"},{\"authorId\":\"1749328\",\"name\":\"Y. Tahara\"},{\"authorId\":\"1394541524\",\"name\":\"Ryohei Orihara\"},{\"authorId\":\"1740433\",\"name\":\"A. Ohsuga\"}],\"doi\":\"10.2991/IJNDC.K.190710.002\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d85b1543ab836881d3f0992395e8f210b8fc07f4\",\"title\":\"\\\"Never fry carrots without chopping\\\" Generating Cooking Recipes from Cooking Videos Using Deep Learning Considering Previous Process\",\"url\":\"https://www.semanticscholar.org/paper/d85b1543ab836881d3f0992395e8f210b8fc07f4\",\"venue\":\"Int. J. Networked Distributed Comput.\",\"year\":2019},{\"arxivId\":\"2009.03949\",\"authors\":[{\"authorId\":\"1699605396\",\"name\":\"Zeyu Wang\"},{\"authorId\":\"114200472\",\"name\":\"Berthy Feng\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"}],\"doi\":\"10.1007/978-3-030-58571-6_37\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"81be56a5783552d5b32463b392ff0499dd86a5ab\",\"title\":\"Towards Unique and Informative Captioning of Images\",\"url\":\"https://www.semanticscholar.org/paper/81be56a5783552d5b32463b392ff0499dd86a5ab\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1909.03396\",\"authors\":[{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"2904055\",\"name\":\"Ashish V. Thapliyal\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bd153dbee4f4b86962ddacbc47010785f9cdec3c\",\"title\":\"Quality Estimation for Image Captions Based on Large-scale Human Evaluations\",\"url\":\"https://www.semanticscholar.org/paper/bd153dbee4f4b86962ddacbc47010785f9cdec3c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7675774\",\"name\":\"C. Manning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"627107c02c2df1366965f11678dd3c4fb14ac9b3\",\"title\":\"CONNECTING IMAGES AND NATURAL LANGUAGE A DISSERTATION SUBMITTED TO THE DEPARTMENT OF COMPUTER SCIENCE AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY\",\"url\":\"https://www.semanticscholar.org/paper/627107c02c2df1366965f11678dd3c4fb14ac9b3\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"40401747\",\"name\":\"A. Lucchi\"},{\"authorId\":\"143936663\",\"name\":\"Thomas Hofmann\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4fa6a688f350831503d158f8f618c58d1e06bc5d\",\"title\":\"A Semi-supervised Framework for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4fa6a688f350831503d158f8f618c58d1e06bc5d\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1905.08110\",\"authors\":[{\"authorId\":\"47904580\",\"name\":\"Yiyu Wang\"},{\"authorId\":\"2073589\",\"name\":\"Jungang Xu\"},{\"authorId\":\"46676156\",\"name\":\"Yingfei Sun\"},{\"authorId\":\"40368776\",\"name\":\"Ben He\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8ef4a09eb86e150b08a80d59c2c092d1a56be780\",\"title\":\"Image Captioning based on Deep Learning Methods: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/8ef4a09eb86e150b08a80d59c2c092d1a56be780\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31497621\",\"name\":\"W. S. Cho\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":null,\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144628574\",\"name\":\"Chenyan Xiong\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"50468734\",\"name\":\"M. Wang\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"baba959c5defcf4e2c74e34ddcb135d029e1902f\",\"title\":\"Unsupervised Common Question Generation from Multiple Documents using Reinforced Contrastive Coordinator\",\"url\":\"https://www.semanticscholar.org/paper/baba959c5defcf4e2c74e34ddcb135d029e1902f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":\"1809.00862\",\"authors\":[{\"authorId\":\"144674044\",\"name\":\"Omar Mohammed\"},{\"authorId\":\"1754561\",\"name\":\"G. Bailly\"},{\"authorId\":\"1777631\",\"name\":\"D. Pellier\"}],\"doi\":\"10.1109/SNAMS.2018.8554834\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"000bd470128362423c2fe1354cc4a24b376018d4\",\"title\":\"Handwriting Styles: Benchmarks and Evaluation Metrics\",\"url\":\"https://www.semanticscholar.org/paper/000bd470128362423c2fe1354cc4a24b376018d4\",\"venue\":\"2018 Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS)\",\"year\":2018},{\"arxivId\":\"1604.02125\",\"authors\":[{\"authorId\":\"50005563\",\"name\":\"G. Christie\"},{\"authorId\":\"48222562\",\"name\":\"A. Laddha\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"37226164\",\"name\":\"Yash Goyal\"},{\"authorId\":\"98391857\",\"name\":\"K. Kochersberger\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.18653/v1/D16-1156\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"26eb2c900814707ae962184ad4173e754247a80a\",\"title\":\"Resolving Language and Vision Ambiguities Together: Joint Segmentation & Prepositional Attachment Resolution in Captioned Scenes\",\"url\":\"https://www.semanticscholar.org/paper/26eb2c900814707ae962184ad4173e754247a80a\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1911.03705\",\"authors\":[{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"150341221\",\"name\":\"Wangchunshu Zhou\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.165\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"title\":\"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"1790251284\",\"name\":\"Lin Li\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"9594118\",\"name\":\"C. Gu\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/s11063-020-10352-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"title\":\"Adaptively Converting Auxiliary Attributes and Textual Embedding for Video Captioning Based on BiLSTM\",\"url\":\"https://www.semanticscholar.org/paper/3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"venue\":\"Neural Process. Lett.\",\"year\":2020},{\"arxivId\":\"2003.00387\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"1490938689\",\"name\":\"Peng Wang\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/cvpr42600.2020.00998\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b4916e339caef2d2a98e633e1f0b2144e2b0c9e2\",\"title\":\"Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/b4916e339caef2d2a98e633e1f0b2144e2b0c9e2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2010.10604\",\"authors\":[{\"authorId\":\"8010189\",\"name\":\"Xinjie Fan\"},{\"authorId\":\"1515867113\",\"name\":\"Shujian Zhang\"},{\"authorId\":\"1409955695\",\"name\":\"B. Chen\"},{\"authorId\":\"38026572\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"83243b3639bbb42566bc300b4999db9a2b7a93c3\",\"title\":\"Bayesian Attention Modules\",\"url\":\"https://www.semanticscholar.org/paper/83243b3639bbb42566bc300b4999db9a2b7a93c3\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2786437\",\"name\":\"Linghui Li\"},{\"authorId\":\"144044848\",\"name\":\"Sheng Tang\"},{\"authorId\":\"4303531\",\"name\":\"Lixi Deng\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d1a7dae43b630d61d19d6cf139824380f2cf42f\",\"title\":\"Image Caption with Global-Local Attention\",\"url\":\"https://www.semanticscholar.org/paper/7d1a7dae43b630d61d19d6cf139824380f2cf42f\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1912.08226\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"261570379fd841b426a4c51e8004f2cf9f1df771\",\"title\":\"M2: Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/261570379fd841b426a4c51e8004f2cf9f1df771\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1718590\",\"name\":\"R. Gaizauskas\"}],\"doi\":\"10.18653/v1/W16-6631\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d6369cd3c8102550605bdbbaa14bc79caae6ad18\",\"title\":\"Don't Mention the Shoe! A Learning to Rank Approach to Content Selection for Image Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/d6369cd3c8102550605bdbbaa14bc79caae6ad18\",\"venue\":\"INLG\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2715920\",\"name\":\"Malihe Alikhani\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"51019115\",\"name\":\"S. Li\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"144884556\",\"name\":\"Matthew Stone\"}],\"doi\":\"10.18653/v1/2020.acl-main.583\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bb05ea53c4bfee60e045d26cc487d20141482949\",\"title\":\"Cross-modal Coherence Modeling for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/bb05ea53c4bfee60e045d26cc487d20141482949\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67010907\",\"name\":\"Martina Toshevska\"},{\"authorId\":\"67275493\",\"name\":\"Frosina Stojanovska\"},{\"authorId\":\"2269444\",\"name\":\"Eftim Zdravevski\"},{\"authorId\":\"1769456\",\"name\":\"Petre Lameski\"},{\"authorId\":\"144293427\",\"name\":\"S. Gievska\"}],\"doi\":\"10.15439/2020F57\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a8becc75e050e9ea52a1e4bb4faf9c1e39e5d599\",\"title\":\"Exploration into Deep Learning Text Generation Architectures for Dense Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a8becc75e050e9ea52a1e4bb4faf9c1e39e5d599\",\"venue\":\"2020 15th Conference on Computer Science and Information Systems (FedCSIS)\",\"year\":2020},{\"arxivId\":\"2003.08897\",\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"49051904\",\"name\":\"P. Yao\"},{\"authorId\":\"116829059\",\"name\":\"Shi-chen Lu\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/cvpr42600.2020.01034\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"833560cd68a3e3d1be1bc650756dd6c679798551\",\"title\":\"Normalized and Geometry-Aware Self-Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/833560cd68a3e3d1be1bc650756dd6c679798551\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Huang\"},{\"authorId\":\"51231229\",\"name\":\"Fengqi Yan\"},{\"authorId\":\"40515617\",\"name\":\"W. Xu\"},{\"authorId\":\"1716059\",\"name\":\"M. Li\"}],\"doi\":\"10.1109/ACCESS.2019.2947134\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"12e9fb63e5b46b241fed3ab2e9d95219db80e056\",\"title\":\"Multi-Attention and Incorporating Background Information Model for Chest X-Ray Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/12e9fb63e5b46b241fed3ab2e9d95219db80e056\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1804.08274\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2018.00782\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"title\":\"Jointly Localizing and Describing Events for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151034836\",\"name\":\"I. Hrga\"},{\"authorId\":\"1382503013\",\"name\":\"Marina Ivasic-Kos\"}],\"doi\":\"10.23919/MIPRO.2019.8756821\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2c602c5ac9c6de0108b8bd7d4075f5f2da2e790c\",\"title\":\"Deep Image Captioning: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/2c602c5ac9c6de0108b8bd7d4075f5f2da2e790c\",\"venue\":\"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30532805\",\"name\":\"Qingle Huang\"},{\"authorId\":\"2928799\",\"name\":\"Zicheng Liao\"}],\"doi\":\"10.5244/c.31.126\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"61cf3b276defcc82ccba3566da4a44a88740f013\",\"title\":\"A Convolutional Temporal Encoder for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/61cf3b276defcc82ccba3566da4a44a88740f013\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":\"1901.08942\",\"authors\":[{\"authorId\":\"143959713\",\"name\":\"Yimin Zhou\"},{\"authorId\":\"3156408\",\"name\":\"Y. Sun\"},{\"authorId\":\"145513516\",\"name\":\"Vasant G Honavar\"}],\"doi\":\"10.1109/WACV.2019.00036\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f4c45108cb41051010d8a5175b8da23eb246c967\",\"title\":\"Improving Image Captioning by Leveraging Knowledge Graphs\",\"url\":\"https://www.semanticscholar.org/paper/f4c45108cb41051010d8a5175b8da23eb246c967\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd6a2c7bbb54ad5c1350eb02718211fe86e125e5\",\"title\":\"End-to-End Multimodal Dialog Systems with Hierarchical Multimodal Attention on Video Features\",\"url\":\"https://www.semanticscholar.org/paper/fd6a2c7bbb54ad5c1350eb02718211fe86e125e5\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3727644\",\"name\":\"Satoshi Tsutsui\"},{\"authorId\":\"48387892\",\"name\":\"Manish Kumar\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9a71ccb31533936a7c4405453a0496e22a1d49f5\",\"title\":\"Scene Graph Generation from Images\",\"url\":\"https://www.semanticscholar.org/paper/9a71ccb31533936a7c4405453a0496e22a1d49f5\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"35573122\",\"name\":\"Dingdong Yang\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":\"10.1007/978-3-030-28954-6_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"25f1d8619897093fd6eab70b2cb9c44fd030203c\",\"title\":\"Interpretable Text-to-Image Synthesis with Hierarchical Semantic Layout Generation\",\"url\":\"https://www.semanticscholar.org/paper/25f1d8619897093fd6eab70b2cb9c44fd030203c\",\"venue\":\"Explainable AI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123726071\",\"name\":\"Lingbo Mo\"},{\"authorId\":\"1792246\",\"name\":\"C. Zhang\"},{\"authorId\":\"144105871\",\"name\":\"Y. Ji\"},{\"authorId\":\"144909678\",\"name\":\"Zheng Hu\"}],\"doi\":\"10.1007/978-3-030-20870-7_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a050168894dc08ee5b21c62c9085cef633cf32cf\",\"title\":\"Adversarial Learning for Visual Storytelling with Sense Group Partition\",\"url\":\"https://www.semanticscholar.org/paper/a050168894dc08ee5b21c62c9085cef633cf32cf\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":\"1612.09161\",\"authors\":[{\"authorId\":\"145476835\",\"name\":\"Ang Li\"},{\"authorId\":\"14258597\",\"name\":\"A. Jabri\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":\"10.1109/ICCV.2017.449\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8938988d82c6eb18e47deb5e69220652446c60bd\",\"title\":\"Learning Visual N-Grams from Web Data\",\"url\":\"https://www.semanticscholar.org/paper/8938988d82c6eb18e47deb5e69220652446c60bd\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1701.02870\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"}],\"doi\":\"10.1109/CVPR.2017.120\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e782437503f2a24fd1a836a434da395bf15c88c2\",\"title\":\"Context-Aware Captions from Context-Agnostic Supervision\",\"url\":\"https://www.semanticscholar.org/paper/e782437503f2a24fd1a836a434da395bf15c88c2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1611.08034\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"2624978\",\"name\":\"C. Chen\"},{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2482836\",\"name\":\"Qinliang Su\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.18653/v1/P17-1030\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f35f9a967fea696f2522d395ceae0988a53ddeae\",\"title\":\"Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling\",\"url\":\"https://www.semanticscholar.org/paper/f35f9a967fea696f2522d395ceae0988a53ddeae\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144566098\",\"name\":\"Yang Fan\"},{\"authorId\":\"2073589\",\"name\":\"Jungang Xu\"},{\"authorId\":\"2643170\",\"name\":\"Yingfei Sun\"},{\"authorId\":null,\"name\":\"Yiyu Wang\"}],\"doi\":\"10.1007/978-3-030-30490-4_23\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74e4ab92bea5086239d4c0bae100897bb7f833b7\",\"title\":\"A Novel Image Captioning Method Based on Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/74e4ab92bea5086239d4c0bae100897bb7f833b7\",\"venue\":\"ICANN\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2388012\",\"name\":\"S. Dash\"},{\"authorId\":\"102265808\",\"name\":\"S. Saha\"},{\"authorId\":\"1882574\",\"name\":\"Partha Pakray\"},{\"authorId\":\"1747784\",\"name\":\"Alexander Gelbukh\"}],\"doi\":\"10.3233/JIFS-179027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"200fb3952463d0051a613e14710b7f2ad5ffc173\",\"title\":\"Generating image captions through multimodal embedding\",\"url\":\"https://www.semanticscholar.org/paper/200fb3952463d0051a613e14710b7f2ad5ffc173\",\"venue\":\"J. Intell. Fuzzy Syst.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49528408\",\"name\":\"H. Wang\"},{\"authorId\":\"46867445\",\"name\":\"Y. Zhang\"},{\"authorId\":\"9305704\",\"name\":\"Xiaosheng Yu\"}],\"doi\":\"10.1155/2020/3062706\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4b652fa2b4ea242a25a62e2e67eddde867e7e189\",\"title\":\"An Overview of Image Caption Generation Methods\",\"url\":\"https://www.semanticscholar.org/paper/4b652fa2b4ea242a25a62e2e67eddde867e7e189\",\"venue\":\"Comput. Intell. Neurosci.\",\"year\":2020},{\"arxivId\":\"1809.02805\",\"authors\":[{\"authorId\":\"35585536\",\"name\":\"Jialin Wu\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":\"10.18653/v1/W19-4812\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"491d0101110fcacfad7c739d5fd807cf8b79de18\",\"title\":\"Faithful Multimodal Explanation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/491d0101110fcacfad7c739d5fd807cf8b79de18\",\"venue\":\"ACL 2019\",\"year\":2018},{\"arxivId\":\"1908.10731\",\"authors\":[{\"authorId\":\"3014143\",\"name\":\"Semih Yavuz\"},{\"authorId\":\"2188497\",\"name\":\"Abhinav Rastogi\"},{\"authorId\":\"2531558\",\"name\":\"G. Chao\"},{\"authorId\":\"152325757\",\"name\":\"D. Hakkani-T\\u00fcr\"}],\"doi\":\"10.18653/v1/W19-5917\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9e62f783e1a3f8c56c9bb117b7b0e0cce5870551\",\"title\":\"DeepCopy: Grounded Response Generation with Hierarchical Pointer Networks\",\"url\":\"https://www.semanticscholar.org/paper/9e62f783e1a3f8c56c9bb117b7b0e0cce5870551\",\"venue\":\"SIGdial\",\"year\":2019},{\"arxivId\":\"1908.11310\",\"authors\":[{\"authorId\":\"19444389\",\"name\":\"Koustav Ghosal\"},{\"authorId\":\"36809068\",\"name\":\"Aakanksha Rana\"},{\"authorId\":\"118065745\",\"name\":\"A. Smolic\"}],\"doi\":\"10.1109/ICCVW.2019.00556\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5d204156022f65e34706d4211e05bcb578940939\",\"title\":\"Aesthetic Image Captioning From Weakly-Labelled Photographs\",\"url\":\"https://www.semanticscholar.org/paper/5d204156022f65e34706d4211e05bcb578940939\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1811.10787\",\"authors\":[{\"authorId\":\"144599697\",\"name\":\"Y. Feng\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474876\",\"name\":\"Wei Liu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2019.00425\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"580fd9a601314ea32dc85ec98267b411dd3465cf\",\"title\":\"Unsupervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/580fd9a601314ea32dc85ec98267b411dd3465cf\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.09421\",\"authors\":[{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"12122088\",\"name\":\"Aihong Yuan\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1007/s11042-018-5856-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c03b9770f8cfa7b1420af68cd4d2236c182df967\",\"title\":\"Multi-modal gated recurrent units for image description\",\"url\":\"https://www.semanticscholar.org/paper/c03b9770f8cfa7b1420af68cd4d2236c182df967\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"153598395\",\"name\":\"J. Ross\"},{\"authorId\":\"2500466\",\"name\":\"Tom Sercu\"}],\"doi\":\"10.1109/CVPR.2019.01071\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"24fdb40c354599ee33a25530c3fa7b9ebc75e840\",\"title\":\"Adversarial Semantic Alignment for Improved Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/24fdb40c354599ee33a25530c3fa7b9ebc75e840\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"144404414\",\"name\":\"S. Satoh\"}],\"doi\":\"10.1145/3123266.3127898\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2e0e08e4d4c722d0f54f5a124ca28a67d74ce3e\",\"title\":\"MANet: A Modal Attention Network for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/b2e0e08e4d4c722d0f54f5a124ca28a67d74ce3e\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66528414\",\"name\":\"Xiangheng He\"},{\"authorId\":\"48569127\",\"name\":\"Xinde Li\"}],\"doi\":\"10.1109/ICARM49381.2020.9195335\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"287a19d2ec291618882c0660d2c4856642559227\",\"title\":\"Modeling coherence and diversity for image paragraph captioning\",\"url\":\"https://www.semanticscholar.org/paper/287a19d2ec291618882c0660d2c4856642559227\",\"venue\":\"2020 5th International Conference on Advanced Robotics and Mechatronics (ICARM)\",\"year\":2020},{\"arxivId\":\"1608.04959\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1145/2964284.2984062\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41eec260e0980f8fd0af46f0dfc043139087a928\",\"title\":\"Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/41eec260e0980f8fd0af46f0dfc043139087a928\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.24963/ijcai.2019/877\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"title\":\"Deep Learning for Video Captioning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1145/3239576.3239580\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"title\":\"Video Captioning using Hierarchical Multi-Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"venue\":\"ICAIP '18\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"47659605\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/TPAMI.2017.2700381\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"17d4fee6b21c9277375d6cf0c9087828595009b6\",\"title\":\"Retrieval of Sentence Sequences for an Image Stream via Coherence Recurrent Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/17d4fee6b21c9277375d6cf0c9087828595009b6\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2006291\",\"name\":\"Harsh Jhamtani\"},{\"authorId\":\"3375999\",\"name\":\"Varun Gangal\"},{\"authorId\":\"144547315\",\"name\":\"E. Hovy\"},{\"authorId\":\"1700325\",\"name\":\"Graham Neubig\"},{\"authorId\":\"1400419309\",\"name\":\"Taylor Berg-Kirkpatrick\"}],\"doi\":\"10.18653/v1/P18-1154\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"de5834305ea419c25b17f0c8d27bad6a5feb311a\",\"title\":\"Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data\",\"url\":\"https://www.semanticscholar.org/paper/de5834305ea419c25b17f0c8d27bad6a5feb311a\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1809.06227\",\"authors\":[{\"authorId\":\"3441420\",\"name\":\"Tszhang Guo\"},{\"authorId\":\"3307026\",\"name\":\"S. Chang\"},{\"authorId\":\"2482533\",\"name\":\"Mo Yu\"},{\"authorId\":\"144654778\",\"name\":\"K. Bai\"}],\"doi\":\"10.18653/v1/D18-1083\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a78199a2cc678818489087454fe1150db4870196\",\"title\":\"Improving Reinforcement Learning Based Image Captioning with Natural Language Prior\",\"url\":\"https://www.semanticscholar.org/paper/a78199a2cc678818489087454fe1150db4870196\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47859297\",\"name\":\"D. Wang\"},{\"authorId\":\"143984297\",\"name\":\"Daniel Beck\"},{\"authorId\":\"143620680\",\"name\":\"Trevor Cohn\"}],\"doi\":\"10.18653/v1/D19-6405\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"93447c939657029b6305053599f51e78ba8a4c3d\",\"title\":\"On the Role of Scene Graphs in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93447c939657029b6305053599f51e78ba8a4c3d\",\"venue\":\"LANTERN@EMNLP-IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145522783\",\"name\":\"Z. Sun\"},{\"authorId\":\"144361839\",\"name\":\"X. Lin\"},{\"authorId\":\"50218964\",\"name\":\"Zhaohui Wang\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"}],\"doi\":\"10.1007/978-3-030-00767-6_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8b600d520f95b811857052d864ee54567064dd9\",\"title\":\"Multi-decoder Based Co-attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b8b600d520f95b811857052d864ee54567064dd9\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.1109/CVPR.2018.00521\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7fdac774e51e0aa8f9921e857067801d73a8d2d0\",\"title\":\"Categorizing Concepts with Basic Level for Vision-to-Language\",\"url\":\"https://www.semanticscholar.org/paper/7fdac774e51e0aa8f9921e857067801d73a8d2d0\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49672743\",\"name\":\"C. Li\"},{\"authorId\":\"50763392\",\"name\":\"J. Chen\"},{\"authorId\":\"47718901\",\"name\":\"Weitao Wan\"},{\"authorId\":\"2641581\",\"name\":\"Tianpeng Li\"}],\"doi\":\"10.1007/978-3-319-71607-7_54\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"edc7f610c6dadc2e8c854c0fd9de320457ae942c\",\"title\":\"Combining Object-Based Attention and Attributes for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/edc7f610c6dadc2e8c854c0fd9de320457ae942c\",\"venue\":\"ICIG\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.18653/v1/W16-3203\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ecbaa92c289f4f5ff9a57b19a2725036a92311f5\",\"title\":\"Focused Evaluation for Image Description with Binary Forced-Choice Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ecbaa92c289f4f5ff9a57b19a2725036a92311f5\",\"venue\":\"VL@ACL\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1830455445\",\"name\":\"Nishant Prabhu\"},{\"authorId\":\"1704709\",\"name\":\"Vasudeva Varma\"},{\"authorId\":\"2881425\",\"name\":\"Balaji Vasan Srinivasan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"30b9a631d23b756680da6c27f44b254997d98c3d\",\"title\":\"Text Simplification: From Daedalian to Simple\",\"url\":\"https://www.semanticscholar.org/paper/30b9a631d23b756680da6c27f44b254997d98c3d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\"},{\"authorId\":\"1380374156\",\"name\":\"C V Jawahar\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":\"10.1145/3343031.3350939\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eeacc1da20db16e2a2bcf4a5902bcff556c7a0ed\",\"title\":\"Towards Increased Accessibility of Meme Images with the Help of Rich Face Emotion Captions\",\"url\":\"https://www.semanticscholar.org/paper/eeacc1da20db16e2a2bcf4a5902bcff556c7a0ed\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1603.07141\",\"authors\":[{\"authorId\":\"1780343\",\"name\":\"Arnau Ramisa\"},{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1109/TPAMI.2017.2721945\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82d9b588eabeb6bf4baa945d5c71b3bf89dd1e69\",\"title\":\"BreakingNews: Article Annotation by Image and Text Processing\",\"url\":\"https://www.semanticscholar.org/paper/82d9b588eabeb6bf4baa945d5c71b3bf89dd1e69\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"1901.00398\",\"authors\":[{\"authorId\":\"3360992\",\"name\":\"Cristina Garbacea\"},{\"authorId\":\"40502796\",\"name\":\"Samuel Carton\"},{\"authorId\":\"2601405\",\"name\":\"S. Yan\"},{\"authorId\":\"1743469\",\"name\":\"Q. Mei\"}],\"doi\":\"10.18653/v1/D19-1409\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4260a9214dc23ce8149cf966ecf047d54597d798\",\"title\":\"Judge the Judges: A Large-Scale Evaluation Study of Neural Language Models for Online Review Generation\",\"url\":\"https://www.semanticscholar.org/paper/4260a9214dc23ce8149cf966ecf047d54597d798\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1646716323\",\"name\":\"Zihao Fu\"},{\"authorId\":\"1996394\",\"name\":\"Lidong Bing\"},{\"authorId\":\"144594306\",\"name\":\"Wai Lam\"},{\"authorId\":\"38797620\",\"name\":\"S. Jameel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c8d0788ebea09f653e3b105825e2d9636c881a7d\",\"title\":\"Dynamic Topic Tracker for KB-to-Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/c8d0788ebea09f653e3b105825e2d9636c881a7d\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738607561\",\"name\":\"S. Nikiforova\"},{\"authorId\":\"2398266\",\"name\":\"Tejaswini Deoskar\"},{\"authorId\":\"2129425\",\"name\":\"D. Paperno\"},{\"authorId\":\"2021738\",\"name\":\"Y. Winter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"569459a9b1ca74bbf4b74908c30dc583d78b59ab\",\"title\":\"Geo-Aware Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/569459a9b1ca74bbf4b74908c30dc583d78b59ab\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2010.10042\",\"authors\":[{\"authorId\":\"2965600\",\"name\":\"Y. Miura\"},{\"authorId\":\"49889487\",\"name\":\"Yuhao Zhang\"},{\"authorId\":\"2356307\",\"name\":\"C. Langlotz\"},{\"authorId\":\"1746807\",\"name\":\"Dan Jurafsky\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9e90c17ef40404b79ad0f12d9b9c94656f12dfcd\",\"title\":\"Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/9e90c17ef40404b79ad0f12d9b9c94656f12dfcd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.07771\",\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"1703003796\",\"name\":\"Anish Madan\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"152253423\",\"name\":\"Y. Yu\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3949fff9e84261dee6e856faf4b04a27dfa460cd\",\"title\":\"C3VQG: Category Consistent Cyclic Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/3949fff9e84261dee6e856faf4b04a27dfa460cd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.00163\",\"authors\":[{\"authorId\":\"15401738\",\"name\":\"Zekang Li\"},{\"authorId\":\"115419547\",\"name\":\"Zongjia Li\"},{\"authorId\":\"27672597\",\"name\":\"Jinchao Zhang\"},{\"authorId\":\"49771779\",\"name\":\"Yang Feng\"},{\"authorId\":\"150954670\",\"name\":\"Cheng Niu\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f4e0723e048941ea73c77a7c69dbb731ef8de750\",\"title\":\"Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/f4e0723e048941ea73c77a7c69dbb731ef8de750\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1707.06436\",\"authors\":[{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"},{\"authorId\":\"3393640\",\"name\":\"Soma Shirakabe\"},{\"authorId\":null,\"name\":\"Yun He\"},{\"authorId\":\"9935341\",\"name\":\"S. Ueta\"},{\"authorId\":\"5014206\",\"name\":\"T. Suzuki\"},{\"authorId\":\"49897653\",\"name\":\"K. Abe\"},{\"authorId\":\"2554424\",\"name\":\"Asako Kanezaki\"},{\"authorId\":\"49133490\",\"name\":\"Shinichiro Morita\"},{\"authorId\":\"22219521\",\"name\":\"Toshiyuki Yabe\"},{\"authorId\":\"50544018\",\"name\":\"Yoshihiro Kanehara\"},{\"authorId\":\"22174281\",\"name\":\"Hiroya Yatsuyanagi\"},{\"authorId\":\"1692565\",\"name\":\"S. Maruyama\"},{\"authorId\":\"10756539\",\"name\":\"Ryousuke Takasawa\"},{\"authorId\":\"3217653\",\"name\":\"Masataka Fuchida\"},{\"authorId\":\"2642022\",\"name\":\"Y. Miyashita\"},{\"authorId\":\"34935749\",\"name\":\"Kazushige Okayasu\"},{\"authorId\":\"20505300\",\"name\":\"Yuta Matsuzaki\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a9ca15aebadad4b7cfa18c5af91c431d6045b86\",\"title\":\"cvpaper.challenge in 2016: Futuristic Computer Vision through 1, 600 Papers Survey\",\"url\":\"https://www.semanticscholar.org/paper/8a9ca15aebadad4b7cfa18c5af91c431d6045b86\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1908.00169\",\"authors\":[{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"145527564\",\"name\":\"Z. Zhang\"},{\"authorId\":\"31115284\",\"name\":\"Jingjing Li\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"}],\"doi\":\"10.1145/3343031.3350961\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250\",\"title\":\"Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1609.06647\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"48818137\",\"name\":\"Samy Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/TPAMI.2016.2587640\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e\",\"title\":\"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge\",\"url\":\"https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1906.02365\",\"authors\":[{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144864336\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TPAMI.2019.2909864\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d0134f63879cedf3cdfe795bd2fd7c48d9554e4a\",\"title\":\"Context-Aware Visual Policy Network for Fine-Grained Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d0134f63879cedf3cdfe795bd2fd7c48d9554e4a\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2019},{\"arxivId\":\"1603.09046\",\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"3369734\",\"name\":\"M. Yamaguchi\"},{\"authorId\":\"8197937\",\"name\":\"K. Ohnishi\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a12540c85c6835eb6fd36131107d82c50d2b8d0\",\"title\":\"Dense Image Representation with Spatial Pyramid VLAD Coding of CNN for Locally Robust Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8a12540c85c6835eb6fd36131107d82c50d2b8d0\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"506ea19145838a035e7dba535519fb40a3a0018c\",\"title\":\"Learning Shared Multimodal Embeddings with Unpaired Data\",\"url\":\"https://www.semanticscholar.org/paper/506ea19145838a035e7dba535519fb40a3a0018c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144841441\",\"name\":\"J. Xu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3123266.3123448\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"title\":\"Learning Multimodal Attention LSTM Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7165718\",\"name\":\"Yinong Long\"},{\"authorId\":\"49606678\",\"name\":\"Jianan Wang\"},{\"authorId\":\"50069915\",\"name\":\"Zhen Xu\"},{\"authorId\":\"10125385\",\"name\":\"Z. Wang\"},{\"authorId\":\"2806178\",\"name\":\"Baoxun Wang\"},{\"authorId\":\"47196992\",\"name\":\"Zhuoran Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f9d0fd61d0d4a537e1a88f432f7bf381257c70f4\",\"title\":\"A Knowledge Enhanced Generative Conversational Service Agent\",\"url\":\"https://www.semanticscholar.org/paper/f9d0fd61d0d4a537e1a88f432f7bf381257c70f4\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1910.01863\",\"authors\":[{\"authorId\":\"1776599\",\"name\":\"J. Kanerva\"},{\"authorId\":\"1779597\",\"name\":\"Samuel R\\u00f6nnqvist\"},{\"authorId\":\"1382474649\",\"name\":\"Riina Kekki\"},{\"authorId\":\"1680811\",\"name\":\"T. Salakoski\"},{\"authorId\":\"1694491\",\"name\":\"F. Ginter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d50e176bc2d3bafda791f253f604ec9041fd8a7\",\"title\":\"Template-free Data-to-Text Generation of Finnish Sports News\",\"url\":\"https://www.semanticscholar.org/paper/5d50e176bc2d3bafda791f253f604ec9041fd8a7\",\"venue\":\"NODALIDA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143852685\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"1708642\",\"name\":\"J. Laaksonen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"204184e8a54c42bac589c19c3fa0c5d3d2c6009b\",\"title\":\"Can Saliency Information Benefit Image Captioning Models?\",\"url\":\"https://www.semanticscholar.org/paper/204184e8a54c42bac589c19c3fa0c5d3d2c6009b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.18653/v1/d19-64\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"55c9ec9ee9bcedafc502684623dfa799c6ce35e7\",\"title\":\"Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)\",\"url\":\"https://www.semanticscholar.org/paper/55c9ec9ee9bcedafc502684623dfa799c6ce35e7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Valeo. ai\"},{\"authorId\":null,\"name\":\"Valeo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9c8707ffb59b37029942baf4606b415b462de8eb\",\"title\":\"Driving Behavior Explanation with Multi-level Fusion\",\"url\":\"https://www.semanticscholar.org/paper/9c8707ffb59b37029942baf4606b415b462de8eb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ayushman Singh Sisodiya\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"981075404a933851c3fde8ee0b220e1c6ff2a2f5\",\"title\":\"Review of deep learning approaches for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/981075404a933851c3fde8ee0b220e1c6ff2a2f5\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1609.03976\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"2934336\",\"name\":\"Lo\\u00efc Barrault\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7876f448942e2658c3911c42b32ced10f85a4800\",\"title\":\"Multimodal Attention for Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/7876f448942e2658c3911c42b32ced10f85a4800\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1912.06365\",\"authors\":[{\"authorId\":\"1443435125\",\"name\":\"Zhengcong Fei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38e48d6b39ce94ddc2a0bf20320598739187bfef\",\"title\":\"Fast Image Caption Generation with Position Alignment\",\"url\":\"https://www.semanticscholar.org/paper/38e48d6b39ce94ddc2a0bf20320598739187bfef\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3014143\",\"name\":\"Semih Yavuz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"76ad7b1e95c853a55f0f0ddf264a771b2c952647\",\"title\":\"DeepAssist: Deep Knowledge Grounding for Factual and Conversational Natural Language Interfaces\",\"url\":\"https://www.semanticscholar.org/paper/76ad7b1e95c853a55f0f0ddf264a771b2c952647\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1606.04631\",\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"83672162\",\"name\":\"Zi Huang\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967258\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"title\":\"Bidirectional Long-Short Term Memory for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36251013\",\"name\":\"Wei Li\"},{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1016/j.patrec.2017.10.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ba881ec9ed2b435468ba6bbdc1821bde7778417\",\"title\":\"Multimodal architecture for video captioning with memory networks and an attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/0ba881ec9ed2b435468ba6bbdc1821bde7778417\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46671592\",\"name\":\"Y. Xian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e11ad2a6c18112c13e9e65d64ef85cb1ff4883d0\",\"title\":\"Exploring the Internal Statistics: Single Image Super-Resolution, Completion and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e11ad2a6c18112c13e9e65d64ef85cb1ff4883d0\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"2010.02824\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144721617\",\"name\":\"Y. Asano\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"145414740\",\"name\":\"J. Henriques\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"title\":\"Support-set bottlenecks for video-text representation learning\",\"url\":\"https://www.semanticscholar.org/paper/78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.11696\",\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"8350409\",\"name\":\"I. Padhi\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"},{\"authorId\":\"153598395\",\"name\":\"J. Ross\"},{\"authorId\":\"1999174380\",\"name\":\"Yair Schiff\"},{\"authorId\":\"49832828\",\"name\":\"R. A. Young\"},{\"authorId\":\"2679155\",\"name\":\"Brian M. Belgodere\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"title\":\"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5434752\",\"name\":\"Y. Jiang\"}],\"doi\":\"10.5120/IJCA2019918660\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"43c22ebb5ff264a5ea996c163464cf761035a405\",\"title\":\"Multi-Feature Fusion for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/43c22ebb5ff264a5ea996c163464cf761035a405\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"505a777fbf3760c16a80a3e893017d91d2c35b08\",\"title\":\"Supplementary Material to : Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/505a777fbf3760c16a80a3e893017d91d2c35b08\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143710447\",\"name\":\"F. Gustafsson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"430cfa330b32633f6022ffdea0102bbd58a2fa49\",\"title\":\"Neural Image Captioning for Intelligent Vehicle-to-Passenger Communication\",\"url\":\"https://www.semanticscholar.org/paper/430cfa330b32633f6022ffdea0102bbd58a2fa49\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"2155311\",\"name\":\"Eunbyung Park\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/ICCV.2015.283\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ced9f7178f8032d3408fcba493c02eb48e8a8636\",\"title\":\"Visual Madlibs: Fill in the Blank Description Generation and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ced9f7178f8032d3408fcba493c02eb48e8a8636\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2794096\",\"name\":\"Yingce Xia\"},{\"authorId\":\"144054173\",\"name\":\"Fei Tian\"},{\"authorId\":\"143826491\",\"name\":\"T. Qin\"},{\"authorId\":\"1708598\",\"name\":\"N. Yu\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":\"10.1007/978-3-319-71249-9_49\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8a1b22883b9ee40414f70fce623365f991a4ef5\",\"title\":\"Sequence Generation with Target Attention\",\"url\":\"https://www.semanticscholar.org/paper/d8a1b22883b9ee40414f70fce623365f991a4ef5\",\"venue\":\"ECML/PKDD\",\"year\":2017},{\"arxivId\":\"1807.09434\",\"authors\":[{\"authorId\":\"2183432\",\"name\":\"Boeun Kim\"},{\"authorId\":\"49380412\",\"name\":\"Y. Lee\"},{\"authorId\":\"3011724\",\"name\":\"Hyedong Jung\"},{\"authorId\":\"2529532\",\"name\":\"C. S. Cho\"}],\"doi\":\"10.1007/978-3-030-11018-5_12\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"239a38663967140e026385f6625a913a3e7b1cd7\",\"title\":\"Distinctive-attribute Extraction for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/239a38663967140e026385f6625a913a3e7b1cd7\",\"venue\":\"ECCV Workshops\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153297544\",\"name\":\"X. Yang\"},{\"authorId\":\"118565563\",\"name\":\"Chong-Yang Gao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1145/3394171.3413859\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"96389251e4f0f8494dfc8dc67c05992eed8e192b\",\"title\":\"Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96389251e4f0f8494dfc8dc67c05992eed8e192b\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48569838\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1109/TMM.2019.2896516\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bc73f0f7d895a089416bc8f6090f3f8707c6a12f\",\"title\":\"Know More Say Less: Image Captioning Based on Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/bc73f0f7d895a089416bc8f6090f3f8707c6a12f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"1903.10118\",\"authors\":[{\"authorId\":\"144362275\",\"name\":\"K. Hagiwara\"},{\"authorId\":\"2374364\",\"name\":\"Yusuke Mukuta\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"604ce3df514d0fddcc944c1741a1217d94aeb19f\",\"title\":\"End-to-End Learning Using Cycle Consistency for Image-to-Caption Transformations\",\"url\":\"https://www.semanticscholar.org/paper/604ce3df514d0fddcc944c1741a1217d94aeb19f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.07482\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"151480727\",\"name\":\"Zixiu Wu\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"37f371fe04f3dc38df7f27f43277ba15d3637890\",\"title\":\"Imperial College London Submission to VATEX Video Captioning Task\",\"url\":\"https://www.semanticscholar.org/paper/37f371fe04f3dc38df7f27f43277ba15d3637890\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50506129\",\"name\":\"E. Barati\"},{\"authorId\":\"2410994\",\"name\":\"Xue-wen Chen\"}],\"doi\":\"10.1145/3343031.3351037\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf8a3f260fbe4ee104380437cd576a556dccd290\",\"title\":\"Critic-based Attention Network for Event-based Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cf8a3f260fbe4ee104380437cd576a556dccd290\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.1016/j.neucom.2020.03.087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1c830d89069401945c430c6ddccc3ea4b3bd924\",\"title\":\"Evolutionary recurrent neural network for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1c830d89069401945c430c6ddccc3ea4b3bd924\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shuang Liu\"},{\"authorId\":\"144769215\",\"name\":\"L. Bai\"},{\"authorId\":\"1687503\",\"name\":\"Yanming Guo\"},{\"authorId\":\"3348635\",\"name\":\"Haoran Wang\"}],\"doi\":\"10.1109/BigMM.2018.8499098\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eaf2eaf8ffd056799ae6bbe44f5939eb562fd8d9\",\"title\":\"Reference Based on Adaptive Attention Mechanism for Image Captioning*\",\"url\":\"https://www.semanticscholar.org/paper/eaf2eaf8ffd056799ae6bbe44f5939eb562fd8d9\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47540106\",\"name\":\"J. Zhang\"},{\"authorId\":\"50828249\",\"name\":\"Q. Wang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1016/j.ipm.2019.102152\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ff7b42d8cc37acfc08210cff20983090a968308\",\"title\":\"Multi-Modal fusion with multi-level attention for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/2ff7b42d8cc37acfc08210cff20983090a968308\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1900967\",\"name\":\"U. Zia\"},{\"authorId\":\"145759322\",\"name\":\"M. M. Riaz\"},{\"authorId\":\"144683577\",\"name\":\"A. Ghafoor\"},{\"authorId\":\"145602758\",\"name\":\"Seyyed Salehi Seyyed Ali\"}],\"doi\":\"10.1007/s00521-019-04587-x\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"48473eb93b43e0130fce0d1eabb9b8fa468d8a41\",\"title\":\"Topic sensitive image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/48473eb93b43e0130fce0d1eabb9b8fa468d8a41\",\"venue\":\"Neural Computing and Applications\",\"year\":2019},{\"arxivId\":\"1906.01965\",\"authors\":[{\"authorId\":\"10775293\",\"name\":\"Yaoming Zhu\"},{\"authorId\":\"134231934\",\"name\":\"Juncheng Wan\"},{\"authorId\":\"145385776\",\"name\":\"Zhiming Zhou\"},{\"authorId\":\"47818593\",\"name\":\"Liheng Chen\"},{\"authorId\":\"144059570\",\"name\":\"Lin Qiu\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"145820291\",\"name\":\"Xin Jiang\"},{\"authorId\":\"92413680\",\"name\":\"Y. Yu\"}],\"doi\":\"10.1145/3331184.3331232\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6658615c2a9b3ade767e960a060ecda99e1c721c\",\"title\":\"Triple-to-Text: Converting RDF Triples into High-Quality Natural Languages via Optimizing an Inverse KL Divergence\",\"url\":\"https://www.semanticscholar.org/paper/6658615c2a9b3ade767e960a060ecda99e1c721c\",\"venue\":\"SIGIR\",\"year\":2019},{\"arxivId\":\"1812.08989\",\"authors\":[{\"authorId\":\"49718206\",\"name\":\"L. Zhou\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"49620738\",\"name\":\"Di Li\"},{\"authorId\":\"144154486\",\"name\":\"H. Shum\"}],\"doi\":\"10.1162/coli_a_00368\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66b7d31527f980bb2eecc23629f08ba6037facc4\",\"title\":\"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot\",\"url\":\"https://www.semanticscholar.org/paper/66b7d31527f980bb2eecc23629f08ba6037facc4\",\"venue\":\"Computational Linguistics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"Changyou Chen\"},{\"authorId\":\"49469303\",\"name\":\"Xin-yuan Zhang\"},{\"authorId\":\"1423652601\",\"name\":\"Ke Bai\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a32535fb09e35a0f8e24e4d4e9bd33222a5258e1\",\"title\":\"Semantic Matching for Sequence-to-Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/a32535fb09e35a0f8e24e4d4e9bd33222a5258e1\",\"venue\":\"EMNLP 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145014498\",\"name\":\"Q. Liu\"},{\"authorId\":\"50580380\",\"name\":\"Yingying Chen\"},{\"authorId\":\"1783122\",\"name\":\"J. Wang\"},{\"authorId\":\"27356041\",\"name\":\"Sijiong Zhang\"}],\"doi\":\"10.1007/978-981-10-8530-7_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b31a49cfbc04f0b5ee174db72f1dc001c42b156f\",\"title\":\"Joint Visual Context for Pedestrian Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b31a49cfbc04f0b5ee174db72f1dc001c42b156f\",\"venue\":\"ICIMCS\",\"year\":2017},{\"arxivId\":\"1908.10072\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"67074535\",\"name\":\"W. Zhang\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"46641690\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/ICCV.2019.00273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"title\":\"Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"48843025\",\"name\":\"Han Guo\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/W17-4504\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a19de85fa1533a1a1929b98b5fc3b1fb618dc668\",\"title\":\"Towards Improving Abstractive Summarization via Entailment Generation\",\"url\":\"https://www.semanticscholar.org/paper/a19de85fa1533a1a1929b98b5fc3b1fb618dc668\",\"venue\":\"NFiS@EMNLP\",\"year\":2017},{\"arxivId\":\"1906.01290\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"097981245eb3c66cc10a3164275d0bd52f5ae22a\",\"title\":\"Relational Reasoning using Prior Knowledge for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/097981245eb3c66cc10a3164275d0bd52f5ae22a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.01288\",\"authors\":[{\"authorId\":\"144407296\",\"name\":\"J. Gao\"},{\"authorId\":\"1960607091\",\"name\":\"Yi Zhou\"},{\"authorId\":\"2721708\",\"name\":\"P. Yu\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8405576136062ea661febeeedbc5076235d5eb42\",\"title\":\"Unsupervised Cross-lingual Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8405576136062ea661febeeedbc5076235d5eb42\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.10563\",\"authors\":[{\"authorId\":\"19324186\",\"name\":\"Pablo Messina\"},{\"authorId\":\"144882483\",\"name\":\"P. Pino\"},{\"authorId\":\"145831267\",\"name\":\"Denis Parra\"},{\"authorId\":\"144949755\",\"name\":\"\\u00c1. Soto\"},{\"authorId\":\"3766360\",\"name\":\"Cecilia Besa\"},{\"authorId\":\"145212929\",\"name\":\"S. Uribe\"},{\"authorId\":\"2595204\",\"name\":\"M. Andia\"},{\"authorId\":\"143767203\",\"name\":\"C. Tejos\"},{\"authorId\":\"2049035\",\"name\":\"C. Prieto\"},{\"authorId\":\"1410498401\",\"name\":\"Daniel Capurro\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6a6d36cbc3ac621a7abdb5bf36baacdb72af587c\",\"title\":\"A Survey on Deep Learning and Explainability for Automatic Image-based Medical Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/6a6d36cbc3ac621a7abdb5bf36baacdb72af587c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.04557\",\"authors\":[{\"authorId\":\"47798280\",\"name\":\"T. Ogura\"},{\"authorId\":\"31428186\",\"name\":\"A. Magassouba\"},{\"authorId\":\"2332462\",\"name\":\"K. Sugiura\"},{\"authorId\":\"134790239\",\"name\":\"Tsubasa Hirakawa\"},{\"authorId\":\"1687819\",\"name\":\"T. Yamashita\"},{\"authorId\":\"1687968\",\"name\":\"H. Fujiyoshi\"},{\"authorId\":\"153476449\",\"name\":\"H. Kawai\"}],\"doi\":\"10.1109/lra.2020.3010735\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8665698dd9c9093f03d3d672454ae4db33d381ef\",\"title\":\"Alleviating the Burden of Labeling: Sentence Generation by Attention Branch Encoder\\u2013Decoder Network\",\"url\":\"https://www.semanticscholar.org/paper/8665698dd9c9093f03d3d672454ae4db33d381ef\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/TIP.2020.3042086\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"title\":\"Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":\"2012.13137\",\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"39689790\",\"name\":\"L. White\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bd3c969b67497459d8f7ef28de5643310589b416\",\"title\":\"WEmbSim: A Simple yet Effective Metric for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bd3c969b67497459d8f7ef28de5643310589b416\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.07956\",\"authors\":[{\"authorId\":\"150341221\",\"name\":\"Wangchunshu Zhou\"},{\"authorId\":\"96694391\",\"name\":\"D. Lee\"},{\"authorId\":\"1838125907\",\"name\":\"Ravi Kiran Selvam\"},{\"authorId\":\"145213015\",\"name\":\"S. Lee\"},{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fffe61fbd225b59d084148611c49585860e24728\",\"title\":\"Pre-training Text-to-Text Transformers for Concept-centric Common Sense\",\"url\":\"https://www.semanticscholar.org/paper/fffe61fbd225b59d084148611c49585860e24728\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00200\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1664725279\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1520007550\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.161\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"title\":\"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"1410066063\",\"name\":\"Yuan Xie\"},{\"authorId\":\"152345893\",\"name\":\"F. Wu\"},{\"authorId\":\"33830793\",\"name\":\"J. Smith\"},{\"authorId\":\"40178769\",\"name\":\"Wenjin Lu\"},{\"authorId\":\"46824190\",\"name\":\"B. Zhang\"}],\"doi\":\"10.1016/j.sigpro.2019.107329\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea53299a067694a24e5e9cf8e852e122d5918847\",\"title\":\"Image captioning via hierarchical attention mechanism and policy gradient optimization\",\"url\":\"https://www.semanticscholar.org/paper/ea53299a067694a24e5e9cf8e852e122d5918847\",\"venue\":\"Signal Process.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52423203\",\"name\":\"L. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3985dbf7616c7d2c6178eaa262389815f95290e6\",\"title\":\"Cross-view learning\",\"url\":\"https://www.semanticscholar.org/paper/3985dbf7616c7d2c6178eaa262389815f95290e6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500399153\",\"name\":\"Burak Makav\"},{\"authorId\":\"3141085\",\"name\":\"V. K\\u0131l\\u0131\\u00e7\"}],\"doi\":\"10.23919/ELECO47770.2019.8990630\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"94b442f133b1c11ccb6fb22affb797818745c4b7\",\"title\":\"A New Image Captioning Approach for Visually Impaired People\",\"url\":\"https://www.semanticscholar.org/paper/94b442f133b1c11ccb6fb22affb797818745c4b7\",\"venue\":\"2019 11th International Conference on Electrical and Electronics Engineering (ELECO)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1608.02717\",\"authors\":[{\"authorId\":\"145478807\",\"name\":\"Mateusz Malinowski\"},{\"authorId\":\"117034854\",\"name\":\"Ashkan Mokarian\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"}],\"doi\":\"10.5244/C.30.111\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0c7c81571ff97881277bc37a218d885ec64beb1\",\"title\":\"Mean Box Pooling: A Rich Image Representation and Output Embedding for the Visual Madlibs Task\",\"url\":\"https://www.semanticscholar.org/paper/e0c7c81571ff97881277bc37a218d885ec64beb1\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36595248\",\"name\":\"Fang Fang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8275214\",\"name\":\"P. Tang\"}],\"doi\":\"10.1109/ICIP.2018.8451558\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c5f6ed9efc222fe2773135ffb4e5c567d98e64ea\",\"title\":\"Image Captioning with Word Level Attention\",\"url\":\"https://www.semanticscholar.org/paper/c5f6ed9efc222fe2773135ffb4e5c567d98e64ea\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152924571\",\"name\":\"Jing Wang\"},{\"authorId\":\"145113946\",\"name\":\"J. Tang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3394171.3413753\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"021d50ba5ae1c66e9175428f546976798126dd9f\",\"title\":\"Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/021d50ba5ae1c66e9175428f546976798126dd9f\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2010.05953\",\"authors\":[{\"authorId\":\"2012510\",\"name\":\"Jena D. Hwang\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"39227408\",\"name\":\"Ronan Le Bras\"},{\"authorId\":\"1380616323\",\"name\":\"Jeff Da\"},{\"authorId\":\"2325708\",\"name\":\"Keisuke Sakaguchi\"},{\"authorId\":\"2691021\",\"name\":\"Antoine Bosselut\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e39503e01ebb108c6773948a24ca798cd444eb62\",\"title\":\"COMET-ATOMIC 2020: On Symbolic and Neural Commonsense Knowledge Graphs\",\"url\":\"https://www.semanticscholar.org/paper/e39503e01ebb108c6773948a24ca798cd444eb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.10695\",\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f771b7514664d2b5e4f7dc12400897db95b0e136\",\"title\":\"Multimodal Transformer with Pointer Network for the DSTC8 AVSD Challenge\",\"url\":\"https://www.semanticscholar.org/paper/f771b7514664d2b5e4f7dc12400897db95b0e136\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.01067\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"title\":\"Video Captioning Using Weak Annotation\",\"url\":\"https://www.semanticscholar.org/paper/aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1901.00097\",\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"2031845\",\"name\":\"Junbo Guo\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"title\":\"Not All Words Are Equal: Video-specific Information Loss for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"50025928\",\"name\":\"Yuqian Li\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"}],\"doi\":\"10.1007/s11063-019-09997-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"65471c0f2a8ee941363ee009e81d450a1b0bbbf9\",\"title\":\"Hierarchical Deep Neural Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/65471c0f2a8ee941363ee009e81d450a1b0bbbf9\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1820795149\",\"name\":\"Harshit Parikh\"},{\"authorId\":\"1820796631\",\"name\":\"Harsh Sawant\"},{\"authorId\":\"1820795100\",\"name\":\"Bhautik Parmar\"},{\"authorId\":\"50206511\",\"name\":\"Rahul Shah\"},{\"authorId\":\"29759011\",\"name\":\"Santosh V. Chapaneri\"},{\"authorId\":\"2123529\",\"name\":\"D. Jayaswal\"}],\"doi\":\"10.1109/CSCITA47329.2020.9137802\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f10723f4bed371af3c23b060fa73bab124fc495c\",\"title\":\"Encoder-Decoder Architecture for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/f10723f4bed371af3c23b060fa73bab124fc495c\",\"venue\":\"2020 3rd International Conference on Communication System, Computing and IT Applications (CSCITA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40564040\",\"name\":\"J. Wu\"},{\"authorId\":\"37403439\",\"name\":\"H. Hu\"},{\"authorId\":\"40468514\",\"name\":\"Liang Yang\"}],\"doi\":\"10.1145/3336495\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"715fe60a8615d283c70d12db13857d8948baebca\",\"title\":\"Pseudo-3D Attention Transfer Network with Content-aware Strategy for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/715fe60a8615d283c70d12db13857d8948baebca\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143729188\",\"name\":\"Jun Song\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"145974111\",\"name\":\"Jun Xiao\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"}],\"doi\":\"10.1007/s41095-016-0059-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c491fc710a671dcb55266264627fe9e155f703f1\",\"title\":\"LSTM-in-LSTM for generating long descriptions of images\",\"url\":\"https://www.semanticscholar.org/paper/c491fc710a671dcb55266264627fe9e155f703f1\",\"venue\":\"Computational Visual Media\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"25080314\",\"name\":\"Teresa Botschen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"478193a30c5e1daf01fb2f380852040fd6f49081\",\"title\":\"Uni- and Multimodal and Structured Representations for Modeling Frame Semantics\",\"url\":\"https://www.semanticscholar.org/paper/478193a30c5e1daf01fb2f380852040fd6f49081\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.02980\",\"authors\":[{\"authorId\":\"2896521\",\"name\":\"Ajoy Mondal\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1109/ICDAR.2019.00210\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed0fdc574d304ea7cb890de445e3537569a5e1dc\",\"title\":\"Textual Description for Mathematical Equations\",\"url\":\"https://www.semanticscholar.org/paper/ed0fdc574d304ea7cb890de445e3537569a5e1dc\",\"venue\":\"2019 International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"title\":\"Image Input OR Video Hierarchical LSTMs with Adaptive Attention ( hLSTMat ) Feature Extraction Generated Captions Losses\",\"url\":\"https://www.semanticscholar.org/paper/e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1718590\",\"name\":\"R. Gaizauskas\"}],\"doi\":\"10.18653/v1/W15-4722\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"39a89dc33082ec5275dee3431802980eeeb47002\",\"title\":\"Generating Image Descriptions with Gold Standard Visual Inputs: Motivation, Evaluation and Baselines\",\"url\":\"https://www.semanticscholar.org/paper/39a89dc33082ec5275dee3431802980eeeb47002\",\"venue\":\"ENLG\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/LRA.2018.2793345\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"title\":\"Full-GRU Natural Language Video Description for Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49386928\",\"name\":\"Y. Xue\"},{\"authorId\":\"143713756\",\"name\":\"Xiaolei Huang\"}],\"doi\":\"10.1007/978-3-030-20351-1_10\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7f364afcbc3ae1cd6ea6546205aaa90c13cfd553\",\"title\":\"Improved Disease Classification in Chest X-Rays with Transferred Features from Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/7f364afcbc3ae1cd6ea6546205aaa90c13cfd553\",\"venue\":\"IPMI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40387200\",\"name\":\"S. Yagcioglu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"}],\"doi\":\"10.3115/v1/P15-2018\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"12aee52ad6c2b15d4006611651400baa9d01dee9\",\"title\":\"A Distributed Representation Based Query Expansion Approach for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/12aee52ad6c2b15d4006611651400baa9d01dee9\",\"venue\":\"ACL\",\"year\":2015},{\"arxivId\":\"1905.01919\",\"authors\":[{\"authorId\":\"36053954\",\"name\":\"Philipp Harzig\"},{\"authorId\":\"2063137\",\"name\":\"D. Zecha\"},{\"authorId\":\"144739319\",\"name\":\"R. Lienhart\"},{\"authorId\":\"40543160\",\"name\":\"C. Kaiser\"},{\"authorId\":\"26895278\",\"name\":\"Ren\\u00e9 Schallner\"}],\"doi\":\"10.1109/MIPR.2019.00085\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dbe83712e8e363f507aa48cf4fa8167a792ea309\",\"title\":\"Image Captioning with Clause-Focused Metrics in a Multi-modal Setting for Marketing\",\"url\":\"https://www.semanticscholar.org/paper/dbe83712e8e363f507aa48cf4fa8167a792ea309\",\"venue\":\"2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47761679\",\"name\":\"Eldan Cohen\"},{\"authorId\":\"144778460\",\"name\":\"J. Beck\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fbc49f54cc1a648e6d31f927120983225314d8b2\",\"title\":\"Empirical Analysis of Beam Search Performance Degradation in Neural Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/fbc49f54cc1a648e6d31f927120983225314d8b2\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3212585\",\"name\":\"Huda Almuzaini\"},{\"authorId\":\"1406444047\",\"name\":\"T. N. Al-Yahya\"},{\"authorId\":\"90527313\",\"name\":\"Hafida Benhidour\"}],\"doi\":\"10.14569/IJACSA.2018.090610\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1e0a5ce5204f3f7503c39df6d200627cc331efe2\",\"title\":\"Automatic Arabic Image Captioning using RNN-LSTM-Based Language Model and CNN\",\"url\":\"https://www.semanticscholar.org/paper/1e0a5ce5204f3f7503c39df6d200627cc331efe2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1707.07102\",\"authors\":[{\"authorId\":\"8135633\",\"name\":\"Xuwang Yin\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":\"10.18653/v1/D17-1017\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2cc6ba3dfd6bf1f6257b2e4651f4cae355284286\",\"title\":\"Obj2Text: Generating Visually Descriptive Language from Object Layouts\",\"url\":\"https://www.semanticscholar.org/paper/2cc6ba3dfd6bf1f6257b2e4651f4cae355284286\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47824598\",\"name\":\"W. Wang\"},{\"authorId\":\"47814961\",\"name\":\"Y. Ding\"},{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"}],\"doi\":\"10.1109/ICASSP.2018.8461507\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"54969341ec539ddaaf7537b7353e3cea84790eac\",\"title\":\"A Novel Semantic Attribute-Based Feature for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/54969341ec539ddaaf7537b7353e3cea84790eac\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":\"1805.03668\",\"authors\":[{\"authorId\":\"3444092\",\"name\":\"Lianhui Qin\"},{\"authorId\":\"2978364\",\"name\":\"L. Liu\"},{\"authorId\":\"51178911\",\"name\":\"Victoria Bi\"},{\"authorId\":\"47906413\",\"name\":\"Y. Wang\"},{\"authorId\":\"3028405\",\"name\":\"Xiaojiang Liu\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"36225434\",\"name\":\"Zhao Hai\"},{\"authorId\":\"34720053\",\"name\":\"Shuming Shi\"}],\"doi\":\"10.18653/v1/P18-2025\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ebbb74559e388b0d4a77e5f51eafdb376fe7797b\",\"title\":\"Automatic Article Commenting: the Task and Dataset\",\"url\":\"https://www.semanticscholar.org/paper/ebbb74559e388b0d4a77e5f51eafdb376fe7797b\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1715610\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"3775903\",\"name\":\"J. Wang\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"55a7286f014cc6b51a3f50b1e6bc8acc8166f231\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/55a7286f014cc6b51a3f50b1e6bc8acc8166f231\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1808.10122\",\"authors\":[{\"authorId\":\"2844243\",\"name\":\"Sam Wiseman\"},{\"authorId\":\"1692491\",\"name\":\"S. Shieber\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":\"10.18653/v1/D18-1356\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"912a9c5a32d50fafd5ddb34b5616d22f87c7d637\",\"title\":\"Learning Neural Templates for Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/912a9c5a32d50fafd5ddb34b5616d22f87c7d637\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1908.05067\",\"authors\":[{\"authorId\":\"47999368\",\"name\":\"Yi-Ting Yeh\"},{\"authorId\":\"145514809\",\"name\":\"Tzu-Chuan Lin\"},{\"authorId\":\"152498628\",\"name\":\"Hsiao-Hua Cheng\"},{\"authorId\":\"152141374\",\"name\":\"Yu-Hsuan Deng\"},{\"authorId\":\"27629426\",\"name\":\"Shang-Yu Su\"},{\"authorId\":\"1725643\",\"name\":\"Yun-Nung (Vivian) Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"f2539fad727e1def50b87fc88d3a499b4fe1f393\",\"title\":\"Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling\",\"url\":\"https://www.semanticscholar.org/paper/f2539fad727e1def50b87fc88d3a499b4fe1f393\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1704.07489\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P17-1117\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9b08d3201af644a638e291755a5e51f6b17a51f3\",\"title\":\"Multi-Task Video Captioning with Video and Entailment Generation\",\"url\":\"https://www.semanticscholar.org/paper/9b08d3201af644a638e291755a5e51f6b17a51f3\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9139235\",\"name\":\"S. Singh\"}],\"doi\":\"10.18653/v1/P18-3005\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8fdfd4c5039cf7d70470a2a3ac52bfd229bcd4e2\",\"title\":\"Pushing the Limits of Radiology with Joint Modeling of Visual and Textual Information\",\"url\":\"https://www.semanticscholar.org/paper/8fdfd4c5039cf7d70470a2a3ac52bfd229bcd4e2\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48490580\",\"name\":\"J. Park\"},{\"authorId\":\"35409051\",\"name\":\"Chibon Song\"},{\"authorId\":\"47180565\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/ICIIBMS.2017.8279760\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"title\":\"A study of evaluation metrics and datasets for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"venue\":\"2017 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314993\",\"name\":\"Wei-ying Wang\"},{\"authorId\":\"1994473516\",\"name\":\"Jieting Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3394171.3413890\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b116c8cd44a34f440f260a890e0600b61d92c262\",\"title\":\"VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation\",\"url\":\"https://www.semanticscholar.org/paper/b116c8cd44a34f440f260a890e0600b61d92c262\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49785011\",\"name\":\"L. Yuan\"},{\"authorId\":\"1556928554\",\"name\":\"Violet Xiang\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"2836466\",\"name\":\"L. Smith\"}],\"doi\":\"10.1016/j.cognition.2020.104243\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ec60e7735a7206dd26ab6484301c082bbeba6a27\",\"title\":\"Learning the generative principles of a symbol system from limited examples\",\"url\":\"https://www.semanticscholar.org/paper/ec60e7735a7206dd26ab6484301c082bbeba6a27\",\"venue\":\"Cognition\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121104319\",\"name\":\"Rehab Alahmadi\"},{\"authorId\":\"1695172\",\"name\":\"C. H. Park\"},{\"authorId\":\"36266636\",\"name\":\"J. Hahn\"}],\"doi\":\"10.1117/12.2523174\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"602cb721618a24a4c48bd53bf47b5a0283e028c9\",\"title\":\"Sequence-to-sequence image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/602cb721618a24a4c48bd53bf47b5a0283e028c9\",\"venue\":\"International Conference on Machine Vision\",\"year\":2019},{\"arxivId\":\"1907.12905\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":\"10.1145/3343031.3351060\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"title\":\"Watch It Twice: Video Captioning with a Refocused Video Encoder\",\"url\":\"https://www.semanticscholar.org/paper/5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1805.07112\",\"authors\":[{\"authorId\":\"1828568\",\"name\":\"Chen Chen\"},{\"authorId\":\"39108991\",\"name\":\"Shuai Mu\"},{\"authorId\":\"1410650653\",\"name\":\"Wanpeng Xiao\"},{\"authorId\":\"1410066883\",\"name\":\"Zexiong Ye\"},{\"authorId\":\"1410052649\",\"name\":\"Liesi Wu\"},{\"authorId\":\"102396462\",\"name\":\"Fuming Ma\"},{\"authorId\":\"34974680\",\"name\":\"Q. Ju\"}],\"doi\":\"10.1609/aaai.v33i01.33018142\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0daa3a4118e00b9f63b2d014a16ff1bc3ca9ff7e\",\"title\":\"Improving Image Captioning with Conditional Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/0daa3a4118e00b9f63b2d014a16ff1bc3ca9ff7e\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2007.03338\",\"authors\":[{\"authorId\":\"40651988\",\"name\":\"Marzieh Heidari\"},{\"authorId\":\"2862462\",\"name\":\"Mehdi Ghatee\"},{\"authorId\":\"1780566\",\"name\":\"A. Nickabadi\"},{\"authorId\":\"1796299590\",\"name\":\"Arash Pourhasan Nezhad\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20b332af8399233b5fc33a7e7f98538bd4a82ab0\",\"title\":\"Diverse and Styled Image Captioning Using SVD-Based Mixture of Recurrent Experts\",\"url\":\"https://www.semanticscholar.org/paper/20b332af8399233b5fc33a7e7f98538bd4a82ab0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1707.06029\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"4945045\",\"name\":\"Yeonhwa Kim\"},{\"authorId\":\"143912065\",\"name\":\"Kyung Yoo\"},{\"authorId\":\"2135453\",\"name\":\"S. Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.648\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"title\":\"Supervising Neural Attention Models for Video Captioning by Human Gaze Data\",\"url\":\"https://www.semanticscholar.org/paper/1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1611.07810\",\"authors\":[{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":\"10.1109/CVPR.2017.778\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"title\":\"A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1607.05910\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1016/j.cviu.2017.05.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88c307c51594c6d802080a0780d0d654e2e2891f\",\"title\":\"Visual question answering: A survey of methods and datasets\",\"url\":\"https://www.semanticscholar.org/paper/88c307c51594c6d802080a0780d0d654e2e2891f\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1711.09151\",\"authors\":[{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"113001756\",\"name\":\"A. Deshpande\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":\"10.1109/CVPR.2018.00583\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fb5e3db385588f671b11cfc8bf18efb90ee7b19\",\"title\":\"Convolutional Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fb5e3db385588f671b11cfc8bf18efb90ee7b19\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1902.01370\",\"authors\":[{\"authorId\":\"3016273\",\"name\":\"Jiatao Gu\"},{\"authorId\":\"50384171\",\"name\":\"Qi Liu\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"}],\"doi\":\"10.1162/tacl_a_00292\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"132b07740db20df2c36d6f939d296a7e941feac7\",\"title\":\"Insertion-based Decoding with Automatically Inferred Generation Order\",\"url\":\"https://www.semanticscholar.org/paper/132b07740db20df2c36d6f939d296a7e941feac7\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2019},{\"arxivId\":\"1608.08716\",\"authors\":[{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1609/aimag.v37i1.2647\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"caf912b716905ccbf46d6d00d6a0b622834a7cd9\",\"title\":\"Measuring Machine Intelligence Through Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/caf912b716905ccbf46d6d00d6a0b622834a7cd9\",\"venue\":\"AI Mag.\",\"year\":2016},{\"arxivId\":\"1506.01144\",\"authors\":[{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2016.29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"title\":\"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?\",\"url\":\"https://www.semanticscholar.org/paper/00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1423621769\",\"name\":\"B. T. Nguyen\"},{\"authorId\":\"50259366\",\"name\":\"O. Prakash\"},{\"authorId\":\"1580282435\",\"name\":\"A. H. Vo\"}],\"doi\":\"10.1007/978-3-030-62324-1_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21a742ee840b4a063deee66028409f9cf7f3829d\",\"title\":\"Attention Mechanism for Fashion Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/21a742ee840b4a063deee66028409f9cf7f3829d\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30658665\",\"name\":\"Z. Li\"},{\"authorId\":\"108085542\",\"name\":\"Rui Wang\"},{\"authorId\":\"2849740\",\"name\":\"Kehai Chen\"},{\"authorId\":\"1583166440\",\"name\":\"Masso Utiyama\"},{\"authorId\":\"1698363\",\"name\":\"Eiichiro Sumita\"},{\"authorId\":\"3322871\",\"name\":\"Zhuosheng Zhang\"},{\"authorId\":\"47941144\",\"name\":\"Hai Zhao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69e4dabf1f140915878a365c1adb86ceb2362ab6\",\"title\":\"Data-dependent Gaussian Prior Objective for Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/69e4dabf1f140915878a365c1adb86ceb2362ab6\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1908.05739\",\"authors\":[{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"39227408\",\"name\":\"Ronan Le Bras\"},{\"authorId\":\"8805254\",\"name\":\"Chaitanya Malaviya\"},{\"authorId\":\"2325708\",\"name\":\"Keisuke Sakaguchi\"},{\"authorId\":\"14487640\",\"name\":\"Ari Holtzman\"},{\"authorId\":\"2516777\",\"name\":\"Hannah Rashkin\"},{\"authorId\":\"145612610\",\"name\":\"Doug Downey\"},{\"authorId\":\"3156075\",\"name\":\"S. Yih\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a550f576ff20b8cce98f3ddad0043d3783fbc9b4\",\"title\":\"Abductive Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/a550f576ff20b8cce98f3ddad0043d3783fbc9b4\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2002.11701\",\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":\"10.1145/3366423.3380137\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a842fe8c25348627764462a57f0cd43d8cef103b\",\"title\":\"CLARA: Clinical Report Auto-completion\",\"url\":\"https://www.semanticscholar.org/paper/a842fe8c25348627764462a57f0cd43d8cef103b\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":\"2001.05614\",\"authors\":[{\"authorId\":\"49178142\",\"name\":\"H. Chen\"},{\"authorId\":\"38376468\",\"name\":\"J. Li\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.3233/FAIA200204\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0c33980d7c011a8c657afb825220632e17b1568\",\"title\":\"Delving Deeper into the Decoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f0c33980d7c011a8c657afb825220632e17b1568\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":\"1906.02850\",\"authors\":[{\"authorId\":\"48240607\",\"name\":\"C. Chen\"},{\"authorId\":\"1940556\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"35910905\",\"name\":\"E. Koh\"},{\"authorId\":\"39618262\",\"name\":\"Sungchul Kim\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"145381969\",\"name\":\"T. Yu\"},{\"authorId\":\"3139133\",\"name\":\"Razvan C. Bunescu\"},{\"authorId\":\"47131074\",\"name\":\"Razvan Bunescu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"755dd3628b04adad423d2418f98c156123ebac2a\",\"title\":\"Figure Captioning with Reasoning and Sequence-Level Training\",\"url\":\"https://www.semanticscholar.org/paper/755dd3628b04adad423d2418f98c156123ebac2a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1805.11797\",\"authors\":[{\"authorId\":\"4527324\",\"name\":\"Xiaoliang Dai\"},{\"authorId\":\"1989015\",\"name\":\"Hongxu Yin\"},{\"authorId\":\"144874163\",\"name\":\"N. Jha\"}],\"doi\":\"10.1109/TC.2019.2954495\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b1cceaf79ce78836390a09718b56b778fa8ba4b\",\"title\":\"Grow and Prune Compact, Fast, and Accurate LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/4b1cceaf79ce78836390a09718b56b778fa8ba4b\",\"venue\":\"IEEE Transactions on Computers\",\"year\":2020},{\"arxivId\":\"1603.09016\",\"authors\":[{\"authorId\":\"143690259\",\"name\":\"K. Tran\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/CVPRW.2016.61\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18f1143c64e6557c933b206fb8b2a7bd1f389afd\",\"title\":\"Rich Image Captioning in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/18f1143c64e6557c933b206fb8b2a7bd1f389afd\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46576324\",\"name\":\"Kevin P. Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbbbd89435dbd21458dda069d00717125f757204\",\"title\":\"C L ] 1 6 A pr 2 01 8 Adversarial Ranking for Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/cbbbd89435dbd21458dda069d00717125f757204\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1906.00717\",\"authors\":[{\"authorId\":\"2903539\",\"name\":\"Junlong Gao\"},{\"authorId\":\"2802555\",\"name\":\"Xi Meng\"},{\"authorId\":null,\"name\":\"Shiqi Wang\"},{\"authorId\":\"50078954\",\"name\":\"Xia Li\"},{\"authorId\":\"1755176\",\"name\":\"Shanshe Wang\"},{\"authorId\":\"51130683\",\"name\":\"S. Ma\"},{\"authorId\":\"48385803\",\"name\":\"W. Gao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"36a3cfa6ed48988e23cf68b3abcbe097b29e594a\",\"title\":\"Masked Non-Autoregressive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/36a3cfa6ed48988e23cf68b3abcbe097b29e594a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72046972\",\"name\":\"Kataoka Hirokatsu\"},{\"authorId\":\"1393888854\",\"name\":\"Miyashita Yudai\"},{\"authorId\":\"1392031925\",\"name\":\"Yamabe Tomoaki\"},{\"authorId\":\"100877779\",\"name\":\"Shirakabe Soma\"},{\"authorId\":\"52601768\",\"name\":\"Sato Shinichi\"},{\"authorId\":\"1392572194\",\"name\":\"Hoshino Hironori\"},{\"authorId\":\"1392011341\",\"name\":\"Kato Ryo\"},{\"authorId\":\"101524854\",\"name\":\"Abe Kaori\"},{\"authorId\":\"1390054231\",\"name\":\"Imanari Takaaki\"},{\"authorId\":\"1394469677\",\"name\":\"Kobayashi Naomichi\"},{\"authorId\":\"100667175\",\"name\":\"Morita Shinichiro\"},{\"authorId\":\"66767948\",\"name\":\"Nakamura Akio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b35a6e738aadd4a974db2d0815879e37e5d7d66f\",\"title\":\"cvpaper.challenge in CVPR2015 -- A review of CVPR2015\",\"url\":\"https://www.semanticscholar.org/paper/b35a6e738aadd4a974db2d0815879e37e5d7d66f\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52327816\",\"name\":\"Jinkyu Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d462a10299ce908574b1a9bbe37d7369bf2b445\",\"title\":\"Explainable and Advisable Learning for Self-driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/9d462a10299ce908574b1a9bbe37d7369bf2b445\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314360\",\"name\":\"Weixuan Wang\"},{\"authorId\":\"49865085\",\"name\":\"Zhihong Chen\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"}],\"doi\":\"10.1007/978-3-030-20876-9_37\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf3f2f2c1b88eb7b031d593c9d83641baea364e2\",\"title\":\"Multivariate Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf3f2f2c1b88eb7b031d593c9d83641baea364e2\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"47295036\",\"name\":\"Zehua Zhang\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":\"10.1016/j.jvcir.2018.05.008\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"26eff1aa014ed0f19fbda0ac6b554f8ba9881f25\",\"title\":\"Deepdiary: Lifelogging image captioning and summarization\",\"url\":\"https://www.semanticscholar.org/paper/26eff1aa014ed0f19fbda0ac6b554f8ba9881f25\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":\"1612.00563\",\"authors\":[{\"authorId\":\"2071376\",\"name\":\"Steven J. Rennie\"},{\"authorId\":\"2293163\",\"name\":\"E. Marcheret\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"39320489\",\"name\":\"J. Ross\"},{\"authorId\":\"1782589\",\"name\":\"V. Goel\"}],\"doi\":\"10.1109/CVPR.2017.131\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c8353697cdbb98dfba4f493875778c4286d3e3a\",\"title\":\"Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c8353697cdbb98dfba4f493875778c4286d3e3a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1611.05594\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1406012647\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"104757141\",\"name\":\"Jian Shao\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/CVPR.2017.667\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88513e738a95840de05a62f0e43d30a67b3c542e\",\"title\":\"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/88513e738a95840de05a62f0e43d30a67b3c542e\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1806.02453\",\"authors\":[{\"authorId\":\"28458352\",\"name\":\"S. Kim\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f27b833c4a0dcb809215b185e8e2601aef6e7fb8\",\"title\":\"Visual Reasoning by Progressive Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/f27b833c4a0dcb809215b185e8e2601aef6e7fb8\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1805.09039\",\"authors\":[{\"authorId\":\"1690722\",\"name\":\"S. Chatzis\"},{\"authorId\":\"15699963\",\"name\":\"Aristotelis Charalampous\"},{\"authorId\":\"46192035\",\"name\":\"Kyriacos Tolias\"},{\"authorId\":\"24281427\",\"name\":\"Sotiris A. Vassou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"293f528b107b3fa74de65826f11245c27355f82a\",\"title\":\"Amortized Context Vector Inference for Sequence-to-Sequence Networks\",\"url\":\"https://www.semanticscholar.org/paper/293f528b107b3fa74de65826f11245c27355f82a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2011.14244\",\"authors\":[{\"authorId\":\"46956602\",\"name\":\"Yao Fu\"},{\"authorId\":\"144120972\",\"name\":\"Chuanqi Tan\"},{\"authorId\":\"1986593\",\"name\":\"B. Bi\"},{\"authorId\":\"2873317\",\"name\":\"M. Chen\"},{\"authorId\":\"1717629\",\"name\":\"Yansong Feng\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2841c420bad3c1e41939b4492fed717f107e286c\",\"title\":\"Latent Template Induction with Gumbel-CRFs\",\"url\":\"https://www.semanticscholar.org/paper/2841c420bad3c1e41939b4492fed717f107e286c\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1807.11546\",\"authors\":[{\"authorId\":\"2569534\",\"name\":\"J. Kim\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":\"10.1007/978-3-030-01216-8_35\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d0907770cd4619aa6a36139a859e8f09bc9f0ef\",\"title\":\"Textual Explanations for Self-Driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/9d0907770cd4619aa6a36139a859e8f09bc9f0ef\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2006.08339\",\"authors\":[{\"authorId\":\"1995250\",\"name\":\"Zhongliang Yang\"},{\"authorId\":\"1750892669\",\"name\":\"Baitao Gong\"},{\"authorId\":\"3334881\",\"name\":\"Y. Li\"},{\"authorId\":\"20751938\",\"name\":\"J. Yang\"},{\"authorId\":\"2943649\",\"name\":\"Zhiwen Hu\"},{\"authorId\":\"49866786\",\"name\":\"Yongfeng Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1adb480750132759d8cab4f15a8a7dc7a27f96d4\",\"title\":\"Graph-Stega: Semantic Controllable Steganographic Text Generation Guided by Knowledge Graph\",\"url\":\"https://www.semanticscholar.org/paper/1adb480750132759d8cab4f15a8a7dc7a27f96d4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26903445\",\"name\":\"H. Mantec\\u00f3n\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"},{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"71caf2f74e94cbb1c7fb3250e083a4d6924fb30e\",\"title\":\"PicSOM and EURECOM Experiments in TRECVID 2019 Pre-workshop draft \\u2013 Revision : 0 . 9\",\"url\":\"https://www.semanticscholar.org/paper/71caf2f74e94cbb1c7fb3250e083a4d6924fb30e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1609/AAAI.V34I07.6731\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4df184d6a74f1ffd84b644735c9afb5060552770\",\"title\":\"Joint Commonsense and Relation Reasoning for Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4df184d6a74f1ffd84b644735c9afb5060552770\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1806.08251\",\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f27df2d4fb7dd7ed5587640dcbe4dc1eb37acfb\",\"title\":\"Unseen Action Recognition with Multimodal Learning\",\"url\":\"https://www.semanticscholar.org/paper/8f27df2d4fb7dd7ed5587640dcbe4dc1eb37acfb\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48707795\",\"name\":\"Ziwei Wang\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"}],\"doi\":\"10.24963/ijcai.2020/92\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"506e7acfb05c851a7b3ccb256f4fed6d5a7dc15c\",\"title\":\"Human Consensus-Oriented Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/506e7acfb05c851a7b3ccb256f4fed6d5a7dc15c\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2004.09272\",\"authors\":[{\"authorId\":\"3469119\",\"name\":\"Daniela Massiceti\"},{\"authorId\":\"3468926\",\"name\":\"Viveka Kulharia\"},{\"authorId\":\"144679302\",\"name\":\"P. Dokania\"},{\"authorId\":\"40155668\",\"name\":\"N. Siddharth\"},{\"authorId\":\"143635540\",\"name\":\"P. Torr\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6ddbe134ed10abaf043d8bcb11343e5c7e7358bf\",\"title\":\"A Revised Generative Evaluation of Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/6ddbe134ed10abaf043d8bcb11343e5c7e7358bf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144526707\",\"name\":\"I. Soboroff\"},{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"1926279\",\"name\":\"A. Butt\"},{\"authorId\":\"144175619\",\"name\":\"K. Curtis\"}],\"doi\":\"10.3389/frai.2020.00032\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d58ad337938eda7df85819aead72cece2f3c3a55\",\"title\":\"Evaluating Multimedia and Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d58ad337938eda7df85819aead72cece2f3c3a55\",\"venue\":\"Frontiers in Artificial Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77537913\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/s11042-019-07948-9\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fea09a15a91558db9090c4cb4b11184b91310839\",\"title\":\"Deep learning ensemble with data augmentation using a transcoder in visual description\",\"url\":\"https://www.semanticscholar.org/paper/fea09a15a91558db9090c4cb4b11184b91310839\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3238568\",\"name\":\"M. Liu\"},{\"authorId\":\"1485768948\",\"name\":\"Lingjun Li\"},{\"authorId\":\"146896370\",\"name\":\"H. Hu\"},{\"authorId\":\"15880069\",\"name\":\"W. Guan\"},{\"authorId\":\"153307124\",\"name\":\"J. Tian\"}],\"doi\":\"10.1016/j.ipm.2019.102178\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4ba3a265763677dfb68567aa6b62cd5fe4a633b\",\"title\":\"Image caption generation with dual attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/e4ba3a265763677dfb68567aa6b62cd5fe4a633b\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"1904.01121\",\"authors\":[{\"authorId\":\"3396987\",\"name\":\"Sharon Zhou\"},{\"authorId\":\"39504881\",\"name\":\"M. Gordon\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"89315058\",\"name\":\"Austin Narcomey\"},{\"authorId\":\"3312398\",\"name\":\"Durim Morina\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"717d85bd48aca5d950a4da4bc63b647a57233075\",\"title\":\"HYPE: Human eYe Perceptual Evaluation of Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/717d85bd48aca5d950a4da4bc63b647a57233075\",\"venue\":\"DGS@ICLR\",\"year\":2019},{\"arxivId\":\"2012.00366\",\"authors\":[{\"authorId\":\"9610143\",\"name\":\"Zhihao Fan\"},{\"authorId\":\"2171182\",\"name\":\"Yeyun Gong\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"50695377\",\"name\":\"Siyuan Wang\"},{\"authorId\":\"121240634\",\"name\":\"Yameng Huang\"},{\"authorId\":\"49097406\",\"name\":\"Jian Jiao\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"2027657172\",\"name\":\"Ruofei Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"023b508aca0e776f6face93548fedb8921cf35ab\",\"title\":\"An Enhanced Knowledge Injection Model for Commonsense Generation\",\"url\":\"https://www.semanticscholar.org/paper/023b508aca0e776f6face93548fedb8921cf35ab\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48380454\",\"name\":\"Yuhao Zhang\"},{\"authorId\":\"51008479\",\"name\":\"Hang Jiang\"},{\"authorId\":\"2965600\",\"name\":\"Y. Miura\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"2356307\",\"name\":\"C. Langlotz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6dd9f99cecd38504b667d320eb2a6267a9fee35d\",\"title\":\"FROM P AIRED I MAGES AND T EXT\",\"url\":\"https://www.semanticscholar.org/paper/6dd9f99cecd38504b667d320eb2a6267a9fee35d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50811366\",\"name\":\"Haojie Pan\"},{\"authorId\":null,\"name\":\"Junpei Zhou\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"49420788\",\"name\":\"Y. Liu\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"144346837\",\"name\":\"Min Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b0846692857eee0a1029adb5782c1136ae3f1c9\",\"title\":\"C L ] 1 N ov 2 01 8 Dial 2 Desc : End-to-end Dialogue Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/9b0846692857eee0a1029adb5782c1136ae3f1c9\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2004.12726\",\"authors\":[{\"authorId\":\"10324691\",\"name\":\"Kawin Ethayarajh\"},{\"authorId\":\"1779671\",\"name\":\"D. Sadigh\"}],\"doi\":\"10.18653/v1/2020.eval4nlp-1.5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"947ef93df0f836cdc6493c08582babaf0edb5f4a\",\"title\":\"BLEU Neighbors: A Reference-less Approach to Automatic Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/947ef93df0f836cdc6493c08582babaf0edb5f4a\",\"venue\":\"EVAL4NLP\",\"year\":2020},{\"arxivId\":\"1611.05321\",\"authors\":[{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"40401747\",\"name\":\"A. Lucchi\"},{\"authorId\":\"143936663\",\"name\":\"Thomas Hofmann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cc23e554d98522b377d227dc78e9382a0ed35e5\",\"title\":\"Bootstrap, Review, Decode: Using Out-of-Domain Textual Data to Improve Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8cc23e554d98522b377d227dc78e9382a0ed35e5\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6806161\",\"name\":\"A. Khamparia\"},{\"authorId\":\"48413825\",\"name\":\"B. Pandey\"},{\"authorId\":\"74600005\",\"name\":\"Shrasti Tiwari\"},{\"authorId\":\"144786228\",\"name\":\"D. Gupta\"},{\"authorId\":\"103319292\",\"name\":\"A. Khanna\"},{\"authorId\":\"144091143\",\"name\":\"J. Rodrigues\"}],\"doi\":\"10.1007/s00034-019-01306-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc8e5e775da93348382f198fa22f3cda7f04c71f\",\"title\":\"An Integrated Hybrid CNN\\u2013RNN Model for Visual Description and Generation of Captions\",\"url\":\"https://www.semanticscholar.org/paper/dc8e5e775da93348382f198fa22f3cda7f04c71f\",\"venue\":\"Circuits Syst. Signal Process.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478695\",\"name\":\"Teng Jiang\"},{\"authorId\":\"1716595\",\"name\":\"Chengjun Zhan\"},{\"authorId\":\"7607492\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1117/1.JEI.28.2.023022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3e21b716ee953230b0261d05db9664cf50e6f906\",\"title\":\"Long short-term memory network with external memories for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/3e21b716ee953230b0261d05db9664cf50e6f906\",\"venue\":\"J. Electronic Imaging\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/3240508.3240677\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5abe63d687f927a0ac61e9ad62f88b355d89caf\",\"title\":\"Spotting and Aggregating Salient Regions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e5abe63d687f927a0ac61e9ad62f88b355d89caf\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1906.03922\",\"authors\":[{\"authorId\":\"20322606\",\"name\":\"H. Lee\"},{\"authorId\":\"49899378\",\"name\":\"S. T. Kim\"},{\"authorId\":\"7251290\",\"name\":\"Yong Man Ro\"}],\"doi\":\"10.1007/978-3-030-33850-3_3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e55decedadcc65b1e3405a5cd66e1f7a790da55b\",\"title\":\"Generation of Multimodal Justification Using Visual Word Constraint Model for Explainable Computer-Aided Diagnosis\",\"url\":\"https://www.semanticscholar.org/paper/e55decedadcc65b1e3405a5cd66e1f7a790da55b\",\"venue\":\"iMIMIC/ML-CDS@MICCAI\",\"year\":2019},{\"arxivId\":\"1906.00283\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"122175026\",\"name\":\"P\\u00e9ter Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"title\":\"Learning to Generate Grounded Image Captions without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"92326466\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/cvpr42600.2020.01059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"title\":\"Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144063214\",\"name\":\"M. Z. Hossain\"},{\"authorId\":\"19324907\",\"name\":\"F. Sohel\"},{\"authorId\":\"96363850\",\"name\":\"Mohd Fairuz Shiratuddin\"},{\"authorId\":\"48540238\",\"name\":\"Hamid Laga\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"}],\"doi\":\"10.1109/DICTA47822.2019.8946003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"098833985221f9f30d547dadf24ae7b0f1433ef5\",\"title\":\"Bi-SAN-CAP: Bi-Directional Self-Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/098833985221f9f30d547dadf24ae7b0f1433ef5\",\"venue\":\"2019 Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2019},{\"arxivId\":\"1502.05678\",\"authors\":[{\"authorId\":\"1387131411\",\"name\":\"Clint Solomon Mathialagan\"},{\"authorId\":\"152854313\",\"name\":\"Andrew C. Gallagher\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPR.2015.7299119\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"858cf8c9a652d61177729f7e97ff96897de256c5\",\"title\":\"VIP: Finding important people in images\",\"url\":\"https://www.semanticscholar.org/paper/858cf8c9a652d61177729f7e97ff96897de256c5\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49672730\",\"name\":\"Chunyuan Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e3c211c6e8dbea9849790a1c9491aed290a1e144\",\"title\":\"Towards Better Representations with Deep/Bayesian Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3c211c6e8dbea9849790a1c9491aed290a1e144\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742688\",\"name\":\"H. Escalante\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"51243976\",\"name\":\"Isabelle Guyon\"},{\"authorId\":\"46176857\",\"name\":\"X. Bar\\u00f3\"},{\"authorId\":\"2706315\",\"name\":\"Ya\\u011fmur G\\u00fc\\u00e7l\\u00fct\\u00fcrk\"},{\"authorId\":\"80777440\",\"name\":\"U. G\\u00fc\\u00e7l\\u00fc\"},{\"authorId\":\"103366015\",\"name\":\"M. V. Gerven\"}],\"doi\":\"10.1007/978-3-319-98131-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a217389b365d06ae323fee744304067c8f62be7\",\"title\":\"Explainable and Interpretable Models in Computer Vision and Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/8a217389b365d06ae323fee744304067c8f62be7\",\"venue\":\"The Springer Series on Challenges in Machine Learning\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"1680068\",\"name\":\"X. Wang\"}],\"doi\":\"10.1145/3126686.3126714\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b5bfe824fc49fe78b538ac15f21c4cd6a9d44347\",\"title\":\"Image Caption with Synchronous Cross-Attention\",\"url\":\"https://www.semanticscholar.org/paper/b5bfe824fc49fe78b538ac15f21c4cd6a9d44347\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2012.04925\",\"authors\":[{\"authorId\":\"30894109\",\"name\":\"A. Chen\"},{\"authorId\":\"49444758\",\"name\":\"Xinyi Huang\"},{\"authorId\":\"15385188\",\"name\":\"Hailan Lin\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f4079b8d38e085386609b82b6bf8ab43d1fab13e\",\"title\":\"Towards Annotation-Free Evaluation of Cross-Lingual Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f4079b8d38e085386609b82b6bf8ab43d1fab13e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.06946\",\"authors\":[{\"authorId\":\"46583603\",\"name\":\"J. Wang\"},{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41171e9024d0082c2a57f4887bac93131669b881\",\"title\":\"MiniVLM: A Smaller and Faster Vision-Language Model\",\"url\":\"https://www.semanticscholar.org/paper/41171e9024d0082c2a57f4887bac93131669b881\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2388012\",\"name\":\"S. Dash\"},{\"authorId\":\"9528018\",\"name\":\"Shantanu Acharya\"},{\"authorId\":\"1882574\",\"name\":\"Partha Pakray\"},{\"authorId\":\"1455172724\",\"name\":\"Ranjita Das\"},{\"authorId\":\"1747784\",\"name\":\"Alexander Gelbukh\"}],\"doi\":\"10.1007/s13369-019-04262-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8ee9c1907aeb58556b74471b876f9c19a76f86f\",\"title\":\"Topic-Based Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/f8ee9c1907aeb58556b74471b876f9c19a76f86f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":\"10.1007/978-3-030-31756-0_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"title\":\"The Encoder-Decoder Framework and Its Applications\",\"url\":\"https://www.semanticscholar.org/paper/0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1807.03658\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"}],\"doi\":\"10.1016/j.neucom.2020.08.035\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"title\":\"Video Captioning with Boundary-aware Hierarchical Language Decoding and Joint Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40895152\",\"name\":\"Henry Elder\"},{\"authorId\":\"1432828435\",\"name\":\"A. O'connor\"},{\"authorId\":\"144116340\",\"name\":\"Jennifer Foster\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.230\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"92faf859293883e7a27240ed3b8263e9caa16d90\",\"title\":\"How to Make Neural Natural Language Generation as Reliable as Templates in Task-Oriented Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/92faf859293883e7a27240ed3b8263e9caa16d90\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32763968\",\"name\":\"A. Fisch\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"144797264\",\"name\":\"J. Clark\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.705\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b01bc15919f72929d36fc0443395e97b632c81b8\",\"title\":\"CapWAP: Image Captioning with a Purpose\",\"url\":\"https://www.semanticscholar.org/paper/b01bc15919f72929d36fc0443395e97b632c81b8\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2008.12009\",\"authors\":[{\"authorId\":\"145338991\",\"name\":\"Ananya B. Sai\"},{\"authorId\":\"1389549528\",\"name\":\"Akash Kumar Mohankumar\"},{\"authorId\":\"2361078\",\"name\":\"Mitesh M. Khapra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"de7f66cccc202dc563d9dcd0887fb37481d61b3d\",\"title\":\"A Survey of Evaluation Metrics Used for NLG Systems\",\"url\":\"https://www.semanticscholar.org/paper/de7f66cccc202dc563d9dcd0887fb37481d61b3d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.09791\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-58589-1_22\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"title\":\"Identity-Aware Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1909.08745\",\"authors\":[{\"authorId\":\"144060080\",\"name\":\"Giang T. Nguyen\"},{\"authorId\":\"8102722\",\"name\":\"T. Jun\"},{\"authorId\":\"144256269\",\"name\":\"Trung Tran\"},{\"authorId\":\"72657968\",\"name\":\"Daeyoung Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b428979e3b67f40fc307b5b166f9c97adba6a63f\",\"title\":\"ContCap: A comprehensive framework for continual image captioning\",\"url\":\"https://www.semanticscholar.org/paper/b428979e3b67f40fc307b5b166f9c97adba6a63f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1611.09053\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1109/CVPR.2017.147\",\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"533d14e539ae5cdca0ece392487a2b19106d468a\",\"title\":\"Bidirectional Multirate Reconstruction for Temporal Modeling in Videos\",\"url\":\"https://www.semanticscholar.org/paper/533d14e539ae5cdca0ece392487a2b19106d468a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46174228\",\"name\":\"Sidra Shabir\"},{\"authorId\":\"49854677\",\"name\":\"Syed Yasser Arafat\"}],\"doi\":\"10.1109/ICPESG.2018.8384519\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4bc176200ff718ea0a0650145d3d263b0b7dfb39\",\"title\":\"An image conveys a message: A brief survey on image description generation\",\"url\":\"https://www.semanticscholar.org/paper/4bc176200ff718ea0a0650145d3d263b0b7dfb39\",\"venue\":\"2018 1st International Conference on Power, Energy and Smart Grid (ICPESG)\",\"year\":2018},{\"arxivId\":\"1909.03169\",\"authors\":[{\"authorId\":\"32095408\",\"name\":\"Fawaz Sammani\"},{\"authorId\":\"67001969\",\"name\":\"Mahmoud Elsayed\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7af5c94fedc32f3104a08301a46c62f51b044a81\",\"title\":\"Look and Modify: Modification Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7af5c94fedc32f3104a08301a46c62f51b044a81\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1906.08094\",\"authors\":[{\"authorId\":\"148411041\",\"name\":\"Yusuke Shido\"},{\"authorId\":\"2675812\",\"name\":\"Y. Kobayashi\"},{\"authorId\":\"38696575\",\"name\":\"A. Yamamoto\"},{\"authorId\":\"50076811\",\"name\":\"A. Miyamoto\"},{\"authorId\":\"51020460\",\"name\":\"T. Matsumura\"}],\"doi\":\"10.1109/IJCNN.2019.8851751\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"08218ed6095e407eba3f02ce141994241d03a5b9\",\"title\":\"Automatic Source Code Summarization with Extended Tree-LSTM\",\"url\":\"https://www.semanticscholar.org/paper/08218ed6095e407eba3f02ce141994241d03a5b9\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"1911.09989\",\"authors\":[{\"authorId\":\"1429191721\",\"name\":\"Menatallh Hammad\"},{\"authorId\":\"1429191719\",\"name\":\"May Hammad\"},{\"authorId\":\"31358369\",\"name\":\"M. ElShenawy\"}],\"doi\":\"10.1007/978-3-030-59830-3_21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"title\":\"Characterizing the impact of using features extracted from pre-trained models on the quality of video captioning sequence-to-sequence models\",\"url\":\"https://www.semanticscholar.org/paper/86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"venue\":\"ICPRAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52150251\",\"name\":\"A. Goel\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"103192742\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3d807fed82f61bc48aea76f191d2e19e2fc38f5d\",\"title\":\"Learning to Caption Images with Two-Stream Attention and Sentence Auto-Encoder\",\"url\":\"https://www.semanticscholar.org/paper/3d807fed82f61bc48aea76f191d2e19e2fc38f5d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5979116\",\"name\":\"T. Alahmadi\"},{\"authorId\":\"144546484\",\"name\":\"S. Drew\"}],\"doi\":\"10.17411/JACCES.V8I2.167\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ace591f861620a146c0b7865b1057793d8ef0971\",\"title\":\"Evaluation of image accessibility for visually impaired users\",\"url\":\"https://www.semanticscholar.org/paper/ace591f861620a146c0b7865b1057793d8ef0971\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29068663\",\"name\":\"Ruize Wang\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"2193560\",\"name\":\"Piji Li\"},{\"authorId\":\"1471463062\",\"name\":\"Qi Zhang\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"}],\"doi\":\"10.1609/AAAI.V34I05.6455\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f676946d1585f802d21272e2b3899a53e7b8187\",\"title\":\"Storytelling from an Image Stream Using Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/1f676946d1585f802d21272e2b3899a53e7b8187\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31465302\",\"name\":\"E. Wang\"},{\"authorId\":\"46182609\",\"name\":\"X. Zhang\"},{\"authorId\":\"39907479\",\"name\":\"F. Wang\"},{\"authorId\":\"1682589\",\"name\":\"T. Wu\"},{\"authorId\":\"144404748\",\"name\":\"Chien-Ming Chen\"}],\"doi\":\"10.1109/ACCESS.2019.2917771\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"751e871c12f70bf05ea012b21f1434849d6fe5ce\",\"title\":\"Multilayer Dense Attention Model for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/751e871c12f70bf05ea012b21f1434849d6fe5ce\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48637710\",\"name\":\"Yongqing Zhu\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1145/3343031.3350932\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"title\":\"Attention-based Densely Connected LSTM for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1704.07434\",\"authors\":[{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1109/ICCV.2017.272\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e4a11cc697c94fdda9eedbac7d83a865646bd4cc\",\"title\":\"Paying Attention to Descriptions Generated by Image Captioning Models\",\"url\":\"https://www.semanticscholar.org/paper/e4a11cc697c94fdda9eedbac7d83a865646bd4cc\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3226409\",\"name\":\"Y. Cao\"},{\"authorId\":\"3040905\",\"name\":\"Q. Wang\"},{\"authorId\":\"5380819\",\"name\":\"K. Huang\"},{\"authorId\":\"80083020\",\"name\":\"Rui Zhang\"}],\"doi\":\"10.1007/978-3-030-39431-8_1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3ab374c50e061b84b7076181522b3684d2273337\",\"title\":\"Improving Image Caption Performance with Linguistic Context\",\"url\":\"https://www.semanticscholar.org/paper/3ab374c50e061b84b7076181522b3684d2273337\",\"venue\":\"BICS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37722032\",\"name\":\"Charese Smiley\"},{\"authorId\":\"3232032\",\"name\":\"E. Davoodi\"},{\"authorId\":\"34695819\",\"name\":\"Dezhao Song\"},{\"authorId\":\"145837551\",\"name\":\"Frank Schilder\"}],\"doi\":\"10.18653/v1/W18-6558\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"86474af967e593a815d9339d4a6c003f250a94a7\",\"title\":\"The E2E NLG Challenge: A Tale of Two Systems\",\"url\":\"https://www.semanticscholar.org/paper/86474af967e593a815d9339d4a6c003f250a94a7\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1654172122\",\"name\":\"Shreyasi Charu\"},{\"authorId\":\"91170072\",\"name\":\"S. Mishra\"},{\"authorId\":\"34383888\",\"name\":\"T. Gandhi\"}],\"doi\":\"10.1109/AISP48273.2020.9073009\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"712ee361721ce88105f9705b178a1a94f5a3fca4\",\"title\":\"Vision to Language: Captioning Images using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/712ee361721ce88105f9705b178a1a94f5a3fca4\",\"venue\":\"2020 International Conference on Artificial Intelligence and Signal Processing (AISP)\",\"year\":2020},{\"arxivId\":\"2008.02693\",\"authors\":[{\"authorId\":\"8314407\",\"name\":\"X. Yang\"},{\"authorId\":\"49724488\",\"name\":\"H. Zhang\"},{\"authorId\":\"145657309\",\"name\":\"D. Jin\"},{\"authorId\":\"49421744\",\"name\":\"Yingru Liu\"},{\"authorId\":\"120931191\",\"name\":\"Chi-Hao Wu\"},{\"authorId\":\"34331333\",\"name\":\"Jianchao Tan\"},{\"authorId\":\"47300385\",\"name\":\"Dongliang Xie\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":null,\"name\":\"Xin Wang\"}],\"doi\":\"10.1007/978-3-030-58601-0_1\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bf72a4a75b6ecd6dbc4c1af52a9592ec61abcfb5\",\"title\":\"Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards\",\"url\":\"https://www.semanticscholar.org/paper/bf72a4a75b6ecd6dbc4c1af52a9592ec61abcfb5\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1685771\",\"name\":\"P. Blunsom\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"40586431\",\"name\":\"S. Cohen\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"2910877\",\"name\":\"K. Hermann\"},{\"authorId\":\"2554988\",\"name\":\"L. Rimell\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"},{\"authorId\":\"3156075\",\"name\":\"S. Yih\"}],\"doi\":\"10.18653/v1/w16-16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e351b9c3c0c056aebb9907441b9ffb35179b6ff5\",\"title\":\"Proceedings of the 1st Workshop on Representation Learning for NLP\",\"url\":\"https://www.semanticscholar.org/paper/e351b9c3c0c056aebb9907441b9ffb35179b6ff5\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1809.02156\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"40895688\",\"name\":\"Kaylee Burns\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D18-1437\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4921243268c81d0d6db99053a9d004852225a622\",\"title\":\"Object Hallucination in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4921243268c81d0d6db99053a9d004852225a622\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"120897486\",\"name\":\"Anwen Hu\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3394171.3413576\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6c0abdf9a9c47e8ec2606b06a5324ec7d2e7ebe7\",\"title\":\"ICECAP: Information Concentrated Entity-aware Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c0abdf9a9c47e8ec2606b06a5324ec7d2e7ebe7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yongzhuang Wang\"},{\"authorId\":\"49746133\",\"name\":\"Yangmei Shen\"},{\"authorId\":\"144045763\",\"name\":\"Hongkai Xiong\"},{\"authorId\":\"8131625\",\"name\":\"W. Lin\"}],\"doi\":\"10.1109/ICIP.2019.8803418\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"93368542c1774e2cbd12187a4ccc2c882c791d94\",\"title\":\"Adaptive Hard Example Mining for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93368542c1774e2cbd12187a4ccc2c882c791d94\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151017519\",\"name\":\"Gilad Vered\"},{\"authorId\":\"151504666\",\"name\":\"Gal Oren\"},{\"authorId\":\"34815079\",\"name\":\"Y. Atzmon\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"}],\"doi\":\"10.1109/ICCV.2019.00899\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6fb03dcb0d57eb74d3395e598ae1712e3e926ec2\",\"title\":\"Joint Optimization for Cooperative Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6fb03dcb0d57eb74d3395e598ae1712e3e926ec2\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2011.07680\",\"authors\":[{\"authorId\":\"1477956290\",\"name\":\"Wenting Xu\"},{\"authorId\":\"2023765018\",\"name\":\"C. Qi\"},{\"authorId\":\"50070382\",\"name\":\"Zhenghua Xu\"},{\"authorId\":\"1690572\",\"name\":\"Thomas Lukasiewicz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"504e7bba81083ebbc4e114e48938ebaffb5ba03c\",\"title\":\"Reinforced Medical Report Generation with X-Linear Attention and Repetition Penalty\",\"url\":\"https://www.semanticscholar.org/paper/504e7bba81083ebbc4e114e48938ebaffb5ba03c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.1016/j.csl.2020.101095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"661f04ecc734ced906e16980a6143c814ce085ed\",\"title\":\"Hierarchical multimodal attention for end-to-end audio-visual scene-aware dialogue response generation\",\"url\":\"https://www.semanticscholar.org/paper/661f04ecc734ced906e16980a6143c814ce085ed\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":\"2007.15916\",\"authors\":[{\"authorId\":\"101809309\",\"name\":\"Justin van der Hout\"},{\"authorId\":\"1698052628\",\"name\":\"Z. D'Haese\"},{\"authorId\":\"1399115926\",\"name\":\"M. Hasegawa-Johnson\"},{\"authorId\":\"144951859\",\"name\":\"O. Scharenborg\"}],\"doi\":\"10.21437/interspeech.2020-2870\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f957d01e613fe14ad73369e4cf7fca7ff1574678\",\"title\":\"Evaluating Automatically Generated Phoneme Captions for Images\",\"url\":\"https://www.semanticscholar.org/paper/f957d01e613fe14ad73369e4cf7fca7ff1574678\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2004.01980\",\"authors\":[{\"authorId\":\"145657309\",\"name\":\"D. Jin\"},{\"authorId\":\"8752221\",\"name\":\"Zhijing Jin\"},{\"authorId\":\"10638646\",\"name\":\"Joey Tianyi Zhou\"},{\"authorId\":\"1612239626\",\"name\":\"Lisa Orii\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"}],\"doi\":\"10.18653/v1/2020.acl-main.456\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"051b1049e0e0cf1e0620065e7c69f908e98e9ece\",\"title\":\"Hooks in the Headline: Learning to Generate Headlines with Controlled Styles\",\"url\":\"https://www.semanticscholar.org/paper/051b1049e0e0cf1e0620065e7c69f908e98e9ece\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1646716323\",\"name\":\"Zihao Fu\"},{\"authorId\":\"46318189\",\"name\":\"Bei Shi\"},{\"authorId\":\"1996394\",\"name\":\"Lidong Bing\"},{\"authorId\":\"144594306\",\"name\":\"Wai Lam\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a4fe6cf1465cea5281e08340fc3d40e55ba6050f\",\"title\":\"Unsupervised KB-to-Text Generation with Auxiliary Triple Extraction using Dual Learning\",\"url\":\"https://www.semanticscholar.org/paper/a4fe6cf1465cea5281e08340fc3d40e55ba6050f\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2011.01385\",\"authors\":[{\"authorId\":\"3280656\",\"name\":\"Litao Yu\"},{\"authorId\":\"123275544\",\"name\":\"Jian Zhang\"},{\"authorId\":\"47506551\",\"name\":\"Qiang Wu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b0aa35ac0caf5230cdd1c5022b246e2381f87aac\",\"title\":\"Dual Attention on Pyramid Feature Maps for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b0aa35ac0caf5230cdd1c5022b246e2381f87aac\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1608.03819\",\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9bdc406ad9e9fc0ce356e6d0e53780534f418849\",\"title\":\"DeepDiary: Automatic Caption Generation for Lifelogging Image Streams\",\"url\":\"https://www.semanticscholar.org/paper/9bdc406ad9e9fc0ce356e6d0e53780534f418849\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92057141\",\"name\":\"H. Wei\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"}],\"doi\":\"10.1007/978-3-030-37731-1_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d98f43eaf8678edbfabe50e4761c7aab550584b9\",\"title\":\"Image Captioning Based on Visual and Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/d98f43eaf8678edbfabe50e4761c7aab550584b9\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":\"1904.07391\",\"authors\":[{\"authorId\":\"46180847\",\"name\":\"Rajarshi Bhowmik\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"}],\"doi\":\"10.1145/3308558.3313656\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62c83cc5c2245e94788c38c50230b52608f84c18\",\"title\":\"Be Concise and Precise: Synthesizing Open-Domain Entity Descriptions from Facts\",\"url\":\"https://www.semanticscholar.org/paper/62c83cc5c2245e94788c38c50230b52608f84c18\",\"venue\":\"WWW\",\"year\":2019},{\"arxivId\":\"1904.03493\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"46255707\",\"name\":\"Lei Li\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00468\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"title\":\"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research\",\"url\":\"https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1806.01353\",\"authors\":[{\"authorId\":\"145075823\",\"name\":\"S. Lee\"}],\"doi\":\"10.1038/s41746-018-0070-0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3762f6706b0fe6dd8c62cd67377ca25417c62e71\",\"title\":\"Natural language generation for electronic health records\",\"url\":\"https://www.semanticscholar.org/paper/3762f6706b0fe6dd8c62cd67377ca25417c62e71\",\"venue\":\"npj Digital Medicine\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"1789736\",\"name\":\"Filip Jurc\\u00edcek\"}],\"doi\":\"10.18653/v1/W19-8670\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b6a265569e020005fbecbaf0f81356b55c460da5\",\"title\":\"Neural Generation for Czech: Data and Baselines\",\"url\":\"https://www.semanticscholar.org/paper/b6a265569e020005fbecbaf0f81356b55c460da5\",\"venue\":\"INLG\",\"year\":2019},{\"arxivId\":\"1810.09630\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"345a222fef6f5c1415056319ae7e87a369940d3f\",\"title\":\"A Neural Compositional Paradigm for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/345a222fef6f5c1415056319ae7e87a369940d3f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2012.09742\",\"authors\":[{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"49336560\",\"name\":\"Weining Wang\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eb3f19de52d330ded5b3eebb79b89876648f67cb\",\"title\":\"AutoCaption: Image Captioning with Neural Architecture Search\",\"url\":\"https://www.semanticscholar.org/paper/eb3f19de52d330ded5b3eebb79b89876648f67cb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1902.10322\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1109/CVPR.2019.01277\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20888a7aebaf77a306c0886f165bd0d468db806d\",\"title\":\"Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20888a7aebaf77a306c0886f165bd0d468db806d\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1811.10652\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2019.00850\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8e59cf8c3becbedced0089028a1cddac8b19b251\",\"title\":\"Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions\",\"url\":\"https://www.semanticscholar.org/paper/8e59cf8c3becbedced0089028a1cddac8b19b251\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1726159226\",\"name\":\"Xiangqing Shen\"},{\"authorId\":\"49167055\",\"name\":\"B. Liu\"},{\"authorId\":\"1697439\",\"name\":\"Yong Zhou\"},{\"authorId\":\"1491078664\",\"name\":\"Jiaqi Zhao\"}],\"doi\":\"10.1007/s11042-020-09294-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20d2de48968f3ed1dc00be6ded0e2271d0f1a1a4\",\"title\":\"Remote sensing image caption generation via transformer and reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/20d2de48968f3ed1dc00be6ded0e2271d0f1a1a4\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"1803.11438\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"37378985\",\"name\":\"Wei Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2018.00795\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"title\":\"Reconstruction Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9037003\",\"name\":\"Duc-Cuong Dao\"},{\"authorId\":\"40429856\",\"name\":\"Thi-Oanh Nguyen\"},{\"authorId\":\"1735321\",\"name\":\"S. Bressan\"}],\"doi\":\"10.1145/3007120.3007136\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e559630c5710afb0d5eb8f95141e451393bbcbd9\",\"title\":\"Factors Influencing The Performance of Image Captioning Model: An Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/e559630c5710afb0d5eb8f95141e451393bbcbd9\",\"venue\":\"MoMM\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2013546\",\"name\":\"B. Wang\"},{\"authorId\":\"2263014\",\"name\":\"Xiangtao Zheng\"},{\"authorId\":\"1930660\",\"name\":\"Bo Qu\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/JSTARS.2019.2959208\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9fbdb53c100005ac890989beb3d78e208ba9acda\",\"title\":\"Retrieval Topic Recurrent Memory Network for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fbdb53c100005ac890989beb3d78e208ba9acda\",\"venue\":\"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing\",\"year\":2020},{\"arxivId\":\"2012.10813\",\"authors\":[{\"authorId\":\"2180892\",\"name\":\"Yikang Li\"},{\"authorId\":\"49459620\",\"name\":\"P. Goel\"},{\"authorId\":\"2039901404\",\"name\":\"Varsha Kuppur Rajendra\"},{\"authorId\":\"81361652\",\"name\":\"Har Simrat Singh\"},{\"authorId\":\"26253744\",\"name\":\"J. Francis\"},{\"authorId\":\"22244290\",\"name\":\"Kaixin Ma\"},{\"authorId\":\"144287919\",\"name\":\"Eric Nyberg\"},{\"authorId\":\"49930888\",\"name\":\"A. Oltramari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d89a37ab65b8826758bbc6655800629c1954343b\",\"title\":\"Lexically-constrained Text Generation through Commonsense Knowledge Extraction and Injection\",\"url\":\"https://www.semanticscholar.org/paper/d89a37ab65b8826758bbc6655800629c1954343b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49418270\",\"name\":\"Yong Wang\"},{\"authorId\":\"1485232293\",\"name\":\"Wenkai Zhang\"},{\"authorId\":\"47362549\",\"name\":\"Qing Liu\"},{\"authorId\":\"9708577\",\"name\":\"Zhengyuan Zhang\"},{\"authorId\":\"11732382\",\"name\":\"X. Gao\"},{\"authorId\":\"9758599\",\"name\":\"Xi-an Sun\"}],\"doi\":\"10.1145/3394171.3413877\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a05a96d5d9bac88d7c8b34be2e57f58f17a6e53d\",\"title\":\"Improving Intra- and Inter-Modality Visual Relation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a05a96d5d9bac88d7c8b34be2e57f58f17a6e53d\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.04264\",\"authors\":[{\"authorId\":\"32763968\",\"name\":\"A. Fisch\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"144797264\",\"name\":\"J. Clark\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ca71f61139c69131ab200368a30a3dc72fa6785\",\"title\":\"CapWAP: Captioning with a Purpose\",\"url\":\"https://www.semanticscholar.org/paper/8ca71f61139c69131ab200368a30a3dc72fa6785\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2002.00175\",\"authors\":[{\"authorId\":\"2318621\",\"name\":\"Q. Lam\"},{\"authorId\":\"104268348\",\"name\":\"Quang Duy \\u2013 Le\"},{\"authorId\":\"10346850\",\"name\":\"Kiet Van Nguyen\"},{\"authorId\":\"2591380\",\"name\":\"N. Nguyen\"}],\"doi\":\"10.1007/978-3-030-63007-2_57\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"245bab73b8921a2fdf1b479b20b4168af26681cf\",\"title\":\"UIT-ViIC: A Dataset for the First Evaluation on Vietnamese Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/245bab73b8921a2fdf1b479b20b4168af26681cf\",\"venue\":\"ICCCI\",\"year\":2020},{\"arxivId\":\"2002.06661\",\"authors\":[{\"authorId\":\"32370882\",\"name\":\"Shweta Mahajan\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"},{\"authorId\":\"49863405\",\"name\":\"S. Roth\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"87415957ec3aa3ae6756e3e7d22873cd9fc74c51\",\"title\":\"Latent Normalizing Flows for Many-to-Many Cross-Domain Mappings\",\"url\":\"https://www.semanticscholar.org/paper/87415957ec3aa3ae6756e3e7d22873cd9fc74c51\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"1926279\",\"name\":\"A. Butt\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"35783968\",\"name\":\"David Joy\"},{\"authorId\":\"74967014\",\"name\":\"A. Delgado\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"},{\"authorId\":\"1562121829\",\"name\":\"Yyette Graham\"},{\"authorId\":\"143723933\",\"name\":\"G. Jones\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1693391\",\"name\":\"G. Qu\\u00e9not\"},{\"authorId\":\"2203861\",\"name\":\"Maria Eskevich\"},{\"authorId\":\"1918199\",\"name\":\"R. Ordelman\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0838dd4c8037b2fc5e9498a8d53797719628048\",\"title\":\"TRECVID 2017: Evaluating Ad-hoc and Instance Video Search, Events Detection, Video Captioning and Hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/a0838dd4c8037b2fc5e9498a8d53797719628048\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":\"1903.10658\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"7574699\",\"name\":\"Handong Zhao\"},{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/ICCV.2019.01042\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc\",\"title\":\"Unpaired Image Captioning via Scene Graph Alignments\",\"url\":\"https://www.semanticscholar.org/paper/f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389373080\",\"name\":\"Ruoyu Chen\"},{\"authorId\":\"2607225\",\"name\":\"Zhongnian Li\"},{\"authorId\":\"1772283\",\"name\":\"D. Zhang\"}],\"doi\":\"10.1007/978-981-15-1398-5_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bab70faa1e36b525766f85b5ff87bff4566d0294\",\"title\":\"Adaptive Joint Attention with Reinforcement Training for Convolutional Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/bab70faa1e36b525766f85b5ff87bff4566d0294\",\"venue\":\"HBAI@IJCAI\",\"year\":2019},{\"arxivId\":\"1908.08527\",\"authors\":[{\"authorId\":\"1911972\",\"name\":\"Tanmay Gupta\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"2433269\",\"name\":\"Derek Hoiem\"}],\"doi\":\"10.1109/ICCV.2019.00752\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ae1764ffaa2fe68dafe33664624be38273c4ccef\",\"title\":\"ViCo: Word Embeddings From Visual Co-Occurrences\",\"url\":\"https://www.semanticscholar.org/paper/ae1764ffaa2fe68dafe33664624be38273c4ccef\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1910.02930\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/K19-1039\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"title\":\"A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152924581\",\"name\":\"Jicheng Wang\"},{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1438588470\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"47957556\",\"name\":\"X. Zhang\"},{\"authorId\":\"31048669\",\"name\":\"M. Wang\"}],\"doi\":\"10.1007/s11042-019-08439-7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0c48de74a40736498d6443f84ecdddc08275359f\",\"title\":\"Sequential image encoding for vision-to-language problems\",\"url\":\"https://www.semanticscholar.org/paper/0c48de74a40736498d6443f84ecdddc08275359f\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1904.01475\",\"authors\":[{\"authorId\":\"35570245\",\"name\":\"Ali Furkan Biten\"},{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"143823474\",\"name\":\"M. Rusi\\u00f1ol\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":\"10.1109/CVPR.2019.01275\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"908c6b1577a1f5309ae183daf2e24363039f22a8\",\"title\":\"Good News, Everyone! Context Driven Entity-Aware Captioning for News Images\",\"url\":\"https://www.semanticscholar.org/paper/908c6b1577a1f5309ae183daf2e24363039f22a8\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48520182\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3313873\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1a756ae8457454120ee2e067d4936d801f56ed62\",\"title\":\"Image Captioning by Asking Questions\",\"url\":\"https://www.semanticscholar.org/paper/1a756ae8457454120ee2e067d4936d801f56ed62\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1906.12188\",\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7c6d0bccb43e886297c5bf41ba7bacbb4ac05ea\",\"title\":\"A Deep Decoder Structure Based on WordEmbedding Regression for An Encoder-Decoder Based Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a7c6d0bccb43e886297c5bf41ba7bacbb4ac05ea\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144063214\",\"name\":\"M. Z. Hossain\"},{\"authorId\":\"19324907\",\"name\":\"F. Sohel\"},{\"authorId\":\"96363850\",\"name\":\"Mohd Fairuz Shiratuddin\"},{\"authorId\":\"47028380\",\"name\":\"Hamid Laga\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"}],\"doi\":\"10.1007/978-3-030-36802-9_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"53234e0927d63bb85121bea44f9a60edf0849a8e\",\"title\":\"Attention-Based Image Captioning Using DenseNet Features\",\"url\":\"https://www.semanticscholar.org/paper/53234e0927d63bb85121bea44f9a60edf0849a8e\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":\"2009.09984\",\"authors\":[{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"1926279\",\"name\":\"A. Butt\"},{\"authorId\":\"144175619\",\"name\":\"K. Curtis\"},{\"authorId\":\"2700261\",\"name\":\"Y. Lee\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"1781968\",\"name\":\"A. Godil\"},{\"authorId\":\"123306611\",\"name\":\"A. Delgado\"},{\"authorId\":\"1519100122\",\"name\":\"Jesse Zhang\"},{\"authorId\":\"1564398035\",\"name\":\"Eliot Godard\"},{\"authorId\":\"29928565\",\"name\":\"Lukas L. Diduch\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"},{\"authorId\":\"1562121829\",\"name\":\"Yyette Graham\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1406426956\",\"name\":\"Georges Qu\\u00e9not\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1de1bbe51ab668424f869b814e51c2f5e8368f2b\",\"title\":\"TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval\",\"url\":\"https://www.semanticscholar.org/paper/1de1bbe51ab668424f869b814e51c2f5e8368f2b\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143696833\",\"name\":\"Wenjie Cai\"},{\"authorId\":\"47362438\",\"name\":\"Q. Liu\"}],\"doi\":\"10.1016/j.neucom.2020.06.112\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c218331cf31c32c46eb660c1b2ce3e506c99b3b0\",\"title\":\"Image captioning with semantic-enhanced features and extremely hard negative examples\",\"url\":\"https://www.semanticscholar.org/paper/c218331cf31c32c46eb660c1b2ce3e506c99b3b0\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1708.09667\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3123420\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6199348281e14a5a127b539f5cdb92fcddbac17\",\"title\":\"Video Captioning with Guidance of Multimodal Latent Topics\",\"url\":\"https://www.semanticscholar.org/paper/a6199348281e14a5a127b539f5cdb92fcddbac17\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1812.02501\",\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"144031869\",\"name\":\"A. Yao\"}],\"doi\":\"10.1109/ICCV.2019.00095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15a36f9639f608c4567302de65355543fdcee910\",\"title\":\"Zero-Shot Anticipation for Instructional Activities\",\"url\":\"https://www.semanticscholar.org/paper/15a36f9639f608c4567302de65355543fdcee910\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145399579\",\"name\":\"Karan Sharma\"},{\"authorId\":\"5864325\",\"name\":\"Arun\"},{\"authorId\":\"1483929243\",\"name\":\"Kumar\"},{\"authorId\":\"3422895\",\"name\":\"S. Bhandarkar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf881e53510b230879aa0d3b02576043b8f881e7\",\"title\":\"Automated Image Captioning Using Nearest-Neighbors Approach Driven by Top-Object Detections\",\"url\":\"https://www.semanticscholar.org/paper/bf881e53510b230879aa0d3b02576043b8f881e7\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1811.09789\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"1795294\",\"name\":\"M. Dras\"},{\"authorId\":\"3093086\",\"name\":\"Stephen Wan\"},{\"authorId\":\"145212976\",\"name\":\"C\\u00e9cile Paris\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9347ee91bf90129582e7ed414d23ad3495180235\",\"title\":\"Senti-Attend: Image Captioning using Sentiment and Attention\",\"url\":\"https://www.semanticscholar.org/paper/9347ee91bf90129582e7ed414d23ad3495180235\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1718590\",\"name\":\"R. Gaizauskas\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b2426d4e4cdd775aa673ffba261c4f744ded5e48\",\"title\":\"Cross-validating Image Description Datasets and Evaluation Metrics\",\"url\":\"https://www.semanticscholar.org/paper/b2426d4e4cdd775aa673ffba261c4f744ded5e48\",\"venue\":\"LREC\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"},{\"authorId\":\"1796216614\",\"name\":\"Siyu Huang\"},{\"authorId\":\"40518823\",\"name\":\"Haoyi Xiong\"},{\"authorId\":\"7824051\",\"name\":\"Xingjian Li\"},{\"authorId\":\"1721158\",\"name\":\"D. Dou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"652885a326e1b5b978f4abeeb88e5fe518733f7e\",\"title\":\"Neighbours Matter: Image Captioning with Similar Images\",\"url\":\"https://www.semanticscholar.org/paper/652885a326e1b5b978f4abeeb88e5fe518733f7e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1802.10240\",\"authors\":[{\"authorId\":\"3174935\",\"name\":\"Wenshan Wang\"},{\"authorId\":\"4456978\",\"name\":\"S. Yang\"},{\"authorId\":\"1954076\",\"name\":\"W. Zhang\"},{\"authorId\":\"2985995\",\"name\":\"J. Zhang\"}],\"doi\":\"10.1049/iet-cvi.2019.0361\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2f950677e6a42beb234e507b3022964980b6556\",\"title\":\"Neural Aesthetic Image Reviewer\",\"url\":\"https://www.semanticscholar.org/paper/e2f950677e6a42beb234e507b3022964980b6556\",\"venue\":\"IET Comput. Vis.\",\"year\":2019},{\"arxivId\":\"1506.03099\",\"authors\":[{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df137487e20ba7c6e1e2b9a1e749f2a578b5ad99\",\"title\":\"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/df137487e20ba7c6e1e2b9a1e749f2a578b5ad99\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1704.06485\",\"authors\":[{\"authorId\":\"47465525\",\"name\":\"Cesc Chunseong Park\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.681\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9d842c2fb5a0c54f15bc4e45c7c93bdb8b0c44de\",\"title\":\"Attend to You: Personalized Image Captioning with Context Sequence Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/9d842c2fb5a0c54f15bc4e45c7c93bdb8b0c44de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144439462\",\"name\":\"Shuang Liu\"},{\"authorId\":\"48499525\",\"name\":\"Liang Richard Bai\"},{\"authorId\":\"46972536\",\"name\":\"Yan-Li Hu\"},{\"authorId\":\"49528408\",\"name\":\"Haoran Wang\"}],\"doi\":\"10.1051/MATECCONF/201823201052\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"730083506b1f6064ae12cc7ced6b3b2093d59bfd\",\"title\":\"Image Captioning Based on Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/730083506b1f6064ae12cc7ced6b3b2093d59bfd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134881509\",\"name\":\"Xiucong Shi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"7e75b28c77c83b3281541b6b9a0222de615ab78b\",\"title\":\"Image Description Generation in Chinese Based on Keywords Guidance\",\"url\":\"https://www.semanticscholar.org/paper/7e75b28c77c83b3281541b6b9a0222de615ab78b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687503\",\"name\":\"Yanming Guo\"},{\"authorId\":\"47908786\",\"name\":\"Y. Liu\"},{\"authorId\":\"36158244\",\"name\":\"Maaike H. T. de Boer\"},{\"authorId\":\"46457827\",\"name\":\"Li Liu\"},{\"authorId\":\"1731570\",\"name\":\"M. Lew\"}],\"doi\":\"10.1109/ICME.2018.8486491\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d27272027cd84341070fd4b7eb7e03dcb514d93f\",\"title\":\"A Dual Prediction Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d27272027cd84341070fd4b7eb7e03dcb514d93f\",\"venue\":\"2018 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23548372\",\"name\":\"Shuang Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3f88b0450f40559d33b113f84d10b1e43ec8c8fc\",\"title\":\"A General Model for Neural Text Generation from Structured Data\",\"url\":\"https://www.semanticscholar.org/paper/3f88b0450f40559d33b113f84d10b1e43ec8c8fc\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98482977\",\"name\":\"\\u0411\\u043e\\u0440\\u0438\\u0441\\u043e\\u0432 \\u0413\\u0435\\u043e\\u0440\\u0433\\u0438\\u0439 \\u0410\\u043b\\u0435\\u043a\\u0441\\u0430\\u043d\\u0434\\u0440\\u043e\\u0432\\u0438\\u0447\"},{\"authorId\":\"97625681\",\"name\":\"\\u0422\\u0438\\u0445\\u043e\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430 \\u0422\\u0430\\u043c\\u0430\\u0440\\u0430 \\u041f\\u0435\\u0442\\u0440\\u043e\\u0432\\u043d\\u0430\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"0de0cec214cf34d089a9fce9015256b63534cd08\",\"title\":\"\\u0417\\u0430\\u0434\\u0430\\u0447\\u0438 \\u0438 \\u043c\\u0435\\u0442\\u043e\\u0434\\u044b \\u0440\\u0435\\u0441\\u0443\\u0440\\u0441\\u043e\\u0441\\u0431\\u0435\\u0440\\u0435\\u0433\\u0430\\u044e\\u0449\\u0435\\u0439 \\u043e\\u043f\\u0442\\u0438\\u043c\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438 \\u0432 \\u044d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u044d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0439 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u0435\",\"url\":\"https://www.semanticscholar.org/paper/0de0cec214cf34d089a9fce9015256b63534cd08\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1907.11565\",\"authors\":[{\"authorId\":\"151017519\",\"name\":\"Gilad Vered\"},{\"authorId\":\"3431267\",\"name\":\"Gal Oren\"},{\"authorId\":\"34815079\",\"name\":\"Yuval Atzmon\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2d69f6cc66cad1ba771b5873de04eeac5dc32eca\",\"title\":\"Cooperative image captioning\",\"url\":\"https://www.semanticscholar.org/paper/2d69f6cc66cad1ba771b5873de04eeac5dc32eca\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"Jinglun Shi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Describing Video with Multiple Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"116712860\",\"name\":\"Natalia N. Iskra\"},{\"authorId\":\"1581636929\",\"name\":\"Vitaly Iskra\"}],\"doi\":\"10.1007/978-3-030-35430-5_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"62e1d2a9bd4c14b4b7bd985f49a996a8108dde97\",\"title\":\"Temporal Convolutional and Recurrent Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/62e1d2a9bd4c14b4b7bd985f49a996a8108dde97\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":null,\"name\":\"Ning Xie\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TCYB.2018.2831447\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"af6d6271f317a1a5a30908fdeac0fc054cd0493b\",\"title\":\"Describing Video With Attention-Based Bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/af6d6271f317a1a5a30908fdeac0fc054cd0493b\",\"venue\":\"IEEE Transactions on Cybernetics\",\"year\":2019},{\"arxivId\":\"1711.05557\",\"authors\":[{\"authorId\":\"143692915\",\"name\":\"Ying Hua Tan\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e70af721dbf04ab8aacc138d75c808588612289b\",\"title\":\"Phrase-based Image Captioning with Hierarchical LSTM Model\",\"url\":\"https://www.semanticscholar.org/paper/e70af721dbf04ab8aacc138d75c808588612289b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"3125776\",\"name\":\"Chris Brockett\"},{\"authorId\":\"143953229\",\"name\":\"Xiang Gao\"},{\"authorId\":\"48441311\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"66648221\",\"name\":\"B. Dolan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"920a003217d9dbc4c68e3097b4f9f54dc73c16de\",\"title\":\"Grounded Response Generation Task at DSTC7\",\"url\":\"https://www.semanticscholar.org/paper/920a003217d9dbc4c68e3097b4f9f54dc73c16de\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1901.03461\",\"authors\":[{\"authorId\":\"2237192\",\"name\":\"Koichiro Yoshino\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"121436310\",\"name\":\"Julien Perez\"},{\"authorId\":\"1405511901\",\"name\":\"Luis Fernando D'Haro\"},{\"authorId\":\"1725498\",\"name\":\"L. Polymenakos\"},{\"authorId\":\"144543562\",\"name\":\"R. Chulaka Gunasekara\"},{\"authorId\":\"2598433\",\"name\":\"Walter S. Lasecki\"},{\"authorId\":\"1727211\",\"name\":\"Jonathan K. Kummerfeld\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"3125776\",\"name\":\"Chris Brockett\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"},{\"authorId\":\"71886367\",\"name\":\"Xiang Gao\"},{\"authorId\":\"2809915\",\"name\":\"H. AlAmri\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69004f329149096b2b672083e4ee4268bb7fef74\",\"title\":\"Dialog System Technology Challenge 7\",\"url\":\"https://www.semanticscholar.org/paper/69004f329149096b2b672083e4ee4268bb7fef74\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"28969396\",\"name\":\"C. Liu\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"31399f850c03d7652869d2d57080c344acc41ecb\",\"title\":\"Attentive Tensor Product Learning for Language Generation and Grammar Parsing\",\"url\":\"https://www.semanticscholar.org/paper/31399f850c03d7652869d2d57080c344acc41ecb\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2005.01279\",\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"Changyou Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"150116410\",\"name\":\"G. Wang\"},{\"authorId\":\"1500397567\",\"name\":\"Zheng Wen\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.18653/v1/2020.acl-main.227\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6675640a8ad5e180a3a68fb5e8b34386df28c68f\",\"title\":\"Improving Adversarial Text Generation by Modeling the Distant Future\",\"url\":\"https://www.semanticscholar.org/paper/6675640a8ad5e180a3a68fb5e8b34386df28c68f\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2009.14405\",\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"47739902\",\"name\":\"J. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6caaf71d18f093aebd0cffdcf246b86400092ab5\",\"title\":\"Teacher-Critical Training Strategies for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6caaf71d18f093aebd0cffdcf246b86400092ab5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"8058716\",\"name\":\"Noriki Nishida\"},{\"authorId\":\"7412686\",\"name\":\"Raphael Shu\"},{\"authorId\":\"35257737\",\"name\":\"Yo Ehara\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf05e710dae791f82cc639a09dbe5ec66fed2008\",\"title\":\"Generating Video Description using Sequence-to-sequence Model with Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/bf05e710dae791f82cc639a09dbe5ec66fed2008\",\"venue\":\"COLING\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":\"10.1017/S1351324918000116\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2df61fcd01b6a70a94dff2b25d6ed8dc4c16e422\",\"title\":\"The role of image representations in vision to language tasks\",\"url\":\"https://www.semanticscholar.org/paper/2df61fcd01b6a70a94dff2b25d6ed8dc4c16e422\",\"venue\":\"Nat. Lang. Eng.\",\"year\":2018},{\"arxivId\":\"1505.05612\",\"authors\":[{\"authorId\":\"2345388\",\"name\":\"Haoyuan Gao\"},{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":null,\"name\":\"Lei Wang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1\",\"title\":\"Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question\",\"url\":\"https://www.semanticscholar.org/paper/2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1612.07600\",\"authors\":[{\"authorId\":\"2492994\",\"name\":\"Mert Kilickaya\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"}],\"doi\":\"10.18653/V1/E17-1019\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f75e7a025bf0eaba3de4efb53d6c2b0e4b3669fd\",\"title\":\"Re-evaluating Automatic Metrics for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f75e7a025bf0eaba3de4efb53d6c2b0e4b3669fd\",\"venue\":\"EACL\",\"year\":2017},{\"arxivId\":\"1911.12018\",\"authors\":[{\"authorId\":\"98831710\",\"name\":\"B. Yang\"},{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"145586191\",\"name\":\"Can Zhang\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9191773630826b15a86148453365aae7703aec6b\",\"title\":\"Non-Autoregressive Coarse-to-Fine Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9191773630826b15a86148453365aae7703aec6b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41048842\",\"name\":\"Tonmoay Deb\"},{\"authorId\":\"152359104\",\"name\":\"M. Ali\"},{\"authorId\":\"152398205\",\"name\":\"S. Bhowmik\"},{\"authorId\":\"1972671\",\"name\":\"A. Firoze\"},{\"authorId\":\"152242250\",\"name\":\"Syed Shahir Ahmed\"},{\"authorId\":\"1395633906\",\"name\":\"Muhammad Abeer Tahmeed\"},{\"authorId\":\"145057622\",\"name\":\"N. Rahman\"},{\"authorId\":\"1732925\",\"name\":\"Rashedur M. Rahman\"}],\"doi\":\"10.3233/JIFS-179351\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"40ba4d6ebb66673bea62b1889f6fcde139f4b816\",\"title\":\"Oboyob: A sequential-semantic Bengali image captioning engine\",\"url\":\"https://www.semanticscholar.org/paper/40ba4d6ebb66673bea62b1889f6fcde139f4b816\",\"venue\":\"J. Intell. Fuzzy Syst.\",\"year\":2019},{\"arxivId\":\"1912.01881\",\"authors\":[{\"authorId\":\"1443435125\",\"name\":\"Zhengcong Fei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"03158341c61b8bfedc9ccd503610ab150678a7c1\",\"title\":\"Better Understanding Hierarchical Visual Relationship for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/03158341c61b8bfedc9ccd503610ab150678a7c1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1703.10476\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2017.445\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c0a6854b793ca8ad281513c184318b73d4868c4\",\"title\":\"Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/1c0a6854b793ca8ad281513c184318b73d4868c4\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121556212\",\"name\":\"R. Subash\"},{\"authorId\":\"145313336\",\"name\":\"R. Jebakumar\"},{\"authorId\":\"1453739025\",\"name\":\"Yash Kamdar\"},{\"authorId\":\"79366636\",\"name\":\"N. Bhatt\"}],\"doi\":\"10.1088/1742-6596/1362/1/012096\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08640d76e5bff226a74a18c9d76ea2347e034473\",\"title\":\"Automatic Image Captioning Using Convolution Neural Networks and LSTM\",\"url\":\"https://www.semanticscholar.org/paper/08640d76e5bff226a74a18c9d76ea2347e034473\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2004.15006\",\"authors\":[{\"authorId\":\"26688118\",\"name\":\"Mihir Kale\"},{\"authorId\":\"2188497\",\"name\":\"Abhinav Rastogi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.527\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e3dd9eb8cc02af9844cc9bfd12a3334f9b4d107\",\"title\":\"Template Guided Text Generation for Task-Oriented Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/1e3dd9eb8cc02af9844cc9bfd12a3334f9b4d107\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2007.15780\",\"authors\":[{\"authorId\":\"3360992\",\"name\":\"Cristina Garbacea\"},{\"authorId\":\"1743469\",\"name\":\"Q. Mei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"64da659c0687762359226b4cf455520c78acd165\",\"title\":\"Neural Language Generation: Formulation, Methods, and Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/64da659c0687762359226b4cf455520c78acd165\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.05608\",\"authors\":[{\"authorId\":\"11007025\",\"name\":\"Junjiao Tian\"},{\"authorId\":\"143904954\",\"name\":\"Jean Oh\"}],\"doi\":\"10.24963/ijcai.2019/496\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"18403a06a67b7060645e137a36ad15122ee2c2f9\",\"title\":\"Image Captioning with Compositional Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/18403a06a67b7060645e137a36ad15122ee2c2f9\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2007.11731\",\"authors\":[{\"authorId\":\"1828787912\",\"name\":\"Yiwu Zhong\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"101777533\",\"name\":\"J. Chen\"},{\"authorId\":null,\"name\":\"Dong Yu\"},{\"authorId\":\"47002659\",\"name\":\"Yanchao Li\"}],\"doi\":\"10.1007/978-3-030-58568-6_13\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"66ed8795eb6de5d2a6b204baac9378d6d28136cc\",\"title\":\"Comprehensive Image Captioning via Scene Graph Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/66ed8795eb6de5d2a6b204baac9378d6d28136cc\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.00380\",\"authors\":[{\"authorId\":\"1421235702\",\"name\":\"Jieshan Chen\"},{\"authorId\":\"46729152\",\"name\":\"Chunyang Chen\"},{\"authorId\":\"3138980\",\"name\":\"Zhenchang Xing\"},{\"authorId\":\"79988928\",\"name\":\"X. Xu\"},{\"authorId\":\"48324793\",\"name\":\"Liming Zhu\"},{\"authorId\":\"49192734\",\"name\":\"Guoqiang Li\"},{\"authorId\":\"8653150\",\"name\":\"J. Wang\"}],\"doi\":\"10.1145/3377811.3380327\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a32ca8fb3ad34374dfb3ef4967d13b89d7d3ffd2\",\"title\":\"Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/a32ca8fb3ad34374dfb3ef4967d13b89d7d3ffd2\",\"venue\":\"2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)\",\"year\":2020},{\"arxivId\":\"2010.03494\",\"authors\":[{\"authorId\":\"7685850\",\"name\":\"Sebastian Goodman\"},{\"authorId\":\"145534763\",\"name\":\"N. Ding\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.702\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a35bf4eb3d6bc8931e52e2cf95dc5187b96804c\",\"title\":\"TeaForN: Teacher-Forcing with N-grams\",\"url\":\"https://www.semanticscholar.org/paper/3a35bf4eb3d6bc8931e52e2cf95dc5187b96804c\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32622125\",\"name\":\"Lily D. Ellebracht\"},{\"authorId\":\"1780343\",\"name\":\"Arnau Ramisa\"},{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1405187132\",\"name\":\"Jose Cordero-Rama\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"},{\"authorId\":\"3171632\",\"name\":\"A. Quattoni\"}],\"doi\":\"10.18653/v1/W15-2806\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fd5d732a3f1070a548c9d2f2f235202e7e097edd\",\"title\":\"Semantic Tuples for Evaluation of Image to Sentence Generation\",\"url\":\"https://www.semanticscholar.org/paper/fd5d732a3f1070a548c9d2f2f235202e7e097edd\",\"venue\":\"VL@EMNLP\",\"year\":2015},{\"arxivId\":\"1811.07234\",\"authors\":[{\"authorId\":\"2389866\",\"name\":\"Yao Wan\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"144346838\",\"name\":\"Min Yang\"},{\"authorId\":\"1747560\",\"name\":\"Guandong Xu\"},{\"authorId\":\"36887453\",\"name\":\"Haochao Ying\"},{\"authorId\":\"39903187\",\"name\":\"J. Wu\"},{\"authorId\":\"144019071\",\"name\":\"Philip S. Yu\"}],\"doi\":\"10.1145/3238147.3238206\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"077ea3a496bd76bd22d76c432b79b9fa40e136e8\",\"title\":\"Improving Automatic Source Code Summarization via Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/077ea3a496bd76bd22d76c432b79b9fa40e136e8\",\"venue\":\"2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE)\",\"year\":2018},{\"arxivId\":\"1805.12589\",\"authors\":[{\"authorId\":\"31121723\",\"name\":\"A. Deshpande\"},{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"39709900\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1109/CVPR.2019.01095\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e7fe886600399448f3282c8da8fd98ab7e50eae3\",\"title\":\"Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech\",\"url\":\"https://www.semanticscholar.org/paper/e7fe886600399448f3282c8da8fd98ab7e50eae3\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938051940\",\"name\":\"Dylan Flaute\"},{\"authorId\":\"2405109\",\"name\":\"B. Narayanan\"}],\"doi\":\"10.1117/12.2568016\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"title\":\"Video captioning using weakly supervised convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":\"2004.14638\",\"authors\":[{\"authorId\":\"7475040\",\"name\":\"S. Tan\"},{\"authorId\":\"2641547\",\"name\":\"H. Liu\"},{\"authorId\":\"144393479\",\"name\":\"D. Guo\"},{\"authorId\":\"2323566\",\"name\":\"Fu-Chun Sun\"}],\"doi\":\"10.15607/rss.2020.xvi.038\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"be6f591cfa15a23a13ea09b336f794601ba5eeca\",\"title\":\"Towards Embodied Scene Description\",\"url\":\"https://www.semanticscholar.org/paper/be6f591cfa15a23a13ea09b336f794601ba5eeca\",\"venue\":\"RSS 2020\",\"year\":2020},{\"arxivId\":\"1604.03390\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1007/978-3-319-44781-0_1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"799271daced99bd88b3a3e15921d5e31d9ea8323\",\"title\":\"Video Description Using Bidirectional Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/799271daced99bd88b3a3e15921d5e31d9ea8323\",\"venue\":\"ICANN\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40468514\",\"name\":\"Liang Yang\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"}],\"doi\":\"10.1007/s11063-019-10045-5\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f15f92647a02e526c02c1b61bf8bcc5d6b206e99\",\"title\":\"Adaptive Syncretic Attention for Constrained Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f15f92647a02e526c02c1b61bf8bcc5d6b206e99\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48387339\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"1761970\",\"name\":\"L. Li\"},{\"authorId\":\"40478963\",\"name\":\"J. Liu\"},{\"authorId\":\"7475375\",\"name\":\"H. Peng\"},{\"authorId\":\"3712008\",\"name\":\"X. Niu\"}],\"doi\":\"10.3390/APP8050739\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"da958d2604e9f86f94a441d60488d0e93451c248\",\"title\":\"Captioning Transformer with Stacked Attention Modules\",\"url\":\"https://www.semanticscholar.org/paper/da958d2604e9f86f94a441d60488d0e93451c248\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134654394\",\"name\":\"Ren C. Luo\"},{\"authorId\":\"34373093\",\"name\":\"Yu-Ting Hsu\"},{\"authorId\":\"151486060\",\"name\":\"Huan-Jun Ye\"}],\"doi\":\"10.1109/ISIE.2019.8781144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02695dca78cfe5c6c4fc7e4659b7ecac2b793bbc\",\"title\":\"Multi-Modal Human-Aware Image Caption System for Intelligent Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/02695dca78cfe5c6c4fc7e4659b7ecac2b793bbc\",\"venue\":\"2019 IEEE 28th International Symposium on Industrial Electronics (ISIE)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059218\",\"name\":\"Ankit Khare\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"01fe0c5f0d033141a29f4958f15520798022bbe7\",\"title\":\"ULTRA-CONTEXT: MAXIMIZING THE CONTEXT FOR BETTER IMAGE CAPTION GENERATION\",\"url\":\"https://www.semanticscholar.org/paper/01fe0c5f0d033141a29f4958f15520798022bbe7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9382626\",\"name\":\"M. Amaresh\"},{\"authorId\":\"144902122\",\"name\":\"S. Chitrakala\"}],\"doi\":\"10.1109/ICCSP.2019.8698097\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aad4525b28b18fde9c793ab387ac327802ef71d2\",\"title\":\"Video Captioning using Deep Learning: An Overview of Methods, Datasets and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/aad4525b28b18fde9c793ab387ac327802ef71d2\",\"venue\":\"2019 International Conference on Communication and Signal Processing (ICCSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1644155380\",\"name\":\"Yoav Shalev\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/WACV45572.2020.9093490\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6230cbf14070f77afe3dc60e474a5e21d2ead7ac\",\"title\":\"End to End Lip Synchronization with a Temporal AutoEncoder\",\"url\":\"https://www.semanticscholar.org/paper/6230cbf14070f77afe3dc60e474a5e21d2ead7ac\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1706.10006\",\"authors\":[{\"authorId\":\"3193584\",\"name\":\"Konstantinos Drossos\"},{\"authorId\":\"9918923\",\"name\":\"Sharath Adavanne\"},{\"authorId\":\"1684454\",\"name\":\"T. Virtanen\"}],\"doi\":\"10.1109/WASPAA.2017.8170058\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f1368b0001e454381eafc35324740c928cb2ad1e\",\"title\":\"Automated audio captioning with recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/f1368b0001e454381eafc35324740c928cb2ad1e\",\"venue\":\"2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15565853\",\"name\":\"Y. Wu\"},{\"authorId\":\"1697925\",\"name\":\"Kun Chen\"},{\"authorId\":\"1515440027\",\"name\":\"Ziyue Wang\"},{\"authorId\":\"1391222888\",\"name\":\"Xuan Zhang\"},{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"2319426\",\"name\":\"Shengchen Li\"},{\"authorId\":\"50425717\",\"name\":\"Xi Shao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"33dcbaa50f22fa82d26f174300fd67600c58b90a\",\"title\":\"AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE Technical Report\",\"url\":\"https://www.semanticscholar.org/paper/33dcbaa50f22fa82d26f174300fd67600c58b90a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1504.06692\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"144287022\",\"name\":\"Xu Wei\"},{\"authorId\":\"143907244\",\"name\":\"Yi Yang\"},{\"authorId\":\"152924551\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/ICCV.2015.291\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb847564774394c484e701437dbcffbf040ff3cc\",\"title\":\"Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images\",\"url\":\"https://www.semanticscholar.org/paper/eb847564774394c484e701437dbcffbf040ff3cc\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"39689790\",\"name\":\"L. White\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"2210661\",\"name\":\"S. Shah\"}],\"doi\":\"10.1007/978-3-030-01237-3_3\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fa9189749a4c95c45ec7d98db49e5f736c51760e\",\"title\":\"NNEval: Neural Network Based Evaluation Metric for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/fa9189749a4c95c45ec7d98db49e5f736c51760e\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49770170\",\"name\":\"C. Xu\"},{\"authorId\":\"33538504\",\"name\":\"Gengming Zhu\"},{\"authorId\":\"40367854\",\"name\":\"Lixin Wang\"}],\"doi\":\"10.1145/3318299.3318375\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"798f1e5ca775fd0186c82786865859cebba52d84\",\"title\":\"Image Captioning Based on Automatic Constraint Loss\",\"url\":\"https://www.semanticscholar.org/paper/798f1e5ca775fd0186c82786865859cebba52d84\",\"venue\":\"ICMLC '19\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"title\":\"Trainable performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"2006.04058\",\"authors\":[{\"authorId\":\"48775710\",\"name\":\"Alok Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"098317f445d4753188658ebd3a72c272d10132cd\",\"title\":\"NITS-VC System for VATEX Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/098317f445d4753188658ebd3a72c272d10132cd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1611.07837\",\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2d692d14b4277e6ad00b9030ad3b68141b3bbc21\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Language\",\"url\":\"https://www.semanticscholar.org/paper/2d692d14b4277e6ad00b9030ad3b68141b3bbc21\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"2010.07447\",\"authors\":[{\"authorId\":\"2744849\",\"name\":\"M. Lukasik\"},{\"authorId\":\"50179902\",\"name\":\"Himanshu Jain\"},{\"authorId\":\"2844480\",\"name\":\"A. Menon\"},{\"authorId\":\"5076305\",\"name\":\"Seungyeon Kim\"},{\"authorId\":\"1798880\",\"name\":\"Srinadh Bhojanapalli\"},{\"authorId\":\"1815972\",\"name\":\"F. Yu\"},{\"authorId\":\"2794322\",\"name\":\"S. Kumar\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.405\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee9fd2d141e13e5d0fdce62204c2ab32a379cba7\",\"title\":\"Semantic Label Smoothing for Sequence to Sequence Problems\",\"url\":\"https://www.semanticscholar.org/paper/ee9fd2d141e13e5d0fdce62204c2ab32a379cba7\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.04295\",\"authors\":[{\"authorId\":\"98177814\",\"name\":\"Y. Li\"},{\"authorId\":\"1490931976\",\"name\":\"Gang Li\"},{\"authorId\":\"2265599\",\"name\":\"Luheng He\"},{\"authorId\":\"2923923\",\"name\":\"Jingjie Zheng\"},{\"authorId\":\"100566447\",\"name\":\"H. Li\"},{\"authorId\":\"46731502\",\"name\":\"Zhiwei Guan\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.443\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b4a8b1c98bd13ce0a6281bb1af3761f7f887235\",\"title\":\"Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements\",\"url\":\"https://www.semanticscholar.org/paper/0b4a8b1c98bd13ce0a6281bb1af3761f7f887235\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2037486210\",\"name\":\"Qiaoqiao Yang\"},{\"authorId\":\"1410201319\",\"name\":\"Guangxing Wang\"},{\"authorId\":\"1695975266\",\"name\":\"Xiaoyu Zhang\"},{\"authorId\":\"2037477833\",\"name\":\"Christos Grecos\"},{\"authorId\":\"143823414\",\"name\":\"Peng Ren\"}],\"doi\":\"10.2112/SI102-018.1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"556edd10f961c30fc523f291fe657db56a297850\",\"title\":\"Coastal Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/556edd10f961c30fc523f291fe657db56a297850\",\"venue\":\"Journal of Coastal Research\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1577606322\",\"name\":\"Shrey Nahar\"},{\"authorId\":\"1576127863\",\"name\":\"S. Naik\"},{\"authorId\":\"15488883\",\"name\":\"Niti H Shah\"},{\"authorId\":\"49485385\",\"name\":\"Saumya Shah\"},{\"authorId\":\"9079986\",\"name\":\"Lakshmi Kurup\"}],\"doi\":\"10.1007/978-3-030-38445-6_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1bac534a3a2672297db2a76a4491b275e464bdbd\",\"title\":\"Automated Question Generation and Answer Verification Using Visual Data\",\"url\":\"https://www.semanticscholar.org/paper/1bac534a3a2672297db2a76a4491b275e464bdbd\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1908.05407\",\"authors\":[{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3343031.3350996\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c1056e6e84d52cf45017aad544fa0406441abda0\",\"title\":\"Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards\",\"url\":\"https://www.semanticscholar.org/paper/c1056e6e84d52cf45017aad544fa0406441abda0\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1808.05864\",\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":null,\"name\":\"Feng Wu\"}],\"doi\":\"10.1145/3240508.3240632\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0574dc64c8275b09ed587dc3977f4d3c990bd4df\",\"title\":\"Context-Aware Visual Policy Network for Sequence-Level Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0574dc64c8275b09ed587dc3977f4d3c990bd4df\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jing Wang\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8218640e95bb2d925a617b1c3012eed7d209351\",\"title\":\"Show, Reward and Tell: Automatic Generation of Narrative Paragraph From Photo Stream by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/b8218640e95bb2d925a617b1c3012eed7d209351\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144792753\",\"name\":\"Y. Qin\"},{\"authorId\":\"151046769\",\"name\":\"Jiajun Du\"},{\"authorId\":\"48379418\",\"name\":\"Yonghua Zhang\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/CVPR.2019.00856\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e0c0ac3bb66203c32be81193fabeee44c3585582\",\"title\":\"Look Back and Predict Forward in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e0c0ac3bb66203c32be81193fabeee44c3585582\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3239379\",\"name\":\"J. Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1007/978-3-030-05710-7_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"title\":\"Hierarchical Vision-Language Alignment for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":\"1803.08314\",\"authors\":[{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"49404547\",\"name\":\"Hongsheng Li\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"},{\"authorId\":\"143982372\",\"name\":\"Dapeng Chen\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-01267-0_21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"caab1c1d53718315f54bc4df42eb9a727fa18483\",\"title\":\"Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data\",\"url\":\"https://www.semanticscholar.org/paper/caab1c1d53718315f54bc4df42eb9a727fa18483\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3123266.3123391\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39836fbbcd2a664edb31119e88870c38b83df352\",\"title\":\"Adaptively Attending to Visual Attributes and Linguistic Knowledge for Captioning\",\"url\":\"https://www.semanticscholar.org/paper/39836fbbcd2a664edb31119e88870c38b83df352\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"},{\"authorId\":\"2797847\",\"name\":\"Carolina Scarton\"},{\"authorId\":\"3302745\",\"name\":\"Gustavo Paetzold\"}],\"doi\":\"10.2200/S00854ED1V01Y201805HLT039\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c09f1e4b31ae5a943117b5341f543028c1dafc55\",\"title\":\"Quality Estimation for Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/c09f1e4b31ae5a943117b5341f543028c1dafc55\",\"venue\":\"Quality Estimation for Machine Translation\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67159407\",\"name\":\"S. Amirian\"},{\"authorId\":\"1438623667\",\"name\":\"Khaled Rasheed\"},{\"authorId\":\"1784399\",\"name\":\"T. Taha\"},{\"authorId\":\"1712033\",\"name\":\"H. Arabnia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"afa8032f794011884be0b06f808540f36b404c0b\",\"title\":\"A Short Review on Image Caption Generation with Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/afa8032f794011884be0b06f808540f36b404c0b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1909.04101\",\"authors\":[{\"authorId\":\"39191185\",\"name\":\"M. Forbes\"},{\"authorId\":\"1403585268\",\"name\":\"Christine Kaeser-Chen\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"}],\"doi\":\"10.18653/v1/D19-1065\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"035c71e4127a1d3b942b043352f1afac3f0ec451\",\"title\":\"Neural Naturalist: Generating Fine-Grained Image Comparisons\",\"url\":\"https://www.semanticscholar.org/paper/035c71e4127a1d3b942b043352f1afac3f0ec451\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"51491153\",\"name\":\"Takayuki Itamochi\"},{\"authorId\":\"3193466\",\"name\":\"K. Narioka\"},{\"authorId\":\"152599661\",\"name\":\"Ikuro Sato\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/ICCV.2019.00589\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"949959250e487617faa3035b05b52aa03a8e9895\",\"title\":\"Generating Easy-to-Understand Referring Expressions for Target Identifications\",\"url\":\"https://www.semanticscholar.org/paper/949959250e487617faa3035b05b52aa03a8e9895\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1909.07583\",\"authors\":[{\"authorId\":\"1388780256\",\"name\":\"Yaser Alwatter\"},{\"authorId\":\"1798719\",\"name\":\"Yuhong Guo\"}],\"doi\":\"10.22215/etd/2019-13929\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"48795928eb87d1e8a038063b3aebee180e424d04\",\"title\":\"Inverse Visual Question Answering with Multi-Level Attentions\",\"url\":\"https://www.semanticscholar.org/paper/48795928eb87d1e8a038063b3aebee180e424d04\",\"venue\":\"ACML\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"46700004\",\"name\":\"J. Liu\"},{\"authorId\":\"116829059\",\"name\":\"Shi-chen Lu\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/TMM.2019.2951226\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"837a513a43c7bcce903edbacbfc507cba6451e21\",\"title\":\"Show, Tell, and Polish: Ruminant Decoding for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/837a513a43c7bcce903edbacbfc507cba6451e21\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ACCESS.2019.2942000\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"801827592d18c4e6170d88f8345465de4a8db7ca\",\"title\":\"Video Captioning With Adaptive Attention and Mixed Loss Optimization\",\"url\":\"https://www.semanticscholar.org/paper/801827592d18c4e6170d88f8345465de4a8db7ca\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"2006.03744\",\"authors\":[{\"authorId\":\"50651835\",\"name\":\"Mingjie Li\"},{\"authorId\":\"46429484\",\"name\":\"F. Wang\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"13246332\",\"name\":\"Xiaodan Liang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"461420c80d3bdc156e5db7af13264a955a6a2010\",\"title\":\"Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/461420c80d3bdc156e5db7af13264a955a6a2010\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.08686\",\"authors\":[{\"authorId\":\"49967124\",\"name\":\"N. Trieu\"},{\"authorId\":\"7685850\",\"name\":\"Sebastian Goodman\"},{\"authorId\":\"145382418\",\"name\":\"P. Narayana\"},{\"authorId\":\"113477341\",\"name\":\"Kazoo Sone\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bf315d3defed0ba7e12a1fc00ebb6444c86e117c\",\"title\":\"Multi-Image Summarization: Textual Summary from a Set of Cohesive Images\",\"url\":\"https://www.semanticscholar.org/paper/bf315d3defed0ba7e12a1fc00ebb6444c86e117c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8772234\",\"name\":\"Hyeryun Park\"},{\"authorId\":\"113066066\",\"name\":\"Kyungmo Kim\"},{\"authorId\":\"72062486\",\"name\":\"J. Yoon\"},{\"authorId\":\"31171717\",\"name\":\"Seongkeun Park\"},{\"authorId\":\"153439158\",\"name\":\"Jinwook Choi\"}],\"doi\":\"10.18653/v1/2020.acl-srw.14\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3b85f64b0a7c8eabd11476fa870b14b4ec696166\",\"title\":\"Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information\",\"url\":\"https://www.semanticscholar.org/paper/3b85f64b0a7c8eabd11476fa870b14b4ec696166\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2006.10923\",\"authors\":[{\"authorId\":\"35594943\",\"name\":\"A. Patel\"},{\"authorId\":\"1500412323\",\"name\":\"Aravind Varier\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8d49553e204563784906a54a3bf5bc93b8e31e65\",\"title\":\"Hyperparameter Analysis for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8d49553e204563784906a54a3bf5bc93b8e31e65\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"117788399\",\"name\":\"Chandresh S. Kanani\"},{\"authorId\":\"1777669\",\"name\":\"Sriparna Saha\"},{\"authorId\":\"145532184\",\"name\":\"P. Bhattacharyya\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206644\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"2194227d8766086b01bfa3e28b863bd5c596efab\",\"title\":\"Improving Diversity and Reducing Redundancy in Paragraph Captions\",\"url\":\"https://www.semanticscholar.org/paper/2194227d8766086b01bfa3e28b863bd5c596efab\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Weiying Wang\"},{\"authorId\":null,\"name\":\"Yongcheng Wang\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.18653/v1/D19-1517\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34e7db6ffcdfeffc29eab0622384ed412d9b4558\",\"title\":\"YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/34e7db6ffcdfeffc29eab0622384ed412d9b4558\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1910.14208\",\"authors\":[{\"authorId\":\"9095876\",\"name\":\"Jipeng Wu\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61b1bb801d03a9b0da5721c6beaabf1dc6cd90c0\",\"title\":\"Hidden State Guidance: Improving Image Captioning Using an Image Conditioned Autoencoder\",\"url\":\"https://www.semanticscholar.org/paper/61b1bb801d03a9b0da5721c6beaabf1dc6cd90c0\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478695\",\"name\":\"Teng Jiang\"},{\"authorId\":\"46232306\",\"name\":\"Zehan Zhang\"},{\"authorId\":\"7607492\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1007/s00371-018-1565-z\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d6a788c65190959c1390d0ba8065d755c78d200a\",\"title\":\"Modeling coverage with semantic embedding for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/d6a788c65190959c1390d0ba8065d755c78d200a\",\"venue\":\"The Visual Computer\",\"year\":2018},{\"arxivId\":\"1609.07859\",\"authors\":[{\"authorId\":\"33253726\",\"name\":\"T. Kim\"},{\"authorId\":\"3468598\",\"name\":\"Seyeong Kim\"},{\"authorId\":\"3468762\",\"name\":\"Sangil Na\"},{\"authorId\":\"21100692\",\"name\":\"Hayoon Kim\"},{\"authorId\":\"2396206\",\"name\":\"Moonki Kim\"},{\"authorId\":\"37733618\",\"name\":\"Beyeongki Jeon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"1aa52a25c2967b8bc228268c9ab5a96a32d2189b\",\"title\":\"Visual Fashion-Product Search at SK Planet\",\"url\":\"https://www.semanticscholar.org/paper/1aa52a25c2967b8bc228268c9ab5a96a32d2189b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10125385\",\"name\":\"Z. Wang\"},{\"authorId\":\"39640395\",\"name\":\"Z. Wang\"},{\"authorId\":\"7165718\",\"name\":\"Yinong Long\"},{\"authorId\":\"49606678\",\"name\":\"Jianan Wang\"},{\"authorId\":\"121832519\",\"name\":\"Zhen Xu\"},{\"authorId\":\"2806178\",\"name\":\"Baoxun Wang\"}],\"doi\":\"10.1016/j.csl.2018.09.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ed28ab8df23883340d69ec0d1346d612487b306\",\"title\":\"Enhancing generative conversational service agents with dialog history and external knowledge\",\"url\":\"https://www.semanticscholar.org/paper/1ed28ab8df23883340d69ec0d1346d612487b306\",\"venue\":\"Comput. Speech Lang.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35212676\",\"name\":\"J. Peyre\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"caffa07ead18aae78bf654bc57023eef58e74faf\",\"title\":\"Learning to detect visual relations\",\"url\":\"https://www.semanticscholar.org/paper/caffa07ead18aae78bf654bc57023eef58e74faf\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1704.01502\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"47008023\",\"name\":\"Z. Su\"},{\"authorId\":\"3700393\",\"name\":\"Minjun Li\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/CVPR.2017.548\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"title\":\"Weakly Supervised Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.14124\",\"authors\":[{\"authorId\":\"47475299\",\"name\":\"K. Shirai\"},{\"authorId\":\"2036498481\",\"name\":\"Kazuma Hashimoto\"},{\"authorId\":\"50123248\",\"name\":\"A. Eriguchi\"},{\"authorId\":\"49584970\",\"name\":\"T. Ninomiya\"},{\"authorId\":\"48608426\",\"name\":\"S. Mori\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f6c65402de67aef611852baa6c89d0b97b3e4f06\",\"title\":\"Neural Text Generation with Artificial Negative Examples\",\"url\":\"https://www.semanticscholar.org/paper/f6c65402de67aef611852baa6c89d0b97b3e4f06\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1506.06863\",\"authors\":[{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"3125776\",\"name\":\"Chris Brockett\"},{\"authorId\":\"2041695\",\"name\":\"Alessandro Sordoni\"},{\"authorId\":\"40608686\",\"name\":\"Yangfeng Ji\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"},{\"authorId\":\"2596310\",\"name\":\"Chris Quirk\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":\"10.3115/v1/P15-2073\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d0684b19ba46e739e28baa1e180c008226f793a\",\"title\":\"deltaBLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets\",\"url\":\"https://www.semanticscholar.org/paper/7d0684b19ba46e739e28baa1e180c008226f793a\",\"venue\":\"ACL\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b5c35e70954a05ec4b836f166882982f459eefa\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/4b5c35e70954a05ec4b836f166882982f459eefa\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92716077\",\"name\":\"Shaokang Yang\"},{\"authorId\":\"122218340\",\"name\":\"J. Niu\"},{\"authorId\":\"1809483\",\"name\":\"Jiyan Wu\"},{\"authorId\":\"37305311\",\"name\":\"X. Liu\"}],\"doi\":\"10.1007/978-3-030-60248-2_48\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0e3f376cd8e08119a2f24821c667b7b9d6ec410\",\"title\":\"Automatic Medical Image Report Generation with Multi-view and Multi-modal Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/c0e3f376cd8e08119a2f24821c667b7b9d6ec410\",\"venue\":\"ICA3PP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"8799308\",\"name\":\"U. Nadeem\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1007/978-3-030-49724-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"title\":\"Vision to Language: Methods, Metrics and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1801.05091\",\"authors\":[{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"35573122\",\"name\":\"Dingdong Yang\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":\"10.1109/CVPR.2018.00833\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc63a155021362b05c3c75bd5040373d72e0623e\",\"title\":\"Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/cc63a155021362b05c3c75bd5040373d72e0623e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2596437\",\"name\":\"S. W. Kim\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ffe11e74e99e152964fc64f7387b5a944a76983b\",\"title\":\"Progressive Reasoning by Module Composition\",\"url\":\"https://www.semanticscholar.org/paper/ffe11e74e99e152964fc64f7387b5a944a76983b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"49991208\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c631bb439284f6a5a90608b715fa631d5c5807e4\",\"title\":\"EEGtoText: Learning to Write Medical Reports from EEG Recordings\",\"url\":\"https://www.semanticscholar.org/paper/c631bb439284f6a5a90608b715fa631d5c5807e4\",\"venue\":\"MLHC\",\"year\":2019},{\"arxivId\":\"1805.08389\",\"authors\":[{\"authorId\":\"35585536\",\"name\":\"Jialin Wu\"},{\"authorId\":\"32193161\",\"name\":\"Zeyuan Hu\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"91956c41190231eefd2186f21b79d1ca1495a68e\",\"title\":\"Joint Image Captioning and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/91956c41190231eefd2186f21b79d1ca1495a68e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1885314583\",\"name\":\"Made Raharja Surya Mahadi\"},{\"authorId\":\"9347718\",\"name\":\"Anditya Arifianto\"},{\"authorId\":\"9308183\",\"name\":\"K. N. Ramadhani\"}],\"doi\":\"10.1109/ICoICT49345.2020.9166244\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"538b261dbdffc4745cb217128c001ef1e63dc6b6\",\"title\":\"Adaptive Attention Generation for Indonesian Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/538b261dbdffc4745cb217128c001ef1e63dc6b6\",\"venue\":\"2020 8th International Conference on Information and Communication Technology (ICoICT)\",\"year\":2020},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"2069818\",\"name\":\"D. Zhang\"},{\"authorId\":\"1706774\",\"name\":\"J. Li\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/CVPR.2017.662\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3b0b706fc94b35a1eddd830685e07870315b9565\",\"title\":\"Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description\",\"url\":\"https://www.semanticscholar.org/paper/3b0b706fc94b35a1eddd830685e07870315b9565\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1810.06245\",\"authors\":[{\"authorId\":\"35935570\",\"name\":\"Jean-Benoit Delbrouck\"},{\"authorId\":\"144436412\",\"name\":\"S. Dupont\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"58d16e23e1192be4acaf6a29c1f5995817146554\",\"title\":\"Bringing back simplicity and lightliness into neural image captioning\",\"url\":\"https://www.semanticscholar.org/paper/58d16e23e1192be4acaf6a29c1f5995817146554\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1611.06492\",\"authors\":[{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"34762956\",\"name\":\"Abhinav Agarwalla\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"144240262\",\"name\":\"P. Mitra\"}],\"doi\":\"10.1109/CVPRW.2017.273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"title\":\"Recurrent Memory Addressing for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1707.07998\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"31790073\",\"name\":\"C. Buehler\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1109/CVPR.2018.00636\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"title\":\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1807.09958\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"50816334\",\"name\":\"D. Ye\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01228-1_18\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"94e3b75a6732b5918c4c2b87d127a9216ff07efc\",\"title\":\"Rethinking the Form of Latent States in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/94e3b75a6732b5918c4c2b87d127a9216ff07efc\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1807.10018\",\"authors\":[{\"authorId\":\"51152390\",\"name\":\"Yilei Xiong\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01252-6_29\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"title\":\"Move Forward and Tell: A Progressive Generator of Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8353931\",\"name\":\"Jiahe Shi\"},{\"authorId\":\"5550675\",\"name\":\"Y. Li\"},{\"authorId\":\"103307901\",\"name\":\"Shengjin Wang\"}],\"doi\":\"10.1109/ICIP.2019.8803149\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c054cda5375018e902daab0b0875773a854d035\",\"title\":\"Cascade Attention: Multiple Feature Based Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c054cda5375018e902daab0b0875773a854d035\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1411260673\",\"name\":\"Luke Melas-Kyriazi\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"},{\"authorId\":\"50552688\",\"name\":\"G. Han\"}],\"doi\":\"10.18653/v1/D18-1084\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8298cf0056af5afa3185181ddd5f6bb03181696\",\"title\":\"Training for Diversity in Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b8298cf0056af5afa3185181ddd5f6bb03181696\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52201852\",\"name\":\"Eleftherios Daskalakis\"},{\"authorId\":\"2817419\",\"name\":\"Maria Tzelepi\"},{\"authorId\":\"1737071\",\"name\":\"A. Tefas\"}],\"doi\":\"10.1016/j.patrec.2018.09.022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fc1332023c370dc55fabb1b6c895af1a5f48f889\",\"title\":\"Learning deep spatiotemporal features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/fc1332023c370dc55fabb1b6c895af1a5f48f889\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chang Liu\"},{\"authorId\":\"2887672\",\"name\":\"A. Shmilovici\"},{\"authorId\":\"3045152\",\"name\":\"Mark Last\"}],\"doi\":\"10.1109/FSKD.2017.8393188\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22648dcd3100432fe0cc71e09de5ee855c61f12b\",\"title\":\"Automatic generation of composite image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/22648dcd3100432fe0cc71e09de5ee855c61f12b\",\"venue\":\"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40034021\",\"name\":\"Y. Sun\"},{\"authorId\":\"144651371\",\"name\":\"B. Ren\"}],\"doi\":\"10.1007/978-981-10-7299-4_63\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"89352b9c8114017b9d99d964b4c57203b1f14ebd\",\"title\":\"Automatic Image Description Generation with Emotional Classifiers\",\"url\":\"https://www.semanticscholar.org/paper/89352b9c8114017b9d99d964b4c57203b1f14ebd\",\"venue\":\"CCCV\",\"year\":2017},{\"arxivId\":\"2004.10796\",\"authors\":[{\"authorId\":\"4868335\",\"name\":\"J. Park\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6966b0018daffa49eb2c38e68eb8964d56440233\",\"title\":\"Visual Commonsense Graphs: Reasoning about the Dynamic Context of a Still Image\",\"url\":\"https://www.semanticscholar.org/paper/6966b0018daffa49eb2c38e68eb8964d56440233\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1809.01110\",\"authors\":[{\"authorId\":\"2218741\",\"name\":\"Fuwen Tan\"},{\"authorId\":\"145480864\",\"name\":\"Song Feng\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2d0b9205d0d19f96e3b75733611d6c88cf948036\",\"title\":\"Text2Scene: Generating Abstract Scenes from Textual Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2d0b9205d0d19f96e3b75733611d6c88cf948036\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2569534\",\"name\":\"Jinkyu Kim\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1729041\",\"name\":\"John F. Canny\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f5536ecbce6fbe991b76c799d3d9c2b8b757892\",\"title\":\"UvA-DARE ( Digital Academic Repository ) Textual Explanations for Self-Driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/0f5536ecbce6fbe991b76c799d3d9c2b8b757892\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1904.11251\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2019.01278\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"title\":\"Pointing Novel Objects in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1905.12243\",\"authors\":[{\"authorId\":\"50080046\",\"name\":\"Xuelong Li\"},{\"authorId\":\"12122088\",\"name\":\"Aihong Yuan\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/TCYB.2019.2914351\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5f46d314bcdbe55183cbe7e0852887c148eb807\",\"title\":\"Vision-to-Language Tasks Based on Attributes and Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/e5f46d314bcdbe55183cbe7e0852887c148eb807\",\"venue\":\"IEEE transactions on cybernetics\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47908786\",\"name\":\"Y. Liu\"},{\"authorId\":\"1687503\",\"name\":\"Yanming Guo\"},{\"authorId\":\"1731570\",\"name\":\"M. Lew\"}],\"doi\":\"10.1007/978-3-319-51811-4_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66156cb48fe184ea98be1c394f70cef78f63b1ae\",\"title\":\"What Convnets Make for Image Captioning?\",\"url\":\"https://www.semanticscholar.org/paper/66156cb48fe184ea98be1c394f70cef78f63b1ae\",\"venue\":\"MMM\",\"year\":2017},{\"arxivId\":\"1809.00681\",\"authors\":[{\"authorId\":\"2479187\",\"name\":\"Moitreya Chatterjee\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":\"10.1007/978-3-030-01216-8_45\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a27973d90c1427369cb10aa0202d671f0422e21e\",\"title\":\"Diverse and Coherent Paragraph Generation from Images\",\"url\":\"https://www.semanticscholar.org/paper/a27973d90c1427369cb10aa0202d671f0422e21e\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1624658620\",\"name\":\"Chunlei Wu\"},{\"authorId\":\"1429199889\",\"name\":\"Shaozu Yuan\"},{\"authorId\":\"51172982\",\"name\":\"Haiwen Cao\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"}],\"doi\":\"10.1109/ACCESS.2020.2981513\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"711be95a04da16c93b6bc880169532b68cdca37a\",\"title\":\"Hierarchical Attention-Based Fusion for Image Caption With Multi-Grained Rewards\",\"url\":\"https://www.semanticscholar.org/paper/711be95a04da16c93b6bc880169532b68cdca37a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3232032\",\"name\":\"E. Davoodi\"},{\"authorId\":\"37722032\",\"name\":\"Charese Smiley\"},{\"authorId\":\"34695819\",\"name\":\"Dezhao Song\"},{\"authorId\":\"145837551\",\"name\":\"Frank Schilder\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6669fbe108d81dc9b0c2a45376952c80927f8036\",\"title\":\"The E 2 E NLG Challenge : Training a Sequence-to-Sequence Approach for Meaning Representation to Natural Language Sentences\",\"url\":\"https://www.semanticscholar.org/paper/6669fbe108d81dc9b0c2a45376952c80927f8036\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32211627\",\"name\":\"Raghuram Vadapalli\"},{\"authorId\":\"50708394\",\"name\":\"B. Syed\"},{\"authorId\":\"46971819\",\"name\":\"Nishant Prabhu\"},{\"authorId\":\"2881425\",\"name\":\"Balaji Vasan Srinivasan\"},{\"authorId\":\"1704709\",\"name\":\"Vasudeva Varma\"}],\"doi\":\"10.18653/v1/D18-2028\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"befdcd19793a4613e395e4cebf5fa715be1c74dd\",\"title\":\"When science journalism meets artificial intelligence : An interactive demonstration\",\"url\":\"https://www.semanticscholar.org/paper/befdcd19793a4613e395e4cebf5fa715be1c74dd\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1812.03849\",\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1688516\",\"name\":\"Jingdong Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"title\":\"Weakly Supervised Dense Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1907.05671\",\"authors\":[{\"authorId\":\"46241709\",\"name\":\"Graham Spinks\"},{\"authorId\":\"145446752\",\"name\":\"Marie-Francine Moens\"}],\"doi\":\"10.1016/j.jbi.2019.103248\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4d1d1c06a87f6a40d1e51d8f9554181b6e4f93d6\",\"title\":\"Justifying Diagnosis Decisions by Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/4d1d1c06a87f6a40d1e51d8f9554181b6e4f93d6\",\"venue\":\"J. Biomed. Informatics\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31468750\",\"name\":\"J. Ye\"},{\"authorId\":\"145834008\",\"name\":\"Le Dong\"},{\"authorId\":\"49191636\",\"name\":\"Wenpu Dong\"},{\"authorId\":\"145901246\",\"name\":\"Ning Feng\"},{\"authorId\":\"145002061\",\"name\":\"N. Zhang\"}],\"doi\":\"10.1145/3321408.3322623\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ef5424c6cb47e17b9aeba447289af8aa1705c339\",\"title\":\"Policy multi-region integration for video description\",\"url\":\"https://www.semanticscholar.org/paper/ef5424c6cb47e17b9aeba447289af8aa1705c339\",\"venue\":\"ACM TUR-C\",\"year\":2019},{\"arxivId\":\"1810.11735\",\"authors\":[{\"authorId\":\"32251567\",\"name\":\"Shikib Mehri\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a82034bd78ee09117baa35ab23b9d600a7509167\",\"title\":\"Middle-Out Decoding\",\"url\":\"https://www.semanticscholar.org/paper/a82034bd78ee09117baa35ab23b9d600a7509167\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1430838755\",\"name\":\"Chien-Yao Wang\"},{\"authorId\":\"1490933657\",\"name\":\"Pei-Sin Liaw\"},{\"authorId\":\"2375118\",\"name\":\"Kai-Wen Liang\"},{\"authorId\":\"1519273435\",\"name\":\"Jai-Ching Wang\"},{\"authorId\":\"145456212\",\"name\":\"P. Chang\"}],\"doi\":\"10.1109/ICCE-Berlin47944.2019.8966173\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6578f9e1c222e2f7ff6ec0d50229f8e2391ec0c\",\"title\":\"Video Captioning Based on Joint Image\\u2013Audio Deep Learning Techniques\",\"url\":\"https://www.semanticscholar.org/paper/a6578f9e1c222e2f7ff6ec0d50229f8e2391ec0c\",\"venue\":\"2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3449429\",\"name\":\"Alexander Kuhnle\"}],\"doi\":\"10.17863/CAM.49177\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c04262cb3f76ff769af32afad05263bd47ebef18\",\"title\":\"Evaluating visually grounded language capabilities using microworlds\",\"url\":\"https://www.semanticscholar.org/paper/c04262cb3f76ff769af32afad05263bd47ebef18\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"151472634\",\"name\":\"Y. Hua\"},{\"authorId\":\"46406827\",\"name\":\"N. Robertson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9f964bc4f8405491bc69c9e03e6f466ec609b9dd\",\"title\":\"SC-RANK: Improving Convolutional Image Captioning with Self-Critical Learning and Ranking Metric-based Reward\",\"url\":\"https://www.semanticscholar.org/paper/9f964bc4f8405491bc69c9e03e6f466ec609b9dd\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"2011.04592\",\"authors\":[{\"authorId\":\"41071249\",\"name\":\"Ece Takmaz\"},{\"authorId\":\"3422247\",\"name\":\"Sandro Pezzelle\"},{\"authorId\":\"2752573\",\"name\":\"Lisa Beinborn\"},{\"authorId\":\"144151273\",\"name\":\"R. Fern\\u00e1ndez\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.377\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"306ec4956aa2bb4e29a0b5c8b52d1c0e6007a32b\",\"title\":\"Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze\",\"url\":\"https://www.semanticscholar.org/paper/306ec4956aa2bb4e29a0b5c8b52d1c0e6007a32b\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2007.10662\",\"authors\":[{\"authorId\":\"69544685\",\"name\":\"Jie Wu\"},{\"authorId\":\"1765674\",\"name\":\"Tianshui Chen\"},{\"authorId\":\"1721715\",\"name\":\"Hefeng Wu\"},{\"authorId\":\"10665619\",\"name\":\"Z. Yang\"},{\"authorId\":\"1773818\",\"name\":\"Guangchun Luo\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/tmm.2020.3011317\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f0edc8eb2d2b53482faf34906dacf996bc4127f6\",\"title\":\"Fine-Grained Image Captioning with Global-Local Discriminative Objective\",\"url\":\"https://www.semanticscholar.org/paper/f0edc8eb2d2b53482faf34906dacf996bc4127f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.09049\",\"authors\":[{\"authorId\":\"1810689822\",\"name\":\"Ganchao Tan\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.24963/ijcai.2020/104\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"title\":\"Learning to Discretely Compose Reasoning Module Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"1806.06004\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ceabd7ff28ce2d501511da998252aeb938adc98b\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ceabd7ff28ce2d501511da998252aeb938adc98b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37414303\",\"name\":\"Cheolho Han\"},{\"authorId\":\"3226948\",\"name\":\"Sang-Woo Lee\"},{\"authorId\":\"15353659\",\"name\":\"Yu-Jung Heo\"},{\"authorId\":\"32849162\",\"name\":\"W. Kang\"},{\"authorId\":\"29818400\",\"name\":\"Jaehyun Jun\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"68c6df1249e1ee56835f79e1877506a16d8418f4\",\"title\":\"Criteria for Human-Compatible AI in Two-Player Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/68c6df1249e1ee56835f79e1877506a16d8418f4\",\"venue\":\"LaCATODA@IJCAI\",\"year\":2017},{\"arxivId\":\"2012.02206\",\"authors\":[{\"authorId\":\"73286206\",\"name\":\"Dave Zhenyu Chen\"},{\"authorId\":\"47621053\",\"name\":\"A. Gholami\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"},{\"authorId\":\"3317599\",\"name\":\"A. X. Chang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7a4ba78d377eea9650e5e399a0878e30bd22f648\",\"title\":\"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans\",\"url\":\"https://www.semanticscholar.org/paper/7a4ba78d377eea9650e5e399a0878e30bd22f648\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.03184\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"22205368\",\"name\":\"Akshay Chaturvedi\"},{\"authorId\":\"143899054\",\"name\":\"Mark Dras\"},{\"authorId\":\"48421321\",\"name\":\"U. Garain\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf2f8c8686cff2643270ed2d61f12d4661cbb75a\",\"title\":\"Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/cf2f8c8686cff2643270ed2d61f12d4661cbb75a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":\"150164718\",\"name\":\"Jialong Tang\"},{\"authorId\":null,\"name\":\"Ziyao Lu\"},{\"authorId\":\"3194601\",\"name\":\"Xianpei Han\"},{\"authorId\":\"2999092\",\"name\":\"H. Zhang\"}],\"doi\":\"10.1016/J.NEUCOM.2019.08.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2fb096118ebbe6615afe98c733c80f55bb40d2cf\",\"title\":\"A neural image captioning model with caption-to-images semantic constructor\",\"url\":\"https://www.semanticscholar.org/paper/2fb096118ebbe6615afe98c733c80f55bb40d2cf\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"1902.03751\",\"authors\":[{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1774780\",\"name\":\"Yilin Shen\"},{\"authorId\":\"1705713\",\"name\":\"Hongxia Jin\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/ICCV.2019.00268\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab9ac4bea4ee49fd879337ac477dc09de4914fbf\",\"title\":\"Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded\",\"url\":\"https://www.semanticscholar.org/paper/ab9ac4bea4ee49fd879337ac477dc09de4914fbf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1611.08002\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"143690259\",\"name\":\"K. Tran\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/CVPR.2017.127\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"778ce81457383bd5e3fdb11b145ded202ebb4970\",\"title\":\"Semantic Compositional Networks for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/778ce81457383bd5e3fdb11b145ded202ebb4970\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"40454588\",\"name\":\"J. Frahm\"}],\"doi\":\"10.1007/978-3-030-58604-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0cd444d79c4b00cf824a8f2dbb16467727c729d9\",\"title\":\"Computer Vision \\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\\u201328, 2020, Proceedings, Part XXVIII\",\"url\":\"https://www.semanticscholar.org/paper/0cd444d79c4b00cf824a8f2dbb16467727c729d9\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1711.07373\",\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dc07e7bf1cf25aab2c39c85ab03bf085bbca31b5\",\"title\":\"Attentive Explanations: Justifying Decisions and Pointing to the Evidence\",\"url\":\"https://www.semanticscholar.org/paper/dc07e7bf1cf25aab2c39c85ab03bf085bbca31b5\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1806.08251\",\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":\"10.1109/WACV45572.2020.9093612\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dbbcf8af70db7533f76e8e55f108fcc6af6e0c0e\",\"title\":\"Learning Multimodal Representations for Unseen Activities\",\"url\":\"https://www.semanticscholar.org/paper/dbbcf8af70db7533f76e8e55f108fcc6af6e0c0e\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1907.10863\",\"authors\":[{\"authorId\":\"12795006\",\"name\":\"Xiaotao Song\"},{\"authorId\":\"145631821\",\"name\":\"Hailong Sun\"},{\"authorId\":\"50142091\",\"name\":\"X. Wang\"},{\"authorId\":\"8247676\",\"name\":\"Jiafei Yan\"}],\"doi\":\"10.1109/ACCESS.2019.2931579\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c427f4dc50cc4016913a697a4de21097842cf09\",\"title\":\"A Survey of Automatic Generation of Source Code Comments: Algorithms and Techniques\",\"url\":\"https://www.semanticscholar.org/paper/1c427f4dc50cc4016913a697a4de21097842cf09\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405511901\",\"name\":\"Luis Fernando D'Haro\"},{\"authorId\":\"1694652\",\"name\":\"Rafael E. Banchs\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"71200803\",\"name\":\"H. Li\"}],\"doi\":\"10.1016/j.csl.2018.12.004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e75d147f483953924ab3ecdd8aa40208199e6c4\",\"title\":\"Automatic evaluation of end-to-end dialog systems with adequacy-fluency metrics\",\"url\":\"https://www.semanticscholar.org/paper/1e75d147f483953924ab3ecdd8aa40208199e6c4\",\"venue\":\"Comput. Speech Lang.\",\"year\":2019},{\"arxivId\":\"1711.06232\",\"authors\":[{\"authorId\":\"2973866\",\"name\":\"J. Huang\"},{\"authorId\":\"3409955\",\"name\":\"C. D. Dao\"},{\"authorId\":\"9951160\",\"name\":\"Modar Alfadly\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1609/aaai.v33i01.33018449\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a26d01dc2b9a4e435c015914690354f67a9b6a08\",\"title\":\"A Novel Framework for Robustness Analysis of Visual QA Models\",\"url\":\"https://www.semanticscholar.org/paper/a26d01dc2b9a4e435c015914690354f67a9b6a08\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143905586\",\"name\":\"Cuirong Long\"},{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1007/s11042-019-7441-7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f147a64982b25ea28a2b0737c68b0f65fdb46bd8\",\"title\":\"Cross-domain personalized image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f147a64982b25ea28a2b0737c68b0f65fdb46bd8\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"2004.14231\",\"authors\":[{\"authorId\":\"47287647\",\"name\":\"Sen He\"},{\"authorId\":\"3157379\",\"name\":\"Wentong Liao\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"1779035\",\"name\":\"B. Rosenhahn\"},{\"authorId\":\"3251678\",\"name\":\"N. Pugeault\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"657cce51f80e272373ab4fc0dabf5dc8b30c0070\",\"title\":\"Image Captioning through Image Transformer\",\"url\":\"https://www.semanticscholar.org/paper/657cce51f80e272373ab4fc0dabf5dc8b30c0070\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10109256\",\"name\":\"C. Lee\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"3192572\",\"name\":\"Emiel van Miltenburg\"},{\"authorId\":\"145210073\",\"name\":\"E. Krahmer\"}],\"doi\":\"10.1016/j.csl.2020.101151\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d7ed6580a26d2ecb8282e81c8649e193e67d81f\",\"title\":\"Human evaluation of automatically generated text: Current trends and best practice guidelines\",\"url\":\"https://www.semanticscholar.org/paper/8d7ed6580a26d2ecb8282e81c8649e193e67d81f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.03829\",\"authors\":[{\"authorId\":\"100466830\",\"name\":\"Zhaojiang Lin\"},{\"authorId\":\"3064807\",\"name\":\"Andrea Madotto\"},{\"authorId\":\"40539650\",\"name\":\"Pascale Fung\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.41\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d49fc81190a65696862e6e88ff80c90c0f88ba01\",\"title\":\"Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/d49fc81190a65696862e6e88ff80c90c0f88ba01\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"19218522\",\"name\":\"Touseef Iqbal\"},{\"authorId\":\"46892607\",\"name\":\"S. Qureshi\"}],\"doi\":\"10.1016/j.jksuci.2020.04.001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"29c3ed6c15fdfab4109f8c8a4347f458c5dac69d\",\"title\":\"The survey: Text generation models in deep learning\",\"url\":\"https://www.semanticscholar.org/paper/29c3ed6c15fdfab4109f8c8a4347f458c5dac69d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"},{\"authorId\":\"51172982\",\"name\":\"Haiwen Cao\"},{\"authorId\":\"134473682\",\"name\":\"Mingwen Shao\"},{\"authorId\":\"46740305\",\"name\":\"Chunlei Wu\"}],\"doi\":\"10.1016/j.neucom.2019.12.073\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"50260b547a481a95a2346759da8dba9366e89348\",\"title\":\"Multi-Attention Generative Adversarial Network for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/50260b547a481a95a2346759da8dba9366e89348\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1912.08960\",\"authors\":[{\"authorId\":\"8716902\",\"name\":\"Huiyuan Xie\"},{\"authorId\":\"20662387\",\"name\":\"Tom Sherborne\"},{\"authorId\":\"3449429\",\"name\":\"Alexander Kuhnle\"},{\"authorId\":\"15379653\",\"name\":\"A. Copestake\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3afabdc3650ef93c771262f31db8ee144d0ff44\",\"title\":\"Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity\",\"url\":\"https://www.semanticscholar.org/paper/a3afabdc3650ef93c771262f31db8ee144d0ff44\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"31790073\",\"name\":\"C. Buehler\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"46878216\",\"name\":\"M. Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a79b694bd4ef51207787da1948ed473903b751ef\",\"title\":\"Bottom-Up and Top-Down Attention for Image Captioning and VQA\",\"url\":\"https://www.semanticscholar.org/paper/a79b694bd4ef51207787da1948ed473903b751ef\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2016.597\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b65faba7088864e134e7eb3b68c8e2f18cc5b4f6\",\"title\":\"Situation Recognition: Visual Semantic Role Labeling for Image Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b65faba7088864e134e7eb3b68c8e2f18cc5b4f6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1806.01954\",\"authors\":[{\"authorId\":\"51011850\",\"name\":\"Iulia Duta\"},{\"authorId\":\"50986865\",\"name\":\"A. Nicolicioiu\"},{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c42f427b54ab12a1d89827ee4c6951efae733b55\",\"title\":\"Mining for meaning: from vision to language through multiple networks consensus\",\"url\":\"https://www.semanticscholar.org/paper/c42f427b54ab12a1d89827ee4c6951efae733b55\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"46447554\",\"name\":\"Xiaodong Zhang\"},{\"authorId\":\"48593034\",\"name\":\"Guohua Dong\"},{\"authorId\":\"145254061\",\"name\":\"Z. Luo\"}],\"doi\":\"10.1109/ACCESS.2019.2917979\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5a759bbf042fe8852f23567cf14daf3d9f180bc\",\"title\":\"Cascade Semantic Fusion for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a759bbf042fe8852f23567cf14daf3d9f180bc\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"31ea3186aa7072a9e25218efe229f5ee3cca3316\",\"title\":\"A ug 2 01 7 Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/31ea3186aa7072a9e25218efe229f5ee3cca3316\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1904.06861\",\"authors\":[{\"authorId\":\"2903539\",\"name\":\"Junlong Gao\"},{\"authorId\":\"1705047\",\"name\":\"S. Wang\"},{\"authorId\":\"1755176\",\"name\":\"Shanshe Wang\"},{\"authorId\":\"10634370\",\"name\":\"S. Ma\"},{\"authorId\":\"48385803\",\"name\":\"W. Gao\"}],\"doi\":\"10.1109/CVPR.2019.00646\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b6b3d6a37e7e77f5d5c763a4abeade256324268c\",\"title\":\"Self-Critical N-Step Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b6b3d6a37e7e77f5d5c763a4abeade256324268c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49776272\",\"name\":\"Pengfei Cao\"},{\"authorId\":\"47087612\",\"name\":\"Z. Yang\"},{\"authorId\":\"144526347\",\"name\":\"L. Sun\"},{\"authorId\":\"145629787\",\"name\":\"Y. Liang\"},{\"authorId\":\"1717198\",\"name\":\"M. Yang\"},{\"authorId\":\"144479376\",\"name\":\"Renchu Guan\"}],\"doi\":\"10.1007/s11063-018-09973-5\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"16efc1a4c7cdf8bc6d49cc326542e1fe1c88456b\",\"title\":\"Image Captioning with Bidirectional Semantic Attention-Based Guiding of Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/16efc1a4c7cdf8bc6d49cc326542e1fe1c88456b\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3428883\",\"name\":\"Zhengxia Gao\"},{\"authorId\":\"3335651\",\"name\":\"Aiwen Jiang\"},{\"authorId\":\"33876479\",\"name\":\"Jianyi Wan\"}],\"doi\":\"10.1117/12.2502901\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2f7b31b06ebdc65a295dff62bae960d5458def3c\",\"title\":\"Explore fine-grained discriminative visual explanation when making classification decision\",\"url\":\"https://www.semanticscholar.org/paper/2f7b31b06ebdc65a295dff62bae960d5458def3c\",\"venue\":\"International Conference on Digital Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394455973\",\"name\":\"Johann Seltmann\"},{\"authorId\":\"3259225\",\"name\":\"Luca Ducceschi\"},{\"authorId\":\"3352951\",\"name\":\"Aur\\u00e9lie Herbelot\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"af712536464e0c12bf785177429c8ec3a639f802\",\"title\":\"How Much Competence Is There in Performance? Assessing the Distributional Hypothesis in Word Bigrams\",\"url\":\"https://www.semanticscholar.org/paper/af712536464e0c12bf785177429c8ec3a639f802\",\"venue\":\"CLiC-it\",\"year\":2019},{\"arxivId\":\"1908.10899\",\"authors\":[{\"authorId\":\"3355010\",\"name\":\"Greg Casta\\u00f1\\u00f3n\"},{\"authorId\":\"2962929\",\"name\":\"Nathan Shnidman\"},{\"authorId\":\"50780334\",\"name\":\"Tim Anderson\"},{\"authorId\":\"145607579\",\"name\":\"Jeffrey Byrne\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8eae86794cb94adfc3022a6929d58c961ad73c1c\",\"title\":\"Out the Window: A Crowd-Sourced Dataset for Activity Classification in Surveillance Video\",\"url\":\"https://www.semanticscholar.org/paper/8eae86794cb94adfc3022a6929d58c961ad73c1c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1901.02527\",\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"17420047ab3c64ebbb63dcf21931d26fa77955d6\",\"title\":\"Viewpoint Invariant Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/17420047ab3c64ebbb63dcf21931d26fa77955d6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5486222\",\"name\":\"Javedul Ferdous\"},{\"authorId\":\"144894381\",\"name\":\"A. S. Saif\"},{\"authorId\":\"152356279\",\"name\":\"D. Nandi\"},{\"authorId\":\"144593525\",\"name\":\"Mashiour Rahman\"}],\"doi\":\"10.5120/ijca2018918048\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ebbb0729c7c6459993296a307ca11d9f9ac80e94\",\"title\":\"An Efficient Hybrid Architecture for Visual Behavior Recognition using Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/ebbb0729c7c6459993296a307ca11d9f9ac80e94\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1904.03366\",\"authors\":[{\"authorId\":\"93872817\",\"name\":\"Yatri Modi\"},{\"authorId\":\"2326758\",\"name\":\"Natalie Parde\"}],\"doi\":\"10.18653/v1/W19-1805\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"8a1269edf443cce55ad8de22822ec65a88a9db49\",\"title\":\"The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models\",\"url\":\"https://www.semanticscholar.org/paper/8a1269edf443cce55ad8de22822ec65a88a9db49\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1712.09532\",\"authors\":[{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"2763884\",\"name\":\"G. Henter\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"1700567\",\"name\":\"Shin'ichi Satoh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"96c866f07ff999ee11459519aa361fa4fdfc2139\",\"title\":\"Consensus-based Sequence Training for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96c866f07ff999ee11459519aa361fa4fdfc2139\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3014143\",\"name\":\"Semih Yavuz\"},{\"authorId\":\"2531558\",\"name\":\"G. Chao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be36d8e7e89fcdf06215ff27f983e9ba3a5c86f3\",\"title\":\"Grounded Response Generation with Hierarchical Pointer Networks\",\"url\":\"https://www.semanticscholar.org/paper/be36d8e7e89fcdf06215ff27f983e9ba3a5c86f3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51444178\",\"name\":\"K. P. Korshunova\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c22aa66b2fad7ed31252b5e6276a0df371c57b4c\",\"title\":\"The neural network image captioning model based on adversarial training\",\"url\":\"https://www.semanticscholar.org/paper/c22aa66b2fad7ed31252b5e6276a0df371c57b4c\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2311107\",\"name\":\"Ambar Pal\"},{\"authorId\":\"144054466\",\"name\":\"G. Sharma\"},{\"authorId\":\"145676235\",\"name\":\"C. Arora\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"135273ea1521236e96bd79a319bf9386969bae5f\",\"title\":\"Exploiting independent visual and textual data sources to improve multi-modal methods for description and querying of visual data\",\"url\":\"https://www.semanticscholar.org/paper/135273ea1521236e96bd79a319bf9386969bae5f\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"C. Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145254492\",\"name\":\"Z. Wen\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0ed94607d6f1506f0cfb1bf0896a41300db52a1a\",\"title\":\"Nested-Wasserstein Distance for Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/0ed94607d6f1506f0cfb1bf0896a41300db52a1a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177145\",\"name\":\"M. Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dadb7ddfde3478238d23a8bacf5eddecc59e84c9\",\"title\":\"Vocabulary Image Captioning with Constrained Beam Search\",\"url\":\"https://www.semanticscholar.org/paper/dadb7ddfde3478238d23a8bacf5eddecc59e84c9\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"108358199\",\"name\":\"Viktar Atliha\"},{\"authorId\":\"1990294\",\"name\":\"D. Sesok\"}],\"doi\":\"10.1109/eStream50540.2020.9108880\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"08aa7d4d4e7fe6637ab83b8f8ae4144da517cdd6\",\"title\":\"Comparison of VGG and ResNet used as Encoders for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/08aa7d4d4e7fe6637ab83b8f8ae4144da517cdd6\",\"venue\":\"2020 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"2751871\",\"name\":\"J. Wan\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":\"10.1109/tpami.2020.3013834\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ba341f992ceb10d9a7f032ac6027f18ef5e5f895\",\"title\":\"On Diversity in Image Captioning: Metrics and Methods.\",\"url\":\"https://www.semanticscholar.org/paper/ba341f992ceb10d9a7f032ac6027f18ef5e5f895\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1726159226\",\"name\":\"Xiangqing Shen\"},{\"authorId\":\"47655360\",\"name\":\"B. Liu\"},{\"authorId\":\"1697439\",\"name\":\"Yong Zhou\"},{\"authorId\":\"1445303213\",\"name\":\"Jiaqi Zhao\"},{\"authorId\":\"49353948\",\"name\":\"Mingming Liu\"}],\"doi\":\"10.1016/j.knosys.2020.105920\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"46fd9996cd945983f5acc9ed073dc244d6d2f32e\",\"title\":\"Remote sensing image captioning via Variational Autoencoder and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/46fd9996cd945983f5acc9ed073dc244d6d2f32e\",\"venue\":\"Knowl. Based Syst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"1791344388\",\"name\":\"Lei Ji\"},{\"authorId\":\"1783553\",\"name\":\"Zhen-dong Niu\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/3394171.3413498\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"title\":\"Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning\",\"url\":\"https://www.semanticscholar.org/paper/de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443435125\",\"name\":\"Zhengcong Fei\"}],\"doi\":\"10.1145/3394171.3413901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0bcec12a99cfcd649ea36d6b7215d025bad12974\",\"title\":\"Iterative Back Modification for Faster Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0bcec12a99cfcd649ea36d6b7215d025bad12974\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41071249\",\"name\":\"Ece Takmaz\"},{\"authorId\":\"24068173\",\"name\":\"Mario Giulianelli\"},{\"authorId\":\"3422247\",\"name\":\"Sandro Pezzelle\"},{\"authorId\":\"153915609\",\"name\":\"Arabella Sinclair\"},{\"authorId\":\"144151273\",\"name\":\"R. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35490092cd5295a4552a6b46b0ec8372beb87089\",\"title\":\"Refer, Reuse, Reduce: Grounding Subsequent References in Visual and Conversational Contexts\",\"url\":\"https://www.semanticscholar.org/paper/35490092cd5295a4552a6b46b0ec8372beb87089\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145116511\",\"name\":\"Jan Deriu\"},{\"authorId\":\"2648584\",\"name\":\"Mark Cieliebak\"}],\"doi\":\"10.21256/ZHAW-4889\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"222aca2014dea760f5a93c21df68744be65ab94e\",\"title\":\"End-to-end trainable system for enhancing diversity in natural language generation\",\"url\":\"https://www.semanticscholar.org/paper/222aca2014dea760f5a93c21df68744be65ab94e\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"2002.12204\",\"authors\":[{\"authorId\":\"134814700\",\"name\":\"T. Wang\"},{\"authorId\":\"50560698\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"32222907\",\"name\":\"Qianru Sun\"}],\"doi\":\"10.1109/cvpr42600.2020.01077\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d9d0197c6d9304d6bc179ac21370c49a33fc2388\",\"title\":\"Visual Commonsense R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/d9d0197c6d9304d6bc179ac21370c49a33fc2388\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390626344\",\"name\":\"Zhiyuan Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"title\":\"Cross-Modal Representation\",\"url\":\"https://www.semanticscholar.org/paper/7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1810.04700\",\"authors\":[{\"authorId\":\"3159346\",\"name\":\"Sebastian Gehrmann\"},{\"authorId\":\"24881182\",\"name\":\"Falcon Z. Dai\"},{\"authorId\":\"40895152\",\"name\":\"Henry Elder\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":\"10.18653/v1/W18-6505\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2de437173b4482b66199c49135a9526db38f68ac\",\"title\":\"End-to-End Content and Plan Selection for Data-to-Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/2de437173b4482b66199c49135a9526db38f68ac\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":\"1805.00063\",\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"39320489\",\"name\":\"J. Ross\"},{\"authorId\":\"2500466\",\"name\":\"Tom Sercu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"240ed539e96b9a304de54c904b375a9870496bf7\",\"title\":\"Improved Image Captioning with Adversarial Semantic Alignment\",\"url\":\"https://www.semanticscholar.org/paper/240ed539e96b9a304de54c904b375a9870496bf7\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4421716\",\"name\":\"F. Wang\"},{\"authorId\":\"21776508\",\"name\":\"X. Gong\"},{\"authorId\":\"1754542\",\"name\":\"Linpeng Huang\"}],\"doi\":\"10.1109/ICPR.2018.8545355\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1a1a77e953736ce25949395d5856fc1762bbf8c6\",\"title\":\"Time-Dependent Pre-attention Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a1a77e953736ce25949395d5856fc1762bbf8c6\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":\"1706.08474\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1145/3177745\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f3ccdf54a3384e601fa36969e7b3f657e2516a3b\",\"title\":\"Paying More Attention to Saliency\",\"url\":\"https://www.semanticscholar.org/paper/f3ccdf54a3384e601fa36969e7b3f657e2516a3b\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2018},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49292427\",\"name\":\"B. Wang\"},{\"authorId\":\"3429418\",\"name\":\"Cun-gang Wang\"},{\"authorId\":\"47834797\",\"name\":\"Qian Zhang\"},{\"authorId\":\"1749725513\",\"name\":\"Ying Su\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"48615794\",\"name\":\"Yanyan Xu\"}],\"doi\":\"10.1109/ACCESS.2020.2999568\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1fac7ac22af5db06edde0fbda2bc61a97c7c9625\",\"title\":\"Cross-Lingual Image Caption Generation Based on Visual Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/1fac7ac22af5db06edde0fbda2bc61a97c7c9625\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"35084211\",\"name\":\"M. Lewis\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D18-1117\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"title\":\"A Dataset for Telling the Stories of Social Media Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"title\":\"Empirical performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97667801\",\"name\":\"Matthew John Marter\"}],\"doi\":\"10.15126/THESIS.00850052\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a0f45a806a38bf38511a9f2fd774d7a18d0fc3b3\",\"title\":\"Learning to recognise visual content from textual annotation\",\"url\":\"https://www.semanticscholar.org/paper/a0f45a806a38bf38511a9f2fd774d7a18d0fc3b3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1904.00767\",\"authors\":[{\"authorId\":\"1807405\",\"name\":\"S. Chen\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"}],\"doi\":\"10.1007/978-3-030-01252-6_5\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5cab3ce511ec8345d16a28c00094a2800b3919ce\",\"title\":\"Boosted Attention: Leveraging Human Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5cab3ce511ec8345d16a28c00094a2800b3919ce\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"1693997\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1145/3240508.3240538\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"72f9116a04e584081635500e9f0789fa26e4d15f\",\"title\":\"Hierarchical Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72f9116a04e584081635500e9f0789fa26e4d15f\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2692833\",\"name\":\"Wenbin Che\"},{\"authorId\":\"1800904\",\"name\":\"X. Fan\"},{\"authorId\":\"145419122\",\"name\":\"R. Xiong\"},{\"authorId\":\"1725937\",\"name\":\"D. Zhao\"}],\"doi\":\"10.1145/3240508.3240695\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0931be61bde7706ad4c10c556495b72effaef820\",\"title\":\"Paragraph Generation Network with Visual Relationship Detection\",\"url\":\"https://www.semanticscholar.org/paper/0931be61bde7706ad4c10c556495b72effaef820\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2720656\",\"name\":\"Danushka Bollegala\"},{\"authorId\":\"1666206198\",\"name\":\"James O'Neill\"}],\"doi\":\"10.1007/978-981-15-6168-9_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18d168eba4fc776dbbb7f55e58d7895fa29a0f99\",\"title\":\"Learning to Evaluate Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/18d168eba4fc776dbbb7f55e58d7895fa29a0f99\",\"venue\":\"PACLING\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"},{\"authorId\":\"50763020\",\"name\":\"Jingwen Chen\"},{\"authorId\":\"41079034\",\"name\":\"Hong-Yang Chao\"},{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"1394465427\",\"name\":\"Qiuyu Cai\"}],\"doi\":\"10.1145/3394171.3416290\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93aed487e9b9f51bf05803ef69c92599001358ac\",\"title\":\"XlanV Model with Adaptively Multi-Modality Feature Fusing for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93aed487e9b9f51bf05803ef69c92599001358ac\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1711.02578\",\"authors\":[{\"authorId\":\"143652809\",\"name\":\"Octavio Arriaga\"},{\"authorId\":\"79429761\",\"name\":\"P. Pl\\u00f6ger\"},{\"authorId\":\"1399503242\",\"name\":\"Matias Valdenegro-Toro\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"8a77b0e52e07edc501473410a4bdcf32ab11a00b\",\"title\":\"Image Captioning and Classification of Dangerous Situations\",\"url\":\"https://www.semanticscholar.org/paper/8a77b0e52e07edc501473410a4bdcf32ab11a00b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"2011.00597\",\"authors\":[{\"authorId\":\"2007582232\",\"name\":\"Simon Ging\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"1835025\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"80089ad641bae28b0e57771afef181b60011069e\",\"title\":\"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/80089ad641bae28b0e57771afef181b60011069e\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yongzhen Wang\"},{\"authorId\":\"7898160\",\"name\":\"J. Wang\"},{\"authorId\":\"1748032\",\"name\":\"Heng Huang\"},{\"authorId\":\"49404593\",\"name\":\"Hongsong Li\"},{\"authorId\":\"1713802\",\"name\":\"Xiaozhong Liu\"}],\"doi\":\"10.1145/3397271.3401140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7080d224ab4cb1c9dd52017d4d7d8b0d0214b9a\",\"title\":\"Evolutionary Product Description Generation: A Dynamic Fine-Tuning Approach Leveraging User Click Behavior\",\"url\":\"https://www.semanticscholar.org/paper/f7080d224ab4cb1c9dd52017d4d7d8b0d0214b9a\",\"venue\":\"SIGIR\",\"year\":2020},{\"arxivId\":\"2006.13608\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"50144812\",\"name\":\"Z. Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394486.3403325\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d271e93c7566b231e560c48b4cc4942077d762f9\",\"title\":\"Comprehensive Information Integration Modeling Framework for Video Titling\",\"url\":\"https://www.semanticscholar.org/paper/d271e93c7566b231e560c48b4cc4942077d762f9\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"2010.10095\",\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.145\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f4a2acfeb1705df3f430cc53ace26e1dbbbcbd16\",\"title\":\"BiST: Bi-directional Spatio-Temporal Reasoning for Video-Grounded Dialogues\",\"url\":\"https://www.semanticscholar.org/paper/f4a2acfeb1705df3f430cc53ace26e1dbbbcbd16\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144755224\",\"name\":\"Y. Xiao\"},{\"authorId\":\"2807642\",\"name\":\"T. Lu\"}],\"doi\":\"10.1109/CRC51253.2020.9253446\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2612d66f919e534c10786f1d001485e5522bb2bb\",\"title\":\"An Improved Method of Cross-Lingual Image Captioning Based on Fluency-Guided\",\"url\":\"https://www.semanticscholar.org/paper/2612d66f919e534c10786f1d001485e5522bb2bb\",\"venue\":\"2020 5th International Conference on Control, Robotics and Cybernetics (CRC)\",\"year\":2020},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.13913\",\"authors\":[{\"authorId\":\"144568152\",\"name\":\"David M. Chan\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ac8179dbdd256e514700b076673b6d5b9083251\",\"title\":\"Active Learning for Video Description With Cluster-Regularized Ensemble Ranking\",\"url\":\"https://www.semanticscholar.org/paper/8ac8179dbdd256e514700b076673b6d5b9083251\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144101175\",\"name\":\"Zhou Lei\"},{\"authorId\":\"1831825\",\"name\":\"Congcong Zhou\"},{\"authorId\":\"35155467\",\"name\":\"Shengbo Chen\"},{\"authorId\":\"1722738282\",\"name\":\"Yiyong Huang\"},{\"authorId\":\"1726027121\",\"name\":\"Xianrui Liu\"}],\"doi\":\"10.1109/ACCESS.2020.3024639\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e85fdae1b5e297f1d8925c9bd5cd7e243a305116\",\"title\":\"A Sparse Transformer-Based Approach for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e85fdae1b5e297f1d8925c9bd5cd7e243a305116\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1704.06972\",\"authors\":[{\"authorId\":\"35259685\",\"name\":\"Y. Wang\"},{\"authorId\":\"145527707\",\"name\":\"Zhe L. Lin\"},{\"authorId\":\"1720987\",\"name\":\"X. Shen\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"48524582\",\"name\":\"G. Cottrell\"}],\"doi\":\"10.1109/CVPR.2017.780\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1de837d76d6513aa664f2d032d49ef12c938bbbb\",\"title\":\"Skeleton Key: Image Captioning by Skeleton-Attribute Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/1de837d76d6513aa664f2d032d49ef12c938bbbb\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"2008.01180\",\"authors\":[{\"authorId\":\"48251796\",\"name\":\"Chenyun Wu\"},{\"authorId\":\"143903550\",\"name\":\"M. Timm\"},{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"}],\"doi\":\"10.1007/978-3-030-58452-8_4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"86f0fb3791761cdb5e9108721a536d752641d4bf\",\"title\":\"Describing Textures using Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/86f0fb3791761cdb5e9108721a536d752641d4bf\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.09413\",\"authors\":[{\"authorId\":\"1999185950\",\"name\":\"Duvsan Varivs\"},{\"authorId\":\"1790811\",\"name\":\"Katsuhito Sudoh\"},{\"authorId\":\"50068540\",\"name\":\"S. Nakamura\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"97b9a163be6fe1924a366129395177991e5eb83d\",\"title\":\"Image Captioning with Visual Object Representations Grounded in the Textual Modality\",\"url\":\"https://www.semanticscholar.org/paper/97b9a163be6fe1924a366129395177991e5eb83d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.00120\",\"authors\":[{\"authorId\":\"2904100\",\"name\":\"Z. Han\"},{\"authorId\":\"102574232\",\"name\":\"C. Chen\"},{\"authorId\":\"46399275\",\"name\":\"Yu-Shen Liu\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"}],\"doi\":\"10.1145/3394171.3413889\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66e87d0adf7df03fdd74e6dfea81f21aa03afab0\",\"title\":\"ShapeCaptioner: Generative Caption Network for 3D Shapes by Learning a Mapping from Parts Detected in Multiple Views to Sentences\",\"url\":\"https://www.semanticscholar.org/paper/66e87d0adf7df03fdd74e6dfea81f21aa03afab0\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2009.13862\",\"authors\":[{\"authorId\":\"50232214\",\"name\":\"Wenjia Xu\"},{\"authorId\":\"51175635\",\"name\":\"Jiuniu Wang\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"3287827\",\"name\":\"Guangluan Xu\"},{\"authorId\":\"7413451\",\"name\":\"Daoyu Lin\"},{\"authorId\":\"1379498558\",\"name\":\"Wei Dai\"},{\"authorId\":\"48607717\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/JSTSP.2020.2987729\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c056c388f4deb0ff87932eb6e7789b23eb22d22\",\"title\":\"Where is the Model Looking At? \\u2013 Concentrate and Explain the Network Attention\",\"url\":\"https://www.semanticscholar.org/paper/7c056c388f4deb0ff87932eb6e7789b23eb22d22\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3372941\",\"name\":\"Rotem Dror\"},{\"authorId\":\"1659314445\",\"name\":\"Lotem Peled-Cohen\"},{\"authorId\":\"20513655\",\"name\":\"Segev Shlomov\"},{\"authorId\":\"1762757\",\"name\":\"Roi Reichart\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"10d13307242462903d3935e5cf73e65d366b130b\",\"title\":\"Statistical Significance Testing for Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/10d13307242462903d3935e5cf73e65d366b130b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2001.08779\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"50975843\",\"name\":\"Vinod K. Kurmi\"},{\"authorId\":null,\"name\":\"Sandeep Kumar\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":\"10.1109/WACV45572.2020.9093293\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e4720942ff8b02d2822aea5d024628139551723\",\"title\":\"Deep Bayesian Network for Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/8e4720942ff8b02d2822aea5d024628139551723\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2003.03983\",\"authors\":[{\"authorId\":\"1557300901\",\"name\":\"Mingshuang Luo\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0872f4a6e14c6da98424e6daefe4d9b626ba4d3d\",\"title\":\"Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading\",\"url\":\"https://www.semanticscholar.org/paper/0872f4a6e14c6da98424e6daefe4d9b626ba4d3d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1710.09511\",\"authors\":[{\"authorId\":\"3379664\",\"name\":\"S. Barratt\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"63199f9d0034e82a0a7c9519d1a5bd31cc9de39f\",\"title\":\"InterpNET: Neural Introspection for Interpretable Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/63199f9d0034e82a0a7c9519d1a5bd31cc9de39f\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1909.09788\",\"authors\":[{\"authorId\":\"1387971311\",\"name\":\"Somaye Jafaritazehjani\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"}],\"doi\":\"10.18653/v1/W19-8625\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6fca889a5d95fa2df47edbfd65f886f8bf4856fe\",\"title\":\"Visuallly Grounded Generation of Entailments from Premises\",\"url\":\"https://www.semanticscholar.org/paper/6fca889a5d95fa2df47edbfd65f886f8bf4856fe\",\"venue\":\"INLG\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3443433\",\"name\":\"William Boag\"},{\"authorId\":\"15743545\",\"name\":\"R. Campos\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1681193\",\"name\":\"Anna Rumshisky\"}],\"doi\":\"10.18653/v1/P16-1182\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d0631ba22add59684fff926d80d2e6948dfb7d7e\",\"title\":\"MUTT: Metric Unit TesTing for Language Generation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d0631ba22add59684fff926d80d2e6948dfb7d7e\",\"venue\":\"ACL\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christopher Elamri\"},{\"authorId\":null,\"name\":\"Teun de Planque\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cc61cd90529fede6e1bfb14042d021bc2a076e99\",\"title\":\"Automated Neural Image Caption Generator for Visually Impaired People\",\"url\":\"https://www.semanticscholar.org/paper/cc61cd90529fede6e1bfb14042d021bc2a076e99\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5813a4a0cca115b05e03d8d8c1ac8bf07176e96\",\"title\":\"Supplementary Material : Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/d5813a4a0cca115b05e03d8d8c1ac8bf07176e96\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2017491\",\"name\":\"J. Wu\"},{\"authorId\":\"1765674\",\"name\":\"Tianshui Chen\"},{\"authorId\":\"1721715\",\"name\":\"Hefeng Wu\"},{\"authorId\":\"20570336\",\"name\":\"Z. Yang\"},{\"authorId\":\"104002286\",\"name\":\"Qing Wang\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/ICME.2019.00227\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4e3b2d5e0dfffc088e75da297a77f63820cd3a31\",\"title\":\"Concrete Image Captioning by Integrating Content Sensitive and Global Discriminative Objective\",\"url\":\"https://www.semanticscholar.org/paper/4e3b2d5e0dfffc088e75da297a77f63820cd3a31\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"18d48837a300b68822b4d2366600e9628c53718a\",\"title\":\"nocaps: novel object captioning at scale Supplementary Material\",\"url\":\"https://www.semanticscholar.org/paper/18d48837a300b68822b4d2366600e9628c53718a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"title\":\"CLARA: Dynamic Doctor Representation Learning for Clinical Trial Recruitment\",\"url\":\"https://www.semanticscholar.org/paper/e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41050714\",\"name\":\"Sandeep Narayan Parameswaran\"},{\"authorId\":\"144009889\",\"name\":\"Sukhendu Das\"}],\"doi\":\"10.1145/3293353.3293391\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"acc9221fbfbfe52a6a81032ea02f42449fc1eca9\",\"title\":\"A Bottom-Up and Top-Down Approach for Image Captioning using Transformer\",\"url\":\"https://www.semanticscholar.org/paper/acc9221fbfbfe52a6a81032ea02f42449fc1eca9\",\"venue\":\"ICVGIP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"1687386\",\"name\":\"S. Kwong\"}],\"doi\":\"10.1016/j.neucom.2018.05.086\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6723a565d4d7bc221fff7160bebfe54d16a40607\",\"title\":\"Deep sequential fusion LSTM network for image description\",\"url\":\"https://www.semanticscholar.org/paper/6723a565d4d7bc221fff7160bebfe54d16a40607\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1611.02145\",\"authors\":[{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"},{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1561/0600000073\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"84c95a8db377c25d4280f188e9477569ab57281b\",\"title\":\"Crowdsourcing in Computer Vision\",\"url\":\"https://www.semanticscholar.org/paper/84c95a8db377c25d4280f188e9477569ab57281b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3456814\",\"name\":\"Yevgeniy Puzikov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d0eecf9ec112d7d8aba444bb2d25cadc7f1fc60a\",\"title\":\"NLG Challenge : Neural Models vs . Templates\",\"url\":\"https://www.semanticscholar.org/paper/d0eecf9ec112d7d8aba444bb2d25cadc7f1fc60a\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145271653\",\"name\":\"Sean C. Dougherty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cb53e4aad72c12c29a62bfd029b6626afeb25735\",\"title\":\"Partnering People with Deep Learning Systems: Human Cognitive Effects of Explanations\",\"url\":\"https://www.semanticscholar.org/paper/cb53e4aad72c12c29a62bfd029b6626afeb25735\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66230036\",\"name\":\"L. Sun\"},{\"authorId\":\"49337262\",\"name\":\"Weipeng Wang\"},{\"authorId\":\"39682947\",\"name\":\"Jiyun Li\"},{\"authorId\":\"13387006\",\"name\":\"Jingsheng Lin\"}],\"doi\":\"10.1007/978-3-030-26763-6_66\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a7983742d4e9bb354217c42b7aa27f45518916b\",\"title\":\"Study on Medical Image Report Generation Based on Improved Encoding-Decoding Method\",\"url\":\"https://www.semanticscholar.org/paper/4a7983742d4e9bb354217c42b7aa27f45518916b\",\"venue\":\"ICIC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2591941\",\"name\":\"Shyam Sundar Rajagopalan\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"},{\"authorId\":\"1756344\",\"name\":\"T. Baltrusaitis\"},{\"authorId\":\"1717204\",\"name\":\"R. Goecke\"}],\"doi\":\"10.1007/978-3-319-46478-7_21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"88e350a82fc6a30a33f231666455d5076f6c3731\",\"title\":\"Extending Long Short-Term Memory for Multi-View Structured Learning\",\"url\":\"https://www.semanticscholar.org/paper/88e350a82fc6a30a33f231666455d5076f6c3731\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1109/CVPR.2015.7298856\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a72b8bbd039989db39769da836cdb287737deb92\",\"title\":\"Mind's eye: A recurrent visual representation for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/a72b8bbd039989db39769da836cdb287737deb92\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40286455\",\"name\":\"Xuelong Li\"},{\"authorId\":\"46448616\",\"name\":\"Xueting Zhang\"},{\"authorId\":\"145909469\",\"name\":\"Wei Huang\"},{\"authorId\":\"50621207\",\"name\":\"Q. Wang\"}],\"doi\":\"10.1109/tgrs.2020.3010106\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f79bc9f9bf488f4580f1f4d9a360ac06a26e9e3e\",\"title\":\"Truncation Cross Entropy Loss for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f79bc9f9bf488f4580f1f4d9a360ac06a26e9e3e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1612.00234\",\"authors\":[{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"}],\"doi\":\"10.1162/tacl_a_00013\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5a732016c3f74dc7d78899bf33cf25df03ef46b4\",\"title\":\"Video Captioning with Multi-Faceted Attention\",\"url\":\"https://www.semanticscholar.org/paper/5a732016c3f74dc7d78899bf33cf25df03ef46b4\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150162416\",\"name\":\"Jialin Shao\"},{\"authorId\":\"150035131\",\"name\":\"A. Uchendu\"},{\"authorId\":\"2123319\",\"name\":\"Dongwon Lee\"}],\"doi\":\"10.1145/3292522.3326042\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ecc560b2299b0167b1b843a416094b70151d187f\",\"title\":\"A Reverse Turing Test for Detecting Machine-Made Texts\",\"url\":\"https://www.semanticscholar.org/paper/ecc560b2299b0167b1b843a416094b70151d187f\",\"venue\":\"WebSci\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46458416\",\"name\":\"L. Liu\"},{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"48263872\",\"name\":\"Ruiguo Zhang\"},{\"authorId\":\"1490736109\",\"name\":\"Yuxuan Ding\"}],\"doi\":\"10.1117/12.2557584\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d11973ef9567133ae8035dfbd4e96c87ca36505\",\"title\":\"Generating description with multi-feature and saliency maps of image\",\"url\":\"https://www.semanticscholar.org/paper/0d11973ef9567133ae8035dfbd4e96c87ca36505\",\"venue\":\"International Conference on Graphic and Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3257551\",\"name\":\"Xintong Yu\"},{\"authorId\":\"3441420\",\"name\":\"Tszhang Guo\"},{\"authorId\":\"145414746\",\"name\":\"Kun Fu\"},{\"authorId\":\"143900006\",\"name\":\"Lei Li\"},{\"authorId\":\"14966740\",\"name\":\"Changshui Zhang\"},{\"authorId\":\"1739414\",\"name\":\"Jianwei Zhang\"}],\"doi\":\"10.1109/IJCNN.2019.8851721\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f33c9e9e76e1d41d64d21c09ae434b05d83bc7f6\",\"title\":\"Image Captioning with Partially Rewarded Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/f33c9e9e76e1d41d64d21c09ae434b05d83bc7f6\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"143852685\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1145/2983563.2983571\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6d6edce271935feec96484d0e1f16dcc24973fd\",\"title\":\"Exploiting Scene Context for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d6d6edce271935feec96484d0e1f16dcc24973fd\",\"venue\":\"iV&L-MM@MM\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32211627\",\"name\":\"Raghuram Vadapalli\"},{\"authorId\":\"50708394\",\"name\":\"B. Syed\"},{\"authorId\":\"46971819\",\"name\":\"Nishant Prabhu\"},{\"authorId\":\"2881425\",\"name\":\"Balaji Vasan Srinivasan\"},{\"authorId\":\"1704709\",\"name\":\"Vasudeva Varma\"}],\"doi\":\"10.1145/3269206.3269303\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7660af2edc67bb3890704b2a9412f16806a4b66e\",\"title\":\"Sci-Blogger: A Step Towards Automated Science Journalism\",\"url\":\"https://www.semanticscholar.org/paper/7660af2edc67bb3890704b2a9412f16806a4b66e\",\"venue\":\"CIKM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"108124522\",\"name\":\"T. Fujii\"},{\"authorId\":\"1394541524\",\"name\":\"Ryohei Orihara\"},{\"authorId\":\"1768926\",\"name\":\"Yuichi Sei\"},{\"authorId\":\"1749328\",\"name\":\"Y. Tahara\"},{\"authorId\":\"1740433\",\"name\":\"A. Ohsuga\"}],\"doi\":\"10.1145/3378184.3378217\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a9fabeb62504598488f174f1b354d948d5dfe43\",\"title\":\"Generating Cooking Recipes from Cooking Videos Using Deep Learning Considering Previous Process with Video Encoding\",\"url\":\"https://www.semanticscholar.org/paper/1a9fabeb62504598488f174f1b354d948d5dfe43\",\"venue\":\"APPIS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1007/978-3-030-14657-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a9bae357bc63540cfbd630f4aaab4088edf2e0\",\"title\":\"Video Captioning Using Hierarchical LSTM and Text-Based Sliding Window\",\"url\":\"https://www.semanticscholar.org/paper/08a9bae357bc63540cfbd630f4aaab4088edf2e0\",\"venue\":\"IoTaaS\",\"year\":2018},{\"arxivId\":\"1807.09986\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1007/978-3-030-01216-8_31\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"title\":\"Recurrent Fusion Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"2011768695\",\"name\":\"Junjun Guo\"},{\"authorId\":\"2409659\",\"name\":\"Shengxiang Gao\"},{\"authorId\":\"121854326\",\"name\":\"Zhengtao Yu\"}],\"doi\":\"10.1016/j.patcog.2020.107702\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6686fadf7f7ef2283cc9286095db281f8520ec04\",\"title\":\"Enhancing the alignment between target words and corresponding frames for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/6686fadf7f7ef2283cc9286095db281f8520ec04\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":\"2012.01186\",\"authors\":[{\"authorId\":\"102432606\",\"name\":\"E. Li\"},{\"authorId\":\"145776789\",\"name\":\"Jingyi Su\"},{\"authorId\":\"46996133\",\"name\":\"Hao Sheng\"},{\"authorId\":\"49046714\",\"name\":\"L. Wai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1890aae22af719cc360155d9c3f54770c6351acd\",\"title\":\"AGenT Zero: Zero-shot Automatic Multiple-Choice Question Generation for Skill Assessments\",\"url\":\"https://www.semanticscholar.org/paper/1890aae22af719cc360155d9c3f54770c6351acd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3055959\",\"name\":\"Veena Thenkanidiyoor\"},{\"authorId\":\"47798961\",\"name\":\"R. Prasath\"},{\"authorId\":\"150255310\",\"name\":\"Odelu Vanga\"},{\"authorId\":\"145960032\",\"name\":\"R. Goebel\"},{\"authorId\":\"144865865\",\"name\":\"Y. Tanaka\"}],\"doi\":\"10.1007/978-3-030-66187-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b326e950ca86368056d2846444c25c61fedbc111\",\"title\":\"Mining Intelligence and Knowledge Exploration: 7th International Conference, MIKE 2019, Goa, India, December 19\\u201322, 2019, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/b326e950ca86368056d2846444c25c61fedbc111\",\"venue\":\"MIKE\",\"year\":2020},{\"arxivId\":\"1704.02163\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"},{\"authorId\":\"87972149\",\"name\":\"Sergi Soler\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1016/j.jvcir.2017.11.022\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"title\":\"Egocentric video description based on temporally-linked sequences\",\"url\":\"https://www.semanticscholar.org/paper/a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":\"1811.00706\",\"authors\":[{\"authorId\":\"144836741\",\"name\":\"L. Borges\"},{\"authorId\":\"144694868\",\"name\":\"Bruno Martins\"},{\"authorId\":\"51898207\",\"name\":\"P. Calado\"}],\"doi\":\"10.1145/3287763\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dcd0ff5319acc779b91457a33a41fdd35203c38\",\"title\":\"Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News\",\"url\":\"https://www.semanticscholar.org/paper/3dcd0ff5319acc779b91457a33a41fdd35203c38\",\"venue\":\"ACM J. Data Inf. Qual.\",\"year\":2019},{\"arxivId\":\"1705.11001\",\"authors\":[{\"authorId\":\"143786724\",\"name\":\"Kevin Lin\"},{\"authorId\":\"7379232\",\"name\":\"Dianqi Li\"},{\"authorId\":\"50045602\",\"name\":\"Xiaodong He\"},{\"authorId\":\"144463556\",\"name\":\"M. Sun\"},{\"authorId\":\"51064498\",\"name\":\"Zhengyou Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a88f86093e6f2d14761d4b8cbdcadfeff496c948\",\"title\":\"Adversarial Ranking for Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/a88f86093e6f2d14761d4b8cbdcadfeff496c948\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"2003.03107\",\"authors\":[{\"authorId\":\"32095408\",\"name\":\"Fawaz Sammani\"},{\"authorId\":\"1411260673\",\"name\":\"Luke Melas-Kyriazi\"}],\"doi\":\"10.1109/cvpr42600.2020.00486\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e9419436682726232e1b37a04c53bba919b12025\",\"title\":\"Show, Edit and Tell: A Framework for Editing Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/e9419436682726232e1b37a04c53bba919b12025\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51934339\",\"name\":\"Nuzhat Naqvi\"},{\"authorId\":\"83256875\",\"name\":\"Z. Ye\"}],\"doi\":\"10.1007/s11042-020-09128-6\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"12b53d372b723e201a234786792c6de002244386\",\"title\":\"Image captions: global-local and joint signals attention model (GL-JSAM)\",\"url\":\"https://www.semanticscholar.org/paper/12b53d372b723e201a234786792c6de002244386\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"2005.08081\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"4ba3190eb0dca6344e43c3eced1788324c169388\",\"title\":\"Layer-Wise Cross-View Decoding for Sequence-to-Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ba3190eb0dca6344e43c3eced1788324c169388\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1712.07835\",\"authors\":[{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"},{\"authorId\":\"2013546\",\"name\":\"Binqiang Wang\"},{\"authorId\":\"2263014\",\"name\":\"Xiangtao Zheng\"},{\"authorId\":\"50080046\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TGRS.2017.2776321\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7cc21bcce77dacd1ea7646244043261167f2dcd0\",\"title\":\"Exploring Models and Data for Remote Sensing Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/7cc21bcce77dacd1ea7646244043261167f2dcd0\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37722032\",\"name\":\"Charese Smiley\"},{\"authorId\":\"3232032\",\"name\":\"E. Davoodi\"},{\"authorId\":\"34695819\",\"name\":\"Dezhao Song\"},{\"authorId\":\"145837551\",\"name\":\"Frank Schilder\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0bb909baae8a32f6933b44303cb7a0f522303688\",\"title\":\"The E 2 E NLG Challenge : End-to-End Generation through Partial Template Mining\",\"url\":\"https://www.semanticscholar.org/paper/0bb909baae8a32f6933b44303cb7a0f522303688\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35227528\",\"name\":\"M. Curcio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f9c1c88e44d8b44e9d2c2313d717d1270654b5a\",\"title\":\"Clarity: An Exploration of Semantic Information Encoded in Mobile Application GUIs\",\"url\":\"https://www.semanticscholar.org/paper/7f9c1c88e44d8b44e9d2c2313d717d1270654b5a\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Se-Yeoung Kim\"},{\"authorId\":\"38557616\",\"name\":\"Sang-il Na\"},{\"authorId\":\"48206462\",\"name\":\"H. Kim\"},{\"authorId\":\"14418201\",\"name\":\"M. Kim\"},{\"authorId\":\"1787204\",\"name\":\"Byoung-Ki Jeon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"05ce73c39368aca1d10ab48dbe0dee80ee084bdb\",\"title\":\"M ULTI-LABEL L EARNING WITH THE RNN S FOR F ASHION S EARCH\",\"url\":\"https://www.semanticscholar.org/paper/05ce73c39368aca1d10ab48dbe0dee80ee084bdb\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"}],\"doi\":\"10.1016/j.csl.2020.101093\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9094fc5d46fe4b81c9b5157b5768ed8e0c955d0d\",\"title\":\"Transfer learning for multimodal dialog\",\"url\":\"https://www.semanticscholar.org/paper/9094fc5d46fe4b81c9b5157b5768ed8e0c955d0d\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":\"2009.12677\",\"authors\":[{\"authorId\":\"49291879\",\"name\":\"Y. Liu\"},{\"authorId\":\"1453924141\",\"name\":\"Yao Wan\"},{\"authorId\":\"40901818\",\"name\":\"Lifang He\"},{\"authorId\":\"1490937486\",\"name\":\"Hao Peng\"},{\"authorId\":\"144019071\",\"name\":\"Philip S. Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"baa8f524c82735f174b8d1ab512ac5750146d67e\",\"title\":\"KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/baa8f524c82735f174b8d1ab512ac5750146d67e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.09340\",\"authors\":[{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":\"10.18653/v1/P19-1654\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fbcea7715f62395fd5a83423d57556e8acca9a62\",\"title\":\"VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/fbcea7715f62395fd5a83423d57556e8acca9a62\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41030694\",\"name\":\"Huanyu Yu\"},{\"authorId\":\"3392007\",\"name\":\"Shuo Cheng\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"7272302\",\"name\":\"Minsi Wang\"},{\"authorId\":\"40430880\",\"name\":\"J. Zhang\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"}],\"doi\":\"10.1109/CVPR.2018.00629\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5876f67129a80a1ee753f715efcd2e2109bf432\",\"title\":\"Fine-Grained Video Captioning for Sports Narrative\",\"url\":\"https://www.semanticscholar.org/paper/f5876f67129a80a1ee753f715efcd2e2109bf432\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1906.06216\",\"authors\":[{\"authorId\":\"51270689\",\"name\":\"Hyounghun Kim\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P19-1351\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8501712706efa6f314438143de18507471781060\",\"title\":\"Improving Visual Question Answering by Referring to Generated Paragraph Captions\",\"url\":\"https://www.semanticscholar.org/paper/8501712706efa6f314438143de18507471781060\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1909.00277\",\"authors\":[{\"authorId\":\"34170717\",\"name\":\"Lifu Huang\"},{\"authorId\":\"39227408\",\"name\":\"Ronan Le Bras\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/D19-1243\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66117f82def0c69a3b9cc77eb3e2694b0245ca86\",\"title\":\"Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/66117f82def0c69a3b9cc77eb3e2694b0245ca86\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9610143\",\"name\":\"Zhihao Fan\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"36620933\",\"name\":\"S. Wang\"},{\"authorId\":\"1681842\",\"name\":\"Y. Liu\"},{\"authorId\":\"1790227\",\"name\":\"X. Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b2d224c8b69191c02dce750257c39d46b1c4a7b\",\"title\":\"A Reinforcement Learning Framework for Natural Question Generation using Bi-discriminators\",\"url\":\"https://www.semanticscholar.org/paper/8b2d224c8b69191c02dce750257c39d46b1c4a7b\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":\"2008.06880\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":null,\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413880\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"title\":\"Poet: Product-oriented Video Captioner for E-commerce\",\"url\":\"https://www.semanticscholar.org/paper/72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1512.02949\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3bcca85ad84806be6d38d3882f7a6aac0ad90253\",\"title\":\"Video captioning with recurrent networks based on frame- and video-level features and visual content classification\",\"url\":\"https://www.semanticscholar.org/paper/3bcca85ad84806be6d38d3882f7a6aac0ad90253\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26554826\",\"name\":\"Yuzhao Mao\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"1680068\",\"name\":\"X. Wang\"},{\"authorId\":\"2462591\",\"name\":\"Ruifan Li\"}],\"doi\":\"10.24963/ijcai.2018/592\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"555e65623326de1b9c32bd22d482071920a6e4f1\",\"title\":\"Show and Tell More: Topic-Oriented Multi-Sentence Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/555e65623326de1b9c32bd22d482071920a6e4f1\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92057141\",\"name\":\"H. Wei\"},{\"authorId\":\"49969428\",\"name\":\"Z. Li\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"144137447\",\"name\":\"Tao Zhou\"},{\"authorId\":\"1391051492\",\"name\":\"Y. Quan\"}],\"doi\":\"10.1109/IJCNN.2019.8852118\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f76cf186f93a0f5b3f57a8d0a1545e5b626e664e\",\"title\":\"Image Captioning Based On Sentence-Level And Word-Level Attention\",\"url\":\"https://www.semanticscholar.org/paper/f76cf186f93a0f5b3f57a8d0a1545e5b626e664e\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50005563\",\"name\":\"G. Christie\"},{\"authorId\":\"2440772\",\"name\":\"Ankit P Laddha\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"37226164\",\"name\":\"Yash Goyal\"},{\"authorId\":\"1942176\",\"name\":\"K. Kochersberger\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1016/j.cviu.2017.09.001\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2dbca69f6e50c058a44243abfaf669097a02c879\",\"title\":\"Resolving vision and language ambiguities together: Joint segmentation & prepositional attachment resolution in captioned scenes\",\"url\":\"https://www.semanticscholar.org/paper/2dbca69f6e50c058a44243abfaf669097a02c879\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1502.04569\",\"authors\":[{\"authorId\":\"2428539\",\"name\":\"M. Jas\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7298889\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34b3250650c06ecb286badc876d43452941ad20b\",\"title\":\"Image specificity\",\"url\":\"https://www.semanticscholar.org/paper/34b3250650c06ecb286badc876d43452941ad20b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114888940\",\"name\":\"C. van der Lee\"},{\"authorId\":\"2709440\",\"name\":\"Chris Emmery\"},{\"authorId\":\"1917518\",\"name\":\"S. Wubben\"},{\"authorId\":\"145210073\",\"name\":\"E. Krahmer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9629f883ab622488ea6d24dd0d3af7014d734852\",\"title\":\"The CACAPO Dataset: A Multilingual, Multi-Domain Dataset for Neural Pipeline and End-to-End Data-to-Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/9629f883ab622488ea6d24dd0d3af7014d734852\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32622125\",\"name\":\"Lily D. Ellebracht\"},{\"authorId\":\"1780343\",\"name\":\"Arnau Ramisa\"},{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1405187132\",\"name\":\"Jose Cordero-Rama\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"},{\"authorId\":\"3171632\",\"name\":\"A. Quattoni\"}],\"doi\":\"10.13039/501100003329\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4281f3936f52204717a3452fa2b7824864d9752b\",\"title\":\"Semantic Tuples for Evaluation of Image to Sentence Generation\",\"url\":\"https://www.semanticscholar.org/paper/4281f3936f52204717a3452fa2b7824864d9752b\",\"venue\":\"EMNLP 2015\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2547721\",\"name\":\"M. Fourati\"},{\"authorId\":\"2245992\",\"name\":\"A. Jedidi\"},{\"authorId\":\"9395641\",\"name\":\"F. Gargouri\"}],\"doi\":\"10.1007/s11042-020-09589-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c1adfd46f461de181edc64bb783eedee45f55ed7\",\"title\":\"A survey on description and modeling of audiovisual documents\",\"url\":\"https://www.semanticscholar.org/paper/c1adfd46f461de181edc64bb783eedee45f55ed7\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"2002.08565\",\"authors\":[{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"},{\"authorId\":\"150353841\",\"name\":\"Yinan Zhao\"},{\"authorId\":\"1409765557\",\"name\":\"Meng Zhang\"},{\"authorId\":\"23364558\",\"name\":\"Nilavra Bhattacharya\"}],\"doi\":\"10.1007/978-3-030-58520-4_25\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c936d878003254cdab662a966cecd29e8be652d0\",\"title\":\"Captioning Images Taken by People Who Are Blind\",\"url\":\"https://www.semanticscholar.org/paper/c936d878003254cdab662a966cecd29e8be652d0\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2006.12442\",\"authors\":[{\"authorId\":\"144745718\",\"name\":\"Stephen Roller\"},{\"authorId\":\"2656573\",\"name\":\"Y.-Lan Boureau\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"31461304\",\"name\":\"Emily Dinan\"},{\"authorId\":\"4861083\",\"name\":\"A. Fan\"},{\"authorId\":\"2121780\",\"name\":\"D. Gunning\"},{\"authorId\":\"3092435\",\"name\":\"Da Ju\"},{\"authorId\":\"6649233\",\"name\":\"Margaret Li\"},{\"authorId\":\"1753626755\",\"name\":\"Spencer Poff\"},{\"authorId\":\"1422035486\",\"name\":\"Pratik Ringshia\"},{\"authorId\":\"35752280\",\"name\":\"Kurt Shuster\"},{\"authorId\":\"51324296\",\"name\":\"Eric Michael Smith\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"39219656\",\"name\":\"Jack Urbanek\"},{\"authorId\":\"49160304\",\"name\":\"M. Williamson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ebf79630966e36761f2275990075384fdcb8d3a7\",\"title\":\"Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions\",\"url\":\"https://www.semanticscholar.org/paper/ebf79630966e36761f2275990075384fdcb8d3a7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71006963\",\"name\":\"Maram Mahmoud A. Monshi\"},{\"authorId\":\"48422087\",\"name\":\"Josiah Poon\"},{\"authorId\":\"1798753\",\"name\":\"Y. Y. Chung\"}],\"doi\":\"10.1016/j.artmed.2020.101878\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"362506fe8e0a20fd60fe6b657004d1fd9e549cef\",\"title\":\"Deep learning in generating radiology reports: A survey\",\"url\":\"https://www.semanticscholar.org/paper/362506fe8e0a20fd60fe6b657004d1fd9e549cef\",\"venue\":\"Artificial Intelligence in Medicine\",\"year\":2020},{\"arxivId\":\"2001.01582\",\"authors\":[{\"authorId\":\"32847181\",\"name\":\"Razieh Baradaran\"},{\"authorId\":\"1481820553\",\"name\":\"Razieh Ghiasi\"},{\"authorId\":\"1837557\",\"name\":\"Hossein Amirkhani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2af67c1063172c924a977d97d4b848651cc1617e\",\"title\":\"A Survey on Machine Reading Comprehension Systems\",\"url\":\"https://www.semanticscholar.org/paper/2af67c1063172c924a977d97d4b848651cc1617e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.07290\",\"authors\":[{\"authorId\":\"5315428\",\"name\":\"Benjamin J. Newman\"},{\"authorId\":\"1398108805\",\"name\":\"Reuben Cohn-Gordon\"},{\"authorId\":\"144922861\",\"name\":\"Christopher Potts\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"673c1512aeb604c9070605ef097d6dfd5e4cd0ba\",\"title\":\"Communication-based Evaluation for Natural Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/673c1512aeb604c9070605ef097d6dfd5e4cd0ba\",\"venue\":\"SCIL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400258083\",\"name\":\"Chunpu Xu\"},{\"authorId\":\"47748857\",\"name\":\"Wei Zhao\"},{\"authorId\":\"31991405\",\"name\":\"Min Yang\"},{\"authorId\":\"2441161\",\"name\":\"Xiang Ao\"},{\"authorId\":\"1405918472\",\"name\":\"Wangrong Cheng\"},{\"authorId\":\"1936983\",\"name\":\"J. Tian\"}],\"doi\":\"10.1145/3357384.3358105\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e81c97c18cb4f4922e4442664350350536a71a13\",\"title\":\"A Unified Generation-Retrieval Framework for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e81c97c18cb4f4922e4442664350350536a71a13\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151485379\",\"name\":\"Jiahong Wu\"},{\"authorId\":\"144331388\",\"name\":\"He Zheng\"},{\"authorId\":\"143946810\",\"name\":\"Bo Zhao\"},{\"authorId\":\"47003565\",\"name\":\"Yixin Li\"},{\"authorId\":\"50736086\",\"name\":\"Baoming Yan\"},{\"authorId\":\"151471989\",\"name\":\"Rui Liang\"},{\"authorId\":\"46314609\",\"name\":\"Wenjia Wang\"},{\"authorId\":\"14547213\",\"name\":\"Shipei Zhou\"},{\"authorId\":\"33344887\",\"name\":\"Guosen Lin\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"26960212\",\"name\":\"Yizhou Wang\"},{\"authorId\":\"47903786\",\"name\":\"Y. Wang\"}],\"doi\":\"10.1109/ICME.2019.00256\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"094159beb3a83f506b532b287c03c2215018f63a\",\"title\":\"Large-Scale Datasets for Going Deeper in Image Understanding\",\"url\":\"https://www.semanticscholar.org/paper/094159beb3a83f506b532b287c03c2215018f63a\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39796129\",\"name\":\"Hayden Faulkner\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":\"10.1109/DICTA.2017.8227494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"title\":\"TenniSet: A Dataset for Dense Fine-Grained Event Recognition, Localisation and Description\",\"url\":\"https://www.semanticscholar.org/paper/4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"venue\":\"2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50111883\",\"name\":\"S. Lee\"},{\"authorId\":\"1784897\",\"name\":\"Incheol Kim\"}],\"doi\":\"10.3745/JIPS.02.0098\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc1d373ca1b7abc470bf6a6a639436ea12461378\",\"title\":\"Video Captioning with Visual and Semantic Features\",\"url\":\"https://www.semanticscholar.org/paper/fc1d373ca1b7abc470bf6a6a639436ea12461378\",\"venue\":\"J. Inf. Process. Syst.\",\"year\":2018},{\"arxivId\":\"1804.00887\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3dc2c3be0796f65154d2106ed4442889c84546df\",\"title\":\"Learning to Guide Decoding for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3dc2c3be0796f65154d2106ed4442889c84546df\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1609.08976\",\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145153424\",\"name\":\"Ricardo Henao\"},{\"authorId\":\"50242822\",\"name\":\"X. Yuan\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"9074631\",\"name\":\"A. Stevens\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f4c5d13a8e9e80edcd4f69f0eab0b4434364c6dd\",\"title\":\"Variational Autoencoder for Deep Learning of Images, Labels and Captions\",\"url\":\"https://www.semanticscholar.org/paper/f4c5d13a8e9e80edcd4f69f0eab0b4434364c6dd\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122822134\",\"name\":\"Gregory Casta\\u00f1\\u00f3n\"},{\"authorId\":\"2962929\",\"name\":\"Nathan Shnidman\"},{\"authorId\":\"145901595\",\"name\":\"Tim Anderson\"},{\"authorId\":\"145607579\",\"name\":\"Jeffrey Byrne\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2b840276017c04e090058e281431379d35de88b\",\"title\":\"Out the Window: A Crowd-Sourced Dataset for Activity Classification in Security Video\",\"url\":\"https://www.semanticscholar.org/paper/a2b840276017c04e090058e281431379d35de88b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"},{\"authorId\":\"145801638\",\"name\":\"J. Kittler\"}],\"doi\":\"10.1109/ICPR.2016.7900036\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cee7ce8d01484dc6c3be38015153d5684f6cafa2\",\"title\":\"Generating commentaries for tennis videos\",\"url\":\"https://www.semanticscholar.org/paper/cee7ce8d01484dc6c3be38015153d5684f6cafa2\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":\"1908.10797\",\"authors\":[{\"authorId\":\"47060391\",\"name\":\"Jia Huei Tan\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"},{\"authorId\":\"38900275\",\"name\":\"Joon Huang Chuah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"04a49009833179d29c541d5775d2dc2f2a0adf4a\",\"title\":\"Image Captioning with Sparse Recurrent Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/04a49009833179d29c541d5775d2dc2f2a0adf4a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.06954\",\"authors\":[{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"46315174\",\"name\":\"Wenmin Wang\"},{\"authorId\":\"40445654\",\"name\":\"J. Chen\"},{\"authorId\":\"144539992\",\"name\":\"Xiao-Yong Wei\"}],\"doi\":\"10.1109/ICCV.2019.00473\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c163d4942117179d3e97182e1b280027d7d60a9\",\"title\":\"Attention on Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4c163d4942117179d3e97182e1b280027d7d60a9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1809.01448\",\"authors\":[{\"authorId\":\"3372941\",\"name\":\"Rotem Dror\"},{\"authorId\":\"1762757\",\"name\":\"Roi Reichart\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f183074396c5ae948c3f6d675a316cb63288414\",\"title\":\"Appendix - Recommended Statistical Significance Tests for NLP Tasks\",\"url\":\"https://www.semanticscholar.org/paper/1f183074396c5ae948c3f6d675a316cb63288414\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2005.00246\",\"authors\":[{\"authorId\":\"2904055\",\"name\":\"Ashish V. Thapliyal\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/2020.acl-main.16\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b88c52150995965a66dc774603444ad570c3941d\",\"title\":\"Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage\",\"url\":\"https://www.semanticscholar.org/paper/b88c52150995965a66dc774603444ad570c3941d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1609.06657\",\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bbccfd4f5ec4bf6a45bc48caf13c11161f9c2da3\",\"title\":\"The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)\",\"url\":\"https://www.semanticscholar.org/paper/bbccfd4f5ec4bf6a45bc48caf13c11161f9c2da3\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72547325\",\"name\":\"Bassel S. Chawky\"},{\"authorId\":\"2805974\",\"name\":\"A. S. Elons\"},{\"authorId\":\"114287783\",\"name\":\"A. Ali\"},{\"authorId\":\"2382767\",\"name\":\"H. A. Shedeed\"}],\"doi\":\"10.1007/978-3-319-63754-9_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88d70d8cf4557e032bc359d2c2a15b06686612ea\",\"title\":\"A Study of Action Recognition Problems: Dataset and Architectures Perspectives\",\"url\":\"https://www.semanticscholar.org/paper/88d70d8cf4557e032bc359d2c2a15b06686612ea\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1909.02489\",\"authors\":[{\"authorId\":\"47748186\",\"name\":\"Wei Wei\"},{\"authorId\":\"144996789\",\"name\":\"L. Cheng\"},{\"authorId\":\"2089102\",\"name\":\"X. Mao\"},{\"authorId\":\"143652253\",\"name\":\"G. Zhou\"},{\"authorId\":\"143663410\",\"name\":\"F. Zhu\"}],\"doi\":\"10.1109/ACCESS.2020.3018752\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1e8e509a62c424055ffc050dc7cb41329674ea9\",\"title\":\"Stack-VS: Stacked Visual-Semantic Attention for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/b1e8e509a62c424055ffc050dc7cb41329674ea9\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39006765\",\"name\":\"J. Wu\"},{\"authorId\":\"152566311\",\"name\":\"Si-ya Xie\"},{\"authorId\":\"5935907\",\"name\":\"Xin-bao Shi\"},{\"authorId\":\"94257262\",\"name\":\"Y. Chen\"}],\"doi\":\"10.1007/S11801-017-7185-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9823c25796a364d70e31059c44305f007a2dbc8a\",\"title\":\"Global-local feature attention network with reranking strategy for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/9823c25796a364d70e31059c44305f007a2dbc8a\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"},{\"authorId\":\"40894914\",\"name\":\"T. Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"49ac61eed8301f41da85e0053be3be790293faac\",\"title\":\"Recurrent Highway Networks with Language CNN for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/49ac61eed8301f41da85e0053be3be790293faac\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2006.10347\",\"authors\":[{\"authorId\":\"143893983\",\"name\":\"Shuai Zhang\"},{\"authorId\":\"47911053\",\"name\":\"X. Xin\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"8673880\",\"name\":\"Yachong Guo\"},{\"authorId\":\"1752568622\",\"name\":\"Qiuqiao Hao\"},{\"authorId\":\"47008979\",\"name\":\"X. Yang\"},{\"authorId\":\"49605996\",\"name\":\"J. Wang\"},{\"authorId\":\"101594813\",\"name\":\"J. Zhang\"},{\"authorId\":\"30634814\",\"name\":\"B. Zhang\"},{\"authorId\":\"144585377\",\"name\":\"Wei Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"27c20cd0bb7a68487ddfb39b91611714e45235d1\",\"title\":\"Automated Radiological Report Generation For Chest X-Rays With Weakly-Supervised End-to-End Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/27c20cd0bb7a68487ddfb39b91611714e45235d1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"153028537\",\"name\":\"X. Wu\"},{\"authorId\":\"36263371\",\"name\":\"Shen Ge\"},{\"authorId\":\"93249636\",\"name\":\"W. Fan\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1609/AAAI.V34I07.6824\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d40c5e2d42d19dd7fd85d45201c048e3768c740d\",\"title\":\"Federated Learning for Vision-and-Language Grounding Problems\",\"url\":\"https://www.semanticscholar.org/paper/d40c5e2d42d19dd7fd85d45201c048e3768c740d\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1912.13151\",\"authors\":[{\"authorId\":\"8010189\",\"name\":\"Xinjie Fan\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1471411319\",\"name\":\"Zhendong Wang\"},{\"authorId\":\"38026572\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"aa950171b25c05466d9d3cc58dff3b9ea9882e4a\",\"title\":\"Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/aa950171b25c05466d9d3cc58dff3b9ea9882e4a\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1810.10126\",\"authors\":[{\"authorId\":\"1678662\",\"name\":\"Yang Li\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"3422911\",\"name\":\"S. Si\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"31b3b0a526683048f69e703d5f098aea0e8a0ce0\",\"title\":\"Area Attention\",\"url\":\"https://www.semanticscholar.org/paper/31b3b0a526683048f69e703d5f098aea0e8a0ce0\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1612.06950\",\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aba537944ace733bbab2290cf2814cf7f4e4e275\",\"title\":\"Temporal Tessellation for Video Annotation and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/aba537944ace733bbab2290cf2814cf7f4e4e275\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1808.10584\",\"authors\":[{\"authorId\":\"2006291\",\"name\":\"Harsh Jhamtani\"},{\"authorId\":\"1400419309\",\"name\":\"Taylor Berg-Kirkpatrick\"}],\"doi\":\"10.18653/v1/D18-1436\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7062b5de5fddb298823cf8969c7dfa6165ea933e\",\"title\":\"Learning to Describe Differences Between Pairs of Similar Images\",\"url\":\"https://www.semanticscholar.org/paper/7062b5de5fddb298823cf8969c7dfa6165ea933e\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1805.08170\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a484b7eda0e5389ae62ab1549f27594050a60f71\",\"title\":\"Turbo Learning for Captionbot and Drawingbot\",\"url\":\"https://www.semanticscholar.org/paper/a484b7eda0e5389ae62ab1549f27594050a60f71\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51145342\",\"name\":\"Sourab Mangrulkar\"},{\"authorId\":\"51145151\",\"name\":\"S. Shrivastava\"},{\"authorId\":\"3055959\",\"name\":\"Veena Thenkanidiyoor\"},{\"authorId\":\"35362808\",\"name\":\"Dileep Aroor Dinesh\"}],\"doi\":\"10.18653/v1/W18-5020\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cfe42d353e39ad9724693a9b110d46ce5001ea22\",\"title\":\"A Context-aware Convolutional Natural Language Generation model for Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/cfe42d353e39ad9724693a9b110d46ce5001ea22\",\"venue\":\"SIGDIAL Conference\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3546791\",\"name\":\"Y. Liu\"},{\"authorId\":\"1704728\",\"name\":\"T. Tuytelaars\"}],\"doi\":\"10.1109/TIP.2020.2975980\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"40eea50b67d04e821117d82153f3f3f36e33d22a\",\"title\":\"A Deep Multi-Modal Explanation Model for Zero-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/40eea50b67d04e821117d82153f3f3f36e33d22a\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2017/307\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e33bc5c83f2cea403a5521385ee8e2794b311275\",\"title\":\"MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e33bc5c83f2cea403a5521385ee8e2794b311275\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39232438\",\"name\":\"C. Liu\"},{\"authorId\":\"40203750\",\"name\":\"F. Sun\"},{\"authorId\":\"1697065\",\"name\":\"C. Wang\"},{\"authorId\":\"49451531\",\"name\":\"F. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2e5ae83f9f44b606898b1795906de5464ac3482e\",\"title\":\"WEOTAWEO 2 LSTM WEO 1 Encoding WSS 1 WSS 2 WSSTB Decoding Attention Cell\",\"url\":\"https://www.semanticscholar.org/paper/2e5ae83f9f44b606898b1795906de5464ac3482e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144897421\",\"name\":\"Rachel N. Simons\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"62cd6ac2eaadf030eac67d9ec33554daee05e178\",\"title\":\"\\\"I Hope This Is Helpful\\\": Understanding Crowdworkers\\u2019 Challenges and Motivations for an Image Description Task\",\"url\":\"https://www.semanticscholar.org/paper/62cd6ac2eaadf030eac67d9ec33554daee05e178\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1392406002\",\"name\":\"Arturs Polis\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4bfeae734bced5b2613af9f7d8271354b614e08e\",\"title\":\"Paragraph-length image captioning using hierarchical recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/4bfeae734bced5b2613af9f7d8271354b614e08e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33698309\",\"name\":\"Zhong-Qiu Zhao\"},{\"authorId\":\"49697077\",\"name\":\"Yue-Lin Sun\"},{\"authorId\":\"1977865899\",\"name\":\"Nan-Xun Wang\"},{\"authorId\":\"144963976\",\"name\":\"Weidong Tian\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206632\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03a53b48a2c869658f969856acd2830711dc9ba9\",\"title\":\"Extricating from GroundTruth: An Unpaired Learning Based Evaluation Metric for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/03a53b48a2c869658f969856acd2830711dc9ba9\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50994378\",\"name\":\"Chen Chen\"},{\"authorId\":\"1940556\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"39618262\",\"name\":\"Sungchul Kim\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"14156382\",\"name\":\"T. Yu\"},{\"authorId\":\"35365476\",\"name\":\"R. Rossi\"},{\"authorId\":\"3139133\",\"name\":\"Razvan C. Bunescu\"}],\"doi\":\"10.1145/3341162.3345601\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"32df387bd612771cfe92f1a34d5de8cdedf7482e\",\"title\":\"Neural caption generation over figures\",\"url\":\"https://www.semanticscholar.org/paper/32df387bd612771cfe92f1a34d5de8cdedf7482e\",\"venue\":\"UbiComp/ISWC Adjunct\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41050714\",\"name\":\"Sandeep Narayan Parameswaran\"},{\"authorId\":\"144009889\",\"name\":\"Sukhendu Das\"}],\"doi\":\"10.1007/978-3-030-36718-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c940e8b66e822dbdd8d0dde9a6640ed01834e31\",\"title\":\"SACIC: A Semantics-Aware Convolutional Image Captioner Using Multi-level Pervasive Attention\",\"url\":\"https://www.semanticscholar.org/paper/9c940e8b66e822dbdd8d0dde9a6640ed01834e31\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":\"1910.02974\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/ICRA40945.2020.9196653\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fe6f2a08cb8911d52533a413b071638d0463f10a\",\"title\":\"SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability\",\"url\":\"https://www.semanticscholar.org/paper/fe6f2a08cb8911d52533a413b071638d0463f10a\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":\"1908.11824\",\"authors\":[{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"47731271\",\"name\":\"Ruiyu Li\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/ICCV.2019.00898\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0db903dd28a3be3e57f40033c16cce574231f78e\",\"title\":\"Reflective Decoding Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0db903dd28a3be3e57f40033c16cce574231f78e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1707.06875\",\"authors\":[{\"authorId\":\"2848048\",\"name\":\"Jekaterina Novikova\"},{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"3451318\",\"name\":\"A. Curry\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/D17-1238\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0d441ab58a1027cb64084ad065cfea5e15b8e74c\",\"title\":\"Why We Need New Evaluation Metrics for NLG\",\"url\":\"https://www.semanticscholar.org/paper/0d441ab58a1027cb64084ad065cfea5e15b8e74c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":\"1505.04467\",\"authors\":[{\"authorId\":\"39172707\",\"name\":\"J. Devlin\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3ca194773fe583661b988fbdf33f7680764438b3\",\"title\":\"Exploring Nearest Neighbor Approaches for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3ca194773fe583661b988fbdf33f7680764438b3\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34659351\",\"name\":\"M\\u00e1rio A. T. Figueiredo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5e5383c9c6cf0a3ee5a09a5dbc4f2f2c7793dda\",\"title\":\"Statistical Learning Course-Final Project\",\"url\":\"https://www.semanticscholar.org/paper/a5e5383c9c6cf0a3ee5a09a5dbc4f2f2c7793dda\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1905.10486\",\"authors\":[{\"authorId\":\"40895152\",\"name\":\"Henry Elder\"},{\"authorId\":\"144739792\",\"name\":\"J. Foster\"},{\"authorId\":\"2242153\",\"name\":\"J. Barry\"},{\"authorId\":\"1397619317\",\"name\":\"A. O'Connor\"}],\"doi\":\"10.18653/v1/W19-2308\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e191b7eca9cf259abab3905dab1a54d1be091a7a\",\"title\":\"Designing a Symbolic Intermediate Representation for Neural Surface Realization\",\"url\":\"https://www.semanticscholar.org/paper/e191b7eca9cf259abab3905dab1a54d1be091a7a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/s11042-020-09251-4\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"af92381f95f28701396abeecaf715383b26ca354\",\"title\":\"A unified cycle-consistent neural model for text and image retrieval\",\"url\":\"https://www.semanticscholar.org/paper/af92381f95f28701396abeecaf715383b26ca354\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12094097\",\"name\":\"Pratik P. Rane\"},{\"authorId\":\"118145941\",\"name\":\"A. Sargar\"},{\"authorId\":\"47039181\",\"name\":\"Faiza Shaikh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f77a604410d88307ec5c6331c8b6133272fbaa10\",\"title\":\"Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f77a604410d88307ec5c6331c8b6133272fbaa10\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"2223252\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"46700604\",\"name\":\"J. Liu\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3394171.3414009\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0f480dadb895975a3921f5fcd03fdbf4085f8e9b\",\"title\":\"Attacking Image Captioning Towards Accuracy-Preserving Target Words Removal\",\"url\":\"https://www.semanticscholar.org/paper/0f480dadb895975a3921f5fcd03fdbf4085f8e9b\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"50219447\",\"name\":\"Zheng Wang\"}],\"doi\":\"10.1145/3123266.3123327\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"title\":\"Catching the Temporal Regions-of-Interest for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1711.06420\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"39298199\",\"name\":\"Li Niu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/CVPR.2018.00750\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"724b253a55e86ad230ba05c7eb78f249e09258d9\",\"title\":\"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/724b253a55e86ad230ba05c7eb78f249e09258d9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1810.04020\",\"authors\":[{\"authorId\":\"47412302\",\"name\":\"M. Z. Hossain\"},{\"authorId\":\"2470423\",\"name\":\"Ferdous Sohel\"},{\"authorId\":\"2222847\",\"name\":\"M. F. Shiratuddin\"},{\"authorId\":\"47028380\",\"name\":\"Hamid Laga\"}],\"doi\":\"10.1145/3295748\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7e27d44e3fac723ccb703e0a83b22711bd42efe8\",\"title\":\"A Comprehensive Survey of Deep Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7e27d44e3fac723ccb703e0a83b22711bd42efe8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47287647\",\"name\":\"Sen He\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"3251678\",\"name\":\"N. Pugeault\"}],\"doi\":\"10.1109/ICCV.2019.00862\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4a91b8c1259e7744784d922f250dd77ea951e9f\",\"title\":\"Human Attention in Image Captioning: Dataset and Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e4a91b8c1259e7744784d922f250dd77ea951e9f\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1808.08732\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"1781885\",\"name\":\"Houfeng Wang\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.18653/v1/D18-1013\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8968072ad12bcb96c513ae1c01abf6abdae810df\",\"title\":\"simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/8968072ad12bcb96c513ae1c01abf6abdae810df\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1511.04670\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9ed7d774684a1770445c1c53e276011a8364b9e2\",\"title\":\"Uncovering Temporal Context for Video Question and Answering\",\"url\":\"https://www.semanticscholar.org/paper/9ed7d774684a1770445c1c53e276011a8364b9e2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"205e895e03969c96f3c482b0bd26308b16a12bd0\",\"title\":\"Image Captioning with an Intermediate Attributes Layer\",\"url\":\"https://www.semanticscholar.org/paper/205e895e03969c96f3c482b0bd26308b16a12bd0\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824699\",\"name\":\"Zheng Lian\"},{\"authorId\":\"49403723\",\"name\":\"H. Li\"},{\"authorId\":\"102696161\",\"name\":\"Rui Wang\"},{\"authorId\":\"38865491\",\"name\":\"Xiaohui Hu\"}],\"doi\":\"10.1109/ICTAI50040.2020.00119\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"45f9791def36fd6e2e701b0d5d5c44063a9da473\",\"title\":\"Enhanced soft attention mechanism with an inception-like module for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/45f9791def36fd6e2e701b0d5d5c44063a9da473\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":\"2012.02128\",\"authors\":[{\"authorId\":\"1768759403\",\"name\":\"Jing Su\"},{\"authorId\":\"49892954\",\"name\":\"Qingyun Dai\"},{\"authorId\":\"8139616\",\"name\":\"F. Guerin\"},{\"authorId\":\"1510708415\",\"name\":\"Mian Zhou\"}],\"doi\":\"10.1016/j.csl.2020.101169\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c1a435ffd06c26f9f4273b11efed10bce2138d08\",\"title\":\"BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/c1a435ffd06c26f9f4273b11efed10bce2138d08\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.05907\",\"authors\":[{\"authorId\":\"2355122\",\"name\":\"Shahinur Alam\"},{\"authorId\":null,\"name\":\"Md Sultan Mahmud\"},{\"authorId\":\"1828610\",\"name\":\"M. Yeasin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a3bbff2b98a2b95cce1e5eee294b5ed8ad58c8e\",\"title\":\"Toward Building Safer Smart Homes for the People with Disabilities\",\"url\":\"https://www.semanticscholar.org/paper/8a3bbff2b98a2b95cce1e5eee294b5ed8ad58c8e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.03708\",\"authors\":[{\"authorId\":\"80389349\",\"name\":\"Zhuowan Li\"},{\"authorId\":\"2536742\",\"name\":\"Quan Hung Tran\"},{\"authorId\":\"2712573\",\"name\":\"Long Mai\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/CVPR42600.2020.00350\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"title\":\"Context-Aware Group Captioning via Self-Attention and Contrastive Features\",\"url\":\"https://www.semanticscholar.org/paper/477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042704741\",\"name\":\"Xiaohan Zou\"},{\"authorId\":\"144289788\",\"name\":\"C. Lin\"},{\"authorId\":\"2042741172\",\"name\":\"Yinjia Zhang\"},{\"authorId\":\"1729695\",\"name\":\"Qinpei Zhao\"}],\"doi\":\"10.1109/ICTAI50040.2020.00124\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8465dc22e66853636edce1cd537317120ecfcbb\",\"title\":\"To be an Artist: Automatic Generation on Food Image Aesthetic Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f8465dc22e66853636edce1cd537317120ecfcbb\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50704161\",\"name\":\"Lingxuan Li\"},{\"authorId\":\"11232438\",\"name\":\"Yihong Zhao\"},{\"authorId\":\"96515479\",\"name\":\"Zhaorui Zhang\"},{\"authorId\":\"1733076573\",\"name\":\"Tianrui Niu\"},{\"authorId\":\"39825530\",\"name\":\"Fangxiang Feng\"},{\"authorId\":\"48631332\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-60457-8_3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0c0d407899b7c535ce90fa798309214902ae0cba\",\"title\":\"Referring Expression Generation via Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/0c0d407899b7c535ce90fa798309214902ae0cba\",\"venue\":\"NLPCC\",\"year\":2020},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1911.05186\",\"authors\":[{\"authorId\":\"7878341\",\"name\":\"Wubo Li\"},{\"authorId\":\"9276071\",\"name\":\"Wei Zou\"},{\"authorId\":\"1898780\",\"name\":\"Xiangang Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"962ec0fc31498001bdd011effb4ba73621dc0a8a\",\"title\":\"TCT: A Cross-supervised Learning Method for Multimodal Sequence Representation\",\"url\":\"https://www.semanticscholar.org/paper/962ec0fc31498001bdd011effb4ba73621dc0a8a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1145/2964284.2984066\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61d409b92860480a9188d23ba59b822ddc6331f9\",\"title\":\"Multimodal Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61d409b92860480a9188d23ba59b822ddc6331f9\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35664665\",\"name\":\"Marco Antonio Sobrevilla Cabezudo\"},{\"authorId\":\"1774467\",\"name\":\"T. A. S. Pardo\"}],\"doi\":\"10.18653/v1/W18-3608\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eef8fa4eb5692eb5a0fe3b14b5f3b90ad53824de\",\"title\":\"NILC-SWORNEMO at the Surface Realization Shared Task: Exploring Syntax-Based Word Ordering using Neural Models\",\"url\":\"https://www.semanticscholar.org/paper/eef8fa4eb5692eb5a0fe3b14b5f3b90ad53824de\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1811.00228\",\"authors\":[{\"authorId\":\"143827145\",\"name\":\"Daouda Sow\"},{\"authorId\":\"70565757\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"51910760\",\"name\":\"Mouhamed Niasse\"},{\"authorId\":\"46579572\",\"name\":\"T. Wan\"}],\"doi\":\"10.1109/ICASSP.2019.8682505\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8ffba70ea41640a4f24fa22613f9e94bdc55576\",\"title\":\"A Sequential Guiding Network with Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f8ffba70ea41640a4f24fa22613f9e94bdc55576\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"2007.04422\",\"authors\":[{\"authorId\":\"1802508687\",\"name\":\"Vatsal Goel\"},{\"authorId\":\"1802505447\",\"name\":\"Mohit Chandak\"},{\"authorId\":\"47583481\",\"name\":\"A. Anand\"},{\"authorId\":\"46401518\",\"name\":\"Prithwijit Guha\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9adb9a076ae1817fbac0dd258bb4a72027456e6e\",\"title\":\"IQ-VQA: Intelligent Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/9adb9a076ae1817fbac0dd258bb4a72027456e6e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"47957770\",\"name\":\"Xuying Zhang\"},{\"authorId\":\"40626740\",\"name\":\"Liujuan Cao\"},{\"authorId\":\"47096329\",\"name\":\"Yongjian Wu\"},{\"authorId\":\"1835006\",\"name\":\"Feiyue Huang\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"}],\"doi\":\"10.1145/3394171.3414008\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c19f8380410181249fdca70ec14f6b5f38ae0846\",\"title\":\"Exploring Language Prior for Mode-Sensitive Visual Attention Modeling\",\"url\":\"https://www.semanticscholar.org/paper/c19f8380410181249fdca70ec14f6b5f38ae0846\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1965909970\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"1755773\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930562\",\"name\":\"G. Chen\"},{\"authorId\":\"153016830\",\"name\":\"J. Guo\"}],\"doi\":\"10.1109/ACCESS.2020.3021857\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"title\":\"Understanding Objects in Video: Object-Oriented Video Captioning via Structured Trajectory and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3441913\",\"name\":\"Zihao Fu\"},{\"authorId\":\"1996394\",\"name\":\"Lidong Bing\"},{\"authorId\":\"144594306\",\"name\":\"Wai Lam\"}],\"doi\":\"10.1609/AAAI.V34I05.6278\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fad83732e02afb74ba52287a2517ac146ec31b29\",\"title\":\"Open Domain Event Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/fad83732e02afb74ba52287a2517ac146ec31b29\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49251978\",\"name\":\"J. Chen\"},{\"authorId\":\"41079034\",\"name\":\"Hong-Yang Chao\"}],\"doi\":\"10.1145/3394171.3416291\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"598ad06c164043c45c952dbde37e0c75991e66aa\",\"title\":\"VideoTRM: Pre-training for Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/598ad06c164043c45c952dbde37e0c75991e66aa\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"65747622\",\"name\":\"Yi Yang\"}],\"doi\":\"10.24963/ijcai.2019/135\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c38ae47ae73d9287e181c3f7693c6ca69aa0432e\",\"title\":\"Video Interactive Captioning with Human Prompts\",\"url\":\"https://www.semanticscholar.org/paper/c38ae47ae73d9287e181c3f7693c6ca69aa0432e\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2490430\",\"name\":\"Carina Silberer\"},{\"authorId\":\"1721364\",\"name\":\"Sina Zarrie\\u00df\"},{\"authorId\":\"2746394\",\"name\":\"M. Westera\"},{\"authorId\":\"1807810\",\"name\":\"Gemma Boleda\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e7155b150abf033bc3a0c24e16202777c9d41367\",\"title\":\"Humans Meet Models on Object Naming: A New Dataset and Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e7155b150abf033bc3a0c24e16202777c9d41367\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2006.11807\",\"authors\":[{\"authorId\":\"144910087\",\"name\":\"Zhan Shi\"},{\"authorId\":\"152482200\",\"name\":\"X. Zhou\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"150345740\",\"name\":\"Xiao-Dan Zhu\"}],\"doi\":\"10.18653/v1/2020.acl-main.664\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7e0f91e51ee372939c96714c7919dde6dc756849\",\"title\":\"Improving Image Captioning with Better Use of Captions\",\"url\":\"https://www.semanticscholar.org/paper/7e0f91e51ee372939c96714c7919dde6dc756849\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2010.03205\",\"authors\":[{\"authorId\":\"3165738\",\"name\":\"Bodhisattwa Prasad Majumder\"},{\"authorId\":\"2006291\",\"name\":\"Harsh Jhamtani\"},{\"authorId\":\"1400419309\",\"name\":\"Taylor Berg-Kirkpatrick\"},{\"authorId\":\"35660011\",\"name\":\"Julian McAuley\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.739\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2797f8c0398af676612698f2ccc1723a8692f271\",\"title\":\"Like hiking? You probably enjoy nature: Persona-grounded Dialog with Commonsense Expansions\",\"url\":\"https://www.semanticscholar.org/paper/2797f8c0398af676612698f2ccc1723a8692f271\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2002.11886\",\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5dff9ae50c0aadbd99ca59ff70425f63213243e\",\"title\":\"Hierarchical Memory Decoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a5dff9ae50c0aadbd99ca59ff70425f63213243e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.12401\",\"authors\":[{\"authorId\":\"1381931976\",\"name\":\"Md Sultan Al Nahian\"},{\"authorId\":\"1389550960\",\"name\":\"Tasmia Tasrin\"},{\"authorId\":\"153194886\",\"name\":\"Sagar Gandhi\"},{\"authorId\":\"1381228192\",\"name\":\"Ryan Gaines\"},{\"authorId\":\"35066258\",\"name\":\"B. Harrison\"}],\"doi\":\"10.1007/978-3-030-33894-7_30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0946687aae33ba39856086aed77bbc28604e54db\",\"title\":\"A Hierarchical Approach for Visual Storytelling Using Image Description\",\"url\":\"https://www.semanticscholar.org/paper/0946687aae33ba39856086aed77bbc28604e54db\",\"venue\":\"ICIDS\",\"year\":2019},{\"arxivId\":\"2003.13942\",\"authors\":[{\"authorId\":\"52170427\",\"name\":\"Boxiao Pan\"},{\"authorId\":\"30017683\",\"name\":\"Haoye Cai\"},{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"144015229\",\"name\":\"Kuan-Hui Lee\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"46408185\",\"name\":\"E. Adeli\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/cvpr42600.2020.01088\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a444d32f30d38fb0cb811fa1a9b601511244fb5b\",\"title\":\"Spatio-Temporal Graph for Video Captioning With Knowledge Distillation\",\"url\":\"https://www.semanticscholar.org/paper/a444d32f30d38fb0cb811fa1a9b601511244fb5b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47666606\",\"name\":\"Hui Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1818920\",\"name\":\"Zijia Lin\"},{\"authorId\":\"34811036\",\"name\":\"Yuchen Guo\"},{\"authorId\":\"10795229\",\"name\":\"Caifeng Shan\"},{\"authorId\":\"144762952\",\"name\":\"J. Han\"}],\"doi\":\"10.1007/s12559-019-09656-w\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e99a3163815a0c40705ffd6347c6cdbf19fa5237\",\"title\":\"Image Captioning with Memorized Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/e99a3163815a0c40705ffd6347c6cdbf19fa5237\",\"venue\":\"Cognitive Computation\",\"year\":2019},{\"arxivId\":\"1709.09118\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6a645ea49b0723c88286cbf416c7956b1e2ea7c\",\"title\":\"Tensor Product Generation Networks\",\"url\":\"https://www.semanticscholar.org/paper/d6a645ea49b0723c88286cbf416c7956b1e2ea7c\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1906.05226\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P19-1185\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ef851a47fc32d596883e08a5f655179b8c5b02d\",\"title\":\"Continual and Multi-Task Architecture Search\",\"url\":\"https://www.semanticscholar.org/paper/2ef851a47fc32d596883e08a5f655179b8c5b02d\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2873524\",\"name\":\"Z. Ma\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"},{\"authorId\":\"150347046\",\"name\":\"Yangyang Cheng\"},{\"authorId\":\"51000590\",\"name\":\"Xinrui Zhu\"}],\"doi\":\"10.1109/ICME.2019.00225\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"daad11aee75bcf597602e654c33a12de61343dda\",\"title\":\"Image-to-Tree: A Tree-Structured Decoder for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/daad11aee75bcf597602e654c33a12de61343dda\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"2007.06877\",\"authors\":[{\"authorId\":\"51175635\",\"name\":\"Jiuniu Wang\"},{\"authorId\":\"50232214\",\"name\":\"Wenjia Xu\"},{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":\"10.1007/978-3-030-58452-8_22\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e38274d1d544af5fad2cb814d62a028e2ff7437b\",\"title\":\"Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets\",\"url\":\"https://www.semanticscholar.org/paper/e38274d1d544af5fad2cb814d62a028e2ff7437b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.01753\",\"authors\":[{\"authorId\":\"2007573924\",\"name\":\"Anubhav Shrimal\"},{\"authorId\":\"152367155\",\"name\":\"Tanmoy Chakraborty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"98d07a5d66091247989b4e757e08b1aa08a5d845\",\"title\":\"Attention Beam: An Image Captioning Approach\",\"url\":\"https://www.semanticscholar.org/paper/98d07a5d66091247989b4e757e08b1aa08a5d845\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50610439\",\"name\":\"Songtao Ding\"},{\"authorId\":\"2641875\",\"name\":\"Shiru Qu\"},{\"authorId\":\"6962569\",\"name\":\"Yuling Xi\"},{\"authorId\":\"49725227\",\"name\":\"Shaohua Wan\"}],\"doi\":\"10.1016/J.NEUCOM.2019.04.095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"088d11c03ac72c6d2a85dea758b283a09d4e519f\",\"title\":\"Stimulus-driven and concept-driven analysis for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/088d11c03ac72c6d2a85dea758b283a09d4e519f\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1909.02217\",\"authors\":[{\"authorId\":\"144889895\",\"name\":\"Ming Jiang\"},{\"authorId\":\"145919382\",\"name\":\"J. Hu\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"3207378\",\"name\":\"J. Diesner\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.18653/v1/D19-1156\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"94c6e5ccd67be60a5ced11d0a5c59e0ab0f749d4\",\"title\":\"REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/94c6e5ccd67be60a5ced11d0a5c59e0ab0f749d4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97713340\",\"name\":\"X. Liu\"},{\"authorId\":\"1943870\",\"name\":\"Weibin Liu\"},{\"authorId\":\"145767616\",\"name\":\"Weiwei Xing\"}],\"doi\":\"10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00152\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f55a588eef043cbb72ee548714d623b573c21e9b\",\"title\":\"Image Caption Generation with Local Semantic Information and Global Information\",\"url\":\"https://www.semanticscholar.org/paper/f55a588eef043cbb72ee548714d623b573c21e9b\",\"venue\":\"2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48707795\",\"name\":\"Ziwei Wang\"},{\"authorId\":\"7988538\",\"name\":\"Yadan Luo\"},{\"authorId\":null,\"name\":\"Yang Li\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"2416851\",\"name\":\"Hongzhi Yin\"}],\"doi\":\"10.1145/3240508.3240583\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f9853c76065b72a8a25d984f7aa4f2c65a2df623\",\"title\":\"Look Deeper See Richer: Depth-aware Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f9853c76065b72a8a25d984f7aa4f2c65a2df623\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"Li Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"117aae1dc5b3aee679a690f7dab84e9a23add930\",\"title\":\"AGE AND VIDEO CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/117aae1dc5b3aee679a690f7dab84e9a23add930\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1811.05253\",\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"5145427\",\"name\":\"Fangyu Wu\"},{\"authorId\":\"33830793\",\"name\":\"Jeremy S. Smith\"},{\"authorId\":\"40178769\",\"name\":\"Wenjin Lu\"},{\"authorId\":\"1782912\",\"name\":\"Bailing Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b7294dd0ec8ff8d3353286bce6b4ace60f957a08\",\"title\":\"Image Captioning Based on a Hierarchical Attention Mechanism and Policy Gradient Optimization\",\"url\":\"https://www.semanticscholar.org/paper/b7294dd0ec8ff8d3353286bce6b4ace60f957a08\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1007/s00530-018-0598-5\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"title\":\"Multi-guiding long short-term memory for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"venue\":\"Multimedia Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"37498905\",\"name\":\"L. Li\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/BigMM.2018.8499257\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7ae5f10acd306a7842a16542b6b236e0a964de10\",\"title\":\"Saliency-Based Spatiotemporal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7ae5f10acd306a7842a16542b6b236e0a964de10\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152509571\",\"name\":\"Yu Cheng\"},{\"authorId\":\"37743814\",\"name\":\"Y. Shi\"},{\"authorId\":\"48064593\",\"name\":\"Zhiyong Sun\"},{\"authorId\":\"152454596\",\"name\":\"Dezhi Feng\"},{\"authorId\":\"46514887\",\"name\":\"L. Dong\"}],\"doi\":\"10.1109/ICRA.2019.8794327\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"63dd1273df71e25bed96f442bafa4e874a1f8929\",\"title\":\"An Interactive Scene Generation Using Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/63dd1273df71e25bed96f442bafa4e874a1f8929\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72388323\",\"name\":\"Akash Abdu Jyothi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"517d923148ce9451777dd47e9cf368203428d6e0\",\"title\":\"Generating Natural Language Summaries for Image Sets by Akash Abdu Jyothi\",\"url\":\"https://www.semanticscholar.org/paper/517d923148ce9451777dd47e9cf368203428d6e0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"46651452\",\"name\":\"Cong Li\"},{\"authorId\":\"2641581\",\"name\":\"Tianpeng Li\"},{\"authorId\":\"47718901\",\"name\":\"Weitao Wan\"},{\"authorId\":\"47739902\",\"name\":\"J. Chen\"}],\"doi\":\"10.1109/ICIP.2019.8803108\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff24050374748529fa2a1fee6941af08296449f8\",\"title\":\"Image Captioning with Attribute Refinement\",\"url\":\"https://www.semanticscholar.org/paper/ff24050374748529fa2a1fee6941af08296449f8\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"title\":\"Large-scale video analysis and understanding\",\"url\":\"https://www.semanticscholar.org/paper/6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3863922\",\"name\":\"C. Yan\"},{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"48631703\",\"name\":\"Xingzheng Wang\"},{\"authorId\":\"5094646\",\"name\":\"Y. Zhang\"},{\"authorId\":\"145922541\",\"name\":\"Xinhong Hao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144954808\",\"name\":\"Q. Dai\"}],\"doi\":\"10.1109/TMM.2019.2924576\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d\",\"title\":\"STAT: Spatial-Temporal Attention Mechanism for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1603.08507\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1007/978-3-319-46493-0_1\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ecf551d532d0e9cfb252a1bea04d14db620bc488\",\"title\":\"Generating Visual Explanations\",\"url\":\"https://www.semanticscholar.org/paper/ecf551d532d0e9cfb252a1bea04d14db620bc488\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"104510214\",\"name\":\"K. Li\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TETCI.2019.2892755\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"title\":\"Visual to Text: Survey of Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/ICMEW.2017.8026277\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"88a9504f6a1e9b6b56393fde313fc0076e0f27b3\",\"title\":\"Visual saliency for image captioning in new multimedia services\",\"url\":\"https://www.semanticscholar.org/paper/88a9504f6a1e9b6b56393fde313fc0076e0f27b3\",\"venue\":\"2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3048367\",\"name\":\"M. Pedersoli\"},{\"authorId\":\"144982160\",\"name\":\"T. Lucas\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"34602236\",\"name\":\"Jakob Verbeek\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ab7048d6fd8ea5b6daa060aa9996554bd4058f09\",\"title\":\"Areas of Attention for Image Captioning \\u2014 Supplementary Material \\u2014\",\"url\":\"https://www.semanticscholar.org/paper/ab7048d6fd8ea5b6daa060aa9996554bd4058f09\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ICIP.2019.8803785\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"da0cce8d791ca90b01696c6ef0de96c7904dd8cf\",\"title\":\"A Novel Attribute Selection Mechanism for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/da0cce8d791ca90b01696c6ef0de96c7904dd8cf\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":\"1909.05316\",\"authors\":[{\"authorId\":\"145919382\",\"name\":\"J. Hu\"},{\"authorId\":\"145215470\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1700325\",\"name\":\"Graham Neubig\"}],\"doi\":\"10.1609/AAAI.V34I05.6305\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f416f27ba8ae0be32bb4c9a3a50995965a09c449\",\"title\":\"What Makes A Good Story? Designing Composite Rewards for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/f416f27ba8ae0be32bb4c9a3a50995965a09c449\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1812.05634\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00676\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"title\":\"Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.02633\",\"authors\":[{\"authorId\":\"3026553\",\"name\":\"G. Liu\"},{\"authorId\":\"31375865\",\"name\":\"T. H. Hsu\"},{\"authorId\":\"41153596\",\"name\":\"Matthew B. A. McDermott\"},{\"authorId\":\"31809608\",\"name\":\"Willie Boag\"},{\"authorId\":\"2088565\",\"name\":\"Wei-Hung Weng\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"},{\"authorId\":\"145348788\",\"name\":\"M. Ghassemi\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"435d5d96daff445bcc5f44a0e3cecdc46d4120d9\",\"title\":\"Clinically Accurate Chest X-Ray Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/435d5d96daff445bcc5f44a0e3cecdc46d4120d9\",\"venue\":\"MLHC\",\"year\":2019},{\"arxivId\":\"1703.09137\",\"authors\":[{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"2370774\",\"name\":\"K. Camilleri\"}],\"doi\":\"10.1017/S1351324918000098\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6cc46899b415ebef4a70068b2cbd8a50e955aeb6\",\"title\":\"Where to put the Image in an Image Caption Generator\",\"url\":\"https://www.semanticscholar.org/paper/6cc46899b415ebef4a70068b2cbd8a50e955aeb6\",\"venue\":\"Nat. Lang. Eng.\",\"year\":2018},{\"arxivId\":\"1805.08298\",\"authors\":[{\"authorId\":\"46194597\",\"name\":\"C. Y. Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e2a2818ec251d947acd9c74c2040337e656946bc\",\"title\":\"Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/e2a2818ec251d947acd9c74c2040337e656946bc\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1710.08049\",\"authors\":[{\"authorId\":\"3456473\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":\"10.1109/CVPR.2018.00100\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e8613d16ae3ec7263a56a58b9ffdd7dc7fe4e18d\",\"title\":\"Feedback-Prop: Convolutional Neural Network Inference Under Partial Evidence\",\"url\":\"https://www.semanticscholar.org/paper/e8613d16ae3ec7263a56a58b9ffdd7dc7fe4e18d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1709.05038\",\"authors\":[{\"authorId\":\"49693168\",\"name\":\"Yang Xian\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/TIP.2019.2917229\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"54f5c9df478dc510ea8f9c3b52917fb8356904e4\",\"title\":\"Self-Guiding Multimodal LSTM\\u2014When We Do Not Have a Perfect Training Dataset for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/54f5c9df478dc510ea8f9c3b52917fb8356904e4\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"1612.01887\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":\"10.1109/CVPR.2017.345\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f4d7d622d1f7319cc511bfef661cd973e881a4c\",\"title\":\"Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9f4d7d622d1f7319cc511bfef661cd973e881a4c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1708.02043\",\"authors\":[{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"2370774\",\"name\":\"K. Camilleri\"}],\"doi\":\"10.18653/v1/W17-3506\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3d22f972448a2336677ae6ff2877fae010c7dfa2\",\"title\":\"What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption Generator?\",\"url\":\"https://www.semanticscholar.org/paper/3d22f972448a2336677ae6ff2877fae010c7dfa2\",\"venue\":\"INLG\",\"year\":2017},{\"arxivId\":\"1602.07332\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"40591424\",\"name\":\"J. Kravitz\"},{\"authorId\":\"6000646\",\"name\":\"Stephanie Chen\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"1388344504\",\"name\":\"David A. Shamma\"},{\"authorId\":\"1379506718\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-016-0981-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"title\":\"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\",\"url\":\"https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Huan Yang\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"},{\"authorId\":\"3000498\",\"name\":\"Lejian Liao\"}],\"doi\":\"10.1007/978-3-319-97310-4_43\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c1473ca131d8a6030def18ca196b8d39e0665613\",\"title\":\"Image Captioning with Relational Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/c1473ca131d8a6030def18ca196b8d39e0665613\",\"venue\":\"PRICAI\",\"year\":2018},{\"arxivId\":\"1608.05203\",\"authors\":[{\"authorId\":\"1751242\",\"name\":\"Yusuke Sugano\"},{\"authorId\":\"3194727\",\"name\":\"Andreas Bulling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c70ad19c90491e2de8de686b6a49f9bbe44692c0\",\"title\":\"Seeing with Humans: Gaze-Assisted Neural Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c70ad19c90491e2de8de686b6a49f9bbe44692c0\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1805.09019\",\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"cbc3ebf2809edcaa04e252d25f4373c924f4136b\",\"title\":\"CNN+CNN: Convolutional Decoders for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cbc3ebf2809edcaa04e252d25f4373c924f4136b\",\"venue\":\"CVPR 2018\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3127901\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"title\":\"Knowing Yourself: Improving Video Caption via In-depth Recap\",\"url\":\"https://www.semanticscholar.org/paper/3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3005221\",\"name\":\"Zhishen Yang\"},{\"authorId\":\"102837708\",\"name\":\"N. Okazaki\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b1dc881bd497d4466897c1ab9111674d87dda9d0\",\"title\":\"Image Caption Generation for News Articles\",\"url\":\"https://www.semanticscholar.org/paper/b1dc881bd497d4466897c1ab9111674d87dda9d0\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"3145905\",\"name\":\"Jingqiu Zhang\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0530-0\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"title\":\"Exploiting long-term temporal dynamics for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748994\",\"name\":\"D. Nguyen\"},{\"authorId\":\"145178790\",\"name\":\"T. Tran\"}],\"doi\":\"10.1007/978-981-33-4370-2_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ae0211d5247f8044bef50558e95b81746d8709d\",\"title\":\"A Template-Based Approach for Generating Vietnamese References from Flat MR Dataset in Restaurant Domain\",\"url\":\"https://www.semanticscholar.org/paper/2ae0211d5247f8044bef50558e95b81746d8709d\",\"venue\":\"FDSE\",\"year\":2020},{\"arxivId\":\"2012.13122\",\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5318abd4f12a5b25c2847e9c66713951341af504\",\"title\":\"SubICap: Towards Subword-informed Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5318abd4f12a5b25c2847e9c66713951341af504\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.04790\",\"authors\":[{\"authorId\":\"1743722\",\"name\":\"Douwe Kiela\"},{\"authorId\":\"22593971\",\"name\":\"Hamed Firooz\"},{\"authorId\":\"152422011\",\"name\":\"Aravind Mohan\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"1422035486\",\"name\":\"Pratik Ringshia\"},{\"authorId\":\"1389630028\",\"name\":\"Davide Testuggine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51b461040c381cb1489e55ea4b9686c709818b10\",\"title\":\"The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes\",\"url\":\"https://www.semanticscholar.org/paper/51b461040c381cb1489e55ea4b9686c709818b10\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4942547\",\"name\":\"Zongjian Zhang\"},{\"authorId\":\"145698633\",\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"145093625\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/DICTA.2018.8615788\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2ce9fd151c7e67581dfa85b662f5034aca1896e6\",\"title\":\"Size-Invariant Attention Accuracy Metric for Image Captioning with High-Resolution Residual Attention\",\"url\":\"https://www.semanticscholar.org/paper/2ce9fd151c7e67581dfa85b662f5034aca1896e6\",\"venue\":\"2018 Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2018},{\"arxivId\":\"1812.00235\",\"authors\":[{\"authorId\":\"1516206362\",\"name\":\"Tingke Shen\"},{\"authorId\":\"24899770\",\"name\":\"Amlan Kar\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/ICCV.2019.01049\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b4cb0a7617212eb40c537f4053d571faa4b8227c\",\"title\":\"Learning to Caption Images Through a Lifetime by Asking Questions\",\"url\":\"https://www.semanticscholar.org/paper/b4cb0a7617212eb40c537f4053d571faa4b8227c\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47056886\",\"name\":\"Xiangpeng Li\"},{\"authorId\":\"30076791\",\"name\":\"Zhilong Zhou\"},{\"authorId\":\"35153304\",\"name\":\"Lijiang Chen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0531-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"title\":\"Residual attention-based LSTM for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"5477477\",\"name\":\"Martin Renqiang Min\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"title\":\"Recent work often develops a probabilistic model of the caption , conditioned on a video\",\"url\":\"https://www.semanticscholar.org/paper/dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1611.08309\",\"authors\":[{\"authorId\":\"2571049\",\"name\":\"Besmira Nushi\"},{\"authorId\":\"1783184\",\"name\":\"Ece Kamar\"},{\"authorId\":\"145479841\",\"name\":\"E. Horvitz\"},{\"authorId\":\"1691108\",\"name\":\"D. Kossmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"76fa3008f02ffabfaedc46abf192ee1feec4651a\",\"title\":\"On Human Intellect and Machine Failures: Troubleshooting Integrative Machine Learning Systems\",\"url\":\"https://www.semanticscholar.org/paper/76fa3008f02ffabfaedc46abf192ee1feec4651a\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1606.07839\",\"authors\":[{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"3234247\",\"name\":\"Senthil Purushwalkam\"},{\"authorId\":\"37824829\",\"name\":\"M. Cogswell\"},{\"authorId\":\"2673180\",\"name\":\"Viresh Ranjan\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c659f4ca9b7e75abf77133c54ad830ed4ebc0ff0\",\"title\":\"Stochastic Multiple Choice Learning for Training Diverse Deep Ensembles\",\"url\":\"https://www.semanticscholar.org/paper/c659f4ca9b7e75abf77133c54ad830ed4ebc0ff0\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"TANG Peng-Jie\"},{\"authorId\":null,\"name\":\"WANG Han-Li\"},{\"authorId\":null,\"name\":\"XU Kai-Sheng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e16e13992b69eebd9e617c9789349b1c6a84ff48\",\"title\":\"Multi-objective Layer-wise Optimization and Multi-level Probability Fusion for Image Description Generation Using LSTM\",\"url\":\"https://www.semanticscholar.org/paper/e16e13992b69eebd9e617c9789349b1c6a84ff48\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144469308\",\"name\":\"Jian Wang\"},{\"authorId\":\"145534714\",\"name\":\"Jie Feng\"}],\"doi\":\"10.1109/ACCESS.2020.3018546\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"123361e4769f2f8a17742197aa52cc676a4caa9a\",\"title\":\"Hybrid Attention Distribution and Factorized Embedding Matrix in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/123361e4769f2f8a17742197aa52cc676a4caa9a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1809.07041\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1007/978-3-030-01264-9_42\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0000fcfd467a19cf0e59169c2f07d730a0f3a8b9\",\"title\":\"Exploring Visual Relationship for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0000fcfd467a19cf0e59169c2f07d730a0f3a8b9\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Teun de Planque\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5983b0c92f2a619157e2bedf15abb97ed1b0b98f\",\"title\":\"Computer Vision and Deep Learning for Automated Surveillance Technology\",\"url\":\"https://www.semanticscholar.org/paper/5983b0c92f2a619157e2bedf15abb97ed1b0b98f\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144933637\",\"name\":\"S. Sen\"},{\"authorId\":\"145291370\",\"name\":\"A. Raghunathan\"}],\"doi\":\"10.1109/TCAD.2018.2858362\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc80d702c6b7bb8f7990ee2e82cc4f01a25ae539\",\"title\":\"Approximate Computing for Long Short Term Memory (LSTM) Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/bc80d702c6b7bb8f7990ee2e82cc4f01a25ae539\",\"venue\":\"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1952857\",\"name\":\"K. Zheng\"},{\"authorId\":\"144469723\",\"name\":\"C. Zhu\"},{\"authorId\":\"2478555\",\"name\":\"Shaopeng Lu\"},{\"authorId\":\"40457369\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/978-3-030-00776-8_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9808d89c32f661d9fd797a97cc7c3ff2ed2cd260\",\"title\":\"Multiple-Level Feature-Based Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9808d89c32f661d9fd797a97cc7c3ff2ed2cd260\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40235164\",\"name\":\"J. Wu\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"},{\"authorId\":\"144084568\",\"name\":\"Yi Wu\"}],\"doi\":\"10.1145/3271485\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"744d79cfe0b38b2e674c7425dea67492d4f14807\",\"title\":\"Image Captioning via Semantic Guidance Attention and Consensus Selection Strategy\",\"url\":\"https://www.semanticscholar.org/paper/744d79cfe0b38b2e674c7425dea67492d4f14807\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145113946\",\"name\":\"J. Tang\"},{\"authorId\":\"144005516\",\"name\":\"Jing Wang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3291925\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a437bb550d1df02188e4b145e01675551da36336\",\"title\":\"Show, Reward, and Tell\",\"url\":\"https://www.semanticscholar.org/paper/a437bb550d1df02188e4b145e01675551da36336\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1804.09066\",\"authors\":[{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"145264990\",\"name\":\"K. Singh\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1007/978-3-030-01216-8_43\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa63893b34f523973d0692dc74ff22512daac322\",\"title\":\"ECO: Efficient Convolutional Network for Online Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/aa63893b34f523973d0692dc74ff22512daac322\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1803.04376\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"31844147\",\"name\":\"Brian L. Price\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"2490189\",\"name\":\"Gregory Shakhnarovich\"}],\"doi\":\"10.1109/CVPR.2018.00728\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7c1802d8d43dfe783650a03f03d41609fa5ae91e\",\"title\":\"Discriminability Objective for Training Descriptive Captions\",\"url\":\"https://www.semanticscholar.org/paper/7c1802d8d43dfe783650a03f03d41609fa5ae91e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7025059\",\"name\":\"C. Orozco\"},{\"authorId\":\"36507903\",\"name\":\"M. E. Buemi\"},{\"authorId\":\"1405801592\",\"name\":\"J. Jacobo-Berlles\"}],\"doi\":\"10.1109/SCCC.2018.8705254\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"00b0e22ff30f68e6163bfe34beebbf23e3a85813\",\"title\":\"Video to Text Study using an Encoder-Decoder Networks Approach\",\"url\":\"https://www.semanticscholar.org/paper/00b0e22ff30f68e6163bfe34beebbf23e3a85813\",\"venue\":\"2018 37th International Conference of the Chilean Computer Science Society (SCCC)\",\"year\":2018},{\"arxivId\":\"2009.14352\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"1410092701\",\"name\":\"Xu Yang\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":\"10.1007/978-3-030-58568-6_34\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b41882903384ef849688a325d747fdaad8ecee82\",\"title\":\"Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b41882903384ef849688a325d747fdaad8ecee82\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92187979\",\"name\":\"C. Xu\"},{\"authorId\":\"103382547\",\"name\":\"Junzhong Ji\"},{\"authorId\":\"16003095\",\"name\":\"Meng-long Zhang\"},{\"authorId\":\"49470161\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1109/ICUSAI47366.2019.9124779\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72364b3cd61221a99fb6be65e34a10c53db531cd\",\"title\":\"Attention-gated LSTM for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72364b3cd61221a99fb6be65e34a10c53db531cd\",\"venue\":\"2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)\",\"year\":2019},{\"arxivId\":\"1802.08129\",\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2018.00915\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef153ece43ee50f8208f6197f0eaf3d324e4475b\",\"title\":\"Multimodal Explanations: Justifying Decisions and Pointing to the Evidence\",\"url\":\"https://www.semanticscholar.org/paper/ef153ece43ee50f8208f6197f0eaf3d324e4475b\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1909.09060\",\"authors\":[{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"1788029\",\"name\":\"W. Wang\"},{\"authorId\":\"49289638\",\"name\":\"Y. Xia\"},{\"authorId\":\"40445654\",\"name\":\"J. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"312799645adfafb886f156708a7a36f2db459c62\",\"title\":\"Adaptively Aligned Image Captioning via Adaptive Attention Time\",\"url\":\"https://www.semanticscholar.org/paper/312799645adfafb886f156708a7a36f2db459c62\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1909.11974\",\"authors\":[{\"authorId\":\"1390867959\",\"name\":\"Ze Yang\"},{\"authorId\":\"46747953\",\"name\":\"Can Xu\"},{\"authorId\":\"72696576\",\"name\":\"W. Wu\"},{\"authorId\":\"1707275\",\"name\":\"Zhoujun Li\"}],\"doi\":\"10.18653/v1/D19-1512\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3e9b702ae00e0ea4368c4314b6b9882a5205c00e\",\"title\":\"Read, Attend and Comment: A Deep Architecture for Automatic News Comment Generation\",\"url\":\"https://www.semanticscholar.org/paper/3e9b702ae00e0ea4368c4314b6b9882a5205c00e\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49865341\",\"name\":\"Z. Chen\"},{\"authorId\":\"37743827\",\"name\":\"Kui Fan\"},{\"authorId\":null,\"name\":\"Shiqi Wang\"},{\"authorId\":\"7667912\",\"name\":\"L. Duan\"},{\"authorId\":\"144968898\",\"name\":\"W. Lin\"},{\"authorId\":\"98738293\",\"name\":\"A. Kot\"}],\"doi\":\"10.1145/3343031.3350849\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79a7936656d8444932137a0166c11ebdfb2e647d\",\"title\":\"Lossy Intermediate Deep Learning Feature Compression and Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/79a7936656d8444932137a0166c11ebdfb2e647d\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47748857\",\"name\":\"Wei Zhao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"144346837\",\"name\":\"Min Yang\"},{\"authorId\":\"145581826\",\"name\":\"Jianbo Ye\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"28106616\",\"name\":\"Yabing Feng\"},{\"authorId\":\"143970608\",\"name\":\"Y. Qiao\"}],\"doi\":\"10.1145/3132847.3132920\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3844a6cef7960729125722e2d07024058dd9204a\",\"title\":\"Dual Learning for Cross-domain Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3844a6cef7960729125722e2d07024058dd9204a\",\"venue\":\"CIKM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jian Yang\"},{\"authorId\":\"2018112\",\"name\":\"ZuQiang Meng\"}],\"doi\":\"10.1007/978-3-030-00828-4_36\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b41ae2a6f7282aa00fa828e3a1e6e6bfc97bbdb\",\"title\":\"Image Semantic Description Based on Deep Learning with Multi-attention Mechanisms\",\"url\":\"https://www.semanticscholar.org/paper/7b41ae2a6f7282aa00fa828e3a1e6e6bfc97bbdb\",\"venue\":\"Intelligent Information Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51933789\",\"name\":\"N. Ilinykh\"},{\"authorId\":\"2995275\",\"name\":\"Simon Dobnik\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4a8a490f8d3a7414444289b8802052eae3ebbe0c\",\"title\":\"When an Image Tells a Story: The Role of Visual and Semantic Information for Generating Paragraph Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/4a8a490f8d3a7414444289b8802052eae3ebbe0c\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49168719\",\"name\":\"C. Yin\"},{\"authorId\":\"39835284\",\"name\":\"B. Qian\"},{\"authorId\":\"39791510\",\"name\":\"Jishang Wei\"},{\"authorId\":\"4185657\",\"name\":\"X. Li\"},{\"authorId\":\"1491248835\",\"name\":\"X. Zhang\"},{\"authorId\":\"48514123\",\"name\":\"Y. Li\"},{\"authorId\":\"50320297\",\"name\":\"Qinghua Zheng\"}],\"doi\":\"10.1109/ICDM.2019.00083\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b19f5a65e327868bf31a85fc03752b58e09d0d03\",\"title\":\"Automatic Generation of Medical Imaging Diagnostic Report with Hierarchical Recurrent Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/b19f5a65e327868bf31a85fc03752b58e09d0d03\",\"venue\":\"2019 IEEE International Conference on Data Mining (ICDM)\",\"year\":2019},{\"arxivId\":\"1704.03899\",\"authors\":[{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145779951\",\"name\":\"X. Wang\"},{\"authorId\":\"145002061\",\"name\":\"N. Zhang\"},{\"authorId\":\"2936952\",\"name\":\"Xutao Lv\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"}],\"doi\":\"10.1109/CVPR.2017.128\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c689f73f8ea65c6e81c628f2b37feae09b29e46b\",\"title\":\"Deep Reinforcement Learning-Based Image Captioning with Embedding Reward\",\"url\":\"https://www.semanticscholar.org/paper/c689f73f8ea65c6e81c628f2b37feae09b29e46b\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2675822\",\"name\":\"Tushar Karayil\"},{\"authorId\":\"67063678\",\"name\":\"A. Irfan\"},{\"authorId\":\"48258541\",\"name\":\"Federico Raue\"},{\"authorId\":\"33884920\",\"name\":\"J. Hees\"},{\"authorId\":\"145279674\",\"name\":\"A. Dengel\"}],\"doi\":\"10.1007/978-3-030-30490-4_25\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"52941a210e543707b530aa6daf32c18561d8ecc3\",\"title\":\"Conditional GANs for Image Captioning with Sentiments\",\"url\":\"https://www.semanticscholar.org/paper/52941a210e543707b530aa6daf32c18561d8ecc3\",\"venue\":\"ICANN\",\"year\":2019},{\"arxivId\":\"1706.09254\",\"authors\":[{\"authorId\":\"2848048\",\"name\":\"Jekaterina Novikova\"},{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/W17-5525\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"531a7f2c659787165df4fd5b4580590b953448e4\",\"title\":\"The E2E Dataset: New Challenges For End-to-End Generation\",\"url\":\"https://www.semanticscholar.org/paper/531a7f2c659787165df4fd5b4580590b953448e4\",\"venue\":\"SIGDIAL Conference\",\"year\":2017},{\"arxivId\":\"1611.06607\",\"authors\":[{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"153365679\",\"name\":\"J. Johnson\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2017.356\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3a7011346ce939e3251915e92ae2f252e4c7f777\",\"title\":\"A Hierarchical Approach for Generating Descriptive Image Paragraphs\",\"url\":\"https://www.semanticscholar.org/paper/3a7011346ce939e3251915e92ae2f252e4c7f777\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"2004.08070\",\"authors\":[{\"authorId\":\"51163002\",\"name\":\"Alasdair Tran\"},{\"authorId\":\"46953477\",\"name\":\"A. Mathews\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"}],\"doi\":\"10.1109/CVPR42600.2020.01305\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8c9f19acb4a9d56085a6d2f8f1acef7514777345\",\"title\":\"Transform and Tell: Entity-Aware News Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c9f19acb4a9d56085a6d2f8f1acef7514777345\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28918194\",\"name\":\"C. Bartz\"},{\"authorId\":\"1772598\",\"name\":\"Nitisha Jain\"},{\"authorId\":\"3264110\",\"name\":\"Ralf Krestel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9e11eb518d20e2009f0c4ae4e957b7c8ce4d139b\",\"title\":\"Automatic Matching of Paintings and Descriptions in Art-Historic Archives using Multimodal Analysis\",\"url\":\"https://www.semanticscholar.org/paper/9e11eb518d20e2009f0c4ae4e957b7c8ce4d139b\",\"venue\":\"AI4HI\",\"year\":2020},{\"arxivId\":\"2001.06944\",\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"Changyou Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145254492\",\"name\":\"Z. Wen\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"86724befacdd9fd8748607f5b025aa59fb7ef010\",\"title\":\"Nested-Wasserstein Self-Imitation Learning for Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/86724befacdd9fd8748607f5b025aa59fb7ef010\",\"venue\":\"AISTATS\",\"year\":2020},{\"arxivId\":\"1605.08247\",\"authors\":[{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"},{\"authorId\":\"2642022\",\"name\":\"Y. Miyashita\"},{\"authorId\":\"3108668\",\"name\":\"Tomoaki K. Yamabe\"},{\"authorId\":\"3393640\",\"name\":\"Soma Shirakabe\"},{\"authorId\":\"47937389\",\"name\":\"Shin-ichi Sato\"},{\"authorId\":\"29998543\",\"name\":\"H. Hoshino\"},{\"authorId\":\"144802310\",\"name\":\"Ryo Kato\"},{\"authorId\":\"49897653\",\"name\":\"K. Abe\"},{\"authorId\":\"3407486\",\"name\":\"Takaaki Imanari\"},{\"authorId\":\"26851746\",\"name\":\"Naomichi Kobayashi\"},{\"authorId\":\"49133490\",\"name\":\"Shinichiro Morita\"},{\"authorId\":\"2462801\",\"name\":\"A. Nakamura\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"74012279e94f13d44c62f5d5b20dc983b7f5cf84\",\"title\":\"cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey\",\"url\":\"https://www.semanticscholar.org/paper/74012279e94f13d44c62f5d5b20dc983b7f5cf84\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1702212\",\"name\":\"T. Milo\"},{\"authorId\":\"2559241\",\"name\":\"Amit Somech\"}],\"doi\":\"10.1145/3318464.3383126\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb3b2da083353e7e938da06e8641f05b8e3be96f\",\"title\":\"Automating Exploratory Data Analysis via Machine Learning: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/eb3b2da083353e7e938da06e8641f05b8e3be96f\",\"venue\":\"SIGMOD Conference\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50079954\",\"name\":\"X. Li\"},{\"authorId\":\"46611108\",\"name\":\"Mingming Sun\"},{\"authorId\":null,\"name\":\"Ping Li\"}],\"doi\":\"10.1609/AAAI.V33I01.33016096\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e3c0d3a9a35a9679f2c4211b6314bc5e1ed7cf\",\"title\":\"Multi-Agent Discussion Mechanism for Natural Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/e0e3c0d3a9a35a9679f2c4211b6314bc5e1ed7cf\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2004.06577\",\"authors\":[{\"authorId\":\"2720602\",\"name\":\"Hamza Harkous\"},{\"authorId\":\"51893886\",\"name\":\"Isabel Groves\"},{\"authorId\":\"1741702\",\"name\":\"Amir Saffari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1b057feda55c8b86ebd0a7ab0af3a3caa7157a7\",\"title\":\"Have Your Text and Use It Too! End-to-End Neural Data-to-Text Generation with Semantic Fidelity\",\"url\":\"https://www.semanticscholar.org/paper/d1b057feda55c8b86ebd0a7ab0af3a3caa7157a7\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7245576\",\"name\":\"Qi Rao\"},{\"authorId\":\"153185012\",\"name\":\"G. Li\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"47190894\",\"name\":\"F. Zhang\"},{\"authorId\":\"1500519009\",\"name\":\"Ziwei Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b984875c3cf6b7dcaafc078fbab12d54c023ca40\",\"title\":\"UTS ISA Submission at the TRECVID 2019 Video to Text Description Task\",\"url\":\"https://www.semanticscholar.org/paper/b984875c3cf6b7dcaafc078fbab12d54c023ca40\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"1703.09902\",\"authors\":[{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"145210073\",\"name\":\"E. Krahmer\"}],\"doi\":\"10.1613/jair.5477\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d13bb317e87f3f6da10da11059ebf4350b754814\",\"title\":\"Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation\",\"url\":\"https://www.semanticscholar.org/paper/d13bb317e87f3f6da10da11059ebf4350b754814\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"3076466\",\"name\":\"X. Liu\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":\"10.1007/978-3-030-00776-8_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"title\":\"Video Captioning Based on the Spatial-Temporal Saliency Tracing\",\"url\":\"https://www.semanticscholar.org/paper/ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":\"1809.10271\",\"authors\":[{\"authorId\":\"48935207\",\"name\":\"C. Zhang\"},{\"authorId\":\"92687779\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"},{\"authorId\":\"34679323\",\"name\":\"A. Loui\"},{\"authorId\":\"2879097\",\"name\":\"C. Salvaggio\"}],\"doi\":\"10.1109/ICIP.2017.8296359\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"83aab455bbb6cde88f77775f357a852717a192b4\",\"title\":\"Batch-normalized recurrent highway networks\",\"url\":\"https://www.semanticscholar.org/paper/83aab455bbb6cde88f77775f357a852717a192b4\",\"venue\":\"2017 IEEE International Conference on Image Processing (ICIP)\",\"year\":2017},{\"arxivId\":\"1804.09160\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.18653/v1/P18-1083\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0af4274ddc948bb175be7662a699a8270878ced2\",\"title\":\"No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/0af4274ddc948bb175be7662a699a8270878ced2\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2490430\",\"name\":\"Carina Silberer\"},{\"authorId\":\"1823362\",\"name\":\"J. Uijlings\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":\"10.1017/S1351324918000104\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a69f7e80bae350dd51518424abd2d63b73cb6ee6\",\"title\":\"Understanding visual scenes\",\"url\":\"https://www.semanticscholar.org/paper/a69f7e80bae350dd51518424abd2d63b73cb6ee6\",\"venue\":\"Nat. Lang. Eng.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/ICCV.2015.279\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"784da2a7b53a16d2243f747e14946cc5e3476af0\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/784da2a7b53a16d2243f747e14946cc5e3476af0\",\"venue\":\"ICCV\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"1974929\",\"name\":\"Jiewu Xia\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1007/s11042-020-09674-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"title\":\"Double-channel language feature mining based model for video description\",\"url\":\"https://www.semanticscholar.org/paper/0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"venue\":\"Multim. Tools Appl.\",\"year\":2020},{\"arxivId\":\"2011.08543\",\"authors\":[{\"authorId\":\"152601809\",\"name\":\"Minh Thu Nguyen\"},{\"authorId\":\"6195410\",\"name\":\"D. Phung\"},{\"authorId\":\"2356016\",\"name\":\"Minh Hoai\"},{\"authorId\":\"2008200586\",\"name\":\"Thien Huu Nguyen\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.411\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28ffdee5f50398202f236fd088fbc0c624b9f9ce\",\"title\":\"Structural and Functional Decomposition for Personality Image Captioning in a Communication Game\",\"url\":\"https://www.semanticscholar.org/paper/28ffdee5f50398202f236fd088fbc0c624b9f9ce\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"20332986\",\"name\":\"Q. Hu\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TCSVT.2019.2956593\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"230a8581672b3147238eaab2cf686c70fe4f672b\",\"title\":\"Convolutional Reconstruction-to-Sequence for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/230a8581672b3147238eaab2cf686c70fe4f672b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9358850\",\"name\":\"Ruifan Li\"},{\"authorId\":\"4189987\",\"name\":\"Haoyu Liang\"},{\"authorId\":\"46571714\",\"name\":\"Yihui Shi\"},{\"authorId\":\"39825530\",\"name\":\"Fangxiang Feng\"},{\"authorId\":\"39527132\",\"name\":\"Xiaojie Wang\"}],\"doi\":\"10.1016/j.neucom.2020.02.041\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f85b7e09e60315d725b316ffc813d20535b21b2\",\"title\":\"Dual-CNN: A Convolutional language decoder for paragraph image captioning\",\"url\":\"https://www.semanticscholar.org/paper/7f85b7e09e60315d725b316ffc813d20535b21b2\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1911.10082\",\"authors\":[{\"authorId\":\"52150251\",\"name\":\"A. Goel\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"103192742\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4dcd3fa308b263cee8c7581e2d695dc15c2f51f\",\"title\":\"Injecting Prior Knowledge into Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/e4dcd3fa308b263cee8c7581e2d695dc15c2f51f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.07333\",\"authors\":[{\"authorId\":\"1733071048\",\"name\":\"Chao Zeng\"},{\"authorId\":\"1687386\",\"name\":\"S. Kwong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7212df671c50beb567e3d3d608b0c14405c40e3\",\"title\":\"Intrinsic Image Captioning Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/d7212df671c50beb567e3d3d608b0c14405c40e3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145414742\",\"name\":\"Kun Fu\"},{\"authorId\":\"2512850\",\"name\":\"Junqi Jin\"},{\"authorId\":\"2153397\",\"name\":\"Runpeng Cui\"},{\"authorId\":\"39543696\",\"name\":\"Fei Sha\"},{\"authorId\":\"14966740\",\"name\":\"Changshui Zhang\"}],\"doi\":\"10.1109/TPAMI.2016.2642953\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"afb353801ce723951f0d8f9ed4b5ff9b41615601\",\"title\":\"Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts\",\"url\":\"https://www.semanticscholar.org/paper/afb353801ce723951f0d8f9ed4b5ff9b41615601\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1705.08759\",\"authors\":[{\"authorId\":\"144207643\",\"name\":\"Q. Sun\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPR.2017.763\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1cee733ee31e245dac4655a870fd9226163a52b5\",\"title\":\"Bidirectional Beam Search: Forward-Backward Inference in Neural Sequence Models for Fill-in-the-Blank Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1cee733ee31e245dac4655a870fd9226163a52b5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6aee8323be7ad5e568d62ba368bc7123f750515f\",\"title\":\"Paying More Attention to Saliency: Image Captioning with Saliency and Context Attention\",\"url\":\"https://www.semanticscholar.org/paper/6aee8323be7ad5e568d62ba368bc7123f750515f\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1806.02934\",\"authors\":[{\"authorId\":\"51043791\",\"name\":\"A. Kalyan\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"145721096\",\"name\":\"A. Kannan\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d2237f995d280edb8a47e27886261709ea3f654a\",\"title\":\"Learn from Your Neighbor: Learning Multi-modal Mappings from Sparse Annotations\",\"url\":\"https://www.semanticscholar.org/paper/d2237f995d280edb8a47e27886261709ea3f654a\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042646882\",\"name\":\"Yifan Fan\"},{\"authorId\":\"1899109615\",\"name\":\"Xudong Luo\"}],\"doi\":\"10.1109/ICTAI50040.2020.00182\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"511007727c15180c60096d2bbdd0d13677b1f799\",\"title\":\"A Survey of Dialogue System Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/511007727c15180c60096d2bbdd0d13677b1f799\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":\"1807.09418\",\"authors\":[{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TMM.2019.2930041\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"92e02bd58b99ac17b475081611f091f4b0776482\",\"title\":\"Video Storytelling: Textual Summaries for Events\",\"url\":\"https://www.semanticscholar.org/paper/92e02bd58b99ac17b475081611f091f4b0776482\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"2004.12274\",\"authors\":[{\"authorId\":\"29860450\",\"name\":\"Baoyu Jing\"},{\"authorId\":\"1905077\",\"name\":\"Zeya Wang\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.18653/v1/P19-1657\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f08b8d6f2df54675c9b83fb115e63df763ea32fb\",\"title\":\"Show, Describe and Conclude: On Exploiting the Structure Information of Chest X-ray Reports\",\"url\":\"https://www.semanticscholar.org/paper/f08b8d6f2df54675c9b83fb115e63df763ea32fb\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.1145/3123266.3127895\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"title\":\"Richer Semantic Visual and Language Representation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2795882\",\"name\":\"Samuel Humeau\"},{\"authorId\":null,\"name\":\"Hexiang Hu\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5c8ad080ccb3f5e3c999c2948029f0bd005d5635\",\"title\":\"ENGAGING IMAGE CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/5c8ad080ccb3f5e3c999c2948029f0bd005d5635\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145959949\",\"name\":\"J. Serrano\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e07f8a6b7d805b3622a584ec97a269d54a405e1d\",\"title\":\"Boosting image captioning with an attentional mechanism = Boosting image captioning using diverse beam search\",\"url\":\"https://www.semanticscholar.org/paper/e07f8a6b7d805b3622a584ec97a269d54a405e1d\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b02dba59a087f16d8286aec5e6481d5952a37df5\",\"title\":\"CMU Sinbad\\u2019s Submission for the DSTC7 AVSD Challenge\",\"url\":\"https://www.semanticscholar.org/paper/b02dba59a087f16d8286aec5e6481d5952a37df5\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1944765097\",\"name\":\"Beigeng Zhao\"}],\"doi\":\"10.1109/ACCESS.2020.3021312\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"acbbb090caf30b6f8bf140f03ebe912c9e6a8c6d\",\"title\":\"DrunaliaCap: Image Captioning for Drug-Related Paraphernalia With Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/acbbb090caf30b6f8bf140f03ebe912c9e6a8c6d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2003.04865\",\"authors\":[{\"authorId\":\"3087214\",\"name\":\"Yutaro Shigeto\"},{\"authorId\":\"31678456\",\"name\":\"Y. Yoshikawa\"},{\"authorId\":\"2996464\",\"name\":\"Jiaqing Lin\"},{\"authorId\":\"39702069\",\"name\":\"A. Takeuchi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"title\":\"Video Caption Dataset for Describing Human Actions in Japanese\",\"url\":\"https://www.semanticscholar.org/paper/46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":\"1906.04375\",\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/CVPR.2019.00852\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"title\":\"Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1685660929\",\"name\":\"L. Abisha Anto Ignatious.\"},{\"authorId\":\"49378012\",\"name\":\"S. Jeevitha\"},{\"authorId\":\"1685613977\",\"name\":\"M. Madhurambigai.\"},{\"authorId\":\"145007954\",\"name\":\"M. Hemalatha\"}],\"doi\":\"10.1109/ICoAC48765.2019.246867\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61418aa297aa5b2353c8456e4aa801ae75ab2a47\",\"title\":\"A Semantic Driven CNN \\u2013 LSTM Architecture for Personalised Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/61418aa297aa5b2353c8456e4aa801ae75ab2a47\",\"venue\":\"2019 11th International Conference on Advanced Computing (ICoAC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"2841633\",\"name\":\"Hyunmin Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.18653/v1/N19-1011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4798919e74411d87f7745840e45b8bcf61128ff\",\"title\":\"AudioCaps: Generating Captions for Audios in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/c4798919e74411d87f7745840e45b8bcf61128ff\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/MSP.2017.2741510\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c5b803c2fee9bbf6d9132f633de70332b5e80a4d\",\"title\":\"Deep Learning for Image-to-Text Generation: A Technical Overview\",\"url\":\"https://www.semanticscholar.org/paper/c5b803c2fee9bbf6d9132f633de70332b5e80a4d\",\"venue\":\"IEEE Signal Processing Magazine\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7504183\",\"name\":\"B. Zhang\"},{\"authorId\":\"50703807\",\"name\":\"L. Li\"},{\"authorId\":\"153142919\",\"name\":\"Li Su\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"1994580036\",\"name\":\"Jincan Deng\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1145/3394171.3413885\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4b0209f6fa41030e7e590f2cb5568f47563a759a\",\"title\":\"Structural Semantic Adversarial Active Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b0209f6fa41030e7e590f2cb5568f47563a759a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20630261\",\"name\":\"X. Xiao\"},{\"authorId\":\"40585252\",\"name\":\"L. Wang\"},{\"authorId\":\"33969294\",\"name\":\"K. Ding\"},{\"authorId\":\"1683738\",\"name\":\"S. Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.1016/J.PATCOG.2019.01.028\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18cb67e5970c4b1f0949a6f6db8f8cbaa68b2eb1\",\"title\":\"Dense semantic embedding network for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/18cb67e5970c4b1f0949a6f6db8f8cbaa68b2eb1\",\"venue\":\"Pattern Recognit.\",\"year\":2019},{\"arxivId\":\"2012.04983\",\"authors\":[{\"authorId\":\"1405301761\",\"name\":\"Hedi Ben-younes\"},{\"authorId\":\"2034015064\",\"name\":\"'Eloi Zablocki\"},{\"authorId\":\"1398301486\",\"name\":\"P. P\\u00e9rez\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44194713255b6ee890ce1a2d2b727d30e0b71cdb\",\"title\":\"Driving Behavior Explanation with Multi-level Fusion\",\"url\":\"https://www.semanticscholar.org/paper/44194713255b6ee890ce1a2d2b727d30e0b71cdb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2065332\",\"name\":\"H. Lee\"},{\"authorId\":\"152333274\",\"name\":\"Seunghyun Yoon\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"2007775508\",\"name\":\"Doo Soon Kim\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"1731707\",\"name\":\"K. Jung\"}],\"doi\":\"10.18653/v1/2020.eval4nlp-1.4\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2be4e374800a0db69695eb4c558a6653dd258fcd\",\"title\":\"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT\",\"url\":\"https://www.semanticscholar.org/paper/2be4e374800a0db69695eb4c558a6653dd258fcd\",\"venue\":\"EVAL4NLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314360\",\"name\":\"Weixuan Wang\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"}],\"doi\":\"10.1007/s11063-019-10005-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2cf83b5d0c02cfdf877edc5547737ef3501e5deb\",\"title\":\"Image Captioning Using Region-Based Attention Joint with Time-Varying Attention\",\"url\":\"https://www.semanticscholar.org/paper/2cf83b5d0c02cfdf877edc5547737ef3501e5deb\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":\"1903.12020\",\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":\"10.1109/CVPR.2019.00432\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"90579a68e46b772a8e9aaca8ecbd06942d0b9b35\",\"title\":\"Describing Like Humans: On Diversity in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/90579a68e46b772a8e9aaca8ecbd06942d0b9b35\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122865911\",\"name\":\"Mie Mie Aung\"},{\"authorId\":\"145750914\",\"name\":\"Myint San\"},{\"authorId\":\"3169864\",\"name\":\"Phyu Phyu Khaing\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"45505889a0cf75a830545080d6b8b57cc2e2525f\",\"title\":\"Natural Language Description Generation for Image using Deep Learning Architecture\",\"url\":\"https://www.semanticscholar.org/paper/45505889a0cf75a830545080d6b8b57cc2e2525f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66320916\",\"name\":\"Ruturaj Nene\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7813cdf27e637118a7f3e804a745854dca78ceae\",\"title\":\"Caption Generation for Images Using Deep Multimodal Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/7813cdf27e637118a7f3e804a745854dca78ceae\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1701.03126\",\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1747615\",\"name\":\"Teng-Yok Lee\"},{\"authorId\":\"7969330\",\"name\":\"Ziming Zhang\"},{\"authorId\":\"145222187\",\"name\":\"B. Harsham\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"145441213\",\"name\":\"K. Sumi\"}],\"doi\":\"10.1109/ICCV.2017.450\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08903ceeee6420992d30ff3f3b8b4830118af4d9\",\"title\":\"Attention-Based Multimodal Fusion for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/08903ceeee6420992d30ff3f3b8b4830118af4d9\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1810.12535\",\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":\"10.1007/978-3-030-20870-7_2\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"74eda5e2a4a34b9d4a737da755b136455c947339\",\"title\":\"Gated Hierarchical Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/74eda5e2a4a34b9d4a737da755b136455c947339\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51893886\",\"name\":\"Isabel Groves\"},{\"authorId\":\"145509096\",\"name\":\"Y. Tian\"},{\"authorId\":\"51891223\",\"name\":\"Ioannis Douratsos\"}],\"doi\":\"10.18653/v1/W18-6512\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"417654e325d79405c2a10d9d017dbba6d35170a3\",\"title\":\"Treat the system like a human student: Automatic naturalness evaluation of generated text without reference texts\",\"url\":\"https://www.semanticscholar.org/paper/417654e325d79405c2a10d9d017dbba6d35170a3\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":\"1511.07571\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"title\":\"DenseCap: Fully Convolutional Localization Networks for Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47523598\",\"name\":\"T. Nguyen\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/WNYIPW.2017.8356255\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3417673c59544fcd33820a0a583a7543c70ac595\",\"title\":\"Multistream hierarchical boundary network for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/3417673c59544fcd33820a0a583a7543c70ac595\",\"venue\":\"2017 IEEE Western New York Image and Signal Processing Workshop (WNYISPW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3180110\",\"name\":\"Renyu Zhang\"},{\"authorId\":\"145533896\",\"name\":\"C. Weber\"},{\"authorId\":\"153660172\",\"name\":\"R. Grossman\"},{\"authorId\":\"1794512\",\"name\":\"A. Khan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"629b93eaf76a445ae5672b30a13a6815c6242c42\",\"title\":\"Evaluating and interpreting caption prediction for histopathology images\",\"url\":\"https://www.semanticscholar.org/paper/629b93eaf76a445ae5672b30a13a6815c6242c42\",\"venue\":\"MLHC\",\"year\":2020},{\"arxivId\":\"2010.15251\",\"authors\":[{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"51510489\",\"name\":\"Marius Mosbach\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5fc773d9c8896b25c87e4a6f97fc47cbe6ab74ef\",\"title\":\"Fusion Models for Improved Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5fc773d9c8896b25c87e4a6f97fc47cbe6ab74ef\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144751998\",\"name\":\"C. He\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"}],\"doi\":\"10.1145/3292058\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b0ef7ccad215c682a784d1aec4988b675d779b9\",\"title\":\"Image Captioning With Visual-Semantic Double Attention\",\"url\":\"https://www.semanticscholar.org/paper/1b0ef7ccad215c682a784d1aec4988b675d779b9\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1807.02250\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"1795294\",\"name\":\"M. Dras\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"119899233\",\"name\":\"Len Hamey\"}],\"doi\":\"10.1007/978-3-030-10925-7_14\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"036c29c2c2a2494faae205c0f270ce4d076cc3f2\",\"title\":\"Face-Cap: Image Captioning using Facial Expression Analysis\",\"url\":\"https://www.semanticscholar.org/paper/036c29c2c2a2494faae205c0f270ce4d076cc3f2\",\"venue\":\"ECML/PKDD\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876240\",\"name\":\"G. Awad\"},{\"authorId\":\"1926279\",\"name\":\"A. Butt\"},{\"authorId\":\"144175619\",\"name\":\"K. Curtis\"},{\"authorId\":\"2700261\",\"name\":\"Y. Lee\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"1387499537\",\"name\":\"Afzad Godil\"},{\"authorId\":\"35783968\",\"name\":\"David Joy\"},{\"authorId\":\"74967014\",\"name\":\"A. Delgado\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"},{\"authorId\":\"2000709\",\"name\":\"Y. Graham\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1380928907\",\"name\":\"G. Qu\\u00e9not\"},{\"authorId\":\"144431718\",\"name\":\"J. Magalh\\u00e3es\"},{\"authorId\":\"3442611\",\"name\":\"David Semedo\"},{\"authorId\":\"1795778\",\"name\":\"Saverio G. Blasi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"129a80d668d0bc9e82889459a636986bc1691018\",\"title\":\"TRECVID 2018: Benchmarking Video Activity Detection, Video Captioning and Matching, Video Storytelling Linking and Video Search\",\"url\":\"https://www.semanticscholar.org/paper/129a80d668d0bc9e82889459a636986bc1691018\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"title\":\"Incorporating Semantic Attention in Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"venue\":\"LREC\",\"year\":2018},{\"arxivId\":\"1809.06214\",\"authors\":[{\"authorId\":null,\"name\":\"Cheng Kuan Chen\"},{\"authorId\":\"1383271298\",\"name\":\"Zhufeng Pan\"},{\"authorId\":\"30885811\",\"name\":\"Min Sun\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"}],\"doi\":\"10.1609/AAAI.V33I01.33018151\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5e79138fd656ebea59eb008fcb97c97be7be8007\",\"title\":\"Unsupervised Stylish Image Description Generation via Domain Layer Norm\",\"url\":\"https://www.semanticscholar.org/paper/5e79138fd656ebea59eb008fcb97c97be7be8007\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"153576781\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/JIOT.2017.2779865\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"title\":\"Attention-in-Attention Networks for Surveillance Video Understanding in Internet of Things\",\"url\":\"https://www.semanticscholar.org/paper/d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"venue\":\"IEEE Internet of Things Journal\",\"year\":2018},{\"arxivId\":\"1711.03800\",\"authors\":[{\"authorId\":\"2326243\",\"name\":\"Arun Balajee Vasudevan\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/WACV.2018.00206\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d6a4a34829b3b55497210ddbe88ad63ff801faae\",\"title\":\"Object Referring in Visual Scene with Spoken Language\",\"url\":\"https://www.semanticscholar.org/paper/d6a4a34829b3b55497210ddbe88ad63ff801faae\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"1711.06330\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":\"10.1109/CVPR.2018.00710\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66aebb3af16aaa78579344784212ae10f60ec27e\",\"title\":\"Attend and Interact: Higher-Order Object Interactions for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/66aebb3af16aaa78579344784212ae10f60ec27e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2010.03644\",\"authors\":[{\"authorId\":\"51439692\",\"name\":\"Wanrong Zhu\"},{\"authorId\":\"47120131\",\"name\":\"X. Wang\"},{\"authorId\":\"145382418\",\"name\":\"P. Narayana\"},{\"authorId\":\"113477341\",\"name\":\"Kazoo Sone\"},{\"authorId\":\"40632403\",\"name\":\"S. Basu\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.708\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1e1b42c2dccb19d9ac441a3c0e62d2d21e3bd198\",\"title\":\"Towards Understanding Sample Variance in Visually Grounded Language Generation: Evaluations and Observations\",\"url\":\"https://www.semanticscholar.org/paper/1e1b42c2dccb19d9ac441a3c0e62d2d21e3bd198\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2002.11566\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":null,\"name\":\"Bing Li\"},{\"authorId\":\"39397292\",\"name\":\"Peijin Wang\"},{\"authorId\":\"48594951\",\"name\":\"Weiming Hu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1109/cvpr42600.2020.01329\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f1dd557a8839733a5ee06d19989a265e61f603c1\",\"title\":\"Object Relational Graph With Teacher-Recommended Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1dd557a8839733a5ee06d19989a265e61f603c1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1911.02739\",\"authors\":[{\"authorId\":\"72871419\",\"name\":\"Zhihan Zhang\"},{\"authorId\":\"1770874\",\"name\":\"Zhiyi Yin\"},{\"authorId\":\"1906099\",\"name\":\"Shuhuai Ren\"},{\"authorId\":\"78145275\",\"name\":\"Xinhang Li\"},{\"authorId\":\"50341802\",\"name\":\"S. Li\"}],\"doi\":\"10.1007/978-3-030-60457-8_1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"422f098e1e5b6fa34e73ad83557ba793ca3c0403\",\"title\":\"DCA: Diversified Co-Attention towards Informative Live Video Commenting\",\"url\":\"https://www.semanticscholar.org/paper/422f098e1e5b6fa34e73ad83557ba793ca3c0403\",\"venue\":\"NLPCC\",\"year\":2020},{\"arxivId\":\"1909.04402\",\"authors\":[{\"authorId\":\"1387994359\",\"name\":\"Mitja Nikolaus\"},{\"authorId\":\"30671790\",\"name\":\"M. Abdou\"},{\"authorId\":\"48024953\",\"name\":\"Matthew Lamm\"},{\"authorId\":\"19509693\",\"name\":\"Rahul Aralikatte\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"}],\"doi\":\"10.18653/v1/K19-1009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"11557ee241f6b5dc0a8c62bb8014c0111bbe45ba\",\"title\":\"Compositional Generalization in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/11557ee241f6b5dc0a8c62bb8014c0111bbe45ba\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"1904.01766\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"49588480\",\"name\":\"A. Myers\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2019.00756\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c41a11c0e9b8b92b4faaf97749841170b760760a\",\"title\":\"VideoBERT: A Joint Model for Video and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2012.07061\",\"authors\":[{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"46491945\",\"name\":\"Yunpeng Luo\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"1993645531\",\"name\":\"Gen Luo\"},{\"authorId\":\"47096329\",\"name\":\"Yongjian Wu\"},{\"authorId\":\"40366236\",\"name\":\"Yue Gao\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7217b5d8d0fb753532026cc36b0aaa056960c6f8\",\"title\":\"Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network\",\"url\":\"https://www.semanticscholar.org/paper/7217b5d8d0fb753532026cc36b0aaa056960c6f8\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1702.05658\",\"authors\":[{\"authorId\":null,\"name\":\"Chang Liu\"},{\"authorId\":\"143823065\",\"name\":\"F. Sun\"},{\"authorId\":\"1697065\",\"name\":\"C. Wang\"},{\"authorId\":\"39369840\",\"name\":\"Feng Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.24963/ijcai.2017/563\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2498124e6466ccde28c95477c923e7cd5843f4c0\",\"title\":\"MAT: A Multimodal Attentive Translator for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2498124e6466ccde28c95477c923e7cd5843f4c0\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/ICCV.2017.20\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"62e6b749ed5825739aa906021c5e613803d5cbe2\",\"title\":\"Temporal Tessellation: A Unified Approach for Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/62e6b749ed5825739aa906021c5e613803d5cbe2\",\"venue\":\"ICCV\",\"year\":2017},{\"arxivId\":\"1905.12198\",\"authors\":[{\"authorId\":\"5040052\",\"name\":\"Jiangjie Chen\"},{\"authorId\":\"145601976\",\"name\":\"Ao Wang\"},{\"authorId\":\"48579460\",\"name\":\"Haiyun Jiang\"},{\"authorId\":\"51225575\",\"name\":\"S. Feng\"},{\"authorId\":\"2243964\",\"name\":\"C. Li\"},{\"authorId\":\"3011950\",\"name\":\"Y. Xiao\"}],\"doi\":\"10.18653/v1/P19-1196\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"de6219302bae6dfcabb5a4088777ee2afc97d6bc\",\"title\":\"Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/de6219302bae6dfcabb5a4088777ee2afc97d6bc\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1905.07841\",\"authors\":[{\"authorId\":\"9919436\",\"name\":\"J. Yu\"},{\"authorId\":\"31115234\",\"name\":\"Jing Li\"},{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/TCSVT.2019.2947482\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"99b5153235b0ee583803bbd7cd6bd9da161d5348\",\"title\":\"Multimodal Transformer With Multi-View Visual Representation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/99b5153235b0ee583803bbd7cd6bd9da161d5348\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"153173208\",\"name\":\"J. Xu\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2019.11.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"title\":\"Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into LSTM-based model\",\"url\":\"https://www.semanticscholar.org/paper/2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":\"1511.04590\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.5244/C.30.141\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"16aac81ae033f7295d82e5b679400d105170a3e1\",\"title\":\"Oracle Performance for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16aac81ae033f7295d82e5b679400d105170a3e1\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":\"1805.00460\",\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/CVPR.2018.00930\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"09efde3bd0a380e8cbcd55a13694648276c2c166\",\"title\":\"Customized Image Narrative Generation via Interactive Visual Question Generation and Answering\",\"url\":\"https://www.semanticscholar.org/paper/09efde3bd0a380e8cbcd55a13694648276c2c166\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2010.12852\",\"authors\":[{\"authorId\":\"34408936\",\"name\":\"R. Dua\"},{\"authorId\":\"2003624403\",\"name\":\"Sai Srinivas Kancheti\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc80d27ca291823fc0be464a736cf72dbb4ac191\",\"title\":\"Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions\",\"url\":\"https://www.semanticscholar.org/paper/fc80d27ca291823fc0be464a736cf72dbb4ac191\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.07999\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.706\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"9f2a7a912624d850cf9e0587263b4fcd88d44f18\",\"title\":\"What is More Likely to Happen Next? Video-and-Language Future Event Prediction\",\"url\":\"https://www.semanticscholar.org/paper/9f2a7a912624d850cf9e0587263b4fcd88d44f18\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67084368\",\"name\":\"J. Zhang\"},{\"authorId\":\"1977355586\",\"name\":\"Xu Wang\"},{\"authorId\":\"153505016\",\"name\":\"H. Zhang\"},{\"authorId\":\"145631821\",\"name\":\"Hailong Sun\"},{\"authorId\":\"49543976\",\"name\":\"Xudong Liu\"}],\"doi\":\"10.1145/3377811.3380383\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bd47bb8cdd749a3356149da6155d2dcd7458779f\",\"title\":\"Retrieval-based Neural Source Code Summarization\",\"url\":\"https://www.semanticscholar.org/paper/bd47bb8cdd749a3356149da6155d2dcd7458779f\",\"venue\":\"2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)\",\"year\":2020},{\"arxivId\":\"2010.11140\",\"authors\":[{\"authorId\":\"151504883\",\"name\":\"Yan Zeng\"},{\"authorId\":\"50204644\",\"name\":\"J. Nie\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"314358d6e9969cb89da0ad0b6aa4f406294d3ff4\",\"title\":\"Generalized Conditioned Dialogue Generation Based on Pre-trained Language Model\",\"url\":\"https://www.semanticscholar.org/paper/314358d6e9969cb89da0ad0b6aa4f406294d3ff4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2015.7298878\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5838af587938e74b5758414c384dcf16dd6e1d1e\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/5838af587938e74b5758414c384dcf16dd6e1d1e\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":\"2002.05637\",\"authors\":[{\"authorId\":\"9546292\",\"name\":\"Justin Sybrandt\"},{\"authorId\":\"2926850\",\"name\":\"Ilya Safro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c754f1b71591f5ec0a75c041f9cc231c1cc939ac\",\"title\":\"CBAG: Conditional Biomedical Abstract Generation\",\"url\":\"https://www.semanticscholar.org/paper/c754f1b71591f5ec0a75c041f9cc231c1cc939ac\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"90745780\",\"name\":\"Thomas Scialom\"},{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"1502219404\",\"name\":\"Paul-Alexis Dray\"},{\"authorId\":\"1767493\",\"name\":\"Jacopo Staiano\"},{\"authorId\":\"67192547\",\"name\":\"P. Gallinari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b32d02d7f6fa954e7ae500f94f6a5efd113751a4\",\"title\":\"BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations\",\"url\":\"https://www.semanticscholar.org/paper/b32d02d7f6fa954e7ae500f94f6a5efd113751a4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Jiang\"},{\"authorId\":null,\"name\":\"Qi Zhao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3977c0056755c2911811509ac38e0cef532004b0\",\"title\":\"Self-Distillation for Few-Shot Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3977c0056755c2911811509ac38e0cef532004b0\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"150898549\",\"name\":\"Alessandro Devo\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/TMM.2019.2924598\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"61a10a61e95ede1064567be38196a2348af3fb6c\",\"title\":\"The Role of the Input in Natural Language Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61a10a61e95ede1064567be38196a2348af3fb6c\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":\"10.1007/978-3-319-46604-0_33\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"95df7770a5036c87104df23f333aa05e67723cdc\",\"title\":\"DeepDiary: Automatically Captioning Lifelogging Image Streams\",\"url\":\"https://www.semanticscholar.org/paper/95df7770a5036c87104df23f333aa05e67723cdc\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35869944\",\"name\":\"Yanwu Shu\"},{\"authorId\":\"48570973\",\"name\":\"L. Zhang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"}],\"doi\":\"10.1007/978-981-10-8530-7_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5450079dfa0f405f01b3669b60601e752eaad8cf\",\"title\":\"Bidirectional Multimodal Recurrent Neural Networks with Refined Visual Features for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5450079dfa0f405f01b3669b60601e752eaad8cf\",\"venue\":\"ICIMCS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49645155\",\"name\":\"Yan-shuo Chang\"}],\"doi\":\"10.1007/s11042-017-4593-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f2617da2d4a0466784809981babcdb7f697b5f24\",\"title\":\"Fine-grained attention for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/f2617da2d4a0466784809981babcdb7f697b5f24\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2017},{\"arxivId\":\"2007.02676\",\"authors\":[{\"authorId\":\"2120694\",\"name\":\"K. Nguyen\"},{\"authorId\":\"3193584\",\"name\":\"Konstantinos Drossos\"},{\"authorId\":\"50195877\",\"name\":\"T. Virtanen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"55ed3b1d5665036082d5a91b5f66a61a689b583e\",\"title\":\"Temporal Sub-sampling of Audio Feature Sequences for Automated Audio Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55ed3b1d5665036082d5a91b5f66a61a689b583e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Yunchen Pu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ce86da3e0a0ab5f6cf944cdfef7a607a61b16e2\",\"title\":\"Deep Generative Models for Image Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/9ce86da3e0a0ab5f6cf944cdfef7a607a61b16e2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103839060\",\"name\":\"Ariyo Oluwasanmi\"},{\"authorId\":\"32593111\",\"name\":\"Muhammad Umar Aftab\"},{\"authorId\":\"30511154\",\"name\":\"Eatedal Alabdulkreem\"},{\"authorId\":\"1395577736\",\"name\":\"Bulbula Kumeda\"},{\"authorId\":\"46352756\",\"name\":\"Edward Y. Baagyere\"},{\"authorId\":\"152179239\",\"name\":\"Zhiquang Qin\"}],\"doi\":\"10.1109/ACCESS.2019.2931223\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cfe95e9dca1f07db5f0672c65943f22aace00e04\",\"title\":\"CaptionNet: Automatic End-to-End Siamese Difference Captioning Model With Attention\",\"url\":\"https://www.semanticscholar.org/paper/cfe95e9dca1f07db5f0672c65943f22aace00e04\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3355010\",\"name\":\"Greg Casta\\u00f1\\u00f3n\"},{\"authorId\":\"2962929\",\"name\":\"Nathan Shnidman\"},{\"authorId\":\"50780334\",\"name\":\"Tim Anderson\"},{\"authorId\":\"152187759\",\"name\":\"Jeffrey Byrne\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aeea6a28ff7290e9e7d9188df569ad75e79788bc\",\"title\":\"Dataset Paper Videos Hours Classes Description Charades Hollywood in Homes : Crowdsourcing Data Collection for Activity Understanding 9848\",\"url\":\"https://www.semanticscholar.org/paper/aeea6a28ff7290e9e7d9188df569ad75e79788bc\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"title\":\"Natural Language Description of Images and Videos\",\"url\":\"https://www.semanticscholar.org/paper/d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66511580\",\"name\":\"Mirza Muhammad Ali Baig\"},{\"authorId\":\"67307335\",\"name\":\"Mian Ihtisham Shah\"},{\"authorId\":\"9223428\",\"name\":\"Muhammad Abdullah Wajahat\"},{\"authorId\":\"2384836\",\"name\":\"Nauman Zafar\"},{\"authorId\":\"1744665\",\"name\":\"Omar Arif\"}],\"doi\":\"10.1109/DICTA.2018.8615810\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d3aef1e8acf2b1760d7685d73f08d614f95ef75d\",\"title\":\"Image Caption Generator with Novel Object Injection\",\"url\":\"https://www.semanticscholar.org/paper/d3aef1e8acf2b1760d7685d73f08d614f95ef75d\",\"venue\":\"2018 Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471428747\",\"name\":\"Yang Zhen-yu\"},{\"authorId\":\"1471428729\",\"name\":\"Zhang Jiao\"}],\"doi\":\"10.1109/ICAIT.2019.8935913\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bfa80ccf8530de52fe9ae4f617cdb57fa81f8f4c\",\"title\":\"Image Caption Method Combining Multi-angle with Multi-modality\",\"url\":\"https://www.semanticscholar.org/paper/bfa80ccf8530de52fe9ae4f617cdb57fa81f8f4c\",\"venue\":\"2019 IEEE 11th International Conference on Advanced Infocomm Technology (ICAIT)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145408682\",\"name\":\"J. Lambert\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac207f5e368e285b0dd54387e3a898c550249b20\",\"title\":\"Stacked RNNs for Encoder-Decoder Networks : Accurate Machine Understanding of Images\",\"url\":\"https://www.semanticscholar.org/paper/ac207f5e368e285b0dd54387e3a898c550249b20\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2010.01999\",\"authors\":[{\"authorId\":\"1986560192\",\"name\":\"Ruchika Chavhan\"},{\"authorId\":\"49124777\",\"name\":\"Biplab Banerjee\"},{\"authorId\":\"40049070\",\"name\":\"X. Zhu\"},{\"authorId\":\"144527833\",\"name\":\"S. Chaudhuri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"2e67be13c2d1d8bb45cd2a3130532a4d8f7f82be\",\"title\":\"A Novel Actor Dual-Critic Model for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2e67be13c2d1d8bb45cd2a3130532a4d8f7f82be\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46583706\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"},{\"authorId\":\"50857536\",\"name\":\"Z. Wang\"},{\"authorId\":\"145855523\",\"name\":\"D. Feng\"},{\"authorId\":\"145808910\",\"name\":\"Tieniu Tan\"}],\"doi\":\"10.1016/j.patcog.2019.107075\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"70b03dfb947ca136e3476c9f49b8f6b044d0c1d9\",\"title\":\"Learning visual relationship and context-aware attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/70b03dfb947ca136e3476c9f49b8f6b044d0c1d9\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":\"2010.03743\",\"authors\":[{\"authorId\":\"13941532\",\"name\":\"Fuxiao Liu\"},{\"authorId\":null,\"name\":\"Yinghan Wang\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"51f2eb8b16d132d3c403ed76d3ac816a8f58f25c\",\"title\":\"VisualNews : Benchmark and Challenges in Entity-aware Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/51f2eb8b16d132d3c403ed76d3ac816a8f58f25c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143692915\",\"name\":\"Ying Hua Tan\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"}],\"doi\":\"10.1016/j.neucom.2018.12.026\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"760a7ed58cf49fa5dbcfb7f06b67cea2cd35f768\",\"title\":\"Phrase-based image caption generator with hierarchical LSTM network\",\"url\":\"https://www.semanticscholar.org/paper/760a7ed58cf49fa5dbcfb7f06b67cea2cd35f768\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"4770950\",\"name\":\"D. Joy\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1406426956\",\"name\":\"Georges Qu\\u00e9not\"},{\"authorId\":\"2203861\",\"name\":\"Maria Eskevich\"},{\"authorId\":\"144597794\",\"name\":\"R. Aly\"},{\"authorId\":\"32488932\",\"name\":\"Roeland Ordelman\"},{\"authorId\":\"51091173\",\"name\":\"M. Ritter\"},{\"authorId\":\"143723939\",\"name\":\"G. Jones\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"},{\"authorId\":\"104810482\",\"name\":\"M. Larson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c10c5d3fd3575570ec55ddbe838fb2a52cf97bc9\",\"title\":\"TRECVID 2016: Evaluating Video Search, Video Event Detection, Localization, and Hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/c10c5d3fd3575570ec55ddbe838fb2a52cf97bc9\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"145558284\",\"name\":\"Kai Lei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4eb0b7ced6c022eef4c6fb2ed3dcdbdccfc056dc\",\"title\":\"Aligning Visual Regions and Textual Concepts: Learning Fine-Grained Image Representations for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4eb0b7ced6c022eef4c6fb2ed3dcdbdccfc056dc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"3493516\",\"name\":\"Yifan Xiong\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/2964284.2984065\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7492cac0babe8d514995bcde6456ae00c17325a3\",\"title\":\"Describing Videos using Multi-modal Fusion\",\"url\":\"https://www.semanticscholar.org/paper/7492cac0babe8d514995bcde6456ae00c17325a3\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52177940\",\"name\":\"Edy Mulyanto\"},{\"authorId\":\"9723334\",\"name\":\"Esther Irawati Setiawan\"},{\"authorId\":\"3408219\",\"name\":\"E. M. Yuniarno\"},{\"authorId\":\"153121000\",\"name\":\"M. H. Purnomo\"}],\"doi\":\"10.1109/CIVEMSA45640.2019.9071632\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d009ef2e76203ce6801b7740b97ba6354d9c0e59\",\"title\":\"Automatic Indonesian Image Caption Generation using CNN-LSTM Model and FEEH-ID Dataset\",\"url\":\"https://www.semanticscholar.org/paper/d009ef2e76203ce6801b7740b97ba6354d9c0e59\",\"venue\":\"2019 IEEE International Conference on Computational Intelligence and Virtual Environments for Measurement Systems and Applications (CIVEMSA)\",\"year\":2019},{\"arxivId\":\"1902.00669\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"47190894\",\"name\":\"F. Zhang\"}],\"doi\":\"10.1609/aaai.v33i01.33018909\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe33597affd4e99a5dc979ef4ed99ee6311fdc2b\",\"title\":\"Hierarchical Photo-Scene Encoder for Album Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/fe33597affd4e99a5dc979ef4ed99ee6311fdc2b\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1607.00410\",\"authors\":[{\"authorId\":\"144270828\",\"name\":\"Yusuke Watanabe\"},{\"authorId\":\"20851195\",\"name\":\"Kazuma Hashimoto\"},{\"authorId\":\"143946906\",\"name\":\"Yoshimasa Tsuruoka\"}],\"doi\":\"10.18653/v1/W16-1629\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"7277fdf51bb3fc38f254a5546c53caa4169653c9\",\"title\":\"Domain Adaptation for Neural Networks by Parameter Augmentation\",\"url\":\"https://www.semanticscholar.org/paper/7277fdf51bb3fc38f254a5546c53caa4169653c9\",\"venue\":\"Rep4NLP@ACL\",\"year\":2016},{\"arxivId\":\"1812.08658\",\"authors\":[{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"46395829\",\"name\":\"Yufei Wang\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"145461380\",\"name\":\"Rishabh Jain\"},{\"authorId\":\"1607624548\",\"name\":\"Mark Johnson\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606363958\",\"name\":\"Devi Parikh\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"}],\"doi\":\"10.1109/ICCV.2019.00904\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"title\":\"nocaps: novel object captioning at scale\",\"url\":\"https://www.semanticscholar.org/paper/8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34692779\",\"name\":\"K. Chang\"},{\"authorId\":\"10421443\",\"name\":\"Kung-Hung Lu\"},{\"authorId\":\"1720473\",\"name\":\"Chu-Song Chen\"}],\"doi\":\"10.1109/ICCV.2017.380\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c86716de32aaa0c9bce6f711b67c76c9fcecbb85\",\"title\":\"Aesthetic Critiques Generation for Photos\",\"url\":\"https://www.semanticscholar.org/paper/c86716de32aaa0c9bce6f711b67c76c9fcecbb85\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1507.01053\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.1109/TMM.2015.2477044\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"654a3e53fb41d8168798ee0ee61dfab73739b1ed\",\"title\":\"Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks\",\"url\":\"https://www.semanticscholar.org/paper/654a3e53fb41d8168798ee0ee61dfab73739b1ed\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2015},{\"arxivId\":\"1803.07950\",\"authors\":[{\"authorId\":\"4322411\",\"name\":\"L. Li\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/WACV.2019.00042\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"title\":\"End-to-End Video Captioning With Multitask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":\"1508.02091\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"2761119\",\"name\":\"N. Savva\"},{\"authorId\":\"3035230\",\"name\":\"Michael J. Wilber\"}],\"doi\":\"10.18653/v1/W15-2807\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9b40f47c522093d4fc674b1247953e7475426fb6\",\"title\":\"Image Representations and New Domains in Neural Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9b40f47c522093d4fc674b1247953e7475426fb6\",\"venue\":\"VL@EMNLP\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49292515\",\"name\":\"Z. Liu\"},{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":null,\"name\":\"Chen Shen\"},{\"authorId\":\"145592290\",\"name\":\"R. Ji\"}],\"doi\":\"10.1007/978-3-319-97304-3_32\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe3af3e0e06c1d0adf5173adea06c268692be10d\",\"title\":\"Topic-Guided Automatical Human-Simulated Tweeting System\",\"url\":\"https://www.semanticscholar.org/paper/fe3af3e0e06c1d0adf5173adea06c268692be10d\",\"venue\":\"PRICAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288466\",\"name\":\"Xian-Hua Zeng\"},{\"authorId\":\"145780928\",\"name\":\"B. Liu\"},{\"authorId\":\"40318021\",\"name\":\"M. Zhou\"}],\"doi\":\"10.1007/s11390-018-1874-8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f0243850576e364368c3f743012e78165d8bf249\",\"title\":\"Understanding and Generating Ultrasound Image Description\",\"url\":\"https://www.semanticscholar.org/paper/f0243850576e364368c3f743012e78165d8bf249\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36595248\",\"name\":\"Fang Fang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"50581334\",\"name\":\"Y. Chen\"},{\"authorId\":\"8275214\",\"name\":\"P. Tang\"}],\"doi\":\"10.1007/s11042-018-6228-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c580b0a8dc655a8831ddbb7954bb929b1f236ebc\",\"title\":\"Looking deeper and transferring attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/c580b0a8dc655a8831ddbb7954bb929b1f236ebc\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":\"1709.09346\",\"authors\":[{\"authorId\":\"145534769\",\"name\":\"N. Ding\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9bcef44c5a383499f80df861744465e62f6ba33f\",\"title\":\"Cold-Start Reinforcement Learning with Softmax Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/9bcef44c5a383499f80df861744465e62f6ba33f\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1908.04919\",\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"58a77455b1c38afe1eab4bec664bd866eba1573d\",\"title\":\"Towards Diverse and Accurate Image Captions via Reinforcing Determinantal Point Process\",\"url\":\"https://www.semanticscholar.org/paper/58a77455b1c38afe1eab4bec664bd866eba1573d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2706292\",\"name\":\"Xianhua Zeng\"},{\"authorId\":\"145117241\",\"name\":\"L. Wen\"},{\"authorId\":\"115986457\",\"name\":\"Yang Xu\"},{\"authorId\":\"1893927760\",\"name\":\"Conghui Ji\"}],\"doi\":\"10.1016/j.cmpb.2020.105700\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cbe08984fe9bb669e11e73b8fb14c9d43ab57017\",\"title\":\"Generating diagnostic report for medical image by high-middle-level visual information incorporation on double deep learning models\",\"url\":\"https://www.semanticscholar.org/paper/cbe08984fe9bb669e11e73b8fb14c9d43ab57017\",\"venue\":\"Comput. Methods Programs Biomed.\",\"year\":2020},{\"arxivId\":\"1706.00457\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"1389738351\",\"name\":\"M. Garc\\u00eda-Mart\\u00ednez\"},{\"authorId\":\"19192985\",\"name\":\"Adrien Bardet\"},{\"authorId\":\"3202143\",\"name\":\"Walid Aransa\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"2934336\",\"name\":\"Lo\\u00efc Barrault\"}],\"doi\":\"10.1515/pralin-2017-0035\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e42fcc6fd59289a196f6ca50b43c9c1d7b2fab8\",\"title\":\"NMTPY: A Flexible Toolkit for Advanced Neural Machine Translation Systems\",\"url\":\"https://www.semanticscholar.org/paper/2e42fcc6fd59289a196f6ca50b43c9c1d7b2fab8\",\"venue\":\"Prague Bull. Math. Linguistics\",\"year\":2017},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2003.12058\",\"authors\":[{\"authorId\":\"4055152\",\"name\":\"Sarah Pratt\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"20745881\",\"name\":\"Luca Weihs\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2684226\",\"name\":\"Aniruddha Kembhavi\"}],\"doi\":\"10.1007/978-3-030-58548-8_19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc261c0efb5f9ce82581932d1440630b861fb85f\",\"title\":\"Grounded Situation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fc261c0efb5f9ce82581932d1440630b861fb85f\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14320047\",\"name\":\"C. Patil\"},{\"authorId\":\"34326205\",\"name\":\"Manasi S. Patwardhan\"}],\"doi\":\"10.1145/3383465\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9e081431cfbab4d5c8823efa2620ca0accfa124e\",\"title\":\"Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/9e081431cfbab4d5c8823efa2620ca0accfa124e\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":\"2008.07935\",\"authors\":[{\"authorId\":\"48269537\",\"name\":\"Ye Zhu\"},{\"authorId\":\"50118837\",\"name\":\"Y. Wu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"96519714\",\"name\":\"Yan Yan\"}],\"doi\":\"10.1007/978-3-030-58592-1_10\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"29ef96b859c2ec55b67fca6f2e59ac27a6bc2cb1\",\"title\":\"Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents\",\"url\":\"https://www.semanticscholar.org/paper/29ef96b859c2ec55b67fca6f2e59ac27a6bc2cb1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1612.07833\",\"authors\":[{\"authorId\":\"145534769\",\"name\":\"N. Ding\"},{\"authorId\":\"7685850\",\"name\":\"Sebastian Goodman\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6edb41364802b0fdd1e3e98d644fe78b1ecbbe45\",\"title\":\"Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task\",\"url\":\"https://www.semanticscholar.org/paper/6edb41364802b0fdd1e3e98d644fe78b1ecbbe45\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2007.04660\",\"authors\":[{\"authorId\":\"9561899\",\"name\":\"Emre \\u00c7akir\"},{\"authorId\":\"3193584\",\"name\":\"Konstantinos Drossos\"},{\"authorId\":\"50195877\",\"name\":\"T. Virtanen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6288091c99a1d6b4fbc5d173a9b16c5501f9f550\",\"title\":\"Multi-task Regularization Based on Infrequent Classes for Audio Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6288091c99a1d6b4fbc5d173a9b16c5501f9f550\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.15406\",\"authors\":[{\"authorId\":\"1417133896\",\"name\":\"Sergi Perez-Castanos\"},{\"authorId\":\"1419458795\",\"name\":\"Javier Naranjo-Alcazar\"},{\"authorId\":\"2450434\",\"name\":\"P. Zuccarello\"},{\"authorId\":\"2432536\",\"name\":\"M. Cobos\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6b789ac73566e685839fdb610cbcd9cd43ca899a\",\"title\":\"Listen carefully and tell: an audio captioning system based on residual learning and gammatone audio representation\",\"url\":\"https://www.semanticscholar.org/paper/6b789ac73566e685839fdb610cbcd9cd43ca899a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3396987\",\"name\":\"Sharon Zhou\"},{\"authorId\":\"39504881\",\"name\":\"M. Gordon\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"89315058\",\"name\":\"Austin Narcomey\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dbdeac01daf69c3725032c42be04f42c17ea439d\",\"title\":\"HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/dbdeac01daf69c3725032c42be04f42c17ea439d\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1909.02622\",\"authors\":[{\"authorId\":\"47748857\",\"name\":\"Wei Zhao\"},{\"authorId\":\"35512303\",\"name\":\"Maxime Peyrard\"},{\"authorId\":\"144544919\",\"name\":\"Fei Liu\"},{\"authorId\":\"152128108\",\"name\":\"Yang Gao\"},{\"authorId\":\"145575695\",\"name\":\"C. Meyer\"},{\"authorId\":\"2620186\",\"name\":\"Steffen Eger\"}],\"doi\":\"10.18653/v1/D19-1053\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"635cb6fb865e86c108c5d1d895aeac0e759eb199\",\"title\":\"MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance\",\"url\":\"https://www.semanticscholar.org/paper/635cb6fb865e86c108c5d1d895aeac0e759eb199\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478695\",\"name\":\"Teng Jiang\"},{\"authorId\":\"2598444\",\"name\":\"Chengjun Zhang\"},{\"authorId\":\"7607492\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1145/3319921.3319934\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1da9da475a471eacdb47a7f71d81cc3517188054\",\"title\":\"Reconstructing Attention with Dynamic Regularization\",\"url\":\"https://www.semanticscholar.org/paper/1da9da475a471eacdb47a7f71d81cc3517188054\",\"venue\":\"ICIAI 2019\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"49470127\",\"name\":\"X. Zhang\"},{\"authorId\":\"48593034\",\"name\":\"Guohua Dong\"},{\"authorId\":\"1406222945\",\"name\":\"Zhigang Luo\"}],\"doi\":\"10.1007/s11042-019-08209-5\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"311ee1284e6bdf035f8c3a1a20d00c5841b2e341\",\"title\":\"Object-aware semantics of attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/311ee1284e6bdf035f8c3a1a20d00c5841b2e341\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721364\",\"name\":\"Sina Zarrie\\u00df\"},{\"authorId\":\"1817455\",\"name\":\"David Schlangen\"}],\"doi\":\"10.18653/v1/W18-6563\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f5350ef1d45574e33f5b0f1c013a5bb00e1b1c55\",\"title\":\"Decoding Strategies for Neural Referring Expression Generation\",\"url\":\"https://www.semanticscholar.org/paper/f5350ef1d45574e33f5b0f1c013a5bb00e1b1c55\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"65956766\",\"name\":\"B. Barrows\"},{\"authorId\":\"39679686\",\"name\":\"L. L. Vie\"},{\"authorId\":\"30820439\",\"name\":\"Erica L. Meszaros\"},{\"authorId\":\"150143077\",\"name\":\"James E. Ecker\"},{\"authorId\":\"144911735\",\"name\":\"B. Allen\"}],\"doi\":\"10.2514/6.2020-1113\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"26812aec173e7129004062f3517d2804afde90e9\",\"title\":\"Analyzing Natural Language Context in Human-Machine Teaming using Supervised Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/26812aec173e7129004062f3517d2804afde90e9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1947015\",\"name\":\"Zhaowei Qu\"},{\"authorId\":\"1845790885\",\"name\":\"Luhan Zhang\"},{\"authorId\":\"38435706\",\"name\":\"X. Wang\"},{\"authorId\":\"2148442\",\"name\":\"Bingyu Cao\"},{\"authorId\":\"93648239\",\"name\":\"Yueli Li\"},{\"authorId\":\"145987554\",\"name\":\"F. Li\"}],\"doi\":\"10.1109/IWCMC48107.2020.9148294\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e7c1b1ec7fd51c8ee9e2b45eee9c095834b0b56\",\"title\":\"KSF-ST: Video Captioning Based on Key Semantic Frames Extraction and Spatio-Temporal Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/8e7c1b1ec7fd51c8ee9e2b45eee9c095834b0b56\",\"venue\":\"2020 International Wireless Communications and Mobile Computing (IWCMC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26903445\",\"name\":\"H. Mantec\\u00f3n\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"},{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d0af1363c2d03e5d2435d9ba2b05ca5aecd568fd\",\"title\":\"PicSOM and EURECOM Experiments in TRECVID 2019\",\"url\":\"https://www.semanticscholar.org/paper/d0af1363c2d03e5d2435d9ba2b05ca5aecd568fd\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7934466\",\"name\":\"Junwei Zhou\"},{\"authorId\":\"47119743\",\"name\":\"X. Wang\"},{\"authorId\":\"2710247\",\"name\":\"Jizhong Han\"},{\"authorId\":\"144553025\",\"name\":\"S. Hu\"},{\"authorId\":\"2755326\",\"name\":\"Hongchao Gao\"}],\"doi\":\"10.1109/BigMM.2018.8499060\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0486c77b8f251e8ae52f5ab6304288c1a8bcc48a\",\"title\":\"Spatial- Temporal Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0486c77b8f251e8ae52f5ab6304288c1a8bcc48a\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"36251013\",\"name\":\"Wei Li\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1007/s11063-017-9591-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"37eaf94fa6619ee857019937677cb055a2a51bf3\",\"title\":\"Capturing Temporal Structures for Video Captioning by Spatio-temporal Contexts and Channel Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/37eaf94fa6619ee857019937677cb055a2a51bf3\",\"venue\":\"Neural Processing Letters\",\"year\":2017},{\"arxivId\":\"2005.08045\",\"authors\":[{\"authorId\":\"152516150\",\"name\":\"Aniket Agarwal\"},{\"authorId\":\"1703122502\",\"name\":\"Ayush Mangal\"},{\"authorId\":\"80465887\",\"name\":\"Vipul\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ac73d0abab68604b9c7a2b98635516eda899c7b3\",\"title\":\"Visual Relationship Detection using Scene Graphs: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/ac73d0abab68604b9c7a2b98635516eda899c7b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"},{\"authorId\":\"2013546\",\"name\":\"B. Wang\"},{\"authorId\":\"2263014\",\"name\":\"Xiangtao Zheng\"}],\"doi\":\"10.1109/TGRS.2019.2951636\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a86714cb7ac711054244aeea51a55715e679ebb\",\"title\":\"Sound Active Attention Framework for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2a86714cb7ac711054244aeea51a55715e679ebb\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2020},{\"arxivId\":\"2012.13136\",\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"39689790\",\"name\":\"L. White\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"46641911\",\"name\":\"W. Liu\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"}],\"doi\":\"10.1007/s11263-019-01206-z\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"47286105575aacaed5ef74af9ae007e258abc60a\",\"title\":\"LCEval: Learned Composite Metric for Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/47286105575aacaed5ef74af9ae007e258abc60a\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"2011.12870\",\"authors\":[{\"authorId\":\"1960607091\",\"name\":\"Yi Zhou\"},{\"authorId\":\"8157338\",\"name\":\"Zhenhao Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"14e5d984af4ac965579644bd0ea4ae42587b6a00\",\"title\":\"Multimodal Learning for Hateful Memes Detection\",\"url\":\"https://www.semanticscholar.org/paper/14e5d984af4ac965579644bd0ea4ae42587b6a00\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.00569\",\"authors\":[{\"authorId\":\"50535497\",\"name\":\"Jiahong Huang\"},{\"authorId\":\"46962482\",\"name\":\"C. H. Yang\"},{\"authorId\":\"144097210\",\"name\":\"Fangyu Liu\"},{\"authorId\":\"1935549028\",\"name\":\"Meng Tian\"},{\"authorId\":\"51224607\",\"name\":\"Yi-Chieh Liu\"},{\"authorId\":\"48865440\",\"name\":\"Tingwei Wu\"},{\"authorId\":\"2607585\",\"name\":\"I. Lin\"},{\"authorId\":\"1771700\",\"name\":\"K. Wang\"},{\"authorId\":\"7505210\",\"name\":\"H. Morikawa\"},{\"authorId\":\"34452443\",\"name\":\"Herng-Hua Chang\"},{\"authorId\":\"1402915033\",\"name\":\"Jesper Tegn\\u00e9r\"},{\"authorId\":\"1717056\",\"name\":\"M. Worring\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3e77b77dacb36bae9264efb93685efcab126171e\",\"title\":\"DeepOpht: Medical Report Generation for Retinal Images via Deep Models and Visual Explanation\",\"url\":\"https://www.semanticscholar.org/paper/3e77b77dacb36bae9264efb93685efcab126171e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.05876\",\"authors\":[{\"authorId\":\"46659203\",\"name\":\"L. Wang\"},{\"authorId\":\"1486057423\",\"name\":\"Zechen Bai\"},{\"authorId\":\"48378975\",\"name\":\"Yonghua Zhang\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"}],\"doi\":\"10.1609/aaai.v34i07.6898\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"85079d64d6fdd0ba5318fda119d152f2d2946391\",\"title\":\"Show, Recall, and Tell: Image Captioning with Recall Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/85079d64d6fdd0ba5318fda119d152f2d2946391\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"145303641\",\"name\":\"Yu Xing\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"145201124\",\"name\":\"Xiang Ren\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b7cb2e9d884427ef50b564d97d3fd403953afa6\",\"title\":\"COMMONGEN: Towards Generative Commonsense Reasoning via A Constrained Text Generation Challenge\",\"url\":\"https://www.semanticscholar.org/paper/8b7cb2e9d884427ef50b564d97d3fd403953afa6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3192572\",\"name\":\"Emiel van Miltenburg\"},{\"authorId\":\"2045556\",\"name\":\"R. Koolen\"},{\"authorId\":\"145210073\",\"name\":\"E. Krahmer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d42142285c46207a16bd4294e437d504e419a9b7\",\"title\":\"Varying image description tasks: spoken versus written descriptions\",\"url\":\"https://www.semanticscholar.org/paper/d42142285c46207a16bd4294e437d504e419a9b7\",\"venue\":\"VarDial@COLING 2018\",\"year\":2018},{\"arxivId\":\"1612.03557\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"72643925\",\"name\":\"Minsu Cho\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"afbff808f4a4c6eafcce3858451b9b1a508ecba3\",\"title\":\"Text-Guided Attention Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/afbff808f4a4c6eafcce3858451b9b1a508ecba3\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11082747\",\"name\":\"A. Wilson\"},{\"authorId\":\"19251475\",\"name\":\"A. Raglin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"516d084e014cb91b91eed5fdbd5a4a2c4bbeb928\",\"title\":\"The Effect of Training Data Set Composition on the Performance of a Neural Image Caption Generator\",\"url\":\"https://www.semanticscholar.org/paper/516d084e014cb91b91eed5fdbd5a4a2c4bbeb928\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50754085\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"49479171\",\"name\":\"Chen Fang\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"2047181\",\"name\":\"Seokhwan Kim\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"144015161\",\"name\":\"Y. Fu\"}],\"doi\":\"10.1109/cvpr42600.2020.01254\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1a780c6219996c8481c117056efcf071cbfbd15\",\"title\":\"Screencast Tutorial Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b1a780c6219996c8481c117056efcf071cbfbd15\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9432180\",\"name\":\"R. Kumar\"}],\"doi\":\"10.1007/s42979-020-00135-w\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9b3a8a857467fd389d835f4f6c9fcf5777ed026a\",\"title\":\"Visual Linguistic Model and Its Applications in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9b3a8a857467fd389d835f4f6c9fcf5777ed026a\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":\"1906.06362\",\"authors\":[{\"authorId\":\"7975935\",\"name\":\"Daphne Ippolito\"},{\"authorId\":\"46218926\",\"name\":\"Reno Kriz\"},{\"authorId\":\"51119291\",\"name\":\"M. Kustikova\"},{\"authorId\":\"2662374\",\"name\":\"Jo\\u00e3o Sedoc\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"}],\"doi\":\"10.18653/v1/P19-1365\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd846869e6f25d9b1a524aef8b54a08b81a1b1fa\",\"title\":\"Comparison of Diverse Decoding Methods from Conditional Language Models\",\"url\":\"https://www.semanticscholar.org/paper/fd846869e6f25d9b1a524aef8b54a08b81a1b1fa\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2692833\",\"name\":\"Wenbin Che\"},{\"authorId\":\"152213403\",\"name\":\"Xiaopeng Fan\"},{\"authorId\":\"145419122\",\"name\":\"R. Xiong\"},{\"authorId\":\"47783359\",\"name\":\"D. Zhao\"}],\"doi\":\"10.1109/TMM.2019.2954750\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4021baab5b3cda4347bab93a0426b0ab122764a7\",\"title\":\"Visual Relationship Embedding Network for Image Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/4021baab5b3cda4347bab93a0426b0ab122764a7\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145093155\",\"name\":\"Min Xu\"},{\"authorId\":\"1677510167\",\"name\":\"Lingxiang Wu\"},{\"authorId\":\"121134294\",\"name\":\"S. Qian\"},{\"authorId\":\"3061725\",\"name\":\"Jianwei Cui\"}],\"doi\":\"10.1145/3381858\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"238611bc2a79d64271a2238cf164bcfde3e5cb13\",\"title\":\"Image to Modern Chinese Poetry Creation via a Constrained Topic-aware Model\",\"url\":\"https://www.semanticscholar.org/paper/238611bc2a79d64271a2238cf164bcfde3e5cb13\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"1912.03098\",\"authors\":[{\"authorId\":\"1403171438\",\"name\":\"J. Pont-Tuset\"},{\"authorId\":\"1823362\",\"name\":\"J. Uijlings\"},{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"}],\"doi\":\"10.1007/978-3-030-58558-7_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"title\":\"Connecting Vision and Language with Localized Narratives\",\"url\":\"https://www.semanticscholar.org/paper/439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151497541\",\"name\":\"X. Meng\"},{\"authorId\":\"151474857\",\"name\":\"Hao Kong\"},{\"authorId\":\"151486196\",\"name\":\"D. Tang\"},{\"authorId\":\"144720255\",\"name\":\"T. Lu\"}],\"doi\":\"10.1109/ICME.2019.00229\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"95ddec4b0ab0024da1e80b66b0a29e0102c0b8fe\",\"title\":\"Multimodal Image Captioning Through Combining Reinforced Cross Entropy Loss and Stochastic Deprecation\",\"url\":\"https://www.semanticscholar.org/paper/95ddec4b0ab0024da1e80b66b0a29e0102c0b8fe\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1911.03905\",\"authors\":[{\"authorId\":\"1387241031\",\"name\":\"Ondvrej Duvsek\"},{\"authorId\":\"1845460\",\"name\":\"David M. Howcroft\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/W19-8652\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e1241f017a627beca2542e378a88c642c32098b\",\"title\":\"Semantic Noise Matters for Neural Natural Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/9e1241f017a627beca2542e378a88c642c32098b\",\"venue\":\"INLG\",\"year\":2019},{\"arxivId\":\"1810.10665\",\"authors\":[{\"authorId\":\"35752280\",\"name\":\"Kurt Shuster\"},{\"authorId\":\"2795882\",\"name\":\"Samuel Humeau\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":\"10.1109/CVPR.2019.01280\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c677000c9078fdff8622be15a37db7d4945f36c2\",\"title\":\"Engaging Image Captioning via Personality\",\"url\":\"https://www.semanticscholar.org/paper/c677000c9078fdff8622be15a37db7d4945f36c2\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1907.13280\",\"authors\":[{\"authorId\":\"2531558\",\"name\":\"G. Chao\"},{\"authorId\":\"2188497\",\"name\":\"Abhinav Rastogi\"},{\"authorId\":\"3014143\",\"name\":\"Semih Yavuz\"},{\"authorId\":\"152325757\",\"name\":\"D. Hakkani-T\\u00fcr\"},{\"authorId\":\"47740493\",\"name\":\"Jindong Chen\"},{\"authorId\":\"5347612\",\"name\":\"I. Lane\"}],\"doi\":\"10.18653/v1/W19-5926\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fb5af7b7d5ebf9e8ff4164233a73b8fe4fe737a3\",\"title\":\"Learning Question-Guided Video Representation for Multi-Turn Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/fb5af7b7d5ebf9e8ff4164233a73b8fe4fe737a3\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":\"1803.05526\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1007/978-3-030-01246-5_31\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05544876b7bc58b59b9ec5a8ee09e3ce9b4791ce\",\"title\":\"Unpaired Image Captioning by Language Pivoting\",\"url\":\"https://www.semanticscholar.org/paper/05544876b7bc58b59b9ec5a8ee09e3ce9b4791ce\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1912522\",\"name\":\"R. Qader\"},{\"authorId\":\"51929029\",\"name\":\"Khoder Jneid\"},{\"authorId\":\"1730318\",\"name\":\"Fran\\u00e7ois Portet\"},{\"authorId\":\"34737743\",\"name\":\"C. Labb\\u00e9\"}],\"doi\":\"10.18653/v1/W18-6532\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a446e4648fd6801b550379bc40be063682c6ba33\",\"title\":\"Generation of Company descriptions using concept-to-text and text-to-text deep models: dataset collection and systems evaluation\",\"url\":\"https://www.semanticscholar.org/paper/a446e4648fd6801b550379bc40be063682c6ba33\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":\"1903.02507\",\"authors\":[{\"authorId\":\"3259825\",\"name\":\"Jiayun Li\"},{\"authorId\":\"39367903\",\"name\":\"Mohammad K. Ebrahimpour\"},{\"authorId\":\"33129821\",\"name\":\"Azadeh Moghtaderi\"},{\"authorId\":\"1915432\",\"name\":\"Yen-Yun Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ea752a837e24686a5774e6cbe6f787a76cd014d\",\"title\":\"Image captioning with weakly-supervised attention penalty\",\"url\":\"https://www.semanticscholar.org/paper/2ea752a837e24686a5774e6cbe6f787a76cd014d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1911.03738\",\"authors\":[{\"authorId\":\"32227979\",\"name\":\"Marc Tanti\"},{\"authorId\":\"145464131\",\"name\":\"Albert Gatt\"},{\"authorId\":\"2370774\",\"name\":\"Kenneth P. Camilleri\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a55f5de768a455ab01fbe0432962f2f6f1a0a7db\",\"title\":\"On Architectures for Including Visual Information in Neural Language Models for Image Description\",\"url\":\"https://www.semanticscholar.org/paper/a55f5de768a455ab01fbe0432962f2f6f1a0a7db\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144081629\",\"name\":\"Mario G\\u00f3mez Mart\\u00ednez\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"faa8314259e9de1af8e841c265b0251531b32e04\",\"title\":\"Deep learning for image captioning: an encoder-decoder architecture with soft attention\",\"url\":\"https://www.semanticscholar.org/paper/faa8314259e9de1af8e841c265b0251531b32e04\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47458613\",\"name\":\"A. Tripathi\"},{\"authorId\":\"145305735\",\"name\":\"Siddharth Srivastava\"},{\"authorId\":\"144174561\",\"name\":\"R. Kothari\"}],\"doi\":\"10.1007/978-3-030-04780-1_23\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a819e54b2d980dcff85ac8ab13801178aac5b59a\",\"title\":\"Deep Neural Network Based Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a819e54b2d980dcff85ac8ab13801178aac5b59a\",\"venue\":\"BDA\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145359993\",\"name\":\"Fabian Karl\"},{\"authorId\":\"2388081\",\"name\":\"Mikko Lauri\"},{\"authorId\":\"31565315\",\"name\":\"Chris Biemann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dfd1533d49d0213278d3d3301491cbe40f4d100b\",\"title\":\"Creating Information-maximizing Natural Language Messages Through Image Captioning-Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/dfd1533d49d0213278d3d3301491cbe40f4d100b\",\"venue\":\"KONVENS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3444092\",\"name\":\"Lianhui Qin\"},{\"authorId\":\"2978364\",\"name\":\"L. Liu\"},{\"authorId\":\"51178911\",\"name\":\"Victoria Bi\"},{\"authorId\":\"90862747\",\"name\":\"Y. Wang\"},{\"authorId\":\"3028405\",\"name\":\"Xiaojiang Liu\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"97628251\",\"name\":\"Hai Zhao\"},{\"authorId\":\"47295584\",\"name\":\"Shuming Shi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"88cfd800eab72c6efd3555b65e5ca5725664acf4\",\"title\":\"Commenting : the Task and Dataset\",\"url\":\"https://www.semanticscholar.org/paper/88cfd800eab72c6efd3555b65e5ca5725664acf4\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"title\":\"Multi-Modal Deep Learning to Understand Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4237bdd7df710c08b58d02bb48f8dcecd2521a80\",\"title\":\"Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/4237bdd7df710c08b58d02bb48f8dcecd2521a80\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1606.03632\",\"authors\":[{\"authorId\":\"145478041\",\"name\":\"Shikhar Sharma\"},{\"authorId\":\"143852605\",\"name\":\"Jing He\"},{\"authorId\":\"2987426\",\"name\":\"Kaheer Suleman\"},{\"authorId\":\"1944614\",\"name\":\"Hannes Schulz\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8932c433b41c955ef87acefbdbfcbf81a00faeef\",\"title\":\"Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data\",\"url\":\"https://www.semanticscholar.org/paper/8932c433b41c955ef87acefbdbfcbf81a00faeef\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134654394\",\"name\":\"Ren C. Luo\"},{\"authorId\":\"93576138\",\"name\":\"H. Lin\"}],\"doi\":\"10.1109/ICPHYS.2019.8780257\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db10bea7273386e48db1907bdf7d8d86d6cb5484\",\"title\":\"Coping with Overfitting Problems of Image Caption Models for Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/db10bea7273386e48db1907bdf7d8d86d6cb5484\",\"venue\":\"2019 IEEE International Conference on Industrial Cyber Physical Systems (ICPS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"3374688\",\"name\":\"M. Tian\"},{\"authorId\":\"151118825\",\"name\":\"Mengmeng Jiang\"},{\"authorId\":null,\"name\":\"Heng Liu\"},{\"authorId\":\"30631999\",\"name\":\"Donghu Deng\"}],\"doi\":\"10.1016/J.PATREC.2019.07.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"749f376d0addb83569fcc7536e46308abbb232d4\",\"title\":\"How much do cross-modal related semantics benefit image captioning by weighting attributes and re-ranking sentences?\",\"url\":\"https://www.semanticscholar.org/paper/749f376d0addb83569fcc7536e46308abbb232d4\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26903445\",\"name\":\"H. Mantec\\u00f3n\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"174d1b037e62ab7ae5f73b597b025d5faf0fadb4\",\"title\":\"Deep Reinforcement Sequence Learning for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/174d1b037e62ab7ae5f73b597b025d5faf0fadb4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1901.01216\",\"authors\":[{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"2370774\",\"name\":\"K. Camilleri\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59d1603327fe00989f73d8b3316157903c34b020\",\"title\":\"Transfer learning from language models to image caption generators: Better models may not transfer better\",\"url\":\"https://www.semanticscholar.org/paper/59d1603327fe00989f73d8b3316157903c34b020\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2018/164\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2f4821a615f08fdad69957a19366c79d939bfd5f\",\"title\":\"Video Captioning with Tube Features\",\"url\":\"https://www.semanticscholar.org/paper/2f4821a615f08fdad69957a19366c79d939bfd5f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1907.03240\",\"authors\":[{\"authorId\":\"38921864\",\"name\":\"J. Li\"},{\"authorId\":\"21184593\",\"name\":\"Haizhou Shi\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3343031.3350918\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b2bb2ce93f2a48f998b6ca1e4364e8f2707e3e6\",\"title\":\"Informative Visual Storytelling with Cross-modal Rules\",\"url\":\"https://www.semanticscholar.org/paper/1b2bb2ce93f2a48f998b6ca1e4364e8f2707e3e6\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98831710\",\"name\":\"B. Yang\"},{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"86010b6cb557103eda7e28fa2b497c9a9697fa8d\",\"title\":\"Non-Autoregressive Video Captioning with Iterative Refinement\",\"url\":\"https://www.semanticscholar.org/paper/86010b6cb557103eda7e28fa2b497c9a9697fa8d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"83039268\",\"name\":\"Soichiro Oura\"},{\"authorId\":\"2816822\",\"name\":\"T. Matsukawa\"},{\"authorId\":\"1690503\",\"name\":\"E. Suzuki\"}],\"doi\":\"10.1109/IJCNN.2018.8489668\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e564f2ed4796e32fab8f9b90a52be8d6481a7fa\",\"title\":\"Multimodal Deep Neural Network with Image Sequence Features for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9e564f2ed4796e32fab8f9b90a52be8d6481a7fa\",\"venue\":\"2018 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9734988\",\"name\":\"Yuecong Xu\"},{\"authorId\":\"2562263\",\"name\":\"Jianfei Yang\"},{\"authorId\":\"144067957\",\"name\":\"K. Mao\"}],\"doi\":\"10.1016/J.NEUCOM.2019.05.027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd9a6ff5f908a6e8e785eb0a1432a5c9da2c2192\",\"title\":\"Semantic-filtered Soft-Split-Aware video captioning with audio-augmented feature\",\"url\":\"https://www.semanticscholar.org/paper/fd9a6ff5f908a6e8e785eb0a1432a5c9da2c2192\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.24963/ijcai.2018/114\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2b5a95ec7882bf329c4203513e606aa91d4174c5\",\"title\":\"Multi-Level Policy and Reward Reinforcement Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2b5a95ec7882bf329c4203513e606aa91d4174c5\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390818869\",\"name\":\"Jinlei Xu\"},{\"authorId\":\"144546140\",\"name\":\"T. Xu\"},{\"authorId\":\"123432231\",\"name\":\"Xin Tian\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144911521\",\"name\":\"Y. Ji\"}],\"doi\":\"10.1109/IJCNN.2019.8851897\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf7b38dd24c20223e006066be4202d1da700af37\",\"title\":\"Context Gating with Short Temporal Information for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf7b38dd24c20223e006066be4202d1da700af37\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8842143\",\"name\":\"Harrisen Scells\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"74b2f06903787f2942d77ac7865eb63a37090f43\",\"title\":\"Investigating Methods Of Annotating Lifelogs For Use In Search\",\"url\":\"https://www.semanticscholar.org/paper/74b2f06903787f2942d77ac7865eb63a37090f43\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36595248\",\"name\":\"Fang Fang\"},{\"authorId\":\"8194130\",\"name\":\"Qinyu Li\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8275214\",\"name\":\"P. Tang\"}],\"doi\":\"10.1109/ICME.2018.8486437\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e0f0076476fc81a344b8bdec771802a8584dd10f\",\"title\":\"Refining Attention: A Sequential Attention Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e0f0076476fc81a344b8bdec771802a8584dd10f\",\"venue\":\"2018 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2018},{\"arxivId\":\"1612.07086\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"40894914\",\"name\":\"T. Chen\"}],\"doi\":\"10.1109/ICCV.2017.138\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d85704f4814e9fa5ff0b68b1e5cad9e6527d0bbf\",\"title\":\"An Empirical Study of Language CNN for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d85704f4814e9fa5ff0b68b1e5cad9e6527d0bbf\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2343931\",\"name\":\"Miao Ma\"},{\"authorId\":\"49292300\",\"name\":\"Bolong Wang\"}],\"doi\":\"10.1109/GSIS.2017.8077673\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d466c9ceeecb326c5f2c834b8f424d5384a200a\",\"title\":\"A grey relational analysis based evaluation metric for image captioning and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/9d466c9ceeecb326c5f2c834b8f424d5384a200a\",\"venue\":\"2017 International Conference on Grey Systems and Intelligent Services (GSIS)\",\"year\":2017},{\"arxivId\":\"1704.03493\",\"authors\":[{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"3095572\",\"name\":\"Ziyu Zhang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":\"10.1109/CVPR.2017.575\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"390cac0a7e2f90c8efb12a1a9ff850ef386e5db6\",\"title\":\"Creativity: Generating Diverse Questions Using Variational Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/390cac0a7e2f90c8efb12a1a9ff850ef386e5db6\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1610.01465\",\"authors\":[{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.1016/j.cviu.2017.06.005\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d92ce1c4f7f0ccfe068e663903e4dd614a15ede\",\"title\":\"Visual question answering: Datasets, algorithms, and future challenges\",\"url\":\"https://www.semanticscholar.org/paper/6d92ce1c4f7f0ccfe068e663903e4dd614a15ede\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144478646\",\"name\":\"Z. Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967242\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"90c4a6c6f790dbcef9a29c9a755458be09e319b6\",\"title\":\"Attention-based LSTM with Semantic Consistency for Videos Captioning\",\"url\":\"https://www.semanticscholar.org/paper/90c4a6c6f790dbcef9a29c9a755458be09e319b6\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1708.04390\",\"authors\":[{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"}],\"doi\":\"10.1145/3123266.3123366\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"39f3f9d22a072d0ccf423aa31bacbb4071ac0644\",\"title\":\"Fluency-Guided Cross-Lingual Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/39f3f9d22a072d0ccf423aa31bacbb4071ac0644\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1605.01379\",\"authors\":[{\"authorId\":\"9136919\",\"name\":\"X. Lin\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1007/978-3-319-46475-6_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c94217efec8773ef947df2772f92df8c5726f855\",\"title\":\"Leveraging Visual Question Answering for Image-Caption Ranking\",\"url\":\"https://www.semanticscholar.org/paper/c94217efec8773ef947df2772f92df8c5726f855\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"2003.11743\",\"authors\":[{\"authorId\":\"1491233100\",\"name\":\"Pranav Agarwal\"},{\"authorId\":\"145086911\",\"name\":\"A. Betancourt\"},{\"authorId\":\"1594025086\",\"name\":\"V. Panagiotou\"},{\"authorId\":\"2251072\",\"name\":\"N. Rodr\\u00edguez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b430a5384c82beb6102106fbea0a134425a08c23\",\"title\":\"Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models\",\"url\":\"https://www.semanticscholar.org/paper/b430a5384c82beb6102106fbea0a134425a08c23\",\"venue\":\"ICLR 2020\",\"year\":2020},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1724911762\",\"name\":\"Robin Rojowiec\"},{\"authorId\":\"2242878\",\"name\":\"Jana G\\u00f6tze\"},{\"authorId\":\"84039136\",\"name\":\"P. Sadler\"},{\"authorId\":\"153588186\",\"name\":\"Henrik Voigt\"},{\"authorId\":\"1721364\",\"name\":\"Sina Zarrie\\u00df\"},{\"authorId\":\"69056125\",\"name\":\"D. Schlangen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ea8089b4ecf41028e9e5bb01c50680303fcbae3\",\"title\":\"From \\\"Before\\\" to \\\"After\\\": Generating Natural Language Instructions from Image Pairs in a Simple Visual Domain\",\"url\":\"https://www.semanticscholar.org/paper/7ea8089b4ecf41028e9e5bb01c50680303fcbae3\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":\"1901.07931\",\"authors\":[{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"2848048\",\"name\":\"Jekaterina Novikova\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.1016/j.csl.2019.06.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fedbcfe03e44f4f9610e2b2164c5673516543f38\",\"title\":\"Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge\",\"url\":\"https://www.semanticscholar.org/paper/fedbcfe03e44f4f9610e2b2164c5673516543f38\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":\"1805.08191\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"38504661\",\"name\":\"J. Wang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":\"10.1609/aaai.v33i01.33018465\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c2b02822cfbc50d17ec5220a19556be9d601c132\",\"title\":\"Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation\",\"url\":\"https://www.semanticscholar.org/paper/c2b02822cfbc50d17ec5220a19556be9d601c132\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144603000\",\"name\":\"Xinwei He\"},{\"authorId\":\"2276155\",\"name\":\"Baoguang Shi\"},{\"authorId\":\"145905113\",\"name\":\"X. Bai\"},{\"authorId\":\"39943835\",\"name\":\"Gui-Song Xia\"},{\"authorId\":\"145274329\",\"name\":\"Zhaoxiang Zhang\"},{\"authorId\":\"2872774\",\"name\":\"W. Dong\"}],\"doi\":\"10.1016/J.PATREC.2017.10.018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"390d0bb977b7473b8b76d045875c767d743de943\",\"title\":\"Image Caption Generation with Part of Speech Guidance\",\"url\":\"https://www.semanticscholar.org/paper/390d0bb977b7473b8b76d045875c767d743de943\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2008034295\",\"name\":\"Nayu Liu\"},{\"authorId\":\"2946890\",\"name\":\"Xian Sun\"},{\"authorId\":\"150129463\",\"name\":\"Hongfeng Yu\"},{\"authorId\":\"1485232293\",\"name\":\"Wenkai Zhang\"},{\"authorId\":\"3287827\",\"name\":\"Guangluan Xu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.144\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e993f052fc1a860011e34e7043af9b3b1a2c8897\",\"title\":\"Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos\",\"url\":\"https://www.semanticscholar.org/paper/e993f052fc1a860011e34e7043af9b3b1a2c8897\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2009.12506\",\"authors\":[{\"authorId\":\"50629423\",\"name\":\"Sashank Santhanam\"},{\"authorId\":\"1887537109\",\"name\":\"Zhuo Cheng\"},{\"authorId\":\"1505503134\",\"name\":\"Brodie Mather\"},{\"authorId\":\"1752326\",\"name\":\"Bonnie J. Dorr\"},{\"authorId\":\"2632964\",\"name\":\"Archna Bhatia\"},{\"authorId\":\"1505542669\",\"name\":\"B. Hebenstreit\"},{\"authorId\":\"1877429\",\"name\":\"Alan Zemel\"},{\"authorId\":\"36235196\",\"name\":\"Adam Dalton\"},{\"authorId\":\"1791072\",\"name\":\"T. Strzalkowski\"},{\"authorId\":\"39049552\",\"name\":\"Samira Shaikh\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.247\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4cf9df6c82eb67cabde04d21e399078f899ec65e\",\"title\":\"Learning to Plan and Realize Separately for Open-Ended Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/4cf9df6c82eb67cabde04d21e399078f899ec65e\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.12780\",\"authors\":[{\"authorId\":\"151504883\",\"name\":\"Yan Zeng\"},{\"authorId\":\"50204644\",\"name\":\"J. Nie\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4f027ccc32429b798f7ed2d20a69b1a645569087\",\"title\":\"Open-Domain Dialogue Generation Based on Pre-trained Language Models\",\"url\":\"https://www.semanticscholar.org/paper/4f027ccc32429b798f7ed2d20a69b1a645569087\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.00927\",\"authors\":[{\"authorId\":\"2000125058\",\"name\":\"Feicheng Huang\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"92057141\",\"name\":\"H. Wei\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"1998838209\",\"name\":\"Huifang Ma\"}],\"doi\":\"10.1007/s10994-020-05919-y\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"899b365ce70207f1fd456e982583841d9e4701bf\",\"title\":\"Boost Image Captioning with Knowledge Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/899b365ce70207f1fd456e982583841d9e4701bf\",\"venue\":\"Mach. Learn.\",\"year\":2020},{\"arxivId\":\"2010.00747\",\"authors\":[{\"authorId\":\"49889487\",\"name\":\"Yuhao Zhang\"},{\"authorId\":\"48579520\",\"name\":\"Hang Jiang\"},{\"authorId\":\"2965600\",\"name\":\"Y. Miura\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"2356307\",\"name\":\"C. Langlotz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6dd9f99cecd38504b667d320eb2a6267a9fee35d\",\"title\":\"Contrastive Learning of Medical Visual Representations from Paired Images and Text\",\"url\":\"https://www.semanticscholar.org/paper/6dd9f99cecd38504b667d320eb2a6267a9fee35d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.06666\",\"authors\":[{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"31039758\",\"name\":\"J. Johnson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1932edb4e5b2aae30ec1e7344b16d6110f52ef4\",\"title\":\"VirTex: Learning Visual Representations from Textual Annotations\",\"url\":\"https://www.semanticscholar.org/paper/b1932edb4e5b2aae30ec1e7344b16d6110f52ef4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00908\",\"authors\":[{\"authorId\":\"2715920\",\"name\":\"Malihe Alikhani\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"122615952\",\"name\":\"Sheng-Jie Li\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"48060050\",\"name\":\"M. Stone\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"7cc8205ddf8211d6ae2fce03de2953a4b978b66a\",\"title\":\"Clue: Cross-modal Coherence Modeling for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/7cc8205ddf8211d6ae2fce03de2953a4b978b66a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"},{\"authorId\":\"15862607\",\"name\":\"Xiaoliang Chu\"},{\"authorId\":\"1954076\",\"name\":\"W. Zhang\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"2037988\",\"name\":\"Weichen Sun\"},{\"authorId\":\"32324177\",\"name\":\"C. Wu\"}],\"doi\":\"10.3390/s18020646\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4b9c14951cea6259dd9d522586ba2c5bb1fbcce\",\"title\":\"Social Image Captioning: Exploring Visual Attention and User Attention\",\"url\":\"https://www.semanticscholar.org/paper/e4b9c14951cea6259dd9d522586ba2c5bb1fbcce\",\"venue\":\"Sensors\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7762524\",\"name\":\"Xiao-Yu Du\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"46554639\",\"name\":\"Liu Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"7477697\",\"name\":\"Zhiguang Qin\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"}],\"doi\":\"10.1007/s11390-017-1738-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69a9cf9bc8e585782824666fa3fb5ce5cf07cef2\",\"title\":\"Captioning Videos Using Large-Scale Image Corpus\",\"url\":\"https://www.semanticscholar.org/paper/69a9cf9bc8e585782824666fa3fb5ce5cf07cef2\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chi Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ad9e18e919b4452bd6f021fd7c52c0e361f62d4\",\"title\":\"Evolution of A Common Vector Space Approach to Multi-Modal Problems\",\"url\":\"https://www.semanticscholar.org/paper/5ad9e18e919b4452bd6f021fd7c52c0e361f62d4\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52366010\",\"name\":\"Zhi-bin Guan\"},{\"authorId\":\"49600007\",\"name\":\"Kang Liu\"},{\"authorId\":\"47009350\",\"name\":\"Yan Ma\"},{\"authorId\":\"144222488\",\"name\":\"Xu Qian\"},{\"authorId\":\"35260608\",\"name\":\"Tongkai Ji\"}],\"doi\":\"10.3390/sym10110626\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0d6b701c003c3aecfdbcf1700496cde74ea05642\",\"title\":\"Sequential Dual Attention: Coarse-to-Fine-Grained Hierarchical Generation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0d6b701c003c3aecfdbcf1700496cde74ea05642\",\"venue\":\"Symmetry\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"116567136\",\"name\":\"Vasiliki Kougia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b493fecc32759e614ab5f30c6c1fc05147fcb84\",\"title\":\"\\u201c Medical Image Labeling and Report Generation \\u201d\",\"url\":\"https://www.semanticscholar.org/paper/7b493fecc32759e614ab5f30c6c1fc05147fcb84\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"2168767\",\"name\":\"Umanga Bista\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"1748257\",\"name\":\"Achim Rettinger\"}],\"doi\":\"10.1007/978-3-319-93417-4_27\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f8e9524051c60fa11f498ae05710dd8239474ce\",\"title\":\"Knowledge Guided Attention and Inference for Describing Images Containing Unseen Objects\",\"url\":\"https://www.semanticscholar.org/paper/7f8e9524051c60fa11f498ae05710dd8239474ce\",\"venue\":\"ESWC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d660fdc8e7b23b5fa877a735744d1323a196cdb\",\"title\":\"A Simple Baseline for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/1d660fdc8e7b23b5fa877a735744d1323a196cdb\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"title\":\"Video Captioning and Retrieval Models with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1709.04625\",\"authors\":[{\"authorId\":\"2973866\",\"name\":\"J. Huang\"},{\"authorId\":\"9951160\",\"name\":\"Modar Alfadly\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"28853de59af8560dca5ff83f68f5af1cec0493d9\",\"title\":\"Robustness Analysis of Visual QA Models by Basic Questions\",\"url\":\"https://www.semanticscholar.org/paper/28853de59af8560dca5ff83f68f5af1cec0493d9\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28458352\",\"name\":\"S. Kim\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7af4a37e6e63b5f06e7bfb6e7c8910322774efb9\",\"title\":\"PROGRESSIVE MODULE NETWORKS\",\"url\":\"https://www.semanticscholar.org/paper/7af4a37e6e63b5f06e7bfb6e7c8910322774efb9\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0052b6a2c3533e30d5b521785345594c496f3683\",\"title\":\"Grammatically-Interpretable Learned Representations in Deep NLP Models\",\"url\":\"https://www.semanticscholar.org/paper/0052b6a2c3533e30d5b521785345594c496f3683\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TMM.2019.2896515\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"title\":\"Generating Video Descriptions With Latent Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"2002.12585\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"145558281\",\"name\":\"Kai Lei\"},{\"authorId\":\"48305273\",\"name\":\"Xu Sun\"}],\"doi\":\"10.24963/ijcai.2019/708\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b23622ba3eef8a82036710271c6c35bd8a49ce8f\",\"title\":\"Exploring and Distilling Cross-Modal Information for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b23622ba3eef8a82036710271c6c35bd8a49ce8f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47075011\",\"name\":\"Cheng Wang\"},{\"authorId\":\"1688587\",\"name\":\"Haojin Yang\"},{\"authorId\":\"1708312\",\"name\":\"C. Meinel\"}],\"doi\":\"10.1145/3115432\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0fe69c9ebf4ede1eb8e2ec233b632031173dd8cb\",\"title\":\"Image Captioning with Deep Bidirectional LSTMs and Multi-Task Learning\",\"url\":\"https://www.semanticscholar.org/paper/0fe69c9ebf4ede1eb8e2ec233b632031173dd8cb\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2018},{\"arxivId\":\"1809.07424\",\"authors\":[{\"authorId\":\"2571049\",\"name\":\"Besmira Nushi\"},{\"authorId\":\"1783184\",\"name\":\"Ece Kamar\"},{\"authorId\":\"145479841\",\"name\":\"E. Horvitz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c280c95c17194de83e5234cca3586cd567ecd47c\",\"title\":\"Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure\",\"url\":\"https://www.semanticscholar.org/paper/c280c95c17194de83e5234cca3586cd567ecd47c\",\"venue\":\"HCOMP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/CVPR.2017.108\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"561ed7e47524fb3218e6a38f41cd877a9c33d3b9\",\"title\":\"StyleNet: Generating Attractive Visual Captions with Styles\",\"url\":\"https://www.semanticscholar.org/paper/561ed7e47524fb3218e6a38f41cd877a9c33d3b9\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.5244/C.30.53\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02d6fb270c82c390476faffc6015b3116ddbb60c\",\"title\":\"Image Captioning with Sentiment Terms via Weakly-Supervised Sentiment Dataset\",\"url\":\"https://www.semanticscholar.org/paper/02d6fb270c82c390476faffc6015b3116ddbb60c\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144364295\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"title\":\"TVT: Two-View Transformer Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"venue\":\"ACML\",\"year\":2018},{\"arxivId\":\"1605.05440\",\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"8197937\",\"name\":\"K. Ohnishi\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/ICIP.2016.7532983\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"41aa209e9d294d370357434f310d49b2b0baebeb\",\"title\":\"Beyond caption to narrative: Video captioning with multiple sentences\",\"url\":\"https://www.semanticscholar.org/paper/41aa209e9d294d370357434f310d49b2b0baebeb\",\"venue\":\"2016 IEEE International Conference on Image Processing (ICIP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118079565\",\"name\":\"Mengjun Ni\"},{\"authorId\":\"145039750\",\"name\":\"J. Yang\"},{\"authorId\":\"144361839\",\"name\":\"X. Lin\"},{\"authorId\":\"145836225\",\"name\":\"Liang He\"}],\"doi\":\"10.1007/978-3-319-68612-7_62\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"89f523605f8d0a59f584053beb94cc52b7180f47\",\"title\":\"Reducing Unknown Unknowns with Guidance in Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/89f523605f8d0a59f584053beb94cc52b7180f47\",\"venue\":\"ICANN\",\"year\":2017},{\"arxivId\":\"1804.07889\",\"authors\":[{\"authorId\":\"144690460\",\"name\":\"Di Lu\"},{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"34170717\",\"name\":\"Lifu Huang\"},{\"authorId\":\"144016781\",\"name\":\"Heng Ji\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.18653/v1/D18-1435\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1e8feffa2280e41ceb864b940869c5408db89285\",\"title\":\"Entity-aware Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/1e8feffa2280e41ceb864b940869c5408db89285\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"2896799\",\"name\":\"Ye Zhao\"},{\"authorId\":\"3076466\",\"name\":\"X. Liu\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"}],\"doi\":\"10.1109/BigMM.2018.8499172\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"96418bb981ac738468340c7836a8362fa08cc1f2\",\"title\":\"Enhanced Text-Guided Attention Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96418bb981ac738468340c7836a8362fa08cc1f2\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":\"1910.12129\",\"authors\":[{\"authorId\":\"47080255\",\"name\":\"Juraj Juraska\"},{\"authorId\":\"144428010\",\"name\":\"K. Bowden\"},{\"authorId\":\"1760530\",\"name\":\"M. Walker\"}],\"doi\":\"10.18653/v1/W19-8623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f06d725ccd33bfea7937b756d47f506aea362f6a\",\"title\":\"ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation\",\"url\":\"https://www.semanticscholar.org/paper/f06d725ccd33bfea7937b756d47f506aea362f6a\",\"venue\":\"INLG\",\"year\":2019},{\"arxivId\":\"2003.12511\",\"authors\":[{\"authorId\":\"3428457\",\"name\":\"Tai-Yin Chiu\"},{\"authorId\":\"31812669\",\"name\":\"Y. Zhao\"},{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"}],\"doi\":\"10.1109/cvpr42600.2020.00370\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"795435da0de2cb9772e8ebec9a4242de7e677b30\",\"title\":\"Assessing Image Quality Issues for Real-World Problems\",\"url\":\"https://www.semanticscholar.org/paper/795435da0de2cb9772e8ebec9a4242de7e677b30\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255213\",\"name\":\"Z. Zhang\"},{\"authorId\":\"38188040\",\"name\":\"Dong Xu\"},{\"authorId\":\"47337540\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"2597292\",\"name\":\"Chuanqi Tan\"}],\"doi\":\"10.1109/TCSVT.2019.2936526\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"title\":\"Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization\",\"url\":\"https://www.semanticscholar.org/paper/b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"144654777\",\"name\":\"Ke Bai\"},{\"authorId\":\"46387857\",\"name\":\"Chenyang Tao\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1700522\",\"name\":\"Guoyin Wang\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"51030446\",\"name\":\"R. Henao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.1609/AAAI.V34I05.6249\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"826db2e5f340a90fc9672279f9e921b596aba4b7\",\"title\":\"Sequence Generation with Optimal-Transport-Enhanced Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/826db2e5f340a90fc9672279f9e921b596aba4b7\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46740305\",\"name\":\"Chunlei Wu\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"15862607\",\"name\":\"Xiaoliang Chu\"},{\"authorId\":\"152343380\",\"name\":\"Fei Su\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"}],\"doi\":\"10.1016/j.image.2018.06.002\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f20d5a6f10c269582bd00fd4733bb0066faee302\",\"title\":\"Modeling visual and word-conditional semantic attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f20d5a6f10c269582bd00fd4733bb0066faee302\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2018},{\"arxivId\":\"1505.00468\",\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"118707418\",\"name\":\"M. Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/s11263-016-0966-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145848392\",\"name\":\"Weidong Li\"},{\"authorId\":\"145122500\",\"name\":\"R. Peng\"},{\"authorId\":null,\"name\":\"Yaqian Wang\"},{\"authorId\":\"1390567330\",\"name\":\"Zhihuan Yan\"}],\"doi\":\"10.1016/j.neucom.2019.11.079\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eef0a2a678e92e7129dc7ec0cc0df716f68fc16c\",\"title\":\"Knowledge graph based natural language generation with adapted pointer-generator networks\",\"url\":\"https://www.semanticscholar.org/paper/eef0a2a678e92e7129dc7ec0cc0df716f68fc16c\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1909.03622\",\"authors\":[{\"authorId\":\"36538344\",\"name\":\"James O'Neill\"},{\"authorId\":\"2720656\",\"name\":\"Danushka Bollegala\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e382d10993daae43659a06c99b7ae8f3b00ad75\",\"title\":\"Transfer Reward Learning for Policy Gradient-Based Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/8e382d10993daae43659a06c99b7ae8f3b00ad75\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391204924\",\"name\":\"Zongjian Zhang\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"50528721\",\"name\":\"Qiuyun Wu\"},{\"authorId\":\"94294263\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/IJCNN.2019.8851832\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a273df2ac9fc4e796c532feb76f5e676a0773b1c\",\"title\":\"Visual Relationship Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a273df2ac9fc4e796c532feb76f5e676a0773b1c\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"1908.02923\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"143899054\",\"name\":\"Mark Dras\"},{\"authorId\":\"3093086\",\"name\":\"Stephen Wan\"},{\"authorId\":\"145212976\",\"name\":\"C\\u00e9cile Paris\"}],\"doi\":\"10.1613/jair.1.12025\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"29f5da6a626c32506a1ef93300a16d1f2c812a11\",\"title\":\"Image Captioning using Facial Expression and Attention\",\"url\":\"https://www.semanticscholar.org/paper/29f5da6a626c32506a1ef93300a16d1f2c812a11\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74437294\",\"name\":\"Sk. Arif Ahmed\"},{\"authorId\":\"3320759\",\"name\":\"D. P. Dogra\"},{\"authorId\":\"32614479\",\"name\":\"S. Kar\"},{\"authorId\":\"40813600\",\"name\":\"P. Roy\"}],\"doi\":\"10.1007/978-981-10-7590-2_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"522b13ea02d6d62e54180bd13595eb0e40333d48\",\"title\":\"Natural Language Description of Surveillance Events\",\"url\":\"https://www.semanticscholar.org/paper/522b13ea02d6d62e54180bd13595eb0e40333d48\",\"venue\":\"ICITAM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46958420\",\"name\":\"Tianyi Wang\"},{\"authorId\":\"47539594\",\"name\":\"Jiang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1007/978-3-030-00776-8_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c50c91875767ec7c6391d99d30838d90275a0f1b\",\"title\":\"Collaborative Detection and Caption Network\",\"url\":\"https://www.semanticscholar.org/paper/c50c91875767ec7c6391d99d30838d90275a0f1b\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"}],\"doi\":\"10.1007/978-3-030-11018-5_10\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b0f446d229a857350fc4b23bc6b062c147fcae2d\",\"title\":\"Pre-gen metrics: Predicting caption quality metrics without generating captions\",\"url\":\"https://www.semanticscholar.org/paper/b0f446d229a857350fc4b23bc6b062c147fcae2d\",\"venue\":\"ECCV Workshops\",\"year\":2018},{\"arxivId\":\"1809.06309\",\"authors\":[{\"authorId\":\"12620674\",\"name\":\"Lisa Bauer\"},{\"authorId\":\"3100759\",\"name\":\"Yicheng Wang\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D18-1454\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"711b1f7cc4e92d6f40c7813c6f0e1c2e179d48ad\",\"title\":\"Commonsense for Generative Multi-Hop Question Answering Tasks\",\"url\":\"https://www.semanticscholar.org/paper/711b1f7cc4e92d6f40c7813c6f0e1c2e179d48ad\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"title\":\"Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"2012.05545\",\"authors\":[{\"authorId\":\"67333377\",\"name\":\"Zeliang Song\"},{\"authorId\":\"1727617\",\"name\":\"Xiaofei Zhou\"},{\"authorId\":\"1855978\",\"name\":\"Zhendong Mao\"},{\"authorId\":\"2573626\",\"name\":\"Jianlong Tan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6f237f4ce10001a9b6cb47fae5333fa5ddb7e2d3\",\"title\":\"Image Captioning with Context-Aware Auxiliary Guidance\",\"url\":\"https://www.semanticscholar.org/paper/6f237f4ce10001a9b6cb47fae5333fa5ddb7e2d3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797144\",\"name\":\"G. Friedland\"},{\"authorId\":\"1772549\",\"name\":\"D. Borth\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"df9a08016fa553a169d893ce2d3fca375bab4781\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/df9a08016fa553a169d893ce2d3fca375bab4781\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"1890615\",\"name\":\"Y. Huo\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1145/2964284.2984064\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f28cd3ad13fe0b3e94d7c49886648fb164601f20\",\"title\":\"Early Embedding and Late Reranking for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f28cd3ad13fe0b3e94d7c49886648fb164601f20\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73312190\",\"name\":\"W. Zhang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"1591412916\",\"name\":\"Jiajie Su\"},{\"authorId\":\"145974114\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1007/s11042-020-08832-7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"814cfb898aa0bd011a97ebcf1774e42c32cf469e\",\"title\":\"Tell and guess: cooperative learning for natural image caption generation with hierarchical refined attention\",\"url\":\"https://www.semanticscholar.org/paper/814cfb898aa0bd011a97ebcf1774e42c32cf469e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ba67de7e822a94091b9e57cc9be3069822a4f5c8\",\"title\":\"A Neural-Symbolic Approach to Natural Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ba67de7e822a94091b9e57cc9be3069822a4f5c8\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"2009.11692\",\"authors\":[{\"authorId\":\"51111817\",\"name\":\"Haozhe Ji\"},{\"authorId\":\"1886879\",\"name\":\"Pei Ke\"},{\"authorId\":\"3110003\",\"name\":\"Shaohan Huang\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"48386682\",\"name\":\"X. Zhu\"},{\"authorId\":\"1730108\",\"name\":\"Minlie Huang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.54\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7e8457393ff1b40ddd099f195af9d3b14c5a934f\",\"title\":\"Language Generation with Multi-Hop Reasoning on Commonsense Knowledge Graph\",\"url\":\"https://www.semanticscholar.org/paper/7e8457393ff1b40ddd099f195af9d3b14c5a934f\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.01268\",\"authors\":[{\"authorId\":\"1646716323\",\"name\":\"Zihao Fu\"},{\"authorId\":\"46318189\",\"name\":\"Bei Shi\"},{\"authorId\":\"144594306\",\"name\":\"Wai Lam\"},{\"authorId\":\"1996394\",\"name\":\"Lidong Bing\"},{\"authorId\":null,\"name\":\"Zhiyuan Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.738\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fb276e89b8b39cd9ef0108e6d9624ffae81c17a\",\"title\":\"Partially-Aligned Data-to-Text Generation with Distant Supervision\",\"url\":\"https://www.semanticscholar.org/paper/0fb276e89b8b39cd9ef0108e6d9624ffae81c17a\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46276803\",\"name\":\"J. Li\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1016/j.jvcir.2020.102875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"title\":\"Translating video into language by enhancing visual and language representations\",\"url\":\"https://www.semanticscholar.org/paper/32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"1807.04219\",\"authors\":[{\"authorId\":\"47001910\",\"name\":\"Y. Li\"},{\"authorId\":\"49681507\",\"name\":\"Liqiang Wang\"},{\"authorId\":\"40381920\",\"name\":\"Tianbao Yang\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1007/978-3-030-01237-3_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cdc254119ff7166d67aeedb544b987ccb8b9eae6\",\"title\":\"How Local is the Local Diversity? Reinforcing Sequential Determinantal Point Processes with Dynamic Ground Sets for Supervised Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/cdc254119ff7166d67aeedb544b987ccb8b9eae6\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"50272388\",\"name\":\"Ioana Croitoru\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"title\":\"A hierarchical approach to vision-based language generation: from simple sentences to complex natural language\",\"url\":\"https://www.semanticscholar.org/paper/9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1905.02963\",\"authors\":[{\"authorId\":\"145114776\",\"name\":\"L. Sun\"},{\"authorId\":\"143721383\",\"name\":\"Bing Li\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":\"10.1109/ICME.2019.00226\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"title\":\"Multimodal Semantic Attention Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77537913\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/s11042-019-08011-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2623c1b4aee3f043da30d05f3e2c0f62fca5d5e\",\"title\":\"Deep multimodal embedding for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/c2623c1b4aee3f043da30d05f3e2c0f62fca5d5e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1805.06087\",\"authors\":[{\"authorId\":\"14487640\",\"name\":\"Ari Holtzman\"},{\"authorId\":\"144685020\",\"name\":\"Jan Buys\"},{\"authorId\":\"39191185\",\"name\":\"M. Forbes\"},{\"authorId\":\"2691021\",\"name\":\"Antoine Bosselut\"},{\"authorId\":\"145798491\",\"name\":\"D. Golub\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/P18-1152\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6db2b93a2d4007371030644173f1001c959214d2\",\"title\":\"Learning to Write with Cooperative Discriminators\",\"url\":\"https://www.semanticscholar.org/paper/6db2b93a2d4007371030644173f1001c959214d2\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1812.02378\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/CVPR.2019.01094\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f6feb1af1809dfd872d868dfcc13021cc42f496c\",\"title\":\"Auto-Encoding Scene Graphs for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f6feb1af1809dfd872d868dfcc13021cc42f496c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49543226\",\"name\":\"Xiaoxiao Liu\"},{\"authorId\":\"40096492\",\"name\":\"Q. Xu\"},{\"authorId\":null,\"name\":\"Ning Wang\"}],\"doi\":\"10.1007/s00371-018-1566-y\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f79c03f977d1c9acb71d87301272682422b0b14f\",\"title\":\"A survey on deep neural network-based image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f79c03f977d1c9acb71d87301272682422b0b14f\",\"venue\":\"The Visual Computer\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9741677\",\"name\":\"Senmao Ye\"},{\"authorId\":\"7181955\",\"name\":\"J. Han\"},{\"authorId\":\"143905571\",\"name\":\"Nian Liu\"}],\"doi\":\"10.1109/TIP.2018.2855406\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4dea9cce0825c0cdb1a4a28c4ab8416d6e3e047c\",\"title\":\"Attentive Linear Transformation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4dea9cce0825c0cdb1a4a28c4ab8416d6e3e047c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.1613/jair.4900\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c162d791b63682d928c09578bd38c3dd61f78c8c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures\",\"url\":\"https://www.semanticscholar.org/paper/c162d791b63682d928c09578bd38c3dd61f78c8c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2016},{\"arxivId\":\"1711.07068\",\"authors\":[{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"82247c9e74ddebb4dce65560ee69620579358f2d\",\"title\":\"Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space\",\"url\":\"https://www.semanticscholar.org/paper/82247c9e74ddebb4dce65560ee69620579358f2d\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"}],\"doi\":\"10.18653/v1/N18-1114\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f2df65bc38690ec56c4fe24380f9993ba2904e9c\",\"title\":\"Tensor Product Generation Networks for Deep NLP Modeling\",\"url\":\"https://www.semanticscholar.org/paper/f2df65bc38690ec56c4fe24380f9993ba2904e9c\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":\"1803.09845\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"94908120\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2018.00754\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"title\":\"Neural Baby Talk\",\"url\":\"https://www.semanticscholar.org/paper/3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"49292319\",\"name\":\"Bo Wang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/3123266.3127904\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a736b7347fc5ea93c196ddfe0630ecddc17d324\",\"title\":\"Multirate Multimodal Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a736b7347fc5ea93c196ddfe0630ecddc17d324\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1709.03376\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"},{\"authorId\":\"40894914\",\"name\":\"T. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f14e73dade94b8b1f276dcd91257aa7de5f19d7\",\"title\":\"Stack-Captioning: Coarse-to-Fine Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7f14e73dade94b8b1f276dcd91257aa7de5f19d7\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1612.00576\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.18653/v1/D17-1098\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c\",\"title\":\"Guided Open Vocabulary Image Captioning with Constrained Beam Search\",\"url\":\"https://www.semanticscholar.org/paper/086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/s11263-017-1033-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"828ac57f755db989e2886042a85278ae4823297c\",\"title\":\"Uncovering the Temporal Context for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/828ac57f755db989e2886042a85278ae4823297c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"2011.00966\",\"authors\":[{\"authorId\":\"32370882\",\"name\":\"Shweta Mahajan\"},{\"authorId\":\"49863405\",\"name\":\"S. Roth\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d805e35f17dab7382748130a2ca1bda629cdceb\",\"title\":\"Diverse Image Captioning with Context-Object Split Latent Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7d805e35f17dab7382748130a2ca1bda629cdceb\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2008.10966\",\"authors\":[{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"47268124\",\"name\":\"T. Li\"},{\"authorId\":\"46499812\",\"name\":\"Yuan Yuan\"},{\"authorId\":\"1785714\",\"name\":\"D. Katabi\"}],\"doi\":\"10.1007/978-3-030-58536-5_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4\",\"title\":\"In-Home Daily-Life Captioning Using Radio Signals\",\"url\":\"https://www.semanticscholar.org/paper/fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.14080\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":\"10.1109/cvpr42600.2020.01098\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0\",\"title\":\"X-Linear Attention Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2001.03615\",\"authors\":[{\"authorId\":\"40175280\",\"name\":\"Huaizu Jiang\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1389846455\",\"name\":\"E. Learned-Miller\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"}],\"doi\":\"10.1109/cvpr42600.2020.01028\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e4e57e0f7030f2c71ff7e9e3583d8ec908ca16e\",\"title\":\"In Defense of Grid Features for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8e4e57e0f7030f2c71ff7e9e3583d8ec908ca16e\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1910.05298\",\"authors\":[{\"authorId\":\"1387241031\",\"name\":\"Ondvrej Duvsek\"},{\"authorId\":\"1417454265\",\"name\":\"Filip Jurvc'ivcek\"}],\"doi\":\"10.18653/v1/w19-8670\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe9b98b145ce9e6363fb41324dfa898990df9310\",\"title\":\"Neural Generation for Czech: Data and Baselines\",\"url\":\"https://www.semanticscholar.org/paper/fe9b98b145ce9e6363fb41324dfa898990df9310\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47968201\",\"name\":\"Lixin Liu\"},{\"authorId\":\"46741143\",\"name\":\"Jiajun Tang\"},{\"authorId\":\"145078589\",\"name\":\"Xiaojun Wan\"},{\"authorId\":\"35310979\",\"name\":\"Zongming Guo\"}],\"doi\":\"10.1109/ICCV.2019.00434\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b774d6cd89cb27e62f03f183b89b7cacc412e131\",\"title\":\"Generating Diverse and Descriptive Image Captions Using Visual Paraphrases\",\"url\":\"https://www.semanticscholar.org/paper/b774d6cd89cb27e62f03f183b89b7cacc412e131\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49891089\",\"name\":\"Y. Zhang\"},{\"authorId\":\"9610896\",\"name\":\"Y. Ding\"},{\"authorId\":\"144265847\",\"name\":\"Rui Wu\"},{\"authorId\":\"50822330\",\"name\":\"F. Xue\"}],\"doi\":\"10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00151\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4a08403806ca61227b7a4780094fd8e652379362\",\"title\":\"A Denoising Framework for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/4a08403806ca61227b7a4780094fd8e652379362\",\"venue\":\"2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"title\":\"Video representation learning with deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1608.07068\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-46475-6_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"title\":\"Title Generation for User Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751242\",\"name\":\"Yusuke Sugano\"},{\"authorId\":\"3194727\",\"name\":\"Andreas Bulling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"456983805a8781d6429bed1ed66dc9f3902767af\",\"title\":\"Seeing with Humans : Gaze-Assisted Neural Image\",\"url\":\"https://www.semanticscholar.org/paper/456983805a8781d6429bed1ed66dc9f3902767af\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"2248826\",\"name\":\"R. Hong\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TIP.2018.2846664\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7fe2ab9f54242ef8609ef9bf988f008c7d42407c\",\"title\":\"Sequential Video VLAD: Training the Aggregation Locally and Temporally\",\"url\":\"https://www.semanticscholar.org/paper/7fe2ab9f54242ef8609ef9bf988f008c7d42407c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1812.08407\",\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"51011510\",\"name\":\"Juan Jose Alvarado Leanos\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"title\":\"Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TCSVT.2018.2867286\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"title\":\"Dual-Stream Recurrent Neural Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3358971\",\"name\":\"Ashwin K. Vijayakumar\"},{\"authorId\":\"37824829\",\"name\":\"M. Cogswell\"},{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"144207643\",\"name\":\"Q. Sun\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b1e3f7218f1c0f0db56bf2bd9475521454693a1\",\"title\":\"Diverse Beam Search for Improved Description of Complex Scenes\",\"url\":\"https://www.semanticscholar.org/paper/1b1e3f7218f1c0f0db56bf2bd9475521454693a1\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49813626\",\"name\":\"Y. Guo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eecfaf49500434d91970b24831081d5d2c68697e\",\"title\":\"Sequence to Sequence Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/eecfaf49500434d91970b24831081d5d2c68697e\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1601.03896\",\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.24963/ijcai.2017/704\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72417796\",\"name\":\"Z. Xu\"},{\"authorId\":\"2913207\",\"name\":\"Chengjie Sun\"},{\"authorId\":\"7165718\",\"name\":\"Yinong Long\"},{\"authorId\":\"144174278\",\"name\":\"B. Liu\"},{\"authorId\":\"2806178\",\"name\":\"Baoxun Wang\"},{\"authorId\":\"2424958\",\"name\":\"Mingjiang Wang\"},{\"authorId\":\"47474457\",\"name\":\"Min Zhang\"},{\"authorId\":\"39635018\",\"name\":\"Xiaolong Wang\"}],\"doi\":\"10.1109/TASLP.2019.2915922\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"f3e32ea6d2f99f5f2dc9a5c25f7c708572e0aab3\",\"title\":\"Dynamic Working Memory for Context-Aware Response Generation\",\"url\":\"https://www.semanticscholar.org/paper/f3e32ea6d2f99f5f2dc9a5c25f7c708572e0aab3\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9073332\",\"name\":\"J. Li\"},{\"authorId\":\"1923156\",\"name\":\"P. Yao\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"49039585\",\"name\":\"Wei-Cun Zhang\"}],\"doi\":\"10.3390/APP9163260\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"47716ca06a3ad4226a6d8706b6f02793b9892fe4\",\"title\":\"Boosted Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/47716ca06a3ad4226a6d8706b6f02793b9892fe4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1703.06233\",\"authors\":[{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1109/ICCV.2017.57\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"afc7d76c94c9b73aa28f4aa42407b21aa0572a1b\",\"title\":\"Recurrent Models for Situation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/afc7d76c94c9b73aa28f4aa42407b21aa0572a1b\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82564091\",\"name\":\"Ey\\u00fcp \\u00d6zer\"},{\"authorId\":\"1658966195\",\"name\":\"\\u0130lteber Nur Karap\\u0131nar\"},{\"authorId\":\"1658997114\",\"name\":\"Sena Ba\\u015fbu\\u011f\"},{\"authorId\":\"1657299979\",\"name\":\"S\\u00fcmeyye Turan\"},{\"authorId\":\"52219742\",\"name\":\"An\\u0131l Utku\"},{\"authorId\":\"153780569\",\"name\":\"M. Akcayol\"}],\"doi\":\"10.14569/ijacsa.2020.0110365\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f682a618e680e6c488f620a65cf5cf657fc6986b\",\"title\":\"Deep Learning based, a New Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f682a618e680e6c488f620a65cf5cf657fc6986b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fc8df4ad35282ccf19261e02de87d8e35c956537\",\"title\":\"Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/fc8df4ad35282ccf19261e02de87d8e35c956537\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"}],\"doi\":\"10.1609/AAAI.V34I07.6998\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9421fa26257e6a8d59bb874cf3b376c6d4c4118b\",\"title\":\"MemCap: Memorizing Style Knowledge for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9421fa26257e6a8d59bb874cf3b376c6d4c4118b\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2007.06077\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"51510489\",\"name\":\"Marius Mosbach\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7c6d4f0afbe3a7d6ba8e94f446878977721a21eb\",\"title\":\"Sparse Graph to Sequence Learning for Vision Conditioned Long Textual Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/7c6d4f0afbe3a7d6ba8e94f446878977721a21eb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1411500195\",\"name\":\"D. K. Guevara-Flores\"},{\"authorId\":\"1397808828\",\"name\":\"Fernando Perez-Tellez\"},{\"authorId\":\"3469668\",\"name\":\"D. Avenda\\u00f1o\"}],\"doi\":\"10.13053/cys-24-2-3393\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b2c6ba89a8bb820055319df10fcc861840a9a36b\",\"title\":\"Analysis of Automatic Annotations of Real Video Surveillance Images\",\"url\":\"https://www.semanticscholar.org/paper/b2c6ba89a8bb820055319df10fcc861840a9a36b\",\"venue\":\"Computaci\\u00f3n y Sistemas\",\"year\":2020},{\"arxivId\":\"1710.11475\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"96692a8fc740eee99e3ad5e48e73eae8de2578db\",\"title\":\"A Neural-Symbolic Approach to Design of CAPTCHA.\",\"url\":\"https://www.semanticscholar.org/paper/96692a8fc740eee99e3ad5e48e73eae8de2578db\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49163024\",\"name\":\"Shuang Bai\"},{\"authorId\":\"3380543\",\"name\":\"S. An\"}],\"doi\":\"10.1016/j.neucom.2018.05.080\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe66fc9e3d9a52497119bab89ac54fc8b5f8859a\",\"title\":\"A survey on automatic image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/fe66fc9e3d9a52497119bab89ac54fc8b5f8859a\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"144478646\",\"name\":\"Z. Guo\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TMM.2017.2729019\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"51b2c1e750b1d3b893072829d012f2352d6bd373\",\"title\":\"Video Captioning With Attention-Based LSTM and Semantic Consistency\",\"url\":\"https://www.semanticscholar.org/paper/51b2c1e750b1d3b893072829d012f2352d6bd373\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":\"1611.02796\",\"authors\":[{\"authorId\":\"3106683\",\"name\":\"N. Jaques\"},{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1388574431\",\"name\":\"Jos\\u00e9 Miguel Hern\\u00e1ndez-Lobato\"},{\"authorId\":\"145369890\",\"name\":\"R. Turner\"},{\"authorId\":\"153329923\",\"name\":\"Douglas Eck\"}],\"doi\":\"10.17863/CAM.21343\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a870df7e7d43c9144e2520ef4e4779f1672dd654\",\"title\":\"Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models with KL-control\",\"url\":\"https://www.semanticscholar.org/paper/a870df7e7d43c9144e2520ef4e4779f1672dd654\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1805.07030\",\"authors\":[{\"authorId\":\"3175685\",\"name\":\"A. Mathews\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"33913193\",\"name\":\"Xuming He\"}],\"doi\":\"10.1109/CVPR.2018.00896\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"beeebd2af0d8f130dcf234231de4569d584cb7fd\",\"title\":\"SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text\",\"url\":\"https://www.semanticscholar.org/paper/beeebd2af0d8f130dcf234231de4569d584cb7fd\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1805.03766\",\"authors\":[{\"authorId\":\"2691021\",\"name\":\"Antoine Bosselut\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"2421691\",\"name\":\"Po-Sen Huang\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/N18-1016\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"680bfa179c33d56f524c6adf9b7f7f5a62e5ef46\",\"title\":\"Discourse-Aware Neural Rewards for Coherent Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/680bfa179c33d56f524c6adf9b7f7f5a62e5ef46\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145471480\",\"name\":\"Yunmeng Feng\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"46446912\",\"name\":\"X. Zhang\"},{\"authorId\":\"7521170\",\"name\":\"Chuanfu Xu\"},{\"authorId\":\"2243533\",\"name\":\"Zhenghua Wang\"},{\"authorId\":\"145254061\",\"name\":\"Z. Luo\"}],\"doi\":\"10.1145/3302425.3302464\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"11a8cc3fa18ab4f7158447cc1fc8800489e82f9c\",\"title\":\"AttResNet: Attention-based ResNet for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/11a8cc3fa18ab4f7158447cc1fc8800489e82f9c\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1813915\",\"name\":\"S. Liu\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3240508.3240667\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"849642b4701ac11c035326069f707f23a51a6f1a\",\"title\":\"SibNet: Sibling Convolutional Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/849642b4701ac11c035326069f707f23a51a6f1a\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"42afa9a0166eeb45cb2e9b37e0a8eec482f78fe0\",\"title\":\"Visual Question Answering and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/42afa9a0166eeb45cb2e9b37e0a8eec482f78fe0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738647655\",\"name\":\"Emiel van Miltenburg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"adf57a2bdcffc38238e64d4ae797c12405281173\",\"title\":\"How Do Image Description Systems Describe People? A Targeted Assessment of System Competence in the PEOPLE-domain\",\"url\":\"https://www.semanticscholar.org/paper/adf57a2bdcffc38238e64d4ae797c12405281173\",\"venue\":\"LANTERN\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"3256307\",\"name\":\"M. A. A. K. Jalwana\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"},{\"authorId\":\"2210661\",\"name\":\"S. Shah\"}],\"doi\":\"10.1109/IVCNZ51579.2020.9290719\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b88b5bc5fe9af08df2d953b1c14c6f5cacd9564\",\"title\":\"Leveraging Linguistically-aware Object Relations and NASNet for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8b88b5bc5fe9af08df2d953b1c14c6f5cacd9564\",\"venue\":\"2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ)\",\"year\":2020},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"49470127\",\"name\":\"X. Zhang\"},{\"authorId\":\"1406222945\",\"name\":\"Zhigang Luo\"}],\"doi\":\"10.1007/s11042-019-08567-0\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"49280cc9fa8c9854a655afbfd213c1437c52f1e3\",\"title\":\"GateCap: Gated spatial and semantic attention model for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/49280cc9fa8c9854a655afbfd213c1437c52f1e3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/TIP.2020.2988435\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"title\":\"Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1724902\",\"name\":\"M. Yang\"},{\"authorId\":null,\"name\":\"Wei Zhao\"},{\"authorId\":\"145738414\",\"name\":\"Wei Xu\"},{\"authorId\":\"28106616\",\"name\":\"Yabing Feng\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"40265331\",\"name\":\"X. Chen\"},{\"authorId\":\"1784737\",\"name\":\"K. Lei\"}],\"doi\":\"10.1109/TMM.2018.2869276\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"aaa5625e0dbd9119b54dbb9c3840efd1199a071f\",\"title\":\"Multitask Learning for Cross-Domain Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aaa5625e0dbd9119b54dbb9c3840efd1199a071f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"1901.06283\",\"authors\":[{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1940556\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"46387857\",\"name\":\"Chenyang Tao\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"46701859\",\"name\":\"H. Zhang\"},{\"authorId\":null,\"name\":\"Bai Li\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"2624978\",\"name\":\"C. Chen\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"717daba98eb57b898687fc013b705f763eb2916b\",\"title\":\"Improving Sequence-to-Sequence Learning via Optimal Transport\",\"url\":\"https://www.semanticscholar.org/paper/717daba98eb57b898687fc013b705f763eb2916b\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1611.06949\",\"authors\":[{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"3355264\",\"name\":\"Kevin D. Tang\"},{\"authorId\":\"1706007\",\"name\":\"Jianchao Yang\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"}],\"doi\":\"10.1109/CVPR.2017.214\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"21fa67345e49642b8ebb22a59c4b2799a56e996f\",\"title\":\"Dense Captioning with Joint Inference and Visual Context\",\"url\":\"https://www.semanticscholar.org/paper/21fa67345e49642b8ebb22a59c4b2799a56e996f\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1903.10122\",\"authors\":[{\"authorId\":\"46194597\",\"name\":\"C. Y. Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.1609/AAAI.V33I01.33016666\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"adc998ac4fa71bdab19537c50e3d84bf982974c1\",\"title\":\"Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/adc998ac4fa71bdab19537c50e3d84bf982974c1\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1904.11838\",\"authors\":[{\"authorId\":\"115238110\",\"name\":\"Marco Roberti\"},{\"authorId\":\"66951248\",\"name\":\"Giovanni Bonetta\"},{\"authorId\":\"2853804\",\"name\":\"R. Cancelliere\"},{\"authorId\":\"1741426\",\"name\":\"P. Gallinari\"}],\"doi\":\"10.1007/978-3-030-46147-8_39\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c80892794bab9a18cb4f64433518658148df5802\",\"title\":\"Copy mechanism and tailored training for character-based data-to-text generation\",\"url\":\"https://www.semanticscholar.org/paper/c80892794bab9a18cb4f64433518658148df5802\",\"venue\":\"ECML/PKDD\",\"year\":2019},{\"arxivId\":\"1906.08876\",\"authors\":[{\"authorId\":\"3038511\",\"name\":\"Sanqiang Zhao\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/P19-1650\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"68490e9e0bca2f0b1ae2ca636effcd8fc63d2008\",\"title\":\"Informative Image Captioning with External Sources of Information\",\"url\":\"https://www.semanticscholar.org/paper/68490e9e0bca2f0b1ae2ca636effcd8fc63d2008\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50111883\",\"name\":\"S. Lee\"},{\"authorId\":\"48084799\",\"name\":\"In-Cheol Kim\"}],\"doi\":\"10.1155/2018/3125879\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c919568836e236da738282f9015f58c455d26f7\",\"title\":\"Multimodal Feature Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c919568836e236da738282f9015f58c455d26f7\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000125058\",\"name\":\"Feicheng Huang\"},{\"authorId\":\"49969428\",\"name\":\"Z. Li\"},{\"authorId\":\"1856671082\",\"name\":\"Shengjia Chen\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"1998838209\",\"name\":\"Huifang Ma\"}],\"doi\":\"10.1145/3340531.3411948\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3428bac9141f6db961fc4665db1cbc9a196152da\",\"title\":\"Image Captioning with Internal and External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/3428bac9141f6db961fc4665db1cbc9a196152da\",\"venue\":\"CIKM\",\"year\":2020},{\"arxivId\":\"1906.01081\",\"authors\":[{\"authorId\":\"34994191\",\"name\":\"Bhuwan Dhingra\"},{\"authorId\":\"1779225\",\"name\":\"Manaal Faruqui\"},{\"authorId\":\"144729896\",\"name\":\"Ankur Parikh\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"143790066\",\"name\":\"Dipanjan Das\"},{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"}],\"doi\":\"10.18653/v1/P19-1483\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"02cbb0db288af2c83b48a023f245812bd22a2408\",\"title\":\"Handling Divergent Reference Texts when Evaluating Table-to-Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/02cbb0db288af2c83b48a023f245812bd22a2408\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1912.01452\",\"authors\":[{\"authorId\":\"50535497\",\"name\":\"Jiahong Huang\"},{\"authorId\":\"9951160\",\"name\":\"Modar Alfadly\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"1717056\",\"name\":\"M. Worring\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"01aa5668a618fbb376b6ab6608defc074ed355ac\",\"title\":\"Assessing the Robustness of Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/01aa5668a618fbb376b6ab6608defc074ed355ac\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40526720\",\"name\":\"Pengtao Xie\"}],\"doi\":\"10.1184/R1/7553468.V1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04f11687c7b918501c1a24195d8336936adf194d\",\"title\":\"Diversity-Promoting and Large-Scale Machine Learning for Healthcare\",\"url\":\"https://www.semanticscholar.org/paper/04f11687c7b918501c1a24195d8336936adf194d\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2006.04702\",\"authors\":[{\"authorId\":\"153683057\",\"name\":\"Qipeng Guo\"},{\"authorId\":\"8752221\",\"name\":\"Zhijing Jin\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"46794011\",\"name\":\"D. Wipf\"},{\"authorId\":\"1852415\",\"name\":\"Zheng Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f09c496b3412de5c7e53e4bd0b3619d0db92e9a8\",\"title\":\"CycleGT: Unsupervised Graph-to-Text and Text-to-Graph Generation via Cycle Training\",\"url\":\"https://www.semanticscholar.org/paper/f09c496b3412de5c7e53e4bd0b3619d0db92e9a8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152281902\",\"name\":\"S. Wu\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"1700911\",\"name\":\"Zhiqi Shen\"},{\"authorId\":\"145977143\",\"name\":\"Mohan S. Kankanhalli\"},{\"authorId\":\"1699730\",\"name\":\"Anthony K. H. Tung\"}],\"doi\":\"10.1145/3394171.3413589\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"35ba6ed07ef68db187674498e684de7f3e160716\",\"title\":\"Who You Are Decides How You Tell\",\"url\":\"https://www.semanticscholar.org/paper/35ba6ed07ef68db187674498e684de7f3e160716\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2005.04560\",\"authors\":[{\"authorId\":\"32551341\",\"name\":\"Xiang Lisa Li\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":\"10.18653/v1/2020.acl-main.243\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fd061aa7d44021e71186e7798ad332f19be1ce1f\",\"title\":\"Posterior Control of Blackbox Generation\",\"url\":\"https://www.semanticscholar.org/paper/fd061aa7d44021e71186e7798ad332f19be1ce1f\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1908.01741\",\"authors\":[{\"authorId\":\"2967089\",\"name\":\"Duc Minh Vo\"},{\"authorId\":\"47344473\",\"name\":\"A. Sugimoto\"}],\"doi\":\"10.1007/978-3-030-58604-1_18\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5db99735355dd70be728d67e20e6561d8959f89a\",\"title\":\"Visual-Relation Conscious Image Generation from Structured-Text\",\"url\":\"https://www.semanticscholar.org/paper/5db99735355dd70be728d67e20e6561d8959f89a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.03238\",\"authors\":[{\"authorId\":\"46314641\",\"name\":\"W. Wang\"},{\"authorId\":\"2173199\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1557363420\",\"name\":\"Zhengran Zeng\"},{\"authorId\":\"1747560\",\"name\":\"Guandong Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2c0f78fbf5fc4421a97b265969b4c469e8c45b3\",\"title\":\"TranS^3: A Transformer-based Framework for Unifying Code Summarization and Code Search\",\"url\":\"https://www.semanticscholar.org/paper/a2c0f78fbf5fc4421a97b265969b4c469e8c45b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1874882\",\"name\":\"X. Wang\"},{\"authorId\":\"46867976\",\"name\":\"Y. Zhang\"},{\"authorId\":\"153757329\",\"name\":\"Z. Guo\"},{\"authorId\":\"46276201\",\"name\":\"J. Li\"}],\"doi\":\"10.1007/978-3-030-37446-4_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e57d5231f97aae78f155a3fa36edd40dae8161a5\",\"title\":\"A Computational Framework Towards Medical Image Explanation\",\"url\":\"https://www.semanticscholar.org/paper/e57d5231f97aae78f155a3fa36edd40dae8161a5\",\"venue\":\"KR4HC/ProHealth/TEAAM@AIME\",\"year\":2019},{\"arxivId\":\"1905.06139\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"title\":\"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations\",\"url\":\"https://www.semanticscholar.org/paper/cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144603000\",\"name\":\"Xinwei He\"},{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"2276155\",\"name\":\"Baoguang Shi\"},{\"authorId\":\"145905113\",\"name\":\"X. Bai\"}],\"doi\":\"10.1016/j.neucom.2018.02.106\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a24f013cbae0f349c54aaf958dca944d561a6efd\",\"title\":\"VD-SAN: Visual-Densely Semantic Attention Network for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/a24f013cbae0f349c54aaf958dca944d561a6efd\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2015.7298932\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"title\":\"Deep visual-semantic alignments for generating image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":\"1905.01077\",\"authors\":[{\"authorId\":\"50763020\",\"name\":\"Jingwen Chen\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1609/aaai.v33i01.33018167\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d40892479541c2d173c836534e6fb2acb597de49\",\"title\":\"Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d40892479541c2d173c836534e6fb2acb597de49\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1910.12019\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Diverse Video Captioning Through Latent Variable Expansion with Conditional GAN\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2005.00192\",\"authors\":[{\"authorId\":\"2065332\",\"name\":\"H. Lee\"},{\"authorId\":\"144517919\",\"name\":\"Seunghyun Yoon\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"153586399\",\"name\":\"Doo Soon Kim\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"27582486\",\"name\":\"Joongbo Shin\"},{\"authorId\":\"1731707\",\"name\":\"K. Jung\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb9332fab74f30ce4d4bc2a12fa67fcd658fd460\",\"title\":\"KPQA: A Metric for Generative Question Answering Using Keyphrase Weights.\",\"url\":\"https://www.semanticscholar.org/paper/eb9332fab74f30ce4d4bc2a12fa67fcd658fd460\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.12626\",\"authors\":[{\"authorId\":\"22281632\",\"name\":\"A. R. Fabbri\"},{\"authorId\":\"51232396\",\"name\":\"Wojciech Kryscinski\"},{\"authorId\":\"143775536\",\"name\":\"B. McCann\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"1405531452\",\"name\":\"D. Radev\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dea5eb8c113c549d9e8ceb20abd0a3fdc70efe92\",\"title\":\"SummEval: Re-evaluating Summarization Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/dea5eb8c113c549d9e8ceb20abd0a3fdc70efe92\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144669461\",\"name\":\"Kuncheng Fang\"},{\"authorId\":\"144913277\",\"name\":\"Lian Zhou\"},{\"authorId\":\"145020731\",\"name\":\"Cheng Jin\"},{\"authorId\":\"7550713\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"35632219\",\"name\":\"Kangnian Weng\"},{\"authorId\":\"1689115\",\"name\":\"Tao Zhang\"},{\"authorId\":\"145631869\",\"name\":\"W. Fan\"}],\"doi\":\"10.1609/AAAI.V33I01.33018271\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"title\":\"Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention\",\"url\":\"https://www.semanticscholar.org/paper/506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2001.01037\",\"authors\":[{\"authorId\":\"46969089\",\"name\":\"J. Sun\"},{\"authorId\":\"3633358\",\"name\":\"S. Lapuschkin\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"},{\"authorId\":\"49345823\",\"name\":\"Alexander Binder\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"82e836be97e706dca7029ce6a0553b4890726593\",\"title\":\"Understanding Image Captioning Models beyond Visualizing Attention\",\"url\":\"https://www.semanticscholar.org/paper/82e836be97e706dca7029ce6a0553b4890726593\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.00766\",\"authors\":[{\"authorId\":\"90639891\",\"name\":\"Hassan Maleki Galandouz\"},{\"authorId\":\"2734293\",\"name\":\"Mohsen Ebrahimi Moghaddam\"},{\"authorId\":\"2567327\",\"name\":\"Mehrnoush Shamsfard\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1717cd0899bda3166687281e6ed8b99ca285a311\",\"title\":\"A Weighted Multi-Criteria Decision Making Approach for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1717cd0899bda3166687281e6ed8b99ca285a311\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3456814\",\"name\":\"Yevgeniy Puzikov\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.18653/v1/W18-3602\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"933afbd7aea1671e6950d65511c23af7adf38de1\",\"title\":\"BinLin: A Simple Method of Dependency Tree Linearization\",\"url\":\"https://www.semanticscholar.org/paper/933afbd7aea1671e6950d65511c23af7adf38de1\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"Jeff Donahue\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d1f58798db460996501f224fff6cceada08f59f9\",\"title\":\"Transferrable Representations for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d1f58798db460996501f224fff6cceada08f59f9\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1908.00249\",\"authors\":[{\"authorId\":\"144005516\",\"name\":\"Jing Wang\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.24963/ijcai.2019/132\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0b8f0e00f953ae51a7ea72ba51c699bb959cc948\",\"title\":\"Convolutional Auto-encoding of Sentence Topics for Image Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/0b8f0e00f953ae51a7ea72ba51c699bb959cc948\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1912.09551\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6cfc0933cbc5c4d59cb9a951ea51bae41e150ccc\",\"title\":\"Deep Exemplar Networks for VQA and VQG\",\"url\":\"https://www.semanticscholar.org/paper/6cfc0933cbc5c4d59cb9a951ea51bae41e150ccc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29461008\",\"name\":\"Lecheng Wang\"},{\"authorId\":\"51478887\",\"name\":\"Shizheng Qin\"},{\"authorId\":\"14565924\",\"name\":\"Meng-long Xu\"},{\"authorId\":\"97937527\",\"name\":\"Rui Zhang\"},{\"authorId\":\"1410674575\",\"name\":\"Lizhe Qi\"},{\"authorId\":\"50550297\",\"name\":\"W. Zhang\"}],\"doi\":\"10.1109/ROBIO49542.2019.8961449\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a70981ab7b4011b8f4e656e383685d8c3cd58c79\",\"title\":\"From Quick-draw To Story: A Story Generation System for Kids\\u2019 Robot\",\"url\":\"https://www.semanticscholar.org/paper/a70981ab7b4011b8f4e656e383685d8c3cd58c79\",\"venue\":\"2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471428747\",\"name\":\"Yang Zhen-yu\"},{\"authorId\":\"1471428729\",\"name\":\"Zhang Jiao\"}],\"doi\":\"10.1109/ICCSNT47585.2019.8962488\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"46f87af5af877b2d83b8fdbb9b97c72d0aa14ff9\",\"title\":\"Fine-grained Image Caption based on Multi-level Attention\",\"url\":\"https://www.semanticscholar.org/paper/46f87af5af877b2d83b8fdbb9b97c72d0aa14ff9\",\"venue\":\"2019 IEEE 7th International Conference on Computer Science and Network Technology (ICCSNT)\",\"year\":2019},{\"arxivId\":\"1706.06275\",\"authors\":[{\"authorId\":\"3727644\",\"name\":\"Satoshi Tsutsui\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93168a3dc214447cd938c35e635bea8af4400b12\",\"title\":\"Using Artificial Tokens to Control Languages for Multilingual Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/93168a3dc214447cd938c35e635bea8af4400b12\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1710.06303\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"2168767\",\"name\":\"Umanga Bista\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"1748257\",\"name\":\"Achim Rettinger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"631a1571d1a073369ec7c98e196de07e263ae130\",\"title\":\"Describing Natural Images Containing Novel Objects with Knowledge Guided Assitance\",\"url\":\"https://www.semanticscholar.org/paper/631a1571d1a073369ec7c98e196de07e263ae130\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2786437\",\"name\":\"Linghui Li\"},{\"authorId\":\"46321465\",\"name\":\"Sheng Tang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"4303531\",\"name\":\"Lixi Deng\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TMM.2017.2751140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e0f1c89c4e099b14c4d77bd406be9f7b78d6f6d\",\"title\":\"GLA: Global\\u2013Local Attention for Image Description\",\"url\":\"https://www.semanticscholar.org/paper/2e0f1c89c4e099b14c4d77bd406be9f7b78d6f6d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400984858\",\"name\":\"A. Dorn\"},{\"authorId\":\"1712974\",\"name\":\"J. L. D\\u00edaz\"},{\"authorId\":\"145692288\",\"name\":\"G. Koch\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b0c8a2a79f8eaf404ab93f834b0cfd7700014ad4\",\"title\":\"Language Resource and Evaluation Conference 11 \\u2013 16 May 2020\",\"url\":\"https://www.semanticscholar.org/paper/b0c8a2a79f8eaf404ab93f834b0cfd7700014ad4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.09626\",\"authors\":[{\"authorId\":\"8363301\",\"name\":\"Sungyeon Kim\"},{\"authorId\":\"103550598\",\"name\":\"Minkyo Seo\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"72643925\",\"name\":\"Minsu Cho\"},{\"authorId\":\"2483916\",\"name\":\"Suha Kwak\"}],\"doi\":\"10.1109/CVPR.2019.00239\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"249f0a2ae3540dbe4f2a11806d2ac38581b9ad6b\",\"title\":\"Deep Metric Learning Beyond Binary Supervision\",\"url\":\"https://www.semanticscholar.org/paper/249f0a2ae3540dbe4f2a11806d2ac38581b9ad6b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1805.05062\",\"authors\":[{\"authorId\":\"46183659\",\"name\":\"Maha Elbayad\"},{\"authorId\":\"143823463\",\"name\":\"L. Besacier\"},{\"authorId\":\"34602236\",\"name\":\"Jakob Verbeek\"}],\"doi\":\"10.18653/v1/P18-1195\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db3340742a56a4bbb9d7ee8dba1958fb132a3c0a\",\"title\":\"Token-level and sequence-level loss smoothing for RNN language models\",\"url\":\"https://www.semanticscholar.org/paper/db3340742a56a4bbb9d7ee8dba1958fb132a3c0a\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2386404\",\"name\":\"Sisi Liang\"},{\"authorId\":\"37670557\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"2941643\",\"name\":\"Y. Zhu\"},{\"authorId\":\"145950948\",\"name\":\"Xue Li\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3832a6d6b1f78cdadee6968d51c1c7c2922ab3cd\",\"title\":\"ISIA at the ImageCLEF 2017 Image Caption Task\",\"url\":\"https://www.semanticscholar.org/paper/3832a6d6b1f78cdadee6968d51c1c7c2922ab3cd\",\"venue\":\"CLEF\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"153576783\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"48693981\",\"name\":\"J. Nie\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.1109/TMM.2019.2941820\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aff890f20d28a13b9fb89d192fad35d92381c410\",\"title\":\"Multi-Level Policy and Reward-Based Deep Reinforcement Learning Framework for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aff890f20d28a13b9fb89d192fad35d92381c410\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35330701\",\"name\":\"K. Takanashi\"},{\"authorId\":\"71687093\",\"name\":\"T. Kawahara\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"88452dc70b263442c5da454fa801e80200a4d948\",\"title\":\"18th Annual Meeting of the Special Interest Group on Discourse and Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/88452dc70b263442c5da454fa801e80200a4d948\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48462171\",\"name\":\"Lu Zhang\"},{\"authorId\":\"1519062024\",\"name\":\"Jianming Zhang\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"},{\"authorId\":\"49990648\",\"name\":\"Y. He\"}],\"doi\":\"10.1109/CVPR.2019.00618\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"791fa4797683469f91b27940253c7725c9717f24\",\"title\":\"CapSal: Leveraging Captioning to Boost Semantics for Salient Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/791fa4797683469f91b27940253c7725c9717f24\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"50678151\",\"name\":\"Bingtao Liu\"},{\"authorId\":\"7590116\",\"name\":\"C. Yan\"}],\"doi\":\"10.1145/3123266.3123354\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"title\":\"Video Description with Spatial-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50826972\",\"name\":\"Wei Huang\"},{\"authorId\":\"50621207\",\"name\":\"Q. Wang\"},{\"authorId\":\"40286455\",\"name\":\"Xuelong Li\"}],\"doi\":\"10.1109/lgrs.2020.2980933\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca66b05a647e85426def722931e6b7ff79c672c8\",\"title\":\"Denoising-Based Multiscale Feature Fusion for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ca66b05a647e85426def722931e6b7ff79c672c8\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47098628\",\"name\":\"S. Pillai\"},{\"authorId\":\"50351300\",\"name\":\"Jeremy Brown\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"57c5c512bd8ad09f9eb0b5630b72746d5ffbb6e5\",\"title\":\"Deep Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/57c5c512bd8ad09f9eb0b5630b72746d5ffbb6e5\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143966993\",\"name\":\"X. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bdbf483de5db6645106bb5a1b39b10381ccc74c\",\"title\":\"Frames in places: visual common sense knowledge in context\",\"url\":\"https://www.semanticscholar.org/paper/7bdbf483de5db6645106bb5a1b39b10381ccc74c\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1910.06475\",\"authors\":[{\"authorId\":\"152325076\",\"name\":\"H. Ge\"},{\"authorId\":\"79403975\",\"name\":\"Zehang Yan\"},{\"authorId\":\"96280392\",\"name\":\"K. Zhang\"},{\"authorId\":\"145327825\",\"name\":\"Mingde Zhao\"},{\"authorId\":\"46732983\",\"name\":\"Liang Sun\"}],\"doi\":\"10.1109/ICCV.2019.00184\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4b5231149f566fc8a78797b6fb448f9bca416380\",\"title\":\"Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style\",\"url\":\"https://www.semanticscholar.org/paper/4b5231149f566fc8a78797b6fb448f9bca416380\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1601.06823\",\"authors\":[{\"authorId\":\"47939010\",\"name\":\"Feng Wang\"},{\"authorId\":\"2743835\",\"name\":\"D. Tax\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f660ea723b62f69b9f4c439724a6b73357e1d3c3\",\"title\":\"Survey on the attention based RNN model and its applications in computer vision\",\"url\":\"https://www.semanticscholar.org/paper/f660ea723b62f69b9f4c439724a6b73357e1d3c3\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2011.09257\",\"authors\":[{\"authorId\":\"144882483\",\"name\":\"P. Pino\"},{\"authorId\":\"145831267\",\"name\":\"Denis Parra\"},{\"authorId\":\"19324186\",\"name\":\"Pablo Messina\"},{\"authorId\":\"3766360\",\"name\":\"Cecilia Besa\"},{\"authorId\":\"145212929\",\"name\":\"S. Uribe\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4e4c613a76fd146b1dc1614d14525b72c595321c\",\"title\":\"Inspecting state of the art performance and NLP metrics in image-based medical report generation\",\"url\":\"https://www.semanticscholar.org/paper/4e4c613a76fd146b1dc1614d14525b72c595321c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144897421\",\"name\":\"Rachel N. Simons\"},{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"},{\"authorId\":\"3079031\",\"name\":\"Kenneth R. Fleischmann\"}],\"doi\":\"10.1145/3415176\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f2d4b8de7f9ed9a1a0b403e9f1ad9365b599c5ad\",\"title\":\"\\\"I Hope This Is Helpful\\\"\",\"url\":\"https://www.semanticscholar.org/paper/f2d4b8de7f9ed9a1a0b403e9f1ad9365b599c5ad\",\"venue\":\"Proc. ACM Hum. Comput. Interact.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112858943\",\"name\":\"Z. Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"title\":\"Representation Learning for Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3490541\",\"name\":\"Aaron Chan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b775774edc1c2edcc9782f37eb2682cd01cfd94a\",\"title\":\"A Survey of Image Captioning Methods\",\"url\":\"https://www.semanticscholar.org/paper/b775774edc1c2edcc9782f37eb2682cd01cfd94a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1811.02745\",\"authors\":[{\"authorId\":\"2904100\",\"name\":\"Z. Han\"},{\"authorId\":\"41066029\",\"name\":\"Mingyang Shang\"},{\"authorId\":\"48631765\",\"name\":\"Xiyang Wang\"},{\"authorId\":\"46399275\",\"name\":\"Yu-Shen Liu\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"}],\"doi\":\"10.1609/aaai.v33i01.3301126\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f3b03bc2f5a049fbcd4c7eab74066b562a6ab70\",\"title\":\"Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by Joint Reconstruction and Prediction of View and Word Sequences\",\"url\":\"https://www.semanticscholar.org/paper/1f3b03bc2f5a049fbcd4c7eab74066b562a6ab70\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1998926555\",\"name\":\"Chenxi Yuan\"},{\"authorId\":\"2027167977\",\"name\":\"Yang Bai\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"68c11851ace525b233f05b985f8acf887d03d379\",\"title\":\"Bridge the Gap: High-level Semantic Planning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/68c11851ace525b233f05b985f8acf887d03d379\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31696860\",\"name\":\"Shurong Sheng\"},{\"authorId\":\"145446752\",\"name\":\"Marie-Francine Moens\"}],\"doi\":\"10.1145/3343031.3350972\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d35c56b2a86d928fabe380f5380e83a7992c1d7a\",\"title\":\"Generating Captions for Images of Ancient Artworks\",\"url\":\"https://www.semanticscholar.org/paper/d35c56b2a86d928fabe380f5380e83a7992c1d7a\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"145211780\",\"name\":\"Bin Fan\"},{\"authorId\":\"1380311632\",\"name\":\"Shinming Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.18653/v1/D19-1213\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"title\":\"Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag\",\"url\":\"https://www.semanticscholar.org/paper/ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1809.04682\",\"authors\":[{\"authorId\":\"51450024\",\"name\":\"Amit Zohar\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"980e4fe84fe5feec42c2a2eea7cc738e1af8acdf\",\"title\":\"Automatic Program Synthesis of Long Programs with a Learned Garbage Collector\",\"url\":\"https://www.semanticscholar.org/paper/980e4fe84fe5feec42c2a2eea7cc738e1af8acdf\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71787436\",\"name\":\"F. Xiao\"},{\"authorId\":\"144838968\",\"name\":\"Xue Gong\"},{\"authorId\":\"49890762\",\"name\":\"Yiming Zhang\"},{\"authorId\":\"2879323\",\"name\":\"Yanqing Shen\"},{\"authorId\":\"46276957\",\"name\":\"J. Li\"},{\"authorId\":\"1705421\",\"name\":\"Xieping Gao\"}],\"doi\":\"10.1016/J.NEUCOM.2019.06.085\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f5ec7bdce1acf0415e3c3370b4f634ca3d474cb\",\"title\":\"DAA: Dual LSTMs with adaptive attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/0f5ec7bdce1acf0415e3c3370b4f634ca3d474cb\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47011474\",\"name\":\"Kun Xiong\"},{\"authorId\":\"1807459832\",\"name\":\"Liu Jiang\"},{\"authorId\":\"39056715\",\"name\":\"Xuan Dang\"},{\"authorId\":\"50248791\",\"name\":\"G. Wang\"},{\"authorId\":\"1830569732\",\"name\":\"Wenwen Ye\"},{\"authorId\":\"1489386471\",\"name\":\"Zheng Qin\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206953\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dafb451fc3a51032c825eea6cc2037911089d47c\",\"title\":\"Towards Personalized Aesthetic Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/dafb451fc3a51032c825eea6cc2037911089d47c\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"2005.05402\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"145602574\",\"name\":\"L. Wang\"},{\"authorId\":\"1752875\",\"name\":\"Y. Shen\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.acl-main.233\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70557ea6b65846fc30729ceed224acd4ac64ca5d\",\"title\":\"MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/70557ea6b65846fc30729ceed224acd4ac64ca5d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82972947\",\"name\":\"Felipe Riquelme\"},{\"authorId\":\"1978662985\",\"name\":\"Alfredo De Goyeneche\"},{\"authorId\":\"49890233\",\"name\":\"Yundong Zhang\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"144949755\",\"name\":\"\\u00c1. Soto\"}],\"doi\":\"10.1016/j.imavis.2020.103968\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d2c956be94129de11a4deba6f1d34c5ae18b3fde\",\"title\":\"Explaining VQA predictions using visual grounding and a knowledge base\",\"url\":\"https://www.semanticscholar.org/paper/d2c956be94129de11a4deba6f1d34c5ae18b3fde\",\"venue\":\"Image Vis. Comput.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"66622154\",\"name\":\"Ray Ptucha\"}],\"doi\":\"10.1007/s10044-018-00770-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ddfe22a67bdd1cc2b8f1a2e6663044690226933\",\"title\":\"Understanding temporal structure for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/3ddfe22a67bdd1cc2b8f1a2e6663044690226933\",\"venue\":\"Pattern Analysis and Applications\",\"year\":2019},{\"arxivId\":\"1911.09753\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.1609/AAAI.V34I03.5655\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"title\":\"Reinforcing an Image Caption Generator Using Off-Line Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2008.11009\",\"authors\":[{\"authorId\":\"35466168\",\"name\":\"Jian Han Lim\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"},{\"authorId\":\"145923164\",\"name\":\"K. Ng\"},{\"authorId\":\"2034793\",\"name\":\"Lixin Fan\"},{\"authorId\":\"153096457\",\"name\":\"Q. Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f37d3bd1f656e946ee7e0e99070ad7a6950f688\",\"title\":\"Protect, Show, Attend and Tell: Image Captioning Model with Ownership Protection\",\"url\":\"https://www.semanticscholar.org/paper/0f37d3bd1f656e946ee7e0e99070ad7a6950f688\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.03283\",\"authors\":[{\"authorId\":\"4760298\",\"name\":\"J. Du\"},{\"authorId\":\"144485517\",\"name\":\"Yu Qin\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"},{\"authorId\":\"48378975\",\"name\":\"Yonghua Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b75163ef32d63ba076e94d9321442ca8223fcd4\",\"title\":\"Attend More Times for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0b75163ef32d63ba076e94d9321442ca8223fcd4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145014498\",\"name\":\"Q. Liu\"},{\"authorId\":\"50580380\",\"name\":\"Yingying Chen\"},{\"authorId\":\"1783122\",\"name\":\"J. Wang\"},{\"authorId\":\"27356041\",\"name\":\"Sijiong Zhang\"}],\"doi\":\"10.1016/j.compind.2018.01.015\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ed73cea8227a9a4733146053c2d1baa52de9572\",\"title\":\"Multi-view pedestrian captioning with an attention topic CNN model\",\"url\":\"https://www.semanticscholar.org/paper/1ed73cea8227a9a4733146053c2d1baa52de9572\",\"venue\":\"Comput. Ind.\",\"year\":2018},{\"arxivId\":\"1707.02485\",\"authors\":[{\"authorId\":\"2476328\",\"name\":\"Zizhao Zhang\"},{\"authorId\":\"1877955\",\"name\":\"Yuanpu Xie\"},{\"authorId\":\"2082604\",\"name\":\"F. Xing\"},{\"authorId\":\"46671753\",\"name\":\"M. McGough\"},{\"authorId\":\"144890162\",\"name\":\"L. Yang\"}],\"doi\":\"10.1109/CVPR.2017.378\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4a3df5823d32075ef2227cc3671aff24a8d8634\",\"title\":\"MDNet: A Semantically and Visually Interpretable Medical Image Diagnosis Network\",\"url\":\"https://www.semanticscholar.org/paper/a4a3df5823d32075ef2227cc3671aff24a8d8634\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c1693e1defad1cc8ec36b061add2afcd564013ff\",\"title\":\"Advancing Multi-Modal Deep Learning: Towards Language-Grounded Visual Understanding\",\"url\":\"https://www.semanticscholar.org/paper/c1693e1defad1cc8ec36b061add2afcd564013ff\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49035023\",\"name\":\"T. Nguyen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"64eac8b653194e2d479c2bf28d8f2bd2bfb9f53c\",\"title\":\"Automatic Video Captioning using Deep Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/64eac8b653194e2d479c2bf28d8f2bd2bfb9f53c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"2005.14386\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"46208708\",\"name\":\"G. Shakhnarovich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"57ace777e00c0df5d0b729990eed12ed6b0dcfe9\",\"title\":\"Controlling Length in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/57ace777e00c0df5d0b729990eed12ed6b0dcfe9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1603.02814\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"121177698\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/TPAMI.2017.2708709\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"2006.03391\",\"authors\":[{\"authorId\":\"1739175581\",\"name\":\"Aycsegul Ozkaya Eren\"},{\"authorId\":\"48671036\",\"name\":\"Mustafa Sert\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4e968310af7cf6fa9c82a6abc2737a8de6f21bc0\",\"title\":\"Audio Captioning using Gated Recurrent Units\",\"url\":\"https://www.semanticscholar.org/paper/4e968310af7cf6fa9c82a6abc2737a8de6f21bc0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.08608\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/ICCV.2019.00435\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"45f9856d418527d23dc7c89197627fa1f3b215f9\",\"title\":\"Learning to Collocate Neural Modules for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/45f9856d418527d23dc7c89197627fa1f3b215f9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394741222\",\"name\":\"Yuling Gui\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"97522088\",\"name\":\"Ye Zhao\"}],\"doi\":\"10.1145/3347319.3356839\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"title\":\"Semantic Enhanced Encoder-Decoder Network (SEN) for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47180076\",\"name\":\"Shengyu Zhang\"},{\"authorId\":\"143626361\",\"name\":\"H. Dong\"},{\"authorId\":\"145066190\",\"name\":\"Wei Hu\"},{\"authorId\":\"3194920\",\"name\":\"Y. Guo\"},{\"authorId\":\"49762880\",\"name\":\"Chao Wu\"},{\"authorId\":\"145982709\",\"name\":\"Di Xie\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":\"10.1007/978-3-030-00764-5_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"377b4e9413c5b5fa87577b1208ddcad4436e3e34\",\"title\":\"Text-to-Image Synthesis via Visual-Memory Creative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/377b4e9413c5b5fa87577b1208ddcad4436e3e34\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":\"1809.04835\",\"authors\":[{\"authorId\":\"3404500\",\"name\":\"Haichao Shi\"},{\"authorId\":\"144326612\",\"name\":\"P. Li\"},{\"authorId\":\"49292319\",\"name\":\"Bo Wang\"},{\"authorId\":\"2960930\",\"name\":\"Z. Wang\"}],\"doi\":\"10.1145/3240876.3240900\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dec04588b73efb1192d1778b2b818842ccd242e7\",\"title\":\"Image captioning based on deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/dec04588b73efb1192d1778b2b818842ccd242e7\",\"venue\":\"ICIMCS '18\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144763807\",\"name\":\"M. Yin\"},{\"authorId\":\"5094646\",\"name\":\"Y. Zhang\"},{\"authorId\":\"2693308\",\"name\":\"Xiu Li\"},{\"authorId\":\"1705047\",\"name\":\"S. Wang\"}],\"doi\":\"10.1145/3240508.3240603\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84d78b8c1b47245a56bfc92360892051294f182e\",\"title\":\"When Deep Fool Meets Deep Prior: Adversarial Attack on Super-Resolution Network\",\"url\":\"https://www.semanticscholar.org/paper/84d78b8c1b47245a56bfc92360892051294f182e\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2476328\",\"name\":\"Zizhao Zhang\"},{\"authorId\":\"2488404\",\"name\":\"P. Chen\"},{\"authorId\":\"46671753\",\"name\":\"M. McGough\"},{\"authorId\":\"48312345\",\"name\":\"Fuyong Xing\"},{\"authorId\":\"9652979\",\"name\":\"C. Wang\"},{\"authorId\":\"145882824\",\"name\":\"M. Bui\"},{\"authorId\":\"1877955\",\"name\":\"Yuanpu Xie\"},{\"authorId\":\"143706295\",\"name\":\"Manish Sapkota\"},{\"authorId\":\"47386018\",\"name\":\"Lei Cui\"},{\"authorId\":\"49513123\",\"name\":\"J. Dhillon\"},{\"authorId\":\"50692999\",\"name\":\"Nazeel Ahmad\"},{\"authorId\":\"39442551\",\"name\":\"F. Khalil\"},{\"authorId\":\"9931038\",\"name\":\"S. Dickinson\"},{\"authorId\":\"2766473\",\"name\":\"Xiaoshuang Shi\"},{\"authorId\":\"47185799\",\"name\":\"F. Liu\"},{\"authorId\":\"143729223\",\"name\":\"H. Su\"},{\"authorId\":\"3457945\",\"name\":\"Jinzheng Cai\"},{\"authorId\":\"40248915\",\"name\":\"L. Yang\"}],\"doi\":\"10.1038/S42256-019-0052-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"01b1860903518ec9f2acc31a7712fe532838a98b\",\"title\":\"Pathologist-level interpretable whole-slide cancer diagnosis with deep learning\",\"url\":\"https://www.semanticscholar.org/paper/01b1860903518ec9f2acc31a7712fe532838a98b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1609.06782\",\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3122865.3122867\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"title\":\"Deep Learning for Video Classification and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"venue\":\"Frontiers of Multimedia Research\",\"year\":2018},{\"arxivId\":\"1904.01301\",\"authors\":[{\"authorId\":\"2191455\",\"name\":\"Sheng Shen\"},{\"authorId\":\"47070750\",\"name\":\"Daniel Fried\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"38666915\",\"name\":\"D. Klein\"}],\"doi\":\"10.18653/v1/N19-1410\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"743d1aae44a12fb37b743ec947fad41cba9831b8\",\"title\":\"Pragmatically Informative Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/743d1aae44a12fb37b743ec947fad41cba9831b8\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":\"1905.13302\",\"authors\":[{\"authorId\":\"116567136\",\"name\":\"Vasiliki Kougia\"},{\"authorId\":\"2332587\",\"name\":\"John Pavlopoulos\"},{\"authorId\":\"1752430\",\"name\":\"Ion Androutsopoulos\"}],\"doi\":\"10.18653/v1/W19-1803\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3e7f91e3bcc86360c6eaad7c3bdb3dafbde2bb1e\",\"title\":\"A Survey on Biomedical Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3e7f91e3bcc86360c6eaad7c3bdb3dafbde2bb1e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.06619\",\"authors\":[{\"authorId\":\"2251827\",\"name\":\"Gil Sadeh\"},{\"authorId\":\"46397904\",\"name\":\"L. Fritz\"},{\"authorId\":\"36004650\",\"name\":\"Gabi Shalev\"},{\"authorId\":\"40135367\",\"name\":\"E. Oks\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7b4708bce76d496e0a1083d057cad6e1562a302d\",\"title\":\"Generating Diverse and Informative Natural Language Fashion Feedback\",\"url\":\"https://www.semanticscholar.org/paper/7b4708bce76d496e0a1083d057cad6e1562a302d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1710.02534\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"83d66c1f808962536a68418587b691f30221c5a1\",\"title\":\"Contrastive Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/83d66c1f808962536a68418587b691f30221c5a1\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2017.398\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7fac20f3908c69bd336ea252e28c79f5abaa6dbe\",\"title\":\"Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/7fac20f3908c69bd336ea252e28c79f5abaa6dbe\",\"venue\":\"ICCV 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2492994\",\"name\":\"Mert Kilickaya\"},{\"authorId\":\"2376985\",\"name\":\"Burak Kerim Akkus\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"2011587\",\"name\":\"Nazli Ikizler-Cinbis\"}],\"doi\":\"10.1049/iet-cvi.2016.0286\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c8cf97f00cd8b4303eccc4134fa79b15cc3d564\",\"title\":\"Data-driven image captioning via salient region discovery\",\"url\":\"https://www.semanticscholar.org/paper/3c8cf97f00cd8b4303eccc4134fa79b15cc3d564\",\"venue\":\"IET Comput. Vis.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114276298\",\"name\":\"Soichiro Fujita\"},{\"authorId\":\"8351786\",\"name\":\"Tsutomu Hirao\"},{\"authorId\":\"2300756\",\"name\":\"Hidetaka Kamigaito\"},{\"authorId\":\"144859189\",\"name\":\"M. Okumura\"},{\"authorId\":\"2364073\",\"name\":\"M. Nagata\"}],\"doi\":\"10.1007/978-3-030-58539-6_31\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5a4c5fa5a25cff3c65e74f64504819683353ef1e\",\"title\":\"SODA: Story Oriented Dense Video Captioning Evaluation Framework\",\"url\":\"https://www.semanticscholar.org/paper/5a4c5fa5a25cff3c65e74f64504819683353ef1e\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2615648\",\"name\":\"Xudong Hong\"},{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"153786091\",\"name\":\"Asad Sayeed\"},{\"authorId\":\"52406707\",\"name\":\"Khushboo Mehra\"},{\"authorId\":\"2869436\",\"name\":\"V. Demberg\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.18653/v1/2020.conll-1.34\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8504fe99ba36088b36623441eb468adae67a9b51\",\"title\":\"Diverse and Relevant Visual Storytelling with Scene Graph Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/8504fe99ba36088b36623441eb468adae67a9b51\",\"venue\":\"CoNLL\",\"year\":2020},{\"arxivId\":\"2010.04565\",\"authors\":[{\"authorId\":\"144058898\",\"name\":\"S. Raja\"},{\"authorId\":\"2896521\",\"name\":\"Ajoy Mondal\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1007/978-3-030-58604-1_5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"30860e7c569d777db8d2b6fb0f7c374965ac62dd\",\"title\":\"Table Structure Recognition using Top-Down and Bottom-Up Cues\",\"url\":\"https://www.semanticscholar.org/paper/30860e7c569d777db8d2b6fb0f7c374965ac62dd\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.13588\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e4ca3d95ffb83870661dd66deee143e782f0706\",\"title\":\"Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale\",\"url\":\"https://www.semanticscholar.org/paper/2e4ca3d95ffb83870661dd66deee143e782f0706\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2006.15631\",\"authors\":[{\"authorId\":\"46365808\",\"name\":\"Jialin Wu\"},{\"authorId\":\"46307991\",\"name\":\"Liyan Chen\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f336b80e3f78615fbb414d4c4aed7545024dc4bf\",\"title\":\"Improving VQA and its Explanations by Comparing Competing Explanations\",\"url\":\"https://www.semanticscholar.org/paper/f336b80e3f78615fbb414d4c4aed7545024dc4bf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38889850\",\"name\":\"Pengfei Xia\"},{\"authorId\":\"50774917\",\"name\":\"Jingsong He\"},{\"authorId\":\"153781930\",\"name\":\"Jin Yin\"}],\"doi\":\"10.1007/s11042-020-09110-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"17cefd99f2d04fc01663ba261d6afee54e8408d5\",\"title\":\"Boosting image caption generation with feature fusion module\",\"url\":\"https://www.semanticscholar.org/paper/17cefd99f2d04fc01663ba261d6afee54e8408d5\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mehrdad Hosseinzadeh\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"title\":\"Video Captioning of Future Frames\",\"url\":\"https://www.semanticscholar.org/paper/da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1612.00370\",\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"665a311c538fc021c27acd3953f171924cc5905c\",\"title\":\"Optimization of image description metrics using policy gradient methods\",\"url\":\"https://www.semanticscholar.org/paper/665a311c538fc021c27acd3953f171924cc5905c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2474496\",\"name\":\"Haithem Afli\"},{\"authorId\":\"3330863\",\"name\":\"F. Hu\"},{\"authorId\":\"39785875\",\"name\":\"Jinhua Du\"},{\"authorId\":\"36673232\",\"name\":\"D. Cosgrove\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"40063957\",\"name\":\"Eric Arazo Sanchez\"},{\"authorId\":\"49640190\",\"name\":\"Jiang Zhou\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d1e975e5cb7996a49785153cae31802e0b2b895\",\"title\":\"Dublin City University Participation in the VTT Track at TRECVid 2017\",\"url\":\"https://www.semanticscholar.org/paper/9d1e975e5cb7996a49785153cae31802e0b2b895\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":\"2007.09903\",\"authors\":[{\"authorId\":\"151257945\",\"name\":\"Xiangyang Mou\"},{\"authorId\":\"1824292863\",\"name\":\"Brandyn Sigouin\"},{\"authorId\":\"1824242400\",\"name\":\"Ian Steenstra\"},{\"authorId\":\"1563539653\",\"name\":\"Hui Su\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8cb2782f61c964e9b207f34c5dd1cb367b0fc561\",\"title\":\"Multimodal Dialogue State Tracking By QA Approach with Data Augmentation\",\"url\":\"https://www.semanticscholar.org/paper/8cb2782f61c964e9b207f34c5dd1cb367b0fc561\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.11436\",\"authors\":[{\"authorId\":\"3297022\",\"name\":\"Sang-Ki Ko\"},{\"authorId\":\"50320706\",\"name\":\"C. Kim\"},{\"authorId\":\"3011724\",\"name\":\"Hyedong Jung\"},{\"authorId\":\"2529532\",\"name\":\"C. S. Cho\"}],\"doi\":\"10.3390/APP9132683\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74669631d3f1fcc06d0e5e5daa7e4bfd5b6f6b59\",\"title\":\"Neural Sign Language Translation based on Human Keypoint Estimation\",\"url\":\"https://www.semanticscholar.org/paper/74669631d3f1fcc06d0e5e5daa7e4bfd5b6f6b59\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2589441\",\"name\":\"Shiqi Wu\"},{\"authorId\":\"152899497\",\"name\":\"Xiangrong Zhang\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"1713590317\",\"name\":\"Chen Li\"},{\"authorId\":\"1734497\",\"name\":\"Licheng Jiao\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207381\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8f497460633cf0a61c5749aa02f11bc95a599dc4\",\"title\":\"Scene Attention Mechanism for Remote Sensing Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/8f497460633cf0a61c5749aa02f11bc95a599dc4\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2452223\",\"name\":\"Kazuto Nakashima\"},{\"authorId\":\"1795530\",\"name\":\"Y. Iwashita\"},{\"authorId\":\"1801213\",\"name\":\"R. Kurazume\"}],\"doi\":\"10.1186/s40648-020-00181-2\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"76b4b7aa99a8434d80f3af553fbfa013f5cbfe6b\",\"title\":\"Lifelogging caption generation via fourth-person vision in a human\\u2013robot symbiotic environment\",\"url\":\"https://www.semanticscholar.org/paper/76b4b7aa99a8434d80f3af553fbfa013f5cbfe6b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145840350\",\"name\":\"P. Anderson\"}],\"doi\":\"10.25911/5D00D4EC451CC\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0\",\"title\":\"Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents\",\"url\":\"https://www.semanticscholar.org/paper/c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"}],\"doi\":\"10.1007/s11063-019-10030-y\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ed3215857d14557d0afe517b4d28b0e98b384f4b\",\"title\":\"Refocused Attention: Long Short-Term Rewards Guided Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ed3215857d14557d0afe517b4d28b0e98b384f4b\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":\"2010.11098\",\"authors\":[{\"authorId\":\"144824387\",\"name\":\"A. Tran\"},{\"authorId\":\"3193584\",\"name\":\"Konstantinos Drossos\"},{\"authorId\":\"50195877\",\"name\":\"T. Virtanen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"26aaf442b9acad994eb5c3547bcfce5117445192\",\"title\":\"WaveTransformer: A Novel Architecture for Audio Captioning Based on Learning Temporal and Time-Frequency Information\",\"url\":\"https://www.semanticscholar.org/paper/26aaf442b9acad994eb5c3547bcfce5117445192\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1809.07257\",\"authors\":[{\"authorId\":\"2662002\",\"name\":\"Oliver Nina\"},{\"authorId\":\"47238599\",\"name\":\"W. Garcia\"},{\"authorId\":\"47637016\",\"name\":\"Scott Clouse\"},{\"authorId\":\"1858702\",\"name\":\"A. Yilmaz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"title\":\"MTLE: A Multitask Learning Encoder of Visual Feature Representations for Video and Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39679686\",\"name\":\"L. L. Vie\"},{\"authorId\":\"65956766\",\"name\":\"B. Barrows\"},{\"authorId\":\"144911735\",\"name\":\"B. Allen\"}],\"doi\":\"10.2514/6.2018-4013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0020dc845945fffe1b3ff3090a6050d4963a2e9e\",\"title\":\"Towards Informing an Intuitive Mission Planning Interface for Autonomous Multi-Asset Teams via Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/0020dc845945fffe1b3ff3090a6050d4963a2e9e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"153489843\",\"name\":\"T. Chen\"},{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"145909567\",\"name\":\"Wanli Yu\"}],\"doi\":\"10.1109/ACCESS.2020.3010872\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"title\":\"Attention-Based Convolutional LSTM for Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1906.05963\",\"authors\":[{\"authorId\":\"80236158\",\"name\":\"Simao Herdade\"},{\"authorId\":\"40441990\",\"name\":\"Armin Kappeler\"},{\"authorId\":\"145908678\",\"name\":\"K. Boakye\"},{\"authorId\":\"145730823\",\"name\":\"J. Soares\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b499228aa74b59be32711c3926e44de208d6b636\",\"title\":\"Image Captioning: Transforming Objects into Words\",\"url\":\"https://www.semanticscholar.org/paper/b499228aa74b59be32711c3926e44de208d6b636\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97c881c89166cd4ae5c16050140edbbee417582c\",\"title\":\"Natural Language as a Scaffold for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/97c881c89166cd4ae5c16050140edbbee417582c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49299019\",\"name\":\"Junnan Li\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47521917\",\"name\":\"Q. Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e93c977025e2829f852fc8c1e8f9547c3588dbf0\",\"title\":\"vv 1 camping tent food fire Residual BRNN Input Video Visual Encoder ( CNN ) Video Encoder Sentence Encoder Word 2 Vecs Sentence Semantic Embedding vv 2 vv 3 vvNN \\u2212 1 vvNN vv Video Semantic Embedding xx\",\"url\":\"https://www.semanticscholar.org/paper/e93c977025e2829f852fc8c1e8f9547c3588dbf0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1908.02632\",\"authors\":[{\"authorId\":\"151482698\",\"name\":\"Chen Shen\"},{\"authorId\":\"145592288\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"2642638\",\"name\":\"Fuhai Chen\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"48569526\",\"name\":\"Xiangming Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c7f60b69ccafc9fb1aabc0aaac9942d68d9166cf\",\"title\":\"Scene-based Factored Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c7f60b69ccafc9fb1aabc0aaac9942d68d9166cf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2455036\",\"name\":\"Kevin Shen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e86bf3a31ff3f55b8dae4741ee3642a82f8e6680\",\"title\":\"Learning to Caption Images by Asking Natural Language Questions\",\"url\":\"https://www.semanticscholar.org/paper/e86bf3a31ff3f55b8dae4741ee3642a82f8e6680\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2002.05556\",\"authors\":[{\"authorId\":\"144869806\",\"name\":\"Pedro Henrique Martins\"},{\"authorId\":\"2114966\",\"name\":\"Vlad Niculae\"},{\"authorId\":\"2566656\",\"name\":\"Zita Marinho\"},{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4e627eb9f17369770f2cef5d71360c93c8494785\",\"title\":\"Sparse and Structured Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4e627eb9f17369770f2cef5d71360c93c8494785\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3965d73c9d7c97cdb391bfd86a15bfd3534cbd32\",\"title\":\"Deep Learning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3965d73c9d7c97cdb391bfd86a15bfd3534cbd32\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3456814\",\"name\":\"Yevgeniy Puzikov\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.18653/v1/W18-6557\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"315774a44500517d3de412736cd2653e314e507c\",\"title\":\"E2E NLG Challenge: Neural Models vs. Templates\",\"url\":\"https://www.semanticscholar.org/paper/315774a44500517d3de412736cd2653e314e507c\",\"venue\":\"INLG\",\"year\":2018},{\"arxivId\":\"1811.12104\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"51491153\",\"name\":\"Takayuki Itamochi\"},{\"authorId\":\"3193466\",\"name\":\"K. Narioka\"},{\"authorId\":\"143973868\",\"name\":\"Ikuro Sato\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3c5907ba6b921841a49e77632c14db64e4db4fd4\",\"title\":\"Towards Human-Friendly Referring Expression Generation\",\"url\":\"https://www.semanticscholar.org/paper/3c5907ba6b921841a49e77632c14db64e4db4fd4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kushal Kafle\"},{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e3bb5773205477ae4711524a9d4ae739bee40349\",\"title\":\"Visual semantic role labeling requires recognizing activities and semantic context in images\",\"url\":\"https://www.semanticscholar.org/paper/e3bb5773205477ae4711524a9d4ae739bee40349\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"33969294\",\"name\":\"K. Ding\"},{\"authorId\":\"1683738\",\"name\":\"S. Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.1109/TMM.2019.2915033\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bbb9f5378eac3eb8245cbd0f998a95cef2954508\",\"title\":\"Deep Hierarchical Encoder\\u2013Decoder Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bbb9f5378eac3eb8245cbd0f998a95cef2954508\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"144934447\",\"name\":\"M. Dom\\u00ednguez\"},{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/CVPRW.2017.274\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"477d58ad32e0e54c40da135fb8db28b23ad0ffd0\",\"title\":\"Temporally Steered Gaussian Attention for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/477d58ad32e0e54c40da135fb8db28b23ad0ffd0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1604.00790\",\"authors\":[{\"authorId\":\"47075011\",\"name\":\"Cheng Wang\"},{\"authorId\":\"1688587\",\"name\":\"Haojin Yang\"},{\"authorId\":\"28918194\",\"name\":\"C. Bartz\"},{\"authorId\":\"1708312\",\"name\":\"C. Meinel\"}],\"doi\":\"10.1145/2964284.2964299\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"26e425781e4090abfae65b5d68eac72282dd2e31\",\"title\":\"Image Captioning with Deep Bidirectional LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/26e425781e4090abfae65b5d68eac72282dd2e31\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"2012.04638\",\"authors\":[{\"authorId\":\"21518096\",\"name\":\"Zhengyuan Yang\"},{\"authorId\":\"38534822\",\"name\":\"Y. Lu\"},{\"authorId\":\"46583994\",\"name\":\"Jianfeng Wang\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"1882479\",\"name\":\"D. Flor\\u00eancio\"},{\"authorId\":\"30602591\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"1706673\",\"name\":\"C. Zhang\"},{\"authorId\":\"145637095\",\"name\":\"Lei Zhang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8deceb13cb3afcfbaab06a2c655f1935445635fe\",\"title\":\"TAP: Text-Aware Pre-training for Text-VQA and Text-Caption\",\"url\":\"https://www.semanticscholar.org/paper/8deceb13cb3afcfbaab06a2c655f1935445635fe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2399573\",\"name\":\"Xuan-Son Vu\"},{\"authorId\":\"40647710\",\"name\":\"T. Nguyen\"},{\"authorId\":\"40140266\",\"name\":\"Duc-Trong Le\"},{\"authorId\":\"144835072\",\"name\":\"L. Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eff5bb4dd745993687ff4f349cc1d59de70fc010\",\"title\":\"Multimodal Review Generation with Privacy and Fairness Awareness\",\"url\":\"https://www.semanticscholar.org/paper/eff5bb4dd745993687ff4f349cc1d59de70fc010\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400258083\",\"name\":\"Chunpu Xu\"},{\"authorId\":\"1940342\",\"name\":\"Y. Li\"},{\"authorId\":\"48161719\",\"name\":\"C. Li\"},{\"authorId\":\"1605170624\",\"name\":\"Xiang Ao\"},{\"authorId\":\"1492164677\",\"name\":\"Min Yang\"},{\"authorId\":\"39090191\",\"name\":\"Jin-Wen Tian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"17c2549052978c3be85351541f69bf2b25e75f5f\",\"title\":\"Interactive Key-Value Memory-augmented Attention for Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/17c2549052978c3be85351541f69bf2b25e75f5f\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1808.08003\",\"authors\":[{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"4267008\",\"name\":\"Guanlin Li\"},{\"authorId\":\"1803054\",\"name\":\"Shujie Liu\"},{\"authorId\":\"4947404\",\"name\":\"Zhirui Zhang\"},{\"authorId\":null,\"name\":\"Mu Li\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"747f6c8205061c619d0af6342918c7ae8e17108d\",\"title\":\"Approximate Distribution Matching for Sequence-to-Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/747f6c8205061c619d0af6342918c7ae8e17108d\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398104959\",\"name\":\"Justin R. Lovelace\"},{\"authorId\":\"144156074\",\"name\":\"Bobak Mortazavi\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.110\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a554243e6806ac419ffdb19c354686f77ef2b848\",\"title\":\"Learning to Generate Clinically Coherent Chest X-Ray Reports\",\"url\":\"https://www.semanticscholar.org/paper/a554243e6806ac419ffdb19c354686f77ef2b848\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.00947\",\"authors\":[{\"authorId\":\"51385369\",\"name\":\"Shengyu Zhang\"},{\"authorId\":\"144199812\",\"name\":\"D. Wang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"50322310\",\"name\":\"Di Xie\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"16d4620c3bb1480a654572e2e93329a4f55777fc\",\"title\":\"MGD-GAN: Text-to-Pedestrian generation through Multi-Grained Discrimination\",\"url\":\"https://www.semanticscholar.org/paper/16d4620c3bb1480a654572e2e93329a4f55777fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yang Fan\"},{\"authorId\":\"2073589\",\"name\":\"Jungang Xu\"},{\"authorId\":\"46676156\",\"name\":\"Yingfei Sun\"},{\"authorId\":\"40368776\",\"name\":\"Ben He\"}],\"doi\":\"10.1109/ICTAI.2018.00047\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"cdea9a5054f3b5f4dd4c3e75f9278e9548c2de7a\",\"title\":\"Long-Term Recurrent Merge Network Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cdea9a5054f3b5f4dd4c3e75f9278e9548c2de7a\",\"venue\":\"2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1145/3343031.3351072\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"db6035229a71a6c93d4f15c4a4280eb644228da4\",\"title\":\"Hierarchical Global-Local Temporal Modeling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/db6035229a71a6c93d4f15c4a4280eb644228da4\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2012.02420\",\"authors\":[{\"authorId\":\"1891100220\",\"name\":\"Junyu Luo\"},{\"authorId\":\"2031857668\",\"name\":\"Zifei Zheng\"},{\"authorId\":\"46489195\",\"name\":\"H. Ye\"},{\"authorId\":\"1898157310\",\"name\":\"Muchao Ye\"},{\"authorId\":null,\"name\":\"Yaqing Wang\"},{\"authorId\":\"36610242\",\"name\":\"Quanzeng You\"},{\"authorId\":\"1863907206\",\"name\":\"Cao Xiao\"},{\"authorId\":\"2988239\",\"name\":\"Fenglong Ma\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2de56959d2180137b922c4542759bcdc67f2db4e\",\"title\":\"A Benchmark Dataset for Understandable Medical Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/2de56959d2180137b922c4542759bcdc67f2db4e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.10521\",\"authors\":[{\"authorId\":\"2490092\",\"name\":\"Kyungwoo Song\"},{\"authorId\":\"91586945\",\"name\":\"Joonho Jang\"},{\"authorId\":\"120296402\",\"name\":\"Seung-Jae Shin\"},{\"authorId\":\"1729306\",\"name\":\"I. Moon\"}],\"doi\":\"10.1609/AAAI.V34I04.6039\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61bb0b9bf5aba11ab06e2a578ba32b21c4bd6611\",\"title\":\"Bivariate Beta LSTM\",\"url\":\"https://www.semanticscholar.org/paper/61bb0b9bf5aba11ab06e2a578ba32b21c4bd6611\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c72ab3484bea5aa8abbd041d31f6b17c17513de\",\"title\":\"Expressing an Image Stream with a Sequence of Natural Sentences\",\"url\":\"https://www.semanticscholar.org/paper/1c72ab3484bea5aa8abbd041d31f6b17c17513de\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1909.02050\",\"authors\":[{\"authorId\":\"144889895\",\"name\":\"Ming Jiang\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"50141029\",\"name\":\"Xin Wang\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"3207378\",\"name\":\"J. Diesner\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.18653/v1/D19-1220\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ce01073130ff984eb43cbf43f6bdcbe4d5a09df9\",\"title\":\"TIGEr: Text-to-Image Grounding for Image Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/ce01073130ff984eb43cbf43f6bdcbe4d5a09df9\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1810.04101\",\"authors\":[{\"authorId\":\"1809420\",\"name\":\"Loris Bazzani\"},{\"authorId\":\"2120874\",\"name\":\"Tobias Domhan\"},{\"authorId\":\"2521764\",\"name\":\"F. Hieber\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d76c73ab62d960f5417dfa8aff9e63d357c5fd5d\",\"title\":\"Image Captioning as Neural Machine Translation Task in SOCKEYE\",\"url\":\"https://www.semanticscholar.org/paper/d76c73ab62d960f5417dfa8aff9e63d357c5fd5d\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1812.08126\",\"authors\":[{\"authorId\":\"32236286\",\"name\":\"Annika Lindh\"},{\"authorId\":\"144394786\",\"name\":\"R. Ross\"},{\"authorId\":\"31179447\",\"name\":\"Abhijit Mahalunkar\"},{\"authorId\":\"31071031\",\"name\":\"Giancarlo Salton\"},{\"authorId\":\"34967075\",\"name\":\"John D. Kelleher\"}],\"doi\":\"10.1007/978-3-030-01418-6_18\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93a10137a41f9e1c8f939517bfc75379b47cb1a1\",\"title\":\"Generating Diverse and Meaningful Captions - Unsupervised Specificity Optimization for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93a10137a41f9e1c8f939517bfc75379b47cb1a1\",\"venue\":\"ICANN\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2123563\",\"name\":\"Shihao Wang\"},{\"authorId\":\"81983870\",\"name\":\"Hong Mo\"},{\"authorId\":\"1749615\",\"name\":\"Yue Xu\"},{\"authorId\":\"145717893\",\"name\":\"W. Wu\"},{\"authorId\":\"144812501\",\"name\":\"Zhong Zhou\"}],\"doi\":\"10.1007/978-3-030-00764-5_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51c48633735a10c2e509374ba7fad8e267f322e1\",\"title\":\"Intra-Image Region Context for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/51c48633735a10c2e509374ba7fad8e267f322e1\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"145497716\",\"name\":\"A. Das\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"}],\"doi\":\"10.18653/v1/P18-5004\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"532837c431617d37c03361ba5a7d5fdb082c55f4\",\"title\":\"Connecting Language and Vision to Actions\",\"url\":\"https://www.semanticscholar.org/paper/532837c431617d37c03361ba5a7d5fdb082c55f4\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1909.09944\",\"authors\":[{\"authorId\":\"49172303\",\"name\":\"T. Rahman\"},{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00900\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a2de516a4e628a30036193d71faac7240d553ef\",\"title\":\"Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a2de516a4e628a30036193d71faac7240d553ef\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113316526\",\"name\":\"Muneeb Ul Hassan\"},{\"authorId\":\"1714010\",\"name\":\"P. Mulhem\"},{\"authorId\":\"31617713\",\"name\":\"D. Pellerin\"},{\"authorId\":\"1380928907\",\"name\":\"G. Qu\\u00e9not\"}],\"doi\":\"10.1109/CBMI.2019.8877393\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"84114ea28401aa2d0a4874a4aeb853ec9887b405\",\"title\":\"Explaining Visual Classification using Attributes\",\"url\":\"https://www.semanticscholar.org/paper/84114ea28401aa2d0a4874a4aeb853ec9887b405\",\"venue\":\"2019 International Conference on Content-Based Multimedia Indexing (CBMI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735878\",\"name\":\"D. Zhao\"},{\"authorId\":\"144481850\",\"name\":\"Zhi Chang\"},{\"authorId\":\"50530237\",\"name\":\"Shutao Guo\"}],\"doi\":\"10.1016/j.neucom.2018.11.004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5612a5adaf8152b85010bf57c322d2fabef0fd40\",\"title\":\"A multimodal fusion approach for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/5612a5adaf8152b85010bf57c322d2fabef0fd40\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"1808.09564\",\"authors\":[{\"authorId\":\"40223399\",\"name\":\"Renjie Zheng\"},{\"authorId\":\"1847848\",\"name\":\"M. Ma\"},{\"authorId\":\"144768480\",\"name\":\"Liang Huang\"}],\"doi\":\"10.18653/v1/D18-1357\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9a716bf565445740f47243bfde06398ded224cb4\",\"title\":\"Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/9a716bf565445740f47243bfde06398ded224cb4\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41052836\",\"name\":\"Anya Belz\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"}],\"doi\":\"10.1017/S1351324918000086\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d740802aa22dbc187fe5df39108ba493b34d2839\",\"title\":\"From image to language and back again\",\"url\":\"https://www.semanticscholar.org/paper/d740802aa22dbc187fe5df39108ba493b34d2839\",\"venue\":\"Nat. Lang. Eng.\",\"year\":2018},{\"arxivId\":\"1806.06422\",\"authors\":[{\"authorId\":\"50355189\",\"name\":\"Yin Cui\"},{\"authorId\":\"29983981\",\"name\":\"Guandao Yang\"},{\"authorId\":\"2799898\",\"name\":\"Andreas Veit\"},{\"authorId\":\"144247007\",\"name\":\"X. Huang\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"}],\"doi\":\"10.1109/CVPR.2018.00608\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6d3d61ef9b5ff6d41badbc3d40ea23acbbc9c3fe\",\"title\":\"Learning to Evaluate Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6d3d61ef9b5ff6d41badbc3d40ea23acbbc9c3fe\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51001584\",\"name\":\"Hyung-min Lee\"},{\"authorId\":\"153481384\",\"name\":\"Il-Koo Kim\"}],\"doi\":\"10.1109/IJCNN.2019.8851892\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"title\":\"Generating Natural Video Descriptions using Semantic Gate\",\"url\":\"https://www.semanticscholar.org/paper/b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2218741\",\"name\":\"Fuwen Tan\"},{\"authorId\":\"145480869\",\"name\":\"Song Feng\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":\"10.1109/CVPR.2019.00687\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2bb57b98a900fe49a9328d8ec76eac600f3c3f88\",\"title\":\"Text2Scene: Generating Compositional Scenes From Textual Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2bb57b98a900fe49a9328d8ec76eac600f3c3f88\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1902.05660\",\"authors\":[{\"authorId\":\"144826412\",\"name\":\"Meet Shah\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2019.00681\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"735a63b58349e07b84c2e31927ce1b1cfaf09980\",\"title\":\"Cycle-Consistency for Robust Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/735a63b58349e07b84c2e31927ce1b1cfaf09980\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37310105\",\"name\":\"L. Zhao\"},{\"authorId\":\"47423527\",\"name\":\"Chunxia Zhang\"},{\"authorId\":null,\"name\":\"Xi Zhang\"},{\"authorId\":\"3034224\",\"name\":\"Yating Hu\"},{\"authorId\":\"8253080\",\"name\":\"Z. Niu\"}],\"doi\":\"10.1007/978-3-319-97304-3_67\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f472524ad1a5989aa1d66634a477041cd6ec9b64\",\"title\":\"A Deep Reinforced Training Method for Location-Based Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f472524ad1a5989aa1d66634a477041cd6ec9b64\",\"venue\":\"PRICAI\",\"year\":2018},{\"arxivId\":\"1709.07192\",\"authors\":[{\"authorId\":\"2180892\",\"name\":\"Yikang Li\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"47353404\",\"name\":\"X. Chu\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CVPR.2018.00640\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"47aff6477f05ec32fc163e1943fe9464a8379552\",\"title\":\"Visual Question Generation as Dual Task of Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/47aff6477f05ec32fc163e1943fe9464a8379552\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"12257085\",\"name\":\"Pang-Chi Huang\"},{\"authorId\":\"3596543\",\"name\":\"Chi-Yu Yang\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ASRU.2017.8268961\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"title\":\"Seeing and hearing too: Audio representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"47775167\",\"name\":\"Teng Li\"},{\"authorId\":\"47906413\",\"name\":\"Y. Wang\"},{\"authorId\":\"1730308\",\"name\":\"X. Wu\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1016/j.cviu.2017.06.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"94a86a758ae2608c00e9690e9951e805755bb1a1\",\"title\":\"Learning explicit video attributes from mid-level representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/94a86a758ae2608c00e9690e9951e805755bb1a1\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1611.01646\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2017.524\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5785466bc14529e94e54baa4ed051f7037f3b1d3\",\"title\":\"Boosting Image Captioning with Attributes\",\"url\":\"https://www.semanticscholar.org/paper/5785466bc14529e94e54baa4ed051f7037f3b1d3\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1706.01554\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"145721096\",\"name\":\"A. Kannan\"},{\"authorId\":\"145743311\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8cdd241b474bf7b0632162403ac2a3c4799252ad\",\"title\":\"Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\",\"url\":\"https://www.semanticscholar.org/paper/8cdd241b474bf7b0632162403ac2a3c4799252ad\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":\"10.1145/3347449.3357484\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"989730c00381805543baa470a2d6490cc5354a13\",\"title\":\"L-STAP: Learned Spatio-Temporal Adaptive Pooling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/989730c00381805543baa470a2d6490cc5354a13\",\"venue\":\"AI4TV@MM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144613170\",\"name\":\"S. Cao\"},{\"authorId\":\"2896701\",\"name\":\"G. An\"},{\"authorId\":\"4464686\",\"name\":\"ZhenXing Zheng\"},{\"authorId\":\"2383779\",\"name\":\"Qiu-Qi Ruan\"}],\"doi\":\"10.1016/j.neucom.2020.08.019\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f235982f603a740dcee9cff7c5de234878f80a3f\",\"title\":\"Interactions Guided Generative Adversarial Network for unsupervised image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f235982f603a740dcee9cff7c5de234878f80a3f\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"163a474747fd63ab62ae586711fa5e5a2ac91bd8\",\"title\":\"Improved Image Captioning via Policy Gradient optimization of SPIDEr\",\"url\":\"https://www.semanticscholar.org/paper/163a474747fd63ab62ae586711fa5e5a2ac91bd8\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145813731\",\"name\":\"X. Xu\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"49958046\",\"name\":\"HaiBin Liu\"},{\"authorId\":null,\"name\":\"Ji Yi\"},{\"authorId\":\"50218964\",\"name\":\"Zhaohui Wang\"}],\"doi\":\"10.2991/ITIM-17.2017.34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd1ee9e1d24ac60e722a8e51518e44669b052557\",\"title\":\"Video Description Using Learning Multiple Features\",\"url\":\"https://www.semanticscholar.org/paper/dd1ee9e1d24ac60e722a8e51518e44669b052557\",\"venue\":\"ICIT 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46447188\",\"name\":\"X. Zhang\"},{\"authorId\":\"2548483\",\"name\":\"Shengfeng He\"},{\"authorId\":\"1688375\",\"name\":\"Xinhang Song\"},{\"authorId\":\"1726262\",\"name\":\"R. Lau\"},{\"authorId\":\"24350293\",\"name\":\"J. Jiao\"},{\"authorId\":\"1694936\",\"name\":\"Q. Ye\"}],\"doi\":\"10.1016/J.NEUCOM.2018.02.112\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7a305d80eae6a9b9fecbeeb7ade0b25d412219d\",\"title\":\"Image captioning via semantic element embedding\",\"url\":\"https://www.semanticscholar.org/paper/f7a305d80eae6a9b9fecbeeb7ade0b25d412219d\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"2004.00390\",\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"5332711\",\"name\":\"Meng Wang\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/cvpr42600.2020.00483\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c936e70cc1de52c5ad0ed5ec7a219bd25a46902c\",\"title\":\"More Grounded Image Captioning by Distilling Image-Text Matching Model\",\"url\":\"https://www.semanticscholar.org/paper/c936e70cc1de52c5ad0ed5ec7a219bd25a46902c\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.00303\",\"authors\":[{\"authorId\":\"51385369\",\"name\":\"Shengyu Zhang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1515187069\",\"name\":\"Qinghao Huang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"2348893\",\"name\":\"Y. Yang\"},{\"authorId\":\"93192602\",\"name\":\"Fei Wu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8c7251fe922e35a982b8d821d332fd5e9a81c0f\",\"title\":\"Grounded and Controllable Image Completion by Incorporating Lexical Semantics\",\"url\":\"https://www.semanticscholar.org/paper/f8c7251fe922e35a982b8d821d332fd5e9a81c0f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.11760\",\"authors\":[{\"authorId\":\"24040986\",\"name\":\"Gabriel Huang\"},{\"authorId\":\"1560385163\",\"name\":\"Bo Pang\"},{\"authorId\":\"2062703\",\"name\":\"Z. Zhu\"},{\"authorId\":\"66193113\",\"name\":\"C. Rivera\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"title\":\"Multimodal Pretraining for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2010.07279\",\"authors\":[{\"authorId\":\"37619618\",\"name\":\"Khyathi Raghavi Chandu\"},{\"authorId\":\"1690706\",\"name\":\"A. Black\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"09835698e85c111992261e299819b5340376ef88\",\"title\":\"Dissecting the components and factors of Neural Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/09835698e85c111992261e299819b5340376ef88\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2569534\",\"name\":\"J. Kim\"},{\"authorId\":\"144887521\",\"name\":\"Suhong Moon\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":\"10.1109/cvpr42600.2020.00968\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b7e34296ea9a74193b906b37c5bb6d939ba68017\",\"title\":\"Advisable Learning for Self-Driving Vehicles by Internalizing Observation-to-Action Rules\",\"url\":\"https://www.semanticscholar.org/paper/b7e34296ea9a74193b906b37c5bb6d939ba68017\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2002.06353\",\"authors\":[{\"authorId\":\"35347136\",\"name\":\"Huaishao Luo\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"66782928\",\"name\":\"Tianrui Li\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4243555758433880a67b15b50f752b1e2a8c4609\",\"title\":\"UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation\",\"url\":\"https://www.semanticscholar.org/paper/4243555758433880a67b15b50f752b1e2a8c4609\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492164677\",\"name\":\"Min Yang\"},{\"authorId\":\"2948588\",\"name\":\"Junhao Liu\"},{\"authorId\":\"143822675\",\"name\":\"Ying Shen\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"40265331\",\"name\":\"X. Chen\"},{\"authorId\":\"31060469\",\"name\":\"Qingyao Wu\"},{\"authorId\":\"48161719\",\"name\":\"C. Li\"}],\"doi\":\"10.1109/TIP.2020.3028651\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f6a09cd467a2752e60a2766160a00c658667043e\",\"title\":\"An Ensemble of Generation- and Retrieval-Based Image Captioning With Dual Generator Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/f6a09cd467a2752e60a2766160a00c658667043e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1978802390\",\"name\":\"Haolei Pei\"},{\"authorId\":\"8559954\",\"name\":\"Q. Chen\"},{\"authorId\":\"13257164\",\"name\":\"J. Wang\"},{\"authorId\":\"123555217\",\"name\":\"Q. Sun\"},{\"authorId\":\"1680030\",\"name\":\"Yubo Jia\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206815\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"title\":\"Visual Relational Reasoning for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2007.05597\",\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"72612482\",\"name\":\"Peiye Zhuang\"},{\"authorId\":\"48188321\",\"name\":\"A. Pyrros\"},{\"authorId\":\"145643745\",\"name\":\"N. Siddiqui\"},{\"authorId\":\"143812875\",\"name\":\"O. Koyejo\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c3612f08814b86a5e44dfd1fe549acb7a1c1801\",\"title\":\"EMIXER: End-to-end Multimodal X-ray Generation via Self-supervision\",\"url\":\"https://www.semanticscholar.org/paper/0c3612f08814b86a5e44dfd1fe549acb7a1c1801\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1657283730\",\"name\":\"K. Nithya\"},{\"authorId\":\"1652924169\",\"name\":\"V. Kumar\"}],\"doi\":\"10.1109/ICCSP48568.2020.9182105\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6acbc2016140058d6ad06c15a46e50af30633f05\",\"title\":\"A Review on Automatic Image Captioning Techniques\",\"url\":\"https://www.semanticscholar.org/paper/6acbc2016140058d6ad06c15a46e50af30633f05\",\"venue\":\"2020 International Conference on Communication and Signal Processing (ICCSP)\",\"year\":2020},{\"arxivId\":\"2006.08792\",\"authors\":[{\"authorId\":\"3192572\",\"name\":\"Emiel van Miltenburg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a624add94183f1638678784d51de4f0f0508fc4\",\"title\":\"On the use of human reference data for evaluating automatic image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/0a624add94183f1638678784d51de4f0f0508fc4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67011698\",\"name\":\"Ori Bar El\"},{\"authorId\":\"1702212\",\"name\":\"T. Milo\"},{\"authorId\":\"2559241\",\"name\":\"Amit Somech\"}],\"doi\":\"10.1145/3318464.3389779\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"86c0107bb37599ab1f69c6a4695a03105e7b8bfb\",\"title\":\"Automatically Generating Data Exploration Sessions Using Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/86c0107bb37599ab1f69c6a4695a03105e7b8bfb\",\"venue\":\"SIGMOD Conference\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3225210\",\"name\":\"Shaohan Hu\"},{\"authorId\":\"15634170\",\"name\":\"S. Huang\"},{\"authorId\":\"50248791\",\"name\":\"G. Wang\"},{\"authorId\":\"49969968\",\"name\":\"Zhipeng Li\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"}],\"doi\":\"10.1007/978-3-030-36802-9_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"797089139d87aeda56bee0b0374bee71521ae169\",\"title\":\"Delving into Precise Attention in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/797089139d87aeda56bee0b0374bee71521ae169\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48520780\",\"name\":\"X. Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"38bbace118817cd18677f169a8c8e6c8b005df18\",\"title\":\"Auto-Encoding Graphical Inductive Bias for Descriptive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/38bbace118817cd18677f169a8c8e6c8b005df18\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1909.04312\",\"authors\":[{\"authorId\":\"1692609\",\"name\":\"S. Yang\"},{\"authorId\":\"143715293\",\"name\":\"W. Zhang\"},{\"authorId\":\"2882531\",\"name\":\"Weizhi Lu\"},{\"authorId\":\"2970970\",\"name\":\"Hesheng Wang\"},{\"authorId\":\"9348561\",\"name\":\"Yibin Li\"}],\"doi\":\"10.1109/IROS40897.2019.8968278\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"07c33f1d771ed265f9c6fe40a43323417d0951a6\",\"title\":\"Learning Actions from Human Demonstration Video for Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/07c33f1d771ed265f9c6fe40a43323417d0951a6\",\"venue\":\"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15181932\",\"name\":\"Rizal Setya Perdana\"},{\"authorId\":\"15167137\",\"name\":\"Y. Ishida\"}],\"doi\":\"10.1109/ELECSYM.2019.8901660\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"348a908617ff00c09c4d4456268da7bb60435441\",\"title\":\"Instance-based Deep Transfer Learning on Cross-domain Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/348a908617ff00c09c4d4456268da7bb60435441\",\"venue\":\"2019 International Electronics Symposium (IES)\",\"year\":2019},{\"arxivId\":\"1705.00823\",\"authors\":[{\"authorId\":\"31678456\",\"name\":\"Y. Yoshikawa\"},{\"authorId\":\"3087214\",\"name\":\"Yutaro Shigeto\"},{\"authorId\":\"39702069\",\"name\":\"A. Takeuchi\"}],\"doi\":\"10.18653/v1/P17-2066\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6128190a8c18cde6b94e0fae934d6fcc406ea0bb\",\"title\":\"STAIR Captions: Constructing a Large-Scale Japanese Image Caption Dataset\",\"url\":\"https://www.semanticscholar.org/paper/6128190a8c18cde6b94e0fae934d6fcc406ea0bb\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":\"1511.02283\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"3317152\",\"name\":\"Oana-Maria Camburu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/CVPR.2016.9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e65142010431ffc089b272a1174214e00693e503\",\"title\":\"Generation and Comprehension of Unambiguous Object Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e65142010431ffc089b272a1174214e00693e503\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145279549\",\"name\":\"Xiao Lin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c822419edde10e21d4d0bb381d97c5f774dcfd19\",\"title\":\"Leveraging Multimodal Perspectives to Learn Common Sense for Vision and Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/c822419edde10e21d4d0bb381d97c5f774dcfd19\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48459964\",\"name\":\"William Needham\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ada886a7e2475062e5a22aa7401733696e2c865\",\"title\":\"A Review of Neural Approaches to the Question Answering Task\",\"url\":\"https://www.semanticscholar.org/paper/9ada886a7e2475062e5a22aa7401733696e2c865\",\"venue\":\"FDIA@ESSIR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1774780\",\"name\":\"Yilin Shen\"},{\"authorId\":\"1705713\",\"name\":\"Hongxia Jin\"},{\"authorId\":\"46848045\",\"name\":\"S. Ghosh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"080a2805853cc0e5a8cba05af1fb7a65ace8f6d7\",\"title\":\"LANGUAGE MODELS MORE GROUNDED\",\"url\":\"https://www.semanticscholar.org/paper/080a2805853cc0e5a8cba05af1fb7a65ace8f6d7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"26554826\",\"name\":\"Yuzhao Mao\"},{\"authorId\":\"1680068\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-319-69005-6_27\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"654afbb93bbff18fb8772993cbdb6e82f9a54716\",\"title\":\"Topic-Specific Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/654afbb93bbff18fb8772993cbdb6e82f9a54716\",\"venue\":\"CCL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47956883\",\"name\":\"Xiangrong Zhang\"},{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"144474380\",\"name\":\"X. Tang\"},{\"authorId\":\"46544755\",\"name\":\"Huiyu Zhou\"},{\"authorId\":\"33161908\",\"name\":\"C. Li\"}],\"doi\":\"10.3390/rs11060612\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"944e93e74379afedced307ca30fc6d31365dc96e\",\"title\":\"Description Generation for Remote Sensing Images Using Attribute Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/944e93e74379afedced307ca30fc6d31365dc96e\",\"venue\":\"Remote. Sens.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748994\",\"name\":\"D. Nguyen\"},{\"authorId\":\"145178790\",\"name\":\"T. Tran\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"fd126d2029aeee68d50beaf88acd6609917e77cb\",\"title\":\"Structure-based Generation System for E 2 E NLG Challenge\",\"url\":\"https://www.semanticscholar.org/paper/fd126d2029aeee68d50beaf88acd6609917e77cb\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1908.02943\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"143899054\",\"name\":\"Mark Dras\"},{\"authorId\":\"3093086\",\"name\":\"Stephen Wan\"},{\"authorId\":\"145212976\",\"name\":\"C\\u00e9cile Paris\"},{\"authorId\":\"119899233\",\"name\":\"Len Hamey\"}],\"doi\":\"10.1007/978-3-030-29908-8_22\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d236d1415ac9a52ec599f0c100e15589cfceefd9\",\"title\":\"Towards Generating Stylized Image Captions via Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/d236d1415ac9a52ec599f0c100e15589cfceefd9\",\"venue\":\"PRICAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/ISM.2018.00021\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"95bba8c514f63c666e0a85bdf9c6e3b8113d1f8f\",\"title\":\"Improved Image Description Via Embedded Object Structure Graph and Semantic Feature Matching\",\"url\":\"https://www.semanticscholar.org/paper/95bba8c514f63c666e0a85bdf9c6e3b8113d1f8f\",\"venue\":\"2018 IEEE International Symposium on Multimedia (ISM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/ACCESS.2018.2879642\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"title\":\"A Fine-Grained Spatial-Temporal Attention Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1906.02467\",\"authors\":[{\"authorId\":\"48567197\",\"name\":\"Zhou Yu\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1720236\",\"name\":\"J. Yu\"},{\"authorId\":\"144478231\",\"name\":\"T. Yu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1609/aaai.v33i01.33019127\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4f2c1af57c056102806a184517313804f66e7447\",\"title\":\"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/4f2c1af57c056102806a184517313804f66e7447\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"47381330\",\"name\":\"A. Dimou\"},{\"authorId\":\"33961149\",\"name\":\"G. Papadopoulos\"},{\"authorId\":\"1747572\",\"name\":\"P. Daras\"}],\"doi\":\"10.1109/ICE.2019.8792602\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"title\":\"Incorporating Textual Similarity in Video Captioning Schemes\",\"url\":\"https://www.semanticscholar.org/paper/2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"venue\":\"2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)\",\"year\":2019},{\"arxivId\":\"2012.03662\",\"authors\":[{\"authorId\":\"50218156\",\"name\":\"Zhaokai Wang\"},{\"authorId\":\"7760591\",\"name\":\"Renda Bao\"},{\"authorId\":\"1720738301\",\"name\":\"Qi Wu\"},{\"authorId\":\"101219260\",\"name\":\"Si Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1fe7a16266cf2316f436688e0df6c6350c885ef\",\"title\":\"Confidence-aware Non-repetitive Multimodal Transformers for TextCaps\",\"url\":\"https://www.semanticscholar.org/paper/b1fe7a16266cf2316f436688e0df6c6350c885ef\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67159407\",\"name\":\"S. Amirian\"},{\"authorId\":\"1799121\",\"name\":\"K. Rasheed\"},{\"authorId\":\"1784399\",\"name\":\"T. Taha\"},{\"authorId\":\"1712033\",\"name\":\"H. Arabnia\"}],\"doi\":\"10.1109/ACCESS.2020.3042484\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"832aafb4989c24211a8377f82228c31f7a90ef81\",\"title\":\"Automatic Image and Video Caption Generation With Deep Learning: A Concise Review and Algorithmic Overlap\",\"url\":\"https://www.semanticscholar.org/paper/832aafb4989c24211a8377f82228c31f7a90ef81\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1610.02424\",\"authors\":[{\"authorId\":\"3358971\",\"name\":\"Ashwin K. Vijayakumar\"},{\"authorId\":\"144354133\",\"name\":\"Michael Cogswell\"},{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"144207643\",\"name\":\"Q. Sun\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e4dd95c4341ec7d14317a3d97022773a0822906c\",\"title\":\"Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/e4dd95c4341ec7d14317a3d97022773a0822906c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34589688\",\"name\":\"Zhenghang Yuan\"},{\"authorId\":\"121856647\",\"name\":\"X. Li\"},{\"authorId\":\"39669401\",\"name\":\"Q. Wang\"}],\"doi\":\"10.1109/ACCESS.2019.2962195\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"52a221f10ec19fb3b20fda0271184b641c2ccc4b\",\"title\":\"Exploring Multi-Level Attention and Semantic Relationship for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/52a221f10ec19fb3b20fda0271184b641c2ccc4b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73195876\",\"name\":\"M. Najman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eea248baa16162da661fbb9255e2cfcd5b9f0c05\",\"title\":\"Bachelor Project Image Captioning with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/eea248baa16162da661fbb9255e2cfcd5b9f0c05\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144354055\",\"name\":\"Elizabeth Clark\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144365875\",\"name\":\"Noah A. Smith\"}],\"doi\":\"10.18653/v1/P19-1264\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6ab3f185d9e4f0c8cfa0331810e15f5db09035c3\",\"title\":\"Sentence Mover's Similarity: Automatic Evaluation for Multi-Sentence Texts\",\"url\":\"https://www.semanticscholar.org/paper/6ab3f185d9e4f0c8cfa0331810e15f5db09035c3\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9610143\",\"name\":\"Zhihao Fan\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"50695111\",\"name\":\"Siyuan Wang\"},{\"authorId\":\"1790227\",\"name\":\"X. Huang\"}],\"doi\":\"10.18653/v1/P19-1652\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"aaf5e3afd61d0df6c483ca32faf8e7a9198b1557\",\"title\":\"Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aaf5e3afd61d0df6c483ca32faf8e7a9198b1557\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152969844\",\"name\":\"N. Zakharov\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"39197903\",\"name\":\"Jun Zhu\"},{\"authorId\":\"2151203\",\"name\":\"J. Gl\\u00e4scher\"}],\"doi\":\"10.1016/J.JVCIR.2019.102574\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e0d5efef6e620050435280f8a3e2df982dce8ea\",\"title\":\"Towards controllable image descriptions with semi-supervised VAE\",\"url\":\"https://www.semanticscholar.org/paper/1e0d5efef6e620050435280f8a3e2df982dce8ea\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51043791\",\"name\":\"A. Kalyan\"},{\"authorId\":\"153149395\",\"name\":\"P. Anderson\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"883cb62237c50de7615c837a0830d6a88721112a\",\"title\":\"Trainable Decoding of Sets of Sequences for Neural Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/883cb62237c50de7615c837a0830d6a88721112a\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"120800460\",\"name\":\"L. Zhou\"},{\"authorId\":\"98110081\",\"name\":\"J. Gao\"},{\"authorId\":\"49620738\",\"name\":\"Di Li\"},{\"authorId\":\"70362337\",\"name\":\"Heung-Yeung Shum\"}],\"doi\":\"10.1162/coli_a_00368\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3737b57e8b739b595cd5b877769cbc719c42f2c7\",\"title\":\"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot\",\"url\":\"https://www.semanticscholar.org/paper/3737b57e8b739b595cd5b877769cbc719c42f2c7\",\"venue\":\"Computational Linguistics\",\"year\":2020},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47084642\",\"name\":\"Heng Song\"},{\"authorId\":\"1756644\",\"name\":\"Junwu Zhu\"},{\"authorId\":\"1591599792\",\"name\":\"Y. Jiang\"}],\"doi\":\"10.1016/j.compeleceng.2020.106630\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9835f38f6d3a8b964d53e78fa01e4490169b9169\",\"title\":\"avtmNet: Adaptive Visual-Text Merging Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9835f38f6d3a8b964d53e78fa01e4490169b9169\",\"venue\":\"Comput. Electr. Eng.\",\"year\":2020},{\"arxivId\":\"1903.02499\",\"authors\":[{\"authorId\":\"47287647\",\"name\":\"Sen He\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"3251678\",\"name\":\"N. Pugeault\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f9f1de36707a7c4885006554be09d16cf3fd6b4\",\"title\":\"A Synchronized Multi-Modal Attention-Caption Dataset and Analysis\",\"url\":\"https://www.semanticscholar.org/paper/3f9f1de36707a7c4885006554be09d16cf3fd6b4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.07268\",\"authors\":[{\"authorId\":\"10098888\",\"name\":\"R. Bigazzi\"},{\"authorId\":\"67344892\",\"name\":\"Federico Landi\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ffc4fa1db8372d763f990a1f0a6985260d693b0\",\"title\":\"Explore and Explain: Self-supervised Navigation and Recounting\",\"url\":\"https://www.semanticscholar.org/paper/9ffc4fa1db8372d763f990a1f0a6985260d693b0\",\"venue\":\"ICPR 2020\",\"year\":2020},{\"arxivId\":\"2009.12313\",\"authors\":[{\"authorId\":\"32556011\",\"name\":\"Victor Milewski\"},{\"authorId\":\"100781843\",\"name\":\"Marie-Francine Moens\"},{\"authorId\":\"2338197\",\"name\":\"Iacer Calixto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b89e002625b0703f08b7590a1763ca7c8be66ad4\",\"title\":\"Are scene graphs good enough to improve Image Captioning?\",\"url\":\"https://www.semanticscholar.org/paper/b89e002625b0703f08b7590a1763ca7c8be66ad4\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"1912.11637\",\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"143725038\",\"name\":\"Qi Su\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"title\":\"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.04731\",\"authors\":[{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"1388166580\",\"name\":\"Karin Sevegnani\"},{\"authorId\":\"2621022\",\"name\":\"Ioannis Konstas\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/W19-8644\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c424857515fc0538ba93c14652c291e2ffc9d3c4\",\"title\":\"Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)\",\"url\":\"https://www.semanticscholar.org/paper/c424857515fc0538ba93c14652c291e2ffc9d3c4\",\"venue\":\"INLG\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36416867\",\"name\":\"J. Wu\"},{\"authorId\":\"30461408\",\"name\":\"Siya Xie\"},{\"authorId\":\"8182911\",\"name\":\"Xinbao Shi\"},{\"authorId\":\"2410016\",\"name\":\"Yaowen Chen\"}],\"doi\":\"10.1007/978-981-10-7299-4_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a1c523c5f30b5d005f79428197fa5e801f5b9d95\",\"title\":\"Global-Local Feature Attention Network with Reranking Strategy for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/a1c523c5f30b5d005f79428197fa5e801f5b9d95\",\"venue\":\"CCCV\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145037543\",\"name\":\"Q. Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"974cadd15684c96618d04f845794cec5568a86a6\",\"title\":\"Greedy Inference Algorithms for Structured and Neural Models\",\"url\":\"https://www.semanticscholar.org/paper/974cadd15684c96618d04f845794cec5568a86a6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1802.08218\",\"authors\":[{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"},{\"authorId\":\"48933900\",\"name\":\"Q. Li\"},{\"authorId\":\"32027456\",\"name\":\"A. J. Stangl\"},{\"authorId\":\"2582404\",\"name\":\"Anhong Guo\"},{\"authorId\":\"47532530\",\"name\":\"Chi Lin\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1744846\",\"name\":\"Jeffrey P. Bigham\"}],\"doi\":\"10.1109/CVPR.2018.00380\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c\",\"title\":\"VizWiz Grand Challenge: Answering Visual Questions from Blind People\",\"url\":\"https://www.semanticscholar.org/paper/a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151487400\",\"name\":\"Chu-yi Li\"},{\"authorId\":\"9319341\",\"name\":\"Wei-yu Yu\"}],\"doi\":\"10.1117/12.2514651\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ddbc1542476237b6ace7b871e34269e790d35bad\",\"title\":\"Spatial-temporal attention in Bi-LSTM networks based on multiple features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ddbc1542476237b6ace7b871e34269e790d35bad\",\"venue\":\"Other Conferences\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39024a168ea1511821e5af17bdf838bf4afb3db8\",\"title\":\"FDU Participation in TRECVID 2019 VTT Task\",\"url\":\"https://www.semanticscholar.org/paper/39024a168ea1511821e5af17bdf838bf4afb3db8\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"144245551\",\"name\":\"M. Tang\"},{\"authorId\":\"50561313\",\"name\":\"J. Zhang\"}],\"doi\":\"10.1117/1.JEI.27.2.023027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"title\":\"Deep hierarchical attention network for video description\",\"url\":\"https://www.semanticscholar.org/paper/8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"26559284\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"title\":\"Captioning Near-Future Activity Sequences\",\"url\":\"https://www.semanticscholar.org/paper/dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"3125776\",\"name\":\"Chris Brockett\"},{\"authorId\":\"66648221\",\"name\":\"B. Dolan\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39bb10ef8080ef8bb32ded0fed524aa3d14b76f0\",\"title\":\"The MSR-NLP System at Dialog System Technology Challenges 6\",\"url\":\"https://www.semanticscholar.org/paper/39bb10ef8080ef8bb32ded0fed524aa3d14b76f0\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1804.00861\",\"authors\":[{\"authorId\":\"7379232\",\"name\":\"Dianqi Li\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"144463557\",\"name\":\"M. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"39144e468bde0424d38a3f20a6b62ddec4b459ae\",\"title\":\"Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/39144e468bde0424d38a3f20a6b62ddec4b459ae\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8275214\",\"name\":\"P. Tang\"}],\"doi\":\"10.1109/ICME.2017.8019408\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2bb66ba1ef18110b422a8a7a2ce0e6400181b952\",\"title\":\"Image captioning with deep LSTM based on sequential residual\",\"url\":\"https://www.semanticscholar.org/paper/2bb66ba1ef18110b422a8a7a2ce0e6400181b952\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405511901\",\"name\":\"Luis Fernando D'Haro\"},{\"authorId\":\"2237192\",\"name\":\"Koichiro Yoshino\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1725498\",\"name\":\"L. Polymenakos\"},{\"authorId\":\"1727211\",\"name\":\"Jonathan K. Kummerfeld\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"71886367\",\"name\":\"Xiang Gao\"}],\"doi\":\"10.1016/j.csl.2020.101068\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2da92688aef91371424f0e3ca5482c9f8dbe67b7\",\"title\":\"Overview of the seventh Dialog System Technology Challenge: DSTC7\",\"url\":\"https://www.semanticscholar.org/paper/2da92688aef91371424f0e3ca5482c9f8dbe67b7\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"145534769\",\"name\":\"N. Ding\"},{\"authorId\":\"7685850\",\"name\":\"Sebastian Goodman\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/P18-1238\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b4df354db88a70183a64dbc9e56cf14e7669a6c0\",\"title\":\"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1906.07689\",\"authors\":[{\"authorId\":\"47300698\",\"name\":\"Hao Tan\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"145527705\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P19-1182\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157\",\"title\":\"Expressing Visual Relationships via Language\",\"url\":\"https://www.semanticscholar.org/paper/4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478695\",\"name\":\"Teng Jiang\"},{\"authorId\":\"144513168\",\"name\":\"L. Gong\"},{\"authorId\":\"7607492\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1142/s146902682050011x\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"81b1a0d8ca4ab3b9807164866de7e5dca73651dd\",\"title\":\"Spatial Relational Attention Using Fully Convolutional Networks for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/81b1a0d8ca4ab3b9807164866de7e5dca73651dd\",\"venue\":\"Int. J. Comput. Intell. Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"L. Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1512.03460\",\"authors\":[{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"47001807\",\"name\":\"Y. Li\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8172a3812e6a451eaac7d4b3e1102e5942279c26\",\"title\":\"Neural Self Talk: Image Understanding via Continuous Questioning and Answering\",\"url\":\"https://www.semanticscholar.org/paper/8172a3812e6a451eaac7d4b3e1102e5942279c26\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1607.08822\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.1007/978-3-319-46454-1_24\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"title\":\"SPICE: Semantic Propositional Image Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546292\",\"name\":\"Justin Sybrandt\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7e5ddb01ac11bd32770e81baede3c96a4ca7dde1\",\"title\":\"Exploiting Latent Features of Text and Graphs\",\"url\":\"https://www.semanticscholar.org/paper/7e5ddb01ac11bd32770e81baede3c96a4ca7dde1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.03966\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"49050519\",\"name\":\"Jiyuan Zhang\"},{\"authorId\":\"47119038\",\"name\":\"X. Wang\"},{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/CVPR.2019.00854\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"title\":\"Memory-Attended Recurrent Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1708.02300\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D17-1103\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"title\":\"Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/3122865\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"title\":\"Frontiers of Multimedia Research\",\"url\":\"https://www.semanticscholar.org/paper/8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1611.09830\",\"authors\":[{\"authorId\":\"3382568\",\"name\":\"Adam Trischler\"},{\"authorId\":\"1695404\",\"name\":\"T. Wang\"},{\"authorId\":\"2854297\",\"name\":\"Xingdi Yuan\"},{\"authorId\":\"37154982\",\"name\":\"J. Harris\"},{\"authorId\":\"2041695\",\"name\":\"Alessandro Sordoni\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"2987426\",\"name\":\"Kaheer Suleman\"}],\"doi\":\"10.18653/v1/W17-2623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3eda43078ae1f4741f09be08c4ecab6229046a5c\",\"title\":\"NewsQA: A Machine Comprehension Dataset\",\"url\":\"https://www.semanticscholar.org/paper/3eda43078ae1f4741f09be08c4ecab6229046a5c\",\"venue\":\"Rep4NLP@ACL\",\"year\":2017},{\"arxivId\":\"2012.10930\",\"authors\":[{\"authorId\":\"51174755\",\"name\":\"X. Zhang\"},{\"authorId\":\"47535646\",\"name\":\"C. Liu\"},{\"authorId\":\"32617816\",\"name\":\"Faliang Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"abd8dd9da90b5b08a8ea67d937b8688101fa0f86\",\"title\":\"Guidance Module Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abd8dd9da90b5b08a8ea67d937b8688101fa0f86\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":\"10.1145/3009906\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6e60536c847ac25dba4c1c071e0355e5537fe061\",\"title\":\"Computer Vision and Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/6e60536c847ac25dba4c1c071e0355e5537fe061\",\"venue\":\"ACM Comput. Surv.\",\"year\":2017},{\"arxivId\":\"1804.00100\",\"authors\":[{\"authorId\":null,\"name\":\"Jingwen Wang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"}],\"doi\":\"10.1109/CVPR.2018.00751\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"title\":\"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145414551\",\"name\":\"Takashi Miyazaki\"},{\"authorId\":\"3037066\",\"name\":\"N. Shimizu\"}],\"doi\":\"10.18653/v1/P16-1168\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"558c587373e2ea44898f70de7858da71aa217b8d\",\"title\":\"Cross-Lingual Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/558c587373e2ea44898f70de7858da71aa217b8d\",\"venue\":\"ACL\",\"year\":2016},{\"arxivId\":\"2003.12462\",\"authors\":[{\"authorId\":\"144656873\",\"name\":\"O. Sidorov\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"}],\"doi\":\"10.1007/978-3-030-58536-5_44\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7442eaaf453e63195cdee037f8e23830b4004027\",\"title\":\"TextCaps: a Dataset for Image Captioning with Reading Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7442eaaf453e63195cdee037f8e23830b4004027\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.03923\",\"authors\":[{\"authorId\":\"1410097225\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1e420efcd3fbace6f219c812a78d33cb64e25445\",\"title\":\"Deconfounded Image Captioning: A Causal Retrospect\",\"url\":\"https://www.semanticscholar.org/paper/1e420efcd3fbace6f219c812a78d33cb64e25445\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.14799\",\"authors\":[{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144354055\",\"name\":\"Elizabeth Clark\"},{\"authorId\":\"48441311\",\"name\":\"Jianfeng Gao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6bec59b9d3b99f6b368d0f587edd252dd30f729\",\"title\":\"Evaluation of Text Generation: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/a6bec59b9d3b99f6b368d0f587edd252dd30f729\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.15454\",\"authors\":[{\"authorId\":null,\"name\":\"Wei-Ning Hsu\"},{\"authorId\":null,\"name\":\"David Harwath\"},{\"authorId\":null,\"name\":\"Christopher Song\"},{\"authorId\":\"152450847\",\"name\":\"J. Glass\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5df12460d9a742b08f82e8b79cb102a8be5dd9b4\",\"title\":\"Text-Free Image-to-Speech Synthesis Using Learned Segmental Units\",\"url\":\"https://www.semanticscholar.org/paper/5df12460d9a742b08f82e8b79cb102a8be5dd9b4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2001.09099\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.1007/978-3-030-58589-1_27\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"37dd9c725cb800fd5f336a5227f02c160b8723cb\",\"title\":\"TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/37dd9c725cb800fd5f336a5227f02c160b8723cb\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8752221\",\"name\":\"Zhijing Jin\"},{\"authorId\":\"153683057\",\"name\":\"Qipeng Guo\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"1852415\",\"name\":\"Zheng Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"582b1f5c08f79d0d62e41a9c0b1185f3e7292e4e\",\"title\":\"GenWiki: A Dataset of 1.3 Million Content-Sharing Text and Graphs for Unsupervised Graph-to-Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/582b1f5c08f79d0d62e41a9c0b1185f3e7292e4e\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1904.09675\",\"authors\":[{\"authorId\":\"123437034\",\"name\":\"Tianyi Zhang\"},{\"authorId\":\"145461044\",\"name\":\"V. Kishore\"},{\"authorId\":\"24277779\",\"name\":\"Felix Wu\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"},{\"authorId\":\"3167681\",\"name\":\"Yoav Artzi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"295065d942abca0711300b2b4c39829551060578\",\"title\":\"BERTScore: Evaluating Text Generation with BERT\",\"url\":\"https://www.semanticscholar.org/paper/295065d942abca0711300b2b4c39829551060578\",\"venue\":\"ICLR\",\"year\":2020}],\"corpusId\":9026666,\"doi\":\"10.1109/CVPR.2015.7299087\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":442,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"73484371\",\"name\":\"H. Mller\"},{\"authorId\":\"1704149\",\"name\":\"P. Clough\"},{\"authorId\":\"1879646\",\"name\":\"Thomas Deselaers\"},{\"authorId\":\"3033284\",\"name\":\"B. Caputo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7524f70d75789da50e1c01b65e654b4ff1e01c3e\",\"title\":\"ImageCLEF: Experimental Evaluation in Visual Information Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/7524f70d75789da50e1c01b65e654b4ff1e01c3e\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6e6f47c4b2109e7824cd475336c3676faf9b113e\",\"title\":\"Baby Talk : Understanding and Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/6e6f47c4b2109e7824cd475336c3676faf9b113e\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1685089\",\"name\":\"Pedro F. Felzenszwalb\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":\"10.1109/TPAMI.2009.167\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e79272fe3d65197100eae8be9fec6469107969ae\",\"title\":\"Object Detection with Discriminatively Trained Part Based Models\",\"url\":\"https://www.semanticscholar.org/paper/e79272fe3d65197100eae8be9fec6469107969ae\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R Vedantam\"},{\"authorId\":null,\"name\":\"C L Zitnick\"},{\"authorId\":null,\"name\":\"D Parikh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cider: Consensus-based image description evaluation. CoRR, abs/1411\",\"url\":\"\",\"venue\":\"Cider: Consensus-based image description evaluation. CoRR, abs/1411\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1613/jair.3994\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9814df8bd00ba999c4d1e305a7e9bca579dc7c75\",\"title\":\"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/9814df8bd00ba999c4d1e305a7e9bca579dc7c75\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"143694777\",\"name\":\"Frank Keller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3f6a4556769e819242d669d073b895f1e45a706f\",\"title\":\"Image Description using Visual Dependency Representations\",\"url\":\"https://www.semanticscholar.org/paper/3f6a4556769e819242d669d073b895f1e45a706f\",\"venue\":\"EMNLP\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Tamuz\"},{\"authorId\":null,\"name\":\"C. Liu\"},{\"authorId\":null,\"name\":\"S. Belongie\"},{\"authorId\":null,\"name\":\"O. Shamir\"},{\"authorId\":null,\"name\":\"A. T. Kalai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CVPRW \\u2019 08\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O Vinyals\"},{\"authorId\":null,\"name\":\"A Toshev\"},{\"authorId\":null,\"name\":\"S Bengio\"},{\"authorId\":null,\"name\":\"D Erhan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Show and tell: A neural image caption generator. CoRR, abs/1411\",\"url\":\"\",\"venue\":\"Show and tell: A neural image caption generator. CoRR, abs/1411\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"title\":\"Im2Text: Describing Images Using 1 Million Captioned Photographs\",\"url\":\"https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":\"1410.1090\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9\",\"title\":\"Explain Images with Multimodal Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/82fdca623c65b6acf6b06bdeed48b2a9ebdb80a9\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf60322f83714523e2d7c1d39983151fe9db7146\",\"title\":\"Collecting Image Annotations Using Amazon's Mechanical Turk\",\"url\":\"https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146\",\"venue\":\"Mturk@HLT-NAACL\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"2057788\",\"name\":\"M. Osborne\"},{\"authorId\":\"1755162\",\"name\":\"Philipp Koehn\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7\",\"title\":\"Re-evaluation the Role of Bleu in Machine Translation Research\",\"url\":\"https://www.semanticscholar.org/paper/0a1f4cc5e1d7ccdce98c65545bbcccc23a6c16e7\",\"venue\":\"EACL\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144096985\",\"name\":\"G. Miller\"}],\"doi\":\"10.1145/219717.219748\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"68c03788224000794d5491ab459be0b2a2c38677\",\"title\":\"WordNet: a lexical database for English\",\"url\":\"https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677\",\"venue\":\"CACM\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"143694777\",\"name\":\"Frank Keller\"}],\"doi\":\"10.3115/v1/P14-2074\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"52f86811b57034ba5c0478b37cab101d9a84024a\",\"title\":\"Comparing Automatic Evaluation Measures for Image Description\",\"url\":\"https://www.semanticscholar.org/paper/52f86811b57034ba5c0478b37cab101d9a84024a\",\"venue\":\"ACL\",\"year\":2014},{\"arxivId\":\"1406.5679\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"153196308\",\"name\":\"F. Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f1b111f0bb703b0bd97aba505728a9b0d9b2a54\",\"title\":\"Deep Fragment Embeddings for Bidirectional Image Sentence Mapping\",\"url\":\"https://www.semanticscholar.org/paper/7f1b111f0bb703b0bd97aba505728a9b0d9b2a54\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82672164\",\"name\":\"M. Bosch\"},{\"authorId\":\"144211528\",\"name\":\"Ashwin Viswanathan\"}],\"doi\":\"10.1007/978-3-642-15181-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"723197b5c1a5ebb5cc748999a1d4bc6f6ec1cde3\",\"title\":\"ImageCLEF, Experimental Evaluation in Visual Information Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/723197b5c1a5ebb5cc748999a1d4bc6f6ec1cde3\",\"venue\":\"\",\"year\":2010},{\"arxivId\":\"1105.1033\",\"authors\":[{\"authorId\":\"1740329\",\"name\":\"O. Tamuz\"},{\"authorId\":\"1681442\",\"name\":\"Ce Liu\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"1768909\",\"name\":\"O. Shamir\"},{\"authorId\":\"2186481\",\"name\":\"A. Kalai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e4fcbb53b4ac42821801c3536603ae3777c37a1\",\"title\":\"Adaptively Learning the Crowd Kernel\",\"url\":\"https://www.semanticscholar.org/paper/2e4fcbb53b4ac42821801c3536603ae3777c37a1\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1888731\",\"name\":\"M. Hejrati\"},{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/978-3-642-15561-1_2\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"title\":\"Every Picture Tells a Story: Generating Sentences from Images\",\"url\":\"https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3115414\",\"name\":\"A. Nenkova\"},{\"authorId\":\"1703046\",\"name\":\"Rebecca J. Passonneau\"}],\"doi\":\"10.7916/D80R9XVD\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70cb232a6e391bfa49b0441a9956820c52ec32f2\",\"title\":\"Evaluating Content Selection in Summarization: The Pyramid Method\",\"url\":\"https://www.semanticscholar.org/paper/70cb232a6e391bfa49b0441a9956820c52ec32f2\",\"venue\":\"HLT-NAACL\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/CVPR.2011.5995631\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12fe91ab616b797e22543ae6c2afa7866dbc9a49\",\"title\":\"Action recognition from a distributed representation of pose and appearance\",\"url\":\"https://www.semanticscholar.org/paper/12fe91ab616b797e22543ae6c2afa7866dbc9a49\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1682965\",\"name\":\"Xufeng Han\"},{\"authorId\":\"144091869\",\"name\":\"Jeff Hayes\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f528b8c80f5420cea76d96520ec81bd9f52caa41\",\"title\":\"Midge: Generating Descriptions of Images\",\"url\":\"https://www.semanticscholar.org/paper/f528b8c80f5420cea76d96520ec81bd9f52caa41\",\"venue\":\"INLG\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fbdbe747c6aa8b35b981d21e475ff1506a1bae66\",\"title\":\"Composing Simple Image Descriptions using Web-scale N-grams\",\"url\":\"https://www.semanticscholar.org/paper/fbdbe747c6aa8b35b981d21e475ff1506a1bae66\",\"venue\":\"CoNLL\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144080148\",\"name\":\"A. Sorokin\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1109/CVPRW.2008.4562953\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d513d8b6470c7cbbeca8563505de8711325a3179\",\"title\":\"Utility data annotation with Amazon Mechanical Turk\",\"url\":\"https://www.semanticscholar.org/paper/d513d8b6470c7cbbeca8563505de8711325a3179\",\"venue\":\"2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"},{\"authorId\":\"34176020\",\"name\":\"Jesse Dodge\"},{\"authorId\":\"46479604\",\"name\":\"Amit Goyal\"},{\"authorId\":\"1682965\",\"name\":\"Xufeng Han\"},{\"authorId\":\"40614240\",\"name\":\"A. Mensch\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"2212748\",\"name\":\"Aneesh Sood\"},{\"authorId\":\"1714215\",\"name\":\"K. Stratos\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"}],\"doi\":\"10.1109/CVPR.2012.6248100\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5ce04063ecf83a6584813e1a09fb3d81642e5790\",\"title\":\"Understanding and predicting importance in images\",\"url\":\"https://www.semanticscholar.org/paper/5ce04063ecf83a6584813e1a09fb3d81642e5790\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2011.5995711\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ec97294c1e5974c6b827f8fda67f2e96cf1d8339\",\"title\":\"Recognition using visual phrases\",\"url\":\"https://www.semanticscholar.org/paper/ec97294c1e5974c6b827f8fda67f2e96cf1d8339\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144430625\",\"name\":\"S. Robertson\"}],\"doi\":\"10.1108/00220410410560582\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8397ab573dd6c97a39ff4feb9c2d9b3c1e16c705\",\"title\":\"Understanding inverse document frequency: on theoretical arguments for IDF\",\"url\":\"https://www.semanticscholar.org/paper/8397ab573dd6c97a39ff4feb9c2d9b3c1e16c705\",\"venue\":\"J. Documentation\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/ICCV.2011.6126281\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"23e568fcf0192e4ff5e6bed7507ee5b9e6c43598\",\"title\":\"Relative attributes\",\"url\":\"https://www.semanticscholar.org/paper/23e568fcf0192e4ff5e6bed7507ee5b9e6c43598\",\"venue\":\"2011 International Conference on Computer Vision\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"1909300\",\"name\":\"Lucy Vanderwende\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":\"10.3115/v1/S14-1015\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"88dfc5148bdd92669bbbd5c64d1ac324207bf831\",\"title\":\"See No Evil, Say No Evil: Description Generation from Densely Labeled Images\",\"url\":\"https://www.semanticscholar.org/paper/88dfc5148bdd92669bbbd5c64d1ac324207bf831\",\"venue\":\"*SEM@COLING\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Everingham\"},{\"authorId\":null,\"name\":\"L. Van Gool\"},{\"authorId\":null,\"name\":\"C.K.I. Williams\"},{\"authorId\":null,\"name\":\"J. Winn\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and A\",\"url\":\"\",\"venue\":\"Zisserman. The PASCAL Visual Object Classes Challenge 2010 \",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/CVPR.2011.5995466\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"169b847e69c35cfd475eb4dcc561a24de11762ca\",\"title\":\"Baby talk: Understanding and generating simple image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/169b847e69c35cfd475eb4dcc561a24de11762ca\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144891282\",\"name\":\"D. Martin\"},{\"authorId\":\"143800213\",\"name\":\"Charless C. Fowlkes\"},{\"authorId\":\"1782479\",\"name\":\"D. Tal\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/ICCV.2001.937655\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a1ed876196ec9733acb1daa6d65e35ff0414291\",\"title\":\"A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics\",\"url\":\"https://www.semanticscholar.org/paper/9a1ed876196ec9733acb1daa6d65e35ff0414291\",\"venue\":\"Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. Vedantam\"},{\"authorId\":null,\"name\":\"C. L. Zitnick\"},{\"authorId\":null,\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Cider: Consensus-based image description evaluation\",\"url\":\"\",\"venue\":\"CoRR, abs/1411.5726,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1644066790\",\"name\":\"EveringhamMark\"},{\"authorId\":\"1644080676\",\"name\":\"M. EslamiS.\"},{\"authorId\":\"1644017278\",\"name\":\"GoolLuc\"},{\"authorId\":\"1644074763\",\"name\":\"K. WilliamsChristopher\"},{\"authorId\":\"1644080846\",\"name\":\"WinnJohn\"},{\"authorId\":\"1643953626\",\"name\":\"ZissermanAndrew\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"535995b77ae23f1a075d2e38be5b670e9c339290\",\"title\":\"The Pascal Visual Object Classes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/535995b77ae23f1a075d2e38be5b670e9c339290\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1787591\",\"name\":\"Christoph H. Lampert\"},{\"authorId\":\"1748758\",\"name\":\"H. Nickisch\"},{\"authorId\":\"1734990\",\"name\":\"S. Harmeling\"}],\"doi\":\"10.1109/CVPRW.2009.5206594\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0566bf06a0368b518b8b474166f7b1dfef3f9283\",\"title\":\"Learning to detect unseen object classes by between-class attribute transfer\",\"url\":\"https://www.semanticscholar.org/paper/0566bf06a0368b518b8b474166f7b1dfef3f9283\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2015.7298932\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"title\":\"Deep visual-semantic alignments for generating image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122862590\",\"name\":\"R. Bogacz\"},{\"authorId\":\"145318505\",\"name\":\"E. Brown\"},{\"authorId\":\"10213765\",\"name\":\"J. Moehlis\"},{\"authorId\":\"40640032\",\"name\":\"P. Holmes\"},{\"authorId\":\"144872055\",\"name\":\"J. Cohen\"}],\"doi\":\"10.1037/0033-295X.113.4.700\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bbdd99468b296c3be48601628ea164411a8b8c75\",\"title\":\"The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks.\",\"url\":\"https://www.semanticscholar.org/paper/bbdd99468b296c3be48601628ea164411a8b8c75\",\"venue\":\"Psychological review\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2013.387\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"051830b0ea58d1568f19ec3297e301d9789c9a76\",\"title\":\"Bringing Semantics into Focus Using Visual Abstraction\",\"url\":\"https://www.semanticscholar.org/paper/051830b0ea58d1568f19ec3297e301d9789c9a76\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":\"1411.5654\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f4af49a1ead3c81cc5d023878cb67c5646dd8a04\",\"title\":\"Learning a Recurrent Visual Representation for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/f4af49a1ead3c81cc5d023878cb67c5646dd8a04\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144679302\",\"name\":\"P. Dokania\"},{\"authorId\":\"2443650\",\"name\":\"A. Behl\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"},{\"authorId\":\"3717791\",\"name\":\"M. Kumar\"}],\"doi\":\"10.1007/978-3-319-10593-2_40\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d6142e95a9bc369332824af9692cb3d0a854bc\",\"title\":\"Learning to Rank Using High-Order Information\",\"url\":\"https://www.semanticscholar.org/paper/69d6142e95a9bc369332824af9692cb3d0a854bc\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49049934\",\"name\":\"J. Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ec48ac86456cea3d6d6172ca81ef68e98b21a61\",\"title\":\"The PASCAL Visual Object Classes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/0ec48ac86456cea3d6d6172ca81ef68e98b21a61\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1756655\",\"name\":\"C. L. Teo\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"76a1dca3a9c2b0229c1b12c95752dcf40dc95a11\",\"title\":\"Corpus-Guided Sentence Generation of Natural Images\",\"url\":\"https://www.semanticscholar.org/paper/76a1dca3a9c2b0229c1b12c95752dcf40dc95a11\",\"venue\":\"EMNLP\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1007/978-3-540-88682-2_3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e523721feebeaee18e487607b7d0920ac6cd3b4\",\"title\":\"Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers\",\"url\":\"https://www.semanticscholar.org/paper/8e523721feebeaee18e487607b7d0920ac6cd3b4\",\"venue\":\"ECCV\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9973734\",\"name\":\"P. Hawley\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"34f8cdb2530376b2651a41a1367ef0c6ffd37d19\",\"title\":\"See no evil.\",\"url\":\"https://www.semanticscholar.org/paper/34f8cdb2530376b2651a41a1367ef0c6ffd37d19\",\"venue\":\"Bulletin of the American College of Surgeons\",\"year\":1953},{\"arxivId\":\"1310.4546\",\"authors\":[{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"38644044\",\"name\":\"Kai Chen\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"87f40e6f3022adbc1f1905e3e506abad05a9964f\",\"title\":\"Distributed Representations of Words and Phrases and their Compositionality\",\"url\":\"https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3288381\",\"name\":\"A. Gupta\"},{\"authorId\":\"2169614\",\"name\":\"Yashaswi Verma\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0ba87571341beaf6a5c9a30e049be7b1fc9a4c60\",\"title\":\"Choosing Linguistics over Vision to Describe Images\",\"url\":\"https://www.semanticscholar.org/paper/0ba87571341beaf6a5c9a30e049be7b1fc9a4c60\",\"venue\":\"AAAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709053\",\"name\":\"D. Scharstein\"},{\"authorId\":\"1717841\",\"name\":\"R. Szeliski\"},{\"authorId\":\"2984143\",\"name\":\"R. Zabih\"}],\"doi\":\"10.1109/SMBV.2001.988771\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2f78c2b2b325d72f359d4c797c9aab6a8e60942\",\"title\":\"A taxonomy and evaluation of dense two-frame stereo correspondence algorithms\",\"url\":\"https://www.semanticscholar.org/paper/d2f78c2b2b325d72f359d4c797c9aab6a8e60942\",\"venue\":\"Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"47680287\",\"name\":\"W. Dong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2009.5206848\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b47265245e8db53a553049dcb27ed3e495fd625\",\"title\":\"ImageNet: A large-scale hierarchical image database\",\"url\":\"https://www.semanticscholar.org/paper/1b47265245e8db53a553049dcb27ed3e495fd625\",\"venue\":\"CVPR 2009\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3141511\",\"name\":\"S. Banerjee\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"title\":\"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments\",\"url\":\"https://www.semanticscholar.org/paper/0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"venue\":\"IEEvaluation@ACL\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015}],\"title\":\"CIDEr: Consensus-based image description evaluation\",\"topics\":[{\"topic\":\"Consensus (computer science)\",\"topicId\":\"16541\",\"url\":\"https://www.semanticscholar.org/topic/16541\"},{\"topic\":\"Natural language processing\",\"topicId\":\"1914\",\"url\":\"https://www.semanticscholar.org/topic/1914\"},{\"topic\":\"Computer vision\",\"topicId\":\"5332\",\"url\":\"https://www.semanticscholar.org/topic/5332\"},{\"topic\":\"Object detection\",\"topicId\":\"14349\",\"url\":\"https://www.semanticscholar.org/topic/14349\"},{\"topic\":\"Programming paradigm\",\"topicId\":\"29522\",\"url\":\"https://www.semanticscholar.org/topic/29522\"},{\"topic\":\"Modality (human\\u2013computer interaction)\",\"topicId\":\"462\",\"url\":\"https://www.semanticscholar.org/topic/462\"},{\"topic\":\"Triplet state\",\"topicId\":\"60687\",\"url\":\"https://www.semanticscholar.org/topic/60687\"},{\"topic\":\"Michel H\\u00e9non\",\"topicId\":\"3533915\",\"url\":\"https://www.semanticscholar.org/topic/3533915\"},{\"topic\":\"Server (computing)\",\"topicId\":\"6042\",\"url\":\"https://www.semanticscholar.org/topic/6042\"},{\"topic\":\"Statistical classification\",\"topicId\":\"715\",\"url\":\"https://www.semanticscholar.org/topic/715\"},{\"topic\":\"Quirks mode\",\"topicId\":\"3575908\",\"url\":\"https://www.semanticscholar.org/topic/3575908\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Mitchell Corporation\",\"topicId\":\"3669865\",\"url\":\"https://www.semanticscholar.org/topic/3669865\"}],\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015}\n"