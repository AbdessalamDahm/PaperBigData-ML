"{\"abstract\":\"The goal of this work is to recognise phrases and sentences being spoken by a talking face, with or without the audio. Unlike previous works that have focussed on recognising a limited number of words or phrases, we tackle lip reading as an open-world problem &#x2013; unconstrained natural language sentences, and in the wild videos. Our key contributions are: (1) a Watch, Listen, Attend and Spell (WLAS) network that learns to transcribe videos of mouth motion to characters, (2) a curriculum learning strategy to accelerate training and to reduce overfitting, (3) a Lip Reading Sentences (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television. The WLAS model trained on the LRS dataset surpasses the performance of all previous work on standard lip reading benchmark datasets, often by a significant margin. This lip reading performance beats a professional lip reader on videos from BBC television, and we also demonstrate that if audio is available, then visual information helps to improve speech recognition performance.\",\"arxivId\":\"1611.05358\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\",\"url\":\"https://www.semanticscholar.org/author/2863890\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\",\"url\":\"https://www.semanticscholar.org/author/33666044\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\",\"url\":\"https://www.semanticscholar.org/author/1689108\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\",\"url\":\"https://www.semanticscholar.org/author/1688869\"}],\"citationVelocity\":97,\"citations\":[{\"arxivId\":\"1708.01565\",\"authors\":[{\"authorId\":\"143910530\",\"name\":\"Michael Wand\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.21437/INTERSPEECH.2017-421\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"bd0d116ba34a29cbd50c636445a93ce78c059f99\",\"title\":\"Improving Speaker-Independent Lipreading with Domain-Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/bd0d116ba34a29cbd50c636445a93ce78c059f99\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":\"2005.08209\",\"authors\":[{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\"},{\"authorId\":\"41052499\",\"name\":\"R. Mukhopadhyay\"},{\"authorId\":\"1744135\",\"name\":\"Vinay Namboodiri\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1109/cvpr42600.2020.01381\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ba684a9966995e5a8c6efef46aeb57bd387ff51f\",\"title\":\"Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/ba684a9966995e5a8c6efef46aeb57bd387ff51f\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1907.01367\",\"authors\":[{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"34604467\",\"name\":\"Rohit Jain\"},{\"authorId\":\"66287688\",\"name\":\"Khwaja Mohd. Salik\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"144221742\",\"name\":\"Yifang Yin\"},{\"authorId\":\"144809527\",\"name\":\"Roger Zimmermann\"}],\"doi\":\"10.1609/aaai.v33i01.33012588\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"174812d636fdb17c1cbe24b8d9888d340a5dbf56\",\"title\":\"Lipper: Synthesizing Thy Speech using Multi-View Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/174812d636fdb17c1cbe24b8d9888d340a5dbf56\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1910.10997\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"50487517\",\"name\":\"Xudong Xu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/ICCV.2019.00037\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82feef7ae3b3d5ea16ce8bfdf9a01d9aadb4b7be\",\"title\":\"Vision-Infused Deep Audio Inpainting\",\"url\":\"https://www.semanticscholar.org/paper/82feef7ae3b3d5ea16ce8bfdf9a01d9aadb4b7be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2679950\",\"name\":\"Tanay Sharma\"},{\"authorId\":\"41195227\",\"name\":\"Rohith Aralikatti\"},{\"authorId\":\"150078626\",\"name\":\"Dilip Kumar Margam\"},{\"authorId\":\"8367233\",\"name\":\"Abhinav Thanda\"},{\"authorId\":\"150305913\",\"name\":\"Sharad Roy\"},{\"authorId\":\"1396720029\",\"name\":\"Pujitha Appan Kandala\"},{\"authorId\":\"3210146\",\"name\":\"S. Venkatesan\"}],\"doi\":\"10.21437/interspeech.2019-3253\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58fce6a18b888d96a41354bfc55b75f7f312ae21\",\"title\":\"Real Time Online Visual End Point Detection Using Unidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/58fce6a18b888d96a41354bfc55b75f7f312ae21\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92454011\",\"name\":\"B. Favre\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"692907903dfbf19922df05e033c5130d09bffa4b\",\"title\":\"Contextual language understanding Thoughts on Machine Learning in Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/692907903dfbf19922df05e033c5130d09bffa4b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2003.07032\",\"authors\":[{\"authorId\":\"1500656676\",\"name\":\"Rongzhi Gu\"},{\"authorId\":\"2213494\",\"name\":\"S. Zhang\"},{\"authorId\":\"121983569\",\"name\":\"Yanchen Xu\"},{\"authorId\":\"92896134\",\"name\":\"Lianwu Chen\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"}],\"doi\":\"10.1109/JSTSP.2020.2980956\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9ca78f3e4a3dc01f9d247bb9592cdc4623d1ee8e\",\"title\":\"Multi-Modal Multi-Channel Target Speech Separation\",\"url\":\"https://www.semanticscholar.org/paper/9ca78f3e4a3dc01f9d247bb9592cdc4623d1ee8e\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":\"1710.07168\",\"authors\":[{\"authorId\":\"26207251\",\"name\":\"M. Zimmermann\"},{\"authorId\":\"1399030182\",\"name\":\"Mostafa Mehdipour-Ghazi\"},{\"authorId\":\"3025777\",\"name\":\"H. K. Ekenel\"},{\"authorId\":\"7195318\",\"name\":\"J-P. Thiran\"}],\"doi\":\"10.21437/AVSP.2017-10\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"2d8e88fec1ffd18675f241f4df55a8846fe237f7\",\"title\":\"Combining Multiple Views for Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2d8e88fec1ffd18675f241f4df55a8846fe237f7\",\"venue\":\"AVSP\",\"year\":2017},{\"arxivId\":\"2005.09297\",\"authors\":[{\"authorId\":\"30606918\",\"name\":\"George Sterpu\"},{\"authorId\":\"1814175\",\"name\":\"Christian Saam\"},{\"authorId\":\"34530970\",\"name\":\"N. Harte\"}],\"doi\":\"10.21437/interspeech.2020-2480\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28c9ae026ab49dc63ad108800615da77e38e4089\",\"title\":\"Should we hard-code the recurrence concept or learn it instead ? Exploring the Transformer architecture for Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/28c9ae026ab49dc63ad108800615da77e38e4089\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2009.00299\",\"authors\":[{\"authorId\":\"2808158\",\"name\":\"Necati Cihan Camg\\u00f6z\"},{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"2417546\",\"name\":\"S. Hadfield\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4fe56f5f7a7196571e81ec243d86df376269695c\",\"title\":\"Multi-channel Transformers for Multi-articulatory Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/4fe56f5f7a7196571e81ec243d86df376269695c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"145420448\",\"name\":\"E. Morais\"}],\"doi\":\"10.1109/CVPRW.2018.00289\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74349b71eeade9e11d43d4e394864fe65da630ca\",\"title\":\"Improving Viseme Recognition Using GAN-Based Frontal View Mapping\",\"url\":\"https://www.semanticscholar.org/paper/74349b71eeade9e11d43d4e394864fe65da630ca\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":\"2011.00030\",\"authors\":[{\"authorId\":\"73580712\",\"name\":\"S. Wang\"},{\"authorId\":\"145022667\",\"name\":\"A. Mesaros\"},{\"authorId\":\"2373836\",\"name\":\"Toni Heittola\"},{\"authorId\":\"50195877\",\"name\":\"T. Virtanen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a9ebd69ccbf6e493abdac6f047b4bc8d3be88412\",\"title\":\"A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis\",\"url\":\"https://www.semanticscholar.org/paper/a9ebd69ccbf6e493abdac6f047b4bc8d3be88412\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.05709\",\"authors\":[{\"authorId\":\"31227090\",\"name\":\"Jingyun Xiao\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"8795388\",\"name\":\"Yuan-Hang Zhang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"46772547\",\"name\":\"Xilin Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4129a667527cf7227c55fdf22743de6e2dedc490\",\"title\":\"Deformation Flow Based Two-Stream Network for Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/4129a667527cf7227c55fdf22743de6e2dedc490\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144275036\",\"name\":\"E. Owusu\"}],\"doi\":\"10.1184/R1/11337008.V1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a345458c504f8101a4c5aca4d2aa8930ed8fcbcf\",\"title\":\"Fog Mediated Security in the Internet of Things\",\"url\":\"https://www.semanticscholar.org/paper/a345458c504f8101a4c5aca4d2aa8930ed8fcbcf\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391213316\",\"name\":\"Chenda Li\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"}],\"doi\":\"10.21437/interspeech.2020-2028\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d9afbe7fd2bec4308454765ce84477046c79eeed\",\"title\":\"Listen, Watch and Understand at the Cocktail Party: Audio-Visual-Contextual Speech Separation\",\"url\":\"https://www.semanticscholar.org/paper/d9afbe7fd2bec4308454765ce84477046c79eeed\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1911.11502\",\"authors\":[{\"authorId\":\"32844760\",\"name\":\"Y. Zhao\"},{\"authorId\":\"152287816\",\"name\":\"Rui Xu\"},{\"authorId\":\"2714904\",\"name\":\"X. Wang\"},{\"authorId\":\"152661064\",\"name\":\"Peng Hou\"},{\"authorId\":\"144465837\",\"name\":\"Haihong Tang\"},{\"authorId\":\"1727111\",\"name\":\"Mingli Song\"}],\"doi\":\"10.1609/AAAI.V34I04.6174\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a19274a50938d70755269ff6aeefe818e44469ad\",\"title\":\"Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers\",\"url\":\"https://www.semanticscholar.org/paper/a19274a50938d70755269ff6aeefe818e44469ad\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1906.12170\",\"authors\":[{\"authorId\":\"150078626\",\"name\":\"Dilip Kumar Margam\"},{\"authorId\":\"41195227\",\"name\":\"Rohith Aralikatti\"},{\"authorId\":\"2679950\",\"name\":\"Tanay Sharma\"},{\"authorId\":\"8367233\",\"name\":\"Abhinav Thanda\"},{\"authorId\":\"1417382302\",\"name\":\"K. PujithaA.\"},{\"authorId\":\"150305913\",\"name\":\"Sharad Roy\"},{\"authorId\":\"3210146\",\"name\":\"S. Venkatesan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eac5a5fa199d00f02f87e88f9b8c38c8fb2872ec\",\"title\":\"LipReading with 3D-2D-CNN BLSTM-HMM and word-CTC models\",\"url\":\"https://www.semanticscholar.org/paper/eac5a5fa199d00f02f87e88f9b8c38c8fb2872ec\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39089342\",\"name\":\"Miroslav Hlav\\u00e1c\"},{\"authorId\":\"2601929\",\"name\":\"I. Gruber\"},{\"authorId\":\"2851907\",\"name\":\"M. Zelezn\\u00fd\"},{\"authorId\":\"145191867\",\"name\":\"Alexey Karpov\"}],\"doi\":\"10.1007/978-3-030-60276-5_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"716af324a2a36cfc464d7fd5b59114e6ce5ceadf\",\"title\":\"Lipreading with LipsID\",\"url\":\"https://www.semanticscholar.org/paper/716af324a2a36cfc464d7fd5b59114e6ce5ceadf\",\"venue\":\"SPECOM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2808158\",\"name\":\"Necati Cihan Camg\\u00f6z\"},{\"authorId\":\"2417546\",\"name\":\"S. Hadfield\"},{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":\"10.1109/CVPR.2018.00812\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"644602c65a5d8f30e62be027eb7b47f7c335191a\",\"title\":\"Neural Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/644602c65a5d8f30e62be027eb7b47f7c335191a\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2002.05314\",\"authors\":[{\"authorId\":\"144506371\",\"name\":\"Y. Ding\"},{\"authorId\":\"121983569\",\"name\":\"Yanchen Xu\"},{\"authorId\":\"2213494\",\"name\":\"S. Zhang\"},{\"authorId\":\"1492006611\",\"name\":\"Yahuan Cong\"},{\"authorId\":\"1390771606\",\"name\":\"Liqiang Wang\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054376\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"85fa01b32eef73ff7af8477af2a172fe6ac27fdf\",\"title\":\"Self-Supervised Learning for Audio-Visual Speaker Diarization\",\"url\":\"https://www.semanticscholar.org/paper/85fa01b32eef73ff7af8477af2a172fe6ac27fdf\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1016/j.cviu.2018.02.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d8a20e330efb3c4285a3698c32551552c46ddfcf\",\"title\":\"Learning to lip read words by watching videos\",\"url\":\"https://www.semanticscholar.org/paper/d8a20e330efb3c4285a3698c32551552c46ddfcf\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49611735\",\"name\":\"Feng Tang\"},{\"authorId\":\"9265616\",\"name\":\"Xinsha Fu\"},{\"authorId\":\"1789173266\",\"name\":\"Mingmao Cai\"},{\"authorId\":\"51373381\",\"name\":\"Yue Lu\"},{\"authorId\":\"152578292\",\"name\":\"Yanjie Zeng\"},{\"authorId\":\"31688927\",\"name\":\"Shiyu Zhong\"},{\"authorId\":\"31440672\",\"name\":\"Y. Huang\"},{\"authorId\":\"1785355471\",\"name\":\"Chongzhen Lu\"}],\"doi\":\"10.1109/ACCESS.2020.3004178\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b1fce214f94d723cf2c93a5f54da21b65b97e68\",\"title\":\"Multilevel Traffic State Detection in Traffic Surveillance System Using a Deep Residual Squeeze-and-Excitation Network and an Improved Triplet Loss\",\"url\":\"https://www.semanticscholar.org/paper/5b1fce214f94d723cf2c93a5f54da21b65b97e68\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145726221\",\"name\":\"S. Bhaskar\"},{\"authorId\":\"1500651922\",\"name\":\"Thasleema Tm\"}],\"doi\":\"10.1109/ICCIKE47802.2019.9004287\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"1c3b84d9981c0e36b200cf91152d0cdf46fccf68\",\"title\":\"Scope for Deep Learning: A Study in Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1c3b84d9981c0e36b200cf91152d0cdf46fccf68\",\"venue\":\"2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE)\",\"year\":2019},{\"arxivId\":\"2006.08599\",\"authors\":[{\"authorId\":\"66016321\",\"name\":\"Dhruva Sahrawat\"},{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"1452341751\",\"name\":\"Shashwat Aggarwal\"},{\"authorId\":\"144221742\",\"name\":\"Yifang Yin\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ab8502508a2eee7025c632b5373dcc48ef042be\",\"title\":\"\\\"Notic My Speech\\\" - Blending Speech Patterns With Multimedia\",\"url\":\"https://www.semanticscholar.org/paper/7ab8502508a2eee7025c632b5373dcc48ef042be\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1396720029\",\"name\":\"Pujitha Appan Kandala\"},{\"authorId\":\"8367233\",\"name\":\"Abhinav Thanda\"},{\"authorId\":\"150078626\",\"name\":\"Dilip Kumar Margam\"},{\"authorId\":\"41195227\",\"name\":\"Rohith Aralikatti\"},{\"authorId\":\"2679950\",\"name\":\"Tanay Sharma\"},{\"authorId\":\"150305913\",\"name\":\"Sharad Roy\"},{\"authorId\":\"3210146\",\"name\":\"S. Venkatesan\"}],\"doi\":\"10.21437/interspeech.2019-3237\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"cf9f309610abad174685f5bc8f80f14ab79ce689\",\"title\":\"Speaker Adaptation for Lip-Reading Using Visual Identity Vectors\",\"url\":\"https://www.semanticscholar.org/paper/cf9f309610abad174685f5bc8f80f14ab79ce689\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1704.06913\",\"authors\":[{\"authorId\":\"1706980\",\"name\":\"Rahma Chaabouni\"},{\"authorId\":\"46967555\",\"name\":\"Ewan Dunbar\"},{\"authorId\":\"3404556\",\"name\":\"Neil Zeghidour\"},{\"authorId\":\"2202008\",\"name\":\"Emmanuel Dupoux\"}],\"doi\":\"10.21437/INTERSPEECH.2017-1689\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83715a45d2680a55727034f682a3fc998a49c302\",\"title\":\"Learning Weakly Supervised Multimodal Phoneme Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/83715a45d2680a55727034f682a3fc998a49c302\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3433735\",\"name\":\"Abderrahim Mesbah\"},{\"authorId\":\"3289153\",\"name\":\"Aissam Berrahou\"},{\"authorId\":\"51120211\",\"name\":\"Hicham Hammouchi\"},{\"authorId\":\"9414422\",\"name\":\"H. Berbia\"},{\"authorId\":\"3261861\",\"name\":\"H. Qjidaa\"},{\"authorId\":\"145633541\",\"name\":\"M. Daoudi\"}],\"doi\":\"10.1016/J.IMAVIS.2019.04.010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d2081403845cb0127cb1b5c4cbfe174834145d98\",\"title\":\"Lip reading with Hahn Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/d2081403845cb0127cb1b5c4cbfe174834145d98\",\"venue\":\"Image Vis. Comput.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145764891\",\"name\":\"Bo Xu\"},{\"authorId\":\"2044516\",\"name\":\"J. Wang\"},{\"authorId\":\"102517285\",\"name\":\"Cheng Lu\"},{\"authorId\":\"49813886\",\"name\":\"Yandong Guo\"}],\"doi\":\"10.1109/WACV45572.2020.9093314\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"58fd3c2efd2c9079c836ad2c9de642315115d779\",\"title\":\"Watch to Listen Clearly: Visual Speech Enhancement Driven Multi-modality Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/58fd3c2efd2c9079c836ad2c9de642315115d779\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2007.14509\",\"authors\":[{\"authorId\":\"4056993\",\"name\":\"J. P. Robinson\"},{\"authorId\":\"29693602\",\"name\":\"Z. Khan\"},{\"authorId\":\"49546023\",\"name\":\"Yu Yin\"},{\"authorId\":\"14580705\",\"name\":\"M. Shao\"},{\"authorId\":\"144015161\",\"name\":\"Y. Fu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c7d70e168bc43e3df1d625068b60a10779f5f7fe\",\"title\":\"Families In Wild Multimedia (FIW-MM): A Multi-Modal Database for Recognizing Kinship\",\"url\":\"https://www.semanticscholar.org/paper/c7d70e168bc43e3df1d625068b60a10779f5f7fe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1709.00443\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"2563750\",\"name\":\"Yujiang Wang\"},{\"authorId\":\"8798628\",\"name\":\"Z. Li\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.5244/C.31.161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f46a201a0462b455fc3d74c496bd55536be57f9\",\"title\":\"End-to-End Multi-View Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/9f46a201a0462b455fc3d74c496bd55536be57f9\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":\"1709.01507\",\"authors\":[{\"authorId\":\"145815850\",\"name\":\"Jie Hu\"},{\"authorId\":\"152148573\",\"name\":\"L. Shen\"},{\"authorId\":\"152274574\",\"name\":\"Gang Sun\"}],\"doi\":\"10.1109/CVPR.2018.00745\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fb37561499573109fc2cebb6a7b08f44917267dd\",\"title\":\"Squeeze-and-Excitation Networks\",\"url\":\"https://www.semanticscholar.org/paper/fb37561499573109fc2cebb6a7b08f44917267dd\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2011.07557\",\"authors\":[{\"authorId\":\"80007827\",\"name\":\"Dalu Feng\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"51069511\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9bd00f2545b89e538a524bf2330ba32d39d9bfcd\",\"title\":\"Learn an Effective Lip Reading Model without Pains\",\"url\":\"https://www.semanticscholar.org/paper/9bd00f2545b89e538a524bf2330ba32d39d9bfcd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.11832\",\"authors\":[{\"authorId\":\"3450321\",\"name\":\"Binghui Chen\"},{\"authorId\":\"1774956\",\"name\":\"W. Deng\"}],\"doi\":\"10.1109/CVPR.2019.00286\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd58ca00535abbeab012c27128c459ad875db95f\",\"title\":\"Hybrid-Attention Based Decoupled Metric Learning for Zero-Shot Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/fd58ca00535abbeab012c27128c459ad875db95f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8726117\",\"name\":\"Jen-Cheng Hou\"},{\"authorId\":\"2426246\",\"name\":\"S. Wang\"},{\"authorId\":\"145274549\",\"name\":\"Ying-Hui Lai\"},{\"authorId\":\"145403933\",\"name\":\"Y. Tsao\"},{\"authorId\":\"144600099\",\"name\":\"Hsiu-Wen Chang\"},{\"authorId\":\"1710199\",\"name\":\"H. Wang\"}],\"doi\":\"10.1109/TETCI.2017.2784878\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ddf1461979a5e39321b931cfe5b470999b5e4aab\",\"title\":\"Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/ddf1461979a5e39321b931cfe5b470999b5e4aab\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3234063\",\"name\":\"Jiankang Deng\"},{\"authorId\":\"2931390\",\"name\":\"A. Roussos\"},{\"authorId\":\"34586458\",\"name\":\"Grigorios G. Chrysos\"},{\"authorId\":\"31243357\",\"name\":\"Evangelos Ververas\"},{\"authorId\":\"1754270\",\"name\":\"I. Kotsia\"},{\"authorId\":\"46904799\",\"name\":\"Jie Shen\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"}],\"doi\":\"10.1007/s11263-018-1134-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e156ece0f070fae28254d8589dc9f8a5a4ac2a08\",\"title\":\"The Menpo Benchmark for Multi-pose 2D and 3D Facial Landmark Localisation and Tracking\",\"url\":\"https://www.semanticscholar.org/paper/e156ece0f070fae28254d8589dc9f8a5a4ac2a08\",\"venue\":\"International Journal of Computer Vision\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1411061932\",\"name\":\"Isaac Griswold-Steiner\"},{\"authorId\":\"2036853461\",\"name\":\"Zachary LeFevre\"},{\"authorId\":\"1931454\",\"name\":\"Abdul Serwadda\"}],\"doi\":\"10.1016/j.cose.2020.102110\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8a0b71166d806b980e8123931950982e438a964\",\"title\":\"Smartphone Speech Privacy Concerns from Side-Channel Attacks on Facial Biomechanics\",\"url\":\"https://www.semanticscholar.org/paper/f8a0b71166d806b980e8123931950982e438a964\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39119171\",\"name\":\"Anand Handa\"},{\"authorId\":\"1642486203\",\"name\":\"Rashi Agarwal\"},{\"authorId\":\"1642556905\",\"name\":\"Narendra Kohli\"}],\"doi\":\"10.1007/s11042-020-08837-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7be18c0d11592f726f673fd2cef3654e0162cd4b\",\"title\":\"A multimodel keyword spotting system based on lip movement and speech features\",\"url\":\"https://www.semanticscholar.org/paper/7be18c0d11592f726f673fd2cef3654e0162cd4b\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"2005.09812\",\"authors\":[{\"authorId\":\"107979102\",\"name\":\"Juan Leon Alcazar\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"2712573\",\"name\":\"Long Mai\"},{\"authorId\":\"2942259\",\"name\":\"Federico Perazzi\"},{\"authorId\":\"1576788264\",\"name\":\"Joon-Young Lee\"},{\"authorId\":\"9739979\",\"name\":\"P. Arbel\\u00e1ez\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1109/CVPR42600.2020.01248\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34c8161a352bce60b64b6382b1ff7280433ad654\",\"title\":\"Active Speakers in Context\",\"url\":\"https://www.semanticscholar.org/paper/34c8161a352bce60b64b6382b1ff7280433ad654\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1708.00052\",\"authors\":[{\"authorId\":\"46906102\",\"name\":\"Chaim Baskin\"},{\"authorId\":\"38375537\",\"name\":\"N. Liss\"},{\"authorId\":\"23913513\",\"name\":\"Evgenii Zheltonozhskii\"},{\"authorId\":\"1397305052\",\"name\":\"Alex M. Bronstein\"},{\"authorId\":\"144188849\",\"name\":\"A. Mendelson\"}],\"doi\":\"10.1109/IPDPSW.2018.00032\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"304585f8ebc9c062915661ec29d0ff286e6390b0\",\"title\":\"Streaming Architecture for Large-Scale Quantized Neural Networks on an FPGA-Based Dataflow Platform\",\"url\":\"https://www.semanticscholar.org/paper/304585f8ebc9c062915661ec29d0ff286e6390b0\",\"venue\":\"2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)\",\"year\":2018},{\"arxivId\":\"1803.08842\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"145458657\",\"name\":\"Jing Shi\"},{\"authorId\":\"2868721\",\"name\":\"Bochen Li\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01216-8_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"title\":\"Audio-Visual Event Localization in Unconstrained Videos\",\"url\":\"https://www.semanticscholar.org/paper/a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"}],\"doi\":\"10.1145/3192714.3192824\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ac69ed7dd2d046c20140a5bbb8cbe41bdff7670\",\"title\":\"Multi-view Mouth Renderization for Assisting Lip-reading\",\"url\":\"https://www.semanticscholar.org/paper/8ac69ed7dd2d046c20140a5bbb8cbe41bdff7670\",\"venue\":\"W4A\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933398\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"7943876\",\"name\":\"Feipeng Cai\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aadcf6136b9eaa0c96358e4351a070258a3bcc40\",\"title\":\"C V ] 2 2 Fe b 20 18 END-TO-END AUDIOVISUAL SPEECH RECOGNITION\",\"url\":\"https://www.semanticscholar.org/paper/aadcf6136b9eaa0c96358e4351a070258a3bcc40\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27028148\",\"name\":\"Jiaxi Gu\"},{\"authorId\":\"50077884\",\"name\":\"Zhiwen Yu\"},{\"authorId\":\"3256409\",\"name\":\"Kele Shen\"}],\"doi\":\"10.1109/JIOT.2019.2946593\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13a7b3835651cde4da9dfa2b74f7cee4e681471d\",\"title\":\"Alohomora: Motion-Based Hotword Detection in Head-Mounted Displays\",\"url\":\"https://www.semanticscholar.org/paper/13a7b3835651cde4da9dfa2b74f7cee4e681471d\",\"venue\":\"IEEE Internet of Things Journal\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500655305\",\"name\":\"PV Sindhura\"},{\"authorId\":\"9316219\",\"name\":\"S. Preethi\"},{\"authorId\":\"51915009\",\"name\":\"Krupa B Niranjana\"}],\"doi\":\"10.1109/ICEECCOT43722.2018.9001505\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9287f712335cb7fa4fbf746ca65a8867a1258de0\",\"title\":\"Convolutional Neural Networks for Predicting Words: A Lip-Reading System\",\"url\":\"https://www.semanticscholar.org/paper/9287f712335cb7fa4fbf746ca65a8867a1258de0\",\"venue\":\"2018 International Conference on Electrical, Electronics, Communication, Computer, and Optimization Techniques (ICEECCOT)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35288970\",\"name\":\"Luojun Lin\"},{\"authorId\":\"2521432\",\"name\":\"Lingyu Liang\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"},{\"authorId\":\"48994044\",\"name\":\"Weijie Chen\"}],\"doi\":\"10.24963/ijcai.2019/119\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"77dcd5ba06dae9b7dbc50f716be46874378ab13f\",\"title\":\"Attribute-Aware Convolutional Neural Networks for Facial Beauty Prediction\",\"url\":\"https://www.semanticscholar.org/paper/77dcd5ba06dae9b7dbc50f716be46874378ab13f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48797555\",\"name\":\"Abhishek Jha\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1109/WACV.2018.00023\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f1ca3f211c6a60d805fc810f8736220be6f0cda9\",\"title\":\"Word Spotting in Silent Lip Videos\",\"url\":\"https://www.semanticscholar.org/paper/f1ca3f211c6a60d805fc810f8736220be6f0cda9\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"2012.14360\",\"authors\":[{\"authorId\":\"1652999406\",\"name\":\"Hang Chen\"},{\"authorId\":\"1515709515\",\"name\":\"Jun Du\"},{\"authorId\":\"1943030\",\"name\":\"Y. Hu\"},{\"authorId\":\"153634883\",\"name\":\"Li-Rong Dai\"},{\"authorId\":\"9391905\",\"name\":\"Chin-Hui Lee\"},{\"authorId\":\"2207938\",\"name\":\"B. Yin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"368b02bb9b7ed2420ce3ade6961d4ba84e03af67\",\"title\":\"Lip-reading with Hierarchical Pyramidal Convolution and Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/368b02bb9b7ed2420ce3ade6961d4ba84e03af67\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.10984\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-030-58621-8_44\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43971a0a2593f660427e016032b983b52f8dd8eb\",\"title\":\"Foley Music: Learning to Generate Music from Videos\",\"url\":\"https://www.semanticscholar.org/paper/43971a0a2593f660427e016032b983b52f8dd8eb\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a593bf44e46bd4cb426c658ed87f4fce43f13aa\",\"title\":\"Lip-syncing Videos In The Wild\",\"url\":\"https://www.semanticscholar.org/paper/8a593bf44e46bd4cb426c658ed87f4fce43f13aa\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2317420\",\"name\":\"T. Saitoh\"},{\"authorId\":\"40099567\",\"name\":\"Michiko Kubokawa\"}],\"doi\":\"10.1109/ICPR.2018.8545664\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f7200b12b779ebaf8761e64881606f28959821ba\",\"title\":\"SSSD: Speech Scene database by Smart Device for Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f7200b12b779ebaf8761e64881606f28959821ba\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2841988\",\"name\":\"Rongfeng Su\"},{\"authorId\":\"1737622\",\"name\":\"X. Liu\"},{\"authorId\":\"48170157\",\"name\":\"L. Wang\"},{\"authorId\":\"79629544\",\"name\":\"J. Yang\"}],\"doi\":\"10.1109/TASLP.2019.2950602\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1618d2a0660a9f9cb8a3ca539d651c2fd299e87b\",\"title\":\"Cross-Domain Deep Visual Feature Generation for Mandarin Audio\\u2013Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1618d2a0660a9f9cb8a3ca539d651c2fd299e87b\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48797555\",\"name\":\"Abhishek Jha\"},{\"authorId\":\"2961618\",\"name\":\"Vikram S. Voleti\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1109/ICASSP.2019.8682275\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"988b493ae494bdf9f31f39a448e47567ec6dcc34\",\"title\":\"Cross-language Speech Dependent Lip-synchronization\",\"url\":\"https://www.semanticscholar.org/paper/988b493ae494bdf9f31f39a448e47567ec6dcc34\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1911.04890\",\"authors\":[{\"authorId\":\"3208950\",\"name\":\"Takaki Makino\"},{\"authorId\":\"145004710\",\"name\":\"H. Liao\"},{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"39833326\",\"name\":\"B. Garc\\u00eda\"},{\"authorId\":\"123492027\",\"name\":\"Otavio Braga\"},{\"authorId\":\"2523830\",\"name\":\"O. Siohan\"}],\"doi\":\"10.1109/ASRU46091.2019.9004036\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4f0556a6225135c4525c5fda2fcab00c91609a3\",\"title\":\"Recurrent Neural Network Transducer for Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a4f0556a6225135c4525c5fda2fcab00c91609a3\",\"venue\":\"2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51265752\",\"name\":\"Alexandros Koumparoulis\"},{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"}],\"doi\":\"10.21437/interspeech.2019-2618\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"442ee1d9a38309fc850455b5e63e4e1499a29a50\",\"title\":\"MobiLipNet: Resource-Efficient Deep Learning Based Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/442ee1d9a38309fc850455b5e63e4e1499a29a50\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46187916\",\"name\":\"Ruben Vereecken\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"19288029\",\"name\":\"Yiannis Panagakis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/FG.2018.00062\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"49484ace2786445c2181d2f2975ec3ce34f9ed6d\",\"title\":\"Online Attention for Interpretable Conflict Estimation in Political Debates\",\"url\":\"https://www.semanticscholar.org/paper/49484ace2786445c2181d2f2975ec3ce34f9ed6d\",\"venue\":\"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)\",\"year\":2018},{\"arxivId\":\"1807.08469\",\"authors\":[{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"}],\"doi\":\"10.1007/978-3-030-01225-0_32\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd38fdd745c5e1b4ee5ce9e646fc36bcb800848d\",\"title\":\"Zero-shot keyword spotting for visual speech recognition in-the-wild\",\"url\":\"https://www.semanticscholar.org/paper/dd38fdd745c5e1b4ee5ce9e646fc36bcb800848d\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145993025\",\"name\":\"Andrew Abel\"},{\"authorId\":\"118565589\",\"name\":\"Chengxiang Gao\"},{\"authorId\":\"144708497\",\"name\":\"L. Smith\"},{\"authorId\":\"48444253\",\"name\":\"R. Watt\"},{\"authorId\":\"144664815\",\"name\":\"A. Hussain\"}],\"doi\":\"10.1109/SSCI.2018.8628931\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d124aa3a7cc0f748e59297eb99e883e163a3371a\",\"title\":\"Fast Lip Feature Extraction Using Psychologically Motivated Gabor Features\",\"url\":\"https://www.semanticscholar.org/paper/d124aa3a7cc0f748e59297eb99e883e163a3371a\",\"venue\":\"2018 IEEE Symposium Series on Computational Intelligence (SSCI)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115685706\",\"name\":\"Souheil Fenghour\"},{\"authorId\":\"47514164\",\"name\":\"Daqing Chen\"},{\"authorId\":\"6064229\",\"name\":\"Perry Xiao\"}],\"doi\":\"10.1117/12.2522936\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"734bf46993c7aa90d56fd2475356a266df0346e7\",\"title\":\"Contour mapping for speaker-independent lip reading system\",\"url\":\"https://www.semanticscholar.org/paper/734bf46993c7aa90d56fd2475356a266df0346e7\",\"venue\":\"International Conference on Machine Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51258901\",\"name\":\"Xingxuan Zhang\"},{\"authorId\":\"46390446\",\"name\":\"F. Cheng\"},{\"authorId\":\"2999987\",\"name\":\"Shilin Wang\"}],\"doi\":\"10.1109/ICCV.2019.00080\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a8e0d6f4c6d0203b9892bfe02eaea13d0e9aa6c\",\"title\":\"Spatio-Temporal Fusion Based Convolutional Sequence Learning for Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/2a8e0d6f4c6d0203b9892bfe02eaea13d0e9aa6c\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1911.12747\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054253\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f2d665862e1372d1860b9abc6214d080067393fc\",\"title\":\"ASR is All You Need: Cross-Modal Distillation for Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/f2d665862e1372d1860b9abc6214d080067393fc\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2769735\",\"name\":\"Leda Sari\"},{\"authorId\":\"1399115926\",\"name\":\"M. Hasegawa-Johnson\"},{\"authorId\":\"152856841\",\"name\":\"S. Kumaran\"},{\"authorId\":\"1708033\",\"name\":\"G. Stemmer\"},{\"authorId\":\"144186042\",\"name\":\"K. N. Nair\"}],\"doi\":\"10.21437/Interspeech.2018-2359\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f3f18782bbb69540e7d29390b1068d5a93e0fd81\",\"title\":\"Speaker Adaptive Audio-Visual Fusion for the Open-Vocabulary Section of AVICAR\",\"url\":\"https://www.semanticscholar.org/paper/f3f18782bbb69540e7d29390b1068d5a93e0fd81\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"2011.11315\",\"authors\":[{\"authorId\":\"48300230\",\"name\":\"J. Luo\"},{\"authorId\":\"66063851\",\"name\":\"Jianzong Wang\"},{\"authorId\":\"145292435\",\"name\":\"N. Cheng\"},{\"authorId\":\"2027691199\",\"name\":\"Guilin Jiang\"},{\"authorId\":\"1779360407\",\"name\":\"Jing Xiao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"edcc7c1c74d44a6cfff068f26eb4d547ef39f774\",\"title\":\"End-to-end Silent Speech Recognition with Acoustic Sensing\",\"url\":\"https://www.semanticscholar.org/paper/edcc7c1c74d44a6cfff068f26eb4d547ef39f774\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.03846\",\"authors\":[{\"authorId\":\"1557300901\",\"name\":\"Mingshuang Luo\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"},{\"authorId\":\"14631164\",\"name\":\"Zhiwei Liu\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b08540af0915cc70b9dcdc16b198abc37178a5a3\",\"title\":\"Synchronous Bidirectional Learning for Multilingual Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/b08540af0915cc70b9dcdc16b198abc37178a5a3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1802.00662\",\"authors\":[{\"authorId\":\"143725625\",\"name\":\"Hung Le\"},{\"authorId\":\"6254479\",\"name\":\"T. Tran\"},{\"authorId\":\"143761093\",\"name\":\"S. Venkatesh\"}],\"doi\":\"10.1145/3219819.3219981\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7178b94774f01d78fe0792db869fe049b835d178\",\"title\":\"Dual Memory Neural Computer for Asynchronous Two-view Sequential Learning\",\"url\":\"https://www.semanticscholar.org/paper/7178b94774f01d78fe0792db869fe049b835d178\",\"venue\":\"KDD\",\"year\":2018},{\"arxivId\":\"1805.05553\",\"authors\":[{\"authorId\":\"9340074\",\"name\":\"Changil Kim\"},{\"authorId\":\"2596714\",\"name\":\"H. Shin\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"2694281\",\"name\":\"Alexandre Kaspar\"},{\"authorId\":\"1854465\",\"name\":\"M. Elgharib\"},{\"authorId\":\"1752521\",\"name\":\"W. Matusik\"}],\"doi\":\"10.1007/978-3-030-20873-8_18\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bdd7ed61a38b61399340c04cd478a96b67a51e5\",\"title\":\"On Learning Associations of Faces and Voices\",\"url\":\"https://www.semanticscholar.org/paper/1bdd7ed61a38b61399340c04cd478a96b67a51e5\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5057032\",\"name\":\"Carol Hsin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f3a049fa2a1ad7784d0eeefe04080fa68bbebd8\",\"title\":\"Implementation and Optimization of Differentiable Neural Computers\",\"url\":\"https://www.semanticscholar.org/paper/6f3a049fa2a1ad7784d0eeefe04080fa68bbebd8\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1811.05250\",\"authors\":[{\"authorId\":\"33481412\",\"name\":\"Pan Zhou\"},{\"authorId\":\"47718595\",\"name\":\"Wenwen Yang\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"47905788\",\"name\":\"Y. Wang\"},{\"authorId\":\"144202060\",\"name\":\"Jia Jia\"}],\"doi\":\"10.1109/ICASSP.2019.8683733\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"65591f0aa156b3bb8bbcbaebc635d905f1b6c6d6\",\"title\":\"Modality Attention for End-to-end Audio-visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/65591f0aa156b3bb8bbcbaebc635d905f1b6c6d6\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391213316\",\"name\":\"Chenda Li\"},{\"authorId\":\"148407193\",\"name\":\"Yanmin Qian\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054180\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"383cba4d01844fbf570a9633746a94b6b8fc213b\",\"title\":\"Deep Audio-Visual Speech Separation with Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/383cba4d01844fbf570a9633746a94b6b8fc213b\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2001.08702\",\"authors\":[{\"authorId\":\"145944235\",\"name\":\"B. Mart\\u00ednez\"},{\"authorId\":\"1384480816\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387779\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053841\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8da125d2511d037df036c0f0da7c57135e24409\",\"title\":\"Lipreading Using Temporal Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d8da125d2511d037df036c0f0da7c57135e24409\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2011.09804\",\"authors\":[{\"authorId\":\"38569871\",\"name\":\"M. Ribeiro\"},{\"authorId\":\"88498397\",\"name\":\"Jennifer C. Sanger\"},{\"authorId\":\"47538856\",\"name\":\"Jingxuan Zhang\"},{\"authorId\":\"2041701\",\"name\":\"Aciel Eshky\"},{\"authorId\":\"2155190\",\"name\":\"A. Wrench\"},{\"authorId\":\"2544653\",\"name\":\"K. Richmond\"},{\"authorId\":\"153725670\",\"name\":\"Steve Renals\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"be157588b289a585e83c4106f1a1697bdf2ccaed\",\"title\":\"TaL: a synchronised multi-speaker corpus of ultrasound tongue imaging, audio, and lip videos\",\"url\":\"https://www.semanticscholar.org/paper/be157588b289a585e83c4106f1a1697bdf2ccaed\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.00579\",\"authors\":[{\"authorId\":\"46183056\",\"name\":\"Johanes Effendi\"},{\"authorId\":\"2894428\",\"name\":\"Andros Tjandra\"},{\"authorId\":\"1783949\",\"name\":\"Sakriani Sakti\"},{\"authorId\":\"50068540\",\"name\":\"S. Nakamura\"}],\"doi\":\"10.1109/ASRU46091.2019.9003899\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c1388aa767fae2df622e3e9e9c42f35263c8c5a\",\"title\":\"Listening While Speaking and Visualizing: Improving ASR Through Multimodal Chain\",\"url\":\"https://www.semanticscholar.org/paper/3c1388aa767fae2df622e3e9e9c42f35263c8c5a\",\"venue\":\"2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2019},{\"arxivId\":\"1911.06095\",\"authors\":[{\"authorId\":\"145864185\",\"name\":\"Shiyang Cheng\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145245424\",\"name\":\"Adrian Bulat\"},{\"authorId\":\"145218422\",\"name\":\"J. Shen\"},{\"authorId\":\"145387779\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054384\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc16e62526a41264077fe406c0729d2a4985c3fc\",\"title\":\"Towards Pose-Invariant Lip-Reading\",\"url\":\"https://www.semanticscholar.org/paper/cc16e62526a41264077fe406c0729d2a4985c3fc\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10437962\",\"name\":\"Soo-Whan Chung\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"153579825\",\"name\":\"Hong-Goo Kang\"}],\"doi\":\"10.1109/JSTSP.2020.2987720\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7431525dd5b821532191c7c078972bc457565d86\",\"title\":\"Perfect Match: Self-Supervised Embeddings for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/7431525dd5b821532191c7c078972bc457565d86\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35007805\",\"name\":\"Thomas Le Cornu\"},{\"authorId\":\"1772594\",\"name\":\"B. Milner\"}],\"doi\":\"10.1109/TASLP.2017.2716178\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a11dca9e8538c4887f525e263e64496aa68457c1\",\"title\":\"Generating Intelligible Audio Speech From Visual Speech\",\"url\":\"https://www.semanticscholar.org/paper/a11dca9e8538c4887f525e263e64496aa68457c1\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3044372\",\"name\":\"M. Bastan\"},{\"authorId\":\"145053416\",\"name\":\"Kim-Hui Yap\"},{\"authorId\":\"145662587\",\"name\":\"Lap-Pui Chau\"}],\"doi\":\"10.1109/ISCAS.2018.8351616\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"215c20b1b012b6aeecf03db24fd6517f0d892d4f\",\"title\":\"Idling Car Detection with ConvNets in Infrared Image Sequences\",\"url\":\"https://www.semanticscholar.org/paper/215c20b1b012b6aeecf03db24fd6517f0d892d4f\",\"venue\":\"2018 IEEE International Symposium on Circuits and Systems (ISCAS)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1579065768\",\"name\":\"Jessica Rinc\\u00f3n-Trujillo\"},{\"authorId\":\"1579101985\",\"name\":\"Diana Margarita C\\u00f3rdova-Esparza\"}],\"doi\":\"10.13053/rcs-148-9-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"887ea4a6101cbd3d76dbd031dd7ed96604c4deb0\",\"title\":\"Analysis of Speech Separation Methods based on Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/887ea4a6101cbd3d76dbd031dd7ed96604c4deb0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1399864025\",\"name\":\"F. Garc\\u00eda-Pe\\u00f1alvo\"}],\"doi\":\"10.5281/ZENODO.2818903\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d2ad756f19fdc2dba17358f290cd7cb854e2e487\",\"title\":\"Inteligencia Artificial. Una perspectiva desde la ficci\\u00f3n a la realidad\",\"url\":\"https://www.semanticscholar.org/paper/d2ad756f19fdc2dba17358f290cd7cb854e2e487\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2251985\",\"name\":\"D. Jang\"},{\"authorId\":\"11661187\",\"name\":\"Hong-In Kim\"},{\"authorId\":\"2054842\",\"name\":\"C. Je\"},{\"authorId\":\"102442672\",\"name\":\"R. Park\"},{\"authorId\":\"145449241\",\"name\":\"H. Park\"}],\"doi\":\"10.1109/ACCESS.2019.2927166\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f305fe4f8889427f1a0533334359ec82f7db5e27\",\"title\":\"Lip Reading Using Committee Networks With Two Different Types of Concatenated Frame Images\",\"url\":\"https://www.semanticscholar.org/paper/f305fe4f8889427f1a0533334359ec82f7db5e27\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50157358\",\"name\":\"M. Brumm\"},{\"authorId\":\"144978755\",\"name\":\"Rolf-Rainer Grigat\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a9af91250f7492ba5a06dceb28199b6e0ef014b\",\"title\":\"Optimised Preprocessing for Automatic Mouth Gesture Classification\",\"url\":\"https://www.semanticscholar.org/paper/0a9af91250f7492ba5a06dceb28199b6e0ef014b\",\"venue\":\"LREC 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41029776\",\"name\":\"M. R. Maulana\"},{\"authorId\":\"2656725\",\"name\":\"M. I. Fanany\"}],\"doi\":\"10.1109/ICACSIS.2017.8355062\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5af2859c764413d5b547fd6c4f7bcfd8ef93c6c1\",\"title\":\"Indonesian audio-visual speech corpus for multimodal automatic speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/5af2859c764413d5b547fd6c4f7bcfd8ef93c6c1\",\"venue\":\"2017 International Conference on Advanced Computer Science and Information Systems (ICACSIS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50556355\",\"name\":\"Fei Tao\"},{\"authorId\":\"2106794\",\"name\":\"C. Busso\"}],\"doi\":\"10.1109/TASLP.2018.2815268\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e3ef26f4203d735c65a9a895433415377fefbe2e\",\"title\":\"Gating Neural Network for Large Vocabulary Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/e3ef26f4203d735c65a9a895433415377fefbe2e\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/interspeech.2020-2921\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b721b51e9753fd9547387387182fcbd0a20c8fb\",\"title\":\"Now You're Speaking My Language: Visual Language Identification\",\"url\":\"https://www.semanticscholar.org/paper/2b721b51e9753fd9547387387182fcbd0a20c8fb\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1905.09773\",\"authors\":[{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"2112779\",\"name\":\"Tali Dekel\"},{\"authorId\":\"9340074\",\"name\":\"Changil Kim\"},{\"authorId\":\"2138834\",\"name\":\"Inbar Mosseri\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"144544291\",\"name\":\"Michael Rubinstein\"},{\"authorId\":\"1752521\",\"name\":\"W. Matusik\"}],\"doi\":\"10.1109/CVPR.2019.00772\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4b54f2e0ebbe450e2c527cf5a00d4816980a913\",\"title\":\"Speech2Face: Learning the Face Behind a Voice\",\"url\":\"https://www.semanticscholar.org/paper/e4b54f2e0ebbe450e2c527cf5a00d4816980a913\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48065888\",\"name\":\"Di Fu\"},{\"authorId\":\"152448143\",\"name\":\"C. Weber\"},{\"authorId\":\"9878059\",\"name\":\"Guochun Yang\"},{\"authorId\":\"2991958\",\"name\":\"Matthias Kerzel\"},{\"authorId\":\"1406707237\",\"name\":\"Weizhi Nan\"},{\"authorId\":\"144039832\",\"name\":\"P. Barros\"},{\"authorId\":\"49498866\",\"name\":\"Haiyan Wu\"},{\"authorId\":\"144227938\",\"name\":\"X. Liu\"},{\"authorId\":\"47291450\",\"name\":\"Stefan Wermter\"}],\"doi\":\"10.3389/fnint.2020.00010\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e302e57072bd160b6420ba5b190b0d7286ab74e9\",\"title\":\"What Can Computational Models Learn From Human Selective Attention? A Review From an Audiovisual Unimodal and Crossmodal Perspective\",\"url\":\"https://www.semanticscholar.org/paper/e302e57072bd160b6420ba5b190b0d7286ab74e9\",\"venue\":\"Frontiers in Integrative Neuroscience\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66607832\",\"name\":\"Y. Xu\"},{\"authorId\":\"15463569\",\"name\":\"Yuexuan Li\"},{\"authorId\":\"48325867\",\"name\":\"A. Abel\"}],\"doi\":\"10.1007/978-3-030-39431-8_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c02ff55f3a01a6ee68e2bc37971e3fb0e4b7877b\",\"title\":\"Gabor Based Lipreading with a New Audiovisual Mandarin Corpus\",\"url\":\"https://www.semanticscholar.org/paper/c02ff55f3a01a6ee68e2bc37971e3fb0e4b7877b\",\"venue\":\"BICS\",\"year\":2019},{\"arxivId\":\"1806.00812\",\"authors\":[{\"authorId\":\"40432084\",\"name\":\"Benjamin M. Gorman\"},{\"authorId\":\"3247725\",\"name\":\"David R. Flatla\"}],\"doi\":\"10.1145/3025453.3025560\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2e26b1eca1243fa3120dbdce8c5b710149c7e53e\",\"title\":\"A Framework for Speechreading Acquisition Tools\",\"url\":\"https://www.semanticscholar.org/paper/2e26b1eca1243fa3120dbdce8c5b710149c7e53e\",\"venue\":\"CHI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506589\",\"name\":\"H. Wang\"},{\"authorId\":\"1419534171\",\"name\":\"Fei Gao\"},{\"authorId\":\"9083623\",\"name\":\"Y. Zhao\"},{\"authorId\":\"50789847\",\"name\":\"Licheng Wu\"}],\"doi\":\"10.1109/ACCESS.2020.3024218\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4f3d570ce25d7e02209ccfc62cdb4202deedd75\",\"title\":\"WaveNet With Cross-Attention for Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a4f3d570ce25d7e02209ccfc62cdb4202deedd75\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3034487\",\"name\":\"K. Thangthai\"},{\"authorId\":\"144439756\",\"name\":\"R. Harvey\"}],\"doi\":\"10.21437/Interspeech.2018-2112\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f0a5cfb63819b639ab30f99949ddfa068523a9c\",\"title\":\"Building Large-vocabulary Speaker-independent Lipreading Systems\",\"url\":\"https://www.semanticscholar.org/paper/8f0a5cfb63819b639ab30f99949ddfa068523a9c\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2808158\",\"name\":\"Necati Cihan Camg\\u00f6z\"},{\"authorId\":\"2417546\",\"name\":\"S. Hadfield\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":\"10.1109/ICCVW.2017.364\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"908667ca31085ff89a56f00f30aa3c6592a223e7\",\"title\":\"Particle Filter Based Probabilistic Forced Alignment for Continuous Gesture Recognition\",\"url\":\"https://www.semanticscholar.org/paper/908667ca31085ff89a56f00f30aa3c6592a223e7\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2831839\",\"name\":\"P. Korshunov\"},{\"authorId\":\"145607451\",\"name\":\"S. Marcel\"}],\"doi\":\"10.23919/EUSIPCO.2018.8553270\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3d5f8487220e9e7f36d079075d1b2e4ea5722aac\",\"title\":\"Speaker Inconsistency Detection in Tampered Video\",\"url\":\"https://www.semanticscholar.org/paper/3d5f8487220e9e7f36d079075d1b2e4ea5722aac\",\"venue\":\"2018 26th European Signal Processing Conference (EUSIPCO)\",\"year\":2018},{\"arxivId\":\"1812.08685\",\"authors\":[{\"authorId\":\"2831839\",\"name\":\"P. Korshunov\"},{\"authorId\":\"145607451\",\"name\":\"S. Marcel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6eb0b0ddc1f87df9c74259feef5c6ccafc334a8f\",\"title\":\"DeepFakes: a New Threat to Face Recognition? Assessment and Detection\",\"url\":\"https://www.semanticscholar.org/paper/6eb0b0ddc1f87df9c74259feef5c6ccafc334a8f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12933846\",\"name\":\"L. Billman\"},{\"authorId\":\"108500495\",\"name\":\"Johan Hullberg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1382e0b479ad92d428d27ab20805342ffb4d4b1e\",\"title\":\"Speech Reading with Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/1382e0b479ad92d428d27ab20805342ffb4d4b1e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1571717758\",\"name\":\"Eslam Eid Elmaghraby\"},{\"authorId\":\"2126522\",\"name\":\"Amr M. Gody\"},{\"authorId\":\"48893823\",\"name\":\"Mohamed Farouk\"}],\"doi\":\"10.21608/ejle.2019.59164\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b6ab637888b11f234bb887f26f5b8babf330dc3\",\"title\":\"Speech Recognition Using Historian Multimodal Approach\",\"url\":\"https://www.semanticscholar.org/paper/7b6ab637888b11f234bb887f26f5b8babf330dc3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115685706\",\"name\":\"Souheil Fenghour\"},{\"authorId\":\"144395119\",\"name\":\"D. Chen\"},{\"authorId\":\"6064229\",\"name\":\"Perry Xiao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3ac92718b874b14a2a197368151b8dd02db91551\",\"title\":\"Recurrent Neural Networks for Decoding Lip Read Speech\",\"url\":\"https://www.semanticscholar.org/paper/3ac92718b874b14a2a197368151b8dd02db91551\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.02515\",\"authors\":[{\"authorId\":\"2031908880\",\"name\":\"Mohit Raghavendra\"},{\"authorId\":\"2031908061\",\"name\":\"Pravan Omprakash\"},{\"authorId\":\"91520493\",\"name\":\"B. Mukesh\"},{\"authorId\":\"2161231\",\"name\":\"Sowmya Kamath\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"80934c344fc682aa7ee1db350cb5c6275b6979aa\",\"title\":\"AuthNet: A Deep Learning based Authentication Mechanism using Temporal Facial Feature Movements\",\"url\":\"https://www.semanticscholar.org/paper/80934c344fc682aa7ee1db350cb5c6275b6979aa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786177\",\"name\":\"M. Mazurowski\"}],\"doi\":\"10.1016/j.jacr.2019.01.026\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7835899bc02c0968e9823e2e4247e842d98d698\",\"title\":\"Artificial Intelligence May Cause a Significant Disruption to the Radiology Workforce.\",\"url\":\"https://www.semanticscholar.org/paper/d7835899bc02c0968e9823e2e4247e842d98d698\",\"venue\":\"Journal of the American College of Radiology : JACR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51265752\",\"name\":\"Alexandros Koumparoulis\"},{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"2071376\",\"name\":\"Steven J. Rennie\"}],\"doi\":\"10.21437/AVSP.2017-13\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c17d280b8f3adc8593bf4c58f445a5f947ed32f2\",\"title\":\"Exploring ROI size in deep learning based lipreading\",\"url\":\"https://www.semanticscholar.org/paper/c17d280b8f3adc8593bf4c58f445a5f947ed32f2\",\"venue\":\"AVSP\",\"year\":2017},{\"arxivId\":\"1812.09336\",\"authors\":[{\"authorId\":\"29951387\",\"name\":\"Devesh Walawalkar\"},{\"authorId\":\"39838894\",\"name\":\"Yihui He\"},{\"authorId\":\"26364117\",\"name\":\"Rohit Pillai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"120ce554a8de66d8f5b28bfbe8f0cb720f0abfa2\",\"title\":\"An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/120ce554a8de66d8f5b28bfbe8f0cb720f0abfa2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1804.04121\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2018-1400\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3\",\"title\":\"The Conversation: Deep Audio-Visual Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"2001.01656\",\"authors\":[{\"authorId\":\"3075402\",\"name\":\"J. Yu\"},{\"authorId\":\"2213494\",\"name\":\"S. Zhang\"},{\"authorId\":\"97569132\",\"name\":\"J. Wu\"},{\"authorId\":\"48414237\",\"name\":\"S. Ghorbani\"},{\"authorId\":\"152365289\",\"name\":\"Bo Wu\"},{\"authorId\":\"2193534\",\"name\":\"Shiyin Kang\"},{\"authorId\":\"9511592\",\"name\":\"Shansong Liu\"},{\"authorId\":\"150344273\",\"name\":\"Xunying Liu\"},{\"authorId\":\"153726255\",\"name\":\"Helen Meng\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054127\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"239c77b203716478bfbd522f6c5b3c81a4d414c5\",\"title\":\"Audio-Visual Recognition of Overlapped Speech for the LRS2 Dataset\",\"url\":\"https://www.semanticscholar.org/paper/239c77b203716478bfbd522f6c5b3c81a4d414c5\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1396191708\",\"name\":\"Aleksander Kusmierczyk\"},{\"authorId\":\"153752763\",\"name\":\"M. S\\u0142awi\\u0144ska\"},{\"authorId\":\"1392774467\",\"name\":\"Kornel Zaba\"},{\"authorId\":\"47054843\",\"name\":\"K. Saeed\"}],\"doi\":\"10.1007/978-981-13-8969-6_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"40e94c82613fe7dc30cd8674c470cdf4092c1718\",\"title\":\"Biometric Fusion System Using Face and Voice Recognition - A Comparison Approach: Biometric Fusion System Using Face and Voice Characteristics\",\"url\":\"https://www.semanticscholar.org/paper/40e94c82613fe7dc30cd8674c470cdf4092c1718\",\"venue\":\"ACSS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1713014\",\"name\":\"A. Czyzewski\"},{\"authorId\":\"1796260\",\"name\":\"B. Kostek\"},{\"authorId\":\"2972926\",\"name\":\"P. Bratoszewski\"},{\"authorId\":\"144030731\",\"name\":\"J. Kotus\"},{\"authorId\":\"1678848\",\"name\":\"Marcin S. Szczuka\"}],\"doi\":\"10.1007/s10844-016-0438-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd54941cc46656005d31b1b24e3a002a7acd5b3f\",\"title\":\"An audio-visual corpus for multimodal automatic speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/cd54941cc46656005d31b1b24e3a002a7acd5b3f\",\"venue\":\"Journal of Intelligent Information Systems\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"},{\"authorId\":\"2814229\",\"name\":\"George Trigeorgis\"},{\"authorId\":\"34586458\",\"name\":\"Grigorios G. Chrysos\"},{\"authorId\":\"3234063\",\"name\":\"Jiankang Deng\"},{\"authorId\":\"46904799\",\"name\":\"Jie Shen\"}],\"doi\":\"10.1109/CVPRW.2017.263\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59d8fa6fd91cdb72cd0fa74c04016d79ef5a752b\",\"title\":\"The Menpo Facial Landmark Localisation Challenge: A Step Towards the Solution\",\"url\":\"https://www.semanticscholar.org/paper/59d8fa6fd91cdb72cd0fa74c04016d79ef5a752b\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"2004.14326\",\"authors\":[{\"authorId\":\"10437962\",\"name\":\"Soo-Whan Chung\"},{\"authorId\":\"9299637\",\"name\":\"Hong-Goo Kang\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"}],\"doi\":\"10.21437/Interspeech.2020-1113\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6ec232e30ae03c1f97981fb00c84195f9f9bff3\",\"title\":\"Seeing voices and hearing voices: learning discriminative embeddings using cross-modal self-supervision\",\"url\":\"https://www.semanticscholar.org/paper/d6ec232e30ae03c1f97981fb00c84195f9f9bff3\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2005.10903\",\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"}],\"doi\":\"10.1007/978-3-030-63820-7_63\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"09da26797d509cc5e193512589e205c088f4b1f4\",\"title\":\"SpotFast Networks with Memory Augmented Lateral Transformers for Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/09da26797d509cc5e193512589e205c088f4b1f4\",\"venue\":\"ICONIP\",\"year\":2020},{\"arxivId\":\"1806.06053\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2018-1943\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04187519dc8c468f2b5b17442413ada7830068e5\",\"title\":\"Deep Lip Reading: a comparison of models and an online application\",\"url\":\"https://www.semanticscholar.org/paper/04187519dc8c468f2b5b17442413ada7830068e5\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"1706.09556\",\"authors\":[{\"authorId\":\"3251468\",\"name\":\"A. Bazzica\"},{\"authorId\":\"144044151\",\"name\":\"J. V. Gemert\"},{\"authorId\":\"1968667\",\"name\":\"Cynthia C. S. Liem\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eadd73c3e1c20d16e32ee8656c4f954603b37450\",\"title\":\"Vision-based Detection of Acoustic Timed Events: a Case Study on Clarinet Note Onsets\",\"url\":\"https://www.semanticscholar.org/paper/eadd73c3e1c20d16e32ee8656c4f954603b37450\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1904.07002\",\"authors\":[{\"authorId\":\"2829366\",\"name\":\"Panagiotis Tzirakis\"},{\"authorId\":\"144848303\",\"name\":\"A. Papaioannou\"},{\"authorId\":\"50815847\",\"name\":\"Alexander Lattas\"},{\"authorId\":\"102361656\",\"name\":\"Michail Tarasiou\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3e8f459230b91b4b055665cbf73bed01d8947e7\",\"title\":\"Synthesising 3D Facial Motion from \\\"In-the-Wild\\\" Speech\",\"url\":\"https://www.semanticscholar.org/paper/a3e8f459230b91b4b055665cbf73bed01d8947e7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1904.04817\",\"authors\":[{\"authorId\":\"71712589\",\"name\":\"L. Courtney\"},{\"authorId\":\"40568918\",\"name\":\"R. Sreenivas\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3034c12a9c5b27592a123216a0cfd79f955b0a0f\",\"title\":\"Learning from Videos with Deep Convolutional LSTM Networks\",\"url\":\"https://www.semanticscholar.org/paper/3034c12a9c5b27592a123216a0cfd79f955b0a0f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1804.03641\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1007/978-3-030-01231-1_39\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"title\":\"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\",\"url\":\"https://www.semanticscholar.org/paper/171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1804.09552\",\"authors\":[{\"authorId\":\"1881792\",\"name\":\"K. Yun\"},{\"authorId\":\"152488216\",\"name\":\"J. Osborne\"},{\"authorId\":\"10994741\",\"name\":\"Madison Lee\"},{\"authorId\":\"6801275\",\"name\":\"T. Lu\"},{\"authorId\":\"145745831\",\"name\":\"E. Chow\"}],\"doi\":\"10.1117/12.2304569\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a729c6c9c70cde2e1b9e7ed546cfb98e141128d1\",\"title\":\"Automatic speech recognition for launch control center communication using recurrent neural networks with data augmentation and custom language model\",\"url\":\"https://www.semanticscholar.org/paper/a729c6c9c70cde2e1b9e7ed546cfb98e141128d1\",\"venue\":\"Defense + Security\",\"year\":2018},{\"arxivId\":\"1802.06399\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"46904799\",\"name\":\"Jie Shen\"},{\"authorId\":\"35728724\",\"name\":\"Doruk Cetin\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP.2018.8461596\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"53c8dbecc9afa31d07f9c78848bba01c21ea5080\",\"title\":\"Visual-Only Recognition of Normal, Whispered and Silent Speech\",\"url\":\"https://www.semanticscholar.org/paper/53c8dbecc9afa31d07f9c78848bba01c21ea5080\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"153677544\",\"name\":\"Asaph Shamir\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.21437/Interspeech.2018-1955\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f997d69d78af086dec4462e4319c6d241f42c0c1\",\"title\":\"Visual Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/f997d69d78af086dec4462e4319c6d241f42c0c1\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"1901.10139\",\"authors\":[{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"66016321\",\"name\":\"Dhruva Sahrawat\"},{\"authorId\":\"51440276\",\"name\":\"S. Maheshwari\"},{\"authorId\":\"2712224\",\"name\":\"Debanjan Mahata\"},{\"authorId\":\"1379977554\",\"name\":\"Amanda Stent\"},{\"authorId\":\"144221742\",\"name\":\"Yifang Yin\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"}],\"doi\":\"10.1609/AAAI.V34I03.5649\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c9e326626879bdec209dde43b420ffca052292d\",\"title\":\"Harnessing GANs for Zero-Shot Learning of New Classes in Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/3c9e326626879bdec209dde43b420ffca052292d\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICASSP.2018.8462527\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e246fd8721f5718981d735c328d74e50afc0e9d0\",\"title\":\"Seeing Through Noise: Visually Driven Speaker Separation And Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/e246fd8721f5718981d735c328d74e50afc0e9d0\",\"venue\":\"ICASSP\",\"year\":2018},{\"arxivId\":\"2009.05784\",\"authors\":[{\"authorId\":\"8790824\",\"name\":\"Weicong Chen\"},{\"authorId\":\"1391138969\",\"name\":\"Xu Tan\"},{\"authorId\":\"2794096\",\"name\":\"Yingce Xia\"},{\"authorId\":\"82620854\",\"name\":\"Tao Qin\"},{\"authorId\":null,\"name\":\"Yu Wang\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":\"10.1145/3394171.3413623\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4dabcac75e2a6e52b2ec915eef1fd74bd5f677f\",\"title\":\"DualLip: A System for Joint Lip Reading and Generation\",\"url\":\"https://www.semanticscholar.org/paper/a4dabcac75e2a6e52b2ec915eef1fd74bd5f677f\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"},{\"authorId\":\"2293163\",\"name\":\"E. Marcheret\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"1782589\",\"name\":\"V. Goel\"},{\"authorId\":\"101408346\",\"name\":\"Alexandros Koumbaroulis\"},{\"authorId\":\"100547861\",\"name\":\"A. Vartholomaios\"},{\"authorId\":\"3418004\",\"name\":\"Spyridon Thermos\"}],\"doi\":\"10.1145/3015783.3015797\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d83fcf829e5f66a0d30bf0f6cda8428423f8479c\",\"title\":\"Audio and visual modality combination in speech processing applications\",\"url\":\"https://www.semanticscholar.org/paper/d83fcf829e5f66a0d30bf0f6cda8428423f8479c\",\"venue\":\"The Handbook of Multimodal-Multisensor Interfaces, Volume 1\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39358728\",\"name\":\"Yongming Rao\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"49640256\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/ICCV.2017.424\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f2700e3d69d3cce2e0b1aea0d7f87e74aff437cd\",\"title\":\"Attention-Aware Deep Reinforcement Learning for Video Face Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f2700e3d69d3cce2e0b1aea0d7f87e74aff437cd\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1802.10185\",\"authors\":[{\"authorId\":\"6783811\",\"name\":\"A. Kurtulmus\"},{\"authorId\":\"1874240\",\"name\":\"K. Daniel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7078de49e9f80a11d1c2f52f8a14bf95f5cacd34\",\"title\":\"Trustless Machine Learning Contracts; Evaluating and Exchanging Machine Learning Models on the Ethereum Blockchain\",\"url\":\"https://www.semanticscholar.org/paper/7078de49e9f80a11d1c2f52f8a14bf95f5cacd34\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33481412\",\"name\":\"Pan Zhou\"},{\"authorId\":\"47718595\",\"name\":\"Wenwen Yang\"},{\"authorId\":\"143832732\",\"name\":\"Wei Chen\"},{\"authorId\":\"47905788\",\"name\":\"Y. Wang\"},{\"authorId\":\"144202061\",\"name\":\"Jia Jia\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3bbe91a696b0ecbb0f7ad766239a617956163e0b\",\"title\":\"N ov 2 01 8 MODALITY ATTENTION FOR END-TO-END AUDIO-VISUAL SPEECH RECOGNITION\",\"url\":\"https://www.semanticscholar.org/paper/3bbe91a696b0ecbb0f7ad766239a617956163e0b\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"78346793\",\"name\":\"T. Saitoh\"},{\"authorId\":\"40099567\",\"name\":\"Michiko Kubokawa\"}],\"doi\":\"10.21437/avsp.2019-17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f8d42e3b7d3e66eb707466681b5dd6eb895fc0c\",\"title\":\"LiP25w: Word-level Lip Reading Web Application for Smart Device\",\"url\":\"https://www.semanticscholar.org/paper/2f8d42e3b7d3e66eb707466681b5dd6eb895fc0c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2004.02541\",\"authors\":[{\"authorId\":\"9686806\",\"name\":\"Daniel Michelsanti\"},{\"authorId\":\"9936815\",\"name\":\"Olga Slizovskaia\"},{\"authorId\":\"1916387\",\"name\":\"G. Haro\"},{\"authorId\":\"145217215\",\"name\":\"E. G\\u00f3mez\"},{\"authorId\":\"71668001\",\"name\":\"Zheng-Hua Tan\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":\"10.21437/interspeech.2020-1026\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a0d2dc8123277cf3c894a10121272207dc39413\",\"title\":\"Vocoder-Based Speech Synthesis from Silent Videos\",\"url\":\"https://www.semanticscholar.org/paper/1a0d2dc8123277cf3c894a10121272207dc39413\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144141009\",\"name\":\"Ivan Fung\"},{\"authorId\":\"144678172\",\"name\":\"B. K. Mak\"}],\"doi\":\"10.1109/ICASSP.2018.8462280\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a49dcb22be5ba1fc1f07b249630c757df8487287\",\"title\":\"End-To-End Low-Resource Lip-Reading with Maxout Cnn and Lstm\",\"url\":\"https://www.semanticscholar.org/paper/a49dcb22be5ba1fc1f07b249630c757df8487287\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51265752\",\"name\":\"Alexandros Koumparoulis\"},{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"}],\"doi\":\"10.1109/SLT.2018.8639698\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"582159865593a54068241808a8c1e6f96a0e36ca\",\"title\":\"Deep View2View Mapping for View-Invariant Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/582159865593a54068241808a8c1e6f96a0e36ca\",\"venue\":\"2018 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8788762\",\"name\":\"Chunlin Tian\"},{\"authorId\":\"145377162\",\"name\":\"Y. Yuan\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1007/978-981-10-7299-4_54\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"164ebdd07c6fd85ca08812ffc0039ffc3db816ce\",\"title\":\"Deep Temporal Architecture for Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/164ebdd07c6fd85ca08812ffc0039ffc3db816ce\",\"venue\":\"CCCV\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48260398\",\"name\":\"Y. Feng\"},{\"authorId\":\"150358505\",\"name\":\"Zhenxing Xu\"},{\"authorId\":\"1491232484\",\"name\":\"Lin Gan\"},{\"authorId\":\"153708417\",\"name\":\"Ning Chen\"},{\"authorId\":\"144219781\",\"name\":\"Bin Yu\"},{\"authorId\":\"145358498\",\"name\":\"Ting Chen\"},{\"authorId\":\"32346302\",\"name\":\"F. Wang\"}],\"doi\":\"10.1109/ICDM.2019.00030\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ca5de7ef6bfb81c0df04b50781beb321be2f295\",\"title\":\"DCMN: Double Core Memory Network for Patient Outcome Prediction with Multimodal Data\",\"url\":\"https://www.semanticscholar.org/paper/3ca5de7ef6bfb81c0df04b50781beb321be2f295\",\"venue\":\"2019 IEEE International Conference on Data Mining (ICDM)\",\"year\":2019},{\"arxivId\":\"1705.02966\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"2727313\",\"name\":\"A. Jamaludin\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.31.109\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a8632cf6c1ef4319966564328d187876d3bef363\",\"title\":\"You said that?\",\"url\":\"https://www.semanticscholar.org/paper/a8632cf6c1ef4319966564328d187876d3bef363\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"51440276\",\"name\":\"S. Maheshwari\"},{\"authorId\":\"66016321\",\"name\":\"Dhruva Sahrawat\"},{\"authorId\":\"6305762\",\"name\":\"Praveen Jhanwar\"},{\"authorId\":\"144718915\",\"name\":\"V. Chaudhary\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"2712224\",\"name\":\"Debanjan Mahata\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1381958d4bd158bd14cad3af21810e41796cd5bd\",\"title\":\"Harnessing GANs for Addition of New Classes in VSR\",\"url\":\"https://www.semanticscholar.org/paper/1381958d4bd158bd14cad3af21810e41796cd5bd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1510709841\",\"name\":\"Zhengtao Li\"},{\"authorId\":\"72808680\",\"name\":\"G. Zhou\"},{\"authorId\":\"40476980\",\"name\":\"Q. Song\"}],\"doi\":\"10.1016/j.infrared.2019.103152\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0350b4e20d63edd206029f89fe488176f02660ca\",\"title\":\"A temporal group attention approach for multitemporal multisensor crop classification\",\"url\":\"https://www.semanticscholar.org/paper/0350b4e20d63edd206029f89fe488176f02660ca\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.14223\",\"authors\":[{\"authorId\":\"47218558\",\"name\":\"Wentao Yu\"},{\"authorId\":\"1701275\",\"name\":\"S. Zeiler\"},{\"authorId\":\"48554322\",\"name\":\"D. Kolossa\"}],\"doi\":\"10.23919/Eusipco47968.2020.9287841\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9408356d40ae3b84527480dbd52c754e5c4bedd1\",\"title\":\"Multimodal Integration for Large-Vocabulary Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9408356d40ae3b84527480dbd52c754e5c4bedd1\",\"venue\":\"2020 28th European Signal Processing Conference (EUSIPCO)\",\"year\":2021},{\"arxivId\":\"1911.12798\",\"authors\":[{\"authorId\":\"3461272\",\"name\":\"Umut Sulubacak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"1438306994\",\"name\":\"Stig-Arne Gronroos\"},{\"authorId\":\"8769200\",\"name\":\"Aku Rouhe\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"},{\"authorId\":\"113391779\",\"name\":\"Jorg Tiedemann\"}],\"doi\":\"10.1007/s10590-020-09250-0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6f62d2365aa63f5d9c90893ab8aaa25551276fe\",\"title\":\"Multimodal machine translation through visuals and speech\",\"url\":\"https://www.semanticscholar.org/paper/a6f62d2365aa63f5d9c90893ab8aaa25551276fe\",\"venue\":\"Machine Translation\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"145420448\",\"name\":\"E. Morais\"}],\"doi\":\"10.1109/ICIP.2018.8451435\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"db64d176ef22091a3249c3a3b95226ad4bc3e19e\",\"title\":\"Towards View-Independent Viseme Recognition Based on CNNS and Synthetic Data\",\"url\":\"https://www.semanticscholar.org/paper/db64d176ef22091a3249c3a3b95226ad4bc3e19e\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":\"1804.10805\",\"authors\":[{\"authorId\":\"3044372\",\"name\":\"M. Bastan\"},{\"authorId\":\"145053416\",\"name\":\"Kim-Hui Yap\"},{\"authorId\":\"145662587\",\"name\":\"Lap-Pui Chau\"}],\"doi\":\"10.1007/s00521-019-04077-0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"231494ee018a853c9497e5d86b71297f206cba2d\",\"title\":\"Remote detection of idling cars using infrared imaging and deep networks\",\"url\":\"https://www.semanticscholar.org/paper/231494ee018a853c9497e5d86b71297f206cba2d\",\"venue\":\"Neural Computing and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150073134\",\"name\":\"Dario Augusto Borges Oliveira\"},{\"authorId\":\"150007774\",\"name\":\"Andrea Britto Mattos\"},{\"authorId\":\"150095880\",\"name\":\"Edmilson da Silva Morais\"}],\"doi\":\"10.1109/FG.2019.8756589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6f5508afa55602265d6a648451089e5428b388fa\",\"title\":\"Improving Viseme Recognition with GAN-based Muti-view Mapping\",\"url\":\"https://www.semanticscholar.org/paper/6f5508afa55602265d6a648451089e5428b388fa\",\"venue\":\"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3404049\",\"name\":\"Leyuan Qu\"},{\"authorId\":\"1798067\",\"name\":\"Cornelius Weber\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.21437/interspeech.2019-1393\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dfba89013c4ca7d90d86ac91fe98586756256440\",\"title\":\"LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/dfba89013c4ca7d90d86ac91fe98586756256440\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2802283\",\"name\":\"H. Liu\"},{\"authorId\":\"46842382\",\"name\":\"Zhan Chen\"},{\"authorId\":\"152515346\",\"name\":\"B. Yang\"}],\"doi\":\"10.21437/interspeech.2020-3146\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"854a9626dc0ad5d52e51a41301b2f69489540d04\",\"title\":\"Lip Graph Assisted Audio-Visual Speech Recognition Using Bidirectional Synchronous Fusion\",\"url\":\"https://www.semanticscholar.org/paper/854a9626dc0ad5d52e51a41301b2f69489540d04\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3343194\",\"name\":\"T. Dumitras\"},{\"authorId\":\"40898242\",\"name\":\"Y. Kaya\"},{\"authorId\":\"4951026\",\"name\":\"R. Marginean\"},{\"authorId\":\"31765629\",\"name\":\"O. Suciu\"}],\"doi\":\"10.1007/978-3-030-03251-7_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"539a80a9ef57cacb6df0af034ec6c73dfa6293dd\",\"title\":\"Too Big to FAIL: What You Need to Know Before Attacking a Machine Learning System\",\"url\":\"https://www.semanticscholar.org/paper/539a80a9ef57cacb6df0af034ec6c73dfa6293dd\",\"venue\":\"Security Protocols Workshop\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000506561\",\"name\":\"Chenzhao Yang\"},{\"authorId\":\"2999987\",\"name\":\"Shilin Wang\"},{\"authorId\":\"51258901\",\"name\":\"Xingxuan Zhang\"},{\"authorId\":\"1973629764\",\"name\":\"Y. Zhu\"}],\"doi\":\"10.1109/ICIP40778.2020.9190780\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"47006ec1c8c14a2297fdb57dafe7aedef93ffd5e\",\"title\":\"Speaker-Independent Lipreading With Limited Data\",\"url\":\"https://www.semanticscholar.org/paper/47006ec1c8c14a2297fdb57dafe7aedef93ffd5e\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":\"2001.04758\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"100602595\",\"name\":\"Mandi Luo\"},{\"authorId\":\"77886607\",\"name\":\"Rui Wang\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"title\":\"Deep Audio-Visual Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.05979\",\"authors\":[{\"authorId\":\"144077750\",\"name\":\"Hang Zhao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"2650832\",\"name\":\"W. Ma\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/ICCV.2019.00182\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c880de441a41c351955ad0bf8f712eeee500ac67\",\"title\":\"The Sound of Motions\",\"url\":\"https://www.semanticscholar.org/paper/c880de441a41c351955ad0bf8f712eeee500ac67\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3034487\",\"name\":\"K. Thangthai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a9f19c1fbf50f3a54d6227383c8b43b2d7fce75c\",\"title\":\"Computer lipreading via hybrid deep neural network hidden Markov models\",\"url\":\"https://www.semanticscholar.org/paper/a9f19c1fbf50f3a54d6227383c8b43b2d7fce75c\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1905.12043\",\"authors\":[{\"authorId\":\"50383147\",\"name\":\"M. Doukas\"},{\"authorId\":\"1790503\",\"name\":\"V. Sharmanska\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2f7e7adbbd2326c7ac7cbb67fa43214108bb372\",\"title\":\"Video-to-Video Translation for Visual Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/a2f7e7adbbd2326c7ac7cbb67fa43214108bb372\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400565050\",\"name\":\"Aniello Castiglione\"},{\"authorId\":\"1453615026\",\"name\":\"Giampiero Grazioli\"},{\"authorId\":\"1453610246\",\"name\":\"Simone Iengo\"},{\"authorId\":\"144759484\",\"name\":\"M. Nappi\"},{\"authorId\":\"49435089\",\"name\":\"S. Ricciardi\"}],\"doi\":\"10.1007/978-981-15-1304-6_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0216bf6072234c24871b64389c5712918ac03038\",\"title\":\"Dependable Person Recognition by Means of Local Descriptors of Dynamic Facial Features\",\"url\":\"https://www.semanticscholar.org/paper/0216bf6072234c24871b64389c5712918ac03038\",\"venue\":\"DependSys\",\"year\":2019},{\"arxivId\":\"2007.12131\",\"authors\":[{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"82657029\",\"name\":\"G. Varol\"},{\"authorId\":\"1828765893\",\"name\":\"Liliane Momeni\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"144863998\",\"name\":\"Neil Fox\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-58621-8_3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58d49e8b79dbc16d8d3ed2bb3e8c96c6f26ab928\",\"title\":\"BSL-1K: Scaling up co-articulated sign language recognition using mouthing cues\",\"url\":\"https://www.semanticscholar.org/paper/58d49e8b79dbc16d8d3ed2bb3e8c96c6f26ab928\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1801.03986\",\"authors\":[{\"authorId\":\"143847408\",\"name\":\"Mingze Xu\"},{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"37649055\",\"name\":\"J. Paden\"},{\"authorId\":\"145943292\",\"name\":\"G. Fox\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":\"10.1109/WACV.2018.00144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35a24f9f2e01edfba8c2e0dad8dc7d53a4def861\",\"title\":\"Multi-task Spatiotemporal Neural Networks for Structured Surface Reconstruction\",\"url\":\"https://www.semanticscholar.org/paper/35a24f9f2e01edfba8c2e0dad8dc7d53a4def861\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"1811.01194\",\"authors\":[{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"40256699\",\"name\":\"M. H. Khan\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"}],\"doi\":\"10.1016/j.cviu.2018.10.003\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2acb0265ad09dd2800c0af4697416c97b86a9738\",\"title\":\"Pushing the boundaries of audiovisual word recognition using Residual Networks and LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/2acb0265ad09dd2800c0af4697416c97b86a9738\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2038514928\",\"name\":\"Ziling Miao\"},{\"authorId\":\"2802283\",\"name\":\"H. Liu\"},{\"authorId\":\"152515346\",\"name\":\"B. Yang\"}],\"doi\":\"10.1109/SMC42975.2020.9283044\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ac6227f1067a83feee43a3d7be640a41f8d041d3\",\"title\":\"Part-Based Lipreading for Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ac6227f1067a83feee43a3d7be640a41f8d041d3\",\"venue\":\"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\",\"year\":2020},{\"arxivId\":\"2009.02110\",\"authors\":[{\"authorId\":\"1752863320\",\"name\":\"Jose A. Gonzalez-Lopez\"},{\"authorId\":\"1403617186\",\"name\":\"Alejandro Gomez-Alanis\"},{\"authorId\":\"1997710923\",\"name\":\"Juan M. Mart\\u00edn Do\\u00f1as\"},{\"authorId\":\"1412663999\",\"name\":\"J. L. Perez-Cordoba\"},{\"authorId\":\"46630656\",\"name\":\"A. M. Gomez\"}],\"doi\":\"10.1109/ACCESS.2020.3026579\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02b75480b6bb368346648e65bd9c722340c37571\",\"title\":\"Silent Speech Interfaces for Speech Restoration: A Review\",\"url\":\"https://www.semanticscholar.org/paper/02b75480b6bb368346648e65bd9c722340c37571\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2008.02516\",\"authors\":[{\"authorId\":\"48211720\",\"name\":\"J. Liu\"},{\"authorId\":\"1500435161\",\"name\":\"Yi Ren\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"145107889\",\"name\":\"Chen Zhang\"},{\"authorId\":\"2422046\",\"name\":\"Baoxing Huai\"},{\"authorId\":\"1390958297\",\"name\":\"J. Yuan\"}],\"doi\":\"10.1145/3394171.3413740\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1af8aa333d7b64417240e7697f15034a3457d86c\",\"title\":\"FastLR: Non-Autoregressive Lipreading Model with Integrate-and-Fire\",\"url\":\"https://www.semanticscholar.org/paper/1af8aa333d7b64417240e7697f15034a3457d86c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2004.09476\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"47940821\",\"name\":\"Hang Zhao\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR42600.2020.01049\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b643a7186c08db9f13d7204f6e5e739f97902e71\",\"title\":\"Music Gesture for Visual Sound Separation\",\"url\":\"https://www.semanticscholar.org/paper/b643a7186c08db9f13d7204f6e5e739f97902e71\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2727313\",\"name\":\"A. Jamaludin\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/s11263-019-01150-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e8cd058ae29c6f60a8750c1df3caa5dc0e99543\",\"title\":\"You Said That?: Synthesising Talking Faces from Audio\",\"url\":\"https://www.semanticscholar.org/paper/0e8cd058ae29c6f60a8750c1df3caa5dc0e99543\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"2006.04946\",\"authors\":[{\"authorId\":\"1729205684\",\"name\":\"Kyongsik Yun\"},{\"authorId\":\"6801275\",\"name\":\"T. Lu\"},{\"authorId\":\"51449322\",\"name\":\"Alexander Huyen\"}],\"doi\":\"10.1117/12.2558157\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9fa7565f3bc3c8f34ebf658b06d2e70b23dba76\",\"title\":\"Transforming unstructured voice and text data into insight for paramedic emergency service using recurrent and convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/b9fa7565f3bc3c8f34ebf658b06d2e70b23dba76\",\"venue\":\"Defense + Commercial Sensing\",\"year\":2020},{\"arxivId\":\"1709.04343\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"2563750\",\"name\":\"Yujiang Wang\"},{\"authorId\":\"8798628\",\"name\":\"Z. Li\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.21437/AVSP.2017-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"acd7bd7932e579ec3641a0191764dc349bfd6e8b\",\"title\":\"End-to-End Audiovisual Fusion with LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/acd7bd7932e579ec3641a0191764dc349bfd6e8b\",\"venue\":\"AVSP\",\"year\":2017},{\"arxivId\":\"1808.00046\",\"authors\":[{\"authorId\":\"2064500\",\"name\":\"Ahsan Adeel\"},{\"authorId\":\"46695361\",\"name\":\"M. Gogate\"},{\"authorId\":\"144664815\",\"name\":\"A. Hussain\"},{\"authorId\":\"4876535\",\"name\":\"W. Whitmer\"}],\"doi\":\"10.1109/tetci.2019.2917039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c72fe6ea37e159216d3fe59cb9893c7ca84c7303\",\"title\":\"Lip-Reading Driven Deep Learning Approach for Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/c72fe6ea37e159216d3fe59cb9893c7ca84c7303\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143819051\",\"name\":\"K. Sun\"},{\"authorId\":\"1772504\",\"name\":\"Chun Yu\"},{\"authorId\":\"10777025\",\"name\":\"Weinan Shi\"},{\"authorId\":\"46458312\",\"name\":\"Lan Liu\"},{\"authorId\":\"1732440\",\"name\":\"Y. Shi\"}],\"doi\":\"10.1145/3242587.3242599\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8294e72a443dc1429ae7111c0ba28c7d1cd67ab4\",\"title\":\"Lip-Interact: Improving Mobile Device Interaction with Silent Speech Commands\",\"url\":\"https://www.semanticscholar.org/paper/8294e72a443dc1429ae7111c0ba28c7d1cd67ab4\",\"venue\":\"UIST\",\"year\":2018},{\"arxivId\":\"1710.01292\",\"authors\":[{\"authorId\":\"3192282\",\"name\":\"Helen L. Bear\"},{\"authorId\":\"145324126\",\"name\":\"Sarah Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ec048f2b3154de16dc4569893dcd559c0b4ff32\",\"title\":\"Visual speech recognition: aligning terminologies for better understanding\",\"url\":\"https://www.semanticscholar.org/paper/3ec048f2b3154de16dc4569893dcd559c0b4ff32\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1416299018\",\"name\":\"Davide Iengo\"},{\"authorId\":\"144759484\",\"name\":\"M. Nappi\"},{\"authorId\":\"49435089\",\"name\":\"S. Ricciardi\"},{\"authorId\":\"1412800330\",\"name\":\"Davide Vanore\"}],\"doi\":\"10.1109/ICIP.2019.8803304\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4ca69796da9f68919e5f1ad0836747d1dbbd02c\",\"title\":\"Dynamic Facial Features for Inherently Safer Face Recognition\",\"url\":\"https://www.semanticscholar.org/paper/e4ca69796da9f68919e5f1ad0836747d1dbbd02c\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410464604\",\"name\":\"A. Fernandez-Lopez\"},{\"authorId\":\"2012978\",\"name\":\"F. Sukno\"}],\"doi\":\"10.1007/978-3-030-12209-6_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"06dd16799c5b939226aee9315340a6547bee4205\",\"title\":\"Optimizing Phoneme-to-Viseme Mapping for Continuous Lip-Reading in Spanish\",\"url\":\"https://www.semanticscholar.org/paper/06dd16799c5b939226aee9315340a6547bee4205\",\"venue\":\"VISIGRAPP\",\"year\":2017},{\"arxivId\":\"1902.07473\",\"authors\":[{\"authorId\":\"2564871\",\"name\":\"Yan-Bo Lin\"},{\"authorId\":\"3312576\",\"name\":\"Yu-Jhe Li\"},{\"authorId\":\"2733735\",\"name\":\"Y. Wang\"}],\"doi\":\"10.1109/ICASSP.2019.8683226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6dad33f2e317cc23abeba8596a4c8a4a13117d54\",\"title\":\"Dual-modality Seq2Seq Network for Audio-visual Event Localization\",\"url\":\"https://www.semanticscholar.org/paper/6dad33f2e317cc23abeba8596a4c8a4a13117d54\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153387571\",\"name\":\"T. Nguyen\"},{\"authorId\":\"153304265\",\"name\":\"C. Nguyen\"},{\"authorId\":\"1390619529\",\"name\":\"Dung T. Nguyen\"},{\"authorId\":\"1779016\",\"name\":\"D. Nguyen\"},{\"authorId\":\"1743136\",\"name\":\"S. Nahavandi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b66e536ad22f4e7b535e82a90d701124b6da2b9e\",\"title\":\"Deep Learning for Deepfakes Creation and Detection\",\"url\":\"https://www.semanticscholar.org/paper/b66e536ad22f4e7b535e82a90d701124b6da2b9e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2808158\",\"name\":\"Necati Cihan Camg\\u00f6z\"},{\"authorId\":\"2417546\",\"name\":\"S. Hadfield\"},{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":\"10.1109/ICCV.2017.332\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a10cd29fec9dca66250dbde19db5e831f7ce6406\",\"title\":\"SubUNets: End-to-End Hand Shape and Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a10cd29fec9dca66250dbde19db5e831f7ce6406\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3839467\",\"name\":\"Toshiki Kikuchi\"},{\"authorId\":\"3057653\",\"name\":\"Yuko Ozasa\"}],\"doi\":\"10.1109/ICASSP.2018.8461853\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e9f113435abe50f2b2fee3f240a3f310491485cc\",\"title\":\"Watch, Listen Once, and Sync: Audio-Visual Synchronization With Multi-Modal Regression Cnn\",\"url\":\"https://www.semanticscholar.org/paper/e9f113435abe50f2b2fee3f240a3f310491485cc\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.31.155\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ec82da82416bb8da8cdf2140c740e1574eaf84f\",\"title\":\"Lip Reading in Profile\",\"url\":\"https://www.semanticscholar.org/paper/8ec82da82416bb8da8cdf2140c740e1574eaf84f\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1584486807\",\"name\":\"Navin Kumar Mudaliar\"},{\"authorId\":\"74578585\",\"name\":\"Kavita T Hegde\"},{\"authorId\":\"39179114\",\"name\":\"Anand Ramesh\"},{\"authorId\":\"32318105\",\"name\":\"V. Patil\"}],\"doi\":\"10.1109/ICCES48766.2020.9137926\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2f0d29f8d8ad5001046515d3ec78b8c48a1885db\",\"title\":\"Visual Speech Recognition: A Deep Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/2f0d29f8d8ad5001046515d3ec78b8c48a1885db\",\"venue\":\"2020 5th International Conference on Communication and Electronics Systems (ICCES)\",\"year\":2020},{\"arxivId\":\"1804.03619\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"2138834\",\"name\":\"Inbar Mosseri\"},{\"authorId\":\"49618488\",\"name\":\"Oran Lang\"},{\"authorId\":\"2112779\",\"name\":\"Tali Dekel\"},{\"authorId\":\"118291142\",\"name\":\"K. Wilson\"},{\"authorId\":\"1639722387\",\"name\":\"Avinatan Hassidim\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"144544291\",\"name\":\"Michael Rubinstein\"}],\"doi\":\"10.1145/3197517.3201357\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b6add50e6be8d4f21e38cca9a154321cad3a4e0\",\"title\":\"Looking to listen at the cocktail party\",\"url\":\"https://www.semanticscholar.org/paper/1b6add50e6be8d4f21e38cca9a154321cad3a4e0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"150092714\",\"name\":\"Edmilson da Silva Morais\"}],\"doi\":\"10.1109/FG.2019.8756589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c9d06ca43fb061e3136d08b74dbe9ea081d17740\",\"title\":\"Improving Viseme Recognition with GAN-based Muti-view Mapping\",\"url\":\"https://www.semanticscholar.org/paper/c9d06ca43fb061e3136d08b74dbe9ea081d17740\",\"venue\":\"FG\",\"year\":2019},{\"arxivId\":\"1807.07860\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"40457380\",\"name\":\"Y. Liu\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33019299\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1816f98e2a4dd54690c2689cf529699d8843e847\",\"title\":\"Talking Face Generation by Adversarially Disentangled Audio-Visual Representation\",\"url\":\"https://www.semanticscholar.org/paper/1816f98e2a4dd54690c2689cf529699d8843e847\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47558089\",\"name\":\"Y. Chen\"},{\"authorId\":\"2113068\",\"name\":\"J. Zhang\"},{\"authorId\":\"49889896\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1734002\",\"name\":\"Yoichi Ochiai\"}],\"doi\":\"10.1007/978-3-030-30712-7_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"498d924c6ff98842d4596430265792ef454ffb51\",\"title\":\"LipSpeaker: Helping Acquired Voice Disorders People Speak Again\",\"url\":\"https://www.semanticscholar.org/paper/498d924c6ff98842d4596430265792ef454ffb51\",\"venue\":\"HCI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115685706\",\"name\":\"Souheil Fenghour\"},{\"authorId\":\"47514164\",\"name\":\"Daqing Chen\"},{\"authorId\":\"6064229\",\"name\":\"Perry Xiao\"}],\"doi\":\"10.1145/3328833.3328845\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"582005cfa57dbf53ccf66097c11dac0f932be376\",\"title\":\"Decoder-Encoder LSTM for Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/582005cfa57dbf53ccf66097c11dac0f932be376\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1708.06767\",\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0ebc58bb5d517db0111f3565c4eb378d93dad908\",\"title\":\"Seeing Through Noise: Speaker Separation and Enhancement using Visually-derived Speech\",\"url\":\"https://www.semanticscholar.org/paper/0ebc58bb5d517db0111f3565c4eb378d93dad908\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1579477104\",\"name\":\"Eslam E. El Maghraby\"},{\"authorId\":\"2126522\",\"name\":\"Amr M. Gody\"}],\"doi\":\"10.19101/ijacr.2019.940134\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ceb3dd09eaf54147e02740b42842095adcbd4f2a\",\"title\":\"Noise robust speech recognition system using multimodal audio-visual approach using different deep learning classification techniques\",\"url\":\"https://www.semanticscholar.org/paper/ceb3dd09eaf54147e02740b42842095adcbd4f2a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.08606\",\"authors\":[{\"authorId\":\"152848162\",\"name\":\"You Jin Kim\"},{\"authorId\":\"1594024908\",\"name\":\"Hee Soo Heo\"},{\"authorId\":\"10437962\",\"name\":\"Soo-Whan Chung\"},{\"authorId\":\"50643194\",\"name\":\"Bong-Jin Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"995fd68acb1ee8c04134be1954cfc4b12f685325\",\"title\":\"End-to-End Lip Synchronisation\",\"url\":\"https://www.semanticscholar.org/paper/995fd68acb1ee8c04134be1954cfc4b12f685325\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.03968\",\"authors\":[{\"authorId\":\"144339857\",\"name\":\"Nilay Shrivastava\"},{\"authorId\":\"51004100\",\"name\":\"Astitwa Saxena\"},{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"2052549\",\"name\":\"Preeti Kaur\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"2712224\",\"name\":\"Debanjan Mahata\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0d5606c0e77ce7a85dadf79a238f4938e29da72\",\"title\":\"MobiVSR: A Visual Speech Recognition Solution for Mobile Devices\",\"url\":\"https://www.semanticscholar.org/paper/f0d5606c0e77ce7a85dadf79a238f4938e29da72\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Linlin Xia\"},{\"authorId\":\"72111341\",\"name\":\"Gang Chen\"},{\"authorId\":\"48670214\",\"name\":\"Xun Xu\"},{\"authorId\":\"1625534225\",\"name\":\"Jiashuo Cui\"},{\"authorId\":\"1488660121\",\"name\":\"Yiping Gao\"}],\"doi\":\"10.1177/1729881420976082\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c356dffe8974a3788c9041821a8e033325e958a7\",\"title\":\"Audiovisual speech recognition: A review and forecast\",\"url\":\"https://www.semanticscholar.org/paper/c356dffe8974a3788c9041821a8e033325e958a7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1901.02212\",\"authors\":[{\"authorId\":\"34659390\",\"name\":\"Umur Aybars Ciftci\"},{\"authorId\":\"3146073\",\"name\":\"Ilke Demir\"}],\"doi\":\"10.1109/TPAMI.2020.3009287\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"e4425f0b540acff10e0ecee1a8a362f584ff4d5c\",\"title\":\"FakeCatcher: Detection of Synthetic Portrait Videos using Biological Signals\",\"url\":\"https://www.semanticscholar.org/paper/e4425f0b540acff10e0ecee1a8a362f584ff4d5c\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797486\",\"name\":\"Hye Yeon Nam\"},{\"authorId\":\"1996388237\",\"name\":\"Iyleah Hernandez\"},{\"authorId\":\"38984724\",\"name\":\"B. Harmon\"}],\"doi\":\"10.1145/3379350.3416137\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c390036caf26e73614b114e1ad917067417902e\",\"title\":\"Unmasked\",\"url\":\"https://www.semanticscholar.org/paper/5c390036caf26e73614b114e1ad917067417902e\",\"venue\":\"UIST\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2831839\",\"name\":\"P. Korshunov\"},{\"authorId\":\"145607451\",\"name\":\"S. Marcel\"}],\"doi\":\"10.1109/ICB45273.2019.8987375\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1273a60c09ea27190d0dcb55e906f9eec14f066\",\"title\":\"Vulnerability assessment and detection of Deepfake videos\",\"url\":\"https://www.semanticscholar.org/paper/d1273a60c09ea27190d0dcb55e906f9eec14f066\",\"venue\":\"2019 International Conference on Biometrics (ICB)\",\"year\":2019},{\"arxivId\":\"1805.11685\",\"authors\":[{\"authorId\":\"30606918\",\"name\":\"George Sterpu\"},{\"authorId\":\"1814175\",\"name\":\"Christian Saam\"},{\"authorId\":\"144686633\",\"name\":\"N. Harte\"}],\"doi\":\"10.1109/ICIP.2018.8451388\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6163457f5eb8b042e17a8c9bb4c153ddfbd3e12e\",\"title\":\"Can DNNs Learn to Lipread Full Sentences?\",\"url\":\"https://www.semanticscholar.org/paper/6163457f5eb8b042e17a8c9bb4c153ddfbd3e12e\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319000\",\"name\":\"Jianguo Wei\"},{\"authorId\":null,\"name\":\"Fan Yang\"},{\"authorId\":\"47540430\",\"name\":\"J. Zhang\"},{\"authorId\":\"2805420\",\"name\":\"Ruiguo Yu\"},{\"authorId\":\"145684947\",\"name\":\"M. Yu\"},{\"authorId\":\"46584998\",\"name\":\"Jianrong Wang\"}],\"doi\":\"10.1109/ICTAI.2018.00155\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b064e5af68fd7fd852c4b81e3442603f99627e75\",\"title\":\"Three-Dimensional Joint Geometric-Physiologic Feature for Lip-Reading\",\"url\":\"https://www.semanticscholar.org/paper/b064e5af68fd7fd852c4b81e3442603f99627e75\",\"venue\":\"2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2018},{\"arxivId\":\"1704.08028\",\"authors\":[{\"authorId\":\"1410464604\",\"name\":\"A. Fernandez-Lopez\"},{\"authorId\":\"145796289\",\"name\":\"Oriol Mart\\u00ednez\"},{\"authorId\":\"2012978\",\"name\":\"F. Sukno\"}],\"doi\":\"10.1109/FG.2017.34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f5d7ff5eb22042e91ab5208795415a9c91ccb3b\",\"title\":\"Towards Estimating the Upper Bound of Visual-Speech Recognition: The Visual Lip-Reading Feasibility Database\",\"url\":\"https://www.semanticscholar.org/paper/2f5d7ff5eb22042e91ab5208795415a9c91ccb3b\",\"venue\":\"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)\",\"year\":2017},{\"arxivId\":\"1808.09825\",\"authors\":[{\"authorId\":\"2064500\",\"name\":\"Ahsan Adeel\"},{\"authorId\":\"46695361\",\"name\":\"M. Gogate\"},{\"authorId\":\"144664815\",\"name\":\"A. Hussain\"}],\"doi\":\"10.1016/j.inffus.2019.08.008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"818052d64c6c085770cd2caa7ef0e866155af883\",\"title\":\"Contextual Audio-Visual Switching For Speech Enhancement in Real-World Environments\",\"url\":\"https://www.semanticscholar.org/paper/818052d64c6c085770cd2caa7ef0e866155af883\",\"venue\":\"Inf. Fusion\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51182843\",\"name\":\"Andrei J\\u00eetaru\"},{\"authorId\":\"1720843365\",\"name\":\"\\u015eeila Abdulamit\"},{\"authorId\":\"1796198\",\"name\":\"B. Ionescu\"}],\"doi\":\"10.1145/3339825.3394932\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc883d3423cde62d85e167e05240d996e41b452a\",\"title\":\"LRRo: a lip reading data set for the under-resourced romanian language\",\"url\":\"https://www.semanticscholar.org/paper/cc883d3423cde62d85e167e05240d996e41b452a\",\"venue\":\"MMSys\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39697129\",\"name\":\"Jochen Hartmann\"},{\"authorId\":\"40424103\",\"name\":\"M. Heitmann\"},{\"authorId\":\"14814784\",\"name\":\"C. Schamp\"},{\"authorId\":\"1692988\",\"name\":\"O. Netzer\"}],\"doi\":\"10.2139/SSRN.3354415\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9eb8009e7c95c9628473b6fb894b48ebca73d2c4\",\"title\":\"The Power of Brand Selfies in Consumer-Generated Brand Imagery\",\"url\":\"https://www.semanticscholar.org/paper/9eb8009e7c95c9628473b6fb894b48ebca73d2c4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1810.00108\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/SLT.2018.8639643\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b553627247acd185fae1e2daff8b155528040be9\",\"title\":\"Audio-Visual Speech Recognition with a Hybrid CTC/Attention Architecture\",\"url\":\"https://www.semanticscholar.org/paper/b553627247acd185fae1e2daff8b155528040be9\",\"venue\":\"2018 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2018},{\"arxivId\":\"2009.14233\",\"authors\":[{\"authorId\":\"1384480816\",\"name\":\"Pingchuan Ma\"},{\"authorId\":null,\"name\":\"Yujiang Wang\"},{\"authorId\":\"145218422\",\"name\":\"J. Shen\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5070c4871691eb53267786c512e84f3c27b354c5\",\"title\":\"Lip-reading with Densely Connected Temporal Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/5070c4871691eb53267786c512e84f3c27b354c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2793787\",\"name\":\"T. Stadelmann\"},{\"authorId\":\"145315755\",\"name\":\"V. Tolkachev\"},{\"authorId\":\"32230111\",\"name\":\"B. Sick\"},{\"authorId\":\"3467993\",\"name\":\"Jan Stampfli\"},{\"authorId\":\"3238279\",\"name\":\"Oliver D\\u00fcrr\"}],\"doi\":\"10.1007/978-3-030-11821-1_12\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"369d882b4f998d9f5ef79b8926d7e56912b50290\",\"title\":\"Beyond ImageNet: Deep Learning in Industrial Practice\",\"url\":\"https://www.semanticscholar.org/paper/369d882b4f998d9f5ef79b8926d7e56912b50290\",\"venue\":\"Applied Data Science\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48282862\",\"name\":\"Xuejuan Chen\"},{\"authorId\":\"1701928\",\"name\":\"Jixiang Du\"},{\"authorId\":\"1419462918\",\"name\":\"Hongbo Zhang\"}],\"doi\":\"10.1007/s11760-019-01630-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"62d04f01a3e91d4304de012263a7f1a265d7b905\",\"title\":\"Lipreading with DenseNet and resBi-LSTM\",\"url\":\"https://www.semanticscholar.org/paper/62d04f01a3e91d4304de012263a7f1a265d7b905\",\"venue\":\"Signal Image Video Process.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"145420448\",\"name\":\"E. Morais\"}],\"doi\":\"10.1109/ICME.2018.8486470\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0710d2df0200af259ea0f7c284f885b81fcb85b7\",\"title\":\"Improving CNN-Based Viseme Recognition Using Synthetic Data\",\"url\":\"https://www.semanticscholar.org/paper/0710d2df0200af259ea0f7c284f885b81fcb85b7\",\"venue\":\"2018 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2018},{\"arxivId\":\"1804.00326\",\"authors\":[{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2018.00879\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c75658b080a9baaac20db39af86016ffa36f6f0\",\"title\":\"Seeing Voices and Hearing Faces: Cross-Modal Biometric Matching\",\"url\":\"https://www.semanticscholar.org/paper/2c75658b080a9baaac20db39af86016ffa36f6f0\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40913232\",\"name\":\"P. Chen\"},{\"authorId\":\"1381434830\",\"name\":\"Alfredo Cuzzocrea\"},{\"authorId\":\"40609330\",\"name\":\"Xiao-Yong Du\"},{\"authorId\":\"144408238\",\"name\":\"Orhun Kara\"},{\"authorId\":\"152244126\",\"name\":\"T. Liu\"},{\"authorId\":\"1731022\",\"name\":\"K. Sivalingam\"},{\"authorId\":\"145514738\",\"name\":\"D. Slezak\"},{\"authorId\":\"1704749\",\"name\":\"T. Washio\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"},{\"authorId\":\"145337089\",\"name\":\"S. Barbosa\"},{\"authorId\":\"47227076\",\"name\":\"G. Wang\"},{\"authorId\":\"134362880\",\"name\":\"Md. Zakirul Alam Bhuiyan\"},{\"authorId\":\"144794379\",\"name\":\"S. D. C. D. Vimercati\"},{\"authorId\":\"1403695497\",\"name\":\"Yizhi Ren Eds\"},{\"authorId\":\"46300991\",\"name\":\"Yizhi Ren\"}],\"doi\":\"10.1007/978-981-15-1304-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cef74856afc04e27aa6ed6cdab2a39725c4d713\",\"title\":\"Dependability in Sensor, Cloud, and Big Data Systems and Applications: 5th International Conference, DependSys 2019, Guangzhou, China, November 12\\u201315, 2019, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/1cef74856afc04e27aa6ed6cdab2a39725c4d713\",\"venue\":\"DependSys\",\"year\":2019},{\"arxivId\":\"2004.08250\",\"authors\":[{\"authorId\":\"30606918\",\"name\":\"George Sterpu\"},{\"authorId\":\"1814175\",\"name\":\"Christian Saam\"},{\"authorId\":\"34530970\",\"name\":\"N. Harte\"}],\"doi\":\"10.1109/TASLP.2020.2980436\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"08b75f2df56b0ddf1fa054931d4b20027fe6f791\",\"title\":\"How to Teach DNNs to Pay Attention to the Visual Modality in Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/08b75f2df56b0ddf1fa054931d4b20027fe6f791\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":\"1803.04988\",\"authors\":[{\"authorId\":\"143885335\",\"name\":\"Kai Xu\"},{\"authorId\":\"49620929\",\"name\":\"Dawei Li\"},{\"authorId\":\"48289425\",\"name\":\"N. Cassimatis\"},{\"authorId\":\"1709719\",\"name\":\"Xiaolong Wang\"}],\"doi\":\"10.1109/FG.2018.00088\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dfa043929dbf123438cbbc1648b0d2252010f34a\",\"title\":\"LCANet: End-to-End Lipreading with Cascaded Attention-CTC\",\"url\":\"https://www.semanticscholar.org/paper/dfa043929dbf123438cbbc1648b0d2252010f34a\",\"venue\":\"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)\",\"year\":2018},{\"arxivId\":\"1906.06301\",\"authors\":[{\"authorId\":\"2160245\",\"name\":\"Konstantinos Vougioukas\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.21437/interspeech.2019-1445\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e17330208d553f184ab91e1fc63fa18a087d9d8a\",\"title\":\"Video-Driven Speech Reconstruction using Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/e17330208d553f184ab91e1fc63fa18a087d9d8a\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3431949\",\"name\":\"Chenhan Xu\"},{\"authorId\":\"48459471\",\"name\":\"Zhengxiong Li\"},{\"authorId\":\"4456540\",\"name\":\"H. Zhang\"},{\"authorId\":\"143944127\",\"name\":\"Aditya Singh Rathore\"},{\"authorId\":\"46382453\",\"name\":\"Huining Li\"},{\"authorId\":\"143997480\",\"name\":\"C. Song\"},{\"authorId\":\"28014924\",\"name\":\"K. Wang\"},{\"authorId\":\"143836292\",\"name\":\"W. Xu\"}],\"doi\":\"10.1145/3307334.3326073\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"67f635ecee64c85bb6edbe8506bf483607b594e4\",\"title\":\"WaveEar: Exploring a mmWave-based Noise-resistant Speech Sensing for Voice-User Interface\",\"url\":\"https://www.semanticscholar.org/paper/67f635ecee64c85bb6edbe8506bf483607b594e4\",\"venue\":\"MobiSys\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66520980\",\"name\":\"Yasufumi Moriya\"},{\"authorId\":\"145981245\",\"name\":\"Gareth J. F. Jones\"}],\"doi\":\"10.1109/ICASSP.2019.8683724\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d8c8d5a31764e348770ab85de4b5fa4b4d261bc7\",\"title\":\"Multimodal Speaker Adaptation of Acoustic Model and Language Model for Asr Using Speaker Face Embedding\",\"url\":\"https://www.semanticscholar.org/paper/d8c8d5a31764e348770ab85de4b5fa4b4d261bc7\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1809.00496\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a46174aa635759070984ed7062c9402695bce830\",\"title\":\"LRS3-TED: a large-scale dataset for visual speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/a46174aa635759070984ed7062c9402695bce830\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49970481\",\"name\":\"Z. Li\"},{\"authorId\":\"152465306\",\"name\":\"Guokun Chen\"},{\"authorId\":\"38144106\",\"name\":\"Tianxu Zhang\"}],\"doi\":\"10.1109/ACCESS.2019.2939152\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"68c82ad323d85a51b38a362c74c5db44e8e5d75a\",\"title\":\"Temporal Attention Networks for Multitemporal Multisensor Crop Classification\",\"url\":\"https://www.semanticscholar.org/paper/68c82ad323d85a51b38a362c74c5db44e8e5d75a\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1703.10893\",\"authors\":[{\"authorId\":\"8726117\",\"name\":\"Jen-Cheng Hou\"},{\"authorId\":\"2426246\",\"name\":\"S. Wang\"},{\"authorId\":\"46329677\",\"name\":\"Ying-Hui Lai\"},{\"authorId\":\"145403933\",\"name\":\"Y. Tsao\"},{\"authorId\":\"144600098\",\"name\":\"Hsiu-Wen Chang\"},{\"authorId\":\"46506453\",\"name\":\"Hsin-Min Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a58ba304ca0c37d94c7227feeeffb600a492d902\",\"title\":\"Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/a58ba304ca0c37d94c7227feeeffb600a492d902\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2560252\",\"name\":\"K. Palecek\"}],\"doi\":\"10.1007/978-3-319-66429-3_77\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ee7229d3173dc4c621188d60fe25addc99985e5\",\"title\":\"Utilizing Lipreading in Large Vocabulary Continuous Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5ee7229d3173dc4c621188d60fe25addc99985e5\",\"venue\":\"SPECOM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48797555\",\"name\":\"Abhishek Jha\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1007/s00138-019-01006-y\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2e9a6c958b8e19bca9ad109c6c026a087e73340c\",\"title\":\"Spotting words in silent speech videos: a retrieval-based approach\",\"url\":\"https://www.semanticscholar.org/paper/2e9a6c958b8e19bca9ad109c6c026a087e73340c\",\"venue\":\"Machine Vision and Applications\",\"year\":2019},{\"arxivId\":\"1912.05396\",\"authors\":[{\"authorId\":\"1454226662\",\"name\":\"Aiham Taleb\"},{\"authorId\":\"38206541\",\"name\":\"C. Lippert\"},{\"authorId\":\"35660331\",\"name\":\"T. Klein\"},{\"authorId\":\"1848946\",\"name\":\"Moin Nabi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b94a3bfe8ca48855e4b4c6bcab5b051197052d9e\",\"title\":\"Multimodal Self-Supervised Learning for Medical Image Analysis\",\"url\":\"https://www.semanticscholar.org/paper/b94a3bfe8ca48855e4b4c6bcab5b051197052d9e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1712.09382\",\"authors\":[{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"},{\"authorId\":\"32273391\",\"name\":\"L. Dery\"},{\"authorId\":\"1411184751\",\"name\":\"Hayden Schoen\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\"}],\"doi\":\"10.1109/CVPR.2018.00790\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6d60aaad68fa78c914ee34c26bceab033a88622\",\"title\":\"Audio to Body Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/c6d60aaad68fa78c914ee34c26bceab033a88622\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1904.01954\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"2563750\",\"name\":\"Yujiang Wang\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"8798628\",\"name\":\"Z. Li\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1016/j.patrec.2020.01.022\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c785dcf80ae0f8d5fab1960ef2afc6cefcebd8a\",\"title\":\"End-to-End Visual Speech Recognition for Small-Scale Datasets\",\"url\":\"https://www.semanticscholar.org/paper/4c785dcf80ae0f8d5fab1960ef2afc6cefcebd8a\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48211981\",\"name\":\"Jun-Wei Liu\"},{\"authorId\":\"121504262\",\"name\":\"Hung-Yi Lin\"},{\"authorId\":\"46843792\",\"name\":\"Yu-fen Huang\"},{\"authorId\":\"8665310\",\"name\":\"H. Kao\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054463\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6b055efd15bca7604ec35806f07092bfa16632cb\",\"title\":\"Body Movement Generation for Expressive Violin Performance Applying Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6b055efd15bca7604ec35806f07092bfa16632cb\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390024605\",\"name\":\"Ziad Al-Halah\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c0c7b90e0c7badaa95924530cb50d86444010ff\",\"title\":\"Semantic Attributes for Transfer Learning in Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6c0c7b90e0c7badaa95924530cb50d86444010ff\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152412578\",\"name\":\"Fei Tao\"},{\"authorId\":\"2106794\",\"name\":\"C. Busso\"}],\"doi\":\"10.1109/TMM.2020.2975922\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"76aae70ca8edec185507b3af220dcb5a0aac1392\",\"title\":\"End-to-End Audiovisual Speech Recognition System With Multitask Learning\",\"url\":\"https://www.semanticscholar.org/paper/76aae70ca8edec185507b3af220dcb5a0aac1392\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2021},{\"arxivId\":\"2003.06439\",\"authors\":[{\"authorId\":\"6191785\",\"name\":\"Xing Zhao\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"634daa582ab0736e7c24a627b96db4cdefb106f1\",\"title\":\"Mutual Information Maximization for Effective Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/634daa582ab0736e7c24a627b96db4cdefb106f1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2560252\",\"name\":\"K. Palecek\"}],\"doi\":\"10.1007/978-3-319-64206-2_49\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78fe3772eacd41e9ae65599a99869a5b26688982\",\"title\":\"Spatiotemporal Convolutional Features for Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/78fe3772eacd41e9ae65599a99869a5b26688982\",\"venue\":\"TSD\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2034276230\",\"name\":\"Maxalmina\"},{\"authorId\":\"2034274644\",\"name\":\"Satria Kahfi\"},{\"authorId\":\"9308183\",\"name\":\"K. N. Ramadhani\"},{\"authorId\":\"9347718\",\"name\":\"Anditya Arifianto\"}],\"doi\":\"10.1109/IC2IE50715.2020.9274562\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc3d507bb0926654ad9c3fe38c3edd481bca072c\",\"title\":\"Lip Motion Recognition for Indonesian Vowel Phonemes Using 3D Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cc3d507bb0926654ad9c3fe38c3edd481bca072c\",\"venue\":\"2020 3rd International Conference on Computer and Informatics Engineering (IC2IE)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2560252\",\"name\":\"K. Palecek\"}],\"doi\":\"10.1007/s12193-018-0266-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d0aa189ab443844f62a7aa973666556ceb9cd79b\",\"title\":\"Experimenting with lipreading for large vocabulary continuous speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/d0aa189ab443844f62a7aa973666556ceb9cd79b\",\"venue\":\"Journal on Multimodal User Interfaces\",\"year\":2018},{\"arxivId\":\"2009.09561\",\"authors\":[{\"authorId\":\"1652999406\",\"name\":\"Hang Chen\"},{\"authorId\":\"1515709515\",\"name\":\"Jun Du\"},{\"authorId\":\"1943030\",\"name\":\"Y. Hu\"},{\"authorId\":\"1860774\",\"name\":\"Li-Rong Dai\"},{\"authorId\":\"2207938\",\"name\":\"B. Yin\"},{\"authorId\":\"9391905\",\"name\":\"Chin-Hui Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"654a07adfe33dda9b336f91a79a2495da6bf5ee9\",\"title\":\"Correlating Subword Articulation with Lip Shapes for Embedding Aware Audio-Visual Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/654a07adfe33dda9b336f91a79a2495da6bf5ee9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2858806\",\"name\":\"D. Esparza\"},{\"authorId\":\"3161727\",\"name\":\"Juan R. Terven\"},{\"authorId\":\"144539063\",\"name\":\"A. Romero\"},{\"authorId\":\"1390029803\",\"name\":\"Ana M. Herrera-Navarro\"}],\"doi\":\"10.1007/978-3-030-33749-0_36\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4bc3eeb6f0c200fcd63daa50da1465bcc330cc4e\",\"title\":\"Audio-Visual Database for Spanish-Based Speech Recognition Systems\",\"url\":\"https://www.semanticscholar.org/paper/4bc3eeb6f0c200fcd63daa50da1465bcc330cc4e\",\"venue\":\"MICAI\",\"year\":2019},{\"arxivId\":\"1904.08248\",\"authors\":[{\"authorId\":\"32246134\",\"name\":\"Luca Pasa\"},{\"authorId\":\"80442851\",\"name\":\"Giovanni Morrone\"},{\"authorId\":\"1749312\",\"name\":\"Leonardo Badino\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054697\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ae87e4469c19dd47bd440a21865c0ab162142499\",\"title\":\"An Analysis of Speech Enhancement and Recognition Losses in Limited Resources Multi-Talker Single Channel Audio-Visual ASR\",\"url\":\"https://www.semanticscholar.org/paper/ae87e4469c19dd47bd440a21865c0ab162142499\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39089342\",\"name\":\"Miroslav Hlav\\u00e1c\"},{\"authorId\":\"2601929\",\"name\":\"I. Gruber\"},{\"authorId\":\"2851907\",\"name\":\"M. Zelezn\\u00fd\"},{\"authorId\":\"145191867\",\"name\":\"Alexey Karpov\"}],\"doi\":\"10.1007/978-3-319-99579-3_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d04e17ead441c874c7b9da8c57df5296ea068234\",\"title\":\"LipsID Using 3D Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/d04e17ead441c874c7b9da8c57df5296ea068234\",\"venue\":\"SPECOM\",\"year\":2018},{\"arxivId\":\"1811.03865\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"2934336\",\"name\":\"Lo\\u00efc Barrault\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":\"10.1109/ICASSP.2019.8682750\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"101b39c530834a8063d033c38d3c6d80dfcfced0\",\"title\":\"Multimodal Grounding for Sequence-to-sequence Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/101b39c530834a8063d033c38d3c6d80dfcfced0\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3034487\",\"name\":\"K. Thangthai\"},{\"authorId\":\"144439756\",\"name\":\"R. Harvey\"}],\"doi\":\"10.21437/INTERSPEECH.2017-106\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"139f53c9371dc76bebd708b862f08ef5a1a8f3b5\",\"title\":\"Improving Computer Lipreading via DNN Sequence Discriminative Training Techniques\",\"url\":\"https://www.semanticscholar.org/paper/139f53c9371dc76bebd708b862f08ef5a1a8f3b5\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":null,\"name\":\"joon Amir Jamaludin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b24ef13fc9a51a9892f164bc142ffefc0b7a8ee\",\"title\":\"CHUNG , JAMALUDIN , ZISSERMAN : YOU SAID THAT ? 1 You said that ?\",\"url\":\"https://www.semanticscholar.org/paper/5b24ef13fc9a51a9892f164bc142ffefc0b7a8ee\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115427394\",\"name\":\"Elham Ideli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2e6b1286b872d7739c4a1acb61c177f1940eb93\",\"title\":\"Audio-visual speech processing using deep learning techniques\",\"url\":\"https://www.semanticscholar.org/paper/a2e6b1286b872d7739c4a1acb61c177f1940eb93\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145815850\",\"name\":\"Jie Hu\"},{\"authorId\":\"152148573\",\"name\":\"L. Shen\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"152274574\",\"name\":\"Gang Sun\"},{\"authorId\":\"145344139\",\"name\":\"Enhua Wu\"}],\"doi\":\"10.1109/TPAMI.2019.2913372\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df67d46e78aae0d2fccfb6212d101a342259c01b\",\"title\":\"Squeeze-and-Excitation Networks\",\"url\":\"https://www.semanticscholar.org/paper/df67d46e78aae0d2fccfb6212d101a342259c01b\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1710.11201\",\"authors\":[{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"}],\"doi\":\"10.1109/ICASSP.2018.8461347\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"cc0513eb2647af223c0670c5d335c7ba91185635\",\"title\":\"Deep Word Embeddings for Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/cc0513eb2647af223c0670c5d335c7ba91185635\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3229776\",\"name\":\"Dimitris Kastaniotis\"},{\"authorId\":\"24336685\",\"name\":\"Dimitrios Tsourounis\"},{\"authorId\":\"1769294\",\"name\":\"S. Fotopoulos\"}],\"doi\":\"10.1109/CISP-BMEI51763.2020.9263634\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eebb830e66c4172cf7f2b376d9804d4da7752aaa\",\"title\":\"Lip Reading modeling with Temporal Convolutional Networks for medical support applications\",\"url\":\"https://www.semanticscholar.org/paper/eebb830e66c4172cf7f2b376d9804d4da7752aaa\",\"venue\":\"2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1474544125\",\"name\":\"A. H. Kulkarni\"},{\"authorId\":\"30504968\",\"name\":\"D. Kirange\"}],\"doi\":\"10.1109/icccnt45670.2019.8944628\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"120c207a68cfd8480d491452a2afea1377f8d073\",\"title\":\"Artificial Intelligence: A Survey on Lip-Reading Techniques\",\"url\":\"https://www.semanticscholar.org/paper/120c207a68cfd8480d491452a2afea1377f8d073\",\"venue\":\"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410464604\",\"name\":\"A. Fernandez-Lopez\"},{\"authorId\":\"1919706\",\"name\":\"A. Karaali\"},{\"authorId\":\"34530970\",\"name\":\"N. Harte\"},{\"authorId\":\"46745494\",\"name\":\"F. Sukno\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053299\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73fa26350abef9907e90334246a3b83faf5fb638\",\"title\":\"Cogans For Unsupervised Visual Speech Adaptation To New Speakers\",\"url\":\"https://www.semanticscholar.org/paper/73fa26350abef9907e90334246a3b83faf5fb638\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2027003826\",\"name\":\"Mingfeng Hao\"},{\"authorId\":\"26957200\",\"name\":\"Mutallip Mamut\"},{\"authorId\":\"2027003608\",\"name\":\"Nurbiya Yadikar\"},{\"authorId\":\"2119595\",\"name\":\"A. Aysa\"},{\"authorId\":\"1869945\",\"name\":\"K. Ubul\"}],\"doi\":\"10.1109/ACCESS.2020.3036865\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3014cc7546ad2c35d47fd6627233d92d7ae40a85\",\"title\":\"A Survey of Research on Lipreading Technology\",\"url\":\"https://www.semanticscholar.org/paper/3014cc7546ad2c35d47fd6627233d92d7ae40a85\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40569867\",\"name\":\"X. Zhang\"},{\"authorId\":\"7352351\",\"name\":\"Haigang Gong\"},{\"authorId\":\"2978106\",\"name\":\"Xili Dai\"},{\"authorId\":null,\"name\":\"Fan Yang\"},{\"authorId\":\"2023550\",\"name\":\"Nianbo Liu\"},{\"authorId\":\"145111944\",\"name\":\"Ming Liu\"}],\"doi\":\"10.1609/AAAI.V33I01.33019211\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"46a1611d41e06f1ca069d5c4b5a5f068e44ac1b4\",\"title\":\"Understanding Pictograph with Facial Features: End-to-End Sentence-Level Lip Reading of Chinese\",\"url\":\"https://www.semanticscholar.org/paper/46a1611d41e06f1ca069d5c4b5a5f068e44ac1b4\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1812.06071\",\"authors\":[{\"authorId\":\"3451652\",\"name\":\"Naji Khosravan\"},{\"authorId\":\"2599451\",\"name\":\"Shervin Ardeshir\"},{\"authorId\":\"3246780\",\"name\":\"Rohit Puri\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2b1cf0f018d4eeca84236347cf25e1b66fbbc61\",\"title\":\"On Attention Modules for Audio-Visual Synchronization\",\"url\":\"https://www.semanticscholar.org/paper/d2b1cf0f018d4eeca84236347cf25e1b66fbbc61\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":\"1812.02863\",\"authors\":[{\"authorId\":\"31357678\",\"name\":\"Jianfeng Chi\"},{\"authorId\":\"144275036\",\"name\":\"E. Owusu\"},{\"authorId\":\"8135633\",\"name\":\"Xuwang Yin\"},{\"authorId\":\"145381969\",\"name\":\"T. Yu\"},{\"authorId\":\"144333684\",\"name\":\"William Chan\"},{\"authorId\":\"2331637\",\"name\":\"P. Tague\"},{\"authorId\":\"144966427\",\"name\":\"Yuan Tian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"218aa551e3947876e746e6de4b2b5b144885bdf3\",\"title\":\"Privacy Partitioning: Protecting User Data During the Deep Learning Inference Phase\",\"url\":\"https://www.semanticscholar.org/paper/218aa551e3947876e746e6de4b2b5b144885bdf3\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1907.00477\",\"authors\":[{\"authorId\":\"150324514\",\"name\":\"Tejas Srinivasan\"},{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c91972b5c192c0e645a540912e4c93100f111ba\",\"title\":\"Analyzing Utility of Visual Context in Multimodal Speech Recognition Under Noisy Conditions\",\"url\":\"https://www.semanticscholar.org/paper/8c91972b5c192c0e645a540912e4c93100f111ba\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144340651\",\"name\":\"D. G\\u00f3mez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ff0810f0b3ac90a06e36a7a6ffd19b7b42be60a5\",\"title\":\"Lectura de labios en im\\u00e1genes de v\\u00eddeo\",\"url\":\"https://www.semanticscholar.org/paper/ff0810f0b3ac90a06e36a7a6ffd19b7b42be60a5\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1906.02112\",\"authors\":[{\"authorId\":\"12558026\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.21437/interspeech.2019-2726\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c56ce6aad3f06e9113de910a2bc31f4ef8a724a0\",\"title\":\"Investigating the Lombard Effect Influence on End-to-End Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/c56ce6aad3f06e9113de910a2bc31f4ef8a724a0\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471698230\",\"name\":\"Ayushi Rastogi\"},{\"authorId\":\"1471613944\",\"name\":\"Ritika Agarwal\"},{\"authorId\":\"1471722416\",\"name\":\"Vishakha Gupta\"},{\"authorId\":\"3169363\",\"name\":\"J. Dhar\"},{\"authorId\":\"1760974\",\"name\":\"M. Bhattacharya\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"708fc4903604fbcdfe0505967ee58530f787852d\",\"title\":\"LRNeuNet: An attention based deep architecture for lipreading from multitudinous sized videos\",\"url\":\"https://www.semanticscholar.org/paper/708fc4903604fbcdfe0505967ee58530f787852d\",\"venue\":\"2019 International Conference on Computing, Power and Communication Technologies (GUCON)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"34604467\",\"name\":\"Rohit Jain\"},{\"authorId\":\"66287688\",\"name\":\"Khwaja Mohd. Salik\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"144809527\",\"name\":\"Roger Zimmermann\"},{\"authorId\":\"144221742\",\"name\":\"Yifang Yin\"}],\"doi\":\"10.1109/ISM.2018.00-19\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dfad754db0d4f0a57c73a931ce86da714793885d\",\"title\":\"MyLipper: A Personalized System for Speech Reconstruction using Multi-view Visual Feeds\",\"url\":\"https://www.semanticscholar.org/paper/dfad754db0d4f0a57c73a931ce86da714793885d\",\"venue\":\"2018 IEEE International Symposium on Multimedia (ISM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2802283\",\"name\":\"H. Liu\"},{\"authorId\":\"48354992\",\"name\":\"Zhengyan Chen\"},{\"authorId\":\"114896414\",\"name\":\"W. Shi\"}],\"doi\":\"10.1109/ICIP40778.2020.9190894\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b0c6adad4daa66127059f114ac7fe880f2cdf1d3\",\"title\":\"Robust Audio-Visual Mandarin Speech Recognition Based On Adaptive Decision Fusion And Tone Features\",\"url\":\"https://www.semanticscholar.org/paper/b0c6adad4daa66127059f114ac7fe880f2cdf1d3\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":\"2001.08740\",\"authors\":[{\"authorId\":\"2299381\",\"name\":\"Fanyi Xiao\"},{\"authorId\":\"144756076\",\"name\":\"Y. Lee\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"153652147\",\"name\":\"J. Malik\"},{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80685dc522d30b18f3feb97d6c977f71fa746325\",\"title\":\"Audiovisual SlowFast Networks for Video Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80685dc522d30b18f3feb97d6c977f71fa746325\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50394552\",\"name\":\"Rui Lu\"},{\"authorId\":\"72028302\",\"name\":\"Zhiyao Duan\"},{\"authorId\":\"14966740\",\"name\":\"Changshui Zhang\"}],\"doi\":\"10.1109/TASLP.2019.2928140\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"90f8b5d87c41230c691e6a243aef9d62063522b0\",\"title\":\"Audio\\u2013Visual Deep Clustering for Speech Separation\",\"url\":\"https://www.semanticscholar.org/paper/90f8b5d87c41230c691e6a243aef9d62063522b0\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2028898625\",\"name\":\"Mingfeng Hao\"},{\"authorId\":\"2028900009\",\"name\":\"Mutelep Mamut\"},{\"authorId\":\"1869945\",\"name\":\"K. Ubul\"}],\"doi\":\"10.1145/3421558.3421563\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"562b845c63b44d5dd695e60c35716034835824fb\",\"title\":\"A Survey of Lipreading Methods Based on Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/562b845c63b44d5dd695e60c35716034835824fb\",\"venue\":\"ICIP 2020\",\"year\":2020},{\"arxivId\":\"2010.00734\",\"authors\":[{\"authorId\":\"2739353\",\"name\":\"Srinivas Parthasarathy\"},{\"authorId\":\"1734989\",\"name\":\"Shiva Sundaram\"}],\"doi\":\"10.1145/3395035.3425202\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12a60c5fe86fbe66c452520991669a4a011ea3c6\",\"title\":\"Training Strategies to Handle Missing Modalities for Audio-Visual Expression Recognition\",\"url\":\"https://www.semanticscholar.org/paper/12a60c5fe86fbe66c452520991669a4a011ea3c6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.05592\",\"authors\":[{\"authorId\":\"145764891\",\"name\":\"Bo Xu\"},{\"authorId\":\"102517285\",\"name\":\"Cheng Lu\"},{\"authorId\":\"49813886\",\"name\":\"Yandong Guo\"},{\"authorId\":\"2044516\",\"name\":\"J. Wang\"}],\"doi\":\"10.1109/CVPR42600.2020.01444\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1a75b57ecbc140553b4dc2ce2337ed78786da338\",\"title\":\"Discriminative Multi-Modality Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1a75b57ecbc140553b4dc2ce2337ed78786da338\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.04298\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"47792365\",\"name\":\"Y. Asano\"},{\"authorId\":\"145891577\",\"name\":\"Ruth Fong\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ddbaa34c574124a91fa3bc217e232e17668e84c\",\"title\":\"Multi-modal Self-Supervision from Generalized Data Transformations\",\"url\":\"https://www.semanticscholar.org/paper/8ddbaa34c574124a91fa3bc217e232e17668e84c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"146935198\",\"name\":\"Harshit Singhania\"}],\"doi\":\"10.22214/ijraset.2020.4025\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a0dd9611e9f82c0c5f50dd896780050eb340808\",\"title\":\"Automated Lip Reading using Word Level Sentence Prediction based on LSTMs and Attention\",\"url\":\"https://www.semanticscholar.org/paper/5a0dd9611e9f82c0c5f50dd896780050eb340808\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":\"10.1145/3372278.3390740\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60fb047e99e4ccdcd501767364db7bd04ac2743f\",\"title\":\"One Perceptron to Rule Them All: Language, Vision, Audio and Speech\",\"url\":\"https://www.semanticscholar.org/paper/60fb047e99e4ccdcd501767364db7bd04ac2743f\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":\"1904.03760\",\"authors\":[{\"authorId\":\"49387520\",\"name\":\"J. Wu\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"},{\"authorId\":\"2213494\",\"name\":\"S. Zhang\"},{\"authorId\":\"92896134\",\"name\":\"Lianwu Chen\"},{\"authorId\":\"143872260\",\"name\":\"M. Yu\"},{\"authorId\":\"144206968\",\"name\":\"L. Xie\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"}],\"doi\":\"10.1109/ASRU46091.2019.9003983\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"aba58caeac0e6ac8d8ade4c7b45a173fd0d116e2\",\"title\":\"Time Domain Audio Visual Speech Separation\",\"url\":\"https://www.semanticscholar.org/paper/aba58caeac0e6ac8d8ade4c7b45a173fd0d116e2\",\"venue\":\"2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32246134\",\"name\":\"Luca Pasa\"},{\"authorId\":\"80442851\",\"name\":\"Giovanni Morrone\"},{\"authorId\":\"1749312\",\"name\":\"Leonardo Badino\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ae87e4469c19dd47bd440a21865c0ab162142499\",\"title\":\"Joined Audio-Visual Speech Enhancement and Recognition in the Cocktail Party: The Tug Of War Between Enhancement and Recognition Losses\",\"url\":\"https://www.semanticscholar.org/paper/ae87e4469c19dd47bd440a21865c0ab162142499\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145798291\",\"name\":\"Hang Zhou\"},{\"authorId\":\"40457380\",\"name\":\"Yu Liu\"},{\"authorId\":\"3243969\",\"name\":\"Ziwei Liu\"},{\"authorId\":\"144389949\",\"name\":\"Ping Luo\"},{\"authorId\":\"31843833\",\"name\":\"Xiaogang Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c79a853f7beb5726506c358b5fd1a1b7ce12aac\",\"title\":\"Word ID Person ID pid information audio wid information visual wid information wid information wid adversarial against pid pid adversarial against wid wid\",\"url\":\"https://www.semanticscholar.org/paper/9c79a853f7beb5726506c358b5fd1a1b7ce12aac\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.04917\",\"authors\":[{\"authorId\":\"46317391\",\"name\":\"Y. Zhao\"},{\"authorId\":\"152287816\",\"name\":\"Rui Xu\"},{\"authorId\":\"1727111\",\"name\":\"Mingli Song\"}],\"doi\":\"10.1145/3338533.3366579\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"659bd234ebbd82a5065f8f8594fe7b272de1b47a\",\"title\":\"A Cascade Sequence-to-Sequence Model for Chinese Mandarin Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/659bd234ebbd82a5065f8f8594fe7b272de1b47a\",\"venue\":\"MMAsia\",\"year\":2019},{\"arxivId\":\"2007.10629\",\"authors\":[{\"authorId\":\"1694704\",\"name\":\"Jiasong Wu\"},{\"authorId\":\"47268472\",\"name\":\"T. Li\"},{\"authorId\":\"2461136\",\"name\":\"Youyong Kong\"},{\"authorId\":\"1803944\",\"name\":\"Guanyu Yang\"},{\"authorId\":\"1684465\",\"name\":\"L. Senhadji\"},{\"authorId\":\"144305249\",\"name\":\"H. Shu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fec5ca52ef0ff8ba395077479aef60b9f780b33d\",\"title\":\"SLNSpeech: solving extended speech separation problem by the help of sign language\",\"url\":\"https://www.semanticscholar.org/paper/fec5ca52ef0ff8ba395077479aef60b9f780b33d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51379429\",\"name\":\"Q. Dang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"45ac5acb2e79b22f3149b4f54a861c8b4614f588\",\"title\":\"Trust assessment in large-scale collaborative systems\",\"url\":\"https://www.semanticscholar.org/paper/45ac5acb2e79b22f3149b4f54a861c8b4614f588\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410464604\",\"name\":\"A. Fernandez-Lopez\"},{\"authorId\":\"2012978\",\"name\":\"F. Sukno\"}],\"doi\":\"10.1016/j.imavis.2018.07.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"938717d66533d900abac7451e15c1f322e78db81\",\"title\":\"Survey on automatic lip-reading in the era of deep learning\",\"url\":\"https://www.semanticscholar.org/paper/938717d66533d900abac7451e15c1f322e78db81\",\"venue\":\"Image Vis. Comput.\",\"year\":2018},{\"arxivId\":\"2004.05830\",\"authors\":[{\"authorId\":\"81678900\",\"name\":\"Hyeong-Seok Choi\"},{\"authorId\":\"24100383\",\"name\":\"Chang-Dae Park\"},{\"authorId\":\"34674393\",\"name\":\"K. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b44c4d94d2b1ce39ccf94677d69085a3bf5b2ff3\",\"title\":\"From Inference to Generation: End-to-end Fully Self-supervised Generation of Human Face from Speech\",\"url\":\"https://www.semanticscholar.org/paper/b44c4d94d2b1ce39ccf94677d69085a3bf5b2ff3\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46621968\",\"name\":\"N. Kimura\"},{\"authorId\":\"2870217\",\"name\":\"K. Hayashi\"},{\"authorId\":\"1685962\",\"name\":\"J. Rekimoto\"}],\"doi\":\"10.1145/3399715.3399852\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e824235447608162a0cd8d198d780fd0d6fa2a2f\",\"title\":\"TieLent: A Casual Neck-Mounted Mouth Capturing Device for Silent Speech Interaction\",\"url\":\"https://www.semanticscholar.org/paper/e824235447608162a0cd8d198d780fd0d6fa2a2f\",\"venue\":\"AVI\",\"year\":2020},{\"arxivId\":\"1809.04553\",\"authors\":[{\"authorId\":\"50556355\",\"name\":\"Fei Tao\"},{\"authorId\":\"2106794\",\"name\":\"C. Busso\"}],\"doi\":\"10.1016/J.SPECOM.2019.07.003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed8ea91f5552d80b9cfac83ae7b564ef97202d4f\",\"title\":\"End-to-end Audiovisual Speech Activity Detection with Bimodal Recurrent Neural Models\",\"url\":\"https://www.semanticscholar.org/paper/ed8ea91f5552d80b9cfac83ae7b564ef97202d4f\",\"venue\":\"Speech Commun.\",\"year\":2019},{\"arxivId\":\"1803.10404\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"48458657\",\"name\":\"Zhiheng Li\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01234-2_32\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"title\":\"Lip Movements Generation at a Glance\",\"url\":\"https://www.semanticscholar.org/paper/d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2001.04001\",\"authors\":[{\"authorId\":\"1482544310\",\"name\":\"Stefania Fresca\"},{\"authorId\":\"2924130\",\"name\":\"Luca Ded\\u00e8\"},{\"authorId\":\"2124095\",\"name\":\"A. Manzoni\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"05e340e290bdc236249f2be0c7749f6c9bebc483\",\"title\":\"A comprehensive deep learning-based approach to reduced order modeling of nonlinear time-dependent parametrized PDEs\",\"url\":\"https://www.semanticscholar.org/paper/05e340e290bdc236249f2be0c7749f6c9bebc483\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1703.04105\",\"authors\":[{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"}],\"doi\":\"10.21437/INTERSPEECH.2017-85\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4afdb836301c0233bb8cf0d8a33212ac0c113381\",\"title\":\"Combining Residual Networks with LSTMs for Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/4afdb836301c0233bb8cf0d8a33212ac0c113381\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":\"2012.02961\",\"authors\":[{\"authorId\":\"49551025\",\"name\":\"Madina Abdrakhmanova\"},{\"authorId\":\"117642390\",\"name\":\"Askat Kuzdeuov\"},{\"authorId\":\"5882345\",\"name\":\"S. Jarju\"},{\"authorId\":\"2439648\",\"name\":\"Yerbolat Khassanov\"},{\"authorId\":\"145806130\",\"name\":\"Michael Lewis\"},{\"authorId\":\"145907578\",\"name\":\"H. A. Varol\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"553f5ab938cf1096308b955f45201fabe6c450b3\",\"title\":\"SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with Visual and Thermal Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/553f5ab938cf1096308b955f45201fabe6c450b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03983\",\"authors\":[{\"authorId\":\"1557300901\",\"name\":\"Mingshuang Luo\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0872f4a6e14c6da98424e6daefe4d9b626ba4d3d\",\"title\":\"Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading\",\"url\":\"https://www.semanticscholar.org/paper/0872f4a6e14c6da98424e6daefe4d9b626ba4d3d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2503971\",\"name\":\"M. Mizrahi\"}],\"doi\":\"10.1007/S10503-017-9434-X\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6684ccb530f1f36f591145062d5545bf19651b7d\",\"title\":\"Arguments from Expert Opinion and Persistent Bias\",\"url\":\"https://www.semanticscholar.org/paper/6684ccb530f1f36f591145062d5545bf19651b7d\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1708.01204\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICCVW.2017.61\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b598c73e9335277106fcb8acdad6cda227c6cdf\",\"title\":\"Improved Speech Reconstruction from Silent Video\",\"url\":\"https://www.semanticscholar.org/paper/2b598c73e9335277106fcb8acdad6cda227c6cdf\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":\"1809.01728\",\"authors\":[{\"authorId\":\"30606918\",\"name\":\"George Sterpu\"},{\"authorId\":\"1814175\",\"name\":\"Christian Saam\"},{\"authorId\":\"144686633\",\"name\":\"N. Harte\"}],\"doi\":\"10.1145/3242969.3243014\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dfd4b7bb92e43db206f853a5c2222e8bc68a4e5e\",\"title\":\"Attention-based Audio-Visual Fusion for Robust Automatic Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/dfd4b7bb92e43db206f853a5c2222e8bc68a4e5e\",\"venue\":\"ICMI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xiaoya Li\"},{\"authorId\":\"145243593\",\"name\":\"D. Neil\"},{\"authorId\":\"1694635\",\"name\":\"T. Delbr\\u00fcck\"},{\"authorId\":\"150301316\",\"name\":\"Shih-Chii Liu\"}],\"doi\":\"10.1109/ISCAS.2019.8702565\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"523aed786d066f33f30d6e612d3d8296a0b61764\",\"title\":\"Lip Reading Deep Network Exploiting Multi-Modal Spiking Visual and Auditory Sensors\",\"url\":\"https://www.semanticscholar.org/paper/523aed786d066f33f30d6e612d3d8296a0b61764\",\"venue\":\"2019 IEEE International Symposium on Circuits and Systems (ISCAS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Khawla Mallat\"},{\"authorId\":null,\"name\":\"Jean-Luc Dugelay\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49f12629af93cd11aaad25c4e275afe726df7b38\",\"title\":\"Facial landmark detection on thermal data via fully annotated visible-to-thermal data synthesis\",\"url\":\"https://www.semanticscholar.org/paper/49f12629af93cd11aaad25c4e275afe726df7b38\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35007805\",\"name\":\"Thomas Le Cornu\"},{\"authorId\":\"1772594\",\"name\":\"B. Milner\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5dc2cd9889de43961596ff38a280fdd55f67593a\",\"title\":\"T Generating Intelligible Audio Speech from Visual Speech\",\"url\":\"https://www.semanticscholar.org/paper/5dc2cd9889de43961596ff38a280fdd55f67593a\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1810.10597\",\"authors\":[{\"authorId\":\"40274148\",\"name\":\"Jake Burton\"},{\"authorId\":\"145175859\",\"name\":\"D. Frank\"},{\"authorId\":\"39930808\",\"name\":\"Mahdi Saleh\"},{\"authorId\":\"145587210\",\"name\":\"Nassir Navab\"},{\"authorId\":\"3192282\",\"name\":\"Helen L. Bear\"}],\"doi\":\"10.1109/IPAS.2018.8708874\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b63ab4ea41b3f6f0b02d1a8d788abf4da441482\",\"title\":\"The speaker-independent lipreading play-off; a survey of lipreading machines\",\"url\":\"https://www.semanticscholar.org/paper/4b63ab4ea41b3f6f0b02d1a8d788abf4da441482\",\"venue\":\"2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS)\",\"year\":2018},{\"arxivId\":\"2008.02686\",\"authors\":[{\"authorId\":\"1856465853\",\"name\":\"Liangfa Wei\"},{\"authorId\":\"40430887\",\"name\":\"J. Zhang\"},{\"authorId\":\"2635200\",\"name\":\"Junfeng Hou\"},{\"authorId\":\"153634883\",\"name\":\"Li-Rong Dai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13d529dc8dd1687526ee9642eb067b49436070e2\",\"title\":\"Attentive Fusion Enhanced Audio-Visual Encoding for Transformer Based Robust Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/13d529dc8dd1687526ee9642eb067b49436070e2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394019256\",\"name\":\"Tatsuya Shirakata\"},{\"authorId\":\"78346793\",\"name\":\"T. Saitoh\"}],\"doi\":\"10.23919/SICE.2019.8859932\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"522b101be763686a8621b5850f1a3bf296ea672f\",\"title\":\"Lip Reading Experiments for Multiple Databases using Conventional Method\",\"url\":\"https://www.semanticscholar.org/paper/522b101be763686a8621b5850f1a3bf296ea672f\",\"venue\":\"2019 58th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71712589\",\"name\":\"L. Courtney\"},{\"authorId\":\"40568918\",\"name\":\"R. Sreenivas\"}],\"doi\":\"10.1007/978-3-030-41299-9_24\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7db0bb6ed3630373be94e678dbc397d619473e90\",\"title\":\"Using Deep Convolutional LSTM Networks for Learning Spatiotemporal Features\",\"url\":\"https://www.semanticscholar.org/paper/7db0bb6ed3630373be94e678dbc397d619473e90\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17139744\",\"name\":\"Y. Nakamura\"},{\"authorId\":\"33280966\",\"name\":\"T. Higaki\"},{\"authorId\":\"6054641\",\"name\":\"F. Tatsugami\"},{\"authorId\":\"48723376\",\"name\":\"Yukiko Honda\"},{\"authorId\":\"11303958\",\"name\":\"K. Narita\"},{\"authorId\":\"47263302\",\"name\":\"Motonori Akagi\"},{\"authorId\":\"1810580\",\"name\":\"K. Awai\"}],\"doi\":\"10.1097/RCT.0000000000000928\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"852be8afb7ef51f68aef80ec6e722d10a2f6c6b0\",\"title\":\"Possibility of Deep Learning in Medical Imaging Focusing Improvement of Computed Tomography Image Quality\",\"url\":\"https://www.semanticscholar.org/paper/852be8afb7ef51f68aef80ec6e722d10a2f6c6b0\",\"venue\":\"Journal of computer assisted tomography\",\"year\":2019},{\"arxivId\":\"2009.08015\",\"authors\":[{\"authorId\":\"8665310\",\"name\":\"H. Kao\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"}],\"doi\":\"10.1145/3394171.3413848\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"daa00c142fdf25aa454d8ca52e4d20f5d0605964\",\"title\":\"Temporally Guided Music-to-Body-Movement Generation\",\"url\":\"https://www.semanticscholar.org/paper/daa00c142fdf25aa454d8ca52e4d20f5d0605964\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.07755\",\"authors\":[{\"authorId\":\"3075402\",\"name\":\"J. Yu\"},{\"authorId\":\"2213494\",\"name\":\"S. Zhang\"},{\"authorId\":\"152365289\",\"name\":\"Bo Wu\"},{\"authorId\":\"9511592\",\"name\":\"Shansong Liu\"},{\"authorId\":\"51265059\",\"name\":\"Shoukang Hu\"},{\"authorId\":\"1654167366\",\"name\":\"Mengzhe Geng\"},{\"authorId\":\"150344273\",\"name\":\"Xunying Liu\"},{\"authorId\":\"1702243\",\"name\":\"H. Meng\"},{\"authorId\":\"114483592\",\"name\":\"D. Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"46258ce3cbf3a7b8037127826a2ed0c04beabcdd\",\"title\":\"Audio-visual Multi-channel Integration and Recognition of Overlapped Speech\",\"url\":\"https://www.semanticscholar.org/paper/46258ce3cbf3a7b8037127826a2ed0c04beabcdd\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1711.08789\",\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"153677544\",\"name\":\"Asaph Shamir\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f04d8a78cd33ac944353c55d3d40230ef504f449\",\"title\":\"Visual Speech Enhancement using Noise-Invariant Training\",\"url\":\"https://www.semanticscholar.org/paper/f04d8a78cd33ac944353c55d3d40230ef504f449\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1802.06424\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"7943876\",\"name\":\"Feipeng Cai\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP.2018.8461326\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43cf4abfe1bc6e38b845740f7ee00715ce9d5a39\",\"title\":\"End-to-End Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/43cf4abfe1bc6e38b845740f7ee00715ce9d5a39\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"N SrikanthG\"},{\"authorId\":\"3181035\",\"name\":\"M. K. Venkatesha\"}],\"doi\":\"10.35940/ijrte.d4238.018520\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b1f640cc0c559f215b7ba4bb0a914f9eef41f56\",\"title\":\"Extraction of Lip features for the Identification of Vowels Utterances using MFCC and Geometrical Aspects\",\"url\":\"https://www.semanticscholar.org/paper/4b1f640cc0c559f215b7ba4bb0a914f9eef41f56\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115685706\",\"name\":\"Souheil Fenghour\"},{\"authorId\":\"67059049\",\"name\":\"D. Chen\"},{\"authorId\":\"2034207177\",\"name\":\"Kun Guo\"},{\"authorId\":\"30685117\",\"name\":\"P. Xiao\"}],\"doi\":\"10.1109/ACCESS.2020.3040906\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5ccee670108f7235a93fc6f445fccecece4123c0\",\"title\":\"Lip Reading Sentences Using Deep Learning With Only Visual Cues\",\"url\":\"https://www.semanticscholar.org/paper/5ccee670108f7235a93fc6f445fccecece4123c0\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1836818\",\"name\":\"J. Wang\"},{\"authorId\":\"8681309\",\"name\":\"Chengfeng Pan\"},{\"authorId\":\"2340572\",\"name\":\"Haojian Jin\"},{\"authorId\":\"1387580873\",\"name\":\"Vaibhav Singh\"},{\"authorId\":\"8347128\",\"name\":\"Yash M. Jain\"},{\"authorId\":\"1689960\",\"name\":\"J. Hong\"},{\"authorId\":\"2292027\",\"name\":\"C. Majidi\"},{\"authorId\":\"40578966\",\"name\":\"S. Kumar\"}],\"doi\":\"10.1145/3369812\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"95e68ff671292e628984db847bbe4988bb63a2c8\",\"title\":\"RFID Tattoo\",\"url\":\"https://www.semanticscholar.org/paper/95e68ff671292e628984db847bbe4988bb63a2c8\",\"venue\":\"Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49906780\",\"name\":\"Y. Koguchi\"},{\"authorId\":\"50291681\",\"name\":\"Kazuya Oharada\"},{\"authorId\":\"1833462\",\"name\":\"Y. Takagi\"},{\"authorId\":\"32672148\",\"name\":\"Yoshiki Sawada\"},{\"authorId\":\"1765222\",\"name\":\"B. Shizuki\"},{\"authorId\":\"48348365\",\"name\":\"S. Takahashi\"}],\"doi\":\"10.1007/978-3-319-91250-9_23\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2c9debe9d2fdadb99d260c90fa1b754bd962896\",\"title\":\"A Mobile Command Input Through Vowel Lip Shape Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d2c9debe9d2fdadb99d260c90fa1b754bd962896\",\"venue\":\"HCI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1516215874\",\"name\":\"Chongyuan Bi\"},{\"authorId\":\"47845124\",\"name\":\"Dongping Zhang\"},{\"authorId\":\"145215278\",\"name\":\"L. Yang\"},{\"authorId\":\"6472957\",\"name\":\"P. Chen\"}],\"doi\":\"10.1109/ICSAI48974.2019.9010432\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2224078fd52ccf742d3b3039003f8d197cc1e6b4\",\"title\":\"An Lipreading Modle with DenseNet and E3D-LSTM\",\"url\":\"https://www.semanticscholar.org/paper/2224078fd52ccf742d3b3039003f8d197cc1e6b4\",\"venue\":\"2019 6th International Conference on Systems and Informatics (ICSAI)\",\"year\":2019},{\"arxivId\":\"1906.04691\",\"authors\":[{\"authorId\":\"48271262\",\"name\":\"Taewan Kim\"},{\"authorId\":\"34724702\",\"name\":\"Joydeep Ghosh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43c5257502b0460ad44bf744c263b5b9120d29e6\",\"title\":\"On Single Source Robustness in Deep Fusion Models\",\"url\":\"https://www.semanticscholar.org/paper/43c5257502b0460ad44bf744c263b5b9120d29e6\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46761465\",\"name\":\"Xiaopeng Hong\"},{\"authorId\":\"1382648588\",\"name\":\"Wei Peng\"},{\"authorId\":\"1686714\",\"name\":\"M. Harandi\"},{\"authorId\":\"104117062\",\"name\":\"Ziheng Zhou\"},{\"authorId\":\"145962204\",\"name\":\"M. Pietik\\u00e4inen\"},{\"authorId\":\"83433495\",\"name\":\"G. Zhao\"}],\"doi\":\"10.1145/3342227\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"cd37895f5adfe8d4dc5276cee543eaf5a1f58a2c\",\"title\":\"Characterizing Subtle Facial Movements via Riemannian Manifold\",\"url\":\"https://www.semanticscholar.org/paper/cd37895f5adfe8d4dc5276cee543eaf5a1f58a2c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1909.05654\",\"authors\":[{\"authorId\":\"40360280\",\"name\":\"Di Fu\"},{\"authorId\":\"1798067\",\"name\":\"Cornelius Weber\"},{\"authorId\":\"3784179\",\"name\":\"G. Yang\"},{\"authorId\":\"2991958\",\"name\":\"Matthias Kerzel\"},{\"authorId\":\"5226074\",\"name\":\"Weizhi Nan\"},{\"authorId\":\"144039833\",\"name\":\"Pablo Barros\"},{\"authorId\":\"1748494\",\"name\":\"H. Wu\"},{\"authorId\":\"144227938\",\"name\":\"X. Liu\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.31234/osf.io/sx3uc\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ba822ad3cd3d2bae8ec94182e57be3f30f21b43\",\"title\":\"What can computational models learn from human selective attention? A review from an audiovisual crossmodal perspective\",\"url\":\"https://www.semanticscholar.org/paper/0ba822ad3cd3d2bae8ec94182e57be3f30f21b43\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67306944\",\"name\":\"Ludvig Malmros\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82994d140a42d8688526e355fee701608dec1025\",\"title\":\"Insect Event Extraction in LIDAR Images using Image Analysis and Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/82994d140a42d8688526e355fee701608dec1025\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33481412\",\"name\":\"Pan Zhou\"},{\"authorId\":\"47718595\",\"name\":\"Wenwen Yang\"},{\"authorId\":\"47151943\",\"name\":\"W. Chen\"},{\"authorId\":\"49417514\",\"name\":\"Y. Wang\"},{\"authorId\":\"144202061\",\"name\":\"Jia Jia\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5b9d2127dd73c319d3519453814612f62d2f8f21\",\"title\":\"FOR END-TO-END AUDIO-VISUAL SPEECH RECOGNITION\",\"url\":\"https://www.semanticscholar.org/paper/5b9d2127dd73c319d3519453814612f62d2f8f21\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3800313\",\"name\":\"Saquib Nadeem Hashmi\"},{\"authorId\":\"47442727\",\"name\":\"Harsh Gupta\"},{\"authorId\":\"51904965\",\"name\":\"Dhruv Mittal\"},{\"authorId\":\"122752009\",\"name\":\"K. Kumar\"},{\"authorId\":\"2317788\",\"name\":\"Aparajita Nanda\"},{\"authorId\":\"51139926\",\"name\":\"Sarishty Gupta\"}],\"doi\":\"10.1109/IC3.2018.8530509\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71c2c1122e808333eaa65c20ae6ed942ccb51372\",\"title\":\"A Lip Reading Model Using CNN with Batch Normalization\",\"url\":\"https://www.semanticscholar.org/paper/71c2c1122e808333eaa65c20ae6ed942ccb51372\",\"venue\":\"2018 Eleventh International Conference on Contemporary Computing (IC3)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123492027\",\"name\":\"Otavio Braga\"},{\"authorId\":\"3208950\",\"name\":\"Takaki Makino\"},{\"authorId\":\"2523830\",\"name\":\"O. Siohan\"},{\"authorId\":\"145004710\",\"name\":\"H. Liao\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053974\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b1e65ade6c3bfd7e0df6d986f9461747ed314c5\",\"title\":\"End-to-End Multi-Person Audio/Visual Automatic Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5b1e65ade6c3bfd7e0df6d986f9461747ed314c5\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2009.01225\",\"authors\":[{\"authorId\":\"1828765893\",\"name\":\"Liliane Momeni\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0835f6c5094b6d1b164f02f31a7234b82177c913\",\"title\":\"Seeing wake words: Audio-visual Keyword Spotting\",\"url\":\"https://www.semanticscholar.org/paper/0835f6c5094b6d1b164f02f31a7234b82177c913\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.12000\",\"authors\":[{\"authorId\":\"52225338\",\"name\":\"Egor Burkov\"},{\"authorId\":\"102773016\",\"name\":\"I. Pasechnik\"},{\"authorId\":\"26957065\",\"name\":\"A. Grigorev\"},{\"authorId\":\"1740145\",\"name\":\"V. Lempitsky\"}],\"doi\":\"10.1109/CVPR42600.2020.01380\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"85b73afafb6fc1527ba46d7bfc1210d5112c1ca2\",\"title\":\"Neural Head Reenactment with Latent Pose Descriptors\",\"url\":\"https://www.semanticscholar.org/paper/85b73afafb6fc1527ba46d7bfc1210d5112c1ca2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33699440\",\"name\":\"M. Veale\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1d8c645460e6a0a0fca0172d30635356d0dbefef\",\"title\":\"Governing machine learning that matters\",\"url\":\"https://www.semanticscholar.org/paper/1d8c645460e6a0a0fca0172d30635356d0dbefef\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"10040378\",\"name\":\"Sepehr Zargaran\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.1109/CVPR.2017.364\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28b85543e8f12c3d2d2227dcc9f5e87c685535ea\",\"title\":\"Re-Sign: Re-Aligned End-to-End Sequence Modelling with Deep Recurrent CNN-HMMs\",\"url\":\"https://www.semanticscholar.org/paper/28b85543e8f12c3d2d2227dcc9f5e87c685535ea\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1907.04011\",\"authors\":[{\"authorId\":\"143959668\",\"name\":\"P. Christiansen\"},{\"authorId\":\"40406527\",\"name\":\"M. Kragh\"},{\"authorId\":\"2884483\",\"name\":\"Y. Brodskiy\"},{\"authorId\":\"2550309\",\"name\":\"H. Karstoft\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2766b135e32168286a382e08fc6a66ae8ecad5a7\",\"title\":\"UnsuperPoint: End-to-end Unsupervised Interest Point Detector and Descriptor\",\"url\":\"https://www.semanticscholar.org/paper/2766b135e32168286a382e08fc6a66ae8ecad5a7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1810.06990\",\"authors\":[{\"authorId\":\"145071104\",\"name\":\"Shuang Yang\"},{\"authorId\":\"2631335\",\"name\":\"Y. Zhang\"},{\"authorId\":\"80007827\",\"name\":\"Dalu Feng\"},{\"authorId\":\"1943661\",\"name\":\"Mingmin Yang\"},{\"authorId\":null,\"name\":\"Chenhao Wang\"},{\"authorId\":\"31227090\",\"name\":\"Jingyun Xiao\"},{\"authorId\":\"2566176\",\"name\":\"K. Long\"},{\"authorId\":\"145455919\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1109/FG.2019.8756582\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60fa866c24c78b3f022651370cc194f3865560d0\",\"title\":\"LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/60fa866c24c78b3f022651370cc194f3865560d0\",\"venue\":\"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39089342\",\"name\":\"Miroslav Hlav\\u00e1c\"},{\"authorId\":\"2601929\",\"name\":\"I. Gruber\"},{\"authorId\":\"2851907\",\"name\":\"M. Zelezn\\u00fd\"},{\"authorId\":\"145191867\",\"name\":\"Alexey Karpov\"}],\"doi\":\"10.1007/978-3-319-66429-3_66\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a14f3a350c1cf61f3b049e141a22aee13766af8b\",\"title\":\"Semi-automatic Facial Key-Point Dataset Creation\",\"url\":\"https://www.semanticscholar.org/paper/a14f3a350c1cf61f3b049e141a22aee13766af8b\",\"venue\":\"SPECOM\",\"year\":2017},{\"arxivId\":\"2007.09902\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"48670507\",\"name\":\"Xudong Xu\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"}],\"doi\":\"10.1007/978-3-030-58610-2_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"73aa926dad010a3f1bb89faa31241f97a89cc461\",\"title\":\"Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/73aa926dad010a3f1bb89faa31241f97a89cc461\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1809.02108\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/TPAMI.2018.2889052\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fcecc4ef2c32dbedda61648febb39a0f905c367e\",\"title\":\"Deep Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fcecc4ef2c32dbedda61648febb39a0f905c367e\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2018},{\"arxivId\":\"1902.03091\",\"authors\":[{\"authorId\":\"33330735\",\"name\":\"C. Kaul\"},{\"authorId\":\"1756438\",\"name\":\"S. Manandhar\"},{\"authorId\":\"1737428\",\"name\":\"Nick E. Pears\"}],\"doi\":\"10.1109/ISBI.2019.8759477\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72115149c6a51dca06b89eb36d68c6396c89dde7\",\"title\":\"Focusnet: An Attention-Based Fully Convolutional Network for Medical Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/72115149c6a51dca06b89eb36d68c6396c89dde7\",\"venue\":\"2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"}],\"doi\":\"10.1109/TASLP.2017.2783545\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2294754f2e362814dcf030327f17a8a4749d00a6\",\"title\":\"Comparing Fusion Models for DNN-Based Audiovisual Continuous Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2294754f2e362814dcf030327f17a8a4749d00a6\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2018},{\"arxivId\":\"1809.08001\",\"authors\":[{\"authorId\":\"10437962\",\"name\":\"Soo-Whan Chung\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"9299637\",\"name\":\"Hong-Goo Kang\"}],\"doi\":\"10.1109/ICASSP.2019.8682524\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d66090e0bddb6f751241acd6e59cf25756e57a9\",\"title\":\"Perfect Match: Improved Cross-modal Embeddings for Audio-visual Synchronisation\",\"url\":\"https://www.semanticscholar.org/paper/4d66090e0bddb6f751241acd6e59cf25756e57a9\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1905.02540\",\"authors\":[{\"authorId\":\"7885067\",\"name\":\"Xinshuo Weng\"},{\"authorId\":\"144040368\",\"name\":\"K. Kitani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"607048b431cea997ae9dd01f029a73c502d0273f\",\"title\":\"Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/607048b431cea997ae9dd01f029a73c502d0273f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1904.11829\",\"authors\":[{\"authorId\":\"3422375\",\"name\":\"L. Arras\"},{\"authorId\":\"143757036\",\"name\":\"Ahmed Osman\"},{\"authorId\":\"145034054\",\"name\":\"K. M\\u00fcller\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"}],\"doi\":\"10.18653/v1/W19-4813\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9e01ef82b54fbef69a0704f1b11882b28c69762f\",\"title\":\"Evaluating Recurrent Neural Network Explanations\",\"url\":\"https://www.semanticscholar.org/paper/9e01ef82b54fbef69a0704f1b11882b28c69762f\",\"venue\":\"ACL 2019\",\"year\":2019},{\"arxivId\":\"1909.11573\",\"authors\":[{\"authorId\":\"32579488\",\"name\":\"T. T. Nguyen\"},{\"authorId\":\"153304265\",\"name\":\"C. Nguyen\"},{\"authorId\":\"1390619529\",\"name\":\"Dung T. Nguyen\"},{\"authorId\":\"1779016\",\"name\":\"D. Nguyen\"},{\"authorId\":\"1743136\",\"name\":\"S. Nahavandi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ec3d4da1e035f95488f3587ec5c9ec7354711ab\",\"title\":\"Deep Learning for Deepfakes Creation and Detection: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/2ec3d4da1e035f95488f3587ec5c9ec7354711ab\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1807.10550\",\"authors\":[{\"authorId\":\"8792285\",\"name\":\"Olivia Wiles\"},{\"authorId\":\"32445716\",\"name\":\"A. Koepke\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01261-8_41\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9ea992f009492888c482d5f4006281eaa8b758e7\",\"title\":\"X2Face: A network for controlling face generation by using images, audio, and pose codes\",\"url\":\"https://www.semanticscholar.org/paper/9ea992f009492888c482d5f4006281eaa8b758e7\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1903.10195\",\"authors\":[{\"authorId\":\"144404308\",\"name\":\"Amanda Duarte\"},{\"authorId\":\"153738374\",\"name\":\"Francisco Roldan\"},{\"authorId\":\"118129922\",\"name\":\"Miquel Tubau\"},{\"authorId\":\"115672881\",\"name\":\"Janna Escur\"},{\"authorId\":\"144077573\",\"name\":\"S. Pascual\"},{\"authorId\":\"31571033\",\"name\":\"A. Salvador\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"145470863\",\"name\":\"K. McGuinness\"},{\"authorId\":\"144345280\",\"name\":\"J. Torres\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":\"10.1109/ICASSP.2019.8682970\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d68ae09863d3ec1831392204f7a062ebebaacff9\",\"title\":\"Wav2Pix: Speech-conditioned Face Generation Using Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/d68ae09863d3ec1831392204f7a062ebebaacff9\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1701.00495\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICASSP.2017.7953127\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c87c275ddde2e0b75264fe9dad7b130db410601\",\"title\":\"Vid2speech: Speech reconstruction from silent video\",\"url\":\"https://www.semanticscholar.org/paper/5c87c275ddde2e0b75264fe9dad7b130db410601\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1802.05521\",\"authors\":[{\"authorId\":\"145616463\",\"name\":\"M. Faisal\"},{\"authorId\":\"2209797\",\"name\":\"Sanaullah Manzoor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a248078631d3fa90e127147eb38d8eb0a235cd3e\",\"title\":\"Deep Learning for Lip Reading using Audio-Visual Information for Urdu Language\",\"url\":\"https://www.semanticscholar.org/paper/a248078631d3fa90e127147eb38d8eb0a235cd3e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39564005\",\"name\":\"Amanda Duarte\"},{\"authorId\":\"153738374\",\"name\":\"Francisco Roldan\"},{\"authorId\":\"118129922\",\"name\":\"Miquel Tubau\"},{\"authorId\":\"115672881\",\"name\":\"Janna Escur\"},{\"authorId\":\"144077573\",\"name\":\"S. Pascual\"},{\"authorId\":\"69464879\",\"name\":\"Amaia\"},{\"authorId\":\"46753290\",\"name\":\"Salvador\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"145470863\",\"name\":\"K. McGuinness\"},{\"authorId\":\"144345280\",\"name\":\"J. Torres\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e93efe94dcd8d5de9c5efc7961c2448a6759b8c\",\"title\":\"WAV 2 PIX : SPEECH-CONDITIONED FACE GENERATION USING GENERATIVE ADVERSARIAL NETWORKS\",\"url\":\"https://www.semanticscholar.org/paper/8e93efe94dcd8d5de9c5efc7961c2448a6759b8c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49605588\",\"name\":\"Jinjiang Wang\"},{\"authorId\":\"117252002\",\"name\":\"Peilun Fu\"},{\"authorId\":\"1784352\",\"name\":\"L. Zhang\"},{\"authorId\":\"1700762\",\"name\":\"R. Gao\"},{\"authorId\":\"143624101\",\"name\":\"R. Zhao\"}],\"doi\":\"10.1109/TMECH.2019.2928967\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf0bba2fc9b333229dca63eaeafb8db50c2ea0a0\",\"title\":\"Multilevel Information Fusion for Induction Motor Fault Diagnosis\",\"url\":\"https://www.semanticscholar.org/paper/bf0bba2fc9b333229dca63eaeafb8db50c2ea0a0\",\"venue\":\"IEEE/ASME Transactions on Mechatronics\",\"year\":2019},{\"arxivId\":\"1709.00944\",\"authors\":[{\"authorId\":\"8726117\",\"name\":\"Jen-Cheng Hou\"},{\"authorId\":\"2426246\",\"name\":\"S. Wang\"},{\"authorId\":\"145274548\",\"name\":\"Y. Lai\"},{\"authorId\":\"66191041\",\"name\":\"Jen-Chun Lin\"},{\"authorId\":\"145403933\",\"name\":\"Y. Tsao\"},{\"authorId\":\"144600099\",\"name\":\"Hsiu-Wen Chang\"},{\"authorId\":\"1710199\",\"name\":\"H. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b83a4395be8b79e693118b95cd7cca764390ec8f\",\"title\":\"Audio-Visual Speech Enhancement based on Multimodal Deep Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/b83a4395be8b79e693118b95cd7cca764390ec8f\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51265752\",\"name\":\"Alexandros Koumparoulis\"},{\"authorId\":\"1423737852\",\"name\":\"Gerasimos Potamianos\"},{\"authorId\":\"70913918\",\"name\":\"S. Thomas\"},{\"authorId\":\"150092714\",\"name\":\"Edmilson da Silva Morais\"}],\"doi\":\"10.21437/interspeech.2020-3003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"821ebc91199f1d08f21f67c3ef56aa733c529201\",\"title\":\"Resource-Adaptive Deep Learning for Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/821ebc91199f1d08f21f67c3ef56aa733c529201\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145236670\",\"name\":\"R. Lu\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"1700883\",\"name\":\"C. Zhang\"}],\"doi\":\"10.1109/LSP.2018.2853566\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2176fe5d53f69c26ae88c4ffe6607f7466ef33bc\",\"title\":\"Listen and Look: Audio\\u2013Visual Matching Assisted Speech Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/2176fe5d53f69c26ae88c4ffe6607f7466ef33bc\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14924930\",\"name\":\"M. Bourguignon\"},{\"authorId\":\"2546626\",\"name\":\"M. Baart\"},{\"authorId\":\"3383330\",\"name\":\"Efthymia C Kapnoula\"},{\"authorId\":\"1713794\",\"name\":\"Nicola Molinaro\"}],\"doi\":\"10.1523/JNEUROSCI.1101-19.2019\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"714600ae36c7282a928a519044a629a14da9acde\",\"title\":\"Lip-Reading Enables the Brain to Synthesize Auditory Features of Unknown Silent Speech\",\"url\":\"https://www.semanticscholar.org/paper/714600ae36c7282a928a519044a629a14da9acde\",\"venue\":\"The Journal of Neuroscience\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"40163061\",\"name\":\"N. C. Camgoz\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":\"10.1109/TPAMI.2019.2911077\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"964502ea316fe9049c464a0925e3dc4246023a0d\",\"title\":\"Weakly Supervised Learning with Multi-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign Language Videos\",\"url\":\"https://www.semanticscholar.org/paper/964502ea316fe9049c464a0925e3dc4246023a0d\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"2005.13616\",\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"},{\"authorId\":\"115268571\",\"name\":\"Barry-John Theobald\"},{\"authorId\":\"145251024\",\"name\":\"P. Dixon\"},{\"authorId\":\"2497993\",\"name\":\"Reinhard Knothe\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"},{\"authorId\":\"123576773\",\"name\":\"Sachin Kajareker\"}],\"doi\":\"10.1145/3382507.3418840\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"title\":\"Modality Dropout for Improved Performance-driven Talking Faces\",\"url\":\"https://www.semanticscholar.org/paper/917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":\"2010.04002\",\"authors\":[{\"authorId\":\"1828765893\",\"name\":\"Liliane Momeni\"},{\"authorId\":\"82657029\",\"name\":\"G. Varol\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"631bbcce16387e76e4780d7c84b07b2a37d6bfc4\",\"title\":\"Watch, read and lookup: learning to spot signs from multiple supervisors\",\"url\":\"https://www.semanticscholar.org/paper/631bbcce16387e76e4780d7c84b07b2a37d6bfc4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.02099\",\"authors\":[{\"authorId\":\"46183056\",\"name\":\"Johanes Effendi\"},{\"authorId\":\"2894428\",\"name\":\"Andros Tjandra\"},{\"authorId\":\"1783949\",\"name\":\"Sakriani Sakti\"},{\"authorId\":\"50068540\",\"name\":\"S. Nakamura\"}],\"doi\":\"10.21437/interspeech.2020-2001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b39c4706503b4f6270abc85410efe3a0e7c26282\",\"title\":\"Augmenting Images for ASR and TTS Through Single-Loop and Dual-Loop Multimodal Chain Framework\",\"url\":\"https://www.semanticscholar.org/paper/b39c4706503b4f6270abc85410efe3a0e7c26282\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2012.07528\",\"authors\":[{\"authorId\":\"115685706\",\"name\":\"Souheil Fenghour\"},{\"authorId\":\"47514164\",\"name\":\"Daqing Chen\"},{\"authorId\":\"2034207177\",\"name\":\"Kun Guo\"},{\"authorId\":\"30685117\",\"name\":\"P. Xiao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7247e231b58345cc0040dd87f653814e5243ce7c\",\"title\":\"Disentangling Homophemes in Lip Reading using Perplexity Analysis\",\"url\":\"https://www.semanticscholar.org/paper/7247e231b58345cc0040dd87f653814e5243ce7c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46183056\",\"name\":\"Johanes Effendi\"},{\"authorId\":\"2894428\",\"name\":\"Andros Tjandra\"},{\"authorId\":\"1783949\",\"name\":\"Sakriani Sakti\"},{\"authorId\":\"145223960\",\"name\":\"S. Nakamura\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db35f5493bd9f486bd3d0304b844dbb7f75e9714\",\"title\":\"From Speech Chain to Multimodal Chain: Leveraging Cross-modal Data Augmentation for Semi-supervised Learning\",\"url\":\"https://www.semanticscholar.org/paper/db35f5493bd9f486bd3d0304b844dbb7f75e9714\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47158219\",\"name\":\"Xuan Xu\"},{\"authorId\":\"50080172\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/CVPRW.2019.00254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"089da19ed3b0142c2de055579049789e1090f278\",\"title\":\"SCAN: Spatial Color Attention Networks for Real Single Image Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/089da19ed3b0142c2de055579049789e1090f278\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720860114\",\"name\":\"Umberto Junior Mele\"},{\"authorId\":\"1409873111\",\"name\":\"Xiaochen Chou\"},{\"authorId\":\"6803671\",\"name\":\"L. Gambardella\"},{\"authorId\":\"1720859506\",\"name\":\"Roberto Montemanni\"}],\"doi\":\"10.1109/ICIEA49774.2020.9101981\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"80a1283ed4e52944a46b33478f14a373e721a3c7\",\"title\":\"Reinforcement Learning and Additional Rewardsfor the Traveling Salesman Problem\",\"url\":\"https://www.semanticscholar.org/paper/80a1283ed4e52944a46b33478f14a373e721a3c7\",\"venue\":\"2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA)\",\"year\":2020},{\"arxivId\":\"1612.02695\",\"authors\":[{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"}],\"doi\":\"10.21437/INTERSPEECH.2017-343\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786\",\"title\":\"Towards Better Decoding and Language Model Integration in Sequence to Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/7dbb2d983ab95da04e5d47c87ddd2cd9a8f20786\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":\"1908.11618\",\"authors\":[{\"authorId\":\"50096461\",\"name\":\"Chenhao Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ad2cda8d34ce747c6dabc0e325311ffa8c44576\",\"title\":\"Multi-Grained Spatio-temporal Modeling for Lip-reading\",\"url\":\"https://www.semanticscholar.org/paper/3ad2cda8d34ce747c6dabc0e325311ffa8c44576\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3086446\",\"name\":\"C. Du\"},{\"authorId\":\"2736221\",\"name\":\"Xiaoqun Yuan\"},{\"authorId\":\"145612191\",\"name\":\"W. Lou\"},{\"authorId\":\"145055445\",\"name\":\"Y. T. Hou\"}],\"doi\":\"10.1109/SAHCN.2018.8397118\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e42c47285649da693ff10a71f55cc70b1603272d\",\"title\":\"Context-Free Fine-Grained Motion Sensing Using WiFi\",\"url\":\"https://www.semanticscholar.org/paper/e42c47285649da693ff10a71f55cc70b1603272d\",\"venue\":\"2018 15th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)\",\"year\":2018},{\"arxivId\":\"1807.05162\",\"authors\":[{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"3243579\",\"name\":\"M. W. Hoffman\"},{\"authorId\":\"145757542\",\"name\":\"T. Paine\"},{\"authorId\":\"49304262\",\"name\":\"C. Hughes\"},{\"authorId\":\"39309000\",\"name\":\"Utsav Prabhu\"},{\"authorId\":\"39977619\",\"name\":\"H. Liao\"},{\"authorId\":\"2670103\",\"name\":\"H. Sak\"},{\"authorId\":\"2251957\",\"name\":\"K. Rao\"},{\"authorId\":\"51121518\",\"name\":\"Lorrayne Bennett\"},{\"authorId\":\"51116420\",\"name\":\"Marie Mulville\"},{\"authorId\":\"48303781\",\"name\":\"Ben Coppin\"},{\"authorId\":\"153774873\",\"name\":\"B. Laurie\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":\"10.21437/interspeech.2019-1669\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5befd105f7bbd373208522d5b85682116b59c38\",\"title\":\"Large-Scale Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/e5befd105f7bbd373208522d5b85682116b59c38\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14924930\",\"name\":\"M. Bourguignon\"},{\"authorId\":\"2546626\",\"name\":\"M. Baart\"},{\"authorId\":\"3383330\",\"name\":\"Efthymia C Kapnoula\"},{\"authorId\":\"1713794\",\"name\":\"Nicola Molinaro\"}],\"doi\":\"10.1101/395483\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b9bb61d14fff09132970c5e40754a0daed86b48\",\"title\":\"Hearing through lip-reading: the brain synthesizes features of absent speech\",\"url\":\"https://www.semanticscholar.org/paper/5b9bb61d14fff09132970c5e40754a0daed86b48\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2680310\",\"name\":\"Amit Aides\"},{\"authorId\":\"2366754\",\"name\":\"D. Dov\"},{\"authorId\":\"2580470\",\"name\":\"H. Aronowitz\"}],\"doi\":\"10.1109/ICASSP.2018.8462307\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e69d4b347a00d4e34aa468bbceb6fe7ab37cb36e\",\"title\":\"Robust Audiovisual Liveness Detection for Biometric Authentication Using Deep Joint Embedding and Dynamic Time Warping\",\"url\":\"https://www.semanticscholar.org/paper/e69d4b347a00d4e34aa468bbceb6fe7ab37cb36e\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":\"1803.04840\",\"authors\":[{\"authorId\":\"40897085\",\"name\":\"Matthijs Van Keirsbilck\"},{\"authorId\":\"2662178\",\"name\":\"Bert Moons\"},{\"authorId\":\"3277060\",\"name\":\"M. Verhelst\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"71d47cee4dde3a1ab027f06d4dc83e15a34bea07\",\"title\":\"Resource aware design of a deep convolutional-recurrent neural network for speech recognition through audio-visual sensor fusion\",\"url\":\"https://www.semanticscholar.org/paper/71d47cee4dde3a1ab027f06d4dc83e15a34bea07\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1909.11519\",\"authors\":[{\"authorId\":\"15556978\",\"name\":\"Zongxin Yang\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":null,\"name\":\"Yu Wu\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.1109/CVPR42600.2020.01181\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a6fa3e3a37b6eebc382a59de8cbc3c6c6eb710d\",\"title\":\"Gated Channel Transformation for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6a6fa3e3a37b6eebc382a59de8cbc3c6c6eb710d\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1802.08717\",\"authors\":[{\"authorId\":\"1786177\",\"name\":\"M. Mazurowski\"},{\"authorId\":\"32749516\",\"name\":\"Mateusz Buda\"},{\"authorId\":\"36032589\",\"name\":\"Ashirbani Saha\"},{\"authorId\":\"2735981\",\"name\":\"M. Bashir\"}],\"doi\":\"10.1002/jmri.26534\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f38b9ca303fc7b2c81470ce0ca2963d8b50474de\",\"title\":\"Deep learning in radiology: An overview of the concepts and a survey of the state of the art with focus on MRI\",\"url\":\"https://www.semanticscholar.org/paper/f38b9ca303fc7b2c81470ce0ca2963d8b50474de\",\"venue\":\"Journal of magnetic resonance imaging : JMRI\",\"year\":2019},{\"arxivId\":\"1808.06250\",\"authors\":[{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICASSP.2019.8682863\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9b2355f1d637c04ccd450c183bebd545f77f4b7\",\"title\":\"Dynamic Temporal Alignment of Speech to Lips\",\"url\":\"https://www.semanticscholar.org/paper/b9b2355f1d637c04ccd450c183bebd545f77f4b7\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3086446\",\"name\":\"C. Du\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d907d9d7d72dae906e3b124f6f5134333cec7e99\",\"title\":\"Exploring the Sensing Capability of Wireless Signals\",\"url\":\"https://www.semanticscholar.org/paper/d907d9d7d72dae906e3b124f6f5134333cec7e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30606918\",\"name\":\"George Sterpu\"}],\"doi\":\"10.1145/3242969.3264976\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b2f5a720eb0600f2750295a4211f19ff14b42a7f\",\"title\":\"Large Vocabulary Continuous Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/b2f5a720eb0600f2750295a4211f19ff14b42a7f\",\"venue\":\"ICMI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39646961\",\"name\":\"A. Jha\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e5918acaa2b08e747da02d7ebfdb67568e8ab937\",\"title\":\"Audio-Visual Speech Recognition and Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/e5918acaa2b08e747da02d7ebfdb67568e8ab937\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.09586\",\"authors\":[{\"authorId\":\"9686806\",\"name\":\"Daniel Michelsanti\"},{\"authorId\":\"71668001\",\"name\":\"Zheng-Hua Tan\"},{\"authorId\":\"47180604\",\"name\":\"Shixiong Zhang\"},{\"authorId\":\"121983569\",\"name\":\"Yanchen Xu\"},{\"authorId\":\"143872259\",\"name\":\"M. Yu\"},{\"authorId\":null,\"name\":\"Dong Yu\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1859067487893f6580e934e9ee3408a2fa8b7e1\",\"title\":\"An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation\",\"url\":\"https://www.semanticscholar.org/paper/e1859067487893f6580e934e9ee3408a2fa8b7e1\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":1662180,\"doi\":\"10.1109/CVPR.2017.367\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":52,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"}],\"doi\":\"10.1109/ASRU.2013.6707742\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1149888d75af4ed5dffc25731b875651c3ccdeb2\",\"title\":\"Hybrid speech recognition with Deep Bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/1149888d75af4ed5dffc25731b875651c3ccdeb2\",\"venue\":\"2013 IEEE Workshop on Automatic Speech Recognition and Understanding\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f\",\"title\":\"Towards End-To-End Speech Recognition with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":\"10.1109/ICCVW.2015.69\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"39cf849f5af8dbc7fca81c6f16cbe482b38ab329\",\"title\":\"Deep Learning of Mouth Shapes for Sign Language\",\"url\":\"https://www.semanticscholar.org/paper/39cf849f5af8dbc7fca81c6f16cbe482b38ab329\",\"venue\":\"2015 IEEE International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2015},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Y. Yamaguchi\"},{\"authorId\":null,\"name\":\"K. Nakadai\"},{\"authorId\":null,\"name\":\"H. G. Okuno\"},{\"authorId\":null,\"name\":\"T. Ogata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Audio - visual speech recognition using deep learn\",\"url\":\"\",\"venue\":\"Applied Intelligence\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3119923\",\"name\":\"K. Noda\"},{\"authorId\":\"145473520\",\"name\":\"Y. Yamaguchi\"},{\"authorId\":\"1764429\",\"name\":\"K. Nakadai\"},{\"authorId\":\"1775800\",\"name\":\"H. Okuno\"},{\"authorId\":\"50527812\",\"name\":\"Tetsuya Ogata\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d2f47c56f9e2545c5381bc41d3efbe7f4be2d61\",\"title\":\"Lipreading using convolutional neural network\",\"url\":\"https://www.semanticscholar.org/paper/1d2f47c56f9e2545c5381bc41d3efbe7f4be2d61\",\"venue\":\"INTERSPEECH\",\"year\":2014},{\"arxivId\":\"1611.01599\",\"authors\":[{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"291c0e453503a704c0fd932a067ca054cc7edad6\",\"title\":\"LipNet: End-to-End Sentence-level Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/291c0e453503a704c0fd932a067ca054cc7edad6\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":\"1604.06573\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2016.213\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d9aced120e530484609164c836da64548693484\",\"title\":\"Convolutional Two-Stream Network Fusion for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1409.4842\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"46641766\",\"name\":\"W. Liu\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"48840704\",\"name\":\"Scott Reed\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"}],\"doi\":\"10.1109/CVPR.2015.7298594\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"title\":\"Going deeper with convolutions\",\"url\":\"https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1934275\",\"name\":\"Yuxuan Lan\"},{\"authorId\":\"144439756\",\"name\":\"R. Harvey\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"},{\"authorId\":\"143894891\",\"name\":\"E. Ong\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7d9cf75b6457f6ed77f1de8d1f5301a2a74e07a\",\"title\":\"Comparing visual features for lipreading\",\"url\":\"https://www.semanticscholar.org/paper/b7d9cf75b6457f6ed77f1de8d1f5301a2a74e07a\",\"venue\":\"AVSP\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738798\",\"name\":\"H. Hermansky\"}],\"doi\":\"10.1121/1.399423\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b578f4faeb00b808e8786d897447f2493b12b4e9\",\"title\":\"Perceptual linear predictive (PLP) analysis of speech.\",\"url\":\"https://www.semanticscholar.org/paper/b578f4faeb00b808e8786d897447f2493b12b4e9\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49049934\",\"name\":\"J. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ec48ac86456cea3d6d6172ca81ef68e98b21a61\",\"title\":\"The PASCAL Visual Object Classes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/0ec48ac86456cea3d6d6172ca81ef68e98b21a61\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"I. Sutskever\"},{\"authorId\":null,\"name\":\"O. Vinyals\"},{\"authorId\":null,\"name\":\"Q. Le\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Sequence to sequence 9  learning with neural networks\",\"url\":\"\",\"venue\":\"Advances in neural information processing systems, pages 3104\\u20133112,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1644066790\",\"name\":\"EveringhamMark\"},{\"authorId\":\"1644080676\",\"name\":\"M. EslamiS.\"},{\"authorId\":\"1644017278\",\"name\":\"GoolLuc\"},{\"authorId\":\"1644074763\",\"name\":\"K. WilliamsChristopher\"},{\"authorId\":\"1644080846\",\"name\":\"WinnJohn\"},{\"authorId\":\"1643953626\",\"name\":\"ZissermanAndrew\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"535995b77ae23f1a075d2e38be5b670e9c339290\",\"title\":\"The Pascal Visual Object Classes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/535995b77ae23f1a075d2e38be5b670e9c339290\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1506.07503\",\"authors\":[{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1862138\",\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b624504240fa52ab76167acfe3156150ca01cf3b\",\"title\":\"Attention-Based Models for Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/b624504240fa52ab76167acfe3156150ca01cf3b\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54427-4_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"87defac1045bfa9af0162cd248d193e9be6eb25b\",\"title\":\"Out of Time: Automated Lip Sync in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/87defac1045bfa9af0162cd248d193e9be6eb25b\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M Assael\"},{\"authorId\":null,\"name\":\"B Shillingford\"},{\"authorId\":null,\"name\":\"S Whiteson\"},{\"authorId\":null,\"name\":\"N De Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lipnet: Sentence-level lipreading. Under submission to ICLR\",\"url\":\"\",\"venue\":\"Lipnet: Sentence-level lipreading. Under submission to ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP.2016.7472088\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6541aff0aa0c24a9bf9d04b448087dee2c38b24c\",\"title\":\"Deep complementary bottleneck features for visual speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/6541aff0aa0c24a9bf9d04b448087dee2c38b24c\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016},{\"arxivId\":\"1511.06709\",\"authors\":[{\"authorId\":\"2082372\",\"name\":\"Rico Sennrich\"},{\"authorId\":\"2259100\",\"name\":\"B. Haddow\"},{\"authorId\":\"2539211\",\"name\":\"Alexandra Birch\"}],\"doi\":\"10.18653/v1/P16-1009\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f3b96ef2dc1fc5e14982f1b963db8db6a54183bb\",\"title\":\"Improving Neural Machine Translation Models with Monolingual Data\",\"url\":\"https://www.semanticscholar.org/paper/f3b96ef2dc1fc5e14982f1b963db8db6a54183bb\",\"venue\":\"ACL\",\"year\":2016},{\"arxivId\":\"1412.1602\",\"authors\":[{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"47d2dc34e1d02a8109f5c04bb6939725de23716d\",\"title\":\"End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results\",\"url\":\"https://www.semanticscholar.org/paper/47d2dc34e1d02a8109f5c04bb6939725de23716d\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Ward\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Audio - visual speech recognition using deep learn\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1601.08188\",\"authors\":[{\"authorId\":\"143910530\",\"name\":\"Michael Wand\"},{\"authorId\":\"2865775\",\"name\":\"J. Koutn\\u00edk\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/ICASSP.2016.7472852\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"675419a4faaf71ab8178f79523308d3dd3392913\",\"title\":\"Lipreading with long short-term memory\",\"url\":\"https://www.semanticscholar.org/paper/675419a4faaf71ab8178f79523308d3dd3392913\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Graves\"},{\"authorId\":null,\"name\":\"N. Jaitly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Towards end-to-end speech recog-  nition with recurrent neural networks\",\"url\":\"\",\"venue\":\"Proceedings of the 31st International Conference on Machine Learning (ICML- 14), pages 1764\\u20131772,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c70b2c373917ba61a871b97119413db1eadcf423\",\"title\":\"LipNet: Sentence-level Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/c70b2c373917ba61a871b97119413db1eadcf423\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33366691\",\"name\":\"Jiahong Yuan\"},{\"authorId\":\"144173823\",\"name\":\"M. Liberman\"}],\"doi\":\"10.1121/1.2935783\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e5ed888bd2f603ada808a571a3f0d1d91dae7be\",\"title\":\"Speaker identification on the SCOTUS corpus\",\"url\":\"https://www.semanticscholar.org/paper/5e5ed888bd2f603ada808a571a3f0d1d91dae7be\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46529897\",\"name\":\"M. Marschark\"},{\"authorId\":\"16078674\",\"name\":\"P. Spencer\"}],\"doi\":\"10.1093/OXFORDHB/9780195390032.001.0001\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"85f2317f41b8d935903b440345293ef72fc612ae\",\"title\":\"The Oxford Handbook of Deaf Studies, Language, and Education, Volume 2.\",\"url\":\"https://www.semanticscholar.org/paper/85f2317f41b8d935903b440345293ef72fc612ae\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741674\",\"name\":\"Georgios Galatas\"},{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"},{\"authorId\":\"1728274\",\"name\":\"F. Makedon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6b3a5341997e1cfb60ee093cb62b28bb5918b87f\",\"title\":\"Audio-visual speech recognition incorporating facial depth information captured by the Kinect\",\"url\":\"https://www.semanticscholar.org/paper/6b3a5341997e1cfb60ee093cb62b28bb5918b87f\",\"venue\":\"2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":\"1508.01211\",\"authors\":[{\"authorId\":\"144333684\",\"name\":\"William Chan\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"dc555e8156c956f823587ebbff018863e6d2a95e\",\"title\":\"Listen, Attend and Spell\",\"url\":\"https://www.semanticscholar.org/paper/dc555e8156c956f823587ebbff018863e6d2a95e\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1713014\",\"name\":\"A. Czyzewski\"},{\"authorId\":\"1796260\",\"name\":\"B. Kostek\"},{\"authorId\":\"2972926\",\"name\":\"P. Bratoszewski\"},{\"authorId\":\"144030731\",\"name\":\"J. Kotus\"},{\"authorId\":\"1678848\",\"name\":\"Marcin S. Szczuka\"}],\"doi\":\"10.1007/s10844-016-0438-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd54941cc46656005d31b1b24e3a002a7acd5b3f\",\"title\":\"An audio-visual corpus for multimodal automatic speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/cd54941cc46656005d31b1b24e3a002a7acd5b3f\",\"venue\":\"Journal of Intelligent Information Systems\",\"year\":2016},{\"arxivId\":\"1506.03099\",\"authors\":[{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df137487e20ba7c6e1e2b9a1e749f2a578b5ad99\",\"title\":\"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/df137487e20ba7c6e1e2b9a1e749f2a578b5ad99\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2626422\",\"name\":\"V. Kazemi\"},{\"authorId\":\"50626295\",\"name\":\"J. Sullivan\"}],\"doi\":\"10.1109/CVPR.2014.241\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d78b6a5b0dcaa81b1faea5fb0000045a62513567\",\"title\":\"One millisecond face alignment with an ensemble of regression trees\",\"url\":\"https://www.semanticscholar.org/paper/d78b6a5b0dcaa81b1faea5fb0000045a62513567\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. Tomasi\"},{\"authorId\":null,\"name\":\"T. Kanade\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Selecting and tracking features for image sequence analysis\",\"url\":\"\",\"venue\":\"Robotics and Automation,\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693571\",\"name\":\"S. Tamura\"},{\"authorId\":\"9198480\",\"name\":\"H. Ninomiya\"},{\"authorId\":\"1743692\",\"name\":\"N. Kitaoka\"},{\"authorId\":\"3390708\",\"name\":\"Shin Osuga\"},{\"authorId\":\"2225348\",\"name\":\"Y. Iribe\"},{\"authorId\":\"1709999\",\"name\":\"K. Takeda\"},{\"authorId\":\"49749635\",\"name\":\"S. Hayamizu\"}],\"doi\":\"10.1109/APSIPA.2015.7415335\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"15f4627b542362b585d045651e58b0f22fe3842c\",\"title\":\"Audio-visual speech recognition using deep bottleneck features and high-performance lipreading\",\"url\":\"https://www.semanticscholar.org/paper/15f4627b542362b585d045651e58b0f22fe3842c\",\"venue\":\"2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143967982\",\"name\":\"M. Cooke\"},{\"authorId\":\"32406400\",\"name\":\"J. Barker\"},{\"authorId\":\"75117630\",\"name\":\"S. Cunningham\"},{\"authorId\":\"48914950\",\"name\":\"X. Shao\"}],\"doi\":\"10.1121/1.2229005\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"title\":\"An audio-visual corpus for speech perception and automatic speech recognition.\",\"url\":\"https://www.semanticscholar.org/paper/5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":2006},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144739319\",\"name\":\"R. Lienhart\"}],\"doi\":\"10.1142/S021946780100027X\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10c8691e23dffadab19c39fb9245496efbf75b2a\",\"title\":\"Reliable Transition Detection in Videos: A Survey and Practitioner's Guide\",\"url\":\"https://www.semanticscholar.org/paper/10c8691e23dffadab19c39fb9245496efbf75b2a\",\"venue\":\"Int. J. Image Graph.\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54184-6_6\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"title\":\"Lip Reading in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144598072\",\"name\":\"D. King\"}],\"doi\":\"10.1145/1577069.1755843\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"title\":\"Dlib-ml: A Machine Learning Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1405.3531\",\"authors\":[{\"authorId\":\"1764761\",\"name\":\"K. Chatfield\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.28.6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14d9be7962a4ec5a6e55755f4c7588ea00793652\",\"title\":\"Return of the Devil in the Details: Delving Deep into Convolutional Nets\",\"url\":\"https://www.semanticscholar.org/paper/14d9be7962a4ec5a6e55755f4c7588ea00793652\",\"venue\":\"BMVC\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"104117062\",\"name\":\"Ziheng Zhou\"},{\"authorId\":\"83433495\",\"name\":\"G. Zhao\"},{\"authorId\":\"46761465\",\"name\":\"Xiaopeng Hong\"},{\"authorId\":\"145962204\",\"name\":\"M. Pietik\\u00e4inen\"}],\"doi\":\"10.1016/j.imavis.2014.06.004\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c842fbd4c032dd4d931eb6ff1eaa2a13450b7af\",\"title\":\"A review of recent advances in visual speech decoding\",\"url\":\"https://www.semanticscholar.org/paper/4c842fbd4c032dd4d931eb6ff1eaa2a13450b7af\",\"venue\":\"Image Vis. Comput.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3119923\",\"name\":\"K. Noda\"},{\"authorId\":\"145473520\",\"name\":\"Y. Yamaguchi\"},{\"authorId\":\"1764429\",\"name\":\"K. Nakadai\"},{\"authorId\":\"1775800\",\"name\":\"H. Okuno\"},{\"authorId\":\"50527812\",\"name\":\"Tetsuya Ogata\"}],\"doi\":\"10.1007/s10489-014-0629-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bca0ab6d69c820263865a17e183c952f3ff5aae7\",\"title\":\"Audio-visual speech recognition using deep learning\",\"url\":\"https://www.semanticscholar.org/paper/bca0ab6d69c820263865a17e183c952f3ff5aae7\",\"venue\":\"Applied Intelligence\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"143913738\",\"name\":\"S. Fern\\u00e1ndez\"},{\"authorId\":\"145842938\",\"name\":\"F. Gomez\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1145/1143844.1143891\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"96494e722f58705fa20302fe6179d483f52705b4\",\"title\":\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/96494e722f58705fa20302fe6179d483f52705b4\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":\"1603.04467\",\"authors\":[{\"authorId\":\"145832079\",\"name\":\"M. Abadi\"},{\"authorId\":\"145984138\",\"name\":\"A. Agarwal\"},{\"authorId\":\"144758007\",\"name\":\"P. Barham\"},{\"authorId\":\"2445241\",\"name\":\"E. Brevdo\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"48738717\",\"name\":\"Craig Citro\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"36347083\",\"name\":\"Andy Davis\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"145139947\",\"name\":\"M. Devin\"},{\"authorId\":\"1780892\",\"name\":\"Sanjay Ghemawat\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"3384453\",\"name\":\"A. Harp\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"2090818\",\"name\":\"M. Isard\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1944541\",\"name\":\"R. J\\u00f3zefowicz\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1942300\",\"name\":\"M. Kudlur\"},{\"authorId\":\"3369421\",\"name\":\"Josh Levenberg\"},{\"authorId\":\"143767989\",\"name\":\"Dan Man\\u00e9\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"144375552\",\"name\":\"Sherry Moore\"},{\"authorId\":\"20154699\",\"name\":\"D. Murray\"},{\"authorId\":\"153301219\",\"name\":\"Chris Olah\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"32163737\",\"name\":\"B. Steiner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"35210462\",\"name\":\"Kunal Talwar\"},{\"authorId\":\"2080690\",\"name\":\"P. Tucker\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"38062095\",\"name\":\"V. Vasudevan\"},{\"authorId\":\"1765169\",\"name\":\"F. Vi\\u00e9gas\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"47941411\",\"name\":\"Pete Warden\"},{\"authorId\":\"145233583\",\"name\":\"M. Wattenberg\"},{\"authorId\":\"35078078\",\"name\":\"Martin Wicke\"},{\"authorId\":\"47112093\",\"name\":\"Y. Yu\"},{\"authorId\":\"2777763\",\"name\":\"X. Zheng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"title\":\"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\",\"url\":\"https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1501.05396\",\"authors\":[{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"2293163\",\"name\":\"E. Marcheret\"},{\"authorId\":\"1782589\",\"name\":\"V. Goel\"}],\"doi\":\"10.1109/ICASSP.2015.7178347\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bceab5ec127b496b83a62131ab12e0b502f86e45\",\"title\":\"Deep multimodal learning for Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bceab5ec127b496b83a62131ab12e0b502f86e45\",\"venue\":\"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2015}],\"title\":\"Lip Reading Sentences in the Wild\",\"topics\":[{\"topic\":\"Speech recognition\",\"topicId\":\"2869\",\"url\":\"https://www.semanticscholar.org/topic/2869\"},{\"topic\":\"Long short-term memory\",\"topicId\":\"117199\",\"url\":\"https://www.semanticscholar.org/topic/117199\"},{\"topic\":\"Sigmoid function\",\"topicId\":\"106754\",\"url\":\"https://www.semanticscholar.org/topic/106754\"},{\"topic\":\"Overfitting\",\"topicId\":\"70499\",\"url\":\"https://www.semanticscholar.org/topic/70499\"},{\"topic\":\"Automated Lip Reading\",\"topicId\":\"1700892\",\"url\":\"https://www.semanticscholar.org/topic/1700892\"},{\"topic\":\"Convolutional neural network\",\"topicId\":\"29860\",\"url\":\"https://www.semanticscholar.org/topic/29860\"},{\"topic\":\"Open world\",\"topicId\":\"72157\",\"url\":\"https://www.semanticscholar.org/topic/72157\"},{\"topic\":\"Recurrence relation\",\"topicId\":\"13370\",\"url\":\"https://www.semanticscholar.org/topic/13370\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Batch processing\",\"topicId\":\"138\",\"url\":\"https://www.semanticscholar.org/topic/138\"},{\"topic\":\"Network model\",\"topicId\":\"20353\",\"url\":\"https://www.semanticscholar.org/topic/20353\"},{\"topic\":\"Constrained optimization\",\"topicId\":\"11366\",\"url\":\"https://www.semanticscholar.org/topic/11366\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Encoder\",\"topicId\":\"16744\",\"url\":\"https://www.semanticscholar.org/topic/16744\"},{\"topic\":\"Online and offline\",\"topicId\":\"12094\",\"url\":\"https://www.semanticscholar.org/topic/12094\"},{\"topic\":\"Discriminative model\",\"topicId\":\"39987\",\"url\":\"https://www.semanticscholar.org/topic/39987\"},{\"topic\":\"Call of Duty: Black Ops\",\"topicId\":\"2395412\",\"url\":\"https://www.semanticscholar.org/topic/2395412\"},{\"topic\":\"Random neural network\",\"topicId\":\"136146\",\"url\":\"https://www.semanticscholar.org/topic/136146\"},{\"topic\":\"Expectation propagation\",\"topicId\":\"527411\",\"url\":\"https://www.semanticscholar.org/topic/527411\"}],\"url\":\"https://www.semanticscholar.org/paper/bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}\n"