"{\"abstract\":\"This paper focuses on a novel and challenging vision task, dense video captioning, which aims to automatically describe a video clip with multiple informative and diverse caption sentences. The proposed method is trained without explicit annotation of fine-grained sentence to video region-sequence correspondence, but is only based on weak video-level sentence annotations. It differs from existing video captioning systems in three technical aspects. First, we propose lexical fully convolutional neural networks (Lexical-FCN) with weakly supervised multi-instance multi-label learning to weakly link video regions with lexical labels. Second, we introduce a novel submodular maximization scheme to generate multiple informative and diverse region-sequences based on the Lexical-FCN outputs. A winner-takes-all scheme is adopted to weakly associate sentences to region-sequences in the training phase. Third, a sequence-to-sequence learning based language model is trained with the weakly supervised information obtained through the association process. We show that the proposed method can not only produce informative and diverse dense captions, but also outperform state-of-the-art single video captioning methods by a large margin.\",\"arxivId\":\"1704.01502\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\",\"url\":\"https://www.semanticscholar.org/author/145314568\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\",\"url\":\"https://www.semanticscholar.org/author/46277052\"},{\"authorId\":\"47008023\",\"name\":\"Z. Su\",\"url\":\"https://www.semanticscholar.org/author/47008023\"},{\"authorId\":\"3700393\",\"name\":\"Minjun Li\",\"url\":\"https://www.semanticscholar.org/author/3700393\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\",\"url\":\"https://www.semanticscholar.org/author/6060281\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\",\"url\":\"https://www.semanticscholar.org/author/1717861\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\",\"url\":\"https://www.semanticscholar.org/author/145905953\"}],\"citationVelocity\":25,\"citations\":[{\"arxivId\":\"1806.01954\",\"authors\":[{\"authorId\":\"51011850\",\"name\":\"Iulia Duta\"},{\"authorId\":\"50986865\",\"name\":\"A. Nicolicioiu\"},{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c42f427b54ab12a1d89827ee4c6951efae733b55\",\"title\":\"Mining for meaning: from vision to language through multiple networks consensus\",\"url\":\"https://www.semanticscholar.org/paper/c42f427b54ab12a1d89827ee4c6951efae733b55\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":\"1910.12019\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Diverse Video Captioning Through Latent Variable Expansion with Conditional GAN\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2444704\",\"name\":\"Xindi Shang\"},{\"authorId\":\"79723716\",\"name\":\"D. Di\"},{\"authorId\":\"66358686\",\"name\":\"J. Xiao\"},{\"authorId\":\"144149886\",\"name\":\"Yu Cao\"},{\"authorId\":\"2028727\",\"name\":\"X. Yang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1145/3323873.3325056\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fa0388f5373a2f17a3c456346a52427c887667ea\",\"title\":\"Annotating Objects and Relations in User-Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/fa0388f5373a2f17a3c456346a52427c887667ea\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":\"2007.11888\",\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"1796216614\",\"name\":\"Siyu Huang\"},{\"authorId\":\"1796254\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.24963/ijcai.2020/88\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"884be34dd5d2ea78940da96d2813be7768933857\",\"title\":\"SBAT: Video Captioning with Sparse Boundary-Aware Transformer\",\"url\":\"https://www.semanticscholar.org/paper/884be34dd5d2ea78940da96d2813be7768933857\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"1809.04938\",\"authors\":[{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"145500855\",\"name\":\"Lei Cui\"},{\"authorId\":\"10780897\",\"name\":\"Damai Dai\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.1609/AAAI.V33I01.33016810\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"09e39f4334417f92bddc20072f43efade9bd5b60\",\"title\":\"LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/09e39f4334417f92bddc20072f43efade9bd5b60\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938051940\",\"name\":\"Dylan Flaute\"},{\"authorId\":\"2405109\",\"name\":\"B. Narayanan\"}],\"doi\":\"10.1117/12.2568016\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"title\":\"Video captioning using weakly supervised convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":\"2005.00596\",\"authors\":[{\"authorId\":\"34145947\",\"name\":\"Zhuolin Jiang\"},{\"authorId\":\"3330139\",\"name\":\"J. Silovsk\\u00fd\"},{\"authorId\":\"143882614\",\"name\":\"M. Siu\"},{\"authorId\":\"144339076\",\"name\":\"W. Hartmann\"},{\"authorId\":\"1793645\",\"name\":\"H. Gish\"},{\"authorId\":\"32484187\",\"name\":\"S. Adali\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0d4ead18fc29e22a5f8b4d42e2f6041861b65b58\",\"title\":\"Learning from Noisy Labels with Noise Modeling Network\",\"url\":\"https://www.semanticscholar.org/paper/0d4ead18fc29e22a5f8b4d42e2f6041861b65b58\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1813915\",\"name\":\"S. Liu\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3240508.3240667\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"849642b4701ac11c035326069f707f23a51a6f1a\",\"title\":\"SibNet: Sibling Convolutional Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/849642b4701ac11c035326069f707f23a51a6f1a\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b47776ecc194616d5ae789357ac69b1298e47ae\",\"title\":\"Frames CNN Low-level Encoder ( Bi-LSTM ) High-level Encoder ( LSTM ) Worker Manager Internal Critic Environment segment signal goal state reward action HRL Agent context context\",\"url\":\"https://www.semanticscholar.org/paper/1b47776ecc194616d5ae789357ac69b1298e47ae\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1910.02930\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/K19-1039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"title\":\"A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"1812.03849\",\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1688516\",\"name\":\"Jingdong Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"title\":\"Weakly Supervised Dense Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"1911.02739\",\"authors\":[{\"authorId\":\"72871419\",\"name\":\"Zhihan Zhang\"},{\"authorId\":\"1770874\",\"name\":\"Zhiyi Yin\"},{\"authorId\":\"1906099\",\"name\":\"Shuhuai Ren\"},{\"authorId\":\"78145275\",\"name\":\"Xinhang Li\"},{\"authorId\":\"50341802\",\"name\":\"S. Li\"}],\"doi\":\"10.1007/978-3-030-60457-8_1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"422f098e1e5b6fa34e73ad83557ba793ca3c0403\",\"title\":\"DCA: Diversified Co-Attention towards Informative Live Video Commenting\",\"url\":\"https://www.semanticscholar.org/paper/422f098e1e5b6fa34e73ad83557ba793ca3c0403\",\"venue\":\"NLPCC\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8194130\",\"name\":\"Qinyu Li\"}],\"doi\":\"10.1145/3303083\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"91aa0eb38446643cd622b060a76043b0ca2d7991\",\"title\":\"Rich Visual and Language Representation with Complementary Semantics for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/91aa0eb38446643cd622b060a76043b0ca2d7991\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1809.09294\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":null,\"name\":\"Zhuang Liu\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/TPAMI.2019.2922181\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1c98035c32e783b7450f1779af572a65e6fc23f\",\"title\":\"Object Detection from Scratch with Deep Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f1c98035c32e783b7450f1779af572a65e6fc23f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1807.10018\",\"authors\":[{\"authorId\":\"51152390\",\"name\":\"Yilei Xiong\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01252-6_29\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"title\":\"Move Forward and Tell: A Progressive Generator of Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2011.10190\",\"authors\":[{\"authorId\":\"102811267\",\"name\":\"Reza Ghoddoosian\"},{\"authorId\":\"34282786\",\"name\":\"Saif Sayed\"},{\"authorId\":\"1720747\",\"name\":\"V. Athitsos\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"041f428cf81939985843402b72521289ea48b2e6\",\"title\":\"Action Duration Prediction for Segment-Level Alignment of Weakly-Labeled Videos\",\"url\":\"https://www.semanticscholar.org/paper/041f428cf81939985843402b72521289ea48b2e6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.13448\",\"authors\":[{\"authorId\":\"153030413\",\"name\":\"Xuenan Xu\"},{\"authorId\":\"2451839\",\"name\":\"H. Dinkel\"},{\"authorId\":\"3000684\",\"name\":\"Mengyue Wu\"},{\"authorId\":\"143819768\",\"name\":\"K. Yu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f3f733d760a9b5ae8453874571765e9e17defdb\",\"title\":\"Audio Caption in a Car Setting with a Sentence-Level Loss.\",\"url\":\"https://www.semanticscholar.org/paper/9f3f733d760a9b5ae8453874571765e9e17defdb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144364295\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"title\":\"TVT: Two-View Transformer Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"venue\":\"ACML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"}],\"doi\":\"10.7916/D8-SRKZ-T696\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ea5b4e02a448174cbc6872eaec27695b04e9b87\",\"title\":\"Deep Learning for Action Understanding in Video\",\"url\":\"https://www.semanticscholar.org/paper/3ea5b4e02a448174cbc6872eaec27695b04e9b87\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"145211780\",\"name\":\"Bin Fan\"},{\"authorId\":\"1380311632\",\"name\":\"Shinming Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.18653/v1/D19-1213\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"title\":\"Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag\",\"url\":\"https://www.semanticscholar.org/paper/ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48625207\",\"name\":\"W. Li\"},{\"authorId\":\"46584793\",\"name\":\"J. Wang\"},{\"authorId\":\"2690741\",\"name\":\"Shengbei Wang\"},{\"authorId\":\"1809607\",\"name\":\"G. Jin\"}],\"doi\":\"10.1007/978-3-319-97289-3_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"473c74782b0120aa0a8b0e94f49fea6542a6d7da\",\"title\":\"Get the Whole Action Event by Action Stage Classification\",\"url\":\"https://www.semanticscholar.org/paper/473c74782b0120aa0a8b0e94f49fea6542a6d7da\",\"venue\":\"PKAW\",\"year\":2018},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51001584\",\"name\":\"Hyung-min Lee\"},{\"authorId\":\"153481384\",\"name\":\"Il-Koo Kim\"}],\"doi\":\"10.1109/IJCNN.2019.8851892\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"title\":\"Generating Natural Video Descriptions using Semantic Gate\",\"url\":\"https://www.semanticscholar.org/paper/b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1805.07054\",\"authors\":[{\"authorId\":\"31943350\",\"name\":\"J. Tremblay\"},{\"authorId\":\"40929818\",\"name\":\"T. To\"},{\"authorId\":\"143625418\",\"name\":\"Artem Molchanov\"},{\"authorId\":\"2342481\",\"name\":\"S. Tyree\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2238841\",\"name\":\"Stan Birchfield\"}],\"doi\":\"10.1109/ICRA.2018.8460642\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1db058654185978c5b1e12a5ceb064567d797947\",\"title\":\"Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/1db058654185978c5b1e12a5ceb064567d797947\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":\"1904.04460\",\"authors\":[{\"authorId\":\"2088511\",\"name\":\"Zeyuan Wang\"},{\"authorId\":\"144179461\",\"name\":\"J. Poon\"},{\"authorId\":\"46238290\",\"name\":\"Shiding Sun\"},{\"authorId\":\"143885147\",\"name\":\"Simon Poon\"}],\"doi\":\"10.1109/IJCNN.2019.8851846\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1221421a69f8677a378bb75e4db8c0a17efe09a3\",\"title\":\"Attention-based Multi-instance Neural Network for Medical Diagnosis from Incomplete and Low Quality Data\",\"url\":\"https://www.semanticscholar.org/paper/1221421a69f8677a378bb75e4db8c0a17efe09a3\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97cf2d8581eb7ec0fcd2ead5410473d8440eb3a6\",\"title\":\"Supplementary Material for CVPR 2018 paper # 330\",\"url\":\"https://www.semanticscholar.org/paper/97cf2d8581eb7ec0fcd2ead5410473d8440eb3a6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1905.13448\",\"authors\":[{\"authorId\":\"153030413\",\"name\":\"Xuenan Xu\"},{\"authorId\":\"2451839\",\"name\":\"H. Dinkel\"},{\"authorId\":\"3000684\",\"name\":\"Mengyue Wu\"},{\"authorId\":\"1736727\",\"name\":\"Kai Yu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"346ed15dca43e302f51571bdb1349380604d1ff8\",\"title\":\"What does a Car-ssette tape tell?\",\"url\":\"https://www.semanticscholar.org/paper/346ed15dca43e302f51571bdb1349380604d1ff8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1806.04860\",\"authors\":[{\"authorId\":\"47008023\",\"name\":\"Z. Su\"},{\"authorId\":\"144469723\",\"name\":\"C. Zhu\"},{\"authorId\":\"3431029\",\"name\":\"Y. Dong\"},{\"authorId\":\"1751449\",\"name\":\"Dongqi Cai\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"}],\"doi\":\"10.1109/CVPR.2018.00807\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33f08157b959070ba802afbb135f4336c5a426fd\",\"title\":\"Learning Visual Knowledge Memory Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/33f08157b959070ba802afbb135f4336c5a426fd\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":null,\"name\":\"Hang Gao\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"1789840\",\"name\":\"K. Miyazawa\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"563734713a37f3db7d37eabde24e6184495a1567\",\"title\":\"AutoLoc: Weakly-supervised Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/563734713a37f3db7d37eabde24e6184495a1567\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255213\",\"name\":\"Z. Zhang\"},{\"authorId\":\"38188040\",\"name\":\"Dong Xu\"},{\"authorId\":\"47337540\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"2597292\",\"name\":\"Chuanqi Tan\"}],\"doi\":\"10.1109/TCSVT.2019.2936526\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"title\":\"Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization\",\"url\":\"https://www.semanticscholar.org/paper/b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"1974929\",\"name\":\"Jiewu Xia\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1007/s11042-020-09674-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"title\":\"Double-channel language feature mining based model for video description\",\"url\":\"https://www.semanticscholar.org/paper/0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"venue\":\"Multim. Tools Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72445881\",\"name\":\"Xinhong Ma\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"48258806\",\"name\":\"C. Xu\"}],\"doi\":\"10.1109/TMM.2019.2902100\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"857210ef01825dd18fa050b36d51c0b28da7744c\",\"title\":\"Deep Multi-Modality Adversarial Networks for Unsupervised Domain Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/857210ef01825dd18fa050b36d51c0b28da7744c\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"2007.10631\",\"authors\":[{\"authorId\":\"1882695\",\"name\":\"X. Wang\"},{\"authorId\":\"49616225\",\"name\":\"Sujoy Paul\"},{\"authorId\":\"30164077\",\"name\":\"Dripta S. Raychaudhuri\"},{\"authorId\":\"94628296\",\"name\":\"M. Liu\"},{\"authorId\":null,\"name\":\"Yaonan Wang\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"50e5ca477ee8cc2b9dd58de78ceacf37cc085d18\",\"title\":\"Learning Person Re-identification Models from Videos with Weak Supervision\",\"url\":\"https://www.semanticscholar.org/paper/50e5ca477ee8cc2b9dd58de78ceacf37cc085d18\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1712.09532\",\"authors\":[{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"2763884\",\"name\":\"G. Henter\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"1700567\",\"name\":\"Shin'ichi Satoh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"96c866f07ff999ee11459519aa361fa4fdfc2139\",\"title\":\"Consensus-based Sequence Training for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96c866f07ff999ee11459519aa361fa4fdfc2139\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145113946\",\"name\":\"J. Tang\"},{\"authorId\":\"144005516\",\"name\":\"Jing Wang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3291925\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a437bb550d1df02188e4b145e01675551da36336\",\"title\":\"Show, Reward, and Tell\",\"url\":\"https://www.semanticscholar.org/paper/a437bb550d1df02188e4b145e01675551da36336\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741830\",\"name\":\"Haibo Li\"},{\"authorId\":\"48204604\",\"name\":\"Liqing Gao\"},{\"authorId\":\"81631713\",\"name\":\"Ruize Han\"},{\"authorId\":\"39124212\",\"name\":\"L. Wan\"},{\"authorId\":\"1409949029\",\"name\":\"W. Feng\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054316\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1cd0738c83bdd4f3b489b30b099d2b85bb9c02d2\",\"title\":\"Key Action and Joint CTC-Attention based Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1cd0738c83bdd4f3b489b30b099d2b85bb9c02d2\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2004.00760\",\"authors\":[{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.14288/1.0392691\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"title\":\"Consistent Multiple Sequence Decoding\",\"url\":\"https://www.semanticscholar.org/paper/5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1808.04091\",\"authors\":[{\"authorId\":\"10780897\",\"name\":\"Damai Dai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"771f66019981b451c29d152c199e9e21a00ef29e\",\"title\":\"Live Video Comment Generation Based on Surrounding Frames and Live Comments\",\"url\":\"https://www.semanticscholar.org/paper/771f66019981b451c29d152c199e9e21a00ef29e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46276803\",\"name\":\"J. Li\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1016/j.jvcir.2020.102875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"title\":\"Translating video into language by enhancing visual and language representations\",\"url\":\"https://www.semanticscholar.org/paper/32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1007/s00530-018-0598-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"title\":\"Multi-guiding long short-term memory for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"venue\":\"Multimedia Systems\",\"year\":2018},{\"arxivId\":\"1708.09667\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3123420\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6199348281e14a5a127b539f5cdb92fcddbac17\",\"title\":\"Video Captioning with Guidance of Multimodal Latent Topics\",\"url\":\"https://www.semanticscholar.org/paper/a6199348281e14a5a127b539f5cdb92fcddbac17\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1901.00097\",\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"2031845\",\"name\":\"Junbo Guo\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"title\":\"Not All Words Are Equal: Video-specific Information Loss for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"96066534\",\"name\":\"P. Joshi\"},{\"authorId\":\"51497543\",\"name\":\"Chitwan Saharia\"},{\"authorId\":\"1557378944\",\"name\":\"V. Singh\"},{\"authorId\":\"51267359\",\"name\":\"Digvijaysingh Gautam\"},{\"authorId\":\"145799547\",\"name\":\"Ganesh Ramakrishnan\"},{\"authorId\":\"1557645545\",\"name\":\"P. Jyothi\"}],\"doi\":\"10.1109/ICCVW.2019.00459\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"title\":\"A Tale of Two Modalities for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018191\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"title\":\"Motion Guided Spatial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1804.05113\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1609/AAAI.V33I01.33019062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"title\":\"Multilevel Language and Vision Integration for Text-to-Clip Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41030694\",\"name\":\"Huanyu Yu\"},{\"authorId\":\"3392007\",\"name\":\"Shuo Cheng\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"7272302\",\"name\":\"Minsi Wang\"},{\"authorId\":\"40430880\",\"name\":\"J. Zhang\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"}],\"doi\":\"10.1109/CVPR.2018.00629\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f5876f67129a80a1ee753f715efcd2e2109bf432\",\"title\":\"Fine-Grained Video Captioning for Sports Narrative\",\"url\":\"https://www.semanticscholar.org/paper/f5876f67129a80a1ee753f715efcd2e2109bf432\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1711.06330\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":\"10.1109/CVPR.2018.00710\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66aebb3af16aaa78579344784212ae10f60ec27e\",\"title\":\"Attend and Interact: Higher-Order Object Interactions for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/66aebb3af16aaa78579344784212ae10f60ec27e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"title\":\"Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886528\",\"name\":\"Guolong Wang\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"},{\"authorId\":\"2168639\",\"name\":\"Kaiping Xu\"},{\"authorId\":\"145489794\",\"name\":\"K. Huang\"},{\"authorId\":\"19204816\",\"name\":\"Shuxiong Ye\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"title\":\"Bridge Video and Text with Cascade Syntactic Structure\",\"url\":\"https://www.semanticscholar.org/paper/b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":\"1907.12905\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":\"10.1145/3343031.3351060\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"title\":\"Watch It Twice: Video Captioning with a Refocused Video Encoder\",\"url\":\"https://www.semanticscholar.org/paper/5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5781871\",\"name\":\"Jiaqi Su\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"title\":\"Study of Video Captioning Problem\",\"url\":\"https://www.semanticscholar.org/paper/511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TCSVT.2018.2867286\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"title\":\"Dual-Stream Recurrent Neural Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1016/j.neucom.2019.08.042\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"00b350e4211dd5ed4791744920e664880cd3fd3a\",\"title\":\"Recurrent convolutional video captioning with global and local attention\",\"url\":\"https://www.semanticscholar.org/paper/00b350e4211dd5ed4791744920e664880cd3fd3a\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"1711.11135\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/CVPR.2018.00443\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"74b284a66e75b65f5970d05bac000fe91243ee49\",\"title\":\"Video Captioning via Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/74b284a66e75b65f5970d05bac000fe91243ee49\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1807.10418\",\"authors\":[{\"authorId\":\"49616225\",\"name\":\"Sujoy Paul\"},{\"authorId\":\"2177805\",\"name\":\"S. Roy\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1007/978-3-030-01225-0_35\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2ed766ca48d42ac57e16f30ca039fc8aa960189\",\"title\":\"W-TALC: Weakly-supervised Temporal Activity Localization and Classification\",\"url\":\"https://www.semanticscholar.org/paper/b2ed766ca48d42ac57e16f30ca039fc8aa960189\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.1145/3123266.3127895\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"title\":\"Richer Semantic Visual and Language Representation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1711.06354\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"title\":\"Grounded Objects and Interactions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TMM.2019.2896515\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"title\":\"Generating Video Descriptions With Latent Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"1803.11438\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"37378985\",\"name\":\"Wei Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2018.00795\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"title\":\"Reconstruction Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1804.05448\",\"authors\":[{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.18653/v1/N18-2125\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"title\":\"Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1591133899\",\"name\":\"Yiwei Zhang\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3351094\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0b12c784965baac88b6597890303fa834fa9eea\",\"title\":\"Watch, Reason and Code: Learning to Represent Videos Using Program\",\"url\":\"https://www.semanticscholar.org/paper/c0b12c784965baac88b6597890303fa834fa9eea\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1807.08333\",\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":null,\"name\":\"Hang Gao\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"1789840\",\"name\":\"K. Miyazawa\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1007/978-3-030-01270-0_10\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"523909d26ea94eaa0dd0285ba6ea0cd00a0aa7ca\",\"title\":\"AutoLoc: Weakly-Supervised Temporal Action Localization in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/523909d26ea94eaa0dd0285ba6ea0cd00a0aa7ca\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"2272096\",\"name\":\"Guo-Jun Qi\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/WACV.2019.00034\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"449b9189e3058d33f871dfd3b07cc75a717038f7\",\"title\":\"Improving Diversity of Image Captioning Through Variational Autoencoders and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/449b9189e3058d33f871dfd3b07cc75a717038f7\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"Jinglun Shi\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Describing Video with Multiple Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.24963/ijcai.2019/877\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"title\":\"Deep Learning for Video Captioning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":\"10.1007/978-3-030-31756-0_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"title\":\"The Encoder-Decoder Framework and Its Applications\",\"url\":\"https://www.semanticscholar.org/paper/0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":\"1902.09254\",\"authors\":[{\"authorId\":\"3000684\",\"name\":\"Mengyue Wu\"},{\"authorId\":\"2451839\",\"name\":\"H. Dinkel\"},{\"authorId\":\"1736727\",\"name\":\"Kai Yu\"}],\"doi\":\"10.1109/ICASSP.2019.8682377\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b31911b0c1e248e998fcd9356ad237578511c41\",\"title\":\"Audio Caption: Listen and Tell\",\"url\":\"https://www.semanticscholar.org/paper/9b31911b0c1e248e998fcd9356ad237578511c41\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1803.07950\",\"authors\":[{\"authorId\":\"4322411\",\"name\":\"L. Li\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/WACV.2019.00042\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"title\":\"End-to-End Video Captioning With Multitask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":\"1911.00212\",\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.18653/v1/D19-1207\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"55f546209c01530a7717d4170aa24947c6b92775\",\"title\":\"Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55f546209c01530a7717d4170aa24947c6b92775\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1906.01452\",\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474871\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/TPAMI.2019.2920899\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83a3fe38887880bccc15daa740d8d5041f826d91\",\"title\":\"Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/83a3fe38887880bccc15daa740d8d5041f826d91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020}],\"corpusId\":1350355,\"doi\":\"10.1109/CVPR.2017.548\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":10,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. J. Kusner\"},{\"authorId\":null,\"name\":\"Y. Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"From word embeddings to document distances. In ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S Deerwester\"},{\"authorId\":null,\"name\":\"S T Dumais\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Indexing by latent semantic analysis. Journal of the American society for information science\",\"url\":\"\",\"venue\":\"Indexing by latent semantic analysis. Journal of the American society for information science\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1702139\",\"name\":\"J. Leskovec\"},{\"authorId\":\"145343838\",\"name\":\"Andreas Krause\"},{\"authorId\":\"1730156\",\"name\":\"Carlos Guestrin\"},{\"authorId\":\"1702392\",\"name\":\"C. Faloutsos\"},{\"authorId\":\"3009469\",\"name\":\"J. VanBriesen\"},{\"authorId\":\"1726974\",\"name\":\"N. Glance\"}],\"doi\":\"10.1145/1281192.1281239\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0f22867694d99decf361092ebf4adf1f153e5e1b\",\"title\":\"Cost-effective outbreak detection in networks\",\"url\":\"https://www.semanticscholar.org/paper/0f22867694d99decf361092ebf4adf1f153e5e1b\",\"venue\":\"KDD '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Q. Jin\"},{\"authorId\":null,\"name\":\"J. Chen\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Describing videos using multi-modal fusion. In ACM Multimedia Grand Challenge\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1705564\",\"name\":\"G. Nemhauser\"},{\"authorId\":\"1736128\",\"name\":\"L. Wolsey\"},{\"authorId\":\"143904924\",\"name\":\"M. L. Fisher\"}],\"doi\":\"10.1007/BF01588971\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b9e43395663f74c581982e9ca97a0d7057a0008c\",\"title\":\"An analysis of approximations for maximizing submodular set functions\\u2014I\",\"url\":\"https://www.semanticscholar.org/paper/b9e43395663f74c581982e9ca97a0d7057a0008c\",\"venue\":\"Math. Program.\",\"year\":1978},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2365155\",\"name\":\"S. Deerwester\"},{\"authorId\":\"1728602\",\"name\":\"S. Dumais\"},{\"authorId\":\"1836606\",\"name\":\"T. Landauer\"},{\"authorId\":\"2737579\",\"name\":\"G. Furnas\"},{\"authorId\":\"3154682\",\"name\":\"R. Harshman\"}],\"doi\":\"10.1002/(SICI)1097-4571(199009)41:6%3C391::AID-ASI1%3E3.0.CO;2-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20a80a7356859daa4170fb4da6b87b84adbb547f\",\"title\":\"Indexing by Latent Semantic Analysis\",\"url\":\"https://www.semanticscholar.org/paper/20a80a7356859daa4170fb4da6b87b84adbb547f\",\"venue\":\"J. Am. Soc. Inf. Sci.\",\"year\":1990},{\"arxivId\":\"1605.05440\",\"authors\":[{\"authorId\":\"145568592\",\"name\":\"Andrew Shin\"},{\"authorId\":\"8197937\",\"name\":\"K. Ohnishi\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/ICIP.2016.7532983\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"41aa209e9d294d370357434f310d49b2b0baebeb\",\"title\":\"Beyond caption to narrative: Video captioning with multiple sentences\",\"url\":\"https://www.semanticscholar.org/paper/41aa209e9d294d370357434f310d49b2b0baebeb\",\"venue\":\"2016 IEEE International Conference on Image Processing (ICIP)\",\"year\":2016},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1145/2964284.2984066\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"61d409b92860480a9188d23ba59b822ddc6331f9\",\"title\":\"Multimodal Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61d409b92860480a9188d23ba59b822ddc6331f9\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2490700\",\"name\":\"Boris Babenko\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"}],\"doi\":\"10.1109/cvprw.2009.5206737\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"29232c81c51b961ead3d38e6838fe0fb9c279e01\",\"title\":\"Visual tracking with online Multiple Instance Learning\",\"url\":\"https://www.semanticscholar.org/paper/29232c81c51b961ead3d38e6838fe0fb9c279e01\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2133220\",\"name\":\"C. Cervantes\"},{\"authorId\":\"145507543\",\"name\":\"Juan C. Caicedo\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1109/ICCV.2015.303\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"11c9c31dff70de92ada9160c78ff8bb46b2912d6\",\"title\":\"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\",\"url\":\"https://www.semanticscholar.org/paper/11c9c31dff70de92ada9160c78ff8bb46b2912d6\",\"venue\":\"ICCV\",\"year\":2015},{\"arxivId\":\"1606.07770\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2017.130\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9aa3bafa9e8e21bb92908ae23b468fa248239b3\",\"title\":\"Captioning Images with Diverse Objects\",\"url\":\"https://www.semanticscholar.org/paper/b9aa3bafa9e8e21bb92908ae23b468fa248239b3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1506.01497\",\"authors\":[{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"144716314\",\"name\":\"J. Sun\"}],\"doi\":\"10.1109/TPAMI.2016.2577031\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"title\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1725557\",\"name\":\"L. Lov\\u00e1sz\"}],\"doi\":\"10.1007/978-3-642-68874-4_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4821d2dee04f27e55112090c9029da30cc8b291\",\"title\":\"Submodular functions and convexity\",\"url\":\"https://www.semanticscholar.org/paper/b4821d2dee04f27e55112090c9029da30cc8b291\",\"venue\":\"ISMP\",\"year\":1982},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Vinyals\"},{\"authorId\":null,\"name\":\"A. Toshev\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Show and tell: A neural image caption generator. In CVPR\",\"year\":2015},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Leskovec\"},{\"authorId\":null,\"name\":\"A. Krause\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Cost-effective outbreak detection in networks. In ACM SIGKDD\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"},{\"authorId\":\"145551629\",\"name\":\"H. Grabner\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/CVPR.2015.7298928\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cfcb9bcc1e8b4d3451578398aeb37f0fa5614632\",\"title\":\"Video summarization by learning submodular mixtures of objectives\",\"url\":\"https://www.semanticscholar.org/paper/cfcb9bcc1e8b4d3451578398aeb37f0fa5614632\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"3493516\",\"name\":\"Yifan Xiong\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/2964284.2984065\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"7492cac0babe8d514995bcde6456ae00c17325a3\",\"title\":\"Describing Videos using Multi-modal Fusion\",\"url\":\"https://www.semanticscholar.org/paper/7492cac0babe8d514995bcde6456ae00c17325a3\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1406.4729\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1007/978-3-319-10578-9_23\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbb19236820a96038d000dc629225d36e0b6294a\",\"title\":\"Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/cbb19236820a96038d000dc629225d36e0b6294a\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Papineni\"},{\"authorId\":null,\"name\":\"S. Roukos\"},{\"authorId\":null,\"name\":\"T. Ward\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Bleu: a method for automatic evaluation of machine translation. In ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Dong\"},{\"authorId\":null,\"name\":\"X. Li\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Early embedding and late reranking for video captioning. In ACM Multimedia Grand Challenge\",\"year\":2016},{\"arxivId\":\"1608.04959\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1145/2964284.2984062\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"41eec260e0980f8fd0af46f0dfc043139087a928\",\"title\":\"Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/41eec260e0980f8fd0af46f0dfc043139087a928\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1409.2329\",\"authors\":[{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97\",\"title\":\"Recurrent Neural Network Regularization\",\"url\":\"https://www.semanticscholar.org/paper/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3331580\",\"name\":\"O. Maron\"},{\"authorId\":\"1388700951\",\"name\":\"Tomas Lozano-Perez\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d8340eae2c98ab5e0a3b1a7e071a7ddb9106cff\",\"title\":\"A Framework for Multiple-Instance Learning\",\"url\":\"https://www.semanticscholar.org/paper/4d8340eae2c98ab5e0a3b1a7e071a7ddb9106cff\",\"venue\":\"NIPS\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Kulkarni\"},{\"authorId\":null,\"name\":\"V. Premraj\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Babytalk: Understanding and generating simple image descriptions. IEEE T PAMI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2594026\",\"name\":\"Stephanie Pancoast\"},{\"authorId\":\"1798628\",\"name\":\"M. Akbacak\"}],\"doi\":\"10.1109/ICASSP.2014.6853821\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"a9250f7aac04d927aa20d343f57a0e8df318ff87\",\"title\":\"Softening quantization in bag-of-audio-words\",\"url\":\"https://www.semanticscholar.org/paper/a9250f7aac04d927aa20d343f57a0e8df318ff87\",\"venue\":\"2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2014},{\"arxivId\":\"1511.07571\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.494\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"title\":\"DenseCap: Fully Convolutional Localization Networks for Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Das\"},{\"authorId\":null,\"name\":\"C. Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching. In CVPR\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Venugopalan\"},{\"authorId\":null,\"name\":\"M. Rohrbach\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Sequence to sequence-video to text. In ICCV\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"L. Anne Hendricks\"},{\"authorId\":null,\"name\":\"S. Venugopalan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Deep compositional captioning: Describing novel object categories without paired training data. In CVPR\",\"year\":2016},{\"arxivId\":\"1506.02640\",\"authors\":[{\"authorId\":\"40497777\",\"name\":\"Joseph Redmon\"},{\"authorId\":\"2038685\",\"name\":\"S. Divvala\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2016.91\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8e79ac0ea341056ef20f2616628b3e964764cfd\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.05284\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPR.2016.8\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"title\":\"Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data\",\"url\":\"https://www.semanticscholar.org/paper/e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Donahue\"},{\"authorId\":null,\"name\":\"L. Anne\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Long-term recurrent convolutional networks for visual recognition and description. In CVPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"B. A. Plummer\"},{\"authorId\":null,\"name\":\"L. Wang\"},{\"authorId\":null,\"name\":\"C. M. Cervantes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models. In ICCV\",\"year\":2015},{\"arxivId\":\"1304.1511\",\"authors\":[{\"authorId\":\"48099028\",\"name\":\"D. Heckerman\"}],\"doi\":\"10.1016/b978-0-444-88738-2.50020-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c136d34c175dd299c260d69c567a1ba21fcd797\",\"title\":\"A Tractable Inference Algorithm for Diagnosing Multiple Diseases\",\"url\":\"https://www.semanticscholar.org/paper/2c136d34c175dd299c260d69c567a1ba21fcd797\",\"venue\":\"UAI\",\"year\":1989},{\"arxivId\":\"1504.08083\",\"authors\":[{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"}],\"doi\":\"10.1109/ICCV.2015.169\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ffdbc358b63378f07311e883dddacc9faeeaf4b\",\"title\":\"Fast R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2013.340\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"title\":\"A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching\",\"url\":\"https://www.semanticscholar.org/paper/a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Deerwester\"},{\"authorId\":null,\"name\":\"S. T. Dumais\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Indexing by latent semantic analysis. Journal of the American society for information science, 41(6):391\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1940272\",\"name\":\"Matt J. Kusner\"},{\"authorId\":\"35760122\",\"name\":\"Yu Sun\"},{\"authorId\":\"1971973\",\"name\":\"Nicholas I. Kolkin\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66021a920001bc3e6258bffe7076d647614147b7\",\"title\":\"From Word Embeddings To Document Distances\",\"url\":\"https://www.semanticscholar.org/paper/66021a920001bc3e6258bffe7076d647614147b7\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Shin\"},{\"authorId\":null,\"name\":\"K. Ohnishi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Beyond caption to narrative: Video captioning with multiple sentences. arXiv:1605.05440\",\"year\":2016},{\"arxivId\":\"1403.6173\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"113090874\",\"name\":\"W. Qiu\"},{\"authorId\":\"33985877\",\"name\":\"Annemarie Friedrich\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-11752-2_15\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"889e723cd6d581e120ee6776b231fdf69707ab50\",\"title\":\"Coherent Multi-sentence Video Description with Variable Level of Detail\",\"url\":\"https://www.semanticscholar.org/paper/889e723cd6d581e120ee6776b231fdf69707ab50\",\"venue\":\"GCPR\",\"year\":2014},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1731948\",\"name\":\"P. Viola\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1706673\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"521768f7772163bc7c57ae2c9855889abb747fbb\",\"title\":\"Multiple Instance Boosting for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/521768f7772163bc7c57ae2c9855889abb747fbb\",\"venue\":\"NIPS\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Tran\"},{\"authorId\":null,\"name\":\"L. Bourdev\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Learning spatiotemporal features with 3d convolutional networks. In ICCV\",\"year\":2015},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H. Fang\"},{\"authorId\":null,\"name\":\"S. Gupta\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"From captions to visual concepts and back. In CVPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"L. Yao\"},{\"authorId\":null,\"name\":\"A. Torabi\"},{\"authorId\":null,\"name\":\"K. Cho\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Describing videos by exploiting temporal structure. In ICCV\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Redmon\"},{\"authorId\":null,\"name\":\"S. Divvala\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"You only look once: Unified, realtime object detection. In CVPR\",\"year\":2016},{\"arxivId\":\"1505.04870\",\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2133220\",\"name\":\"C. Cervantes\"},{\"authorId\":\"145507543\",\"name\":\"Juan C. Caicedo\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1007/s11263-016-0965-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0612745dbd292fc0a548a16d39cd73e127faedde\",\"title\":\"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\",\"url\":\"https://www.semanticscholar.org/paper/0612745dbd292fc0a548a16d39cd73e127faedde\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\"},{\"authorId\":\"3039887\",\"name\":\"M. Zhang\"}],\"doi\":\"10.7551/mitpress/7503.003.0206\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71564232ce7ff2187c1d73cc2d52fa79241c7113\",\"title\":\"Multi-Instance Multi-Label Learning with Application to Scene Classification\",\"url\":\"https://www.semanticscholar.org/paper/71564232ce7ff2187c1d73cc2d52fa79241c7113\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1411.4038\",\"authors\":[{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2572683\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"317aee7fc081f2b137a85c4f20129007fd8e717e\",\"title\":\"Fully Convolutional Networks for Semantic Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/317aee7fc081f2b137a85c4f20129007fd8e717e\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3259253\",\"name\":\"Kristina Toutanova\"},{\"authorId\":\"38666915\",\"name\":\"D. Klein\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"1740765\",\"name\":\"Y. Singer\"}],\"doi\":\"10.3115/1073445.1073478\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42a490cf4f186d3383c92963817d100afd81e2\",\"title\":\"Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network\",\"url\":\"https://www.semanticscholar.org/paper/eb42a490cf4f186d3383c92963817d100afd81e2\",\"venue\":\"HLT-NAACL\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1723930\",\"name\":\"A. Jepson\"},{\"authorId\":\"1793739\",\"name\":\"David J. Fleet\"},{\"authorId\":\"1403822242\",\"name\":\"Thomas F. El-Maraghi\"}],\"doi\":\"10.1109/TPAMI.2003.1233903\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"aa40627d315e814c2ab4c8928942a7b46c7bf01f\",\"title\":\"Robust Online Appearance Models for Visual Tracking\",\"url\":\"https://www.semanticscholar.org/paper/aa40627d315e814c2ab4c8928942a7b46c7bf01f\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2003},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"1890615\",\"name\":\"Y. Huo\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1145/2964284.2984064\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"f28cd3ad13fe0b3e94d7c49886648fb164601f20\",\"title\":\"Early Embedding and Late Reranking for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f28cd3ad13fe0b3e94d7c49886648fb164601f20\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Rohrbach\"},{\"authorId\":null,\"name\":\"M. Rohrbach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Coherent multi-sentence video description with variable level of detail. In GCPR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"36794849\",\"name\":\"L. Zhang\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.1109/ICCVW.2011.6130425\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fbadf89b990acedf23e1df03d4869010d2dbc59e\",\"title\":\"Human Focused Video Description\",\"url\":\"https://www.semanticscholar.org/paper/fbadf89b990acedf23e1df03d4869010d2dbc59e\",\"venue\":\"2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Toutanova\"},{\"authorId\":null,\"name\":\"D. Klein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Feature-rich part-of-speech tagging with a cyclic dependency network. In NAACL\",\"year\":2003},{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Russakovsky\"},{\"authorId\":null,\"name\":\"J. Deng\"},{\"authorId\":null,\"name\":\"H. Su\"},{\"authorId\":null,\"name\":\"J. Krause\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Imagenet large scale visual recognition challenge. IJCV\",\"year\":2015},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/TPAMI.2012.162\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5cb6700d94c6118ee13f4f4fecac99f111189812\",\"title\":\"BabyTalk: Understanding and Generating Simple Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/5cb6700d94c6118ee13f4f4fecac99f111189812\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Mao\"},{\"authorId\":null,\"name\":\"W. Xu\"},{\"authorId\":null,\"name\":\"Y. Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Deep captioning with multimodal recurrent neural networks (m-rnn). arXiv:1412.6632\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Venugopalan\"},{\"authorId\":null,\"name\":\"L. A. Hendricks\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Captioning images with diverse objects. arXiv:1606.07770, 2016\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Y. Pan\"},{\"authorId\":null,\"name\":\"T. Mei\"},{\"authorId\":null,\"name\":\"T. Yao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Jointly modeling embedding and translation to bridge video and language. In CVPR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H. Yu\"},{\"authorId\":null,\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Video paragraph captioning using hierarchical recurrent neural networks. In CVPR\",\"year\":2016},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Ramanishka\"},{\"authorId\":null,\"name\":\"A. Das\"},{\"authorId\":null,\"name\":\"D. H. Park\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Multimodal video description. In ACM Multimedia Grand Challenge\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Xu\"},{\"authorId\":null,\"name\":\"J. Ba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Show, attend and tell: Neural image caption generation with visual attention. arXiv:1502.03044, 2(3):5\",\"year\":2015}],\"title\":\"Weakly Supervised Dense Video Captioning\",\"topics\":[{\"topic\":\"Video clip\",\"topicId\":\"30493\",\"url\":\"https://www.semanticscholar.org/topic/30493\"},{\"topic\":\"Information\",\"topicId\":\"185548\",\"url\":\"https://www.semanticscholar.org/topic/185548\"},{\"topic\":\"Convolutional neural network\",\"topicId\":\"29860\",\"url\":\"https://www.semanticscholar.org/topic/29860\"},{\"topic\":\"Language model\",\"topicId\":\"26812\",\"url\":\"https://www.semanticscholar.org/topic/26812\"},{\"topic\":\"Submodular set function\",\"topicId\":\"86\",\"url\":\"https://www.semanticscholar.org/topic/86\"},{\"topic\":\"Multi-label classification\",\"topicId\":\"31117\",\"url\":\"https://www.semanticscholar.org/topic/31117\"},{\"topic\":\"Microsoft Research\",\"topicId\":\"73897\",\"url\":\"https://www.semanticscholar.org/topic/73897\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Expectation\\u2013maximization algorithm\",\"topicId\":\"52938\",\"url\":\"https://www.semanticscholar.org/topic/52938\"}],\"url\":\"https://www.semanticscholar.org/paper/6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}\n"