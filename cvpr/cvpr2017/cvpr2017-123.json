"{\"abstract\":\"Recent captioning models are limited in their ability to scale and describe concepts unseen in paired image-text corpora. We propose the Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Our model takes advantage of external sources &#x2013; labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text. We propose minimizing a joint objective which can learn from these diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of image-caption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in MSCOCO image-caption training data, as well as many categories that are observed very rarely. Both automatic evaluations and human judgements show that our model considerably outperforms prior work in being able to describe many more categories of objects.\",\"arxivId\":\"1606.07770\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\",\"url\":\"https://www.semanticscholar.org/author/1811430\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\",\"url\":\"https://www.semanticscholar.org/author/2234342\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\",\"url\":\"https://www.semanticscholar.org/author/34849128\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\",\"url\":\"https://www.semanticscholar.org/author/1797655\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\",\"url\":\"https://www.semanticscholar.org/author/1753210\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\",\"url\":\"https://www.semanticscholar.org/author/2903226\"}],\"citationVelocity\":26,\"citations\":[{\"arxivId\":\"2011.00966\",\"authors\":[{\"authorId\":\"32370882\",\"name\":\"Shweta Mahajan\"},{\"authorId\":\"49863405\",\"name\":\"S. Roth\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7d805e35f17dab7382748130a2ca1bda629cdceb\",\"title\":\"Diverse Image Captioning with Context-Object Split Latent Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7d805e35f17dab7382748130a2ca1bda629cdceb\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1911.03738\",\"authors\":[{\"authorId\":\"32227979\",\"name\":\"Marc Tanti\"},{\"authorId\":\"145464131\",\"name\":\"Albert Gatt\"},{\"authorId\":\"2370774\",\"name\":\"Kenneth P. Camilleri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a55f5de768a455ab01fbe0432962f2f6f1a0a7db\",\"title\":\"On Architectures for Including Visual Information in Neural Language Models for Image Description\",\"url\":\"https://www.semanticscholar.org/paper/a55f5de768a455ab01fbe0432962f2f6f1a0a7db\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"8799308\",\"name\":\"U. Nadeem\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1007/978-3-030-49724-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"title\":\"Vision to Language: Methods, Metrics and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405686623\",\"name\":\"Nils Murrugarra-Llerena\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1109/CVPR.2019.00659\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"title\":\"Cross-Modality Personalization for Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145179162\",\"name\":\"Mingxing Zhang\"},{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/TIP.2018.2855415\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"16a2a1bf612f9f9719a7945485f7e73324d18783\",\"title\":\"More is Better: Precise and Detailed Image Captioning Using Online Positive Recall and Missing Concepts Mining\",\"url\":\"https://www.semanticscholar.org/paper/16a2a1bf612f9f9719a7945485f7e73324d18783\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1810.00681\",\"authors\":[{\"authorId\":\"38123220\",\"name\":\"Wasi Uddin Ahmad\"},{\"authorId\":\"8536136\",\"name\":\"Xueying Bai\"},{\"authorId\":\"3157053\",\"name\":\"Nanyun Peng\"},{\"authorId\":\"2782886\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0c6602439185ad8268ebcd99d1ac4afd66fb4c7b\",\"title\":\"Learning Robust, Transferable Sentence Representations for Text Classification\",\"url\":\"https://www.semanticscholar.org/paper/0c6602439185ad8268ebcd99d1ac4afd66fb4c7b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1903.05942\",\"authors\":[{\"authorId\":\"40622539\",\"name\":\"Dong-Jin Kim\"},{\"authorId\":\"49331178\",\"name\":\"Jinsoo Choi\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"}],\"doi\":\"10.1109/CVPR.2019.00643\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79de42d6ca8d1bf2952c46eb74e6e0561f979257\",\"title\":\"Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning\",\"url\":\"https://www.semanticscholar.org/paper/79de42d6ca8d1bf2952c46eb74e6e0561f979257\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1708.05271\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.559\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"10480a42957a8e08e4c543185e135d7c254583a5\",\"title\":\"Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects\",\"url\":\"https://www.semanticscholar.org/paper/10480a42957a8e08e4c543185e135d7c254583a5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1612.00576\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.18653/v1/D17-1098\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c\",\"title\":\"Guided Open Vocabulary Image Captioning with Constrained Beam Search\",\"url\":\"https://www.semanticscholar.org/paper/086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":\"1806.06004\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ceabd7ff28ce2d501511da998252aeb938adc98b\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ceabd7ff28ce2d501511da998252aeb938adc98b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4942547\",\"name\":\"Zongjian Zhang\"},{\"authorId\":\"144066903\",\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"145093625\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/TMM.2018.2888822\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2bc7959cf6bf0ae64f1a83f5fce1ac8cb2256ed3\",\"title\":\"High-Quality Image Captioning With Fine-Grained and Semantic-Guided Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/2bc7959cf6bf0ae64f1a83f5fce1ac8cb2256ed3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"title\":\"CLARA: Dynamic Doctor Representation Learning for Clinical Trial Recruitment\",\"url\":\"https://www.semanticscholar.org/paper/e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1810.10974\",\"authors\":[{\"authorId\":\"37759796\",\"name\":\"S. Palazzo\"},{\"authorId\":\"2441118\",\"name\":\"C. Spampinato\"},{\"authorId\":\"2912641\",\"name\":\"I. Kavasidis\"},{\"authorId\":\"144027622\",\"name\":\"D. Giordano\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":\"10.1109/tpami.2020.2995909\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0232f39cf09a47982c24e311a7424f466f964b22\",\"title\":\"Decoding Brain Representations by Multimodal Learning of Neural Activity and Visual Features\",\"url\":\"https://www.semanticscholar.org/paper/0232f39cf09a47982c24e311a7424f466f964b22\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/TPAMI.2018.2824816\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb28671c43e6f3bfeee3b5bec8023887d8c07bc7\",\"title\":\"Towards Personalized Image Captioning via Multimodal Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/eb28671c43e6f3bfeee3b5bec8023887d8c07bc7\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1911.00147\",\"authors\":[{\"authorId\":\"153650159\",\"name\":\"C. Thomas\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8922345e2aaf4e4a88cf2c2e91b8545ef54c6d2\",\"title\":\"Predicting the Politics of an Image Using Webly Supervised Data\",\"url\":\"https://www.semanticscholar.org/paper/c8922345e2aaf4e4a88cf2c2e91b8545ef54c6d2\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1910.14208\",\"authors\":[{\"authorId\":\"9095876\",\"name\":\"Jipeng Wu\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"61b1bb801d03a9b0da5721c6beaabf1dc6cd90c0\",\"title\":\"Hidden State Guidance: Improving Image Captioning Using an Image Conditioned Autoencoder\",\"url\":\"https://www.semanticscholar.org/paper/61b1bb801d03a9b0da5721c6beaabf1dc6cd90c0\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"L. Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38123220\",\"name\":\"Wasi Uddin Ahmad\"},{\"authorId\":\"8536136\",\"name\":\"Xueying Bai\"},{\"authorId\":\"41020450\",\"name\":\"Zhechao Huang\"},{\"authorId\":null,\"name\":\"Chao Jiang\"},{\"authorId\":\"3157053\",\"name\":\"Nanyun Peng\"},{\"authorId\":\"2782886\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b644cdfceb4dae53f82b4eaedc39f912895fbe8\",\"title\":\"Multi-task Learning for Universal Sentence Representations: What Syntactic and Semantic Information is Captured?\",\"url\":\"https://www.semanticscholar.org/paper/1b644cdfceb4dae53f82b4eaedc39f912895fbe8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2302246\",\"name\":\"Shuaijing Xu\"},{\"authorId\":\"2421895\",\"name\":\"Guangzhi Zhang\"},{\"authorId\":\"145149132\",\"name\":\"R. Bie\"},{\"authorId\":\"47580688\",\"name\":\"Anton Kos\"}],\"doi\":\"10.1007/978-3-030-23597-0_33\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"47c006c56ea260d3f2d2f2dac39ecbe09f5f7cf8\",\"title\":\"CXNet-m2: A Deep Model with Visual and Clinical Contexts for Image-Based Detection of Multiple Lesions\",\"url\":\"https://www.semanticscholar.org/paper/47c006c56ea260d3f2d2f2dac39ecbe09f5f7cf8\",\"venue\":\"WASA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1804.07911\",\"authors\":[{\"authorId\":\"38123220\",\"name\":\"Wasi Uddin Ahmad\"},{\"authorId\":\"8536136\",\"name\":\"Xueying Bai\"},{\"authorId\":\"41020450\",\"name\":\"Zhechao Huang\"},{\"authorId\":\"145755239\",\"name\":\"Chao Jiang\"},{\"authorId\":\"3157053\",\"name\":\"Nanyun Peng\"},{\"authorId\":\"2782886\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f4efccf51d54c1b60823a58bc27eb2092169e675\",\"title\":\"Multi-task Learning for Universal Sentence Embeddings: A Thorough Evaluation using Transfer and Auxiliary Tasks\",\"url\":\"https://www.semanticscholar.org/paper/f4efccf51d54c1b60823a58bc27eb2092169e675\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1807.08205\",\"authors\":[{\"authorId\":\"2365530\",\"name\":\"Mingda Zhang\"},{\"authorId\":\"1726601\",\"name\":\"Rebecca Hwa\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a425e30375e3dd3cb9afed5dc31d2bda8e82384f\",\"title\":\"Equal But Not The Same: Understanding the Implicit Relationship Between Persuasive Images and Text\",\"url\":\"https://www.semanticscholar.org/paper/a425e30375e3dd3cb9afed5dc31d2bda8e82384f\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":\"1910.10994\",\"authors\":[{\"authorId\":\"47958013\",\"name\":\"Xingxing Zhang\"},{\"authorId\":\"3631339\",\"name\":\"Shupeng Gui\"},{\"authorId\":\"1749780\",\"name\":\"Zhenfeng Zhu\"},{\"authorId\":\"152621482\",\"name\":\"Y. Zhao\"},{\"authorId\":\"102551205\",\"name\":\"J. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bd35aa66bced3d4a6d952b30be76de033a170a9\",\"title\":\"ATZSL: Defensive Zero-Shot Recognition in the Presence of Adversaries\",\"url\":\"https://www.semanticscholar.org/paper/3bd35aa66bced3d4a6d952b30be76de033a170a9\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1904.07698\",\"authors\":[{\"authorId\":\"2413405\",\"name\":\"Fahad Sohrab\"},{\"authorId\":\"2495016\",\"name\":\"Jenni Raitoharju\"},{\"authorId\":\"3074923\",\"name\":\"Alexandros Iosifidis\"},{\"authorId\":\"9219875\",\"name\":\"M. Gabbouj\"}],\"doi\":\"10.1016/j.patcog.2020.107648\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c64fd1e19b1d20f35ee110a6259f5d5dd964dfae\",\"title\":\"Multimodal Subspace Support Vector Data Description\",\"url\":\"https://www.semanticscholar.org/paper/c64fd1e19b1d20f35ee110a6259f5d5dd964dfae\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145471480\",\"name\":\"Yunmeng Feng\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"46446912\",\"name\":\"X. Zhang\"},{\"authorId\":\"7521170\",\"name\":\"Chuanfu Xu\"},{\"authorId\":\"2243533\",\"name\":\"Zhenghua Wang\"},{\"authorId\":\"145254061\",\"name\":\"Z. Luo\"}],\"doi\":\"10.1145/3302425.3302464\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"11a8cc3fa18ab4f7158447cc1fc8800489e82f9c\",\"title\":\"AttResNet: Attention-based ResNet for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/11a8cc3fa18ab4f7158447cc1fc8800489e82f9c\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1706.00999\",\"authors\":[{\"authorId\":\"8599825\",\"name\":\"Jonatas Wehrmann\"},{\"authorId\":\"19204942\",\"name\":\"Anderson Mattjie\"},{\"authorId\":\"143914916\",\"name\":\"Rodrigo C. Barros\"}],\"doi\":\"10.1016/j.patrec.2017.11.020\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3053db1016d039757d072f1ae85735d8a21478f\",\"title\":\"Order embeddings and character-level convolutions for multimodal alignment\",\"url\":\"https://www.semanticscholar.org/paper/a3053db1016d039757d072f1ae85735d8a21478f\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"65827589\",\"name\":\"Nivetha Vijayaraju\"}],\"doi\":\"10.31979/etd.vm9n-39ed\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ede7e0721133697781fa3bb4b09e5a99373ab803\",\"title\":\"Image Retrieval Using Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ede7e0721133697781fa3bb4b09e5a99373ab803\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"2168767\",\"name\":\"Umanga Bista\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"1748257\",\"name\":\"Achim Rettinger\"}],\"doi\":\"10.1007/978-3-319-93417-4_27\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7f8e9524051c60fa11f498ae05710dd8239474ce\",\"title\":\"Knowledge Guided Attention and Inference for Describing Images Containing Unseen Objects\",\"url\":\"https://www.semanticscholar.org/paper/7f8e9524051c60fa11f498ae05710dd8239474ce\",\"venue\":\"ESWC\",\"year\":2018},{\"arxivId\":\"1804.03803\",\"authors\":[{\"authorId\":\"1887625\",\"name\":\"Yuehua Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"39978626\",\"name\":\"Lu Jiang\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1145/3240508.3240640\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d64f52b94977b71976327eeb3db702b246ee39ce\",\"title\":\"Decoupled Novel Object Captioner\",\"url\":\"https://www.semanticscholar.org/paper/d64f52b94977b71976327eeb3db702b246ee39ce\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"2010.03855\",\"authors\":[{\"authorId\":\"40622539\",\"name\":\"Dong-Jin Kim\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"49331178\",\"name\":\"Jinsoo Choi\"},{\"authorId\":\"145017149\",\"name\":\"In So Kweon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0bda89ebb8cbec54afb013349c97a8264b36fee0\",\"title\":\"Dense Relational Image Captioning via Multi-task Triple-Stream Networks\",\"url\":\"https://www.semanticscholar.org/paper/0bda89ebb8cbec54afb013349c97a8264b36fee0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1704.01502\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"47008023\",\"name\":\"Z. Su\"},{\"authorId\":\"3700393\",\"name\":\"Minjun Li\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/CVPR.2017.548\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"title\":\"Weakly Supervised Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/TIP.2020.3042086\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"title\":\"Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":\"2009.13682\",\"authors\":[{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"51188307\",\"name\":\"Kevin Lin\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f147279c9d1edddda57f1f21f23b3b58998bad74\",\"title\":\"VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training\",\"url\":\"https://www.semanticscholar.org/paper/f147279c9d1edddda57f1f21f23b3b58998bad74\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.07662\",\"authors\":[{\"authorId\":\"47833739\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2112160\",\"name\":\"Y. Li\"},{\"authorId\":\"1678689\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00859\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"23e943809c131c50dc90c1d308373febc60b9029\",\"title\":\"Intention Oriented Image Captions With Guiding Objects\",\"url\":\"https://www.semanticscholar.org/paper/23e943809c131c50dc90c1d308373febc60b9029\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1801.10300\",\"authors\":[{\"authorId\":\"48149965\",\"name\":\"W. Lin\"},{\"authorId\":\"143672077\",\"name\":\"K. Chen\"},{\"authorId\":\"29837150\",\"name\":\"HungYueh Chiang\"},{\"authorId\":\"31871157\",\"name\":\"Winston Hsu\"}],\"doi\":\"10.1145/3184558.3186354\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e1b3cf334aead8d2c29747f6ee7d1291dd83708\",\"title\":\"Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures\",\"url\":\"https://www.semanticscholar.org/paper/9e1b3cf334aead8d2c29747f6ee7d1291dd83708\",\"venue\":\"WWW\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Jiang\"},{\"authorId\":null,\"name\":\"Qi Zhao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3977c0056755c2911811509ac38e0cef532004b0\",\"title\":\"Self-Distillation for Few-Shot Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3977c0056755c2911811509ac38e0cef532004b0\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"117548227\",\"name\":\"Saloni Kalra\"},{\"authorId\":\"88872765\",\"name\":\"Alka Leekha\"}],\"doi\":\"10.1080/02522667.2020.1715602\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"117333afd2ce2d80dd195dc5c5087f1b2b6bebdc\",\"title\":\"Survey of convolutional neural networks for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/117333afd2ce2d80dd195dc5c5087f1b2b6bebdc\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34589688\",\"name\":\"Zhenghang Yuan\"},{\"authorId\":\"121856647\",\"name\":\"X. Li\"},{\"authorId\":\"39669401\",\"name\":\"Q. Wang\"}],\"doi\":\"10.1109/ACCESS.2019.2962195\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"52a221f10ec19fb3b20fda0271184b641c2ccc4b\",\"title\":\"Exploring Multi-Level Attention and Semantic Relationship for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/52a221f10ec19fb3b20fda0271184b641c2ccc4b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1803.09845\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"94908120\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2018.00754\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"title\":\"Neural Baby Talk\",\"url\":\"https://www.semanticscholar.org/paper/3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1906.00283\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"122175026\",\"name\":\"P\\u00e9ter Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"title\":\"Learning to Generate Grounded Image Captions without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.02726\",\"authors\":[{\"authorId\":\"94845899\",\"name\":\"Qianyu Feng\"},{\"authorId\":\"98264517\",\"name\":\"Y. Wu\"},{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"3863922\",\"name\":\"C. Yan\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.1109/TCSVT.2020.2965966\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"daa87e86b469d975a9ad84bfb5eee230efa8af3e\",\"title\":\"Cascaded Revision Network for Novel Object Captioning\",\"url\":\"https://www.semanticscholar.org/paper/daa87e86b469d975a9ad84bfb5eee230efa8af3e\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1488653711\",\"name\":\"Yanzhi Yi\"},{\"authorId\":\"1486063321\",\"name\":\"Hangyu Deng\"},{\"authorId\":\"153147804\",\"name\":\"Jinglu Hu\"}],\"doi\":\"10.18653/v1/2020.acl-main.93\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0b4d5b7cef06b66182db80803f783d077e3637b6\",\"title\":\"Improving Image Captioning Evaluation by Considering Inter References Variance\",\"url\":\"https://www.semanticscholar.org/paper/0b4d5b7cef06b66182db80803f783d077e3637b6\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1910.09728\",\"authors\":[{\"authorId\":\"24799325\",\"name\":\"Zhizhe Liu\"},{\"authorId\":\"47958013\",\"name\":\"Xingxing Zhang\"},{\"authorId\":\"1749780\",\"name\":\"Zhenfeng Zhu\"},{\"authorId\":\"1410058013\",\"name\":\"Shuai Zheng\"},{\"authorId\":\"152621482\",\"name\":\"Y. Zhao\"},{\"authorId\":\"153513490\",\"name\":\"Jian Cheng\"}],\"doi\":\"10.1016/j.imavis.2020.103924\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17fed5b7cb4a3efda04a40c47c3dda43762e43e8\",\"title\":\"Convolutional Prototype Learning for Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/17fed5b7cb4a3efda04a40c47c3dda43762e43e8\",\"venue\":\"Image Vis. Comput.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34692779\",\"name\":\"K. Chang\"},{\"authorId\":\"10421443\",\"name\":\"Kung-Hung Lu\"},{\"authorId\":\"1720473\",\"name\":\"Chu-Song Chen\"}],\"doi\":\"10.1109/ICCV.2017.380\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c86716de32aaa0c9bce6f711b67c76c9fcecbb85\",\"title\":\"Aesthetic Critiques Generation for Photos\",\"url\":\"https://www.semanticscholar.org/paper/c86716de32aaa0c9bce6f711b67c76c9fcecbb85\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48520182\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3313873\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a756ae8457454120ee2e067d4936d801f56ed62\",\"title\":\"Image Captioning by Asking Questions\",\"url\":\"https://www.semanticscholar.org/paper/1a756ae8457454120ee2e067d4936d801f56ed62\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2001.01037\",\"authors\":[{\"authorId\":\"46969089\",\"name\":\"J. Sun\"},{\"authorId\":\"3633358\",\"name\":\"S. Lapuschkin\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"},{\"authorId\":\"49345823\",\"name\":\"Alexander Binder\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"82e836be97e706dca7029ce6a0553b4890726593\",\"title\":\"Understanding Image Captioning Models beyond Visualizing Attention\",\"url\":\"https://www.semanticscholar.org/paper/82e836be97e706dca7029ce6a0553b4890726593\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1710.06303\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"2168767\",\"name\":\"Umanga Bista\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"1748257\",\"name\":\"Achim Rettinger\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"631a1571d1a073369ec7c98e196de07e263ae130\",\"title\":\"Describing Natural Images Containing Novel Objects with Knowledge Guided Assitance\",\"url\":\"https://www.semanticscholar.org/paper/631a1571d1a073369ec7c98e196de07e263ae130\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1805.06956\",\"authors\":[{\"authorId\":\"46246362\",\"name\":\"Ahmad Babaeian Jelodar\"},{\"authorId\":\"10783605\",\"name\":\"Md Sirajus Salekin\"},{\"authorId\":\"71732787\",\"name\":\"Yu Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7deb2c6b9814296be9b3a048ebbe387edcaf73a9\",\"title\":\"Identifying Object States in Cooking-Related Images\",\"url\":\"https://www.semanticscholar.org/paper/7deb2c6b9814296be9b3a048ebbe387edcaf73a9\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144368930\",\"name\":\"Yan Huang\"},{\"authorId\":\"144143336\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00587\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54f5c5e1f80000e1da9c6b182e0e76f6e61c0ad5\",\"title\":\"ACMM: Aligned Cross-Modal Memory for Few-Shot Image and Sentence Matching\",\"url\":\"https://www.semanticscholar.org/paper/54f5c5e1f80000e1da9c6b182e0e76f6e61c0ad5\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2011.07743\",\"authors\":[{\"authorId\":\"2022231256\",\"name\":\"Yu Gu\"},{\"authorId\":\"145030211\",\"name\":\"Sue E. Kase\"},{\"authorId\":\"36349209\",\"name\":\"Michelle Vanni\"},{\"authorId\":\"1709722\",\"name\":\"Brian M. Sadler\"},{\"authorId\":\"40085065\",\"name\":\"Percy Liang\"},{\"authorId\":\"46580832\",\"name\":\"Xifeng Yan\"},{\"authorId\":\"145244888\",\"name\":\"Y. Su\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3749f27bbb06667b1492f2b2caf1be37808f2ba0\",\"title\":\"Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases\",\"url\":\"https://www.semanticscholar.org/paper/3749f27bbb06667b1492f2b2caf1be37808f2ba0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.08658\",\"authors\":[{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"46395829\",\"name\":\"Yufei Wang\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"145461380\",\"name\":\"Rishabh Jain\"},{\"authorId\":\"1607624548\",\"name\":\"Mark Johnson\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606363958\",\"name\":\"Devi Parikh\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"}],\"doi\":\"10.1109/ICCV.2019.00904\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"title\":\"nocaps: novel object captioning at scale\",\"url\":\"https://www.semanticscholar.org/paper/8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797144\",\"name\":\"G. Friedland\"},{\"authorId\":\"1772549\",\"name\":\"D. Borth\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df9a08016fa553a169d893ce2d3fca375bab4781\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/df9a08016fa553a169d893ce2d3fca375bab4781\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1804.07889\",\"authors\":[{\"authorId\":\"144690460\",\"name\":\"Di Lu\"},{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"34170717\",\"name\":\"Lifu Huang\"},{\"authorId\":\"144016781\",\"name\":\"Heng Ji\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.18653/v1/D18-1435\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1e8feffa2280e41ceb864b940869c5408db89285\",\"title\":\"Entity-aware Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/1e8feffa2280e41ceb864b940869c5408db89285\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1908.00047\",\"authors\":[{\"authorId\":\"9424554\",\"name\":\"Berkan Demirel\"},{\"authorId\":\"1939006\",\"name\":\"Ramazan Gokberk Cinbis\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3d617b8cbf10b3fe776c16fa498e6420def3e000\",\"title\":\"Image Captioning with Unseen Objects\",\"url\":\"https://www.semanticscholar.org/paper/3d617b8cbf10b3fe776c16fa498e6420def3e000\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"1680068\",\"name\":\"X. Wang\"}],\"doi\":\"10.1145/3126686.3126714\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b5bfe824fc49fe78b538ac15f21c4cd6a9d44347\",\"title\":\"Image Caption with Synchronous Cross-Attention\",\"url\":\"https://www.semanticscholar.org/paper/b5bfe824fc49fe78b538ac15f21c4cd6a9d44347\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2003.11743\",\"authors\":[{\"authorId\":\"1491233100\",\"name\":\"Pranav Agarwal\"},{\"authorId\":\"145086911\",\"name\":\"A. Betancourt\"},{\"authorId\":\"1594025086\",\"name\":\"V. Panagiotou\"},{\"authorId\":\"2251072\",\"name\":\"N. Rodr\\u00edguez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b430a5384c82beb6102106fbea0a134425a08c23\",\"title\":\"Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models\",\"url\":\"https://www.semanticscholar.org/paper/b430a5384c82beb6102106fbea0a134425a08c23\",\"venue\":\"ICLR 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35212676\",\"name\":\"J. Peyre\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"caffa07ead18aae78bf654bc57023eef58e74faf\",\"title\":\"Learning to detect visual relations\",\"url\":\"https://www.semanticscholar.org/paper/caffa07ead18aae78bf654bc57023eef58e74faf\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1612.00370\",\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"665a311c538fc021c27acd3953f171924cc5905c\",\"title\":\"Optimization of image description metrics using policy gradient methods\",\"url\":\"https://www.semanticscholar.org/paper/665a311c538fc021c27acd3953f171924cc5905c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1810.02358\",\"authors\":[{\"authorId\":\"2018393\",\"name\":\"Hyeonwoo Noh\"},{\"authorId\":\"1837923\",\"name\":\"Taehoon Kim\"},{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00858\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b80f128830114896df94999b4104cb75408e657e\",\"title\":\"Transfer Learning via Unsupervised Task Discovery for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b80f128830114896df94999b4104cb75408e657e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1908.09317\",\"authors\":[{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"145587209\",\"name\":\"N. Navab\"}],\"doi\":\"10.1109/ICCV.2019.00751\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"title\":\"Towards Unsupervised Image Captioning With Shared Multimodal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8599825\",\"name\":\"Jonatas Wehrmann\"},{\"authorId\":\"145987795\",\"name\":\"M. A. Lopes\"},{\"authorId\":\"145877010\",\"name\":\"Douglas M. Souza\"},{\"authorId\":\"1380051745\",\"name\":\"Rodrigo C. Barros\"}],\"doi\":\"10.1109/ICCV.2019.00590\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84010f883e9a7283666a6628226016ca4f8d28f1\",\"title\":\"Language-Agnostic Visual-Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/84010f883e9a7283666a6628226016ca4f8d28f1\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8599825\",\"name\":\"Jonatas Wehrmann\"},{\"authorId\":\"143914916\",\"name\":\"Rodrigo C. Barros\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1371af87f6d5e22ef6d8c5f9977f5e924f176f6\",\"title\":\"Bidirectional Retrieval Made Simple J\\u00f4natas Wehrmann\",\"url\":\"https://www.semanticscholar.org/paper/e1371af87f6d5e22ef6d8c5f9977f5e924f176f6\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1810.04020\",\"authors\":[{\"authorId\":\"47412302\",\"name\":\"M. Z. Hossain\"},{\"authorId\":\"2470423\",\"name\":\"Ferdous Sohel\"},{\"authorId\":\"2222847\",\"name\":\"M. F. Shiratuddin\"},{\"authorId\":\"47028380\",\"name\":\"Hamid Laga\"}],\"doi\":\"10.1145/3295748\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"7e27d44e3fac723ccb703e0a83b22711bd42efe8\",\"title\":\"A Comprehensive Survey of Deep Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7e27d44e3fac723ccb703e0a83b22711bd42efe8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145840350\",\"name\":\"P. Anderson\"}],\"doi\":\"10.25911/5D00D4EC451CC\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0\",\"title\":\"Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents\",\"url\":\"https://www.semanticscholar.org/paper/c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004502909\",\"name\":\"K. JeevithaV\"},{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"}],\"doi\":\"10.1109/incet49848.2020.9154103\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1504b2eafe0d02f7777804b8d6c9631cfbe2e30\",\"title\":\"Natural Language Description for Videos Using NetVLAD and Attentional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/a1504b2eafe0d02f7777804b8d6c9631cfbe2e30\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1909.02097\",\"authors\":[{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"144865339\",\"name\":\"Bo Pang\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/D19-1155\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b6e6822eabe2f64192a1965c23e38043866319c\",\"title\":\"Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3b6e6822eabe2f64192a1965c23e38043866319c\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2003.03305\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"22d733f5d5a995469dc916102f1806253645ae60\",\"title\":\"Captioning Images with Novel Objects via Online Vocabulary Expansion\",\"url\":\"https://www.semanticscholar.org/paper/22d733f5d5a995469dc916102f1806253645ae60\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177145\",\"name\":\"M. Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dadb7ddfde3478238d23a8bacf5eddecc59e84c9\",\"title\":\"Vocabulary Image Captioning with Constrained Beam Search\",\"url\":\"https://www.semanticscholar.org/paper/dadb7ddfde3478238d23a8bacf5eddecc59e84c9\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1904.11251\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2019.01278\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"title\":\"Pointing Novel Objects in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144081629\",\"name\":\"Mario G\\u00f3mez Mart\\u00ednez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"faa8314259e9de1af8e841c265b0251531b32e04\",\"title\":\"Deep learning for image captioning: an encoder-decoder architecture with soft attention\",\"url\":\"https://www.semanticscholar.org/paper/faa8314259e9de1af8e841c265b0251531b32e04\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1419474253\",\"name\":\"Md Asifuzzaman Jishan\"},{\"authorId\":\"102857185\",\"name\":\"K. R. Mahmud\"},{\"authorId\":\"48058214\",\"name\":\"A. K. Azad\"}],\"doi\":\"10.11591/ijece.v9i4.pp2932-2940\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1f51c42bc6d52e7d46c348c67f1f7b2cc1534f63\",\"title\":\"Natural language description of images using hybrid recurrent neural network\",\"url\":\"https://www.semanticscholar.org/paper/1f51c42bc6d52e7d46c348c67f1f7b2cc1534f63\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1709.05038\",\"authors\":[{\"authorId\":\"49693168\",\"name\":\"Yang Xian\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/TIP.2019.2917229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54f5c9df478dc510ea8f9c3b52917fb8356904e4\",\"title\":\"Self-Guiding Multimodal LSTM\\u2014When We Do Not Have a Perfect Training Dataset for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/54f5c9df478dc510ea8f9c3b52917fb8356904e4\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143905586\",\"name\":\"Cuirong Long\"},{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1007/s11042-019-7441-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f147a64982b25ea28a2b0737c68b0f65fdb46bd8\",\"title\":\"Cross-domain personalized image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f147a64982b25ea28a2b0737c68b0f65fdb46bd8\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1711.06666\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-01267-0_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1be2c16060974f66e5366872ebbee21325075e8\",\"title\":\"ADVISE: Symbolism and External Knowledge for Decoding Advertisements\",\"url\":\"https://www.semanticscholar.org/paper/e1be2c16060974f66e5366872ebbee21325075e8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8599825\",\"name\":\"Jonatas Wehrmann\"},{\"authorId\":\"143914916\",\"name\":\"Rodrigo C. Barros\"}],\"doi\":\"10.1109/CVPR.2018.00805\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"083551c35be43aa3f0cc45408c09ec46b7a239ea\",\"title\":\"Bidirectional Retrieval Made Simple\",\"url\":\"https://www.semanticscholar.org/paper/083551c35be43aa3f0cc45408c09ec46b7a239ea\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"title\":\"Incorporating Semantic Attention in Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"venue\":\"LREC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"163a474747fd63ab62ae586711fa5e5a2ac91bd8\",\"title\":\"Improved Image Captioning via Policy Gradient optimization of SPIDEr\",\"url\":\"https://www.semanticscholar.org/paper/163a474747fd63ab62ae586711fa5e5a2ac91bd8\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40451577\",\"name\":\"W. Wang\"},{\"authorId\":\"3113725\",\"name\":\"V. Zheng\"},{\"authorId\":\"9380191\",\"name\":\"H. Yu\"},{\"authorId\":\"1679209\",\"name\":\"C. Miao\"}],\"doi\":\"10.1145/3293318\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dafa29f1f0534448d205365796d68873a0068c6b\",\"title\":\"A Survey of Zero-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/dafa29f1f0534448d205365796d68873a0068c6b\",\"venue\":\"ACM Trans. Intell. Syst. Technol.\",\"year\":2019},{\"arxivId\":\"2002.11701\",\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":\"10.1145/3366423.3380137\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a842fe8c25348627764462a57f0cd43d8cef103b\",\"title\":\"CLARA: Clinical Report Auto-completion\",\"url\":\"https://www.semanticscholar.org/paper/a842fe8c25348627764462a57f0cd43d8cef103b\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":\"1707.09472\",\"authors\":[{\"authorId\":\"35212676\",\"name\":\"J. Peyre\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2017.554\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5ab64c8da40e5279c243cf18f06498cb2bfe0f7e\",\"title\":\"Weakly-Supervised Learning of Visual Relations\",\"url\":\"https://www.semanticscholar.org/paper/5ab64c8da40e5279c243cf18f06498cb2bfe0f7e\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4942547\",\"name\":\"Zongjian Zhang\"},{\"authorId\":\"145698633\",\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"145093625\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/DICTA.2018.8615788\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ce9fd151c7e67581dfa85b662f5034aca1896e6\",\"title\":\"Size-Invariant Attention Accuracy Metric for Image Captioning with High-Resolution Residual Attention\",\"url\":\"https://www.semanticscholar.org/paper/2ce9fd151c7e67581dfa85b662f5034aca1896e6\",\"venue\":\"2018 Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49336560\",\"name\":\"Weining Wang\"},{\"authorId\":\"144368926\",\"name\":\"Yan Huang\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/CVPR.2019.00042\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0fa68cde4db12779adacb70a24961cf09b1adf73\",\"title\":\"Language-Driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model\",\"url\":\"https://www.semanticscholar.org/paper/0fa68cde4db12779adacb70a24961cf09b1adf73\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1705.00930\",\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2017.64\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"title\":\"Show, Adapt and Tell: Adversarial Training of Cross-Domain Image Captioner\",\"url\":\"https://www.semanticscholar.org/paper/828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"2002.11894\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"8088602\",\"name\":\"Ehsan Abbasnejad\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc3ab3767e40c3f5dd33263386f6177afdda4d23\",\"title\":\"Unshuffling Data for Improved Generalization\",\"url\":\"https://www.semanticscholar.org/paper/fc3ab3767e40c3f5dd33263386f6177afdda4d23\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.02201\",\"authors\":[{\"authorId\":\"40622539\",\"name\":\"Dong-Jin Kim\"},{\"authorId\":\"49331178\",\"name\":\"Jinsoo Choi\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"98758720\",\"name\":\"I. Kweon\"}],\"doi\":\"10.18653/v1/D19-1208\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a87d5654c41f3dfa7252d079045d7094f601f78e\",\"title\":\"Image Captioning with Very Scarce Supervised Data: Adversarial Semi-Supervised Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/a87d5654c41f3dfa7252d079045d7094f601f78e\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48195668\",\"name\":\"Jin Yuan\"},{\"authorId\":\"47059067\",\"name\":\"L. Zhang\"},{\"authorId\":\"2836997\",\"name\":\"Songrui Guo\"},{\"authorId\":\"1406190317\",\"name\":\"Y. Xiao\"},{\"authorId\":\"152985786\",\"name\":\"Z. Li\"}],\"doi\":\"10.1145/3394955\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"title\":\"Image Captioning with a Joint Attention Mechanism by Visual Concept Samples\",\"url\":\"https://www.semanticscholar.org/paper/2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"2003.12633\",\"authors\":[{\"authorId\":\"24339915\",\"name\":\"Davis Gilton\"},{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"145952380\",\"name\":\"R. Willett\"},{\"authorId\":\"46208708\",\"name\":\"G. Shakhnarovich\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"title\":\"Detection and Description of Change in Visual Streams\",\"url\":\"https://www.semanticscholar.org/paper/dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1805.07030\",\"authors\":[{\"authorId\":\"3175685\",\"name\":\"A. Mathews\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"33913193\",\"name\":\"Xuming He\"}],\"doi\":\"10.1109/CVPR.2018.00896\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"beeebd2af0d8f130dcf234231de4569d584cb7fd\",\"title\":\"SemStyle: Learning to Generate Stylised Image Captions Using Unaligned Text\",\"url\":\"https://www.semanticscholar.org/paper/beeebd2af0d8f130dcf234231de4569d584cb7fd\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145414749\",\"name\":\"Kun Fu\"},{\"authorId\":\"3068555\",\"name\":\"Jin Li\"},{\"authorId\":\"2512850\",\"name\":\"Junqi Jin\"},{\"authorId\":\"1700883\",\"name\":\"C. Zhang\"}],\"doi\":\"10.1109/TNNLS.2018.2813306\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f5fd76a48726d0cea93aa2201d3ba3d2c5903007\",\"title\":\"Image-Text Surgery: Efficient Concept Learning in Image Captioning by Generating Pseudopairs\",\"url\":\"https://www.semanticscholar.org/paper/f5fd76a48726d0cea93aa2201d3ba3d2c5903007\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3158899\",\"name\":\"Mariam Bouchakwa\"},{\"authorId\":\"2135034\",\"name\":\"Yassine Ayadi\"},{\"authorId\":\"1784204\",\"name\":\"I. Amous\"}],\"doi\":\"10.1007/s11042-020-08862-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"445df9ebc8c43c39faeaa58518383f980fdd0da9\",\"title\":\"A review on visual content-based and users\\u2019 tags-based image annotation: methods and techniques\",\"url\":\"https://www.semanticscholar.org/paper/445df9ebc8c43c39faeaa58518383f980fdd0da9\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020}],\"corpusId\":8457705,\"doi\":\"10.1109/CVPR.2017.130\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":14,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"b9aa3bafa9e8e21bb92908ae23b468fa248239b3\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Firat\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Devise : A deep visualsemantic embedding model\",\"url\":\"\",\"venue\":\"Advances in Neural Information Processing Systems\",\"year\":2013},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1503.03535\",\"authors\":[{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2345617\",\"name\":\"Orhan Firat\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2934336\",\"name\":\"Lo\\u00efc Barrault\"},{\"authorId\":\"49955352\",\"name\":\"Huei-Chi Lin\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"5fcd41ca42659ff792fc8ee7d535156e8e69f987\",\"title\":\"On Using Monolingual Corpora in Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/5fcd41ca42659ff792fc8ee7d535156e8e69f987\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1312.5650\",\"authors\":[{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1740765\",\"name\":\"Y. Singer\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"2279670\",\"name\":\"Andrea Frome\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"be2f5d8a7e6b415f1e22cee7dfd9be56b1afd8be\",\"title\":\"Zero-Shot Learning by Convex Combination of Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/be2f5d8a7e6b415f1e22cee7dfd9be56b1afd8be\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":\"1511.05284\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPR.2016.8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"title\":\"Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data\",\"url\":\"https://www.semanticscholar.org/paper/e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"1760868\",\"name\":\"M. Surdeanu\"},{\"authorId\":\"144661918\",\"name\":\"John Bauer\"},{\"authorId\":\"2784228\",\"name\":\"Jenny Rose Finkel\"},{\"authorId\":\"2105138\",\"name\":\"Steven Bethard\"},{\"authorId\":\"2240597\",\"name\":\"D. McClosky\"}],\"doi\":\"10.3115/v1/P14-5010\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2f5102ec3f70d0dea98c957cc2cab4d15d83a2da\",\"title\":\"The Stanford CoreNLP Natural Language Processing Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/2f5102ec3f70d0dea98c957cc2cab4d15d83a2da\",\"venue\":\"ACL\",\"year\":2014},{\"arxivId\":\"1504.06692\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"144287022\",\"name\":\"Xu Wei\"},{\"authorId\":\"143907244\",\"name\":\"Yi Yang\"},{\"authorId\":\"152924551\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/ICCV.2015.291\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eb847564774394c484e701437dbcffbf040ff3cc\",\"title\":\"Learning Like a Child: Fast Novel Visual Concept Learning from Sentence Descriptions of Images\",\"url\":\"https://www.semanticscholar.org/paper/eb847564774394c484e701437dbcffbf040ff3cc\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2279670\",\"name\":\"Andrea Frome\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4aa4069693bee00d1b0759ca3df35e59284e9845\",\"title\":\"DeViSE: A Deep Visual-Semantic Embedding Model\",\"url\":\"https://www.semanticscholar.org/paper/4aa4069693bee00d1b0759ca3df35e59284e9845\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kiros et al.2015 Ryan Kiros\"},{\"authorId\":null,\"name\":\"Ruslan Salakhutdinov\"},{\"authorId\":null,\"name\":\"Richard S. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Unifying visualsemantic embeddings with multimodal neural language models. TACL\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"34176020\",\"name\":\"Jesse Dodge\"},{\"authorId\":\"46479604\",\"name\":\"Amit Goyal\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"},{\"authorId\":\"1714215\",\"name\":\"K. Stratos\"},{\"authorId\":\"1682965\",\"name\":\"Xufeng Han\"},{\"authorId\":\"40614240\",\"name\":\"A. Mensch\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"355de7460120ddc1150d9ce3756f9848983f7ff4\",\"title\":\"Midge: Generating Image Descriptions From Computer Vision Detections\",\"url\":\"https://www.semanticscholar.org/paper/355de7460120ddc1150d9ce3756f9848983f7ff4\",\"venue\":\"EACL\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yezhou Yang, Ching Lik Teo, Hal Daum\\u00e9 III, and Yiannis Aloimonos Corpusguided sentence generation of natural images\",\"url\":\"\",\"venue\":\"EMNLP\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Russakovsky\"},{\"authorId\":null,\"name\":\"J. Deng\"},{\"authorId\":null,\"name\":\"H. Su\"},{\"authorId\":null,\"name\":\"J. Krause\"},{\"authorId\":null,\"name\":\"S. Satheesh\"},{\"authorId\":null,\"name\":\"S. Ma\"},{\"authorId\":null,\"name\":\"Z. Huang\"},{\"authorId\":null,\"name\":\"A. Karpathy\"},{\"authorId\":null,\"name\":\"A. Khosla\"},{\"authorId\":null,\"name\":\"M. Bernstein\"},{\"authorId\":null,\"name\":\"A. C. Berg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and L\",\"url\":\"\",\"venue\":\"Fei-Fei. ILSVRC\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145592791\",\"name\":\"P. Kuznetsova\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1162/tacl_a_00188\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"59927ded86ab4f7253fc32efb351e5a13e746ead\",\"title\":\"TreeTalk: Composition and Compression of Trees for Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/59927ded86ab4f7253fc32efb351e5a13e746ead\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143845796\",\"name\":\"Jeffrey Pennington\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1162\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"title\":\"Glove: Global Vectors for Word Representation\",\"url\":\"https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1756655\",\"name\":\"C. L. Teo\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"76a1dca3a9c2b0229c1b12c95752dcf40dc95a11\",\"title\":\"Corpus-Guided Sentence Generation of Natural Images\",\"url\":\"https://www.semanticscholar.org/paper/76a1dca3a9c2b0229c1b12c95752dcf40dc95a11\",\"venue\":\"EMNLP\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1162/tacl_a_00177\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ca7d208ff8d81377e0eaa9723820aeae7a7322d\",\"title\":\"Grounded Compositional Semantics for Finding and Describing Images with Sentences\",\"url\":\"https://www.semanticscholar.org/paper/0ca7d208ff8d81377e0eaa9723820aeae7a7322d\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ryan Kiros\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep visualsemantic alignments for generating image descriptions Multimodal neural language models\",\"url\":\"\",\"venue\":\"TACL\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2672644\",\"name\":\"A. Lazaridou\"},{\"authorId\":\"2552871\",\"name\":\"Elia Bruni\"},{\"authorId\":\"145283199\",\"name\":\"M. Baroni\"}],\"doi\":\"10.3115/v1/P14-1132\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0680e141ca3f12975197f711af98ee2f5022ba64\",\"title\":\"Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world\",\"url\":\"https://www.semanticscholar.org/paper/0680e141ca3f12975197f711af98ee2f5022ba64\",\"venue\":\"ACL\",\"year\":2014},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Frome et al.2013 Andrea Frome\"},{\"authorId\":null,\"name\":\"Greg S Corrado\"},{\"authorId\":null,\"name\":\"Jon Shlens\"},{\"authorId\":null,\"name\":\"Samy Bengio\"},{\"authorId\":null,\"name\":\"Jeff Dean\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Devise: A deep visualsemantic embedding model\",\"url\":\"\",\"venue\":\"In Advances in Neural Information Processing Systems,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"fad611e35b3731740b4d8b754241e77add5a70b9\",\"title\":\"Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/fad611e35b3731740b4d8b754241e77add5a70b9\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2748591\",\"name\":\"Martin Sundermeyer\"},{\"authorId\":\"144490010\",\"name\":\"R. Schl\\u00fcter\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f9a1b3850dfd837793743565a8af95973d395a4e\",\"title\":\"LSTM Neural Networks for Language Modeling\",\"url\":\"https://www.semanticscholar.org/paper/f9a1b3850dfd837793743565a8af95973d395a4e\",\"venue\":\"INTERSPEECH\",\"year\":2012},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015}],\"title\":\"Captioning Images with Diverse Objects\",\"topics\":[{\"topic\":\"ImageNet\",\"topicId\":\"256302\",\"url\":\"https://www.semanticscholar.org/topic/256302\"},{\"topic\":\"Outline of object recognition\",\"topicId\":\"34569\",\"url\":\"https://www.semanticscholar.org/topic/34569\"},{\"topic\":\"Text corpus\",\"topicId\":\"14829\",\"url\":\"https://www.semanticscholar.org/topic/14829\"},{\"topic\":\"Network on a chip\",\"topicId\":\"17316\",\"url\":\"https://www.semanticscholar.org/topic/17316\"}],\"url\":\"https://www.semanticscholar.org/paper/b9aa3bafa9e8e21bb92908ae23b468fa248239b3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}\n"