"{\"abstract\":\"Visual dialog is a challenging vision-language task, which requires the agent to answer multi-round questions about an image. It typically needs to address two major problems: (1) How to answer visually-grounded questions, which is the core challenge in visual question answering (VQA); (2) How to infer the co-reference between questions and the dialog history. An example of visual co-reference is: pronouns (e.g., \\u201cthey\\u201d) in the question (e.g., \\u201cAre they on or off?\\u201d) are linked with nouns (e.g., \\u201clamps\\u201d) appearing in the dialog history (e.g., \\u201cHow many lamps are there?\\u201d) and the object grounded in the image. In this work, to resolve the visual co-reference for visual dialog, we propose a novel attention mechanism called Recursive Visual Attention (RvA). Specifically, our dialog agent browses the dialog history until the agent has sufficient confidence in the visual co-reference resolution, and refines the visual attention recursively. The quantitative and qualitative experimental results on the large-scale VisDial v0.9 and v1.0 datasets demonstrate that the proposed RvA not only outperforms the state-of-the-art methods, but also achieves reasonable recursion and interpretable attention maps without additional annotations. The code is available at https://github.com/yuleiniu/rva.\",\"arxivId\":\"1812.02664\",\"authors\":[{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\",\"url\":\"https://www.semanticscholar.org/author/2520427\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\",\"url\":\"https://www.semanticscholar.org/author/5462268\"},{\"authorId\":\"48985581\",\"name\":\"Manli Zhang\",\"url\":\"https://www.semanticscholar.org/author/48985581\"},{\"authorId\":\"47538869\",\"name\":\"J. Zhang\",\"url\":\"https://www.semanticscholar.org/author/47538869\"},{\"authorId\":\"1776220\",\"name\":\"Zhiwu Lu\",\"url\":\"https://www.semanticscholar.org/author/1776220\"},{\"authorId\":\"2259699\",\"name\":\"Ji-Rong Wen\",\"url\":\"https://www.semanticscholar.org/author/2259699\"}],\"citationVelocity\":21,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"49876189\",\"name\":\"T. Yang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/ICCV.2019.00265\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ecc5cd01261cf9c396689121a3e8c1844c825775\",\"title\":\"Making History Matter: History-Advantage Sequence Training for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/ecc5cd01261cf9c396689121a3e8c1844c825775\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2004.06698\",\"authors\":[{\"authorId\":\"71119060\",\"name\":\"Gi-Cheon Kang\"},{\"authorId\":\"1755502\",\"name\":\"Junseok Park\"},{\"authorId\":\"2294014\",\"name\":\"Hwaran Lee\"},{\"authorId\":\"152705134\",\"name\":\"B. Zhang\"},{\"authorId\":\"153188145\",\"name\":\"J. Kim\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"36642cb0c60047846ccc9bb670a63f6884e976d1\",\"title\":\"DialGraph: Sparse Graph Learning Networks for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/36642cb0c60047846ccc9bb670a63f6884e976d1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"VGG\"},{\"authorId\":null,\"name\":\"RCNN\"},{\"authorId\":null,\"name\":\"LSTM\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"adf9243e6166df170fa1653dafe2d952ad1bfe9f\",\"title\":\"Dual Visual Attention Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/adf9243e6166df170fa1653dafe2d952ad1bfe9f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2005.07493\",\"authors\":[{\"authorId\":\"144992211\",\"name\":\"Shubham Agarwal\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"1576788264\",\"name\":\"Joon-Young Lee\"},{\"authorId\":\"2621022\",\"name\":\"Ioannis Konstas\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/2020.acl-main.728\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c3baa99376fac050160c25a9c6d92d9baa5846c\",\"title\":\"History for Visual Dialog: Do we really need it?\",\"url\":\"https://www.semanticscholar.org/paper/8c3baa99376fac050160c25a9c6d92d9baa5846c\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2004.14025\",\"authors\":[{\"authorId\":\"79778234\",\"name\":\"Sungjin Park\"},{\"authorId\":\"89016637\",\"name\":\"T. Whang\"},{\"authorId\":\"3037023\",\"name\":\"Y. Yoon\"},{\"authorId\":\"1450703435\",\"name\":\"Hueiseok Lim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6a7c8371f6a0646a1ce04bc0c42f56ff1438f9ab\",\"title\":\"Multi-View Attention Networks for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/6a7c8371f6a0646a1ce04bc0c42f56ff1438f9ab\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1785405226\",\"name\":\"Xiaofan Chen\"},{\"authorId\":\"1716428\",\"name\":\"Songyang Lao\"},{\"authorId\":\"1789237897\",\"name\":\"Ting Duan\"}],\"doi\":\"10.1145/3438872.3439098\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0867cec490d447b02c25abaa00363606f205b9c9\",\"title\":\"Multimodal Fusion of Visual Dialog: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/0867cec490d447b02c25abaa00363606f205b9c9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.08360\",\"authors\":[{\"authorId\":\"49102717\",\"name\":\"Feilong Chen\"},{\"authorId\":\"33427918\",\"name\":\"Fandong Meng\"},{\"authorId\":\"46372563\",\"name\":\"Jiaming Xu\"},{\"authorId\":\"144326610\",\"name\":\"Peng Li\"},{\"authorId\":\"153260119\",\"name\":\"Bo Xu\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1609/AAAI.V34I05.6248\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"7ae9c15c90b7d3e3c90c7f6d83743d4a0e07416b\",\"title\":\"DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/7ae9c15c90b7d3e3c90c7f6d83743d4a0e07416b\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1911.07251\",\"authors\":[{\"authorId\":\"15246869\",\"name\":\"X. Jiang\"},{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"70565757\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"15586721\",\"name\":\"Yingying Zhuang\"},{\"authorId\":\"47958013\",\"name\":\"Xingxing Zhang\"},{\"authorId\":null,\"name\":\"Yue Hu\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1609/AAAI.V34I07.6769\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b42cb7889053f5c89380c82604aa33fd6270894\",\"title\":\"DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/0b42cb7889053f5c89380c82604aa33fd6270894\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1902.09368\",\"authors\":[{\"authorId\":\"71119060\",\"name\":\"Gi-Cheon Kang\"},{\"authorId\":\"70262116\",\"name\":\"Jaeseo Lim\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":\"10.18653/v1/D19-1209\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"86754c8a22d5df5636aa5603db01835b5d4ee32c\",\"title\":\"Dual Attention Networks for Visual Reference Resolution in Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/86754c8a22d5df5636aa5603db01835b5d4ee32c\",\"venue\":\"EMNLP\",\"year\":2019},{\"arxivId\":\"2003.06745\",\"authors\":[{\"authorId\":null,\"name\":\"Yi Zhu\"},{\"authorId\":\"94228656\",\"name\":\"Fengda Zhu\"},{\"authorId\":\"83374215\",\"name\":\"Zhaohuan Zhan\"},{\"authorId\":\"16208574\",\"name\":\"Bingqian Lin\"},{\"authorId\":\"73416694\",\"name\":\"Jianbin Jiao\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"13246332\",\"name\":\"Xiaodan Liang\"}],\"doi\":\"10.1109/cvpr42600.2020.01074\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"07de32d212a2d712b43557b4977b2ac76556e1d6\",\"title\":\"Vision-Dialog Navigation by Exploring Cross-Modal Memory\",\"url\":\"https://www.semanticscholar.org/paper/07de32d212a2d712b43557b4977b2ac76556e1d6\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1468726581\",\"name\":\"Baber Khalid\"},{\"authorId\":\"2715920\",\"name\":\"Malihe Alikhani\"},{\"authorId\":\"144884556\",\"name\":\"Matthew Stone\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f2c3bda0854a6dbe45371ca06abd1ebbb17aa21\",\"title\":\"Combining Cognitive Modeling and Reinforcement Learning for Clarification in Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/8f2c3bda0854a6dbe45371ca06abd1ebbb17aa21\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"}],\"doi\":\"10.1109/ICCVW.2019.00536\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"title\":\"EgoVQA - An Egocentric Video Question Answering Benchmark Dataset\",\"url\":\"https://www.semanticscholar.org/paper/71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1812.02347\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"145974111\",\"name\":\"Jun Xiao\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/ICCV.2019.00471\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f37d93ac4dc22b00d8a0cd1b5d54764eb8a2c5d5\",\"title\":\"Counterfactual Critic Multi-Agent Training for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/f37d93ac4dc22b00d8a0cd1b5d54764eb8a2c5d5\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2008.04858\",\"authors\":[{\"authorId\":\"15246869\",\"name\":\"X. Jiang\"},{\"authorId\":\"1643931890\",\"name\":\"Siyi Du\"},{\"authorId\":\"70565757\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"15087270\",\"name\":\"Yajing Sun\"},{\"authorId\":\"119883573\",\"name\":\"J. Yu\"}],\"doi\":\"10.1145/3394171.3413826\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c03ca2e4c26e868b7405e6782c72b85b16db8e4\",\"title\":\"KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning in Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/5c03ca2e4c26e868b7405e6782c72b85b16db8e4\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2002.10340\",\"authors\":[{\"authorId\":\"144052839\",\"name\":\"Wei Pang\"},{\"authorId\":\"50142157\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-58517-4_40\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b7ba12958cb670442236accd5e0579728821ad7d\",\"title\":\"Guessing State Tracking for Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/b7ba12958cb670442236accd5e0579728821ad7d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4009206\",\"name\":\"O. Kovaleva\"},{\"authorId\":\"1866532\",\"name\":\"Chaitanya Shivade\"},{\"authorId\":\"33201965\",\"name\":\"Satyananda Kashyap\"},{\"authorId\":\"1410148565\",\"name\":\"Karina Kanjaria\"},{\"authorId\":\"40346984\",\"name\":\"Joy T. Wu\"},{\"authorId\":\"13403287\",\"name\":\"D. Ballah\"},{\"authorId\":\"1388126424\",\"name\":\"Adam Coy\"},{\"authorId\":\"2308391\",\"name\":\"Alexandros Karargyris\"},{\"authorId\":\"2230103\",\"name\":\"Yufan Guo\"},{\"authorId\":\"1768272818\",\"name\":\"David James Beymer\"},{\"authorId\":\"1681193\",\"name\":\"Anna Rumshisky\"},{\"authorId\":\"80257800\",\"name\":\"Vandana Mukherjee\"}],\"doi\":\"10.18653/v1/2020.bionlp-1.6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6609489a0f800a9ef411efdcfca4c014c4e86aa8\",\"title\":\"Towards Visual Dialog for Radiology\",\"url\":\"https://www.semanticscholar.org/paper/6609489a0f800a9ef411efdcfca4c014c4e86aa8\",\"venue\":\"BioNLP\",\"year\":2020},{\"arxivId\":\"2012.15015\",\"authors\":[{\"authorId\":\"65844131\",\"name\":\"Yuxian Meng\"},{\"authorId\":\"1845298604\",\"name\":\"Shuhe Wang\"},{\"authorId\":\"5439717\",\"name\":\"Qinghong Han\"},{\"authorId\":\"48304805\",\"name\":\"Xiaofei Sun\"},{\"authorId\":\"93192602\",\"name\":\"Fei Wu\"},{\"authorId\":null,\"name\":\"Rui Yan\"},{\"authorId\":\"5183779\",\"name\":\"J. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3470e15ae52baae8b8560dd59c616da9820cf43a\",\"title\":\"OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/3470e15ae52baae8b8560dd59c616da9820cf43a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1812.01880\",\"authors\":[{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143905981\",\"name\":\"Baoyuan Wu\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2019.00678\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88ec56eee6a787ccda0cf7fbfbec48c6b4ad68fe\",\"title\":\"Learning to Compose Dynamic Tree Structures for Visual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/88ec56eee6a787ccda0cf7fbfbec48c6b4ad68fe\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2004.02194\",\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"46507139\",\"name\":\"Haibo Wang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/CVPR42600.2020.01007\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a0d2ea210c9bd21676605682a76cec1a4004320a\",\"title\":\"Iterative Context-Aware Graph Inference for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32543309\",\"name\":\"Tianling Jiang\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"49046633\",\"name\":\"Chunping Liu\"},{\"authorId\":\"21633777\",\"name\":\"Hailin Shao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"78c2f7520becde5e3bcd9b952791d67c33a48612\",\"title\":\"Visual-Textual Alignment for Graph Inference in Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/78c2f7520becde5e3bcd9b952791d67c33a48612\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"15246869\",\"name\":\"X. Jiang\"},{\"authorId\":\"31055300\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"1735739\",\"name\":\"Weifeng Zhang\"},{\"authorId\":\"1581498565\",\"name\":\"Yue Hu\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/TIP.2020.3034494\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72c2a42e00131f63f4898e29f641c32f9cdd24a3\",\"title\":\"Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/72c2a42e00131f63f4898e29f641c32f9cdd24a3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":\"2003.06576\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"1491414917\",\"name\":\"Xin Yan\"},{\"authorId\":\"153269968\",\"name\":\"Jun Xiao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1109/cvpr42600.2020.01081\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dc08d90005b12f66a12798fd79959a8f7f8c4885\",\"title\":\"Counterfactual Samples Synthesizing for Robust Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/dc08d90005b12f66a12798fd79959a8f7f8c4885\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.04998\",\"authors\":[{\"authorId\":null,\"name\":\"Yitong Li\"},{\"authorId\":\"7379232\",\"name\":\"Dianqi Li\"},{\"authorId\":\"28612243\",\"name\":\"S. Prakash\"},{\"authorId\":\"49161455\",\"name\":\"P. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0fa920ea66b7642eb97df389b7e32faa9744b3e\",\"title\":\"Toward Interpretability of Dual-Encoder Models for Dialogue Response Suggestions\",\"url\":\"https://www.semanticscholar.org/paper/f0fa920ea66b7642eb97df389b7e32faa9744b3e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.13278\",\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1785083\",\"name\":\"Michael R. Lyu\"},{\"authorId\":\"145310663\",\"name\":\"Irwin King\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.269\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"06c7269c10125589d2599f684b751b1640f7a0cc\",\"title\":\"VD-BERT: A Unified Vision and Dialog Transformer with BERT\",\"url\":\"https://www.semanticscholar.org/paper/06c7269c10125589d2599f684b751b1640f7a0cc\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1912.02379\",\"authors\":[{\"authorId\":\"46258988\",\"name\":\"Vishvak S. Murahari\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"}],\"doi\":\"10.1007/978-3-030-58523-5_20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"604d7678235f5bb6039794e382d12058cecf8070\",\"title\":\"Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline\",\"url\":\"https://www.semanticscholar.org/paper/604d7678235f5bb6039794e382d12058cecf8070\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.09272\",\"authors\":[{\"authorId\":\"3469119\",\"name\":\"Daniela Massiceti\"},{\"authorId\":\"3468926\",\"name\":\"Viveka Kulharia\"},{\"authorId\":\"144679302\",\"name\":\"P. Dokania\"},{\"authorId\":\"40155668\",\"name\":\"N. Siddharth\"},{\"authorId\":\"143635540\",\"name\":\"P. Torr\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6ddbe134ed10abaf043d8bcb11343e5c7e7358bf\",\"title\":\"A Revised Generative Evaluation of Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/6ddbe134ed10abaf043d8bcb11343e5c7e7358bf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.00579\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145215470\",\"name\":\"Yu Cheng\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"38079056\",\"name\":\"Jingjing Liu\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.18653/v1/P19-1648\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d9f1eaac344b7e6cc76396f576c5dc0bd0b34f4\",\"title\":\"Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/8d9f1eaac344b7e6cc76396f576c5dc0bd0b34f4\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1904.08608\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/ICCV.2019.00435\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"45f9856d418527d23dc7c89197627fa1f3b215f9\",\"title\":\"Learning to Collocate Neural Modules for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/45f9856d418527d23dc7c89197627fa1f3b215f9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2008.00397\",\"authors\":[{\"authorId\":\"51115516\",\"name\":\"L. Yang\"},{\"authorId\":\"49435166\",\"name\":\"Fanqi Meng\"},{\"authorId\":\"31395194\",\"name\":\"Ming-Kuang Daniel Wu\"},{\"authorId\":\"1850625173\",\"name\":\"Vicent Ying\"},{\"authorId\":\"150345115\",\"name\":\"Xianchao Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c711f0b0f9e214d6ee462cffcac733221bf07026\",\"title\":\"SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space\",\"url\":\"https://www.semanticscholar.org/paper/c711f0b0f9e214d6ee462cffcac733221bf07026\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.06206\",\"authors\":[{\"authorId\":\"9628638\",\"name\":\"Yun-Wei Chu\"},{\"authorId\":\"152923110\",\"name\":\"Kuan-Yen Lin\"},{\"authorId\":\"23608432\",\"name\":\"Chao-Chun Hsu\"},{\"authorId\":\"1746959\",\"name\":\"Lun-Wei Ku\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e885f1523349be48c884a4663d705487fde05b8a\",\"title\":\"Multi-step Joint-Modality Attention Network for Scene-Aware Dialogue System\",\"url\":\"https://www.semanticscholar.org/paper/e885f1523349be48c884a4663d705487fde05b8a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49291874\",\"name\":\"Guilherme Toso\"},{\"authorId\":\"1978756495\",\"name\":\"Fabricio Breve\"}],\"doi\":\"10.1007/978-3-030-58799-4_64\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ba3c002c582bfa7cf3f69e871f6b57691e56952\",\"title\":\"Synchronization Analysis in Models of Coupled Oscillators\",\"url\":\"https://www.semanticscholar.org/paper/9ba3c002c582bfa7cf3f69e871f6b57691e56952\",\"venue\":\"ICCSA\",\"year\":2020},{\"arxivId\":\"2011.02709\",\"authors\":[{\"authorId\":\"48806403\",\"name\":\"Zhenxing Zhang\"},{\"authorId\":\"52643489\",\"name\":\"L. Schomaker\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6fe68c8c970ba707e8767e5010c32b3cb1033063\",\"title\":\"DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation\",\"url\":\"https://www.semanticscholar.org/paper/6fe68c8c970ba707e8767e5010c32b3cb1033063\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.11390\",\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1007/978-3-030-58586-0_14\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"468d5c15df63892ff06fb94c7b5cad0242685d02\",\"title\":\"Efficient Attention Mechanism for Visual Dialog that Can Handle All the Interactions Between Multiple Inputs\",\"url\":\"https://www.semanticscholar.org/paper/468d5c15df63892ff06fb94c7b5cad0242685d02\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2008.07935\",\"authors\":[{\"authorId\":\"48269537\",\"name\":\"Ye Zhu\"},{\"authorId\":\"50118837\",\"name\":\"Y. Wu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"96519714\",\"name\":\"Yan Yan\"}],\"doi\":\"10.1007/978-3-030-58592-1_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29ef96b859c2ec55b67fca6f2e59ac27a6bc2cb1\",\"title\":\"Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents\",\"url\":\"https://www.semanticscholar.org/paper/29ef96b859c2ec55b67fca6f2e59ac27a6bc2cb1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1812.03299\",\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1109/ICCV.2019.00477\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e504843bfd3ca81b859895439bbe19efc1f3dc61\",\"title\":\"Learning to Assemble Neural Module Tree Networks for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/e504843bfd3ca81b859895439bbe19efc1f3dc61\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2006.08322\",\"authors\":[{\"authorId\":\"47196880\",\"name\":\"Ziwei Wang\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"},{\"authorId\":\"49407982\",\"name\":\"Hui-min Lu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c87ed5765c51f3ed294a399d9d2384dc296c585\",\"title\":\"ORD: Object Relationship Discovery for Visual Dialogue Generation\",\"url\":\"https://www.semanticscholar.org/paper/6c87ed5765c51f3ed294a399d9d2384dc296c585\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.10496\",\"authors\":[{\"authorId\":\"3167894\",\"name\":\"Jiaxin Qi\"},{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"50560698\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/cvpr42600.2020.01087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ad99fef72abe84e6b5f194d4973ca824812dbb11\",\"title\":\"Two Causal Principles for Improving Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/ad99fef72abe84e6b5f194d4973ca824812dbb11\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2001.06354\",\"authors\":[{\"authorId\":\"51270689\",\"name\":\"Hyounghun Kim\"},{\"authorId\":\"47300698\",\"name\":\"Hao Tan\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.1609/AAAI.V34I05.6320\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cad92e34323aa135d13d692c759246a8da54d05\",\"title\":\"Modality-Balanced Models for Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/0cad92e34323aa135d13d692c759246a8da54d05\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"title\":\"Efficient Attention Mechanism for Handling All the Interactions between Many Inputs with Application to Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.03127\",\"authors\":[{\"authorId\":\"80927455\",\"name\":\"Takuma Udagawa\"},{\"authorId\":\"48342111\",\"name\":\"T. Yamazaki\"},{\"authorId\":\"1705519\",\"name\":\"A. Aizawa\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.67\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6ee340c37a60d20dd8ebe19bc6fa614b981c2060\",\"title\":\"A Linguistic Analysis of Visually Grounded Dialogues Based on Spatial Expressions\",\"url\":\"https://www.semanticscholar.org/paper/6ee340c37a60d20dd8ebe19bc6fa614b981c2060\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.08977\",\"authors\":[{\"authorId\":\"7289061\",\"name\":\"Hyemin Ahn\"},{\"authorId\":\"98605657\",\"name\":\"O. Kwon\"},{\"authorId\":\"9086571\",\"name\":\"Kyoungdo Kim\"},{\"authorId\":\"30742686\",\"name\":\"Dongheui Lee\"},{\"authorId\":\"34184385\",\"name\":\"Songhwai Oh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"052fd049bc6a3cad1dd7d516c406b1d5d08c5d60\",\"title\":\"Visually Grounding Instruction for History-Dependent Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/052fd049bc6a3cad1dd7d516c406b1d5d08c5d60\",\"venue\":\"\",\"year\":2020}],\"corpusId\":54446647,\"doi\":\"10.1109/CVPR.2019.00684\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":10,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"41db31c451cd819d22f9c0b90be110edc4424911\",\"references\":[{\"arxivId\":\"1708.01956\",\"authors\":[{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"26538630\",\"name\":\"Z. Kyaw\"},{\"authorId\":\"46380822\",\"name\":\"Jinyang Yu\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/ICCV.2017.454\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"65429789a95b3026457de76d46b5ec94158ce10e\",\"title\":\"PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN\",\"url\":\"https://www.semanticscholar.org/paper/65429789a95b3026457de76d46b5ec94158ce10e\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1709.07992\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"20812150\",\"name\":\"Andreas M. Lehrmann\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ad8c0489d908d6bb4b48eb56c8c92b8f545216f5\",\"title\":\"Visual Reference Resolution using Attention Memory for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/ad8c0489d908d6bb4b48eb56c8c92b8f545216f5\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1512.02902\",\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2016.501\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"title\":\"MovieQA: Understanding Stories in Movies through Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1809.01816\",\"authors\":[{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1007/978-3-030-01267-0_10\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"782bc02684de81f98c92475957501801bf91e023\",\"title\":\"Visual Coreference Resolution in Visual Dialog using Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/782bc02684de81f98c92475957501801bf91e023\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1602.07332\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"40591424\",\"name\":\"J. Kravitz\"},{\"authorId\":\"6000646\",\"name\":\"Stephanie Chen\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"1388344504\",\"name\":\"David A. Shamma\"},{\"authorId\":\"1379506718\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-016-0981-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"title\":\"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\",\"url\":\"https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"1608.00187\",\"authors\":[{\"authorId\":\"1830034\",\"name\":\"Cewu Lu\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-46448-0_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d9506257186023b78cf19ed4f9e77a4ae4fa0f0\",\"title\":\"Visual Relationship Detection with Language Priors\",\"url\":\"https://www.semanticscholar.org/paper/4d9506257186023b78cf19ed4f9e77a4ae4fa0f0\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1706.01554\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"145721096\",\"name\":\"A. Kannan\"},{\"authorId\":\"145743311\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8cdd241b474bf7b0632162403ac2a3c4799252ad\",\"title\":\"Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model\",\"url\":\"https://www.semanticscholar.org/paper/8cdd241b474bf7b0632162403ac2a3c4799252ad\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49433662\",\"name\":\"E. Gumbel\"}],\"doi\":\"10.2307/2342529\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6e103f45ce2f653fd7aafcf67afa80c39130294f\",\"title\":\"Statistical Theory of Extreme Values and Some Practical Applications : A Series of Lectures\",\"url\":\"https://www.semanticscholar.org/paper/6e103f45ce2f653fd7aafcf67afa80c39130294f\",\"venue\":\"\",\"year\":1954},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Buhrmester\"},{\"authorId\":null,\"name\":\"T. Kwang\"},{\"authorId\":null,\"name\":\"S. D. Gosling\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Amazon\\u2019s mechanical turk: A new source of inexpensive\",\"url\":\"\",\"venue\":\"yet high-quality, data? Perspectives on psychological science, 6(1):3\\u20135\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"40085065\",\"name\":\"Percy Liang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-10590-1_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"title\":\"Linking People in Videos with \\\"Their\\\" Names Using Coreference Resolution\",\"url\":\"https://www.semanticscholar.org/paper/a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"39855500\",\"name\":\"K. Gupta\"},{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"},{\"authorId\":\"24508084\",\"name\":\"Deshraj Yadav\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/TPAMI.2018.2828437\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4c16428a0bae507d2a1785860f07168a807d8e59\",\"title\":\"Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/4c16428a0bae507d2a1785860f07168a807d8e59\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1712.01892\",\"authors\":[{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2018.00437\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d576ffe624f11fe4e84a03d4063856a5af838f\",\"title\":\"Grounding Referring Expressions in Images by Variational Context\",\"url\":\"https://www.semanticscholar.org/paper/69d576ffe624f11fe4e84a03d4063856a5af838f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Anderson\"},{\"authorId\":null,\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"D. Teney\"},{\"authorId\":null,\"name\":\"J. Bruce\"},{\"authorId\":null,\"name\":\"M. Johnson\"},{\"authorId\":null,\"name\":\"N. S\\u00fcnderhauf\"},{\"authorId\":null,\"name\":\"I. Reid\"},{\"authorId\":null,\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and A\",\"url\":\"\",\"venue\":\"van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR\",\"year\":2018},{\"arxivId\":\"1611.05594\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1406012647\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"104757141\",\"name\":\"Jian Shao\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/CVPR.2017.667\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88513e738a95840de05a62f0e43d30a67b3c542e\",\"title\":\"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/88513e738a95840de05a62f0e43d30a67b3c542e\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1511.02799\",\"authors\":[{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"38666915\",\"name\":\"D. Klein\"}],\"doi\":\"10.1109/CVPR.2016.12\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"21c99706bb26e9012bfb4d8d48009a3d45af59b2\",\"title\":\"Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/21c99706bb26e9012bfb4d8d48009a3d45af59b2\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Teney\"},{\"authorId\":null,\"name\":\"P. Anderson\"},{\"authorId\":null,\"name\":\"X. He\"},{\"authorId\":null,\"name\":\"A. van den Hengel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Tips and tricks for visual question answering: Learnings\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1611.08481\",\"authors\":[{\"authorId\":\"153559313\",\"name\":\"H. D. Vries\"},{\"authorId\":\"3367628\",\"name\":\"Florian Strub\"},{\"authorId\":\"144631588\",\"name\":\"A. Chandar\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/CVPR.2017.475\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bed7834ae7d371171977a590872f60d137c2f951\",\"title\":\"GuessWhat?! Visual Object Discovery through Multi-modal Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/bed7834ae7d371171977a590872f60d137c2f951\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144332826\",\"name\":\"Chen Kong\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2014.455\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"13549b4e6fffbb7932b7a83a8eb6be27e6a60eca\",\"title\":\"What Are You Talking About? Text-to-Image Coreference\",\"url\":\"https://www.semanticscholar.org/paper/13549b4e6fffbb7932b7a83a8eb6be27e6a60eca\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1812.01880\",\"authors\":[{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143905981\",\"name\":\"Baoyuan Wu\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2019.00678\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88ec56eee6a787ccda0cf7fbfbec48c6b4ad68fe\",\"title\":\"Learning to Compose Dynamic Tree Structures for Visual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/88ec56eee6a787ccda0cf7fbfbec48c6b4ad68fe\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1704.01518\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"39578749\",\"name\":\"Siyu Tang\"},{\"authorId\":\"2390510\",\"name\":\"Seong Joon Oh\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2017.447\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db2fecc8b1bd175d39687eb471360707a5fddb03\",\"title\":\"Generating Descriptions with Grounded and Co-referenced People\",\"url\":\"https://www.semanticscholar.org/paper/db2fecc8b1bd175d39687eb471360707a5fddb03\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"8983218\",\"name\":\"S. Buch\"},{\"authorId\":\"32273391\",\"name\":\"L. Dery\"},{\"authorId\":\"1873736\",\"name\":\"Animesh Garg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2018.00623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aeac614f10cb2a5dc000fdee30d857bbe5456ce5\",\"title\":\"Finding \\\"It\\\": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/aeac614f10cb2a5dc000fdee30d857bbe5456ce5\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1511.02283\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"3317152\",\"name\":\"Oana-Maria Camburu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/CVPR.2016.9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e65142010431ffc089b272a1174214e00693e503\",\"title\":\"Generation and Comprehension of Unambiguous Object Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e65142010431ffc089b272a1174214e00693e503\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47173549\",\"name\":\"Michael D. Buhrmester\"},{\"authorId\":\"6369977\",\"name\":\"T. Kwang\"},{\"authorId\":\"2705485\",\"name\":\"S. Gosling\"}],\"doi\":\"10.1177/1745691610393980\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce10854b47b1294e61e4b055a6c9a90a49ff3208\",\"title\":\"Amazon's Mechanical Turk\",\"url\":\"https://www.semanticscholar.org/paper/ce10854b47b1294e61e4b055a6c9a90a49ff3208\",\"venue\":\"Perspectives on psychological science : a journal of the Association for Psychological Science\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"P. Wang\"},{\"authorId\":null,\"name\":\"C. Shen\"},{\"authorId\":null,\"name\":\"I. Reid\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and A\",\"url\":\"\",\"venue\":\"van den Hengel. Are you talking to me? Reasoned visual dialog generation through adversarial learning. In CVPR\",\"year\":2018},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3285716\",\"name\":\"Xinye Lin\"},{\"authorId\":\"9527255\",\"name\":\"Yixin Chen\"},{\"authorId\":\"1871246\",\"name\":\"X. Chang\"},{\"authorId\":\"1723807\",\"name\":\"X. Liu\"},{\"authorId\":\"47119262\",\"name\":\"X. Wang\"}],\"doi\":\"10.1145/3161412\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"299b07ec255398c7f9d147cb39d113e4926cc51b\",\"title\":\"SHOW\",\"url\":\"https://www.semanticscholar.org/paper/299b07ec255398c7f9d147cb39d113e4926cc51b\",\"venue\":\"IMWUT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"P. Wang\"},{\"authorId\":null,\"name\":\"C. Shen\"},{\"authorId\":null,\"name\":\"I. Reid\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and A\",\"url\":\"\",\"venue\":\"van den Hengel. Are you talking to me? Reasoned visual dialog generation through adversarial learning. In CVPR\",\"year\":2018},{\"arxivId\":\"1902.09774\",\"authors\":[{\"authorId\":\"145422343\",\"name\":\"Dalu Guo\"},{\"authorId\":\"48258751\",\"name\":\"Chang Xu\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR.2019.01068\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e5eb953608ab5eb95dd054a44980b5258fd7b8d7\",\"title\":\"Image-Question-Answer Synergistic Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/e5eb953608ab5eb95dd054a44980b5258fd7b8d7\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1505.00468\",\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"118707418\",\"name\":\"M. Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/s11263-016-0966-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1707.07998\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"31790073\",\"name\":\"C. Buehler\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1109/CVPR.2018.00636\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"title\":\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jacob Andreas\"},{\"authorId\":null,\"name\":\"Marcus Rohrbach\"},{\"authorId\":null,\"name\":\"Trevor Darrell\"},{\"authorId\":null,\"name\":\"Dan Klein\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Neural module networks Vqa : Visual question answering\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1608.00525\",\"authors\":[{\"authorId\":\"3081378\",\"name\":\"Varun K. Nagaraja\"},{\"authorId\":\"2852035\",\"name\":\"V. Morariu\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1007/978-3-319-46493-0_48\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"86eef3a1dff2bd2808847358cdb7f5ba2b7e0214\",\"title\":\"Modeling Context Between Objects for Referring Expression Understanding\",\"url\":\"https://www.semanticscholar.org/paper/86eef3a1dff2bd2808847358cdb7f5ba2b7e0214\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1611.01144\",\"authors\":[{\"authorId\":\"145116380\",\"name\":\"Eric Jang\"},{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"16443937\",\"name\":\"Ben Poole\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"29e944711a354c396fad71936f536e83025b6ce0\",\"title\":\"Categorical Reparameterization with Gumbel-Softmax\",\"url\":\"https://www.semanticscholar.org/paper/29e944711a354c396fad71936f536e83025b6ce0\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1711.07613\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00639\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9dde6ed569684356c46217fa53224272b668bae8\",\"title\":\"Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/9dde6ed569684356c46217fa53224272b668bae8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Anderson\"},{\"authorId\":null,\"name\":\"Q. Wu\"},{\"authorId\":null,\"name\":\"D. Teney\"},{\"authorId\":null,\"name\":\"J. Bruce\"},{\"authorId\":null,\"name\":\"M. Johnson\"},{\"authorId\":null,\"name\":\"N. S\\u00fcnderhauf\"},{\"authorId\":null,\"name\":\"I. Reid\"},{\"authorId\":null,\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and A\",\"url\":\"\",\"venue\":\"van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR\",\"year\":2018},{\"arxivId\":\"1708.02711\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00444\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81\",\"title\":\"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1611.00712\",\"authors\":[{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"1714004\",\"name\":\"A. Mnih\"},{\"authorId\":\"1725303\",\"name\":\"Y. Teh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"515a21e90117941150923e559729c59f5fdade1c\",\"title\":\"The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\",\"url\":\"https://www.semanticscholar.org/paper/515a21e90117941150923e559729c59f5fdade1c\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143845796\",\"name\":\"Jeffrey Pennington\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1162\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"title\":\"Glove: Global Vectors for Word Representation\",\"url\":\"https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1606.01847\",\"authors\":[{\"authorId\":\"50599725\",\"name\":\"A. Fukui\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"3422876\",\"name\":\"Daylen Yang\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D16-1044\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fddc15480d086629b960be5bff96232f967f2252\",\"title\":\"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/fddc15480d086629b960be5bff96232f967f2252\",\"venue\":\"EMNLP\",\"year\":2016}],\"title\":\"Recursive Visual Attention in Visual Dialog\",\"topics\":[{\"topic\":\"dialog\",\"topicId\":\"14876\",\"url\":\"https://www.semanticscholar.org/topic/14876\"},{\"topic\":\"Recursion (computer science)\",\"topicId\":\"2419\",\"url\":\"https://www.semanticscholar.org/topic/2419\"},{\"topic\":\"Question answering\",\"topicId\":\"18817\",\"url\":\"https://www.semanticscholar.org/topic/18817\"},{\"topic\":\"Map\",\"topicId\":\"2392\",\"url\":\"https://www.semanticscholar.org/topic/2392\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Parsing\",\"topicId\":\"1910\",\"url\":\"https://www.semanticscholar.org/topic/1910\"},{\"topic\":\"Network interface device\",\"topicId\":\"353029\",\"url\":\"https://www.semanticscholar.org/topic/353029\"}],\"url\":\"https://www.semanticscholar.org/paper/41db31c451cd819d22f9c0b90be110edc4424911\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019}\n"