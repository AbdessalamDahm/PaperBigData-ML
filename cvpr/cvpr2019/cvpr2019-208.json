"{\"abstract\":\"Visual-semantic embedding aims to find a shared latent space where related visual and textual instances are close to each other. Most current methods learn injective embedding functions that map an instance to a single point in the shared space. Unfortunately, injective embedding cannot effectively handle polysemous instances with multiple possible meanings; at best, it would find an average representation of different meanings. This hinders its use in real-world scenarios where individual instances and their cross-modal associations are often ambiguous. In this work, we introduce Polysemous Instance Embedding Networks (PIE-Nets) that compute multiple and diverse representations of an instance by combining global context with locally-guided features via multi-head self-attention and residual learning. To learn visual-semantic embedding, we tie-up two PIE-Nets and optimize them jointly in the multiple instance learning framework. Most existing work on cross-modal retrieval focus on image-text pairs of data. Here, we also tackle a more challenging case of video-text retrieval. To facilitate further research in video-text retrieval, we release a new dataset of 50K video-sentence pairs collected from social media, dubbed MRW (my reaction when). We demonstrate our approach on both image-text and video-text retrieval scenarios using MS-COCO, TGIF, and our new MRW dataset.\",\"arxivId\":\"1906.04402\",\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\",\"url\":\"https://www.semanticscholar.org/author/2317183\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\",\"url\":\"https://www.semanticscholar.org/author/152714397\"}],\"citationVelocity\":13,\"citations\":[{\"arxivId\":\"1911.05978\",\"authors\":[{\"authorId\":\"145382418\",\"name\":\"P. Narayana\"},{\"authorId\":\"1406355360\",\"name\":\"Aniket Pednekar\"},{\"authorId\":\"1406355394\",\"name\":\"A. Krishnamoorthy\"},{\"authorId\":\"113477341\",\"name\":\"Kazoo Sone\"},{\"authorId\":\"40632403\",\"name\":\"S. Basu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0ea46e70da5e0882ac6e08afd8a9f6285abfb12a\",\"title\":\"HUSE: Hierarchical Universal Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/0ea46e70da5e0882ac6e08afd8a9f6285abfb12a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.04305\",\"authors\":[{\"authorId\":\"1918424\",\"name\":\"Jiacheng Chen\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1491232360\",\"name\":\"Hao Wu\"},{\"authorId\":\"1691963\",\"name\":\"Yuning Jiang\"},{\"authorId\":\"1906061249\",\"name\":\"Changhu Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a27f4af07d73baccda94b5d2a2535b7dfdb58019\",\"title\":\"Learning the Best Pooling Strategy for Visual Semantic Embedding\",\"url\":\"https://www.semanticscholar.org/paper/a27f4af07d73baccda94b5d2a2535b7dfdb58019\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.05134\",\"authors\":[{\"authorId\":\"49184936\",\"name\":\"S. Wang\"},{\"authorId\":\"3373117\",\"name\":\"R. Wang\"},{\"authorId\":\"12977859\",\"name\":\"Z. Yao\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1109/WACV45572.2020.9093614\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c9f1b9e51c1d16c346e1ef679c243d61702a8a80\",\"title\":\"Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/c9f1b9e51c1d16c346e1ef679c243d61702a8a80\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2008.08716\",\"authors\":[{\"authorId\":\"152666334\",\"name\":\"Sudipta Paul\"},{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2efdeaa68e94eeef2831536cdd53f3e3868bbdca\",\"title\":\"Text-based Localization of Moments in a Video Corpus\",\"url\":\"https://www.semanticscholar.org/paper/2efdeaa68e94eeef2831536cdd53f3e3868bbdca\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.10534\",\"authors\":[{\"authorId\":\"3100027\",\"name\":\"Nikolaos Sarafianos\"},{\"authorId\":\"93809632\",\"name\":\"Xiang Xu\"},{\"authorId\":\"1706204\",\"name\":\"I. Kakadiaris\"}],\"doi\":\"10.1109/ICCV.2019.00591\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed3ed6458abdfb54fa1e3ce4434dfd9adb3b55ad\",\"title\":\"Adversarial Representation Learning for Text-to-Image Matching\",\"url\":\"https://www.semanticscholar.org/paper/ed3ed6458abdfb54fa1e3ce4434dfd9adb3b55ad\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2004.07967\",\"authors\":[{\"authorId\":\"144810262\",\"name\":\"H. M. Nguyen\"},{\"authorId\":\"3388392\",\"name\":\"T. Miyazaki\"},{\"authorId\":\"2137463\",\"name\":\"Y. Sugaya\"},{\"authorId\":\"1740235\",\"name\":\"S. Omachi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20b15ddf9efe511961321355bcb4a62c3a29a9ef\",\"title\":\"Multiple Visual-Semantic Embedding for Video Retrieval from Query Sentence\",\"url\":\"https://www.semanticscholar.org/paper/20b15ddf9efe511961321355bcb4a62c3a29a9ef\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"11179553\",\"name\":\"Zhigang Lin\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"},{\"authorId\":\"40286455\",\"name\":\"Xuelong Li\"}],\"doi\":\"10.1016/j.patcog.2020.107359\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"124f42e4787a81eba2fc6311b11a3afec60c4f57\",\"title\":\"Stacked squeeze-and-excitation recurrent residual network for visual-semantic matching\",\"url\":\"https://www.semanticscholar.org/paper/124f42e4787a81eba2fc6311b11a3afec60c4f57\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1732300163\",\"name\":\"Y. Guo\"},{\"authorId\":\"12564022\",\"name\":\"J. Chen\"},{\"authorId\":\"7214794\",\"name\":\"H. Zhang\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3372278.3390709\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3d1ac5e2f4d70b26ee7e79ee30a38f64676b404a\",\"title\":\"Visual Relations Augmented Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/3d1ac5e2f4d70b26ee7e79ee30a38f64676b404a\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":\"2004.00197\",\"authors\":[{\"authorId\":\"48470090\",\"name\":\"Tong Wang\"},{\"authorId\":\"152366931\",\"name\":\"Lei Zhu\"},{\"authorId\":\"46504200\",\"name\":\"Zhiyong Cheng\"},{\"authorId\":\"31115284\",\"name\":\"Jingjing Li\"},{\"authorId\":\"2856513\",\"name\":\"Huaxiang Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"573023797592ac5e7d165fb4ab4c9d9b12a23117\",\"title\":\"Task-adaptive Asymmetric Deep Cross-modal Hashing\",\"url\":\"https://www.semanticscholar.org/paper/573023797592ac5e7d165fb4ab4c9d9b12a23117\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.09634\",\"authors\":[{\"authorId\":\"1624475253\",\"name\":\"Yujie Zhong\"},{\"authorId\":\"11246861\",\"name\":\"Linhai Xie\"},{\"authorId\":\"1421686725\",\"name\":\"Sen Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"},{\"authorId\":\"2666898\",\"name\":\"Yishu Miao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f416cdb1cd51a73fdb7f8f4194308798ae57033c\",\"title\":\"Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision\",\"url\":\"https://www.semanticscholar.org/paper/f416cdb1cd51a73fdb7f8f4194308798ae57033c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"144547315\",\"name\":\"E. Hovy\"}],\"doi\":\"10.1145/3372278.3390674\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d5018c8bdf61129ca06a333ddbbe32dd84e50c2\",\"title\":\"Forward and Backward Multimodal NMT for Improved Monolingual and Multilingual Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/4d5018c8bdf61129ca06a333ddbbe32dd84e50c2\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":\"1904.09471\",\"authors\":[{\"authorId\":\"143780023\",\"name\":\"Zhong Ji\"},{\"authorId\":\"49528408\",\"name\":\"H. Wang\"},{\"authorId\":\"1783847\",\"name\":\"J. Han\"},{\"authorId\":\"145134722\",\"name\":\"Y. Pang\"}],\"doi\":\"10.1109/ICCV.2019.00585\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3d2d1f64a11ca9234716a777dedc962586930a9\",\"title\":\"Saliency-Guided Attention Network for Image-Sentence Matching\",\"url\":\"https://www.semanticscholar.org/paper/a3d2d1f64a11ca9234716a777dedc962586930a9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490935563\",\"name\":\"Feifei Zhang\"},{\"authorId\":\"49235537\",\"name\":\"M. Xu\"},{\"authorId\":\"3069077\",\"name\":\"Q. Mao\"},{\"authorId\":\"48258806\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3394171.3413917\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"50bd8474487073851b115385b9ded538d8825bc2\",\"title\":\"Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/50bd8474487073851b115385b9ded538d8825bc2\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143780023\",\"name\":\"Zhong Ji\"},{\"authorId\":\"11179553\",\"name\":\"Zhigang Lin\"},{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"101084939\",\"name\":\"Y. He\"}],\"doi\":\"10.1109/ACCESS.2020.2975594\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"284cd0512ecd5d7cec335b0038444085398ebaf5\",\"title\":\"Multi-Modal Memory Enhancement Attention Network for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/284cd0512ecd5d7cec335b0038444085398ebaf5\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52184535\",\"name\":\"Rintaro Yanagi\"},{\"authorId\":\"3470264\",\"name\":\"R. Togo\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ICCE-Taiwan49838.2020.9258183\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6aa9e05fb605ba643402b9dc58354b5236d85e9a\",\"title\":\"Image Retrieval with Data Augmentation of Sentence Labels Based on Paraphrasing\",\"url\":\"https://www.semanticscholar.org/paper/6aa9e05fb605ba643402b9dc58354b5236d85e9a\",\"venue\":\"2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)\",\"year\":2020},{\"arxivId\":\"2007.08617\",\"authors\":[{\"authorId\":\"84267967\",\"name\":\"C. Thomas\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-58523-5_19\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"432921b7a2c782cedc2a7d87b6194b906e31086d\",\"title\":\"Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/432921b7a2c782cedc2a7d87b6194b906e31086d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993602785\",\"name\":\"Ruijian Jia\"},{\"authorId\":\"50142011\",\"name\":\"Xinsheng Wang\"},{\"authorId\":\"2852872\",\"name\":\"S. Pang\"},{\"authorId\":\"2237811\",\"name\":\"Jihua Zhu\"},{\"authorId\":\"40689776\",\"name\":\"J. Xue\"}],\"doi\":\"10.1145/3394171.3414023\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4955fda9e96f97e0dcd45c846ec52cb118d89060\",\"title\":\"Look, Listen and Infer\",\"url\":\"https://www.semanticscholar.org/paper/4955fda9e96f97e0dcd45c846ec52cb118d89060\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1910.06514\",\"authors\":[{\"authorId\":\"144872058\",\"name\":\"Takashi Matsubara\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"743164e8d89d7d6138ce32ee069489d9097bb816\",\"title\":\"Target-Oriented Deformation of Visual-Semantic Embedding Space\",\"url\":\"https://www.semanticscholar.org/paper/743164e8d89d7d6138ce32ee069489d9097bb816\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"144762952\",\"name\":\"J. Han\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"}],\"doi\":\"10.1109/tcyb.2020.2985716\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"758890bef9a1a85a25a1f6831a58f00a462476af\",\"title\":\"SMAN: Stacked Multimodal Attention Network for Cross-Modal Image-Text Retrieval.\",\"url\":\"https://www.semanticscholar.org/paper/758890bef9a1a85a25a1f6831a58f00a462476af\",\"venue\":\"IEEE transactions on cybernetics\",\"year\":2020},{\"arxivId\":\"2009.05381\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"title\":\"Hybrid Space Learning for Language-based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03186\",\"authors\":[{\"authorId\":\"7885575\",\"name\":\"E. Amrani\"},{\"authorId\":\"1397958297\",\"name\":\"R. Ben-Ari\"},{\"authorId\":\"143679933\",\"name\":\"Daniel Rotman\"},{\"authorId\":\"144858358\",\"name\":\"Alex Bronstein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"777873ef6d23c2cdd7dfd6c4834eb56769a25bb1\",\"title\":\"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning\",\"url\":\"https://www.semanticscholar.org/paper/777873ef6d23c2cdd7dfd6c4834eb56769a25bb1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52184535\",\"name\":\"Rintaro Yanagi\"},{\"authorId\":\"3470264\",\"name\":\"R. Togo\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ACCESS.2020.2995815\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fcbabfac84c2fcf1e9d4af4d132e18bf6da5ee1\",\"title\":\"Enhancing Cross-Modal Retrieval Based on Modality-Specific and Embedding Spaces\",\"url\":\"https://www.semanticscholar.org/paper/5fcbabfac84c2fcf1e9d4af4d132e18bf6da5ee1\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2007.08883\",\"authors\":[{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"72095125\",\"name\":\"Y. Zhang\"},{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"}],\"doi\":\"10.1007/978-3-030-58586-0_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe573437cbd4069556348ad28dfeae2df46e22a0\",\"title\":\"Consensus-Aware Visual-Semantic Embedding for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/fe573437cbd4069556348ad28dfeae2df46e22a0\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.03403\",\"authors\":[{\"authorId\":\"1490652152\",\"name\":\"Jiwei Wei\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1524912498\",\"name\":\"Yang Yang\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":null,\"name\":\"Zheng Wang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/CVPR42600.2020.01302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"title\":\"Universal Weighting Metric Learning for Cross-Modal Matching\",\"url\":\"https://www.semanticscholar.org/paper/dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1911.05609\",\"authors\":[{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"1791319\",\"name\":\"Shangfei Wang\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\"},{\"authorId\":\"47669634\",\"name\":\"Dhiraj Joshi\"},{\"authorId\":\"144116884\",\"name\":\"Qiang Ji\"}],\"doi\":\"10.1145/3363560\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efb8a3dded3778d7469ecbc43b655a119f0c9ae4\",\"title\":\"Affective Computing for Large-scale Heterogeneous Multimedia Data\",\"url\":\"https://www.semanticscholar.org/paper/efb8a3dded3778d7469ecbc43b655a119f0c9ae4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1911.12377\",\"authors\":[{\"authorId\":\"67344892\",\"name\":\"Federico Landi\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"40186452\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb4e5d203efae1f0838bb59e31026ce3e7511f87\",\"title\":\"Perceive, Transform, and Act: Multi-Modal Attention Networks for Vision-and-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/bb4e5d203efae1f0838bb59e31026ce3e7511f87\",\"venue\":\"ArXiv\",\"year\":2019}],\"corpusId\":184488029,\"doi\":\"10.1109/CVPR.2019.00208\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"a39d5919531a56de0e36f6b76142041b5d508213\",\"references\":[{\"arxivId\":\"1703.03130\",\"authors\":[{\"authorId\":\"3146592\",\"name\":\"Zhouhan Lin\"},{\"authorId\":\"2521552\",\"name\":\"Minwei Feng\"},{\"authorId\":\"1790831\",\"name\":\"C. D. Santos\"},{\"authorId\":\"2482533\",\"name\":\"Mo Yu\"},{\"authorId\":\"144028698\",\"name\":\"B. Xiang\"},{\"authorId\":\"145218984\",\"name\":\"Bowen Zhou\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"204a4a70428f3938d2c538a4d74c7ae0416306d8\",\"title\":\"A Structured Self-attentive Sentence Embedding\",\"url\":\"https://www.semanticscholar.org/paper/204a4a70428f3938d2c538a4d74c7ae0416306d8\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"145527707\",\"name\":\"Zhe L. Lin\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.5244/C.31.89\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fd52b7cc49c89899ea96cf830428049592447a80\",\"title\":\"Multiple Instance Visual-Semantic Embedding\",\"url\":\"https://www.semanticscholar.org/paper/fd52b7cc49c89899ea96cf830428049592447a80\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143845796\",\"name\":\"Jeffrey Pennington\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1162\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"title\":\"Glove: Global Vectors for Word Representation\",\"url\":\"https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1711.06420\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"39298199\",\"name\":\"Li Niu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/CVPR.2018.00750\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"724b253a55e86ad230ba05c7eb78f249e09258d9\",\"title\":\"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/724b253a55e86ad230ba05c7eb78f249e09258d9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1607.06215\",\"authors\":[{\"authorId\":\"2500189\",\"name\":\"K. Wang\"},{\"authorId\":\"2397961\",\"name\":\"Qiyue Yin\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":\"50425438\",\"name\":\"S. Wu\"},{\"authorId\":null,\"name\":\"Liang Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6721e26e65c6b72dafae74fd1e7a6f2e6023a312\",\"title\":\"A Comprehensive Survey on Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6721e26e65c6b72dafae74fd1e7a6f2e6023a312\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47908786\",\"name\":\"Y. Liu\"},{\"authorId\":\"1687503\",\"name\":\"Yanming Guo\"},{\"authorId\":\"143866184\",\"name\":\"E. Bakker\"},{\"authorId\":\"1731570\",\"name\":\"M. Lew\"}],\"doi\":\"10.1109/ICCV.2017.442\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"90208e8cbda1f39f3d06e41d97898408b13192a7\",\"title\":\"Learning a Recurrent Residual Fusion Network for Multimodal Matching\",\"url\":\"https://www.semanticscholar.org/paper/90208e8cbda1f39f3d06e41d97898408b13192a7\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1810.00319\",\"authors\":[{\"authorId\":\"2390510\",\"name\":\"Seong Joon Oh\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"31846538\",\"name\":\"Jiyan Pan\"},{\"authorId\":\"143980328\",\"name\":\"J. Roth\"},{\"authorId\":\"3302320\",\"name\":\"Florian Schroff\"},{\"authorId\":\"50716462\",\"name\":\"A. Gallagher\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f9f80ab3922edaa469a42b6acc405df76616fe56\",\"title\":\"Modeling Uncertainty with Hedged Instance Embedding\",\"url\":\"https://www.semanticscholar.org/paper/f9f80ab3922edaa469a42b6acc405df76616fe56\",\"venue\":\"ICLR 2018\",\"year\":2018},{\"arxivId\":\"1605.01379\",\"authors\":[{\"authorId\":\"9136919\",\"name\":\"X. Lin\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1007/978-3-319-46475-6_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c94217efec8773ef947df2772f92df8c5726f855\",\"title\":\"Leveraging Visual Question Answering for Image-Caption Ranking\",\"url\":\"https://www.semanticscholar.org/paper/c94217efec8773ef947df2772f92df8c5726f855\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"145527707\",\"name\":\"Zhe L. Lin\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1145/2964284.2967212\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0eca0146c16a758c6b198966605561b4dec5c59a\",\"title\":\"Joint Image-Text Representation by Gaussian Visual-Semantic Embedding\",\"url\":\"https://www.semanticscholar.org/paper/0eca0146c16a758c6b198966605561b4dec5c59a\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"},{\"authorId\":\"145914024\",\"name\":\"R. Lathrop\"},{\"authorId\":\"1388700951\",\"name\":\"Tomas Lozano-Perez\"}],\"doi\":\"10.1016/S0004-3702(96)00034-3\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1c7d38f68fe1150895a186e30b60c02dd89a676a\",\"title\":\"Solving the Multiple Instance Problem with Axis-Parallel Rectangles\",\"url\":\"https://www.semanticscholar.org/paper/1c7d38f68fe1150895a186e30b60c02dd89a676a\",\"venue\":\"Artif. Intell.\",\"year\":1997},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1712.02036\",\"authors\":[{\"authorId\":\"144368930\",\"name\":\"Yan Huang\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"144143336\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/CVPR.2018.00645\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f322eef6a4c965910e03f6997b1bc2acd413e273\",\"title\":\"Learning Semantic Concepts and Order for Image and Sentence Matching\",\"url\":\"https://www.semanticscholar.org/paper/f322eef6a4c965910e03f6997b1bc2acd413e273\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1793182\",\"name\":\"Nikhil Rasiwasia\"},{\"authorId\":\"152929215\",\"name\":\"J. C. Pereira\"},{\"authorId\":\"1808675\",\"name\":\"E. Coviello\"},{\"authorId\":\"2304386\",\"name\":\"G. Doyle\"},{\"authorId\":\"1725533\",\"name\":\"G. Lanckriet\"},{\"authorId\":\"143643017\",\"name\":\"R. Levy\"},{\"authorId\":\"1699559\",\"name\":\"N. Vasconcelos\"}],\"doi\":\"10.1145/1873951.1873987\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6e8c107f5f852640daaf324111fc9cf35b5e4acc\",\"title\":\"A new approach to cross-modal multimedia retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6e8c107f5f852640daaf324111fc9cf35b5e4acc\",\"venue\":\"ACM Multimedia\",\"year\":2010},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1511.06078\",\"authors\":[{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"1738814\",\"name\":\"Y. Li\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1109/CVPR.2016.541\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b27e791e843c924ef052981b79490ab59fc0433d\",\"title\":\"Learning Deep Structure-Preserving Image-Text Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/b27e791e843c924ef052981b79490ab59fc0433d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"0805.2368\",\"authors\":[{\"authorId\":\"1708497\",\"name\":\"A. Gretton\"},{\"authorId\":\"1704422\",\"name\":\"K. Borgwardt\"},{\"authorId\":\"1733256\",\"name\":\"Malte J. Rasch\"},{\"authorId\":\"1707625\",\"name\":\"B. Sch\\u00f6lkopf\"},{\"authorId\":\"46234526\",\"name\":\"Alex Smola\"}],\"doi\":\"10.7551/mitpress/7503.003.0069\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9bca4d7b932e0854c3325f1578cfd17341dd8ea8\",\"title\":\"A Kernel Method for the Two-Sample-Problem\",\"url\":\"https://www.semanticscholar.org/paper/9bca4d7b932e0854c3325f1578cfd17341dd8ea8\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39554008\",\"name\":\"Saeideh Bakhshi\"},{\"authorId\":\"1760364\",\"name\":\"D. Shamma\"},{\"authorId\":\"144557123\",\"name\":\"L. Kennedy\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"1893882\",\"name\":\"Paloma de Juan\"},{\"authorId\":\"143782217\",\"name\":\"J. Kaye\"}],\"doi\":\"10.1145/2858036.2858532\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"345413ede3e5743645d95140e74a73a86a96d7f8\",\"title\":\"Fast, Cheap, and Good: Why Animated GIFs Engage Us\",\"url\":\"https://www.semanticscholar.org/paper/345413ede3e5743645d95140e74a73a86a96d7f8\",\"venue\":\"CHI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34792252\",\"name\":\"J. Amores\"}],\"doi\":\"10.1016/j.artint.2013.06.003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"64372501affd8571db20dc606b0146a76c266303\",\"title\":\"Multiple instance classification: Review, taxonomy and comparative study\",\"url\":\"https://www.semanticscholar.org/paper/64372501affd8571db20dc606b0146a76c266303\",\"venue\":\"Artif. Intell.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46544636\",\"name\":\"T. Mukherjee\"},{\"authorId\":\"1697755\",\"name\":\"Timothy M. Hospedales\"}],\"doi\":\"10.18653/v1/D16-1089\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"492596eb82218085485ad77e7b414250f5289605\",\"title\":\"Gaussian Visual-Linguistic Embedding for Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/492596eb82218085485ad77e7b414250f5289605\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1504.08083\",\"authors\":[{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"}],\"doi\":\"10.1109/ICCV.2015.169\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ffdbc358b63378f07311e883dddacc9faeeaf4b\",\"title\":\"Fast R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/7ffdbc358b63378f07311e883dddacc9faeeaf4b\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1801.09042\",\"authors\":[{\"authorId\":\"49455017\",\"name\":\"Yipin Zhou\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/WACV.2018.00025\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0073c51ee3fcba12c8a01f2481f9fac1101d6e62\",\"title\":\"Image2GIF: Generating Cinemagraphs Using Recurrent Deep Q-Networks\",\"url\":\"https://www.semanticscholar.org/paper/0073c51ee3fcba12c8a01f2481f9fac1101d6e62\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\"}],\"doi\":\"10.1145/2964284.2967195\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fbc97fa87acb767434b21d8d796caf3c3ed7f01\",\"title\":\"Analyzing and Predicting GIF Interestingness\",\"url\":\"https://www.semanticscholar.org/paper/9fbc97fa87acb767434b21d8d796caf3c3ed7f01\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1608.07973\",\"authors\":[{\"authorId\":\"3451681\",\"name\":\"Aviv Eisenschtat\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":\"10.1109/CVPR.2017.201\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2616e0fbce43362a338acedcbb5cd80db7bbb7e5\",\"title\":\"Linking Image and Text with 2-Way Nets\",\"url\":\"https://www.semanticscholar.org/paper/2616e0fbce43362a338acedcbb5cd80db7bbb7e5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1711.06666\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-01267-0_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1be2c16060974f66e5366872ebbee21325075e8\",\"title\":\"ADVISE: Symbolism and External Knowledge for Decoding Advertisements\",\"url\":\"https://www.semanticscholar.org/paper/e1be2c16060974f66e5366872ebbee21325075e8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1109/CVPR.2015.7298966\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"title\":\"Deep correlation for matching images and text\",\"url\":\"https://www.semanticscholar.org/paper/efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1605.04850\",\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"}],\"doi\":\"10.1109/CVPR.2016.114\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28f0e0d3783659bc9adb2cec56f19b1f90cdd2be\",\"title\":\"Video2GIF: Automatic Generation of Animated GIFs from Video\",\"url\":\"https://www.semanticscholar.org/paper/28f0e0d3783659bc9adb2cec56f19b1f90cdd2be\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46868596\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"}],\"doi\":\"10.1007/978-3-030-01246-5_42\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a86eb42952412ee02e3f6da06f874f1946eff6b\",\"title\":\"Deep Cross-Modal Projection Learning for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/1a86eb42952412ee02e3f6da06f874f1946eff6b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1212.4522\",\"authors\":[{\"authorId\":\"5115386\",\"name\":\"Yunchao Gong\"},{\"authorId\":\"2259786\",\"name\":\"Q. Ke\"},{\"authorId\":\"2090818\",\"name\":\"M. Isard\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1007/s11263-013-0658-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7fceccc7a1046caa4936b14eeacb71ccf4d6be10\",\"title\":\"A Multi-View Embedding Space for Modeling Internet Images, Tags, and Their Semantics\",\"url\":\"https://www.semanticscholar.org/paper/7fceccc7a1046caa4936b14eeacb71ccf4d6be10\",\"venue\":\"International Journal of Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrea Frome\"},{\"authorId\":null,\"name\":\"Greg S Corrado\"},{\"authorId\":null,\"name\":\"Jon Shlens\"},{\"authorId\":null,\"name\":\"Samy Bengio\"},{\"authorId\":null,\"name\":\"Jeff Dean\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Devise: A deep visualsemantic embedding model\",\"url\":\"\",\"venue\":\"In NIPS,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2447185\",\"name\":\"B. Jou\"},{\"authorId\":\"144474750\",\"name\":\"Subhabrata Bhattacharya\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/2647868.2656408\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a17d76ae8479b5927df587696c58d690a9e2915d\",\"title\":\"Predicting Viewer Perceived Emotions in Animated GIFs\",\"url\":\"https://www.semanticscholar.org/paper/a17d76ae8479b5927df587696c58d690a9e2915d\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"1707.05612\",\"authors\":[{\"authorId\":\"2978170\",\"name\":\"Fartash Faghri\"},{\"authorId\":\"1793739\",\"name\":\"David J. Fleet\"},{\"authorId\":\"51131802\",\"name\":\"J. Kiros\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"faa093a53b83f0e9c35a0bfbcacee0a16f8eb6d1\",\"title\":\"VSE++: Improved Visual-Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/faa093a53b83f0e9c35a0bfbcacee0a16f8eb6d1\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21451088\",\"name\":\"P. Ekman\"}],\"doi\":\"10.1080/02699939208411068\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bfdbfe3bf703594b884ae69f505f94ce7e98141e\",\"title\":\"An argument for basic emotions\",\"url\":\"https://www.semanticscholar.org/paper/bfdbfe3bf703594b884ae69f505f94ce7e98141e\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30296838\",\"name\":\"Bokun Wang\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3123266.3123326\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61060bea27a3410260988540b627ccc5ba131822\",\"title\":\"Adversarial Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/61060bea27a3410260988540b627ccc5ba131822\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1801452\",\"name\":\"D. McDuff\"},{\"authorId\":\"3100369\",\"name\":\"A. Mahmoud\"},{\"authorId\":\"3396867\",\"name\":\"Mohammad Mavadati\"},{\"authorId\":\"145354041\",\"name\":\"M. Amr\"},{\"authorId\":\"40463348\",\"name\":\"Jay Turcot\"},{\"authorId\":\"1754451\",\"name\":\"R. E. Kaliouby\"}],\"doi\":\"10.1145/2851581.2890247\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7d62f1b94a62f9f649d27eb1396379654498feee\",\"title\":\"AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/7d62f1b94a62f9f649d27eb1396379654498feee\",\"venue\":\"CHI Extended Abstracts\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144339350\",\"name\":\"G. Andrew\"},{\"authorId\":\"144365054\",\"name\":\"R. Arora\"},{\"authorId\":\"1748118\",\"name\":\"J. Bilmes\"},{\"authorId\":\"2924113\",\"name\":\"Karen Livescu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6\",\"title\":\"Deep Canonical Correlation Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e2257e3f56ccb12875a57bc0a8cca1d9d7e93ec6\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1803.08024\",\"authors\":[{\"authorId\":\"1863953\",\"name\":\"Kuang-Huei Lee\"},{\"authorId\":\"1683647\",\"name\":\"X. Chen\"},{\"authorId\":\"144988571\",\"name\":\"Gang Hua\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"}],\"doi\":\"10.1007/978-3-030-01225-0_13\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"45dd2a3cd7c27f2e9509b023d702408f5ac11c9d\",\"title\":\"Stacked Cross Attention for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/45dd2a3cd7c27f2e9509b023d702408f5ac11c9d\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1511.06361\",\"authors\":[{\"authorId\":\"2210865\",\"name\":\"Ivan Vendrov\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"46b8cbcdff87b842c2c1d4a003c831f845096ba7\",\"title\":\"Order-Embeddings of Images and Language\",\"url\":\"https://www.semanticscholar.org/paper/46b8cbcdff87b842c2c1d4a003c831f845096ba7\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1804.04318\",\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd286954bba554035ea98946afdd02262b9bca45\",\"title\":\"Cross-Modal Retrieval with Implicit Concept Association\",\"url\":\"https://www.semanticscholar.org/paper/cd286954bba554035ea98946afdd02262b9bca45\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1606.03498\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"571b0750085ae3d939525e62af510ee2cee9d5ea\",\"title\":\"Improved Techniques for Training GANs\",\"url\":\"https://www.semanticscholar.org/paper/571b0750085ae3d939525e62af510ee2cee9d5ea\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31868671\",\"name\":\"W. Chen\"},{\"authorId\":\"1729713\",\"name\":\"Ognjen Rudovic\"},{\"authorId\":\"1719389\",\"name\":\"Rosalind W. Picard\"}],\"doi\":\"10.1109/ACII.2017.8273647\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dce3bb30f0c19121a71e0bfe1d418f855cb13ce\",\"title\":\"GIFGIF+: Collecting emotional animated GIFs with clustered multi-task learning\",\"url\":\"https://www.semanticscholar.org/paper/3dce3bb30f0c19121a71e0bfe1d418f855cb13ce\",\"venue\":\"2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)\",\"year\":2017},{\"arxivId\":\"1904.09237\",\"authors\":[{\"authorId\":\"1981186\",\"name\":\"S. Reddi\"},{\"authorId\":\"144055676\",\"name\":\"S. Kale\"},{\"authorId\":\"2794322\",\"name\":\"S. Kumar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43b854bb89f212e2d69a4b64ef0c28ff4d09f666\",\"title\":\"On the Convergence of Adam and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/43b854bb89f212e2d69a4b64ef0c28ff4d09f666\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1704.04497\",\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.149\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"title\":\"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1703.05908\",\"authors\":[{\"authorId\":\"145639633\",\"name\":\"Yao-Hung Hubert Tsai\"},{\"authorId\":\"9930869\",\"name\":\"Liang-Kang Huang\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":\"10.1109/ICCV.2017.386\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70bcc6766f055fc279bbb07af967c026ff8a2d9c\",\"title\":\"Learning Robust Visual-Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/70bcc6766f055fc279bbb07af967c026ff8a2d9c\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144598072\",\"name\":\"D. King\"}],\"doi\":\"10.1145/1577069.1755843\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"title\":\"Dlib-ml: A Machine Learning Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1504.06063\",\"authors\":[{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"11955007\",\"name\":\"Z. Lu\"},{\"authorId\":\"50812138\",\"name\":\"L. Shang\"},{\"authorId\":\"49404233\",\"name\":\"Hang Li\"}],\"doi\":\"10.1109/ICCV.2015.301\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"153d6feb7149e063b33e8ee437b74e4a2def8057\",\"title\":\"Multimodal Convolutional Neural Networks for Matching Image and Sentence\",\"url\":\"https://www.semanticscholar.org/paper/153d6feb7149e063b33e8ee437b74e4a2def8057\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1611.05588\",\"authors\":[{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":null,\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/CVPR.2017.767\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e1b6735f6ecb09e1d83b0aa9d2cde42993ee2eb0\",\"title\":\"Instance-Aware Image and Sentence Matching with Selective Multimodal LSTM\",\"url\":\"https://www.semanticscholar.org/paper/e1b6735f6ecb09e1d83b0aa9d2cde42993ee2eb0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145969200\",\"name\":\"B. Klein\"},{\"authorId\":\"3004979\",\"name\":\"G. Lev\"},{\"authorId\":\"2251827\",\"name\":\"Gil Sadeh\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":\"10.1109/CVPR.2015.7299073\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51239b320c73f3f2219286bf62f24d6763379328\",\"title\":\"Associating neural word embeddings with deep image representations using Fisher Vectors\",\"url\":\"https://www.semanticscholar.org/paper/51239b320c73f3f2219286bf62f24d6763379328\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ivan Vendrov\"},{\"authorId\":null,\"name\":\"Ryan Kiros\"},{\"authorId\":null,\"name\":\"Sanja Fidler\"},{\"authorId\":null,\"name\":\"Raquel Urtasun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Learning robust visual - semantic embeddings Attention is all you need\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39825530\",\"name\":\"Fangxiang Feng\"},{\"authorId\":\"47119447\",\"name\":\"Xiao-Jie Wang\"},{\"authorId\":\"2462591\",\"name\":\"Ruifan Li\"}],\"doi\":\"10.1145/2647868.2654902\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b7b00080f83e5b65a10bb07c0a7091e746c3a57e\",\"title\":\"Cross-modal Retrieval with Correspondence Autoencoder\",\"url\":\"https://www.semanticscholar.org/paper/b7b00080f83e5b65a10bb07c0a7091e746c3a57e\",\"venue\":\"ACM Multimedia\",\"year\":2014}],\"title\":\"Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval\",\"topics\":[{\"topic\":\"Modal logic\",\"topicId\":\"61528\",\"url\":\"https://www.semanticscholar.org/topic/61528\"},{\"topic\":\"Document retrieval\",\"topicId\":\"14824\",\"url\":\"https://www.semanticscholar.org/topic/14824\"},{\"topic\":\"Multiple instance learning\",\"topicId\":\"97196\",\"url\":\"https://www.semanticscholar.org/topic/97196\"},{\"topic\":\"Social media\",\"topicId\":\"6015\",\"url\":\"https://www.semanticscholar.org/topic/6015\"},{\"topic\":\"Text corpus\",\"topicId\":\"14829\",\"url\":\"https://www.semanticscholar.org/topic/14829\"},{\"topic\":\"Modality (human\\u2013computer interaction)\",\"topicId\":\"462\",\"url\":\"https://www.semanticscholar.org/topic/462\"},{\"topic\":\"Mount Rainier (packet writing)\",\"topicId\":\"1253621\",\"url\":\"https://www.semanticscholar.org/topic/1253621\"},{\"topic\":\"Information retrieval\",\"topicId\":\"2867\",\"url\":\"https://www.semanticscholar.org/topic/2867\"}],\"url\":\"https://www.semanticscholar.org/paper/a39d5919531a56de0e36f6b76142041b5d508213\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019}\n"