"{\"abstract\":\"We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers sequence-level information along with frame-level information. Thoughtful experiments on several datasets and real-world samples demonstrate significantly better results obtained by our method than the state-of-the-art methods in both quantitative and qualitative comparisons.\",\"arxivId\":\"1905.03820\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\",\"url\":\"https://www.semanticscholar.org/author/1753356\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\",\"url\":\"https://www.semanticscholar.org/author/4053196\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\",\"url\":\"https://www.semanticscholar.org/author/3270912\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\",\"url\":\"https://www.semanticscholar.org/author/2026123\"}],\"citationVelocity\":20,\"citations\":[{\"arxivId\":\"1912.05566\",\"authors\":[{\"authorId\":\"34105638\",\"name\":\"Justus Thies\"},{\"authorId\":\"1854465\",\"name\":\"M. Elgharib\"},{\"authorId\":\"9102722\",\"name\":\"A. Tewari\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"}],\"doi\":\"10.1007/978-3-030-58517-4_42\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7de44a2edb7570fddbdcb2ee69df9f17adc3396\",\"title\":\"Neural Voice Puppetry: Audio-driven Facial Reenactment\",\"url\":\"https://www.semanticscholar.org/paper/b7de44a2edb7570fddbdcb2ee69df9f17adc3396\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102356614\",\"name\":\"X. Zhang\"},{\"authorId\":\"66653936\",\"name\":\"X. Wu\"},{\"authorId\":\"103972572\",\"name\":\"Xinliang Zhai\"},{\"authorId\":\"2163652\",\"name\":\"Xianye Ben\"},{\"authorId\":\"40480346\",\"name\":\"C. Tu\"}],\"doi\":\"10.1109/CVPR42600.2020.01235\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ac827181a5d1d4e27eb662c2547fd2d6eb87c3b\",\"title\":\"DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads\",\"url\":\"https://www.semanticscholar.org/paper/3ac827181a5d1d4e27eb662c2547fd2d6eb87c3b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2012.07304\",\"authors\":[{\"authorId\":\"1488670226\",\"name\":\"N. Kumar\"},{\"authorId\":\"14085625\",\"name\":\"Srishti Goel\"},{\"authorId\":\"34275551\",\"name\":\"A. Narang\"},{\"authorId\":\"2036952884\",\"name\":\"Brejesh Lall\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc9ec9cde734084888a6e529488aed42abbd2c8e\",\"title\":\"Multi Modal Adaptive Normalization for Audio to Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/dc9ec9cde734084888a6e529488aed42abbd2c8e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2001.06427\",\"authors\":[{\"authorId\":\"152875073\",\"name\":\"Lele Chen\"},{\"authorId\":\"46218952\",\"name\":\"J. Tian\"},{\"authorId\":\"145521879\",\"name\":\"G. Li\"},{\"authorId\":\"80062058\",\"name\":\"Cheng-Haw Wu\"},{\"authorId\":\"143630142\",\"name\":\"Erh-Kan King\"},{\"authorId\":\"153819456\",\"name\":\"Kuan\\u2010Ting Chen\"},{\"authorId\":\"1930068\",\"name\":\"Shao-Hang Hsieh\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1109/WACV45572.2020.9093416\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"17965091231d0e3e78163396bfaa5b861d68c2ef\",\"title\":\"TailorGAN: Making User-Defined Fashion Designs\",\"url\":\"https://www.semanticscholar.org/paper/17965091231d0e3e78163396bfaa5b861d68c2ef\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47932716\",\"name\":\"X. Huang\"},{\"authorId\":\"151474565\",\"name\":\"Mingjie Wang\"},{\"authorId\":\"1473876432\",\"name\":\"M. Gong\"}],\"doi\":\"10.1007/S00371-020-01982-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef6467ddba06210c7cd0677955234e2e850274f0\",\"title\":\"Fine-grained talking face generation with video reinterpretation\",\"url\":\"https://www.semanticscholar.org/paper/ef6467ddba06210c7cd0677955234e2e850274f0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.08261\",\"authors\":[{\"authorId\":\"50383147\",\"name\":\"M. Doukas\"},{\"authorId\":\"1379747201\",\"name\":\"S. Zafeiriou\"},{\"authorId\":\"1790503\",\"name\":\"V. Sharmanska\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b9302212440e9be73808d0d3b802c523b4ca1d1\",\"title\":\"HeadGAN: Video-and-Audio-Driven Talking Head Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/7b9302212440e9be73808d0d3b802c523b4ca1d1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.15126\",\"authors\":[{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"88f8974f6f9bc2febfb88ed3ffe12dfc9c441808\",\"title\":\"One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing\",\"url\":\"https://www.semanticscholar.org/paper/88f8974f6f9bc2febfb88ed3ffe12dfc9c441808\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.09902\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"48670507\",\"name\":\"Xudong Xu\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"}],\"doi\":\"10.1007/978-3-030-58610-2_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"73aa926dad010a3f1bb89faa31241f97a89cc461\",\"title\":\"Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/73aa926dad010a3f1bb89faa31241f97a89cc461\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2008.02793\",\"authors\":[{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"},{\"authorId\":\"47932904\",\"name\":\"Xun Huang\"},{\"authorId\":\"150167366\",\"name\":\"Jiahui Yu\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"555e7029e8fd3b759300edfd2a4b7dfa2fd34a48\",\"title\":\"Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications\",\"url\":\"https://www.semanticscholar.org/paper/555e7029e8fd3b759300edfd2a4b7dfa2fd34a48\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.10010\",\"authors\":[{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\"},{\"authorId\":\"41052499\",\"name\":\"R. Mukhopadhyay\"},{\"authorId\":\"1744135\",\"name\":\"Vinay Namboodiri\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1145/3394171.3413532\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9c160a71d3265eedaf7645c39be073c966f10433\",\"title\":\"A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild\",\"url\":\"https://www.semanticscholar.org/paper/9c160a71d3265eedaf7645c39be073c966f10433\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2001.04758\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"100602595\",\"name\":\"Mandi Luo\"},{\"authorId\":\"77886607\",\"name\":\"Rui Wang\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"title\":\"Deep Audio-Visual Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.12318\",\"authors\":[{\"authorId\":\"8524712\",\"name\":\"Sanjana Sinha\"},{\"authorId\":\"18961663\",\"name\":\"S. Biswas\"},{\"authorId\":\"3262263\",\"name\":\"B. Bhowmick\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206665\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"24b3701937bd05ef6377f14682f4f67b40d09b50\",\"title\":\"Identity-Preserving Realistic Talking Face Generation\",\"url\":\"https://www.semanticscholar.org/paper/24b3701937bd05ef6377f14682f4f67b40d09b50\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"2002.10137\",\"authors\":[{\"authorId\":\"50488872\",\"name\":\"R. Yi\"},{\"authorId\":\"7818093\",\"name\":\"Zipeng Ye\"},{\"authorId\":\"2938279\",\"name\":\"J. Zhang\"},{\"authorId\":\"1679542\",\"name\":\"H. Bao\"},{\"authorId\":\"46398687\",\"name\":\"Yongjin Liu\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"bec253e82076dc363b8fd72d5c8fadf8f5b7e475\",\"title\":\"Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose\",\"url\":\"https://www.semanticscholar.org/paper/bec253e82076dc363b8fd72d5c8fadf8f5b7e475\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.01114\",\"authors\":[{\"authorId\":\"73771369\",\"name\":\"Prateek Manocha\"},{\"authorId\":\"46401518\",\"name\":\"Prithwijit Guha\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6873eb283ff1349db303dd79cc4bbb20fea99296\",\"title\":\"Facial Keypoint Sequence Generation from Audio\",\"url\":\"https://www.semanticscholar.org/paper/6873eb283ff1349db303dd79cc4bbb20fea99296\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2695251\",\"name\":\"Byungok Han\"},{\"authorId\":\"2897449\",\"name\":\"Woo-han Yun\"},{\"authorId\":\"1819119\",\"name\":\"J. Yoo\"},{\"authorId\":\"1677917255\",\"name\":\"Won Hwa Kim\"}],\"doi\":\"10.1109/ACCESS.2020.3018738\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"39ebae20ba45b718da4041513144edd4fbe26d6a\",\"title\":\"Toward Unbiased Facial Expression Recognition in the Wild via Cross-Dataset Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/39ebae20ba45b718da4041513144edd4fbe26d6a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49681078\",\"name\":\"L. Wang\"},{\"authorId\":\"3073831\",\"name\":\"Wanchun Chen\"},{\"authorId\":\"1642917405\",\"name\":\"Wenjia Yang\"},{\"authorId\":\"38819886\",\"name\":\"Fangming Bi\"},{\"authorId\":\"1696615\",\"name\":\"F. Yu\"}],\"doi\":\"10.1109/ACCESS.2020.2982224\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb03cff85876b3719fc9843f5bcf77f38dfa4f4a\",\"title\":\"A State-of-the-Art Review on Image Synthesis With Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/cb03cff85876b3719fc9843f5bcf77f38dfa4f4a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2004.11138\",\"authors\":[{\"authorId\":\"2934352\",\"name\":\"Yisroel Mirsky\"},{\"authorId\":\"49627181\",\"name\":\"W. Lee\"}],\"doi\":\"10.1145/3425780\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"92b4c8deecee703569b9e909dfb88aa70e691219\",\"title\":\"The Creation and Detection of Deepfakes: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/92b4c8deecee703569b9e909dfb88aa70e691219\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39906330\",\"name\":\"Sefik Emre Eskimez\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"72028302\",\"name\":\"Zhiyao Duan\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054103\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"db15b6799d7a09fcd41388d9e55d9c267e454965\",\"title\":\"End-To-End Generation of Talking Faces from Noisy Speech\",\"url\":\"https://www.semanticscholar.org/paper/db15b6799d7a09fcd41388d9e55d9c267e454965\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50488872\",\"name\":\"R. Yi\"},{\"authorId\":\"7818093\",\"name\":\"Zipeng Ye\"},{\"authorId\":\"2938279\",\"name\":\"J. Zhang\"},{\"authorId\":\"1679542\",\"name\":\"H. Bao\"},{\"authorId\":\"46398687\",\"name\":\"Yongjin Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d76d7102edb5668cf425af1e806375b5a01dab33\",\"title\":\"Audio-driven Talking Face Video Generation with Natural Head Pose\",\"url\":\"https://www.semanticscholar.org/paper/d76d7102edb5668cf425af1e806375b5a01dab33\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39906330\",\"name\":\"Sefik Emre Eskimez\"},{\"authorId\":\"93506431\",\"name\":\"Y. Zhang\"},{\"authorId\":\"72028302\",\"name\":\"Zhiyao Duan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8fc195090bcfe8de6d075681227a677cd64d2e0b\",\"title\":\"Speech Driven Talking Face Generation from a Single Image and an Emotion Condition\",\"url\":\"https://www.semanticscholar.org/paper/8fc195090bcfe8de6d075681227a677cd64d2e0b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.03201\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"4942685\",\"name\":\"G. Cui\"},{\"authorId\":\"16132631\",\"name\":\"Z. Kou\"},{\"authorId\":\"2743695\",\"name\":\"Haitian Zheng\"},{\"authorId\":\"100887531\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b63a9dc18baf645b4766b2b2ec8461c2c843275a\",\"title\":\"What comprises a good talking-head video generation?: A Survey and Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/b63a9dc18baf645b4766b2b2ec8461c2c843275a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.05225\",\"authors\":[{\"authorId\":\"31601235\",\"name\":\"Nataniel Ruiz\"},{\"authorId\":\"115268571\",\"name\":\"Barry-John Theobald\"},{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"},{\"authorId\":\"2034017347\",\"name\":\"Ahmed Hussein Abdelaziz\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac2a57f40759e353ed27ba912bcdfb5fa6d2cc5f\",\"title\":\"MorphGAN: One-Shot Face Synthesis GAN for Detecting Recognition Bias\",\"url\":\"https://www.semanticscholar.org/paper/ac2a57f40759e353ed27ba912bcdfb5fa6d2cc5f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.10688\",\"authors\":[{\"authorId\":\"8073409\",\"name\":\"Xin-Wei Yao\"},{\"authorId\":\"2416503\",\"name\":\"Ohad Fried\"},{\"authorId\":\"1399047905\",\"name\":\"Kayvon Fatahalian\"},{\"authorId\":\"1820412\",\"name\":\"M. Agrawala\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7984d3ee29eeb4ef9b0dc5fa6ad780bdca14e4b\",\"title\":\"Iterative Text-based Editing of Talking-heads Using Neural Retargeting\",\"url\":\"https://www.semanticscholar.org/paper/f7984d3ee29eeb4ef9b0dc5fa6ad780bdca14e4b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2009454032\",\"name\":\"Kaisiyuan Wang\"},{\"authorId\":\"123182910\",\"name\":\"Qianyi Wu\"},{\"authorId\":\"8785343\",\"name\":\"Linsen Song\"},{\"authorId\":\"51126032\",\"name\":\"Zhuoqian Yang\"},{\"authorId\":\"3096434\",\"name\":\"W. Wu\"},{\"authorId\":\"2005026904\",\"name\":\"Chen Qian\"},{\"authorId\":\"50361997\",\"name\":\"R. He\"},{\"authorId\":\"145858545\",\"name\":\"Y. Qiao\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":\"10.1007/978-3-030-58589-1_42\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ce752f69e167db27d4c186f5dfcd064e9b6e7f3b\",\"title\":\"MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation\",\"url\":\"https://www.semanticscholar.org/paper/ce752f69e167db27d4c186f5dfcd064e9b6e7f3b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"40454588\",\"name\":\"J. Frahm\"}],\"doi\":\"10.1007/978-3-030-58577-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"title\":\"Computer Vision \\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\\u201328, 2020, Proceedings, Part XXX\",\"url\":\"https://www.semanticscholar.org/paper/08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.12130\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"2479187\",\"name\":\"Moitreya Chatterjee\"},{\"authorId\":\"145237406\",\"name\":\"N. Ahuja\"}],\"doi\":\"10.1007/978-3-030-58583-9_42\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"title\":\"Sound2Sight: Generating Visual Dynamics from Sound and Context\",\"url\":\"https://www.semanticscholar.org/paper/02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.02631\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"50024256\",\"name\":\"Y. Li\"},{\"authorId\":\"2007680419\",\"name\":\"Feixia Zhu\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"597e8a6f560fcacc74e7637312c3d799f3dae241\",\"title\":\"Lets Play Music: Audio-driven Performance Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/597e8a6f560fcacc74e7637312c3d799f3dae241\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.05201\",\"authors\":[{\"authorId\":\"8785343\",\"name\":\"Linsen Song\"},{\"authorId\":\"3096434\",\"name\":\"W. Wu\"},{\"authorId\":\"49215730\",\"name\":\"C. Qian\"},{\"authorId\":\"50361997\",\"name\":\"R. He\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"369e9e0bb58bb4602a205a085bd683978a6d1f77\",\"title\":\"Everybody's Talkin': Let Me Talk as You Want\",\"url\":\"https://www.semanticscholar.org/paper/369e9e0bb58bb4602a205a085bd683978a6d1f77\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.12992\",\"authors\":[{\"authorId\":\"32025363\",\"name\":\"Yang Zhou\"},{\"authorId\":\"40580714\",\"name\":\"Dingzeyu Li\"},{\"authorId\":\"1399909799\",\"name\":\"Xintong Han\"},{\"authorId\":\"2808670\",\"name\":\"E. Kalogerakis\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"80452718\",\"name\":\"J. Echevarria\"}],\"doi\":\"10.1145/3414685.3417774\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"739497ec657d2c1e6ed7eb424951e0affe117be4\",\"title\":\"MakeItTalk: Speaker-Aware Talking Head Animation\",\"url\":\"https://www.semanticscholar.org/paper/739497ec657d2c1e6ed7eb424951e0affe117be4\",\"venue\":\"ACM Trans. Graph.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143790066\",\"name\":\"Dipanjan Das\"},{\"authorId\":\"18961663\",\"name\":\"S. Biswas\"},{\"authorId\":\"8524712\",\"name\":\"Sanjana Sinha\"},{\"authorId\":\"3262263\",\"name\":\"B. Bhowmick\"}],\"doi\":\"10.1007/978-3-030-58577-8_25\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c748ae0688a7ba0b6dbc33436a0fe76fa30786c2\",\"title\":\"Speech-Driven Facial Animation Using Cascaded GANs for Learning of Motion and Texture\",\"url\":\"https://www.semanticscholar.org/paper/c748ae0688a7ba0b6dbc33436a0fe76fa30786c2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.03530\",\"authors\":[{\"authorId\":\"7179962\",\"name\":\"Yan-qi Yang\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"5116578\",\"name\":\"Miaosen Wang\"},{\"authorId\":\"2000911990\",\"name\":\"Wendi Liu\"},{\"authorId\":\"1816749492\",\"name\":\"Yutian Chen\"},{\"authorId\":\"80266998\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1413718981\",\"name\":\"Eren Sezener\"},{\"authorId\":\"38712164\",\"name\":\"Luis C. Cobo\"},{\"authorId\":\"1715051\",\"name\":\"Misha Denil\"},{\"authorId\":\"3152281\",\"name\":\"Y. Aytar\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d99611d2b8d1147c72e1e311b50246f0a2d3f0eb\",\"title\":\"Large-scale multilingual audio visual dubbing\",\"url\":\"https://www.semanticscholar.org/paper/d99611d2b8d1147c72e1e311b50246f0a2d3f0eb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39128291\",\"name\":\"Teng Zhang\"},{\"authorId\":\"51492180\",\"name\":\"Lirui Deng\"},{\"authorId\":\"47059067\",\"name\":\"L. Zhang\"},{\"authorId\":\"3203437\",\"name\":\"Xianglei Dang\"}],\"doi\":\"10.1109/CCET50901.2020.9213159\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"42693caf4e6c7480295ad88094fdc8b30509305b\",\"title\":\"Deep Learning in Face Synthesis: A Survey on Deepfakes\",\"url\":\"https://www.semanticscholar.org/paper/42693caf4e6c7480295ad88094fdc8b30509305b\",\"venue\":\"2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET)\",\"year\":2020},{\"arxivId\":\"1906.06337\",\"authors\":[{\"authorId\":\"2160245\",\"name\":\"Konstantinos Vougioukas\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1007/s11263-019-01251-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f06ab43069480c09d7170075a6ea74669a71f139\",\"title\":\"Realistic Speech-Driven Facial Animation with GANs\",\"url\":\"https://www.semanticscholar.org/paper/f06ab43069480c09d7170075a6ea74669a71f139\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50190972\",\"name\":\"Dan Zeng\"},{\"authorId\":\"1753948451\",\"name\":\"Han Liu\"},{\"authorId\":\"46933412\",\"name\":\"H. Lin\"},{\"authorId\":\"39646508\",\"name\":\"Shiming Ge\"}],\"doi\":\"10.1145/3394171.3413844\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"79a511eca03dbfd0b48bd876f6bc99ca1690d1cc\",\"title\":\"Talking Face Generation with Expression-Tailored Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/79a511eca03dbfd0b48bd876f6bc99ca1690d1cc\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47825050\",\"name\":\"W. Wang\"},{\"authorId\":null,\"name\":\"Yan Wang\"},{\"authorId\":\"153552141\",\"name\":\"Jianqing Sun\"},{\"authorId\":\"50384154\",\"name\":\"Qingsong Liu\"},{\"authorId\":\"2013360\",\"name\":\"Jiaen Liang\"},{\"authorId\":\"144905985\",\"name\":\"T. Li\"}],\"doi\":\"10.21437/interspeech.2020-2304\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ed73963ceafc08f3c0c5ca379a8270a0a4a4748d\",\"title\":\"Speech Driven Talking Head Generation via Attentional Landmarks Based Representation\",\"url\":\"https://www.semanticscholar.org/paper/ed73963ceafc08f3c0c5ca379a8270a0a4a4748d\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2011.10727\",\"authors\":[{\"authorId\":\"49375545\",\"name\":\"R. Yadav\"},{\"authorId\":\"50847752\",\"name\":\"Ashish Sardana\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1893306\",\"name\":\"Rajesh M. Hegde\"}],\"doi\":\"10.21437/interspeech.2020-1823\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"411cb36c01d780524bf022d342f41b2db96b2bca\",\"title\":\"Stochastic Talking Face Generation Using Latent Distribution Matching\",\"url\":\"https://www.semanticscholar.org/paper/411cb36c01d780524bf022d342f41b2db96b2bca\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Wen\"},{\"authorId\":\"144638120\",\"name\":\"Miao Wang\"},{\"authorId\":\"1819028\",\"name\":\"C. Richardt\"},{\"authorId\":\"1949320248\",\"name\":\"Ze-Yin Chen\"},{\"authorId\":\"153056090\",\"name\":\"Shi-Min Hu\"}],\"doi\":\"10.1109/TVCG.2020.3023573\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"bae3842b82ffeacada84b9f02c168d6d0ad90b04\",\"title\":\"Photorealistic Audio-driven Video Portraits\",\"url\":\"https://www.semanticscholar.org/paper/bae3842b82ffeacada84b9f02c168d6d0ad90b04\",\"venue\":\"IEEE Transactions on Visualization and Computer Graphics\",\"year\":2020},{\"arxivId\":\"2007.08547\",\"authors\":[{\"authorId\":\"152875073\",\"name\":\"Lele Chen\"},{\"authorId\":\"4942685\",\"name\":\"G. Cui\"},{\"authorId\":\"9585133\",\"name\":\"Celong Liu\"},{\"authorId\":\"49969600\",\"name\":\"Z. Li\"},{\"authorId\":\"16132631\",\"name\":\"Z. Kou\"},{\"authorId\":\"92413558\",\"name\":\"Yi Xu\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-58545-7_3\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7e40f631825034e474be5640e6ac9ed7a6917c7\",\"title\":\"Talking-head Generation with Rhythmic Head Motion\",\"url\":\"https://www.semanticscholar.org/paper/f7e40f631825034e474be5640e6ac9ed7a6917c7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.10947\",\"authors\":[{\"authorId\":\"51494608\",\"name\":\"C. Yuan\"},{\"authorId\":\"2213974\",\"name\":\"M. Moghaddam\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a42274be0b27767a07dc318656945cb17834cb93\",\"title\":\"Garment Design with Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/a42274be0b27767a07dc318656945cb17834cb93\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":109936942,\"doi\":\"10.1109/CVPR.2019.00802\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":11,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"a0852cd9a026bc90168fa85fa422cb0e48f98394\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. S. Chung\"},{\"authorId\":null,\"name\":\"A. Jamaludin\"},{\"authorId\":null,\"name\":\"A. Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"You said that? In British Machine Vision Conference 2017\",\"url\":\"\",\"venue\":\"BMVC 2017, London, UK, September 4-7, 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Hong\"},{\"authorId\":null,\"name\":\"X. Lin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Synthesizing obama : learning lip sync from audio\",\"url\":\"\",\"venue\":\"ACM Trans . Graph .\",\"year\":null},{\"arxivId\":\"1803.10404\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"48458657\",\"name\":\"Zhiheng Li\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01234-2_32\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"title\":\"Lip Movements Generation at a Glance\",\"url\":\"https://www.semanticscholar.org/paper/d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145722155\",\"name\":\"Bo Fan\"},{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"},{\"authorId\":\"144206968\",\"name\":\"L. Xie\"}],\"doi\":\"10.1109/ICASSP.2015.7178899\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c028b86f5551757becd4c4304bddebb49e880b3\",\"title\":\"Photo-real talking head with deep bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/8c028b86f5551757becd4c4304bddebb49e880b3\",\"venue\":\"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2015},{\"arxivId\":\"1603.08155\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"3304525\",\"name\":\"Alexandre Alahi\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-46475-6_43\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fa3720371e78d04973ce9752781bc337480b68f\",\"title\":\"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/9fa3720371e78d04973ce9752781bc337480b68f\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Z.L.P.L.X.W. Hang Zhou\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Yu Liu\",\"url\":\"\",\"venue\":\"Talking face generation by adversarially disentangled audio-visual representation. In AAAI Conference on Artificial Intelligence (AAAI)\",\"year\":2019},{\"arxivId\":\"1710.00421\",\"authors\":[{\"authorId\":\"2664705\",\"name\":\"Y. Li\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"144752689\",\"name\":\"David Edwin Carlson\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a2bbf1f895a1850da3bb6d92b4ffbe61b68145d\",\"title\":\"Video Generation From Text\",\"url\":\"https://www.semanticscholar.org/paper/3a2bbf1f895a1850da3bb6d92b4ffbe61b68145d\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1711.10485\",\"authors\":[{\"authorId\":\"39866461\",\"name\":\"T. Xu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"48213346\",\"name\":\"Han Zhang\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"143713756\",\"name\":\"Xiaolei Huang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":\"10.1109/CVPR.2018.00143\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8b35c00edfa4edfd7a99d816e671023d2c000d55\",\"title\":\"AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8b35c00edfa4edfd7a99d816e671023d2c000d55\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54184-6_6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"title\":\"Lip Reading in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41210105\",\"name\":\"Zhou Wang\"},{\"authorId\":\"144492850\",\"name\":\"A. Bovik\"},{\"authorId\":\"120732976\",\"name\":\"H.R. Sheikh\"},{\"authorId\":\"122128303\",\"name\":\"E.P. Simoncelli\"}],\"doi\":\"10.1109/TIP.2003.819861\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eae2e0fa72e898c289365c0af16daf57a7a6cf40\",\"title\":\"Image quality assessment: from error visibility to structural similarity\",\"url\":\"https://www.semanticscholar.org/paper/eae2e0fa72e898c289365c0af16daf57a7a6cf40\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2004},{\"arxivId\":\"1805.08318\",\"authors\":[{\"authorId\":\"48213346\",\"name\":\"Han Zhang\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"},{\"authorId\":\"2624088\",\"name\":\"Augustus Odena\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a8f3dc53e321fbb2565f5925def4365b9f68d1af\",\"title\":\"Self-Attention Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/a8f3dc53e321fbb2565f5925def4365b9f68d1af\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1705.09368\",\"authors\":[{\"authorId\":\"1847145\",\"name\":\"Liqian Ma\"},{\"authorId\":\"143904396\",\"name\":\"Xu Jia\"},{\"authorId\":\"32222907\",\"name\":\"Qianru Sun\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1704728\",\"name\":\"T. Tuytelaars\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33469aa1130c6bfb838fbb6216f5c42c483a2799\",\"title\":\"Pose Guided Person Image Generation\",\"url\":\"https://www.semanticscholar.org/paper/33469aa1130c6bfb838fbb6216f5c42c483a2799\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1808.07535\",\"authors\":[{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"3084614\",\"name\":\"Xinchen Yan\"},{\"authorId\":\"1564426583\",\"name\":\"Thomas E. Huang\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"160c6e028067c4badfd0c43d44724b87526cc746\",\"title\":\"Learning Hierarchical Semantic Image Manipulation through Structured Representations\",\"url\":\"https://www.semanticscholar.org/paper/160c6e028067c4badfd0c43d44724b87526cc746\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1609.02612\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.13016/M26GIH-TNYZ\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"title\":\"Generating Videos with Scene Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1802.06454\",\"authors\":[{\"authorId\":\"145798572\",\"name\":\"S. Ma\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2018.00593\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"247d40bed85d09e752f60e5183f14b02e100360b\",\"title\":\"DA-GAN: Instance-Level Image Translation by Deep Attention Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/247d40bed85d09e752f60e5183f14b02e100360b\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Erhan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"X 2 face : A network for controlling face generation using images , audio , and pose codes\",\"url\":\"\",\"venue\":\"IEEE Trans . Image Processing\",\"year\":2004},{\"arxivId\":\"1804.04786\",\"authors\":[{\"authorId\":\"115504645\",\"name\":\"Y. Song\"},{\"authorId\":\"145532978\",\"name\":\"Jingwen Zhu\"},{\"authorId\":\"49620929\",\"name\":\"Dawei Li\"},{\"authorId\":\"39635018\",\"name\":\"Xiaolong Wang\"},{\"authorId\":\"144117139\",\"name\":\"Hairong Qi\"}],\"doi\":\"10.24963/ijcai.2019/129\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7c223baccf679fc212dd4d9ecbebf30b7a8616af\",\"title\":\"Talking Face Generation by Conditional Recurrent Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/7c223baccf679fc212dd4d9ecbebf30b7a8616af\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1706.08033\",\"authors\":[{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"10668384\",\"name\":\"Xunyu Lin\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8375ff50b8a6f1a10dd809129a18df96888ac8b\",\"title\":\"Decomposing Motion and Content for Natural Video Sequence Prediction\",\"url\":\"https://www.semanticscholar.org/paper/b8375ff50b8a6f1a10dd809129a18df96888ac8b\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2976930\",\"name\":\"Tero Karras\"},{\"authorId\":\"1761103\",\"name\":\"Timo Aila\"},{\"authorId\":\"36436218\",\"name\":\"S. Laine\"},{\"authorId\":\"3468872\",\"name\":\"Antti Herva\"},{\"authorId\":\"49244945\",\"name\":\"J. Lehtinen\"}],\"doi\":\"10.1145/3072959.3073658\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"title\":\"Audio-driven facial animation by joint end-to-end learning of pose and emotion\",\"url\":\"https://www.semanticscholar.org/paper/95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":\"1710.00962\",\"authors\":[{\"authorId\":\"29673017\",\"name\":\"Xing Di\"},{\"authorId\":\"8809422\",\"name\":\"Vishwanath Sindagi\"},{\"authorId\":\"1741177\",\"name\":\"V. Patel\"}],\"doi\":\"10.1109/ICPR.2018.8545081\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ab14d4a08d1a2c5194870c66719d23bee93adbb\",\"title\":\"GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks\",\"url\":\"https://www.semanticscholar.org/paper/7ab14d4a08d1a2c5194870c66719d23bee93adbb\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":\"1704.05831\",\"authors\":[{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"8299168\",\"name\":\"Y. Zou\"},{\"authorId\":\"2459821\",\"name\":\"Sungryull Sohn\"},{\"authorId\":\"10668384\",\"name\":\"Xunyu Lin\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f230cacc511b17b491bf3d90015bbbf85b9ef6af\",\"title\":\"Learning to Generate Long-term Future via Hierarchical Prediction\",\"url\":\"https://www.semanticscholar.org/paper/f230cacc511b17b491bf3d90015bbbf85b9ef6af\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143967982\",\"name\":\"M. Cooke\"},{\"authorId\":\"32406400\",\"name\":\"J. Barker\"},{\"authorId\":\"75117630\",\"name\":\"S. Cunningham\"},{\"authorId\":\"48914950\",\"name\":\"X. Shao\"}],\"doi\":\"10.1121/1.2229005\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"title\":\"An audio-visual corpus for speech perception and automatic speech recognition.\",\"url\":\"https://www.semanticscholar.org/paper/5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54427-4_19\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"87defac1045bfa9af0162cd248d193e9be6eb25b\",\"title\":\"Out of Time: Automated Lip Sync in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/87defac1045bfa9af0162cd248d193e9be6eb25b\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144598072\",\"name\":\"D. King\"}],\"doi\":\"10.1145/1577069.1755843\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"title\":\"Dlib-ml: A Machine Learning Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1803.07835\",\"authors\":[{\"authorId\":\"144912831\",\"name\":\"Y. Feng\"},{\"authorId\":\"145391146\",\"name\":\"F. Wu\"},{\"authorId\":\"3492237\",\"name\":\"Xiaohu Shao\"},{\"authorId\":\"47905788\",\"name\":\"Y. Wang\"},{\"authorId\":\"39851640\",\"name\":\"X. Zhou\"}],\"doi\":\"10.1007/978-3-030-01264-9_33\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cdb6b1a316995c42bcb90536d6fe17b81cd21b5f\",\"title\":\"Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network\",\"url\":\"https://www.semanticscholar.org/paper/cdb6b1a316995c42bcb90536d6fe17b81cd21b5f\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1807.09251\",\"authors\":[{\"authorId\":\"49107901\",\"name\":\"Albert Pumarola\"},{\"authorId\":\"144003484\",\"name\":\"A. Agudo\"},{\"authorId\":\"145358414\",\"name\":\"A. Mart\\u00ednez\"},{\"authorId\":\"49743313\",\"name\":\"A. Sanfeliu\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"}],\"doi\":\"10.1007/978-3-030-01249-6_50\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80a322fc33c08c9f7a9d4e0b2f417fdb211c76cb\",\"title\":\"GANimation: Anatomically-aware Facial Animation from a Single Image\",\"url\":\"https://www.semanticscholar.org/paper/80a322fc33c08c9f7a9d4e0b2f417fdb211c76cb\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1801.05091\",\"authors\":[{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"35573122\",\"name\":\"Dingdong Yang\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":\"10.1109/CVPR.2018.00833\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc63a155021362b05c3c75bd5040373d72e0623e\",\"title\":\"Inferring Semantic Layout for Hierarchical Text-to-Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/cc63a155021362b05c3c75bd5040373d72e0623e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1706.08612\",\"authors\":[{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2017-950\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0\",\"title\":\"VoxCeleb: A Large-Scale Speaker Identification Dataset\",\"url\":\"https://www.semanticscholar.org/paper/8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39912094\",\"name\":\"P. Garrido\"},{\"authorId\":\"1797649\",\"name\":\"Levi Valgaerts\"},{\"authorId\":\"1401658974\",\"name\":\"Hamid Sarmadi\"},{\"authorId\":\"41178028\",\"name\":\"I. Steiner\"},{\"authorId\":\"13640633\",\"name\":\"K. Varanasi\"},{\"authorId\":\"144565371\",\"name\":\"P. P\\u00e9rez\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":\"10.1111/cgf.12552\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8128fb68dc94c086ba213a28ac132e53eb8da02a\",\"title\":\"VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track\",\"url\":\"https://www.semanticscholar.org/paper/8128fb68dc94c086ba213a28ac132e53eb8da02a\",\"venue\":\"Comput. Graph. Forum\",\"year\":2015},{\"arxivId\":\"1807.10550\",\"authors\":[{\"authorId\":\"8792285\",\"name\":\"Olivia Wiles\"},{\"authorId\":\"32445716\",\"name\":\"A. Koepke\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01261-8_41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ea992f009492888c482d5f4006281eaa8b758e7\",\"title\":\"X2Face: A network for controlling face generation by using images, audio, and pose codes\",\"url\":\"https://www.semanticscholar.org/paper/9ea992f009492888c482d5f4006281eaa8b758e7\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1508.04025\",\"authors\":[{\"authorId\":\"1821711\",\"name\":\"Thang Luong\"},{\"authorId\":\"143950636\",\"name\":\"Hieu Pham\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.18653/v1/D15-1166\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93499a7c7f699b6630a86fad964536f9423bb6d0\",\"title\":\"Effective Approaches to Attention-based Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0\",\"venue\":\"EMNLP\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37016781\",\"name\":\"Supasorn Suwajanakorn\"},{\"authorId\":\"1396612598\",\"name\":\"Steven M. Seitz\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\"}],\"doi\":\"10.1145/3072959.3073640\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"title\":\"Synthesizing Obama\",\"url\":\"https://www.semanticscholar.org/paper/7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3188342\",\"name\":\"Omkar M. Parkhi\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.29.41\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"162ea969d1929ed180cc6de9f0bf116993ff6e06\",\"title\":\"Deep Face Recognition\",\"url\":\"https://www.semanticscholar.org/paper/162ea969d1929ed180cc6de9f0bf116993ff6e06\",\"venue\":\"BMVC\",\"year\":2015},{\"arxivId\":\"1704.08292\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"4037274\",\"name\":\"Sudhanshu Srivastava\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1145/3126686.3126723\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"27e8965cc9c166e9afee46e611039f0ce8263e51\",\"title\":\"Deep Cross-Modal Audio-Visual Generation\",\"url\":\"https://www.semanticscholar.org/paper/27e8965cc9c166e9afee46e611039f0ce8263e51\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Wiles\"},{\"authorId\":null,\"name\":\"A. S. Koepke\"},{\"authorId\":null,\"name\":\"A. Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"X2face: A network for controlling face generation using images\",\"url\":\"\",\"venue\":\"audio, and pose codes. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII, pages 690\\u2013706\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144686633\",\"name\":\"N. Harte\"},{\"authorId\":\"23710772\",\"name\":\"E. Gillen\"}],\"doi\":\"10.1109/TMM.2015.2407694\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6829332d0596659272451920d9ff778b0b400af\",\"title\":\"TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech\",\"url\":\"https://www.semanticscholar.org/paper/d6829332d0596659272451920d9ff778b0b400af\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2015},{\"arxivId\":\"1803.09803\",\"authors\":[{\"authorId\":\"39906330\",\"name\":\"Sefik Emre Eskimez\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"}],\"doi\":\"10.1007/978-3-319-93764-9_35\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5abc6ea4083ff16e8d493828a2477cd4b9cdf1a\",\"title\":\"Generating Talking Face Landmarks from Speech\",\"url\":\"https://www.semanticscholar.org/paper/a5abc6ea4083ff16e8d493828a2477cd4b9cdf1a\",\"venue\":\"LVA/ICA\",\"year\":2018},{\"arxivId\":\"1705.02966\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"2727313\",\"name\":\"A. Jamaludin\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.31.109\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a8632cf6c1ef4319966564328d187876d3bef363\",\"title\":\"You said that?\",\"url\":\"https://www.semanticscholar.org/paper/a8632cf6c1ef4319966564328d187876d3bef363\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":\"1806.04768\",\"authors\":[{\"authorId\":\"50981270\",\"name\":\"Nevan Wichers\"},{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f38b4aa5df2fcdd75815a215fac66e5b38f2fcba\",\"title\":\"Hierarchical Long-term Video Prediction without Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f38b4aa5df2fcdd75815a215fac66e5b38f2fcba\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1708.05980\",\"authors\":[{\"authorId\":\"8268761\",\"name\":\"T. Marwah\"},{\"authorId\":\"47351893\",\"name\":\"G. Mittal\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":\"10.1109/ICCV.2017.159\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1491b98d8e9ccca13bec883f94d935d2dff24053\",\"title\":\"Attentive Semantic Video Generation Using Captions\",\"url\":\"https://www.semanticscholar.org/paper/1491b98d8e9ccca13bec883f94d935d2dff24053\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1807.07860\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"40457380\",\"name\":\"Y. Liu\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33019299\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1816f98e2a4dd54690c2689cf529699d8843e847\",\"title\":\"Talking Face Generation by Adversarially Disentangled Audio-Visual Representation\",\"url\":\"https://www.semanticscholar.org/paper/1816f98e2a4dd54690c2689cf529699d8843e847\",\"venue\":\"AAAI\",\"year\":2019}],\"title\":\"Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss\",\"topics\":[{\"topic\":\"Pixel\",\"topicId\":\"4254\",\"url\":\"https://www.semanticscholar.org/topic/4254\"},{\"topic\":\"Discriminator\",\"topicId\":\"41710\",\"url\":\"https://www.semanticscholar.org/topic/41710\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Internet Information Services\",\"topicId\":\"530982\",\"url\":\"https://www.semanticscholar.org/topic/530982\"},{\"topic\":\"Digital artifact\",\"topicId\":\"375434\",\"url\":\"https://www.semanticscholar.org/topic/375434\"},{\"topic\":\"Level structure\",\"topicId\":\"1350027\",\"url\":\"https://www.semanticscholar.org/topic/1350027\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Relevance\",\"topicId\":\"503\",\"url\":\"https://www.semanticscholar.org/topic/503\"},{\"topic\":\"Facial recognition system\",\"topicId\":\"30847\",\"url\":\"https://www.semanticscholar.org/topic/30847\"},{\"topic\":\"Random neural network\",\"topicId\":\"136146\",\"url\":\"https://www.semanticscholar.org/topic/136146\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"}],\"url\":\"https://www.semanticscholar.org/paper/a0852cd9a026bc90168fa85fa422cb0e48f98394\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019}\n"