"{\"abstract\":\"Audio-driven 3D facial animation has been widely explored, but achieving realistic, human-like performance is still unsolved. This is due to the lack of available 3D datasets, models, and standard evaluation metrics. To address this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans captured at 60 fps and synchronized audio from 12 speakers. We then train a neural network on our dataset that factors identity from facial motion. The learned model, VOCA (Voice Operated Character Animation) takes any speech signal as input\\u2014even speech in languages other than English\\u2014and realistically animates a wide range of adult faces. Conditioning on subject labels during training allows the model to learn a variety of realistic speaking styles. VOCA also provides animator controls to alter speaking style, identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball rotations) during animation. To our knowledge, VOCA is the only realistic 3D facial animation model that is readily applicable to unseen subjects without retargeting. This makes VOCA suitable for tasks like in-game video, virtual reality avatars, or any scenario in which the speaker, speech, or language is not known in advance. We make the dataset and model available for research purposes at http://voca.is.tue.mpg.de.\",\"arxivId\":\"1905.03079\",\"authors\":[{\"authorId\":\"116097067\",\"name\":\"Daniel Cudeiro\",\"url\":\"https://www.semanticscholar.org/author/116097067\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\",\"url\":\"https://www.semanticscholar.org/author/1780750\"},{\"authorId\":\"114339785\",\"name\":\"Cassidy Laidlaw\",\"url\":\"https://www.semanticscholar.org/author/114339785\"},{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\",\"url\":\"https://www.semanticscholar.org/author/1952002\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\",\"url\":\"https://www.semanticscholar.org/author/2105795\"}],\"citationVelocity\":18,\"citations\":[{\"arxivId\":\"2007.07984\",\"authors\":[{\"authorId\":\"8301799\",\"name\":\"Lingyu Zhu\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b2cf1d0538f3ad0d5fae1af4d97571b9291eabc\",\"title\":\"Separating Sounds from a Single Image\",\"url\":\"https://www.semanticscholar.org/paper/7b2cf1d0538f3ad0d5fae1af4d97571b9291eabc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12635912\",\"name\":\"Chang-wei Liang\"},{\"authorId\":\"2444985\",\"name\":\"Xiaosheng Pan\"},{\"authorId\":\"2392258\",\"name\":\"Jiangping Kong\"}],\"doi\":\"10.1145/3429889.3429904\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32f6d85accb87d0560856fe98613f963efccc4d2\",\"title\":\"A Speech-Driven 3-D Lip Synthesis with Realistic Dynamics in Mandarin Chinese\",\"url\":\"https://www.semanticscholar.org/paper/32f6d85accb87d0560856fe98613f963efccc4d2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1910.08462\",\"authors\":[{\"authorId\":\"88741603\",\"name\":\"A. Nivaggioli\"},{\"authorId\":\"2953019\",\"name\":\"D. Rohmer\"}],\"doi\":\"10.1145/3359566.3360067\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03ea37a0d14d28a3b5887ab4450cebeef56d0fa1\",\"title\":\"Animation Synthesis Triggered by Vocal Mimics\",\"url\":\"https://www.semanticscholar.org/paper/03ea37a0d14d28a3b5887ab4450cebeef56d0fa1\",\"venue\":\"MIG\",\"year\":2019},{\"arxivId\":\"1910.00726\",\"authors\":[{\"authorId\":\"47351893\",\"name\":\"G. Mittal\"},{\"authorId\":\"2450889\",\"name\":\"B. Wang\"}],\"doi\":\"10.1109/WACV45572.2020.9093527\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeb47fa4adb457b47f90c018d8048b74ca963cb2\",\"title\":\"Animating Face using Disentangled Audio Representations\",\"url\":\"https://www.semanticscholar.org/paper/eeb47fa4adb457b47f90c018d8048b74ca963cb2\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2004.14569\",\"authors\":[{\"authorId\":\"73329364\",\"name\":\"Jiangning Zhang\"},{\"authorId\":\"1391190989\",\"name\":\"L. Liu\"},{\"authorId\":\"88579203\",\"name\":\"Zhucun Xue\"},{\"authorId\":\"93006732\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1109/ICASSP40776.2020.9052977\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e538538d6b211e23aa85538ce8cf0645eeff43a2\",\"title\":\"APB2FACE: Audio-Guided Face Reenactment with Auxiliary Pose and Blink Signals\",\"url\":\"https://www.semanticscholar.org/paper/e538538d6b211e23aa85538ce8cf0645eeff43a2\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2005482514\",\"name\":\"Aynur Ayratovich Zinnatov\"},{\"authorId\":\"1396998581\",\"name\":\"Vlada V. Kugurakova\"}],\"doi\":\"10.26907/1562-5419-2020-23-5-1011-1025\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5afd0bc53d78cb31b6add3ed9146324c4611c46c\",\"title\":\"Mechanisms of Realistic Facial Expressions for Anthropomorphic Social Agents\",\"url\":\"https://www.semanticscholar.org/paper/5afd0bc53d78cb31b6add3ed9146324c4611c46c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34279376\",\"name\":\"Shohei Iwase\"},{\"authorId\":\"50659426\",\"name\":\"Takuya Kato\"},{\"authorId\":\"3117231\",\"name\":\"Shugo Yamaguchi\"},{\"authorId\":\"2028218761\",\"name\":\"Tsuchiya Yukitaka\"},{\"authorId\":\"1490867805\",\"name\":\"Shigeo Morishima\"}],\"doi\":\"10.1145/3410700.3425435\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f00cc78543a9929ba245d8b8318a7425758ed70\",\"title\":\"Song2Face: Synthesizing Singing Facial Animation from Audio\",\"url\":\"https://www.semanticscholar.org/paper/0f00cc78543a9929ba245d8b8318a7425758ed70\",\"venue\":\"SIGGRAPH 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143790066\",\"name\":\"Dipanjan Das\"},{\"authorId\":\"18961663\",\"name\":\"S. Biswas\"},{\"authorId\":\"8524712\",\"name\":\"Sanjana Sinha\"},{\"authorId\":\"3262263\",\"name\":\"B. Bhowmick\"}],\"doi\":\"10.1007/978-3-030-58577-8_25\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c748ae0688a7ba0b6dbc33436a0fe76fa30786c2\",\"title\":\"Speech-Driven Facial Animation Using Cascaded GANs for Learning of Motion and Texture\",\"url\":\"https://www.semanticscholar.org/paper/c748ae0688a7ba0b6dbc33436a0fe76fa30786c2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2009.00149\",\"authors\":[{\"authorId\":\"1859886299\",\"name\":\"Partha Ghosh\"},{\"authorId\":\"31641185\",\"name\":\"P. S. Gupta\"},{\"authorId\":\"1453675142\",\"name\":\"Roy Uziel\"},{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5a8429c1396fcbdebabf25a0590cc3fde72f319\",\"title\":\"GIF: Generative Interpretable Faces\",\"url\":\"https://www.semanticscholar.org/paper/a5a8429c1396fcbdebabf25a0590cc3fde72f319\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.10004\",\"authors\":[{\"authorId\":\"8190296\",\"name\":\"Jing-ying Liu\"},{\"authorId\":\"151471590\",\"name\":\"B. Hui\"},{\"authorId\":\"1557312538\",\"name\":\"Kun Li\"},{\"authorId\":\"14771941\",\"name\":\"Y. Liu\"},{\"authorId\":\"144891983\",\"name\":\"Yu-Kun Lai\"},{\"authorId\":\"49891126\",\"name\":\"Yuyang Zhang\"},{\"authorId\":\"119924337\",\"name\":\"Y. Liu\"},{\"authorId\":\"145308166\",\"name\":\"Jing-Yu Yang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"26738278bfee3cc9e5b9b82d199ef3bf83e7e56e\",\"title\":\"Geometry-guided Dense Perspective Network for Speech-Driven Facial Animation\",\"url\":\"https://www.semanticscholar.org/paper/26738278bfee3cc9e5b9b82d199ef3bf83e7e56e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.11200\",\"authors\":[{\"authorId\":\"48693082\",\"name\":\"O. Taheri\"},{\"authorId\":\"3843367\",\"name\":\"N. Ghorbani\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"},{\"authorId\":\"1940674\",\"name\":\"Dimitrios Tzionas\"}],\"doi\":\"10.1007/978-3-030-58548-8_34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0808f356856bf8db3f5f24645dadc8e169fd14d2\",\"title\":\"GRAB: A Dataset of Whole-Body Human Grasping of Objects\",\"url\":\"https://www.semanticscholar.org/paper/0808f356856bf8db3f5f24645dadc8e169fd14d2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.08700\",\"authors\":[{\"authorId\":\"50860843\",\"name\":\"Ruobing Zheng\"},{\"authorId\":\"153225505\",\"name\":\"Zhou Zhu\"},{\"authorId\":\"92255644\",\"name\":\"B. Song\"},{\"authorId\":\"150058464\",\"name\":\"Changjiang Ji\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"cf636be537dc13f2e94c5fcde328f34dba70a383\",\"title\":\"Photorealistic Lip Sync with Adversarial Temporal Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/cf636be537dc13f2e94c5fcde328f34dba70a383\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.04012\",\"authors\":[{\"authorId\":\"9196752\",\"name\":\"Yao Feng\"},{\"authorId\":\"2755357\",\"name\":\"Haiwen Feng\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59846afca58769e8e58b10b1ec982344b599aedb\",\"title\":\"Learning an Animatable Detailed 3D Face Model from In-The-Wild Images\",\"url\":\"https://www.semanticscholar.org/paper/59846afca58769e8e58b10b1ec982344b599aedb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Wen\"},{\"authorId\":\"144638120\",\"name\":\"Miao Wang\"},{\"authorId\":\"1819028\",\"name\":\"C. Richardt\"},{\"authorId\":\"1949320248\",\"name\":\"Ze-Yin Chen\"},{\"authorId\":\"153056090\",\"name\":\"Shi-Min Hu\"}],\"doi\":\"10.1109/TVCG.2020.3023573\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bae3842b82ffeacada84b9f02c168d6d0ad90b04\",\"title\":\"Photorealistic Audio-driven Video Portraits\",\"url\":\"https://www.semanticscholar.org/paper/bae3842b82ffeacada84b9f02c168d6d0ad90b04\",\"venue\":\"IEEE Transactions on Visualization and Computer Graphics\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"40454588\",\"name\":\"J. Frahm\"}],\"doi\":\"10.1007/978-3-030-58577-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"title\":\"Computer Vision \\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\\u201328, 2020, Proceedings, Part XXX\",\"url\":\"https://www.semanticscholar.org/paper/08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2006.03028\",\"authors\":[{\"authorId\":\"8301799\",\"name\":\"Lingyu Zhu\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4024fb95ebf35d0edd25123b604d26e832b61a6b\",\"title\":\"Visually Guided Sound Source Separation using Cascaded Opponent Filter Network\",\"url\":\"https://www.semanticscholar.org/paper/4024fb95ebf35d0edd25123b604d26e832b61a6b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.10361\",\"authors\":[{\"authorId\":\"2760845\",\"name\":\"Wolfgang Paier\"},{\"authorId\":\"3353355\",\"name\":\"A. Hilsmann\"},{\"authorId\":\"2516942\",\"name\":\"P. Eisert\"}],\"doi\":\"10.1145/1122445.1122456\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd90fa5ba96a42cc8540eee11c543032fdc0b27c\",\"title\":\"Neural Face Models for Example-Based Visual Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/cd90fa5ba96a42cc8540eee11c543032fdc0b27c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.09805\",\"authors\":[{\"authorId\":\"121927450\",\"name\":\"Rolandos Alexandros Potamias\"},{\"authorId\":\"1491232174\",\"name\":\"Jiali Zheng\"},{\"authorId\":\"2015036\",\"name\":\"Stylianos Ploumpis\"},{\"authorId\":\"7311172\",\"name\":\"Giorgos Bouritsas\"},{\"authorId\":\"31243357\",\"name\":\"Evangelos Ververas\"},{\"authorId\":\"1379747201\",\"name\":\"S. Zafeiriou\"}],\"doi\":\"10.1007/978-3-030-58526-6_17\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5cd4257d58ad2759d8d2d3deefe0b6c6a911a43b\",\"title\":\"Learning to Generate Customized Dynamic 3D Facial Expressions\",\"url\":\"https://www.semanticscholar.org/paper/5cd4257d58ad2759d8d2d3deefe0b6c6a911a43b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"146669538\",\"name\":\"Eleanor Tursman\"},{\"authorId\":\"115448407\",\"name\":\"Marilyn George\"},{\"authorId\":\"144032384\",\"name\":\"S. Kamara\"},{\"authorId\":\"1854493\",\"name\":\"J. Tompkin\"}],\"doi\":\"10.1109/CVPRW50498.2020.00335\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e9991314d96a93e841cb3b6b80a0eb1cae9988b5\",\"title\":\"Towards Untrusted Social Video Verification to Combat Deepfakes via Face Geometry Consistency\",\"url\":\"https://www.semanticscholar.org/paper/e9991314d96a93e841cb3b6b80a0eb1cae9988b5\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50190972\",\"name\":\"Dan Zeng\"},{\"authorId\":\"1753948451\",\"name\":\"Han Liu\"},{\"authorId\":\"46933412\",\"name\":\"H. Lin\"},{\"authorId\":\"39646508\",\"name\":\"Shiming Ge\"}],\"doi\":\"10.1145/3394171.3413844\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79a511eca03dbfd0b48bd876f6bc99ca1690d1cc\",\"title\":\"Talking Face Generation with Expression-Tailored Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/79a511eca03dbfd0b48bd876f6bc99ca1690d1cc\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2760845\",\"name\":\"Wolfgang Paier\"},{\"authorId\":\"1390111049\",\"name\":\"A. Hilsmann\"},{\"authorId\":\"46588404\",\"name\":\"P. Eisert\"}],\"doi\":\"10.1145/3429341.3429356\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a45e54f09a35922064f85d572fa015a5c907eaec\",\"title\":\"Neural Face Models for Example-Based Visual Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/a45e54f09a35922064f85d572fa015a5c907eaec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1403064530\",\"name\":\"Mar Gonz\\u00e1lez-Franco\"},{\"authorId\":\"1491789857\",\"name\":\"Anthony Steed\"},{\"authorId\":\"1499280729\",\"name\":\"Steve Hoogendyk\"},{\"authorId\":\"20592981\",\"name\":\"E. Ofek\"}],\"doi\":\"10.1109/TVCG.2020.2973075\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b10b6a1d0311b9fd5ba0f62cc58bfd6610a83fd3\",\"title\":\"Using Facial Animation to Increase the Enfacement Illusion and Avatar Self-Identification\",\"url\":\"https://www.semanticscholar.org/paper/b10b6a1d0311b9fd5ba0f62cc58bfd6610a83fd3\",\"venue\":\"IEEE Transactions on Visualization and Computer Graphics\",\"year\":2020},{\"arxivId\":\"2011.12999\",\"authors\":[{\"authorId\":\"143778670\",\"name\":\"J. P. Ferreira\"},{\"authorId\":\"2029248405\",\"name\":\"Thiago M. Coutinho\"},{\"authorId\":\"144012660\",\"name\":\"Thiago L. Gomes\"},{\"authorId\":\"35151106\",\"name\":\"J. F. Neto\"},{\"authorId\":\"2599699\",\"name\":\"R. Azevedo\"},{\"authorId\":\"144190194\",\"name\":\"Renato Martins\"},{\"authorId\":\"2310152\",\"name\":\"Erickson R. Nascimento\"}],\"doi\":\"10.1016/j.cag.2020.09.009\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"833b66d4636636b79ef86ed7b27733a88f4e0c3e\",\"title\":\"Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio\",\"url\":\"https://www.semanticscholar.org/paper/833b66d4636636b79ef86ed7b27733a88f4e0c3e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.12992\",\"authors\":[{\"authorId\":\"32025363\",\"name\":\"Yang Zhou\"},{\"authorId\":\"40580714\",\"name\":\"Dingzeyu Li\"},{\"authorId\":\"1399909799\",\"name\":\"Xintong Han\"},{\"authorId\":\"2808670\",\"name\":\"E. Kalogerakis\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"80452718\",\"name\":\"J. Echevarria\"}],\"doi\":\"10.1145/3414685.3417774\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"739497ec657d2c1e6ed7eb424951e0affe117be4\",\"title\":\"MakeItTalk: Speaker-Aware Talking Head Animation\",\"url\":\"https://www.semanticscholar.org/paper/739497ec657d2c1e6ed7eb424951e0affe117be4\",\"venue\":\"ACM Trans. Graph.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202217\",\"name\":\"I. Habibie\"},{\"authorId\":\"2470018\",\"name\":\"WeiPeng Xu\"},{\"authorId\":\"39503308\",\"name\":\"Dushyant Mehta\"},{\"authorId\":\"46458089\",\"name\":\"Lingjie Liu\"},{\"authorId\":\"145361968\",\"name\":\"H. Seidel\"},{\"authorId\":\"1403428213\",\"name\":\"Gerard Pons-Moll\"},{\"authorId\":\"1854465\",\"name\":\"M. Elgharib\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b1afeacdae33129e84406ee2ed635fd81ae6efa7\",\"title\":\"Learning Speech-driven 3D Conversational Gestures from Video\",\"url\":\"https://www.semanticscholar.org/paper/b1afeacdae33129e84406ee2ed635fd81ae6efa7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.13616\",\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"},{\"authorId\":\"115268571\",\"name\":\"Barry-John Theobald\"},{\"authorId\":\"145251024\",\"name\":\"P. Dixon\"},{\"authorId\":\"2497993\",\"name\":\"Reinhard Knothe\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"},{\"authorId\":\"123576773\",\"name\":\"Sachin Kajareker\"}],\"doi\":\"10.1145/3382507.3418840\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"title\":\"Modality Dropout for Improved Performance-driven Talking Faces\",\"url\":\"https://www.semanticscholar.org/paper/917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"}],\"doi\":\"10.15496/PUBLIKATION-4524\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3d7ebfa0199d7d49f4ca95b80fa136aafb43b88\",\"title\":\"Towards Geometric Understanding of Motion\",\"url\":\"https://www.semanticscholar.org/paper/c3d7ebfa0199d7d49f4ca95b80fa136aafb43b88\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152894018\",\"name\":\"C. Stephanidis\"},{\"authorId\":\"145344139\",\"name\":\"Enhua Wu\"},{\"authorId\":\"48825107\",\"name\":\"D. Thalmann\"},{\"authorId\":\"144157386\",\"name\":\"Bin Sheng\"},{\"authorId\":\"46454386\",\"name\":\"Jinman Kim\"},{\"authorId\":\"2896538\",\"name\":\"G. Papagiannakis\"},{\"authorId\":\"1679427\",\"name\":\"M. Gavrilova\"}],\"doi\":\"10.1007/978-3-030-61864-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1810ed8cd63a1f3c68423791aa795891c088ddd\",\"title\":\"Advances in Computer Graphics: 37th Computer Graphics International Conference, CGI 2020, Geneva, Switzerland, October 20\\u201323, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/b1810ed8cd63a1f3c68423791aa795891c088ddd\",\"venue\":\"CGI\",\"year\":2020},{\"arxivId\":\"2010.13017\",\"authors\":[{\"authorId\":\"73329364\",\"name\":\"Jiangning Zhang\"},{\"authorId\":\"22248940\",\"name\":\"X. Zeng\"},{\"authorId\":\"145194966\",\"name\":\"Chao Xu\"},{\"authorId\":\"50762378\",\"name\":\"Jun Chen\"},{\"authorId\":\"93006732\",\"name\":\"Y. Liu\"},{\"authorId\":\"122376816\",\"name\":\"Yunliang Jiang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"61dd5d814c82f2b1a2be416f859b7c426ac35cc2\",\"title\":\"APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment\",\"url\":\"https://www.semanticscholar.org/paper/61dd5d814c82f2b1a2be416f859b7c426ac35cc2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10755743\",\"name\":\"Takayuki Nakatsuka\"},{\"authorId\":\"2844427\",\"name\":\"M. Hamanaka\"},{\"authorId\":\"1490867805\",\"name\":\"Shigeo Morishima\"}],\"doi\":\"10.5220/0008876600270035\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c1de74772253f4b9ebb4ea86b1602fa1445786f\",\"title\":\"Audio-guided Video Interpolation via Human Pose Features\",\"url\":\"https://www.semanticscholar.org/paper/6c1de74772253f4b9ebb4ea86b1602fa1445786f\",\"venue\":\"VISIGRAPP\",\"year\":2020},{\"arxivId\":\"1912.05566\",\"authors\":[{\"authorId\":\"34105638\",\"name\":\"Justus Thies\"},{\"authorId\":\"1854465\",\"name\":\"M. Elgharib\"},{\"authorId\":\"9102722\",\"name\":\"A. Tewari\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"}],\"doi\":\"10.1007/978-3-030-58517-4_42\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b7de44a2edb7570fddbdcb2ee69df9f17adc3396\",\"title\":\"Neural Voice Puppetry: Audio-driven Facial Reenactment\",\"url\":\"https://www.semanticscholar.org/paper/b7de44a2edb7570fddbdcb2ee69df9f17adc3396\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.09367\",\"authors\":[{\"authorId\":\"28903267\",\"name\":\"Elo\\u00efse Berson\"},{\"authorId\":\"1704722\",\"name\":\"Catherine Soladi\\u00e9\"},{\"authorId\":\"3408696\",\"name\":\"Vincent Barrielle\"},{\"authorId\":\"1688878\",\"name\":\"Nicolas Stoiber\"}],\"doi\":\"10.1145/3359566.3360076\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"671312ae3f0507556f86194b4020cca903183476\",\"title\":\"A Robust Interactive Facial Animation Editing System\",\"url\":\"https://www.semanticscholar.org/paper/671312ae3f0507556f86194b4020cca903183476\",\"venue\":\"MIG\",\"year\":2019},{\"arxivId\":\"2008.05023\",\"authors\":[{\"authorId\":\"32774629\",\"name\":\"A. Richard\"},{\"authorId\":\"98658818\",\"name\":\"C. Lea\"},{\"authorId\":\"2863531\",\"name\":\"Shugao Ma\"},{\"authorId\":\"145689714\",\"name\":\"Juergen Gall\"},{\"authorId\":\"143867160\",\"name\":\"F. Torre\"},{\"authorId\":\"1774867\",\"name\":\"Yaser Sheikh\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"66d8edc6dc19550eff7a0ea930cdd431d6ed1c6a\",\"title\":\"Audio- and Gaze-driven Facial Animation of Codec Avatars\",\"url\":\"https://www.semanticscholar.org/paper/66d8edc6dc19550eff7a0ea930cdd431d6ed1c6a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.09888\",\"authors\":[{\"authorId\":\"2824339\",\"name\":\"Patrik Jonell\"},{\"authorId\":\"145372964\",\"name\":\"Taras Kucherenko\"},{\"authorId\":\"2763884\",\"name\":\"G. Henter\"},{\"authorId\":\"1826819\",\"name\":\"J. Beskow\"}],\"doi\":\"10.1145/3383652.3423911\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bcf195fb11c6fb85ee55ccc4f136fad810ac6891\",\"title\":\"Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation of Facial Gestures in Dyadic Settings\",\"url\":\"https://www.semanticscholar.org/paper/bcf195fb11c6fb85ee55ccc4f136fad810ac6891\",\"venue\":\"IVA\",\"year\":2020},{\"arxivId\":\"1909.01815\",\"authors\":[{\"authorId\":\"34460642\",\"name\":\"B. Egger\"},{\"authorId\":\"145242734\",\"name\":\"W. Smith\"},{\"authorId\":\"9102722\",\"name\":\"A. Tewari\"},{\"authorId\":\"1792200\",\"name\":\"Stefanie Wuhrer\"},{\"authorId\":\"1699058\",\"name\":\"M. Zollh\\u00f6fer\"},{\"authorId\":\"2486770\",\"name\":\"T. Beeler\"},{\"authorId\":\"39600032\",\"name\":\"F. Bernard\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"2780587\",\"name\":\"Adam Kortylewski\"},{\"authorId\":\"3293655\",\"name\":\"S. Romdhani\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"},{\"authorId\":\"2880906\",\"name\":\"V. Blanz\"},{\"authorId\":\"152979813\",\"name\":\"T. Vetter\"}],\"doi\":\"10.1145/3395208\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"579fe5fa694cf6b27229fd0f0ce7dc487e0ceb18\",\"title\":\"3D Morphable Face Models\\u2014Past, Present, and Future\",\"url\":\"https://www.semanticscholar.org/paper/579fe5fa694cf6b27229fd0f0ce7dc487e0ceb18\",\"venue\":\"ACM Trans. Graph.\",\"year\":2020}],\"corpusId\":122437581,\"doi\":\"10.1109/CVPR.2019.01034\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"91a1370d26ee903296bb4990a84c23f2fa8e8c83\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Pavlakos\"},{\"authorId\":null,\"name\":\"V. Choutas\"},{\"authorId\":null,\"name\":\"N. Ghorbani\"},{\"authorId\":null,\"name\":\"T. Bolkart\"},{\"authorId\":null,\"name\":\"A.A.A. Osman\"},{\"authorId\":null,\"name\":\"D. Tzionas\"},{\"authorId\":null,\"name\":\"M. J. Black\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Expressive body capture: 3d hands\",\"url\":\"\",\"venue\":\"face, and body from a single image. In Computer Vision and Pattern Recognition\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50290121\",\"name\":\"Tianye Li\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"},{\"authorId\":\"1706574\",\"name\":\"Hao Li\"},{\"authorId\":\"143881914\",\"name\":\"J. Romero\"}],\"doi\":\"10.1145/3130800.3130813\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a84d6c761fbbc66a3d6e0a5ab8a1dcfc944f6753\",\"title\":\"Learning a model of facial shape and expression from 4D scans\",\"url\":\"https://www.semanticscholar.org/paper/a84d6c761fbbc66a3d6e0a5ab8a1dcfc944f6753\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2037061\",\"name\":\"Chuang Ding\"},{\"authorId\":\"144206968\",\"name\":\"L. Xie\"},{\"authorId\":\"145417170\",\"name\":\"P. Zhu\"}],\"doi\":\"10.1007/s11042-014-2156-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e17b12e26864e94f0e353f33f0dd3356570916d4\",\"title\":\"Head motion synthesis from speech using deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/e17b12e26864e94f0e353f33f0dd3356570916d4\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144549270\",\"name\":\"M. Brand\"}],\"doi\":\"10.1145/311535.311537\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2062c4359e57b22789cc38d0a97cc12acb930f43\",\"title\":\"Voice puppetry\",\"url\":\"https://www.semanticscholar.org/paper/2062c4359e57b22789cc38d0a97cc12acb930f43\",\"venue\":\"SIGGRAPH '99\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"143739485\",\"name\":\"Wei Han\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"},{\"authorId\":\"2316043\",\"name\":\"Q. Huo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28f628af0c0521f574831b16b57941d8d75a6db8\",\"title\":\"Text Driven 3D Photo-Realistic Talking Head\",\"url\":\"https://www.semanticscholar.org/paper/28f628af0c0521f574831b16b57941d8d75a6db8\",\"venue\":\"INTERSPEECH\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145789960\",\"name\":\"Ya Chang\"},{\"authorId\":\"3052490\",\"name\":\"M. Vieira\"},{\"authorId\":\"144097660\",\"name\":\"M. Turk\"},{\"authorId\":\"144343218\",\"name\":\"L. Velho\"}],\"doi\":\"10.1007/11564386_23\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2efb8bbbb79bb92757e22c7fdb0bf49318898c9b\",\"title\":\"Automatic 3D Facial Expression Analysis in Videos\",\"url\":\"https://www.semanticscholar.org/paper/2efb8bbbb79bb92757e22c7fdb0bf49318898c9b\",\"venue\":\"AMFG\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2467151\",\"name\":\"John S. Garofolo\"},{\"authorId\":\"145204681\",\"name\":\"L. Lamel\"},{\"authorId\":\"144982775\",\"name\":\"W. Fisher\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"1786370\",\"name\":\"D. Pallett\"},{\"authorId\":\"35669756\",\"name\":\"Nancy L. Dahlgren\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"47128bb3ce4ed00691c0d7d58c02791c3e963ab7\",\"title\":\"Darpa Timit Acoustic-Phonetic Continuous Speech Corpus CD-ROM {TIMIT} | NIST\",\"url\":\"https://www.semanticscholar.org/paper/47128bb3ce4ed00691c0d7d58c02791c3e963ab7\",\"venue\":\"\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. M. Fisher\"},{\"authorId\":null,\"name\":\"G. R. Doddington\"},{\"authorId\":null,\"name\":\"K. M. Goudie- Marshall\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The DARPA speech recognition research database: Specifications and status\",\"url\":\"\",\"venue\":\"DARPA Speech Recognition Workshop\",\"year\":1986},{\"arxivId\":\"1703.07332\",\"authors\":[{\"authorId\":\"145245424\",\"name\":\"Adrian Bulat\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"}],\"doi\":\"10.1109/ICCV.2017.116\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aadf777ef924ac93317550fbdfb9649a10d8aa82\",\"title\":\"How Far are We from Solving the 2D & 3D Face Alignment Problem? (and a Dataset of 230,000 3D Facial Landmarks)\",\"url\":\"https://www.semanticscholar.org/paper/aadf777ef924ac93317550fbdfb9649a10d8aa82\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738798\",\"name\":\"H. Hermansky\"},{\"authorId\":\"144798098\",\"name\":\"N. Morgan\"}],\"doi\":\"10.1109/89.326616\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2b439b063874df0a074449c7c8616ac0880c9c5\",\"title\":\"RASTA processing of speech\",\"url\":\"https://www.semanticscholar.org/paper/a2b439b063874df0a074449c7c8616ac0880c9c5\",\"venue\":\"IEEE Trans. Speech Audio Process.\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Lopyrev\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Expressive body capture : 3 d hands , face , and body from a single image Speech - driven 3 D facial animation with implicit emotional awareness : A deep learning approach\",\"url\":\"\",\"venue\":\"Conference on Computer Vision and Pattern Recognition Workshop\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2428034\",\"name\":\"C. Bregler\"},{\"authorId\":\"1800748\",\"name\":\"M. Covell\"},{\"authorId\":\"145290352\",\"name\":\"M. Slaney\"}],\"doi\":\"10.1145/258734.258880\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a78995510cf33edf0ee4265abe23ffdc55986cb\",\"title\":\"Video Rewrite: driving visual speech with audio\",\"url\":\"https://www.semanticscholar.org/paper/3a78995510cf33edf0ee4265abe23ffdc55986cb\",\"venue\":\"SIGGRAPH '97\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2875539\",\"name\":\"O. Alexander\"},{\"authorId\":\"145448025\",\"name\":\"Mike Rogers\"},{\"authorId\":\"2938800\",\"name\":\"W. Lambeth\"},{\"authorId\":\"31518750\",\"name\":\"M. Chiang\"},{\"authorId\":\"1778676\",\"name\":\"P. Debevec\"}],\"doi\":\"10.1145/1667239.1667251\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a00cd40ca44289b401371b335b5d941156aa5466\",\"title\":\"The Digital Emily project: photoreal facial modeling and animation\",\"url\":\"https://www.semanticscholar.org/paper/a00cd40ca44289b401371b335b5d941156aa5466\",\"venue\":\"SIGGRAPH 2009\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1932050\",\"name\":\"T. Ezzat\"}],\"doi\":\"10.1145/566570.566594\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b989c8d332ce16f35e9edf90a1194373f046fee\",\"title\":\"Trainable videorealistic speech animation\",\"url\":\"https://www.semanticscholar.org/paper/8b989c8d332ce16f35e9edf90a1194373f046fee\",\"venue\":\"Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Taylor\"},{\"authorId\":null,\"name\":\"A. Kato\"},{\"authorId\":null,\"name\":\"B. Milner\"},{\"authorId\":null,\"name\":\"I. Matthews\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Audio-tovisual speech conversion using deep neural networks. 2016\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2106794\",\"name\":\"C. Busso\"},{\"authorId\":\"145140508\",\"name\":\"Zhigang Deng\"},{\"authorId\":\"144881634\",\"name\":\"M. Grimm\"},{\"authorId\":\"143840663\",\"name\":\"U. Neumann\"},{\"authorId\":\"145254843\",\"name\":\"Shrikanth S. Narayanan\"}],\"doi\":\"10.1109/TASL.2006.885910\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f5ee27a63b3df7bf86ff9dc9b1ddb353589826a\",\"title\":\"Rigid Head Motion in Expressive Speech Animation: Analysis and Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/6f5ee27a63b3df7bf86ff9dc9b1ddb353589826a\",\"venue\":\"IEEE Transactions on Audio, Speech, and Language Processing\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144039941\",\"name\":\"K. Dale\"},{\"authorId\":\"2454127\",\"name\":\"Kalyan Sunkavalli\"},{\"authorId\":\"13594727\",\"name\":\"Micah K. Johnson\"},{\"authorId\":\"1880628\",\"name\":\"D. Vlasic\"},{\"authorId\":\"1752521\",\"name\":\"W. Matusik\"},{\"authorId\":\"143758236\",\"name\":\"H. Pfister\"}],\"doi\":\"10.1145/2024156.2024164\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f326fd805ae74f3773c5eda789aee890b59cc1fb\",\"title\":\"Video face replacement\",\"url\":\"https://www.semanticscholar.org/paper/f326fd805ae74f3773c5eda789aee890b59cc1fb\",\"venue\":\"SA '11\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zheng Zhang\"},{\"authorId\":\"36185909\",\"name\":\"J. M. Girard\"},{\"authorId\":\"144319631\",\"name\":\"Y. Wu\"},{\"authorId\":\"46447561\",\"name\":\"X. Zhang\"},{\"authorId\":\"145779142\",\"name\":\"Peng Liu\"},{\"authorId\":\"9116364\",\"name\":\"Umur A. Ciftci\"},{\"authorId\":\"3145132\",\"name\":\"Shaun J. Canavan\"},{\"authorId\":\"144966578\",\"name\":\"M. Reale\"},{\"authorId\":\"46319646\",\"name\":\"Andrew Horowitz\"},{\"authorId\":\"2671017\",\"name\":\"Huiyuan Yang\"},{\"authorId\":\"1737918\",\"name\":\"J. Cohn\"},{\"authorId\":\"50426357\",\"name\":\"Q. Ji\"},{\"authorId\":\"143736991\",\"name\":\"L. Yin\"}],\"doi\":\"10.1109/CVPR.2016.374\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5f268d58d574b775d04bf137c380141a512a11a\",\"title\":\"Multimodal Spontaneous Emotion Corpus for Human Behavior Analysis\",\"url\":\"https://www.semanticscholar.org/paper/d5f268d58d574b775d04bf137c380141a512a11a\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47562869\",\"name\":\"Taiki Shimba\"},{\"authorId\":\"2616395\",\"name\":\"R. Sakurai\"},{\"authorId\":\"2108938\",\"name\":\"H. Yamazoe\"},{\"authorId\":\"4165428\",\"name\":\"J. Lee\"}],\"doi\":\"10.1109/SII.2015.7404961\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3205f8a9c046e6d504f628bbfabc01af94105a32\",\"title\":\"Talking heads synthesis from audio with deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/3205f8a9c046e6d504f628bbfabc01af94105a32\",\"venue\":\"2015 IEEE/SICE International Symposium on System Integration (SII)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36005450\",\"name\":\"Chen Cao\"},{\"authorId\":\"7939453\",\"name\":\"Q. Hou\"},{\"authorId\":\"144078074\",\"name\":\"K. Zhou\"}],\"doi\":\"10.1145/2601097.2601204\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"75ed53365aace52cae1128aacc84b51536fe10ca\",\"title\":\"Displaced dynamic expression regression for real-time facial tracking and animation\",\"url\":\"https://www.semanticscholar.org/paper/75ed53365aace52cae1128aacc84b51536fe10ca\",\"venue\":\"ACM Trans. Graph.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Mori\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bukimi no tani [the uncanny valley\",\"url\":\"\",\"venue\":\"Energy, 7:33\\u2013 35\",\"year\":1970},{\"arxivId\":\"1606.05250\",\"authors\":[{\"authorId\":\"2706258\",\"name\":\"Pranav Rajpurkar\"},{\"authorId\":null,\"name\":\"Jian Zhang\"},{\"authorId\":\"2787620\",\"name\":\"Konstantin Lopyrev\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"}],\"doi\":\"10.18653/v1/D16-1264\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"05dd7254b632376973f3a1b4d39485da17814df5\",\"title\":\"SQuAD: 100, 000+ Questions for Machine Comprehension of Text\",\"url\":\"https://www.semanticscholar.org/paper/05dd7254b632376973f3a1b4d39485da17814df5\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143773131\",\"name\":\"C. Cao\"},{\"authorId\":\"143663883\",\"name\":\"Y. Weng\"},{\"authorId\":\"14547116\",\"name\":\"S. Zhou\"},{\"authorId\":\"3225345\",\"name\":\"Y. Tong\"},{\"authorId\":\"48918534\",\"name\":\"K. Zhou\"}],\"doi\":\"10.1109/TVCG.2013.249\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"df3a207258f3febb98d3dcaf890e8a0f09cd5c12\",\"title\":\"FaceWarehouse: A 3D Facial Expression Database for Visual Computing\",\"url\":\"https://www.semanticscholar.org/paper/df3a207258f3febb98d3dcaf890e8a0f09cd5c12\",\"venue\":\"IEEE Transactions on Visualization and Computer Graphics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2935689\",\"name\":\"Yilong Liu\"},{\"authorId\":\"143979425\",\"name\":\"F. Xu\"},{\"authorId\":\"1759700\",\"name\":\"Jinxiang Chai\"},{\"authorId\":\"49144235\",\"name\":\"X. Tong\"},{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"2316043\",\"name\":\"Q. Huo\"}],\"doi\":\"10.1145/2816795.2818122\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e656deb595c384026d901f3c23b4e9dcafb02182\",\"title\":\"Video-audio driven real-time facial animation\",\"url\":\"https://www.semanticscholar.org/paper/e656deb595c384026d901f3c23b4e9dcafb02182\",\"venue\":\"ACM Trans. Graph.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143636342\",\"name\":\"Sarah L. Taylor\"},{\"authorId\":\"30303590\",\"name\":\"M. Mahler\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.2312/SCA/SCA12/275-284\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"910bb564f42f298898c1831e6650b1f2efa07b42\",\"title\":\"Dynamic units of visual speech\",\"url\":\"https://www.semanticscholar.org/paper/910bb564f42f298898c1831e6650b1f2efa07b42\",\"venue\":\"SCA '12\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2246174\",\"name\":\"T. Weise\"},{\"authorId\":\"35119991\",\"name\":\"Sofien Bouaziz\"},{\"authorId\":\"144966719\",\"name\":\"Hao Li\"},{\"authorId\":\"143674021\",\"name\":\"M. Pauly\"}],\"doi\":\"10.1145/1964921.1964972\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"177e876c9e0f1ade353b4744dcee7754d6411837\",\"title\":\"Realtime performance-based facial animation\",\"url\":\"https://www.semanticscholar.org/paper/177e876c9e0f1ade353b4744dcee7754d6411837\",\"venue\":\"SIGGRAPH '11\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33846296\",\"name\":\"Chenglei Wu\"},{\"authorId\":\"143929823\",\"name\":\"D. Bradley\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"},{\"authorId\":\"2486770\",\"name\":\"T. Beeler\"}],\"doi\":\"10.1145/2897824.2925882\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9fd8b6a575bfe01d2003c62f3254ff907544ae2\",\"title\":\"An anatomically-constrained local deformation model for monocular face capture\",\"url\":\"https://www.semanticscholar.org/paper/b9fd8b6a575bfe01d2003c62f3254ff907544ae2\",\"venue\":\"ACM Trans. Graph.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144206968\",\"name\":\"L. Xie\"},{\"authorId\":\"46270996\",\"name\":\"Z. Liu\"}],\"doi\":\"10.1109/TMM.2006.888009\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bafc2727929bc1f421d3b74ac2f7da4279c6e04e\",\"title\":\"Realistic Mouth-Synching for Speech-Driven Talking Face Using Articulatory Modelling\",\"url\":\"https://www.semanticscholar.org/paper/bafc2727929bc1f421d3b74ac2f7da4279c6e04e\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3069144\",\"name\":\"H. X. Pham\"},{\"authorId\":\"37782042\",\"name\":\"Samuel Cheung\"},{\"authorId\":\"144658464\",\"name\":\"V. Pavlovic\"}],\"doi\":\"10.1109/CVPRW.2017.287\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cb7cf162fb44ef06abd6aa30026c99ded8cbcdf8\",\"title\":\"Speech-Driven 3D Facial Animation with Implicit Emotional Awareness: A Deep Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/cb7cf162fb44ef06abd6aa30026c99ded8cbcdf8\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11378252\",\"name\":\"Xinjian Zhang\"},{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"38387695\",\"name\":\"G. Li\"},{\"authorId\":\"1745715\",\"name\":\"F. Seide\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ec3dcb42ef35cb552bc6bfe2d83ca854c0ed14a7\",\"title\":\"A new language independent, photo-realistic talking head driven by voice only\",\"url\":\"https://www.semanticscholar.org/paper/ec3dcb42ef35cb552bc6bfe2d83ca854c0ed14a7\",\"venue\":\"INTERSPEECH\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2976930\",\"name\":\"Tero Karras\"},{\"authorId\":\"1761103\",\"name\":\"Timo Aila\"},{\"authorId\":\"36436218\",\"name\":\"S. Laine\"},{\"authorId\":\"3468872\",\"name\":\"Antti Herva\"},{\"authorId\":\"49244945\",\"name\":\"J. Lehtinen\"}],\"doi\":\"10.1145/3072959.3073658\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"title\":\"Audio-driven facial animation by joint end-to-end learning of pose and emotion\",\"url\":\"https://www.semanticscholar.org/paper/95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1792288\",\"name\":\"D. Cosker\"},{\"authorId\":\"2035177\",\"name\":\"Eva Krumhuber\"},{\"authorId\":\"144046599\",\"name\":\"A. Hilton\"}],\"doi\":\"10.1109/ICCV.2011.6126510\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"79b46da995fb2d941ba32f571fab4b83f87ac6fc\",\"title\":\"A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling\",\"url\":\"https://www.semanticscholar.org/paper/79b46da995fb2d941ba32f571fab4b83f87ac6fc\",\"venue\":\"2011 International Conference on Computer Vision\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145803757\",\"name\":\"R. Anderson\"},{\"authorId\":\"144746940\",\"name\":\"B. Stenger\"},{\"authorId\":\"2196928\",\"name\":\"V. Wan\"},{\"authorId\":\"1745672\",\"name\":\"R. Cipolla\"}],\"doi\":\"10.1109/CVPR.2013.434\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d8de70f089b581d386aaa0ced5f9421a3fd08c0\",\"title\":\"Expressive Visual Text-to-Speech Using Active Appearance Models\",\"url\":\"https://www.semanticscholar.org/paper/2d8de70f089b581d386aaa0ced5f9421a3fd08c0\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":\"1609.06536\",\"authors\":[{\"authorId\":\"36436218\",\"name\":\"S. Laine\"},{\"authorId\":\"2976930\",\"name\":\"Tero Karras\"},{\"authorId\":\"1761103\",\"name\":\"Timo Aila\"},{\"authorId\":\"3468872\",\"name\":\"Antti Herva\"},{\"authorId\":\"2059597\",\"name\":\"S. Saito\"},{\"authorId\":\"9965153\",\"name\":\"Ronald Yu\"},{\"authorId\":\"1706574\",\"name\":\"Hao Li\"},{\"authorId\":\"49244945\",\"name\":\"J. Lehtinen\"}],\"doi\":\"10.1145/3099564.3099581\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e519ec5b979330a77db97b0605d67b229ec2ddf\",\"title\":\"Production-level facial performance capture using deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/0e519ec5b979330a77db97b0605d67b229ec2ddf\",\"venue\":\"Symposium on Computer Animation\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143773131\",\"name\":\"C. Cao\"},{\"authorId\":\"144863596\",\"name\":\"D. Bradley\"},{\"authorId\":\"48918534\",\"name\":\"K. Zhou\"},{\"authorId\":\"2486770\",\"name\":\"T. Beeler\"}],\"doi\":\"10.1145/2766943\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd671b6dc3394682048a7053bed20eff9691414c\",\"title\":\"Real-time high-fidelity facial performance capture\",\"url\":\"https://www.semanticscholar.org/paper/bd671b6dc3394682048a7053bed20eff9691414c\",\"venue\":\"ACM Trans. Graph.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Hong\"},{\"authorId\":null,\"name\":\"Z. Wen\"},{\"authorId\":null,\"name\":\"T. S. Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Real-time speechdriven face animation with expressions using neural networks\",\"url\":\"\",\"venue\":\"Transactions on Neural Networks, 13(4):916\\u2013927\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38655449\",\"name\":\"G. Salvi\"},{\"authorId\":\"1826819\",\"name\":\"J. Beskow\"},{\"authorId\":\"32201536\",\"name\":\"S. Moubayed\"},{\"authorId\":\"144966453\",\"name\":\"B. Granstr\\u00f6m\"}],\"doi\":\"10.1155/2009/191940\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef9aa7df9be36af6d1d00a093e6f456a9d80d601\",\"title\":\"SynFace\\u2014Speech-Driven Facial Animation for Virtual Speech-Reading Support\",\"url\":\"https://www.semanticscholar.org/paper/ef9aa7df9be36af6d1d00a093e6f456a9d80d601\",\"venue\":\"EURASIP J. Audio Speech Music. Process.\",\"year\":2009},{\"arxivId\":\"1609.03499\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"df0402517a7338ae28bc54acaac400de6b456a46\",\"title\":\"WaveNet: A Generative Model for Raw Audio\",\"url\":\"https://www.semanticscholar.org/paper/df0402517a7338ae28bc54acaac400de6b456a46\",\"venue\":\"SSW\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49469701\",\"name\":\"X. Zhang\"},{\"authorId\":\"143736991\",\"name\":\"L. Yin\"},{\"authorId\":\"1737918\",\"name\":\"J. Cohn\"},{\"authorId\":\"3145132\",\"name\":\"Shaun J. Canavan\"},{\"authorId\":\"144966578\",\"name\":\"M. Reale\"},{\"authorId\":\"34311917\",\"name\":\"Andy Horowitz\"},{\"authorId\":\"145779142\",\"name\":\"Peng Liu\"},{\"authorId\":\"36185909\",\"name\":\"J. M. Girard\"}],\"doi\":\"10.1016/j.imavis.2014.06.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ba7180cab691fdbeca176ee18ac0bccd75d5f0b1\",\"title\":\"BP4D-Spontaneous: a high-resolution spontaneous 3D dynamic facial expression database\",\"url\":\"https://www.semanticscholar.org/paper/ba7180cab691fdbeca176ee18ac0bccd75d5f0b1\",\"venue\":\"Image Vis. Comput.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2409071\",\"name\":\"Taleb Alashkar\"},{\"authorId\":\"2125606\",\"name\":\"B. B. Amor\"},{\"authorId\":\"2909056\",\"name\":\"M. Daoudi\"},{\"authorId\":\"2507859\",\"name\":\"S. Berretti\"}],\"doi\":\"10.15221/14.357\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d4bb462c9f1d4e4ab1e4aa6a75cc0bc71b38461\",\"title\":\"A 3D Dynamic Database for Unconstrained Face Recognition\",\"url\":\"https://www.semanticscholar.org/paper/4d4bb462c9f1d4e4ab1e4aa6a75cc0bc71b38461\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145722155\",\"name\":\"Bo Fan\"},{\"authorId\":\"144206962\",\"name\":\"Lei Xie\"},{\"authorId\":\"144947353\",\"name\":\"S. Yang\"},{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"}],\"doi\":\"10.1007/s11042-015-2944-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ced771ffdb701592f49aee69aa61c45b790c9889\",\"title\":\"A deep bidirectional LSTM approach for video-realistic talking head\",\"url\":\"https://www.semanticscholar.org/paper/ced771ffdb701592f49aee69aa61c45b790c9889\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37016781\",\"name\":\"Supasorn Suwajanakorn\"},{\"authorId\":\"1396612598\",\"name\":\"Steven M. Seitz\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\"}],\"doi\":\"10.1145/3072959.3073640\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"title\":\"Synthesizing Obama\",\"url\":\"https://www.semanticscholar.org/paper/7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":\"1805.09488\",\"authors\":[{\"authorId\":\"1825026\",\"name\":\"Yang Zhou\"},{\"authorId\":\"145841883\",\"name\":\"Shan Xu\"},{\"authorId\":\"2483922\",\"name\":\"C. Landreth\"},{\"authorId\":\"2808670\",\"name\":\"E. Kalogerakis\"},{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"},{\"authorId\":\"144319838\",\"name\":\"K. Singh\"}],\"doi\":\"10.1145/3197517.3201292\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"846372d7181279e93860d54ae3b665ae0ef19d08\",\"title\":\"VisemeNet: Audio-Driven Animator-Centric Speech Animation\",\"url\":\"https://www.semanticscholar.org/paper/846372d7181279e93860d54ae3b665ae0ef19d08\",\"venue\":\"ACM Trans. Graph.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"X. Zhang\"},{\"authorId\":null,\"name\":\"L. Wang\"},{\"authorId\":null,\"name\":\"G. Li\"},{\"authorId\":null,\"name\":\"F. Seide\"},{\"authorId\":null,\"name\":\"F. K. Soong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A new language independent\",\"url\":\"\",\"venue\":\"photo-realistic talking head driven by voice only. In INTERSPEECH, pages 2743\\u20132747\",\"year\":2013},{\"arxivId\":\"1603.04467\",\"authors\":[{\"authorId\":\"145832079\",\"name\":\"M. Abadi\"},{\"authorId\":\"145984138\",\"name\":\"A. Agarwal\"},{\"authorId\":\"144758007\",\"name\":\"P. Barham\"},{\"authorId\":\"2445241\",\"name\":\"E. Brevdo\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"48738717\",\"name\":\"Craig Citro\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"36347083\",\"name\":\"Andy Davis\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"145139947\",\"name\":\"M. Devin\"},{\"authorId\":\"1780892\",\"name\":\"Sanjay Ghemawat\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"3384453\",\"name\":\"A. Harp\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"2090818\",\"name\":\"M. Isard\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1944541\",\"name\":\"R. J\\u00f3zefowicz\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1942300\",\"name\":\"M. Kudlur\"},{\"authorId\":\"3369421\",\"name\":\"Josh Levenberg\"},{\"authorId\":\"143767989\",\"name\":\"Dan Man\\u00e9\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"144375552\",\"name\":\"Sherry Moore\"},{\"authorId\":\"20154699\",\"name\":\"D. Murray\"},{\"authorId\":\"153301219\",\"name\":\"Chris Olah\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"32163737\",\"name\":\"B. Steiner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"35210462\",\"name\":\"Kunal Talwar\"},{\"authorId\":\"2080690\",\"name\":\"P. Tucker\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"38062095\",\"name\":\"V. Vasudevan\"},{\"authorId\":\"1765169\",\"name\":\"F. Vi\\u00e9gas\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"47941411\",\"name\":\"Pete Warden\"},{\"authorId\":\"145233583\",\"name\":\"M. Wattenberg\"},{\"authorId\":\"35078078\",\"name\":\"Martin Wicke\"},{\"authorId\":\"47112093\",\"name\":\"Y. Yu\"},{\"authorId\":\"2777763\",\"name\":\"X. Zheng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"title\":\"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\",\"url\":\"https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1412.5567\",\"authors\":[{\"authorId\":\"2893056\",\"name\":\"Awni Y. Hannun\"},{\"authorId\":\"145353944\",\"name\":\"C. Case\"},{\"authorId\":\"48991386\",\"name\":\"J. Casper\"},{\"authorId\":\"2301680\",\"name\":\"Bryan Catanzaro\"},{\"authorId\":\"2322582\",\"name\":\"Greg Diamos\"},{\"authorId\":\"152585800\",\"name\":\"E. Elsen\"},{\"authorId\":\"3168041\",\"name\":\"Ryan Prenger\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"2264597\",\"name\":\"S. Sengupta\"},{\"authorId\":\"144638694\",\"name\":\"A. Coates\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"24741d280869ad9c60321f5ab6e5f01b7852507d\",\"title\":\"Deep Speech: Scaling up end-to-end speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/24741d280869ad9c60321f5ab6e5f01b7852507d\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97712036\",\"name\":\"P. Kakumanu\"},{\"authorId\":\"1389225550\",\"name\":\"R. Gutierrez-Osuna\"},{\"authorId\":\"144658624\",\"name\":\"A. Esposito\"},{\"authorId\":\"2387793\",\"name\":\"R. Bryll\"},{\"authorId\":\"144685422\",\"name\":\"A. Goshtasby\"},{\"authorId\":\"3024925\",\"name\":\"O. N. Garcia\"}],\"doi\":\"10.1145/971478.971488\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"018e2a90ef0f2861db2b3e0f59def03abd02f63b\",\"title\":\"Speech driven facial animation\",\"url\":\"https://www.semanticscholar.org/paper/018e2a90ef0f2861db2b3e0f59def03abd02f63b\",\"venue\":\"PUI '01\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145722155\",\"name\":\"Bo Fan\"},{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"},{\"authorId\":\"144206968\",\"name\":\"L. Xie\"}],\"doi\":\"10.1109/ICASSP.2015.7178899\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c028b86f5551757becd4c4304bddebb49e880b3\",\"title\":\"Photo-real talking head with deep bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/8c028b86f5551757becd4c4304bddebb49e880b3\",\"venue\":\"2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2015},{\"arxivId\":\"1803.10404\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"48458657\",\"name\":\"Zhiheng Li\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01234-2_32\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"title\":\"Lip Movements Generation at a Glance\",\"url\":\"https://www.semanticscholar.org/paper/d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1804.03619\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"2138834\",\"name\":\"Inbar Mosseri\"},{\"authorId\":\"49618488\",\"name\":\"Oran Lang\"},{\"authorId\":\"2112779\",\"name\":\"Tali Dekel\"},{\"authorId\":\"118291142\",\"name\":\"K. Wilson\"},{\"authorId\":\"1639722387\",\"name\":\"Avinatan Hassidim\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"144544291\",\"name\":\"Michael Rubinstein\"}],\"doi\":\"10.1145/3197517.3201357\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b6add50e6be8d4f21e38cca9a154321cad3a4e0\",\"title\":\"Looking to listen at the cocktail party\",\"url\":\"https://www.semanticscholar.org/paper/1b6add50e6be8d4f21e38cca9a154321cad3a4e0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. C. Tien\"},{\"authorId\":null,\"name\":\"P. Faloutsos\"},{\"authorId\":null,\"name\":\"F. Pighin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Faceware - house : A 3 D facial expression database for visual comput\",\"url\":\"\",\"venue\":\"Transactions on Visualization and Computer Graphics\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. S. Garofolo\"},{\"authorId\":null,\"name\":\"L. F. Lamel\"},{\"authorId\":null,\"name\":\"W. M. Fisher\"},{\"authorId\":null,\"name\":\"J. G. Fiscus\"},{\"authorId\":null,\"name\":\"D. S. Pallett\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and N\",\"url\":\"\",\"venue\":\"L. Dahlgren. Darpa timit acoustic phonetic continuous speech corpus cdrom\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37945166\",\"name\":\"S. Sako\"},{\"authorId\":\"1723069\",\"name\":\"K. Tokuda\"},{\"authorId\":\"2882083\",\"name\":\"T. Masuko\"},{\"authorId\":\"48648898\",\"name\":\"T. Kobayashi\"},{\"authorId\":\"1788590\",\"name\":\"T. Kitamura\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26b64165589ef64317972ca4db0db25c63fa7475\",\"title\":\"HMM-based text-to-audio-visual speech synthesis\",\"url\":\"https://www.semanticscholar.org/paper/26b64165589ef64317972ca4db0db25c63fa7475\",\"venue\":\"INTERSPEECH\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143736991\",\"name\":\"L. Yin\"},{\"authorId\":\"46772708\",\"name\":\"Xiaochen Chen\"},{\"authorId\":\"1681656\",\"name\":\"Y. Sun\"},{\"authorId\":\"153615744\",\"name\":\"T. Worm\"},{\"authorId\":\"144966578\",\"name\":\"M. Reale\"}],\"doi\":\"10.1109/AFGR.2008.4813324\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd6c52fe253f972a54102f43f7d6ee9827eeafd0\",\"title\":\"A high-resolution 3D dynamic facial expression database\",\"url\":\"https://www.semanticscholar.org/paper/fd6c52fe253f972a54102f43f7d6ee9827eeafd0\",\"venue\":\"2008 8th IEEE International Conference on Automatic Face & Gesture Recognition\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143636342\",\"name\":\"Sarah L. Taylor\"},{\"authorId\":\"2066626\",\"name\":\"T. Kim\"},{\"authorId\":\"1740159\",\"name\":\"Yisong Yue\"},{\"authorId\":\"30303590\",\"name\":\"M. Mahler\"},{\"authorId\":\"1988242\",\"name\":\"James Krahe\"},{\"authorId\":\"36969558\",\"name\":\"Anastasio Garcia Rodriguez\"},{\"authorId\":\"1788773\",\"name\":\"J. Hodgins\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.1145/3072959.3073699\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"title\":\"A deep learning approach for generalized speech animation\",\"url\":\"https://www.semanticscholar.org/paper/cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"1792200\",\"name\":\"Stefanie Wuhrer\"}],\"doi\":\"10.1109/ICCV.2015.411\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05b64ba1d7082365ef68f48da9243f9af0237f1f\",\"title\":\"A Groupwise Multilinear Correspondence Optimization for 3D Faces\",\"url\":\"https://www.semanticscholar.org/paper/05b64ba1d7082365ef68f48da9243f9af0237f1f\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"2007.14808\",\"authors\":[{\"authorId\":\"34105638\",\"name\":\"Justus Thies\"},{\"authorId\":\"1699058\",\"name\":\"M. Zollh\\u00f6fer\"},{\"authorId\":\"144140066\",\"name\":\"M. Stamminger\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"}],\"doi\":\"10.1145/3292039\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd52d94dd51b131519bbf22641f334e72e5ad3da\",\"title\":\"Face2Face: real-time face capture and reenactment of RGB videos\",\"url\":\"https://www.semanticscholar.org/paper/bd52d94dd51b131519bbf22641f334e72e5ad3da\",\"venue\":\"Commun. ACM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1839621\",\"name\":\"A. Savran\"},{\"authorId\":\"1692892\",\"name\":\"N. Aly\\u00fcz\"},{\"authorId\":\"1723272\",\"name\":\"H. Dibeklioglu\"},{\"authorId\":\"1717313\",\"name\":\"Oya Celiktutan\"},{\"authorId\":\"1743634\",\"name\":\"B. G\\u00f6kberk\"},{\"authorId\":\"145940271\",\"name\":\"B. Sankur\"},{\"authorId\":\"1694599\",\"name\":\"L. Akarun\"}],\"doi\":\"10.1007/978-3-540-89991-4_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2acf7e58f0a526b957be2099c10aab693f795973\",\"title\":\"Bosphorus Database for 3D Face Analysis\",\"url\":\"https://www.semanticscholar.org/paper/2acf7e58f0a526b957be2099c10aab693f795973\",\"venue\":\"BIOID\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46319694\",\"name\":\"F. Yang\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"36541522\",\"name\":\"J. Wang\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1109/CVPR.2012.6247759\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53ba88d5d5355d070361d594c68476754099128d\",\"title\":\"Facial expression editing in video using a temporally-smooth factorization\",\"url\":\"https://www.semanticscholar.org/paper/53ba88d5d5355d070361d594c68476754099128d\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Faloutsos\"},{\"authorId\":null,\"name\":\"F. Pighin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Faceware - house : A 3 D facial expression database for visual comput\",\"url\":\"\",\"venue\":\"Transactions on Visualization and Computer Graphics\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143736991\",\"name\":\"L. Yin\"},{\"authorId\":\"1808842\",\"name\":\"Xiaozhou Wei\"},{\"authorId\":\"1681656\",\"name\":\"Y. Sun\"},{\"authorId\":null,\"name\":\"Jun Wang\"},{\"authorId\":\"32512499\",\"name\":\"M. Rosato\"}],\"doi\":\"10.1109/FGR.2006.6\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc589c499dcf323fe4a143bbef0074c3e31f9b60\",\"title\":\"A 3D facial expression database for facial behavior research\",\"url\":\"https://www.semanticscholar.org/paper/cc589c499dcf323fe4a143bbef0074c3e31f9b60\",\"venue\":\"7th International Conference on Automatic Face and Gesture Recognition (FGR06)\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144966715\",\"name\":\"H. Li\"},{\"authorId\":\"2246174\",\"name\":\"T. Weise\"},{\"authorId\":\"143674021\",\"name\":\"M. Pauly\"}],\"doi\":\"10.1145/1833349.1778769\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7bf1bfc569978310b676b2b0652e0f5447d9d667\",\"title\":\"Example-based facial rigging\",\"url\":\"https://www.semanticscholar.org/paper/7bf1bfc569978310b676b2b0652e0f5447d9d667\",\"venue\":\"SIGGRAPH '10\",\"year\":2010},{\"arxivId\":\"1904.05866\",\"authors\":[{\"authorId\":\"2829330\",\"name\":\"Georgios Pavlakos\"},{\"authorId\":\"52022007\",\"name\":\"Vasileios Choutas\"},{\"authorId\":\"3843367\",\"name\":\"N. Ghorbani\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"144261075\",\"name\":\"Ahmed A. A. Osman\"},{\"authorId\":\"1940674\",\"name\":\"Dimitrios Tzionas\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1109/CVPR.2019.01123\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4be4707aba8d622a0553aa159dc92ae7f9af9c5e\",\"title\":\"Expressive Body Capture: 3D Hands, Face, and Body From a Single Image\",\"url\":\"https://www.semanticscholar.org/paper/4be4707aba8d622a0553aa159dc92ae7f9af9c5e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1807.10267\",\"authors\":[{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"50456334\",\"name\":\"Soubhik Sanyal\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1007/978-3-030-01219-9_43\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ccb7ad8ea38991798172c33aee83f4d68a45d376\",\"title\":\"Generating 3D faces using Convolutional Mesh Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/ccb7ad8ea38991798172c33aee83f4d68a45d376\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1880628\",\"name\":\"D. Vlasic\"},{\"authorId\":\"144549270\",\"name\":\"M. Brand\"},{\"authorId\":\"143758236\",\"name\":\"H. Pfister\"},{\"authorId\":\"145492783\",\"name\":\"J. Popovic\"}],\"doi\":\"10.1145/1186822.1073209\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2290765ce72ea84af8ac7535d4159f223eefdd3\",\"title\":\"Face transfer with multilinear models\",\"url\":\"https://www.semanticscholar.org/paper/a2290765ce72ea84af8ac7535d4159f223eefdd3\",\"venue\":\"ACM Trans. Graph.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152782371\",\"name\":\"P. Edwards\"},{\"authorId\":\"2483922\",\"name\":\"C. Landreth\"},{\"authorId\":\"3018043\",\"name\":\"E. Fiume\"},{\"authorId\":\"34664064\",\"name\":\"Karan Singh\"}],\"doi\":\"10.1145/2897824.2925984\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"339e662f4a9489e7f5910bc2b047ba86b0f3b7a5\",\"title\":\"JALI: an animator-centric viseme model for expressive lip synchronization\",\"url\":\"https://www.semanticscholar.org/paper/339e662f4a9489e7f5910bc2b047ba86b0f3b7a5\",\"venue\":\"ACM Trans. Graph.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48696060\",\"name\":\"Y. Cao\"},{\"authorId\":\"32362814\",\"name\":\"Wen C. Tien\"},{\"authorId\":\"1737527\",\"name\":\"P. Faloutsos\"},{\"authorId\":\"1817134\",\"name\":\"F. Pighin\"}],\"doi\":\"10.1145/1095878.1095881\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"029a24e233440512e6923868f83af92832bcbaf6\",\"title\":\"Expressive speech-driven facial animation\",\"url\":\"https://www.semanticscholar.org/paper/029a24e233440512e6923868f83af92832bcbaf6\",\"venue\":\"TOGS\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693475\",\"name\":\"R. W. Sumner\"},{\"authorId\":\"145492783\",\"name\":\"J. Popovic\"}],\"doi\":\"10.1145/1186562.1015736\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6a5dcfbd6498a36976a8ea22ed21bb21601b7999\",\"title\":\"Deformation transfer for triangle meshes\",\"url\":\"https://www.semanticscholar.org/paper/6a5dcfbd6498a36976a8ea22ed21bb21601b7999\",\"venue\":\"ACM Trans. Graph.\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48095899\",\"name\":\"Gabriele Fanelli\"},{\"authorId\":\"145689714\",\"name\":\"Juergen Gall\"},{\"authorId\":\"2646830\",\"name\":\"Harald Romsdorfer\"},{\"authorId\":\"2246174\",\"name\":\"T. Weise\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/TMM.2010.2052239\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d08cc366a4a0192a01e9a7495af1eb5d9f9e73ae\",\"title\":\"A 3-D Audio-Visual Corpus of Affective Communication\",\"url\":\"https://www.semanticscholar.org/paper/d08cc366a4a0192a01e9a7495af1eb5d9f9e73ae\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145864185\",\"name\":\"Shiyang Cheng\"},{\"authorId\":\"1754270\",\"name\":\"I. Kotsia\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"}],\"doi\":\"10.1109/CVPR.2018.00537\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bc249c9be803af3d4a5a6de495e1c85578fce84b\",\"title\":\"4DFAB: A Large Scale 4D Database for Facial Expression Analysis and Biometric Applications\",\"url\":\"https://www.semanticscholar.org/paper/bc249c9be803af3d4a5a6de495e1c85578fce84b\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145595134\",\"name\":\"P. Hong\"},{\"authorId\":\"144136791\",\"name\":\"Zhen Wen\"},{\"authorId\":\"1739208\",\"name\":\"T. Huang\"}],\"doi\":\"10.1109/TNN.2002.1021892\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f3d2f11d0f6bb46b63262e0e28da72e3431971fe\",\"title\":\"Real-time speech-driven face animation with expressions using neural networks\",\"url\":\"https://www.semanticscholar.org/paper/f3d2f11d0f6bb46b63262e0e28da72e3431971fe\",\"venue\":\"IEEE Trans. Neural Networks\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35119991\",\"name\":\"Sofien Bouaziz\"},{\"authorId\":\"47906224\",\"name\":\"Yangang Wang\"},{\"authorId\":\"143674021\",\"name\":\"M. Pauly\"}],\"doi\":\"10.1145/2461912.2461976\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"944a54f54b156ffced95c0ea0bb47b35ee1a62d1\",\"title\":\"Online modeling for realtime facial animation\",\"url\":\"https://www.semanticscholar.org/paper/944a54f54b156ffced95c0ea0bb47b35ee1a62d1\",\"venue\":\"ACM Trans. Graph.\",\"year\":2013}],\"title\":\"Capture, Learning, and Synthesis of 3D Speaking Styles\",\"topics\":[{\"topic\":\"Virtual reality\",\"topicId\":\"4462\",\"url\":\"https://www.semanticscholar.org/topic/4462\"},{\"topic\":\"Speech processing\",\"topicId\":\"47583\",\"url\":\"https://www.semanticscholar.org/topic/47583\"},{\"topic\":\"Amazon Mechanical Turk\",\"topicId\":\"84\",\"url\":\"https://www.semanticscholar.org/topic/84\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Message Passing Interface\",\"topicId\":\"27332\",\"url\":\"https://www.semanticscholar.org/topic/27332\"},{\"topic\":\"Data acquisition\",\"topicId\":\"7391\",\"url\":\"https://www.semanticscholar.org/topic/7391\"},{\"topic\":\"Retargeting\",\"topicId\":\"112399\",\"url\":\"https://www.semanticscholar.org/topic/112399\"},{\"topic\":\"TensorFlow\",\"topicId\":\"264254\",\"url\":\"https://www.semanticscholar.org/topic/264254\"},{\"topic\":\"Virtual world\",\"topicId\":\"1413\",\"url\":\"https://www.semanticscholar.org/topic/1413\"},{\"topic\":\"CT scan\",\"topicId\":\"3286\",\"url\":\"https://www.semanticscholar.org/topic/3286\"},{\"topic\":\"Hypertext Transfer Protocol\",\"topicId\":\"28225\",\"url\":\"https://www.semanticscholar.org/topic/28225\"}],\"url\":\"https://www.semanticscholar.org/paper/91a1370d26ee903296bb4990a84c23f2fa8e8c83\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019}\n"