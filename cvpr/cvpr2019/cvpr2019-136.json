"{\"abstract\":\"Motion has shown to be useful for video understanding, where motion is typically represented by optical flow. However, computing flow from video frames is very timeconsuming. Recent works directly leverage the motion vectors and residuals readily available in the compressed video to represent motion at no cost. While this avoids flow computation, it also hurts accuracy since the motion vector is noisy and has substantially reduced resolution, which makes it a less discriminative motion representation. To remedy these issues, we propose a lightweight generator network, which reduces noises in motion vectors and captures fine motion details, achieving a more Discriminative Motion Cue (DMC) representation. Since optical flow is a more accurate motion representation, we train the DMC generator to approximate flow using a reconstruction loss and a generative adversarial loss, jointly with the downstream action classification task. Extensive evaluations on three action recognition benchmarks (HMDB-51, UCF-101, and a subset of Kinetics) confirm the effectiveness of our method. Our full system, consisting of the generator and the classifier, is coined as DMC-Net which obtains high accuracy close to that of using flow and runs two orders of magnitude faster than using optical flow at inference time.\",\"arxivId\":\"1901.03460\",\"authors\":[{\"authorId\":\"73423138\",\"name\":\"Zheng Shou\",\"url\":\"https://www.semanticscholar.org/author/73423138\"},{\"authorId\":\"3305169\",\"name\":\"Zhicheng Yan\",\"url\":\"https://www.semanticscholar.org/author/3305169\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\",\"url\":\"https://www.semanticscholar.org/author/1944225\"},{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\",\"url\":\"https://www.semanticscholar.org/author/1403581832\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\",\"url\":\"https://www.semanticscholar.org/author/34849128\"},{\"authorId\":\"48030192\",\"name\":\"Xudong Lin\",\"url\":\"https://www.semanticscholar.org/author/48030192\"},{\"authorId\":\"70351911\",\"name\":\"Shih-Fu Chang\",\"url\":\"https://www.semanticscholar.org/author/70351911\"}],\"citationVelocity\":13,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"46740612\",\"name\":\"C. Wu\"},{\"authorId\":\"50171534\",\"name\":\"Xiaojun Wu\"},{\"authorId\":\"47091894\",\"name\":\"J. Kittler\"}],\"doi\":\"10.1109/ICCVW.2019.00216\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d0678d9b06fa66786cc6213254820184d8ebfd2a\",\"title\":\"Spatial Residual Layer and Dense Connection Block Enhanced Spatial Temporal Graph Convolutional Network for Skeleton-Based Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d0678d9b06fa66786cc6213254820184d8ebfd2a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1911.08206\",\"authors\":[{\"authorId\":\"1382652819\",\"name\":\"Barak Battash\"},{\"authorId\":\"1420126809\",\"name\":\"Haim Barad\"},{\"authorId\":\"39278465\",\"name\":\"Hanlin Tang\"},{\"authorId\":\"3243137\",\"name\":\"Amit Bleiweiss\"}],\"doi\":\"10.1109/CVPRW50498.2020.00350\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"da2934c24a9de690ff399736711b754cc10ae1ec\",\"title\":\"Mimic The Raw Domain: Accelerating Action Recognition in the Compressed Domain\",\"url\":\"https://www.semanticscholar.org/paper/da2934c24a9de690ff399736711b754cc10ae1ec\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"94041056\",\"name\":\"Q. Liu\"},{\"authorId\":\"3150525\",\"name\":\"E. Chen\"},{\"authorId\":\"145735014\",\"name\":\"L. Gao\"},{\"authorId\":\"144446381\",\"name\":\"Chengwu Liang\"},{\"authorId\":\"48447072\",\"name\":\"H. Liu\"}],\"doi\":\"10.3390/s20174673\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc19368b11d0f782fe321e55904b04b57bf6bdd3\",\"title\":\"Energy-Guided Temporal Segmentation Network for Multimodal Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fc19368b11d0f782fe321e55904b04b57bf6bdd3\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151472652\",\"name\":\"Yuqi Huo\"},{\"authorId\":\"101246507\",\"name\":\"Xiaoli Xu\"},{\"authorId\":\"46215480\",\"name\":\"Yao Lu\"},{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"48876151\",\"name\":\"Mingyu Ding\"},{\"authorId\":\"1776220\",\"name\":\"Zhiwu Lu\"},{\"authorId\":\"145406420\",\"name\":\"Tao Xiang\"},{\"authorId\":\"153693432\",\"name\":\"Jirong Wen\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"89fafbe91b31d43dc3314ffabffa79a5f5ad746f\",\"title\":\"Lightweight Action Recognition in Compressed Videos\",\"url\":\"https://www.semanticscholar.org/paper/89fafbe91b31d43dc3314ffabffa79a5f5ad746f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50201744\",\"name\":\"Shengjun Zhang\"},{\"authorId\":\"1471440106\",\"name\":\"W. Meng\"},{\"authorId\":\"4169031\",\"name\":\"Hui-quan Li\"},{\"authorId\":\"8628276\",\"name\":\"Xuehong Cui\"}],\"doi\":\"10.1109/ACCESS.2019.2959206\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"430ee2f6b5a1d4c194e26088ca8e3b58ae571605\",\"title\":\"Multimodal Spatiotemporal Networks for Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/430ee2f6b5a1d4c194e26088ca8e3b58ae571605\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1912.04462\",\"authors\":[{\"authorId\":\"3264239\",\"name\":\"Shi-Yuan Huang\"},{\"authorId\":\"1821514\",\"name\":\"Xu-Dong Lin\"},{\"authorId\":\"35862299\",\"name\":\"S. Karaman\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1c801bcaa007c6bf6ce4027bdb45f88f7a861db3\",\"title\":\"Flow-Distilled IP Two-Stream Networks for Compressed Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1c801bcaa007c6bf6ce4027bdb45f88f7a861db3\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yingying Wang\"},{\"authorId\":\"50135244\",\"name\":\"Wenjia Li\"},{\"authorId\":\"143677598\",\"name\":\"R. Tao\"}],\"doi\":\"10.1109/LSP.2019.2940111\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab9b40a95e4b2328cb681de4413ba918a978ecff\",\"title\":\"Multi-Branch Spatial-Temporal Network for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ab9b40a95e4b2328cb681de4413ba918a978ecff\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490672305\",\"name\":\"M. Suresha\"},{\"authorId\":\"98570196\",\"name\":\"S. Kuppa\"},{\"authorId\":\"1490675042\",\"name\":\"D. S. Raghukumar\"}],\"doi\":\"10.1007/s13735-019-00190-x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac8f926b205bba37df42bab279c397a5eca07445\",\"title\":\"A study on deep learning spatiotemporal models and feature extraction techniques for video understanding\",\"url\":\"https://www.semanticscholar.org/paper/ac8f926b205bba37df42bab279c397a5eca07445\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2020},{\"arxivId\":\"2010.11594\",\"authors\":[{\"authorId\":\"3191371\",\"name\":\"Yuanhao Zhai\"},{\"authorId\":\"153755094\",\"name\":\"L. Wang\"},{\"authorId\":\"2914452\",\"name\":\"W. Tang\"},{\"authorId\":\"46324995\",\"name\":\"Q. Zhang\"},{\"authorId\":\"48837492\",\"name\":\"J. Yuan\"},{\"authorId\":\"144988570\",\"name\":\"Gang Hua\"}],\"doi\":\"10.1007/978-3-030-58539-6_3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b41ec90b8e8972e6d09ae129ce4e004e37ad4015\",\"title\":\"Two-Stream Consensus Network for Weakly-Supervised Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/b41ec90b8e8972e6d09ae129ce4e004e37ad4015\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491240780\",\"name\":\"Z. Shi\"},{\"authorId\":\"144962375\",\"name\":\"Cheng Guan\"},{\"authorId\":\"1491233401\",\"name\":\"Liangjie Cao\"},{\"authorId\":\"1632356255\",\"name\":\"Qianqian Li\"},{\"authorId\":\"1993660232\",\"name\":\"Ju Liang\"},{\"authorId\":\"29344734\",\"name\":\"Zhaorui Gu\"},{\"authorId\":\"2336297\",\"name\":\"Haiyong Zheng\"},{\"authorId\":\"1470709309\",\"name\":\"Bing Zheng\"}],\"doi\":\"10.1007/978-3-030-58539-6_23\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c826e169f505f6fda0872d0a3e1e156e15b5111e\",\"title\":\"CoTeRe-Net: Discovering Collaborative Ternary Relations in Videos\",\"url\":\"https://www.semanticscholar.org/paper/c826e169f505f6fda0872d0a3e1e156e15b5111e\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.12619\",\"authors\":[{\"authorId\":\"147084112\",\"name\":\"Jack Humphreys\"},{\"authorId\":\"48354826\",\"name\":\"Z. Chen\"},{\"authorId\":\"145047838\",\"name\":\"Dacheng Tao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e88bff50c417703239e0a832fddb267196ab5a99\",\"title\":\"Recent Progress in Appearance-based Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/e88bff50c417703239e0a832fddb267196ab5a99\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.06567\",\"authors\":[{\"authorId\":\"1805946041\",\"name\":\"Yi Zhu\"},{\"authorId\":\"21657733\",\"name\":\"X. Li\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"22539483\",\"name\":\"Chongruo Wu\"},{\"authorId\":\"152781163\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2397422\",\"name\":\"Joseph Tighe\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"49140510\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"title\":\"A Comprehensive Study of Deep Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.13260\",\"authors\":[{\"authorId\":\"1508389232\",\"name\":\"Junyi Feng\"},{\"authorId\":\"47319889\",\"name\":\"Songyuan Li\"},{\"authorId\":\"92384987\",\"name\":\"X. Li\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"29116642\",\"name\":\"H. Ling\"}],\"doi\":\"10.1109/TPAMI.2020.3024646\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f86bfe5174aa24a33087a4456b1a9734ffb68f8c\",\"title\":\"TapLab: A Fast Framework for Semantic Video Segmentation Tapping into Compressed-Domain Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/f86bfe5174aa24a33087a4456b1a9734ffb68f8c\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1821514\",\"name\":\"Xu-Dong Lin\"},{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e753a55e8905cb0b72672e72e9b8fa8f34694dbc\",\"title\":\"LPAT: Learning to Predict Adaptive Threshold for Weakly-supervised Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/e753a55e8905cb0b72672e72e9b8fa8f34694dbc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1659125162\",\"name\":\"Danfeng Zhuang\"},{\"authorId\":\"145309461\",\"name\":\"Min Jiang\"},{\"authorId\":\"1644912978\",\"name\":\"Jun Kong\"},{\"authorId\":\"1878899344\",\"name\":\"Tianshan Liu\"}],\"doi\":\"10.1007/s13042-020-01204-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88f3b42b45e5f5ee60a22eb30cd41e7acc8662c9\",\"title\":\"Spatiotemporal attention enhanced features fusion network for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/88f3b42b45e5f5ee60a22eb30cd41e7acc8662c9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2537286\",\"name\":\"H. Eun\"},{\"authorId\":\"3035146\",\"name\":\"Jinyoung Moon\"},{\"authorId\":\"2800227\",\"name\":\"Jongyoul Park\"},{\"authorId\":\"2024203\",\"name\":\"Chanho Jung\"},{\"authorId\":\"145568138\",\"name\":\"Changick Kim\"}],\"doi\":\"10.1016/j.patcog.2020.107695\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86e69494bfc8e8156a7f4cc42d9052b57b6e4241\",\"title\":\"Temporal filtering networks for online action detection\",\"url\":\"https://www.semanticscholar.org/paper/86e69494bfc8e8156a7f4cc42d9052b57b6e4241\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1904697\",\"name\":\"Zhenbing Liu\"},{\"authorId\":\"1796258521\",\"name\":\"Zeya Li\"},{\"authorId\":\"49908315\",\"name\":\"Ruili Wang\"},{\"authorId\":\"51251812\",\"name\":\"Ming Zong\"},{\"authorId\":\"29790429\",\"name\":\"Wanting Ji\"}],\"doi\":\"10.1007/s00521-020-05144-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"575448ab04aef0856a73d8eba3a75a5321e0506d\",\"title\":\"Spatiotemporal saliency-based multi-stream networks with attention-aware LSTM for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/575448ab04aef0856a73d8eba3a75a5321e0506d\",\"venue\":\"Neural Computing and Applications\",\"year\":2020},{\"arxivId\":\"1912.04487\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":\"10.1109/cvpr42600.2020.01047\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"title\":\"Listen to Look: Action Recognition by Previewing Audio\",\"url\":\"https://www.semanticscholar.org/paper/c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1910.11285\",\"authors\":[{\"authorId\":\"1821514\",\"name\":\"Xu-Dong Lin\"},{\"authorId\":\"73423138\",\"name\":\"Zheng Shou\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"45f395d2bb7cb41a7aff934cbecc9f567a27a070\",\"title\":\"Towards Train-Test Consistency for Semi-supervised Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/45f395d2bb7cb41a7aff934cbecc9f567a27a070\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46276482\",\"name\":\"J. Li\"},{\"authorId\":\"145708711\",\"name\":\"Ping Wei\"},{\"authorId\":\"1978363545\",\"name\":\"Yongchi Zhang\"},{\"authorId\":\"153873673\",\"name\":\"N. Zheng\"}],\"doi\":\"10.1145/3394171.3413641\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"05444a5a49f717dc199957b66e9c472219171f88\",\"title\":\"A Slow-I-Fast-P Architecture for Compressed Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/05444a5a49f717dc199957b66e9c472219171f88\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2010.10637\",\"authors\":[{\"authorId\":\"120281172\",\"name\":\"Xiaofeng Liu\"},{\"authorId\":\"2000231390\",\"name\":\"Linghao Jin\"},{\"authorId\":\"145343933\",\"name\":\"X. Han\"},{\"authorId\":\"144329386\",\"name\":\"J. You\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"07bc6dd2a128e0a433b928faba484c5831227690\",\"title\":\"Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video\",\"url\":\"https://www.semanticscholar.org/paper/07bc6dd2a128e0a433b928faba484c5831227690\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.07848\",\"authors\":[{\"authorId\":\"80447592\",\"name\":\"Yunzhong Hou\"},{\"authorId\":\"144436089\",\"name\":\"L. Zheng\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.1109/cvpr42600.2020.01013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"94889c780749b6c86a7be8defbb1fc4e915ccbd4\",\"title\":\"Learning to Structure an Image With Few Colors\",\"url\":\"https://www.semanticscholar.org/paper/94889c780749b6c86a7be8defbb1fc4e915ccbd4\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31595791\",\"name\":\"J. Li\"},{\"authorId\":\"6820648\",\"name\":\"Xianglong Liu\"},{\"authorId\":\"150341144\",\"name\":\"Wenxuan Zhang\"},{\"authorId\":\"47473953\",\"name\":\"Mingyuan Zhang\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1109/TMM.2020.2965434\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ea173f972d39b8631e9ac8fe59a947c70ba31e3\",\"title\":\"Spatio-Temporal Attention Networks for Action Recognition and Detection\",\"url\":\"https://www.semanticscholar.org/paper/2ea173f972d39b8631e9ac8fe59a947c70ba31e3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1912.00381\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":\"10.1109/cvpr42600.2020.00118\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"57cee868188127305f966a178ca22025b397d911\",\"title\":\"Gate-Shift Networks for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/57cee868188127305f966a178ca22025b397d911\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.05732\",\"authors\":[{\"authorId\":\"103102295\",\"name\":\"Benjamin Deguerre\"},{\"authorId\":\"1712446\",\"name\":\"Cl\\u00e9ment Chatelain\"},{\"authorId\":\"2378576\",\"name\":\"G. Gasso\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"623a8a93e5a76ef7873210ac78a093d2441c6326\",\"title\":\"Object Detection in the DCT Domain: is Luminance the Solution?\",\"url\":\"https://www.semanticscholar.org/paper/623a8a93e5a76ef7873210ac78a093d2441c6326\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.03548\",\"authors\":[{\"authorId\":\"36290866\",\"name\":\"Anyi Rao\"},{\"authorId\":\"1557390077\",\"name\":\"Jiaze Wang\"},{\"authorId\":\"150196512\",\"name\":\"Linning Xu\"},{\"authorId\":\"80180784\",\"name\":\"Xuekun Jiang\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-58621-8_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49b2d6f498bde324151cc5d0f8a9bba70540efbd\",\"title\":\"A Unified Framework for Shot Type Classification Based on Subject Centric Lens\",\"url\":\"https://www.semanticscholar.org/paper/49b2d6f498bde324151cc5d0f8a9bba70540efbd\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2005.01344\",\"authors\":[{\"authorId\":\"1508389232\",\"name\":\"Junyi Feng\"},{\"authorId\":\"47319889\",\"name\":\"Songyuan Li\"},{\"authorId\":\"1596827299\",\"name\":\"Yifeng Chen\"},{\"authorId\":\"50187585\",\"name\":\"Fuxian Huang\"},{\"authorId\":\"10250973\",\"name\":\"Jiabao Cui\"},{\"authorId\":\"92384987\",\"name\":\"X. Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9b3438f35ab6ed712b97ebec534c2c7781524daf\",\"title\":\"How to Train Your Dragon: Tamed Warping Network for Semantic Video Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/9b3438f35ab6ed712b97ebec534c2c7781524daf\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":57825774,\"doi\":\"10.1109/CVPR.2019.00136\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":4,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"d878aac73038c3bc175ccc2b93acc04675f33bbd\",\"references\":[{\"arxivId\":\"1804.07667\",\"authors\":[{\"authorId\":\"2820136\",\"name\":\"Yu-Wei Chao\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"2535887\",\"name\":\"Bryan Seybold\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"}],\"doi\":\"10.1109/CVPR.2018.00124\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e85c01ff4979357b428538c9f224fa4259541c1a\",\"title\":\"Rethinking the Faster R-CNN Architecture for Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/e85c01ff4979357b428538c9f224fa4259541c1a\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144031005\",\"name\":\"A. Bruhn\"},{\"authorId\":\"7789445\",\"name\":\"J. Weickert\"},{\"authorId\":\"1679944\",\"name\":\"C. Schn\\u00f6rr\"}],\"doi\":\"10.1023/B:VISI.0000045324.43199.43\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c0222b86881860bf61a5c11d39bd9254a6196633\",\"title\":\"Lucas/Kanade Meets Horn/Schunck: Combining Local and Global Optic Flow Methods\",\"url\":\"https://www.semanticscholar.org/paper/c0222b86881860bf61a5c11d39bd9254a6196633\",\"venue\":\"International Journal of Computer Vision\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"3261451\",\"name\":\"Benjamin Rozenfeld\"}],\"doi\":\"10.1109/CVPR.2008.4587756\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f86767732f76f478d5845f2e59f99ba106e9265\",\"title\":\"Learning realistic human actions from movies\",\"url\":\"https://www.semanticscholar.org/paper/0f86767732f76f478d5845f2e59f99ba106e9265\",\"venue\":\"2008 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2017.319\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d2892f82a89bfc81f9924adb8bd070fe007adf7\",\"title\":\"Generating the Future with Adversarial Transformers\",\"url\":\"https://www.semanticscholar.org/paper/6d2892f82a89bfc81f9924adb8bd070fe007adf7\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1611.00850\",\"authors\":[{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1109/CVPR.2017.291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a93d81c7f033f1e2b54d3288b60d214f55ccc010\",\"title\":\"Optical Flow Estimation Using a Spatial Pyramid Network\",\"url\":\"https://www.semanticscholar.org/paper/a93d81c7f033f1e2b54d3288b60d214f55ccc010\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1807.11195\",\"authors\":[{\"authorId\":\"1713312\",\"name\":\"Y. Chen\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"65737740\",\"name\":\"J. Li\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"}],\"doi\":\"10.1007/978-3-030-01246-5_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe82d072a8d13cfefcd575db893f3374251f04a8\",\"title\":\"Multi-Fiber Networks for Video Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fe82d072a8d13cfefcd575db893f3374251f04a8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1707.06750\",\"authors\":[{\"authorId\":\"6873935\",\"name\":\"T. Lin\"},{\"authorId\":\"1758267\",\"name\":\"X. Zhao\"},{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"af3bdb1739826b1a6ada23b9fb18eaa241a444e3\",\"title\":\"Temporal Convolution Based Action Proposal: Submission to ActivityNet 2017\",\"url\":\"https://www.semanticscholar.org/paper/af3bdb1739826b1a6ada23b9fb18eaa241a444e3\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xiaolong Wang\"},{\"authorId\":null,\"name\":\"Ross Girshick\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Abhinav Gupta, and Kaiming He. Non-local neural networks\",\"url\":\"\",\"venue\":\"In CVPR\",\"year\":2018},{\"arxivId\":\"1804.00413\",\"authors\":[{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"},{\"authorId\":\"1768190\",\"name\":\"J. Huang\"}],\"doi\":\"10.1109/CVPR.2018.00630\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"259dcd1afe22e569132ccc41697ac368504c4dd1\",\"title\":\"End-to-End Learning of Motion Representation for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/259dcd1afe22e569132ccc41697ac368504c4dd1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":\"1711.07971\",\"authors\":[{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"}],\"doi\":\"10.1109/CVPR.2018.00813\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8899094797e82c5c185a0893896320ef77f60e64\",\"title\":\"Non-local Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/8899094797e82c5c185a0893896320ef77f60e64\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"367f2c63a6f6a10b3b64b8729d601e69337ee3cc\",\"title\":\"Rectifier Nonlinearities Improve Neural Network Acoustic Models\",\"url\":\"https://www.semanticscholar.org/paper/367f2c63a6f6a10b3b64b8729d601e69337ee3cc\",\"venue\":\"\",\"year\":2013},{\"arxivId\":\"1709.08421\",\"authors\":[{\"authorId\":\"1399435786\",\"name\":\"Antonio Tejero-de-Pablos\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"1762686\",\"name\":\"T. Sato\"},{\"authorId\":\"1771769\",\"name\":\"N. Yokoya\"},{\"authorId\":\"1412591633\",\"name\":\"Marko Linna\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/TMM.2018.2794265\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35e6f6e5f4f780508e5f58e87f9efe2b07d8a864\",\"title\":\"Summarization of User-Generated Sports Video by Using Deep Action Recognition Features\",\"url\":\"https://www.semanticscholar.org/paper/35e6f6e5f4f780508e5f58e87f9efe2b07d8a864\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":\"1609.04802\",\"authors\":[{\"authorId\":\"1779917\",\"name\":\"C. Ledig\"},{\"authorId\":\"2073063\",\"name\":\"L. Theis\"},{\"authorId\":\"3108066\",\"name\":\"Ferenc Husz\\u00e1r\"},{\"authorId\":\"79382929\",\"name\":\"J. Caballero\"},{\"authorId\":\"83015038\",\"name\":\"Andrew Aitken\"},{\"authorId\":\"41203992\",\"name\":\"Alykhan Tejani\"},{\"authorId\":\"1853456\",\"name\":\"J. Totz\"},{\"authorId\":\"34627233\",\"name\":\"Zehan Wang\"},{\"authorId\":\"152554375\",\"name\":\"W. Shi\"}],\"doi\":\"10.1109/CVPR.2017.19\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3\",\"title\":\"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Ffmpeg: A complete, cross-platform solution to record\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1412.6806\",\"authors\":[{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"2841331\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f84a81f431b18a78bd97f59ed4b9d8eda390970\",\"title\":\"Striving for Simplicity: The All Convolutional Net\",\"url\":\"https://www.semanticscholar.org/paper/0f84a81f431b18a78bd97f59ed4b9d8eda390970\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1612.00738\",\"authors\":[{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"2304222\",\"name\":\"E. Gavves\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":\"10.1109/TPAMI.2017.2769085\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2fe8c43aa9427582906c684afadebbc6a86fa036\",\"title\":\"Action Recognition with Dynamic Image Networks\",\"url\":\"https://www.semanticscholar.org/paper/2fe8c43aa9427582906c684afadebbc6a86fa036\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2551905\",\"name\":\"E. Mendi\"},{\"authorId\":\"1400031791\",\"name\":\"H\\u00e9lio B. Clemente\"},{\"authorId\":\"2132384\",\"name\":\"Coskun Bayrak\"}],\"doi\":\"10.1016/j.compeleceng.2012.11.020\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a5b66c1d18382a2be4035f5af03683d6c0bb83c\",\"title\":\"Sports video summarization based on motion analysis\",\"url\":\"https://www.semanticscholar.org/paper/2a5b66c1d18382a2be4035f5af03683d6c0bb83c\",\"venue\":\"Comput. Electr. Eng.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1713941\",\"name\":\"Christopher Zach\"},{\"authorId\":\"1730097\",\"name\":\"T. Pock\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"}],\"doi\":\"10.1007/978-3-540-74936-3_22\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c\",\"title\":\"A Duality Based Approach for Realtime TV-L1 Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c\",\"venue\":\"DAGM-Symposium\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3047890\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"}],\"doi\":\"10.1109/TIP.2018.2791180\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e720ef927bb03b4cd821a1d65a2b18d46748e2dc\",\"title\":\"Real-Time Action Recognition With Deeply Transferred Motion Vector CNNs\",\"url\":\"https://www.semanticscholar.org/paper/e720ef927bb03b4cd821a1d65a2b18d46748e2dc\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1705.07750\",\"authors\":[{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.502\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"title\":\"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\"url\":\"https://www.semanticscholar.org/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1711.11248\",\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"4439383\",\"name\":\"Jamie Ray\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/CVPR.2018.00675\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"89c3050522a0bb9820c32dc7444e003ef0d3e2e4\",\"title\":\"A Closer Look at Spatiotemporal Convolutions for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/89c3050522a0bb9820c32dc7444e003ef0d3e2e4\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"101841672\",\"name\":\"Moritz Menze\"},{\"authorId\":\"2998107\",\"name\":\"C. Heipke\"},{\"authorId\":\"47237027\",\"name\":\"Andreas Geiger\"}],\"doi\":\"10.5194/ISPRSANNALS-II-3-W5-427-2015\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"73a184ff95cb0341d7f2b975575ba5db397700c1\",\"title\":\"Joint 3d Estimation of Vehicles and Scene Flow\",\"url\":\"https://www.semanticscholar.org/paper/73a184ff95cb0341d7f2b975575ba5db397700c1\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1612.01925\",\"authors\":[{\"authorId\":\"48105320\",\"name\":\"Eddy Ilg\"},{\"authorId\":\"153200643\",\"name\":\"N. Mayer\"},{\"authorId\":\"2872102\",\"name\":\"Tonmoy Saikia\"},{\"authorId\":\"3316866\",\"name\":\"Margret Keuper\"},{\"authorId\":\"2841331\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/CVPR.2017.179\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"edd846e76cacfba5be37da99c006e3ccc9b861b0\",\"title\":\"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/edd846e76cacfba5be37da99c006e3ccc9b861b0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123446103\",\"name\":\"Hilde Kuehne\"},{\"authorId\":\"119268487\",\"name\":\"Hueihan Jhuang\"},{\"authorId\":\"1930964\",\"name\":\"E. Garrote\"},{\"authorId\":\"145031878\",\"name\":\"T. Poggio\"},{\"authorId\":\"1981539\",\"name\":\"Thomas Serre\"}],\"doi\":\"10.1109/ICCV.2011.6126543\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b3b8848a311c501e704c45c6d50430ab7068956\",\"title\":\"HMDB: A large video database for human motion recognition\",\"url\":\"https://www.semanticscholar.org/paper/8b3b8848a311c501e704c45c6d50430ab7068956\",\"venue\":\"2011 International Conference on Computer Vision\",\"year\":2011},{\"arxivId\":\"1704.06228\",\"authors\":[{\"authorId\":\"145454812\",\"name\":\"Yue Zhao\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"119770705\",\"name\":\"L. Wang\"},{\"authorId\":\"152247501\",\"name\":\"Zhirong Wu\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/ICCV.2017.317\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59c3a05eac92285aece62bb90d289f8904f11683\",\"title\":\"Temporal Action Detection with Structured Segment Networks\",\"url\":\"https://www.semanticscholar.org/paper/59c3a05eac92285aece62bb90d289f8904f11683\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1708.05038\",\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"4439383\",\"name\":\"Jamie Ray\"},{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a92adfdd8996ab2bd7cdc910ea1d3db03c66d34f\",\"title\":\"ConvNet Architecture Search for Spatiotemporal Feature Learning\",\"url\":\"https://www.semanticscholar.org/paper/a92adfdd8996ab2bd7cdc910ea1d3db03c66d34f\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.441\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d721f4d64b8e722222c876f0a0f226ed49476347\",\"title\":\"Action Recognition with Improved Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/d721f4d64b8e722222c876f0a0f226ed49476347\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gao Huang\"},{\"authorId\":null,\"name\":\"Zhuang Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks\",\"url\":\"\",\"venue\":\"CVPR\",\"year\":2017},{\"arxivId\":\"1711.09125\",\"authors\":[{\"authorId\":\"119770705\",\"name\":\"L. Wang\"},{\"authorId\":\"48625175\",\"name\":\"W. Li\"},{\"authorId\":\"145344553\",\"name\":\"W. Li\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/CVPR.2018.00155\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e6b534d8838f92461a478a3e737b73e08db94748\",\"title\":\"Appearance-and-Relation Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/e6b534d8838f92461a478a3e737b73e08db94748\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1604.07669\",\"authors\":[{\"authorId\":\"3047890\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"}],\"doi\":\"10.1109/CVPR.2016.297\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3d4cf68fad61cd7dc9061670ba3864a7fdf87d4c\",\"title\":\"Real-Time Action Recognition with Enhanced Motion Vector CNNs\",\"url\":\"https://www.semanticscholar.org/paper/3d4cf68fad61cd7dc9061670ba3864a7fdf87d4c\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1611.07004\",\"authors\":[{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"1822702\",\"name\":\"Tinghui Zhou\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2017.632\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8acbe90d5b852dadea7810345451a99608ee54c7\",\"title\":\"Image-to-Image Translation with Conditional Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1709096\",\"name\":\"R. Wildes\"}],\"doi\":\"10.1109/CVPR.2017.787\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"265cd694e8886a7f8413dd334662f269c6ac2bfc\",\"title\":\"Spatiotemporal Multiplier Networks for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/265cd694e8886a7f8413dd334662f269c6ac2bfc\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2972680\",\"name\":\"D. L. Gall\"}],\"doi\":\"10.1145/103085.103090\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bafe1e3aba293f978ec0f44599d7d62eca9206c\",\"title\":\"MPEG: a video compression standard for multimedia applications\",\"url\":\"https://www.semanticscholar.org/paper/6bafe1e3aba293f978ec0f44599d7d62eca9206c\",\"venue\":\"CACM\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"3428663\",\"name\":\"J\\u00e9r\\u00f4me Revaud\"},{\"authorId\":\"113130084\",\"name\":\"Zaid Harchaoui\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.175\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"56493a503eecb659bbef2fe4dd1fb915d251cac0\",\"title\":\"DeepFlow: Large Displacement Optical Flow with Deep Matching\",\"url\":\"https://www.semanticscholar.org/paper/56493a503eecb659bbef2fe4dd1fb915d251cac0\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zheng Shou\"},{\"authorId\":null,\"name\":\"Jonathan Chan\"},{\"authorId\":null,\"name\":\"Alireza Zareian\"},{\"authorId\":null,\"name\":\"Kazuyuki Miyazawa\"},{\"authorId\":null,\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cdc: Convolutional-deconvolutional networks for precise temporal action local-  ization in untrimmed videos\",\"url\":\"\",\"venue\":\"In Computer Vision and Pattern Recognition (CVPR),\",\"year\":2017},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1604.06573\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2016.213\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d9aced120e530484609164c836da64548693484\",\"title\":\"Convolutional Two-Stream Network Fusion for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1709.02371\",\"authors\":[{\"authorId\":\"3232265\",\"name\":\"Deqing Sun\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00931\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d92ef81e39d65d89f2c35d63088d1950ed862e7d\",\"title\":\"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume\",\"url\":\"https://www.semanticscholar.org/paper/d92ef81e39d65d89f2c35d63088d1950ed862e7d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1801.09184\",\"authors\":[{\"authorId\":\"2860057\",\"name\":\"Yancheng Bai\"},{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"601d81b164afceecf6f155d60bfb400510a5be4e\",\"title\":\"Contextual Multi-Scale Region Convolutional 3D Network for Activity Detection\",\"url\":\"https://www.semanticscholar.org/paper/601d81b164afceecf6f155d60bfb400510a5be4e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1712.00636\",\"authors\":[{\"authorId\":\"2978413\",\"name\":\"Chao-Yuan Wu\"},{\"authorId\":\"1771307\",\"name\":\"M. Zaheer\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"46234526\",\"name\":\"Alex Smola\"},{\"authorId\":\"2562966\",\"name\":\"Philipp Kr\\u00e4henb\\u00fchl\"}],\"doi\":\"10.1109/CVPR.2018.00631\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9d98a956aadaff727e495b14b7c532d40ea49e16\",\"title\":\"Compressed Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9d98a956aadaff727e495b14b7c532d40ea49e16\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143814637\",\"name\":\"B. Horn\"},{\"authorId\":\"1717435\",\"name\":\"B. Schunck\"}],\"doi\":\"10.1016/0004-3702(81)90024-2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbe9176abf0c98c7abdfd136f53d58620f8acfec\",\"title\":\"Determining Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/cbe9176abf0c98c7abdfd136f53d58620f8acfec\",\"venue\":\"Artif. Intell.\",\"year\":1981},{\"arxivId\":\"1511.05440\",\"authors\":[{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2341378\",\"name\":\"C. Couprie\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"title\":\"Deep multi-scale video prediction beyond mean square error\",\"url\":\"https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1608.00859\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-46484-8_2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"title\":\"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1703.01515\",\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":\"144983134\",\"name\":\"J. Chan\"},{\"authorId\":\"2778637\",\"name\":\"Alireza Zareian\"},{\"authorId\":\"1789840\",\"name\":\"K. Miyazawa\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2017.155\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd3a96a94a9ca7fa62f1aec6e868ca03a56c08b9\",\"title\":\"CDC: Convolutional-De-Convolutional Networks for Precise Temporal Action Localization in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/bd3a96a94a9ca7fa62f1aec6e868ca03a56c08b9\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zheng Shou\"},{\"authorId\":null,\"name\":\"Xudong Lin\"},{\"authorId\":null,\"name\":\"Yannis Kalantidis\"},{\"authorId\":null,\"name\":\"Laura Sevilla- Lara\"},{\"authorId\":null,\"name\":\"Marcus Rohrbach\"},{\"authorId\":null,\"name\":\"Shih-Fu Chang\"},{\"authorId\":null,\"name\":\"Zhicheng Yan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Dmc-net: Generating discriminative motion cues for fast compressed video action recognition\",\"url\":\"\",\"venue\":\"arXiv preprint arXiv:1901.03460,\",\"year\":2019},{\"arxivId\":\"1604.07379\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"2562966\",\"name\":\"Philipp Kr\\u00e4henb\\u00fchl\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2016.278\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d0effebfa4bed19b6ba41f3af5b7e5b6890de87\",\"title\":\"Context Encoders: Feature Learning by Inpainting\",\"url\":\"https://www.semanticscholar.org/paper/7d0effebfa4bed19b6ba41f3af5b7e5b6890de87\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1804.01523\",\"authors\":[{\"authorId\":\"49250083\",\"name\":\"Alex X. Lee\"},{\"authorId\":\"2844849\",\"name\":\"Richard Zhang\"},{\"authorId\":\"27535721\",\"name\":\"Frederik Ebert\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9f67271a1edea3bf80c64c2d54e2a0a57612a567\",\"title\":\"Stochastic Adversarial Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/9f67271a1edea3bf80c64c2d54e2a0a57612a567\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1712.04109\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"144752314\",\"name\":\"Bo Xiong\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2018.00622\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"89498817a49d9c349ec9f67375023ead0411b865\",\"title\":\"Im2Flow: Motion Hallucination from Static Images for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/89498817a49d9c349ec9f67375023ead0411b865\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ruohan Gao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Im 2 flow : Motion hallucination from static images for action recognition Generative adversarial nets\",\"url\":\"\",\"venue\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"150426657\",\"name\":\"\\u62d3\\u6d77 \\u6749\\u5c71\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8a5f27e7805f8de84ea008d59452ff864271696\",\"title\":\"\\u201cUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\\u201d\\u306e\\u5b66\\u7fd2\\u5831\\u544a\",\"url\":\"https://www.semanticscholar.org/paper/e8a5f27e7805f8de84ea008d59452ff864271696\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1512.04150\",\"authors\":[{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"2677488\",\"name\":\"\\u00c0gata Lapedriza\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2016.319\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"31f9eb39d840821979e5df9f34a6e92dd9c879f2\",\"title\":\"Learning Deep Features for Discriminative Localization\",\"url\":\"https://www.semanticscholar.org/paper/31f9eb39d840821979e5df9f34a6e92dd9c879f2\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":\"2704179\",\"name\":\"Dongang Wang\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2016.119\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"317eaf94573857bec786bbf030605ccdb0fd624d\",\"title\":\"Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs\",\"url\":\"https://www.semanticscholar.org/paper/317eaf94573857bec786bbf030605ccdb0fd624d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1710.06236\",\"authors\":[{\"authorId\":\"6873935\",\"name\":\"T. Lin\"},{\"authorId\":\"1758267\",\"name\":\"X. Zhao\"},{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"}],\"doi\":\"10.1145/3123266.3123343\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4fdbe0c627eb374fa9f347157ae94c5a2fcad67\",\"title\":\"Single Shot Temporal Action Detection\",\"url\":\"https://www.semanticscholar.org/paper/b4fdbe0c627eb374fa9f347157ae94c5a2fcad67\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wei Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Learning spatiotemporal features with 3 d convolutional networks A closer look at spatiotemporal convolutions for action recognition Action recognition with improved trajectories\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1703.07814\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2017.617\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f8324dda6261ec293e9705c13a0e96b9ab63474\",\"title\":\"R-C3D: Region Convolutional 3D Network for Temporal Activity Detection\",\"url\":\"https://www.semanticscholar.org/paper/7f8324dda6261ec293e9705c13a0e96b9ab63474\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1807.08333\",\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":null,\"name\":\"Hang Gao\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"1789840\",\"name\":\"K. Miyazawa\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1007/978-3-030-01270-0_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"523909d26ea94eaa0dd0285ba6ea0cd00a0aa7ca\",\"title\":\"AutoLoc: Weakly-Supervised Temporal Action Localization in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/523909d26ea94eaa0dd0285ba6ea0cd00a0aa7ca\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1712.08416\",\"authors\":[{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\"},{\"authorId\":\"2699340\",\"name\":\"Yiyi Liao\"},{\"authorId\":\"3349249\",\"name\":\"Fatma G\\u00fcney\"},{\"authorId\":\"2745026\",\"name\":\"V. Jampani\"},{\"authorId\":\"150013821\",\"name\":\"A. Geiger\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1007/978-3-030-12939-2_20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4990d5eff926fb9dadceef5bb79fa1932904eeb\",\"title\":\"On the Integration of Optical Flow and Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/b4990d5eff926fb9dadceef5bb79fa1932904eeb\",\"venue\":\"GCPR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"144983134\",\"name\":\"J. Chan\"},{\"authorId\":\"1789840\",\"name\":\"K. Miyazawa\"},{\"authorId\":\"145730148\",\"name\":\"Hassan Mansour\"},{\"authorId\":\"1690385\",\"name\":\"A. Vetro\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d5d85a741f7926d8774e037c57a245ae6c94356\",\"title\":\"Online Action Detection in Untrimmed, Streaming Videos - Modeling and Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/2d5d85a741f7926d8774e037c57a245ae6c94356\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1817030\",\"name\":\"Saining Xie\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"144035504\",\"name\":\"Zhuowen Tu\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fa0d73b8ba114578744c2ebaf610d2ca9694f45\",\"title\":\"Rethinking Spatiotemporal Feature Learning For Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/4fa0d73b8ba114578744c2ebaf610d2ca9694f45\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3407277\",\"name\":\"Adam Paszke\"},{\"authorId\":\"39793298\",\"name\":\"S. Gross\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"114250963\",\"name\":\"G. Chanan\"},{\"authorId\":\"50064334\",\"name\":\"E. Yang\"},{\"authorId\":\"81505016\",\"name\":\"Zachary Devito\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"3050846\",\"name\":\"Alban Desmaison\"},{\"authorId\":\"3029482\",\"name\":\"L. Antiga\"},{\"authorId\":\"1977806\",\"name\":\"A. Lerer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b36a5bb1707bb9c70025294b3a310138aae8327a\",\"title\":\"Automatic differentiation in PyTorch\",\"url\":\"https://www.semanticscholar.org/paper/b36a5bb1707bb9c70025294b3a310138aae8327a\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3232265\",\"name\":\"Deqing Sun\"},{\"authorId\":\"145920814\",\"name\":\"S. Roth\"},{\"authorId\":\"69395700\",\"name\":\"J. Lewis\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1007/978-3-540-88690-7_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad61eebde119131f845dc902e1c8f7a4c6d66233\",\"title\":\"Learning Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/ad61eebde119131f845dc902e1c8f7a4c6d66233\",\"venue\":\"ECCV\",\"year\":2008},{\"arxivId\":\"1612.03052\",\"authors\":[{\"authorId\":\"2340579\",\"name\":\"J. Ng\"},{\"authorId\":\"119675494\",\"name\":\"Jonghyun Choi\"},{\"authorId\":\"144660077\",\"name\":\"Jan Neumann\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1109/WACV.2018.00179\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6db73f10084ce6a4186363ea9d7475a9a658a11\",\"title\":\"ActionFlowNet: Learning Motion Representation for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a6db73f10084ce6a4186363ea9d7475a9a658a11\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"1504.06852\",\"authors\":[{\"authorId\":\"1382344214\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"152702479\",\"name\":\"P. Fischer\"},{\"authorId\":\"48105320\",\"name\":\"Eddy Ilg\"},{\"authorId\":\"2880264\",\"name\":\"Philip H\\u00e4usser\"},{\"authorId\":\"3322806\",\"name\":\"Caner Hazirbas\"},{\"authorId\":\"2943639\",\"name\":\"V. Golkov\"},{\"authorId\":\"1715782\",\"name\":\"P. V. D. Smagt\"},{\"authorId\":\"153685345\",\"name\":\"D. Cremers\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/ICCV.2015.316\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2fb5b39428818d7ec8cc78e152e19c21b7db568\",\"title\":\"FlowNet: Learning Optical Flow with Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/c2fb5b39428818d7ec8cc78e152e19c21b7db568\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1849598\",\"name\":\"D. Butler\"},{\"authorId\":\"49820715\",\"name\":\"J. Wulff\"},{\"authorId\":\"2715753\",\"name\":\"G. Stanley\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1007/978-3-642-33783-3_44\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62\",\"title\":\"A Naturalistic Open Source Movie for Optical Flow Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/7d53f0c87c8ab0de6f3e74515e3ffaf3fab40c62\",\"venue\":\"ECCV\",\"year\":2012},{\"arxivId\":\"1705.06950\",\"authors\":[{\"authorId\":\"21028601\",\"name\":\"W. Kay\"},{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"11809518\",\"name\":\"Brian Zhang\"},{\"authorId\":\"38961760\",\"name\":\"Chloe Hillier\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"143740871\",\"name\":\"F. Viola\"},{\"authorId\":\"143897708\",\"name\":\"T. Green\"},{\"authorId\":\"2830305\",\"name\":\"T. Back\"},{\"authorId\":\"1820908\",\"name\":\"A. Natsev\"},{\"authorId\":\"2573615\",\"name\":\"Mustafa Suleyman\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6\",\"title\":\"The Kinetics Human Action Video Dataset\",\"url\":\"https://www.semanticscholar.org/paper/86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1703.10593\",\"authors\":[{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"145599603\",\"name\":\"T. Park\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/ICCV.2017.244\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c43d954cf8133e6254499f3d68e45218067e4941\",\"title\":\"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1311.2901\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":\"10.1007/978-3-319-10590-1_53\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1a2a770d23b4a171fa81de62a78a3deb0588f238\",\"title\":\"Visualizing and Understanding Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/1a2a770d23b4a171fa81de62a78a3deb0588f238\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1608.06993\",\"authors\":[{\"authorId\":\"143983679\",\"name\":\"Gao Huang\"},{\"authorId\":null,\"name\":\"Zhuang Liu\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":\"10.1109/CVPR.2017.243\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5694e46284460a648fe29117cbc55f6c9be3fa3c\",\"title\":\"Densely Connected Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}],\"title\":\"DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition\",\"topics\":[{\"topic\":\"Optical flow\",\"topicId\":\"26430\",\"url\":\"https://www.semanticscholar.org/topic/26430\"},{\"topic\":\"Data compression\",\"topicId\":\"41454\",\"url\":\"https://www.semanticscholar.org/topic/41454\"},{\"topic\":\"Frame (video)\",\"topicId\":\"4108263\",\"url\":\"https://www.semanticscholar.org/topic/4108263\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"},{\"topic\":\"Downstream (software development)\",\"topicId\":\"10055\",\"url\":\"https://www.semanticscholar.org/topic/10055\"},{\"topic\":\"Kinetics Internet Protocol\",\"topicId\":\"3591972\",\"url\":\"https://www.semanticscholar.org/topic/3591972\"},{\"topic\":\"Dynamic Markov compression\",\"topicId\":\"312719\",\"url\":\"https://www.semanticscholar.org/topic/312719\"}],\"url\":\"https://www.semanticscholar.org/paper/d878aac73038c3bc175ccc2b93acc04675f33bbd\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019}\n"