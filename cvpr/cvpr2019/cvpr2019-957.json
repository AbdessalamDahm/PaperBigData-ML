"{\"abstract\":\"This paper attacks the challenging problem of zero-example video retrieval. In such a retrieval paradigm, an end user searches for unlabeled videos by ad-hoc queries described in natural language text with no visual example provided. Given videos as sequences of frames and queries as sequences of words, an effective sequence-to-sequence cross-modal matching is required. The majority of existing methods are concept based, extracting relevant concepts from queries and videos and accordingly establishing associations between the two modalities. In contrast, this paper takes a concept-free approach, proposing a dual deep encoding network that encodes videos and queries into powerful dense representations of their own. Dual encoding is conceptually simple, practically effective and end-to-end. As experiments on three benchmarks, i.e. MSR-VTT, TRECVID 2016 and 2017 Ad-hoc Video Search show, the proposed solution establishes a new state-of-the-art for zero-example video retrieval.\",\"arxivId\":\"1809.06181\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\",\"url\":\"https://www.semanticscholar.org/author/40240283\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\",\"url\":\"https://www.semanticscholar.org/author/9931285\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\",\"url\":\"https://www.semanticscholar.org/author/46200183\"},{\"authorId\":\"134724966\",\"name\":\"Shouling Ji\",\"url\":\"https://www.semanticscholar.org/author/134724966\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\",\"url\":\"https://www.semanticscholar.org/author/143605211\"},{\"authorId\":\"98289428\",\"name\":\"G. Yang\",\"url\":\"https://www.semanticscholar.org/author/98289428\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\",\"url\":\"https://www.semanticscholar.org/author/39742349\"}],\"citationVelocity\":17,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Oleh Onyshchak\"},{\"authorId\":\"2109913\",\"name\":\"Miriam Redi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fb79a0073e8b7eb199162f1b3385ffd438b2396f\",\"title\":\"Image Recommendation for Wikipedia Articles\",\"url\":\"https://www.semanticscholar.org/paper/fb79a0073e8b7eb199162f1b3385ffd438b2396f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50086111\",\"name\":\"K. Niu\"},{\"authorId\":\"39937384\",\"name\":\"Yan Huang\"},{\"authorId\":\"145769446\",\"name\":\"L. Wang\"}],\"doi\":\"10.1145/3394171.3413895\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"98b354ddfa46ccf9fc604c751eb9deebf4c909f8\",\"title\":\"Textual Dependency Embedding for Person Search by Language\",\"url\":\"https://www.semanticscholar.org/paper/98b354ddfa46ccf9fc604c751eb9deebf4c909f8\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2008.02448\",\"authors\":[{\"authorId\":\"51912474\",\"name\":\"Xiaoye Qu\"},{\"authorId\":\"1855095179\",\"name\":\"Pengwei Tang\"},{\"authorId\":\"3008849\",\"name\":\"Zhikang Zhou\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"}],\"doi\":\"10.1145/3394171.3414053\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4d78df0b175e3cbf811e652184f6b1e9d11caca\",\"title\":\"Fine-grained Iterative Attention Network for Temporal Language Localization in Videos\",\"url\":\"https://www.semanticscholar.org/paper/a4d78df0b175e3cbf811e652184f6b1e9d11caca\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2009.10434\",\"authors\":[{\"authorId\":\"49687714\",\"name\":\"Haoyu Tang\"},{\"authorId\":\"2237811\",\"name\":\"Jihua Zhu\"},{\"authorId\":\"38813990\",\"name\":\"M. Liu\"},{\"authorId\":\"40465774\",\"name\":\"Z. Gao\"},{\"authorId\":\"46504200\",\"name\":\"Zhiyong Cheng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d1d28e778c391d07a6c3e7a2e96312fdc53c0ae2\",\"title\":\"Frame-wise Cross-modal Match for Video Moment Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d1d28e778c391d07a6c3e7a2e96312fdc53c0ae2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.06430\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"1466466597\",\"name\":\"Lucas Smaira\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/cvpr42600.2020.00990\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fb40df31aa7177c9d009478479db61c39caebd54\",\"title\":\"End-to-End Learning of Visual Representations From Uncurated Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/fb40df31aa7177c9d009478479db61c39caebd54\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7314814\",\"name\":\"H. Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1818920\",\"name\":\"Zijia Lin\"},{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"1435766877\",\"name\":\"Gu Xiao-peng\"}],\"doi\":\"10.1145/3362065\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d39fb70393e9a17e5556708852829743380eeed\",\"title\":\"ACMNet: Adaptive Confidence Matching Network for Human Behavior Analysis via Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/0d39fb70393e9a17e5556708852829743380eeed\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390706104\",\"name\":\"Jiaxin Wu\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1145/3394171.3413916\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5fe4199042c91f3bed8073eec9c350f81b4cb368\",\"title\":\"Interpretable Embedding for Ad-Hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/5fe4199042c91f3bed8073eec9c350f81b4cb368\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490935563\",\"name\":\"Feifei Zhang\"},{\"authorId\":\"49235537\",\"name\":\"M. Xu\"},{\"authorId\":\"3069077\",\"name\":\"Q. Mao\"},{\"authorId\":\"48258806\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3394171.3413917\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"50bd8474487073851b115385b9ded538d8825bc2\",\"title\":\"Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/50bd8474487073851b115385b9ded538d8825bc2\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"1562588517\",\"name\":\"Jinde Ye\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"1560419017\",\"name\":\"Shanjinwen Yun\"},{\"authorId\":\"153823918\",\"name\":\"Leimin Zhang\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"},{\"authorId\":\"47519958\",\"name\":\"Rui Qian\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eb45a4faba986187bac097f1e73d2eacaad0daf8\",\"title\":\"Renmin University of China and Zhejiang Gongshang University at TRECVID 2019: Learn to Search and Describe Videos\",\"url\":\"https://www.semanticscholar.org/paper/eb45a4faba986187bac097f1e73d2eacaad0daf8\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"2004.04959\",\"authors\":[{\"authorId\":\"1395873384\",\"name\":\"Rui Zhao\"},{\"authorId\":\"84005711\",\"name\":\"Kecheng Zheng\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1109/ICME46284.2020.9102913\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f752bbe2fc42b65671cee9a7032326acf11c90f2\",\"title\":\"Stacked Convolutional Deep Encoding Network For Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f752bbe2fc42b65671cee9a7032326acf11c90f2\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"2004.03815\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"},{\"authorId\":\"153823918\",\"name\":\"Leimin Zhang\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":\"10.1109/TKDE.2019.2947442\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":false,\"paperId\":\"5de14b6b89666ab6ca9fd13e089e9ca18173a046\",\"title\":\"Feature Re-Learning with Data Augmentation for Video Relevance Prediction\",\"url\":\"https://www.semanticscholar.org/paper/5de14b6b89666ab6ca9fd13e089e9ca18173a046\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1de16ce45d9dbfa315f44136923b90b4d37ff0f0\",\"title\":\"RUC_AIM3 at TRECVID 2019: Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/1de16ce45d9dbfa315f44136923b90b4d37ff0f0\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"2003.12299\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"47090142\",\"name\":\"Seunghwan Lee\"},{\"authorId\":\"90802314\",\"name\":\"Yun-cheol Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"26ecc6b8911d5ae683873adb311ce4362416bd5f\",\"title\":\"CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data\",\"url\":\"https://www.semanticscholar.org/paper/26ecc6b8911d5ae683873adb311ce4362416bd5f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"93768875\",\"name\":\"X. Wang\"},{\"authorId\":\"145824830\",\"name\":\"Yali Du\"},{\"authorId\":\"153823918\",\"name\":\"Leimin Zhang\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46324827\",\"name\":\"Miao Zhang\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"}],\"doi\":\"10.1145/3343031.3356053\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b0c166759059395262e45681c8a7f15001bbb12\",\"title\":\"Exploring Content-based Video Relevance for Video Click-Through Rate Prediction\",\"url\":\"https://www.semanticscholar.org/paper/2b0c166759059395262e45681c8a7f15001bbb12\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390538450\",\"name\":\"Xun Yang\"},{\"authorId\":\"49544292\",\"name\":\"Xueliang Liu\"},{\"authorId\":\"2980051\",\"name\":\"Meng Jian\"},{\"authorId\":\"1993659018\",\"name\":\"Xinjian Gao\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1145/3394171.3413610\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b839e6b1f65672d154feb6f6668d64a2333c71ee\",\"title\":\"Weakly-Supervised Video Object Grounding by Exploring Spatio-Temporal Contexts\",\"url\":\"https://www.semanticscholar.org/paper/b839e6b1f65672d154feb6f6668d64a2333c71ee\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"47470215\",\"name\":\"J. Cao\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"},{\"authorId\":\"145789911\",\"name\":\"Gang Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bbb4a5344dd5fb69d007014939307838b218a8ce\",\"title\":\"Renmin University of China and Zhejiang Gongshang University at TRECVID 2018: Deep Cross-Modal Embeddings for Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/bbb4a5344dd5fb69d007014939307838b218a8ce\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1780348\",\"name\":\"M. Tran\"},{\"authorId\":\"7213584\",\"name\":\"Thanh-An Nguyen\"},{\"authorId\":\"1737086259\",\"name\":\"Quoc-Cuong Tran\"},{\"authorId\":\"35505989\",\"name\":\"Mai-Khiem Tran\"},{\"authorId\":\"47742564\",\"name\":\"Khanh Nguyen\"},{\"authorId\":\"7736164\",\"name\":\"Van-Tu Ninh\"},{\"authorId\":\"38994364\",\"name\":\"Tu-Khiem Le\"},{\"authorId\":\"1738575578\",\"name\":\"Hoang-Phuc Trang-Trung\"},{\"authorId\":\"152328643\",\"name\":\"Hoang-Anh Le\"},{\"authorId\":\"1737836940\",\"name\":\"Hai-Dang Nguyen\"},{\"authorId\":\"51128412\",\"name\":\"Trong-Le Do\"},{\"authorId\":\"1410455580\",\"name\":\"Viet-Khoa Vo-Ho\"},{\"authorId\":\"1737981\",\"name\":\"C. Gurrin\"}],\"doi\":\"10.1145/3379172.3391726\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42b619de2a0887d6e86d6e520cbf607e58e0f50a\",\"title\":\"FIRST - Flexible Interactive Retrieval SysTem for Visual Lifelog Exploration at LSC 2020\",\"url\":\"https://www.semanticscholar.org/paper/42b619de2a0887d6e86d6e520cbf607e58e0f50a\",\"venue\":\"LSC@ICMR\",\"year\":2020},{\"arxivId\":\"2006.08889\",\"authors\":[{\"authorId\":\"1751482331\",\"name\":\"Zerun Feng\"},{\"authorId\":\"7801828\",\"name\":\"Zhimin Zeng\"},{\"authorId\":\"47932273\",\"name\":\"Caili Guo\"},{\"authorId\":\"2123874\",\"name\":\"Z. Li\"}],\"doi\":\"10.24963/ijcai.2020/140\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1ab464a59bbe790ffa33d028d72b3a20cfe89d9f\",\"title\":\"Exploiting Visual Semantic Reasoning for Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/1ab464a59bbe790ffa33d028d72b3a20cfe89d9f\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2011.00597\",\"authors\":[{\"authorId\":\"2007582232\",\"name\":\"Simon Ging\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"1835025\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80089ad641bae28b0e57771afef181b60011069e\",\"title\":\"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/80089ad641bae28b0e57771afef181b60011069e\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1908.03477\",\"authors\":[{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"},{\"authorId\":\"2295553\",\"name\":\"Diane Larlus\"},{\"authorId\":\"1808423\",\"name\":\"G. Csurka\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":\"10.1109/ICCV.2019.00054\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee3ea76cc54ffae2cdd9ef0e91e05f39c9810a15\",\"title\":\"Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/ee3ea76cc54ffae2cdd9ef0e91e05f39c9810a15\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2004.07967\",\"authors\":[{\"authorId\":\"144810262\",\"name\":\"H. M. Nguyen\"},{\"authorId\":\"3388392\",\"name\":\"T. Miyazaki\"},{\"authorId\":\"2137463\",\"name\":\"Y. Sugaya\"},{\"authorId\":\"1740235\",\"name\":\"S. Omachi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"20b15ddf9efe511961321355bcb4a62c3a29a9ef\",\"title\":\"Multiple Visual-Semantic Embedding for Video Retrieval from Query Sentence\",\"url\":\"https://www.semanticscholar.org/paper/20b15ddf9efe511961321355bcb4a62c3a29a9ef\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.11009\",\"authors\":[{\"authorId\":\"47424372\",\"name\":\"Yu Xiong\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":\"10357054\",\"name\":\"L. Guo\"},{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/ICCV.2019.00469\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"87699cff38982712ddb0b2349313077779a5d0ff\",\"title\":\"A Graph-Based Framework to Bridge Movies and Synopses\",\"url\":\"https://www.semanticscholar.org/paper/87699cff38982712ddb0b2349313077779a5d0ff\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2006.16228\",\"authors\":[{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"145721402\",\"name\":\"Ros\\u00e1lia G. Schneider\"},{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"16092809\",\"name\":\"Jason Ramapuram\"},{\"authorId\":\"3364908\",\"name\":\"J. Fauw\"},{\"authorId\":\"1466466597\",\"name\":\"Lucas Smaira\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4174f03c7d8d9add62ae4ecd0ec90efba680b7ae\",\"title\":\"Self-Supervised MultiModal Versatile Networks\",\"url\":\"https://www.semanticscholar.org/paper/4174f03c7d8d9add62ae4ecd0ec90efba680b7ae\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"98289428\",\"name\":\"G. Yang\"},{\"authorId\":\"8157255\",\"name\":\"Zhineng Chen\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"}],\"doi\":\"10.1145/3343031.3350906\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fa52f285eeb28d9eb25ad70a04df17b36a0fd664\",\"title\":\"W2VV++: Fully Deep Learning for Ad-hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/fa52f285eeb28d9eb25ad70a04df17b36a0fd664\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2007.07306\",\"authors\":[{\"authorId\":\"3313330\",\"name\":\"Gedas Bertasius\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c26974821644f8ae9ab2619c2b2725136214330d\",\"title\":\"COBE: Contextualized Object Embeddings from Narrated Instructional Video\",\"url\":\"https://www.semanticscholar.org/paper/c26974821644f8ae9ab2619c2b2725136214330d\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"}],\"doi\":\"10.1145/3372278.3390737\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"bf89a2c1c58bcd5aefd1bb08d9773a33084867f0\",\"title\":\"Attention Mechanisms, Signal Encodings and Fusion Strategies for Improved Ad-hoc Video Search with Dual Encoding Networks\",\"url\":\"https://www.semanticscholar.org/paper/bf89a2c1c58bcd5aefd1bb08d9773a33084867f0\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1692966\",\"name\":\"J. Choi\"},{\"authorId\":\"104810482\",\"name\":\"M. Larson\"},{\"authorId\":\"1452353442\",\"name\":\"Gerald Friedland\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"}],\"doi\":\"10.1109/BigMM.2019.00-48\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7759d2df9310c44d2cb4da133b7035b9e83f33b0\",\"title\":\"From Intra-Modal to Inter-Modal Space: Multi-task Learning of Shared Representations for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/7759d2df9310c44d2cb4da133b7035b9e83f33b0\",\"venue\":\"2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)\",\"year\":2019},{\"arxivId\":\"2011.12091\",\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"49490337\",\"name\":\"F. Zhou\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"47768909\",\"name\":\"Jiaqi Ji\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"}],\"doi\":\"10.1109/tmm.2020.3042067\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d6220c46f381c48a5033dc2cd80ea276d9b5c46\",\"title\":\"SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries\",\"url\":\"https://www.semanticscholar.org/paper/7d6220c46f381c48a5033dc2cd80ea276d9b5c46\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150291092\",\"name\":\"Stelios Andreadis\"},{\"authorId\":\"2559834\",\"name\":\"A. Moumtzidou\"},{\"authorId\":\"2261494\",\"name\":\"K. Apostolidis\"},{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"2675084\",\"name\":\"E. Michail\"},{\"authorId\":\"1988554\",\"name\":\"Ilias Gialampoukidis\"},{\"authorId\":\"1381295446\",\"name\":\"S. Vrochidis\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"},{\"authorId\":\"1715604\",\"name\":\"Y. Kompatsiaris\"}],\"doi\":\"10.1007/978-3-030-37734-2_69\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4c64613c8d2bb192c4ff177e5f3c767fd5b80d2\",\"title\":\"VERGE in VBS 2020\",\"url\":\"https://www.semanticscholar.org/paper/b4c64613c8d2bb192c4ff177e5f3c767fd5b80d2\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":\"2008.08716\",\"authors\":[{\"authorId\":\"152666334\",\"name\":\"Sudipta Paul\"},{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2efdeaa68e94eeef2831536cdd53f3e3868bbdca\",\"title\":\"Text-based Localization of Moments in a Video Corpus\",\"url\":\"https://www.semanticscholar.org/paper/2efdeaa68e94eeef2831536cdd53f3e3868bbdca\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693655\",\"name\":\"Jakub Loko\\u010d\"},{\"authorId\":\"40037607\",\"name\":\"T. Soucek\"},{\"authorId\":\"1474228023\",\"name\":\"P. Vesel\\u00fd\"},{\"authorId\":\"1474227937\",\"name\":\"Frantisek Mejzl\\u00edk\"},{\"authorId\":\"47768909\",\"name\":\"Jiaqi Ji\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":\"10.1145/3394171.3414002\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"f0b056a88198c32baa48a0497efe117c8b704d3c\",\"title\":\"A W2VV++ Case Study with Automated and Interactive Text-to-Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f0b056a88198c32baa48a0497efe117c8b704d3c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642269\",\"name\":\"Da Chen\"},{\"authorId\":\"38273401\",\"name\":\"X. Wu\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\"},{\"authorId\":\"49930315\",\"name\":\"H. Xue\"},{\"authorId\":\"1505834084\",\"name\":\"Feng Mao\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054195\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c49aea12dd3d9478e3139c6d17fb6405b8f043a\",\"title\":\"Hierarchical Sequence Representation with Graph Network\",\"url\":\"https://www.semanticscholar.org/paper/3c49aea12dd3d9478e3139c6d17fb6405b8f043a\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51243809\",\"name\":\"Yining Lang\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\"},{\"authorId\":\"145338228\",\"name\":\"Fan Yang\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"1645209767\",\"name\":\"H. Xue\"}],\"doi\":\"10.1109/CVPR42600.2020.00267\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e62b18e358282c22260fdf7d309f7d7dbf5e9eb7\",\"title\":\"Which Is Plagiarism: Fashion Image Retrieval Based on Regional Representation for Design Protection\",\"url\":\"https://www.semanticscholar.org/paper/e62b18e358282c22260fdf7d309f7d7dbf5e9eb7\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38273401\",\"name\":\"X. Wu\"},{\"authorId\":\"144395119\",\"name\":\"D. Chen\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\"},{\"authorId\":\"49930315\",\"name\":\"H. Xue\"},{\"authorId\":\"1727111\",\"name\":\"Mingli Song\"},{\"authorId\":\"1505834084\",\"name\":\"Feng Mao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eb8a2f20aff35966e7e40fc5c4e398a6287e0570\",\"title\":\"Hybrid Sequence Encoder for Text Based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/eb8a2f20aff35966e7e40fc5c4e398a6287e0570\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"2001.05691\",\"authors\":[{\"authorId\":\"122460701\",\"name\":\"Tianhao Li\"},{\"authorId\":\"119770705\",\"name\":\"L. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2783c13471060300e62a1eed458d9d2888b5b5be\",\"title\":\"Learning Spatiotemporal Features via Video and Text Pair Discrimination\",\"url\":\"https://www.semanticscholar.org/paper/2783c13471060300e62a1eed458d9d2888b5b5be\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.02426\",\"authors\":[{\"authorId\":\"1684736163\",\"name\":\"Arvind Srinivasan\"},{\"authorId\":\"48736679\",\"name\":\"Aprameya Bharadwaj\"},{\"authorId\":\"1576315010\",\"name\":\"Aveek Saha\"},{\"authorId\":\"153832009\",\"name\":\"S. Natarajan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d6986c4c4b0094f046c8d2352226229c5d41912\",\"title\":\"Graph Based Temporal Aggregation for Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8d6986c4c4b0094f046c8d2352226229c5d41912\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.02503\",\"authors\":[{\"authorId\":\"1390538450\",\"name\":\"Xun Yang\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145014675\",\"name\":\"Yixin Cao\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1145/3397271.3401151\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"3da2ca65b90e841a586f55ec9df36e3c6f000077\",\"title\":\"Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/3da2ca65b90e841a586f55ec9df36e3c6f000077\",\"venue\":\"SIGIR\",\"year\":2020},{\"arxivId\":\"1906.03327\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2019.00272\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9311779489e597315488749ee6c386bfa3f3512e\",\"title\":\"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\"url\":\"https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993663564\",\"name\":\"Mingyao Hong\"},{\"authorId\":\"46439250\",\"name\":\"Guorong Li\"},{\"authorId\":\"3175853\",\"name\":\"Xinfeng Zhang\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1145/3394171.3413517\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2009d525afc02cdc60270669ec760d222ef5bd32\",\"title\":\"Generalized Zero-Shot Video Classification via Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/2009d525afc02cdc60270669ec760d222ef5bd32\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":\"10.1145/3347447.3350565\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c68d3866c181c62fa59b611983dfef9a773c8fc7\",\"title\":\"Deep Learning for Video Retrieval by Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/c68d3866c181c62fa59b611983dfef9a773c8fc7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2010.02824\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144721617\",\"name\":\"Y. Asano\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"145414740\",\"name\":\"J. Henriques\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"title\":\"Support-set bottlenecks for video-text representation learning\",\"url\":\"https://www.semanticscholar.org/paper/78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.09984\",\"authors\":[{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"1926279\",\"name\":\"A. Butt\"},{\"authorId\":\"144175619\",\"name\":\"K. Curtis\"},{\"authorId\":\"2700261\",\"name\":\"Y. Lee\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"1781968\",\"name\":\"A. Godil\"},{\"authorId\":\"123306611\",\"name\":\"A. Delgado\"},{\"authorId\":\"1519100122\",\"name\":\"Jesse Zhang\"},{\"authorId\":\"1564398035\",\"name\":\"Eliot Godard\"},{\"authorId\":\"29928565\",\"name\":\"Lukas L. Diduch\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"},{\"authorId\":\"1562121829\",\"name\":\"Yyette Graham\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1406426956\",\"name\":\"Georges Qu\\u00e9not\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1de1bbe51ab668424f869b814e51c2f5e8368f2b\",\"title\":\"TRECVID 2019: An evaluation campaign to benchmark Video Activity Detection, Video Captioning and Matching, and Video Search & retrieval\",\"url\":\"https://www.semanticscholar.org/paper/1de1bbe51ab668424f869b814e51c2f5e8368f2b\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389284356\",\"name\":\"Kaixu Cui\"},{\"authorId\":\"72217143\",\"name\":\"H. Liu\"},{\"authorId\":null,\"name\":\"Chen Wang\"},{\"authorId\":\"3040310\",\"name\":\"Y. Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6cabf4630bfd14a00045ba2d35c474791420c71f\",\"title\":\"Hybrid Sequence Encoder Of Collaborative Experts For Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6cabf4630bfd14a00045ba2d35c474791420c71f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121544498\",\"name\":\"Z. Li\"},{\"authorId\":\"47932273\",\"name\":\"Caili Guo\"},{\"authorId\":\"143787140\",\"name\":\"B. Yang\"},{\"authorId\":\"1751482331\",\"name\":\"Zerun Feng\"},{\"authorId\":\"38952862\",\"name\":\"Hao Zhang\"}],\"doi\":\"10.1109/ICME46284.2020.9102760\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf1362088de663d0848fdff21af09b0c0920581e\",\"title\":\"A Novel Convolutional Architecture For Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/cf1362088de663d0848fdff21af09b0c0920581e\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"2011.10132\",\"authors\":[{\"authorId\":\"51218991\",\"name\":\"Sisi Qu\"},{\"authorId\":\"150234800\",\"name\":\"Mattia Soldan\"},{\"authorId\":\"97375393\",\"name\":\"Mengmeng Xu\"},{\"authorId\":\"1402915033\",\"name\":\"Jesper Tegn\\u00e9r\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"075ce64105cc54cec2b54c24f8f890f2c62d090b\",\"title\":\"VLG-Net: Video-Language Graph Matching Network for Video Grounding\",\"url\":\"https://www.semanticscholar.org/paper/075ce64105cc54cec2b54c24f8f890f2c62d090b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.01403\",\"authors\":[{\"authorId\":\"98579574\",\"name\":\"Daizong Liu\"},{\"authorId\":\"51912474\",\"name\":\"Xiaoye Qu\"},{\"authorId\":\"4029028\",\"name\":\"Xiao-Yang Liu\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"},{\"authorId\":\"2956815\",\"name\":\"Zichuan Xu\"}],\"doi\":\"10.1145/3394171.3414026\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b632209923bfe3452ac19f23b46c70455fae465\",\"title\":\"Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization\",\"url\":\"https://www.semanticscholar.org/paper/2b632209923bfe3452ac19f23b46c70455fae465\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993602785\",\"name\":\"Ruijian Jia\"},{\"authorId\":\"50142011\",\"name\":\"Xinsheng Wang\"},{\"authorId\":\"2852872\",\"name\":\"S. Pang\"},{\"authorId\":\"2237811\",\"name\":\"Jihua Zhu\"},{\"authorId\":\"40689776\",\"name\":\"J. Xue\"}],\"doi\":\"10.1145/3394171.3414023\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4955fda9e96f97e0dcd45c846ec52cb118d89060\",\"title\":\"Look, Listen and Infer\",\"url\":\"https://www.semanticscholar.org/paper/4955fda9e96f97e0dcd45c846ec52cb118d89060\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2009.05381\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"title\":\"Hybrid Space Learning for Language-based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.09183\",\"authors\":[{\"authorId\":\"1381411615\",\"name\":\"Seito Kasai\"},{\"authorId\":\"150348841\",\"name\":\"Yuchi Ishikawa\"},{\"authorId\":\"40495154\",\"name\":\"M. Hayashi\"},{\"authorId\":\"10664005\",\"name\":\"Y. Aoki\"},{\"authorId\":\"2199251\",\"name\":\"K. Hara\"},{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"}],\"doi\":\"10.1109/ICIP40778.2020.9190820\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"63a5b2f09fd2b217fa5a3792b84a78397fc10be4\",\"title\":\"Retrieving and Highlighting Action with Spatiotemporal Reference\",\"url\":\"https://www.semanticscholar.org/paper/63a5b2f09fd2b217fa5a3792b84a78397fc10be4\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020}],\"corpusId\":81982618,\"doi\":\"10.1109/CVPR.2019.00957\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"6a976c123037b138946f6767e1aa0de84b1682d4\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"1926279\",\"name\":\"A. Butt\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"35783968\",\"name\":\"David Joy\"},{\"authorId\":\"74967014\",\"name\":\"A. Delgado\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"},{\"authorId\":\"1562121829\",\"name\":\"Yyette Graham\"},{\"authorId\":\"143723933\",\"name\":\"G. Jones\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1693391\",\"name\":\"G. Qu\\u00e9not\"},{\"authorId\":\"2203861\",\"name\":\"Maria Eskevich\"},{\"authorId\":\"1918199\",\"name\":\"R. Ordelman\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0838dd4c8037b2fc5e9498a8d53797719628048\",\"title\":\"TRECVID 2017: Evaluating Ad-hoc and Instance Video Search, Events Detection, Video Captioning and Hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/a0838dd4c8037b2fc5e9498a8d53797719628048\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"},{\"authorId\":\"40508553\",\"name\":\"Y. Yu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"727e01f52bd6ad0a9778049bb81ffe1437c41070\",\"title\":\"Semantic Concept Discovery for Large-Scale Zero-Shot Event Detection\",\"url\":\"https://www.semanticscholar.org/paper/727e01f52bd6ad0a9778049bb81ffe1437c41070\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118150711\",\"name\":\"J. Liang\"},{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"2314980\",\"name\":\"X. Li\"},{\"authorId\":\"1481734737\",\"name\":\"Lu Jiang\"},{\"authorId\":\"34692532\",\"name\":\"Zhenzhong Lan\"},{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"51299154\",\"name\":\"Jiande Sun\"},{\"authorId\":\"145906066\",\"name\":\"Yang Chen\"},{\"authorId\":\"79327094\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b9ce2f554881a921557f8e9f47e1249c07027c0\",\"title\":\"Informedia @ TRECVID 2016\",\"url\":\"https://www.semanticscholar.org/paper/4b9ce2f554881a921557f8e9f47e1249c07027c0\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":\"1512.02167\",\"authors\":[{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"39402399\",\"name\":\"Yuandong Tian\"},{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"267fb4ac632449dbe84f7acf17c8c7527cb25b0d\",\"title\":\"Simple Baseline for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/267fb4ac632449dbe84f7acf17c8c7527cb25b0d\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21243543\",\"name\":\"Shuang Wu\"},{\"authorId\":\"1828838\",\"name\":\"Sravanthi Bondugula\"},{\"authorId\":\"1689313\",\"name\":\"Florian Luisier\"},{\"authorId\":\"2433508\",\"name\":\"Xiaodan Zhuang\"},{\"authorId\":\"49824581\",\"name\":\"P. Natarajan\"}],\"doi\":\"10.1109/CVPR.2014.341\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16a4e39dfa15e9ae5906a521464e008a24e628b4\",\"title\":\"Zero-Shot Event Detection Using Multi-modal Fusion of Weakly Supervised Concepts\",\"url\":\"https://www.semanticscholar.org/paper/16a4e39dfa15e9ae5906a521464e008a24e628b4\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1611.05431\",\"authors\":[{\"authorId\":\"1817030\",\"name\":\"Saining Xie\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"144035504\",\"name\":\"Zhuowen Tu\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"}],\"doi\":\"10.1109/CVPR.2017.634\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f6e0856b4a9199fa968ac00da612a9407b5cb85c\",\"title\":\"Aggregated Residual Transformations for Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f6e0856b4a9199fa968ac00da612a9407b5cb85c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1707.05612\",\"authors\":[{\"authorId\":\"2978170\",\"name\":\"Fartash Faghri\"},{\"authorId\":\"1793739\",\"name\":\"David J. Fleet\"},{\"authorId\":\"51131802\",\"name\":\"J. Kiros\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"faa093a53b83f0e9c35a0bfbcacee0a16f8eb6d1\",\"title\":\"VSE++: Improved Visual-Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/faa093a53b83f0e9c35a0bfbcacee0a16f8eb6d1\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145269114\",\"name\":\"Jeffrey Dalton\"},{\"authorId\":\"144890574\",\"name\":\"James Allan\"},{\"authorId\":\"2046556\",\"name\":\"Pranav Mirajkar\"}],\"doi\":\"10.1145/2505515.2507880\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c323be9c750978b603048db02d759bb617bae003\",\"title\":\"Zero-shot video retrieval using content and concepts\",\"url\":\"https://www.semanticscholar.org/paper/c323be9c750978b603048db02d759bb617bae003\",\"venue\":\"CIKM\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"2378908\",\"name\":\"K. Ioannidis\"},{\"authorId\":\"1381295446\",\"name\":\"S. Vrochidis\"},{\"authorId\":\"1715604\",\"name\":\"Y. Kompatsiaris\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2fa40ad7b87549ad2f2fa39b1d421a4ff50aaf99\",\"title\":\"ITI-CERTH participation in TRECVID 2018\",\"url\":\"https://www.semanticscholar.org/paper/2fa40ad7b87549ad2f2fa39b1d421a4ff50aaf99\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"48099761\",\"name\":\"K. Paliwal\"}],\"doi\":\"10.1109/78.650093\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e23c34414e66118ecd9b08cf0cd4d016f59b0b85\",\"title\":\"Bidirectional recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/e23c34414e66118ecd9b08cf0cd4d016f59b0b85\",\"venue\":\"IEEE Trans. Signal Process.\",\"year\":1997},{\"arxivId\":\"1408.5882\",\"authors\":[{\"authorId\":\"38367242\",\"name\":\"Yoon Kim\"}],\"doi\":\"10.3115/v1/D14-1181\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba\",\"title\":\"Convolutional Neural Networks for Sentence Classification\",\"url\":\"https://www.semanticscholar.org/paper/1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"title\":\"Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework\",\"url\":\"https://www.semanticscholar.org/paper/1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40501192\",\"name\":\"Yi-Jie Lu\"},{\"authorId\":\"1682058\",\"name\":\"H. Zhang\"},{\"authorId\":\"144217107\",\"name\":\"M. D. Boer\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1145/2911996.2912015\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d5b16ee7a6312c34c5ed3da93f927a61e7013f3\",\"title\":\"Event Detection with Zero Example: Select the Right and Suppress the Wrong Concepts\",\"url\":\"https://www.semanticscholar.org/paper/6d5b16ee7a6312c34c5ed3da93f927a61e7013f3\",\"venue\":\"ICMR\",\"year\":2016},{\"arxivId\":\"1301.3781\",\"authors\":[{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"38644044\",\"name\":\"Kai Chen\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"330da625c15427c6e42ccfa3b747fb29e5835bf0\",\"title\":\"Efficient Estimation of Word Representations in Vector Space\",\"url\":\"https://www.semanticscholar.org/paper/330da625c15427c6e42ccfa3b747fb29e5835bf0\",\"venue\":\"ICLR\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"145297531\",\"name\":\"A. Lai\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1162/tacl_a_00166\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44040913380206991b1991daf1192942e038fe31\",\"title\":\"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\"url\":\"https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143804033\",\"name\":\"K. Ueki\"},{\"authorId\":\"34711903\",\"name\":\"Y. Nakagome\"},{\"authorId\":\"14410793\",\"name\":\"Koji Hirakawa\"},{\"authorId\":\"144177702\",\"name\":\"K. Kikuchi\"},{\"authorId\":\"32639661\",\"name\":\"Yoshihiko Hayashi\"},{\"authorId\":\"3279652\",\"name\":\"T. Ogawa\"},{\"authorId\":\"1709528\",\"name\":\"T. Kobayashi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0fdd5f7170128769904cf49d7f0f7836f5005629\",\"title\":\"Waseda_Meisei at TRECVID 2018: Ad-hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/0fdd5f7170128769904cf49d7f0f7836f5005629\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3000952\",\"name\":\"A. Habibian\"},{\"authorId\":\"1722052\",\"name\":\"Thomas Mensink\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1145/2578726.2578746\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"666e4da5bad5c4895952246895b64441f08f7a09\",\"title\":\"Composite Concept Discovery for Zero-Shot Video Event Detection\",\"url\":\"https://www.semanticscholar.org/paper/666e4da5bad5c4895952246895b64441f08f7a09\",\"venue\":\"ICMR\",\"year\":2014},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2559834\",\"name\":\"A. Moumtzidou\"},{\"authorId\":\"47381330\",\"name\":\"A. Dimou\"},{\"authorId\":\"1827419\",\"name\":\"Nikolaos Gkalelis\"},{\"authorId\":\"3019137\",\"name\":\"S. Vrochidis\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"},{\"authorId\":\"1715604\",\"name\":\"Y. Kompatsiaris\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1344ea042ebccc4503ca2a864474fea997ae692d\",\"title\":\"ITI-CERTH participation to TRECVID 2015\",\"url\":\"https://www.semanticscholar.org/paper/1344ea042ebccc4503ca2a864474fea997ae692d\",\"venue\":\"TRECVID\",\"year\":2015},{\"arxivId\":\"1511.02492\",\"authors\":[{\"authorId\":\"3000952\",\"name\":\"A. Habibian\"},{\"authorId\":\"1722052\",\"name\":\"Thomas Mensink\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1109/TPAMI.2016.2627563\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"de7bdc64e75f008efd9b25fdc5250f528757a698\",\"title\":\"Video2vec Embeddings Recognize Events When Examples Are Scarce\",\"url\":\"https://www.semanticscholar.org/paper/de7bdc64e75f008efd9b25fdc5250f528757a698\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2376208\",\"name\":\"Fotini Markatopoulou\"},{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"},{\"authorId\":\"50058816\",\"name\":\"I. Patras\"}],\"doi\":\"10.1145/3078971.3079041\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"89de178c9f8306f2bf64040a6609eefb8cb2fd49\",\"title\":\"Query and Keyframe Representations for Ad-hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/89de178c9f8306f2bf64040a6609eefb8cb2fd49\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"47470215\",\"name\":\"J. Cao\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"},{\"authorId\":\"145789911\",\"name\":\"Gang Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bbb4a5344dd5fb69d007014939307838b218a8ce\",\"title\":\"Renmin University of China and Zhejiang Gongshang University at TRECVID 2018: Deep Cross-Modal Embeddings for Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/bbb4a5344dd5fb69d007014939307838b218a8ce\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2411436\",\"name\":\"Martin Klinkigt\"},{\"authorId\":\"1802416\",\"name\":\"D. Le\"},{\"authorId\":\"2687076\",\"name\":\"A. Hiroike\"},{\"authorId\":\"3417350\",\"name\":\"Hung Quoc Vo\"},{\"authorId\":\"1564010116\",\"name\":\"Mohit Chabra\"},{\"authorId\":\"1471395364\",\"name\":\"Vu-Minh-Hieu Dang\"},{\"authorId\":\"1557386872\",\"name\":\"Quan Kong\"},{\"authorId\":\"34453615\",\"name\":\"V. Nguyen\"},{\"authorId\":\"2668511\",\"name\":\"T. Murakami\"},{\"authorId\":\"1561042963\",\"name\":\"Tien-Van Do\"},{\"authorId\":\"32710145\",\"name\":\"Tomoaki Yoshinaga\"},{\"authorId\":\"1560660589\",\"name\":\"Duy-Nhat Nguyen\"},{\"authorId\":\"1564000905\",\"name\":\"Sinha Saptarshi\"},{\"authorId\":\"3080041\",\"name\":\"Thanh Duc Ngo\"},{\"authorId\":\"1563939623\",\"name\":\"Charles Limasanches\"},{\"authorId\":\"48447688\",\"name\":\"Tushar Agrawal\"},{\"authorId\":\"66535001\",\"name\":\"J. Vora\"},{\"authorId\":\"147577950\",\"name\":\"Manikandan Ravikiran\"},{\"authorId\":null,\"name\":\"Zheng Wang\"},{\"authorId\":\"144404414\",\"name\":\"S. Satoh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b452ac0c1f6f189cbdeaac48d1d89621a3560f63\",\"title\":\"NII-HITACHI-UIT at TRECVID 2017\",\"url\":\"https://www.semanticscholar.org/paper/b452ac0c1f6f189cbdeaac48d1d89621a3560f63\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":\"1709.01362\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1109/TMM.2018.2832602\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"031af500679ae544d0fc614f938de45a07c87c82\",\"title\":\"Predicting Visual Features From Text for Image and Video Caption Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/031af500679ae544d0fc614f938de45a07c87c82\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39978626\",\"name\":\"Lu Jiang\"},{\"authorId\":\"26957307\",\"name\":\"Deyu Meng\"},{\"authorId\":\"1706595\",\"name\":\"T. Mitamura\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/2647868.2654918\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"934b09cd0a3bc008435d9c509915d6842bc59aa4\",\"title\":\"Easy Samples First: Self-paced Reranking for Zero-Example Multimedia Search\",\"url\":\"https://www.semanticscholar.org/paper/934b09cd0a3bc008435d9c509915d6842bc59aa4\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32860700\",\"name\":\"P. Nguyen\"},{\"authorId\":\"145138436\",\"name\":\"Qing Li\"},{\"authorId\":\"3493929\",\"name\":\"Zhi-Qi Cheng\"},{\"authorId\":\"40501192\",\"name\":\"Yi-Jie Lu\"},{\"authorId\":\"71777847\",\"name\":\"H. Zhang\"},{\"authorId\":\"1772198\",\"name\":\"X. Wu\"},{\"authorId\":\"152650698\",\"name\":\"Chong-Wah Ngo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3bf2e9704e49b9ca57794bb54e29997f3bebbe6a\",\"title\":\"VIREO @ TRECVID 2017: Video-to-Text, Ad-hoc Video Search, and Video hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/3bf2e9704e49b9ca57794bb54e29997f3bebbe6a\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"26832002\",\"name\":\"X. Wang\"},{\"authorId\":\"24332496\",\"name\":\"Q. Wei\"},{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"2304222\",\"name\":\"E. Gavves\"},{\"authorId\":\"13142264\",\"name\":\"Noureldien Hussein\"},{\"authorId\":\"1769315\",\"name\":\"D. Koelma\"},{\"authorId\":\"144638781\",\"name\":\"A. Smeulders\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d7bbd75e9471dbcb20a04043f8156cb967567f3f\",\"title\":\"University of Amsterdam and Renmin University at TRECVID 2016: Searching Video, Detecting Events and Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d7bbd75e9471dbcb20a04043f8156cb967567f3f\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1808.02559\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"153188107\",\"name\":\"Jongseok Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1007/978-3-030-01234-2_29\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8befcd91c24038e5c26df0238d26e2311b21719a\",\"title\":\"A Joint Sequence Fusion Model for Video Question Answering and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8befcd91c24038e5c26df0238d26e2311b21719a\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1608.06993\",\"authors\":[{\"authorId\":\"143983679\",\"name\":\"Gao Huang\"},{\"authorId\":null,\"name\":\"Zhuang Liu\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":\"10.1109/CVPR.2017.243\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5694e46284460a648fe29117cbc55f6c9be3fa3c\",\"title\":\"Densely Connected Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"3428237\",\"name\":\"Juncheng Billy Li\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"2968713\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1145/3206025.3206064\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9dbca9da6a72ba3739813288b677888a6cf76272\",\"title\":\"Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/9dbca9da6a72ba3739813288b677888a6cf76272\",\"venue\":\"ICMR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"4770950\",\"name\":\"D. Joy\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1406426956\",\"name\":\"Georges Qu\\u00e9not\"},{\"authorId\":\"2203861\",\"name\":\"Maria Eskevich\"},{\"authorId\":\"144597794\",\"name\":\"R. Aly\"},{\"authorId\":\"32488932\",\"name\":\"Roeland Ordelman\"},{\"authorId\":\"51091173\",\"name\":\"M. Ritter\"},{\"authorId\":\"143723939\",\"name\":\"G. Jones\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"},{\"authorId\":\"104810482\",\"name\":\"M. Larson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c10c5d3fd3575570ec55ddbe838fb2a52cf97bc9\",\"title\":\"TRECVID 2016: Evaluating Video Search, Video Event Detection, Localization, and Hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/c10c5d3fd3575570ec55ddbe838fb2a52cf97bc9\",\"venue\":\"TRECVID\",\"year\":2016}],\"title\":\"Dual Encoding for Zero-Example Video Retrieval\",\"topics\":[{\"topic\":\"Dual\",\"topicId\":\"28901\",\"url\":\"https://www.semanticscholar.org/topic/28901\"},{\"topic\":\"Arabic numeral 0\",\"topicId\":\"104427\",\"url\":\"https://www.semanticscholar.org/topic/104427\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Question answering\",\"topicId\":\"18817\",\"url\":\"https://www.semanticscholar.org/topic/18817\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Modal logic\",\"topicId\":\"61528\",\"url\":\"https://www.semanticscholar.org/topic/61528\"},{\"topic\":\"Hoc (programming language)\",\"topicId\":\"3446\",\"url\":\"https://www.semanticscholar.org/topic/3446\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Programming paradigm\",\"topicId\":\"29522\",\"url\":\"https://www.semanticscholar.org/topic/29522\"},{\"topic\":\"Frame (physical object)\",\"topicId\":\"4537\",\"url\":\"https://www.semanticscholar.org/topic/4537\"},{\"topic\":\"Mental association\",\"topicId\":\"250316\",\"url\":\"https://www.semanticscholar.org/topic/250316\"},{\"topic\":\"videocassette\",\"topicId\":\"20721\",\"url\":\"https://www.semanticscholar.org/topic/20721\"},{\"topic\":\"MATCHING\",\"topicId\":\"516219\",\"url\":\"https://www.semanticscholar.org/topic/516219\"},{\"topic\":\"Numerous\",\"topicId\":\"16861\",\"url\":\"https://www.semanticscholar.org/topic/16861\"}],\"url\":\"https://www.semanticscholar.org/paper/6a976c123037b138946f6767e1aa0de84b1682d4\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019}\n"