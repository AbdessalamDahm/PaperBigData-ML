"{\"abstract\":\"With the recent popularity of animated GIFs on social media, there is need for ways to index them with rich meta-data. To advance research on animated GIF understanding, we collected a new dataset, Tumblr GIF (TGIF), with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing. The motivation for this work is to develop a testbed for image sequence description systems, where the task is to generate natural language descriptions for animated GIFs or video clips. To ensure a high quality dataset, we developed a series of novel quality controls to validate free-form text input from crowd-workers. We show that there is unambiguous association between visual content and natural language descriptions in our dataset, making it an ideal benchmark for the visual content captioning task. We perform extensive statistical analyses to compare our dataset to existing image and video description datasets. Next, we provide baseline results on the animated GIF description task, using three representative techniques: nearest neighbor, statistical machine translation, and recurrent neural networks. Finally, we show that models fine-tuned from our animated GIF description dataset can be helpful for automatic movie description.\",\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\",\"url\":\"https://www.semanticscholar.org/author/66508219\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\",\"url\":\"https://www.semanticscholar.org/author/2317183\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\",\"url\":\"https://www.semanticscholar.org/author/48749954\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\",\"url\":\"https://www.semanticscholar.org/author/1739099\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\",\"url\":\"https://www.semanticscholar.org/author/39420932\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\",\"url\":\"https://www.semanticscholar.org/author/144633617\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\",\"url\":\"https://www.semanticscholar.org/author/33642939\"}],\"citationVelocity\":22,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"7381623\",\"name\":\"Dongkun Lee\"},{\"authorId\":\"145530103\",\"name\":\"Ho-Jin Choi\"}],\"doi\":\"10.1109/BigComp.2018.00134\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de6744233a657313b374f3ec2550f73201239980\",\"title\":\"Text to Game Characterization: A Starting Point for Generative Adversarial Video Composition\",\"url\":\"https://www.semanticscholar.org/paper/de6744233a657313b374f3ec2550f73201239980\",\"venue\":\"2018 IEEE International Conference on Big Data and Smart Computing (BigComp)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693655\",\"name\":\"Jakub Loko\\u010d\"},{\"authorId\":\"40037607\",\"name\":\"T. Soucek\"},{\"authorId\":\"1474228023\",\"name\":\"P. Vesel\\u00fd\"},{\"authorId\":\"1474227937\",\"name\":\"Frantisek Mejzl\\u00edk\"},{\"authorId\":\"47768909\",\"name\":\"Jiaqi Ji\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":\"10.1145/3394171.3414002\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0b056a88198c32baa48a0497efe117c8b704d3c\",\"title\":\"A W2VV++ Case Study with Automated and Interactive Text-to-Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f0b056a88198c32baa48a0497efe117c8b704d3c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2416019\",\"name\":\"Lin Yuan\"}],\"doi\":\"10.5075/EPFL-THESIS-7828\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7d64f6c88623acd53c7aff9d6062f749a464325\",\"title\":\"Privacy-Friendly Photo Sharing and Relevant Applications Beyond\",\"url\":\"https://www.semanticscholar.org/paper/f7d64f6c88623acd53c7aff9d6062f749a464325\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1708.09667\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3123420\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6199348281e14a5a127b539f5cdb92fcddbac17\",\"title\":\"Video Captioning with Guidance of Multimodal Latent Topics\",\"url\":\"https://www.semanticscholar.org/paper/a6199348281e14a5a127b539f5cdb92fcddbac17\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51427469\",\"name\":\"Alexander Braylan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efce24608a6c0953fa5782a4c255aa21fc034753\",\"title\":\"Modeling Complex Annotations\",\"url\":\"https://www.semanticscholar.org/paper/efce24608a6c0953fa5782a4c255aa21fc034753\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.03493\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"46255707\",\"name\":\"Lei Li\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00468\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"title\":\"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research\",\"url\":\"https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7212904\",\"name\":\"Jo\\u00ebl Theytaz\"},{\"authorId\":\"153253305\",\"name\":\"Lin Yuan\"},{\"authorId\":\"49201667\",\"name\":\"D. Mcnally\"},{\"authorId\":\"1681498\",\"name\":\"T. Ebrahimi\"}],\"doi\":\"10.1117/12.2240283\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"09e853af6817b4cb36225df13f640dd7d876fd05\",\"title\":\"Towards an animated JPEG\",\"url\":\"https://www.semanticscholar.org/paper/09e853af6817b4cb36225df13f640dd7d876fd05\",\"venue\":\"Optical Engineering + Applications\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":null,\"name\":\"Mohammad Soleymani\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"890935379e6fc3a724d2ef5bb22775bdcfd0888b\",\"title\":\"Supplementary Material for Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/890935379e6fc3a724d2ef5bb22775bdcfd0888b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1707.00836\",\"authors\":[{\"authorId\":\"2593979\",\"name\":\"Kyung-Min Kim\"},{\"authorId\":\"2939188\",\"name\":\"Min-Oh Heo\"},{\"authorId\":\"117172343\",\"name\":\"Seongho Choi\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":\"10.24963/ijcai.2017/280\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e6cc717311c9c3dcf7279bc44e0c25b29650c15\",\"title\":\"DeepStory: Video Story QA by Deep Embedded Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/7e6cc717311c9c3dcf7279bc44e0c25b29650c15\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yang Fan\"},{\"authorId\":\"2073589\",\"name\":\"Jungang Xu\"},{\"authorId\":\"46676156\",\"name\":\"Yingfei Sun\"},{\"authorId\":\"40368776\",\"name\":\"Ben He\"}],\"doi\":\"10.1109/ICTAI.2018.00047\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cdea9a5054f3b5f4dd4c3e75f9278e9548c2de7a\",\"title\":\"Long-Term Recurrent Merge Network Model for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cdea9a5054f3b5f4dd4c3e75f9278e9548c2de7a\",\"venue\":\"2018 IEEE 30th International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1de16ce45d9dbfa315f44136923b90b4d37ff0f0\",\"title\":\"RUC_AIM3 at TRECVID 2019: Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/1de16ce45d9dbfa315f44136923b90b4d37ff0f0\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"title\":\"Video Captioning and Retrieval Models with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26903445\",\"name\":\"H. Mantec\\u00f3n\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"},{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71caf2f74e94cbb1c7fb3250e083a4d6924fb30e\",\"title\":\"PicSOM and EURECOM Experiments in TRECVID 2019 Pre-workshop draft \\u2013 Revision : 0 . 9\",\"url\":\"https://www.semanticscholar.org/paper/71caf2f74e94cbb1c7fb3250e083a4d6924fb30e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2098749\",\"name\":\"Erick Elejalde\"},{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"1718268\",\"name\":\"C. Nieder\\u00e9e\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69267dca2ada17f6f0fc71d4f1f01c84145af5d5\",\"title\":\"Migration-Related Semantic Concepts for the Retrieval of Relevant Video Content\",\"url\":\"https://www.semanticscholar.org/paper/69267dca2ada17f6f0fc71d4f1f01c84145af5d5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1902160423\",\"name\":\"Alison Reboud\"},{\"authorId\":\"1902021703\",\"name\":\"Ismail Harrando\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"},{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"1684267\",\"name\":\"Rapha\\u00ebl Troncy\"},{\"authorId\":\"26903445\",\"name\":\"H. Mantec\\u00f3n\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab0d7b171424ded00acba294e2f349f627c94107\",\"title\":\"Combining Textual and Visual Modeling for Predicting Media Memorability\",\"url\":\"https://www.semanticscholar.org/paper/ab0d7b171424ded00acba294e2f349f627c94107\",\"venue\":\"MediaEval\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"2081160\",\"name\":\"S. Ji\"},{\"authorId\":\"1725522\",\"name\":\"X. Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a7214c15fef26ce470a285ab808a6be92d4193e7\",\"title\":\"Dual Dense Encoding for Zero-Example Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/a7214c15fef26ce470a285ab808a6be92d4193e7\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394475099\",\"name\":\"Beg\\u00fcm \\u00c7itamak\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"}],\"doi\":\"10.1109/SIU.2019.8806555\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"title\":\"MSVD-Turkish: A Large-Scale Dataset for Video Captioning in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"venue\":\"2019 27th Signal Processing and Communications Applications Conference (SIU)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"1562588517\",\"name\":\"Jinde Ye\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"1560419017\",\"name\":\"Shanjinwen Yun\"},{\"authorId\":\"153823918\",\"name\":\"Leimin Zhang\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"},{\"authorId\":\"47519958\",\"name\":\"Rui Qian\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb45a4faba986187bac097f1e73d2eacaad0daf8\",\"title\":\"Renmin University of China and Zhejiang Gongshang University at TRECVID 2019: Learn to Search and Describe Videos\",\"url\":\"https://www.semanticscholar.org/paper/eb45a4faba986187bac097f1e73d2eacaad0daf8\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"31081539\",\"name\":\"Pengpeng Zeng\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"4495301\",\"name\":\"Yuan-Fang Li\"},{\"authorId\":\"1686917\",\"name\":\"Wu Liu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1609/AAAI.V33I01.33016391\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"524879e9a072489110e9578cf2689e50c5531f05\",\"title\":\"Structured Two-Stream Attention Network for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/524879e9a072489110e9578cf2689e50c5531f05\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30332764\",\"name\":\"Luis Lebron\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d44fced35990194ddba63a283dc198e89a63c09d\",\"title\":\"INSIGHT@DCU TRECVID 2019: Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/d44fced35990194ddba63a283dc198e89a63c09d\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"2011.06829\",\"authors\":[{\"authorId\":\"2024715153\",\"name\":\"Elejalde Erick\"},{\"authorId\":\"2024715152\",\"name\":\"Galanopoulos Damianos\"},{\"authorId\":\"2022866326\",\"name\":\"Niederee Claudia\"},{\"authorId\":\"147624914\",\"name\":\"Mezaris Vasileios\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aaf9ee673f85ae4979f1808a5474a200d9317225\",\"title\":\"Migration-Related Semantic Concepts for the Retrieval of Relevant Video Content\",\"url\":\"https://www.semanticscholar.org/paper/aaf9ee673f85ae4979f1808a5474a200d9317225\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123024384\",\"name\":\"Liu Tian-liang\"},{\"authorId\":\"50708017\",\"name\":\"Wan Junwei\"},{\"authorId\":\"71916637\",\"name\":\"Dai Xiu-bin\"},{\"authorId\":\"1734409\",\"name\":\"Feng Liu\"},{\"authorId\":\"36610242\",\"name\":\"Quanzeng You\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/TMM.2019.2936805\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59737509cf1c8d93a33ee7558f6492b7934b236f\",\"title\":\"Sentiment Recognition for Short Annotated GIFs Using Visual-Textual Fusion\",\"url\":\"https://www.semanticscholar.org/paper/59737509cf1c8d93a33ee7558f6492b7934b236f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153185012\",\"name\":\"G. Li\"},{\"authorId\":\"1500519009\",\"name\":\"Ziwei Wang\"},{\"authorId\":null,\"name\":\"Yi Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c5ff92cb44f42fdd3537cb6e6a226465d8e9df9e\",\"title\":\"UTS_CETC_D2DCRC Submission at the TRECVID 2018 Video to Text Description Task\",\"url\":\"https://www.semanticscholar.org/paper/c5ff92cb44f42fdd3537cb6e6a226465d8e9df9e\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":\"1811.10830\",\"authors\":[{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1109/CVPR.2019.00688\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6dfc2ff03534a4325d06c6f88c3144831996629b\",\"title\":\"From Recognition to Cognition: Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/6dfc2ff03534a4325d06c6f88c3144831996629b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1491414917\",\"name\":\"Xin Yan\"},{\"authorId\":\"4004957\",\"name\":\"W. Cheng\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.1145/3366710\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"title\":\"Multichannel Attention Refinement for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1906.03327\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2019.00272\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9311779489e597315488749ee6c386bfa3f3512e\",\"title\":\"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\"url\":\"https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24545031\",\"name\":\"Jiayin Cai\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"},{\"authorId\":\"145280977\",\"name\":\"Cheng Shi\"},{\"authorId\":null,\"name\":\"Lei Li\"},{\"authorId\":\"150347046\",\"name\":\"Yangyang Cheng\"},{\"authorId\":\"1387190008\",\"name\":\"Ying Shan\"}],\"doi\":\"10.24963/ijcai.2020/139\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3bcf4c354f68d5e85ffcc6d69c1348e69d241857\",\"title\":\"Feature Augmented Memory with Global Attention Network for VideoQA\",\"url\":\"https://www.semanticscholar.org/paper/3bcf4c354f68d5e85ffcc6d69c1348e69d241857\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2003.03749\",\"authors\":[{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1109/CVPR42600.2020.01090\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"96485bda4f4118da249cc8a898230281ac8040a7\",\"title\":\"Better Captioning With Sequence-Level Exploration\",\"url\":\"https://www.semanticscholar.org/paper/96485bda4f4118da249cc8a898230281ac8040a7\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"118150711\",\"name\":\"J. Liang\"},{\"authorId\":\"1558681028\",\"name\":\"Jiang Liu\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"145196759\",\"name\":\"Chenqiang Gao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c023a11e4b165672ae65acc3081c5b943163e9df\",\"title\":\"Informedia @ TRECVID 2017\",\"url\":\"https://www.semanticscholar.org/paper/c023a11e4b165672ae65acc3081c5b943163e9df\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"L. Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2712862\",\"name\":\"D. Zhang\"},{\"authorId\":\"145690873\",\"name\":\"R. Cao\"},{\"authorId\":\"1765710\",\"name\":\"Sai Wu\"}],\"doi\":\"10.1016/J.INFFUS.2019.03.005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"91118408f8192c2addade2a0401a32c3bbd47818\",\"title\":\"Information fusion in visual question answering: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/91118408f8192c2addade2a0401a32c3bbd47818\",\"venue\":\"Inf. Fusion\",\"year\":2019},{\"arxivId\":\"1804.01373\",\"authors\":[{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"47740660\",\"name\":\"Jingyuan Chen\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"144188763\",\"name\":\"Jian Yao\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1145/3184558.3186584\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8ba45c4f82b0328dfdf5dc93dfd88e3d9ac76bad\",\"title\":\"Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset\",\"url\":\"https://www.semanticscholar.org/paper/8ba45c4f82b0328dfdf5dc93dfd88e3d9ac76bad\",\"venue\":\"WWW\",\"year\":2018},{\"arxivId\":\"1804.04318\",\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd286954bba554035ea98946afdd02262b9bca45\",\"title\":\"Cross-Modal Retrieval with Implicit Concept Association\",\"url\":\"https://www.semanticscholar.org/paper/cd286954bba554035ea98946afdd02262b9bca45\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1801.01582\",\"authors\":[{\"authorId\":\"2326243\",\"name\":\"Arun Balajee Vasudevan\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/CVPR.2018.00434\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a82d83f818cdc4ac714e468446bc2499ff9caa7\",\"title\":\"Object Referring in Videos with Language and Human Gaze\",\"url\":\"https://www.semanticscholar.org/paper/7a82d83f818cdc4ac714e468446bc2499ff9caa7\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49046603\",\"name\":\"C. Liu\"},{\"authorId\":\"2887672\",\"name\":\"A. Shmilovici\"}],\"doi\":\"10.1007/978-3-030-47124-8_39\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f78ec67e18413fe70e37eee132e7527ffee5ec62\",\"title\":\"Towards Automatic Textual Summarization of Movies\",\"url\":\"https://www.semanticscholar.org/paper/f78ec67e18413fe70e37eee132e7527ffee5ec62\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51427469\",\"name\":\"Alexander Braylan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9796f3a0a3608b44cc27f90dadbeef1e229ba1c9\",\"title\":\"Distance-based Consensus Modeling for Complex Annotations\",\"url\":\"https://www.semanticscholar.org/paper/9796f3a0a3608b44cc27f90dadbeef1e229ba1c9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.24963/ijcai.2019/877\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"title\":\"Deep Learning for Video Captioning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26903445\",\"name\":\"H. Mantec\\u00f3n\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"},{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d0af1363c2d03e5d2435d9ba2b05ca5aecd568fd\",\"title\":\"PicSOM and EURECOM Experiments in TRECVID 2019\",\"url\":\"https://www.semanticscholar.org/paper/d0af1363c2d03e5d2435d9ba2b05ca5aecd568fd\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"32860700\",\"name\":\"P. Nguyen\"},{\"authorId\":\"145880167\",\"name\":\"B. Huet\"},{\"authorId\":\"152650698\",\"name\":\"Chong-Wah Ngo\"}],\"doi\":\"10.1109/ICCVW.2019.00233\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc505fdd23c8771182a601f01df9b1a64bf9ba34\",\"title\":\"Fusion of Multimodal Embeddings for Ad-Hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/cc505fdd23c8771182a601f01df9b1a64bf9ba34\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"35084211\",\"name\":\"M. Lewis\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D18-1117\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"title\":\"A Dataset for Telling the Stories of Social Media Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"2518836\",\"name\":\"Shaoli Huang\"},{\"authorId\":\"7471918\",\"name\":\"Duanqing Xu\"},{\"authorId\":\"145047838\",\"name\":\"Dacheng Tao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c32dc9d1ffb79495571b68e004a199ddac4bc6c5\",\"title\":\"DL-61-86 at TRECVID 2017: Video-to-Text Description\",\"url\":\"https://www.semanticscholar.org/paper/c32dc9d1ffb79495571b68e004a199ddac4bc6c5\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":\"1704.04497\",\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.149\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"title\":\"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1909.02218\",\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"2061528\",\"name\":\"Wenqing Chu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2018.2859820\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"96f0908cc138aceb2d5e0180c440e5adc711d855\",\"title\":\"A Better Way to Attend: Attention With Trees for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/96f0908cc138aceb2d5e0180c440e5adc711d855\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"47470215\",\"name\":\"J. Cao\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"},{\"authorId\":\"145789911\",\"name\":\"Gang Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bbb4a5344dd5fb69d007014939307838b218a8ce\",\"title\":\"Renmin University of China and Zhejiang Gongshang University at TRECVID 2018: Deep Cross-Modal Embeddings for Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/bbb4a5344dd5fb69d007014939307838b218a8ce\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"32860700\",\"name\":\"P. Nguyen\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"},{\"authorId\":\"152650698\",\"name\":\"Chong-Wah Ngo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d898f41a031213247470068c629715fadc120a7e\",\"title\":\"EURECOM at TRECVid AVS 2019\",\"url\":\"https://www.semanticscholar.org/paper/d898f41a031213247470068c629715fadc120a7e\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"94228656\",\"name\":\"Fengda Zhu\"},{\"authorId\":\"144652817\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"9929684\",\"name\":\"Xuanyi Dong\"},{\"authorId\":\"79327094\",\"name\":\"Y. Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d1a88f5e1287a5d32114cd2eea1febf88d73e841\",\"title\":\"UTS_CAI submission at TRECVID 2018 Ad-hoc Video Search Task\",\"url\":\"https://www.semanticscholar.org/paper/d1a88f5e1287a5d32114cd2eea1febf88d73e841\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":\"2012.10285\",\"authors\":[{\"authorId\":\"14358891\",\"name\":\"T. Winterbottom\"},{\"authorId\":\"7750732\",\"name\":\"S. Xiao\"},{\"authorId\":\"145147517\",\"name\":\"A. McLean\"},{\"authorId\":\"1875235\",\"name\":\"N. A. Moubayed\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f06224d597451ce1d440ca0c8542dee4a5767afe\",\"title\":\"Trying Bilinear Pooling in Video-QA\",\"url\":\"https://www.semanticscholar.org/paper/f06224d597451ce1d440ca0c8542dee4a5767afe\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"26900125\",\"name\":\"Jinghao Lin\"},{\"authorId\":\"2440041\",\"name\":\"X. Jiang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3123266.3123364\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2411270f111a160c9289d56132651c896a5738f6\",\"title\":\"Video Question Answering via Hierarchical Dual-Level Attention Network Learning\",\"url\":\"https://www.semanticscholar.org/paper/2411270f111a160c9289d56132651c896a5738f6\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2011.12091\",\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"49490337\",\"name\":\"F. Zhou\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"47768909\",\"name\":\"Jiaqi Ji\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"}],\"doi\":\"10.1109/tmm.2020.3042067\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7d6220c46f381c48a5033dc2cd80ea276d9b5c46\",\"title\":\"SEA: Sentence Encoder Assembly for Video Retrieval by Textual Queries\",\"url\":\"https://www.semanticscholar.org/paper/7d6220c46f381c48a5033dc2cd80ea276d9b5c46\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1414740491\",\"name\":\"E. Paiz-Reyes\"},{\"authorId\":\"1414752909\",\"name\":\"Nadile Nunes-de-Lima\"},{\"authorId\":\"2939019\",\"name\":\"Sule YAYILGAN YILDIRIM\"}],\"doi\":\"10.1007/978-3-319-93000-8_30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"208a340302af5e004d1b6773611417d2119f7995\",\"title\":\"GIF Image Retrieval in Cloud Computing Environment\",\"url\":\"https://www.semanticscholar.org/paper/208a340302af5e004d1b6773611417d2119f7995\",\"venue\":\"ICIAR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39024a168ea1511821e5af17bdf838bf4afb3db8\",\"title\":\"FDU Participation in TRECVID 2019 VTT Task\",\"url\":\"https://www.semanticscholar.org/paper/39024a168ea1511821e5af17bdf838bf4afb3db8\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"1906.04402\",\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\"}],\"doi\":\"10.1109/CVPR.2019.00208\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a39d5919531a56de0e36f6b76142041b5d508213\",\"title\":\"Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/a39d5919531a56de0e36f6b76142041b5d508213\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"}],\"doi\":\"10.1109/ICCVW.2019.00536\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"title\":\"EgoVQA - An Egocentric Video Question Answering Benchmark Dataset\",\"url\":\"https://www.semanticscholar.org/paper/71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"9390267\",\"name\":\"Qifan Yang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.24963/ijcai.2017/492\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a05e84f77e1dacaa1c59ba0d92919bdcfe4debbb\",\"title\":\"Video Question Answering via Hierarchical Spatio-Temporal Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/a05e84f77e1dacaa1c59ba0d92919bdcfe4debbb\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390706104\",\"name\":\"Jiaxin Wu\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1145/3394171.3413916\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fe4199042c91f3bed8073eec9c350f81b4cb368\",\"title\":\"Interpretable Embedding for Ad-Hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/5fe4199042c91f3bed8073eec9c350f81b4cb368\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150291092\",\"name\":\"Stelios Andreadis\"},{\"authorId\":\"2559834\",\"name\":\"A. Moumtzidou\"},{\"authorId\":\"2261494\",\"name\":\"K. Apostolidis\"},{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"2675084\",\"name\":\"E. Michail\"},{\"authorId\":\"1988554\",\"name\":\"Ilias Gialampoukidis\"},{\"authorId\":\"1381295446\",\"name\":\"S. Vrochidis\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"},{\"authorId\":\"1715604\",\"name\":\"Y. Kompatsiaris\"}],\"doi\":\"10.1007/978-3-030-37734-2_69\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4c64613c8d2bb192c4ff177e5f3c767fd5b80d2\",\"title\":\"VERGE in VBS 2020\",\"url\":\"https://www.semanticscholar.org/paper/b4c64613c8d2bb192c4ff177e5f3c767fd5b80d2\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2017.2746267\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1b94c49c119c7490d2df6a2dd093e5ddd8bfba14\",\"title\":\"Unifying the Video and Question Attentions for Open-Ended Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1b94c49c119c7490d2df6a2dd093e5ddd8bfba14\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15181932\",\"name\":\"Rizal Setya Perdana\"},{\"authorId\":\"15167137\",\"name\":\"Y. Ishida\"}],\"doi\":\"10.1109/ELECSYM.2019.8901660\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"348a908617ff00c09c4d4456268da7bb60435441\",\"title\":\"Instance-based Deep Transfer Learning on Cross-domain Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/348a908617ff00c09c4d4456268da7bb60435441\",\"venue\":\"2019 International Electronics Symposium (IES)\",\"year\":2019},{\"arxivId\":\"2007.04717\",\"authors\":[{\"authorId\":\"1797543\",\"name\":\"O. Giudice\"},{\"authorId\":\"10170508\",\"name\":\"Dario Allegra\"},{\"authorId\":\"1396272251\",\"name\":\"Francesco Guarnera\"},{\"authorId\":\"2577760\",\"name\":\"F. Stanco\"},{\"authorId\":\"104159140\",\"name\":\"S. Battiato\"}],\"doi\":\"10.1109/ICIP40778.2020.9190967\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"14888e1b2a21c16cbe7497cb9313eb8f46568e5b\",\"title\":\"Animated Gif Optimization By Adaptive Color Local Table Management\",\"url\":\"https://www.semanticscholar.org/paper/14888e1b2a21c16cbe7497cb9313eb8f46568e5b\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1611.10314\",\"authors\":[{\"authorId\":\"47351893\",\"name\":\"G. Mittal\"},{\"authorId\":\"8268761\",\"name\":\"T. Marwah\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":\"10.1145/3123266.3123309\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"881c6840e0e5610d73ce3f78d529459552f3b8d9\",\"title\":\"Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive Architectures\",\"url\":\"https://www.semanticscholar.org/paper/881c6840e0e5610d73ce3f78d529459552f3b8d9\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1809.06181\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"134724966\",\"name\":\"Shouling Ji\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\"},{\"authorId\":\"98289428\",\"name\":\"G. Yang\"},{\"authorId\":\"39742349\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00957\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6a976c123037b138946f6767e1aa0de84b1682d4\",\"title\":\"Dual Encoding for Zero-Example Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6a976c123037b138946f6767e1aa0de84b1682d4\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"2003.00392\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/cvpr42600.2020.01065\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b78e14dfc2050878e8c817e4782c0c81ee7f5dd\",\"title\":\"Fine-Grained Video-Text Retrieval With Hierarchical Graph Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/0b78e14dfc2050878e8c817e4782c0c81ee7f5dd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1705.00930\",\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2017.64\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"title\":\"Show, Adapt and Tell: Adversarial Training of Cross-Domain Image Captioner\",\"url\":\"https://www.semanticscholar.org/paper/828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1758267\",\"name\":\"Xu Zhao\"},{\"authorId\":\"2671321\",\"name\":\"Lianli Gao\"},{\"authorId\":\"48749954\",\"name\":\"Liangliang Cao\"}],\"doi\":\"10.1002/9781119376996.CH4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a057ef78fca7d7758020d8befb77ac5aac2e765\",\"title\":\"Large-Scale Video Understanding with Limited Training Labels\",\"url\":\"https://www.semanticscholar.org/paper/2a057ef78fca7d7758020d8befb77ac5aac2e765\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1563941288\",\"name\":\"Riiya Kondo\"},{\"authorId\":\"2575952\",\"name\":\"T. Yukawa\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e17451559ccb1b4baa7a608ab0c3f4c0530161fd\",\"title\":\"An Automatic Caption Generation for video clip with reducing frames in order to shorten processing time\",\"url\":\"https://www.semanticscholar.org/paper/e17451559ccb1b4baa7a608ab0c3f4c0530161fd\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7245576\",\"name\":\"Qi Rao\"},{\"authorId\":\"153185012\",\"name\":\"G. Li\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"47190894\",\"name\":\"F. Zhang\"},{\"authorId\":\"1500519009\",\"name\":\"Ziwei Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b984875c3cf6b7dcaafc078fbab12d54c023ca40\",\"title\":\"UTS ISA Submission at the TRECVID 2019 Video to Text Description Task\",\"url\":\"https://www.semanticscholar.org/paper/b984875c3cf6b7dcaafc078fbab12d54c023ca40\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"}],\"doi\":\"10.1145/3372278.3390737\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf89a2c1c58bcd5aefd1bb08d9773a33084867f0\",\"title\":\"Attention Mechanisms, Signal Encodings and Fusion Strategies for Improved Ad-hoc Video Search with Dual Encoding Networks\",\"url\":\"https://www.semanticscholar.org/paper/bf89a2c1c58bcd5aefd1bb08d9773a33084867f0\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1007/s11263-019-01189-x\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e1bd132613866c176a8fc780cb1b9f9aa43feeb\",\"title\":\"Video Question Answering with Spatio-Temporal Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/1e1bd132613866c176a8fc780cb1b9f9aa43feeb\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chang Liu\"},{\"authorId\":\"2887672\",\"name\":\"A. Shmilovici\"},{\"authorId\":\"3045152\",\"name\":\"Mark Last\"}],\"doi\":\"10.1109/FSKD.2017.8393188\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"22648dcd3100432fe0cc71e09de5ee855c61f12b\",\"title\":\"Automatic generation of composite image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/22648dcd3100432fe0cc71e09de5ee855c61f12b\",\"venue\":\"2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1930287\",\"name\":\"L. Zhang\"},{\"authorId\":\"1772337\",\"name\":\"R. Radke\"}],\"doi\":\"10.1145/3382507.3418886\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c43909efc347aab615a8688e0329d5b3d2cc1b62\",\"title\":\"Temporal Attention and Consistency Measuring for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/c43909efc347aab615a8688e0329d5b3d2cc1b62\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48213375\",\"name\":\"Hao Zhang\"},{\"authorId\":\"144834368\",\"name\":\"L. Pang\"},{\"authorId\":\"47006432\",\"name\":\"Y. Lu\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b768ef34c4d072abf60c7f4202c017649a473ae6\",\"title\":\"VIREO @ TRECVID 2016: Multimedia Event Detection, Ad-hoc Video Search, Video to Text Description\",\"url\":\"https://www.semanticscholar.org/paper/b768ef34c4d072abf60c7f4202c017649a473ae6\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000825730\",\"name\":\"Encarnaci\\u00f3n Vergara S\\u00e1nchez\"}],\"doi\":\"10.4995/ANIAV.2020.13322\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c89ef25fb797568aa76cc634cd4fda4c73c23b99\",\"title\":\"Alicia en el pa\\u00eds cin\\u00e9tico del GIF: lenguaje art\\u00edstico-narrativo en el siglo XXI\",\"url\":\"https://www.semanticscholar.org/paper/c89ef25fb797568aa76cc634cd4fda4c73c23b99\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.04357\",\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"49469577\",\"name\":\"X. Zhang\"},{\"authorId\":\"50202300\",\"name\":\"Shu Zhang\"},{\"authorId\":\"46314996\",\"name\":\"Wensheng Wang\"},{\"authorId\":null,\"name\":\"Chi Zhang\"},{\"authorId\":\"46675463\",\"name\":\"Heng Huang\"}],\"doi\":\"10.1109/CVPR.2019.00210\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c5d99eff1377e141be293336a14ffddb323c364\",\"title\":\"Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/5c5d99eff1377e141be293336a14ffddb323c364\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1708.02970\",\"authors\":[{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"36400073\",\"name\":\"Kyungdon Joo\"},{\"authorId\":\"39678486\",\"name\":\"N. Joshi\"},{\"authorId\":\"2450889\",\"name\":\"B. Wang\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"},{\"authorId\":\"1738740\",\"name\":\"S. B. Kang\"}],\"doi\":\"10.1109/ICCV.2017.552\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4661d57e381bee9f9eda7c7a33592ad4e605f27c\",\"title\":\"Personalized Cinemagraphs Using Semantic Understanding and Collaborative Learning\",\"url\":\"https://www.semanticscholar.org/paper/4661d57e381bee9f9eda7c7a33592ad4e605f27c\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"2009.05381\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"title\":\"Hybrid Space Learning for Language-based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51427469\",\"name\":\"Alexander Braylan\"},{\"authorId\":\"1747771\",\"name\":\"Matthew Lease\"}],\"doi\":\"10.1145/3366423.3380250\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a898c3000df8e9bd30f69d77f965e698c440ad71\",\"title\":\"Modeling and Aggregation of Complex Annotations via Annotation Distances\",\"url\":\"https://www.semanticscholar.org/paper/a898c3000df8e9bd30f69d77f965e698c440ad71\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jo\\u00ebl Theytaz\"},{\"authorId\":null,\"name\":\"Lin Yuan\"},{\"authorId\":null,\"name\":\"David McNally\"},{\"authorId\":null,\"name\":\"Touradj Ebrahimi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"09e853af6817b4cb36225df13f640dd7d876fd05\",\"title\":\"2 Motion JPEG and Motion JPEG 2000\",\"url\":\"https://www.semanticscholar.org/paper/09e853af6817b4cb36225df13f640dd7d876fd05\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"145880168\",\"name\":\"B. Huet\"},{\"authorId\":\"1686820\",\"name\":\"B. M\\u00e9rialdo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4535891852c58d992305ee440391cfa1ba33cb69\",\"title\":\"EURECOM participation in TrecVid VTT 2018\",\"url\":\"https://www.semanticscholar.org/paper/4535891852c58d992305ee440391cfa1ba33cb69\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":\"1705.01253\",\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1016/j.neucom.2018.06.069\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"783e48629dfbb44697b15a3bc0cb2aa3eea490eb\",\"title\":\"The Forgettable-Watcher Model for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/783e48629dfbb44697b15a3bc0cb2aa3eea490eb\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51021338\",\"name\":\"Nelson Ruwa\"},{\"authorId\":\"3069077\",\"name\":\"Q. Mao\"},{\"authorId\":\"79927338\",\"name\":\"L. Wang\"},{\"authorId\":\"37233332\",\"name\":\"J. Gou\"}],\"doi\":\"10.1016/J.NEUCOM.2019.06.046\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b6ce7e65130431eecf98db2e6c162893bbe5127\",\"title\":\"Affective question answering on video\",\"url\":\"https://www.semanticscholar.org/paper/1b6ce7e65130431eecf98db2e6c162893bbe5127\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"98289428\",\"name\":\"G. Yang\"},{\"authorId\":\"8157255\",\"name\":\"Zhineng Chen\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"}],\"doi\":\"10.1145/3343031.3350906\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"fa52f285eeb28d9eb25ad70a04df17b36a0fd664\",\"title\":\"W2VV++: Fully Deep Learning for Ad-hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/fa52f285eeb28d9eb25ad70a04df17b36a0fd664\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49602133\",\"name\":\"S. Braun\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0586116ff3d690755ba35dcb5c17dafab13e291f\",\"title\":\"Finding the Right Words: Investigating Machine-Generated Video Description Quality Using a Corpus-Based Approach\",\"url\":\"https://www.semanticscholar.org/paper/0586116ff3d690755ba35dcb5c17dafab13e291f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/TIP.2020.3042086\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"title\":\"Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021}],\"corpusId\":6262415,\"doi\":\"10.1109/CVPR.2016.502\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":15,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"references\":[{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1312.3005\",\"authors\":[{\"authorId\":\"1802969\",\"name\":\"Ciprian Chelba\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"46503039\",\"name\":\"M. Schuster\"},{\"authorId\":\"50402949\",\"name\":\"Qi Ge\"},{\"authorId\":\"1784037\",\"name\":\"T. Brants\"},{\"authorId\":\"1389605976\",\"name\":\"Phillipp Koehn\"},{\"authorId\":\"152149122\",\"name\":\"T. Robinson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d833331b0e22ff359db05c62a8bca18c4f04b68\",\"title\":\"One billion word benchmark for measuring progress in statistical language modeling\",\"url\":\"https://www.semanticscholar.org/paper/5d833331b0e22ff359db05c62a8bca18c4f04b68\",\"venue\":\"INTERSPEECH\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"I. Sutskever A. Krizhevsky\"},{\"authorId\":null,\"name\":\"G. E. Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"agenet classification with deep convolutional neural networks\",\"url\":\"\",\"venue\":\"In NIPS\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1505.00468\",\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"118707418\",\"name\":\"M. Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/s11263-016-0966-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143790066\",\"name\":\"Dipanjan Das\"},{\"authorId\":\"1701750\",\"name\":\"Desai Chen\"},{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"},{\"authorId\":\"145254207\",\"name\":\"Nathan Schneider\"},{\"authorId\":\"144365875\",\"name\":\"Noah A. Smith\"}],\"doi\":\"10.1162/COLI_a_00163\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"942cdb6804c5863f281d51d7bec43f87f623be2b\",\"title\":\"Frame-Semantic Parsing\",\"url\":\"https://www.semanticscholar.org/paper/942cdb6804c5863f281d51d7bec43f87f623be2b\",\"venue\":\"Computational Linguistics\",\"year\":2014},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/TPAMI.2012.162\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5cb6700d94c6118ee13f4f4fecac99f111189812\",\"title\":\"BabyTalk: Understanding and Generating Simple Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/5cb6700d94c6118ee13f4f4fecac99f111189812\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2013},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49604675\",\"name\":\"P. Koehn\"},{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"2529583\",\"name\":\"Wade Shen\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"143832874\",\"name\":\"Ondrej Bojar\"},{\"authorId\":\"46898156\",\"name\":\"B. Cowan\"},{\"authorId\":\"1745899\",\"name\":\"Chris Dyer\"},{\"authorId\":\"152378023\",\"name\":\"Hieu T. Hoang\"},{\"authorId\":\"1983801\",\"name\":\"R. Zens\"},{\"authorId\":\"31542143\",\"name\":\"A. Constantin\"},{\"authorId\":\"6376655\",\"name\":\"E. Herbst\"},{\"authorId\":\"89224350\",\"name\":\"C. C. Moran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"99e8d34817ae10d7304521e89c5fbf908b9d856b\",\"title\":\"Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding\",\"url\":\"https://www.semanticscholar.org/paper/99e8d34817ae10d7304521e89c5fbf908b9d856b\",\"venue\":\"\",\"year\":2006},{\"arxivId\":\"1408.5093\",\"authors\":[{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"3049736\",\"name\":\"S. Karayev\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1145/2647868.2654889\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6bdb186ec4726e00a8051119636d4df3b94043b5\",\"title\":\"Caffe: Convolutional Architecture for Fast Feature Embedding\",\"url\":\"https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39998815\",\"name\":\"J. Read\"},{\"authorId\":\"1737420\",\"name\":\"B. Pfahringer\"},{\"authorId\":\"144282963\",\"name\":\"G. Holmes\"},{\"authorId\":\"143713826\",\"name\":\"Eibe Frank\"}],\"doi\":\"10.1007/s10994-011-5256-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e75a71274522980dce85b8ad00369d0a33667ea4\",\"title\":\"Classifier chains for multi-label classification\",\"url\":\"https://www.semanticscholar.org/paper/e75a71274522980dce85b8ad00369d0a33667ea4\",\"venue\":\"Machine Learning\",\"year\":2011},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144096985\",\"name\":\"G. Miller\"}],\"doi\":\"10.1145/219717.219748\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"68c03788224000794d5491ab459be0b2a2c38677\",\"title\":\"WordNet: a lexical database for English\",\"url\":\"https://www.semanticscholar.org/paper/68c03788224000794d5491ab459be0b2a2c38677\",\"venue\":\"CACM\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1888731\",\"name\":\"M. Hejrati\"},{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/978-3-642-15561-1_2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"title\":\"Every Picture Tells a Story: Generating Sentences from Images\",\"url\":\"https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"24138684\",\"name\":\"Dominikus Wetzel\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"}],\"doi\":\"10.1162/tacl_a_00207\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"title\":\"Grounding Action Descriptions in Videos\",\"url\":\"https://www.semanticscholar.org/paper/21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"title\":\"Im2Text: Describing Images Using 1 Million Captioned Photographs\",\"url\":\"https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696803\",\"name\":\"Joachim Daiber\"},{\"authorId\":\"2144277\",\"name\":\"Max Jakob\"},{\"authorId\":\"82576134\",\"name\":\"Chris Hokamp\"},{\"authorId\":\"1692493\",\"name\":\"Pablo N. Mendes\"}],\"doi\":\"10.1145/2506182.2506198\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a97fe719f2f1e62169320e1c45e3f96e094ccd19\",\"title\":\"Improving efficiency and accuracy in multilingual entity extraction\",\"url\":\"https://www.semanticscholar.org/paper/a97fe719f2f1e62169320e1c45e3f96e094ccd19\",\"venue\":\"I-SEMANTICS '13\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Bakhshi\"},{\"authorId\":null,\"name\":\"D. Shamma\"},{\"authorId\":null,\"name\":\"L. Kennedy\"},{\"authorId\":null,\"name\":\"Y. Song\"},{\"authorId\":null,\"name\":\"P. de Juan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"VQA : Visual question answering The berkeley framenet project\",\"url\":\"\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35371521\",\"name\":\"Kristina Toutanvoa\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/1117794.1117802\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b\",\"title\":\"Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-Speech Tagger\",\"url\":\"https://www.semanticscholar.org/paper/1504a9d5829033a8cb4cf37b8bb13dfd4baddc7b\",\"venue\":\"EMNLP\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153920582\",\"name\":\"Luk\\u00e1s Neumann\"},{\"authorId\":\"1691679\",\"name\":\"Juan E. Sala Matas\"}],\"doi\":\"10.1109/CVPR.2012.6248097\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8b595c9e969e5605f62da51b6c16dad8aad3e0e\",\"title\":\"Real-time scene text localization and recognition\",\"url\":\"https://www.semanticscholar.org/paper/d8b595c9e969e5605f62da51b6c16dad8aad3e0e\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39554008\",\"name\":\"Saeideh Bakhshi\"},{\"authorId\":\"1760364\",\"name\":\"D. Shamma\"},{\"authorId\":\"144557123\",\"name\":\"L. Kennedy\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"1893882\",\"name\":\"Paloma de Juan\"},{\"authorId\":\"143782217\",\"name\":\"J. Kaye\"}],\"doi\":\"10.1145/2858036.2858532\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"345413ede3e5743645d95140e74a73a86a96d7f8\",\"title\":\"Fast, Cheap, and Good: Why Animated GIFs Engage Us\",\"url\":\"https://www.semanticscholar.org/paper/345413ede3e5743645d95140e74a73a86a96d7f8\",\"venue\":\"CHI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1755162\",\"name\":\"Philipp Koehn\"},{\"authorId\":\"152378023\",\"name\":\"Hieu T. Hoang\"},{\"authorId\":\"2539211\",\"name\":\"Alexandra Birch\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"46898156\",\"name\":\"B. Cowan\"},{\"authorId\":\"2529583\",\"name\":\"Wade Shen\"},{\"authorId\":\"145046497\",\"name\":\"C. Moran\"},{\"authorId\":\"1983801\",\"name\":\"R. Zens\"},{\"authorId\":\"1745899\",\"name\":\"Chris Dyer\"},{\"authorId\":\"143832874\",\"name\":\"Ondrej Bojar\"},{\"authorId\":\"4733879\",\"name\":\"A. Constantin\"},{\"authorId\":\"6376655\",\"name\":\"E. Herbst\"}],\"doi\":\"10.3115/1557769.1557821\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ee2eab4c298c1824a9fb8799ad8eed21be38d21\",\"title\":\"Moses: Open Source Toolkit for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/4ee2eab4c298c1824a9fb8799ad8eed21be38d21\",\"venue\":\"ACL\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Shamma\"},{\"authorId\":null,\"name\":\"L. Kennedy\"},{\"authorId\":null,\"name\":\"Y. Song\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"VQA : Visual question answering The berkeley framenet project\",\"url\":\"\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2015.7298932\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"title\":\"Deep visual-semantic alignments for generating image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"2677488\",\"name\":\"\\u00c0gata Lapedriza\"},{\"authorId\":\"40599257\",\"name\":\"J. Xiao\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9667f8264745b626c6173b1310e2ff0298b09cfc\",\"title\":\"Learning Deep Features for Scene Recognition using Places Database\",\"url\":\"https://www.semanticscholar.org/paper/9667f8264745b626c6173b1310e2ff0298b09cfc\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf60322f83714523e2d7c1d39983151fe9db7146\",\"title\":\"Collecting Image Annotations Using Amazon's Mechanical Turk\",\"url\":\"https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146\",\"venue\":\"Mturk@HLT-NAACL\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"2155311\",\"name\":\"Eunbyung Park\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/ICCV.2015.283\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ced9f7178f8032d3408fcba493c02eb48e8a8636\",\"title\":\"Visual Madlibs: Fill in the Blank Description Generation and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ced9f7178f8032d3408fcba493c02eb48e8a8636\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2013.340\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"title\":\"A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching\",\"url\":\"https://www.semanticscholar.org/paper/a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1749194\",\"name\":\"Collin F. Baker\"},{\"authorId\":\"2912454\",\"name\":\"C. Fillmore\"},{\"authorId\":\"145543490\",\"name\":\"J. Lowe\"}],\"doi\":\"10.3115/980845.980860\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"547f23597f9ec8a93f66cedaa6fbfb73960426b1\",\"title\":\"The Berkeley FrameNet Project\",\"url\":\"https://www.semanticscholar.org/paper/547f23597f9ec8a93f66cedaa6fbfb73960426b1\",\"venue\":\"COLING-ACL\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. Mooney\"},{\"authorId\":null,\"name\":\"T. Darrell\"},{\"authorId\":null,\"name\":\"K. Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Youtube 2 text : Recognizing and describing arbitrary activities using semantic hierarchies and zero - shot recognition Long short - term memory\",\"url\":\"\",\"venue\":\"Neural computation\",\"year\":2010},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1307.2982\",\"authors\":[{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"2355911\",\"name\":\"Ali Punjani\"},{\"authorId\":\"1793739\",\"name\":\"David J. Fleet\"}],\"doi\":\"10.1109/TPAMI.2013.231\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e754edf1328481474c69fb47a99e3ab41785c21\",\"title\":\"Fast Exact Search in Hamming Space With Multi-Index Hashing\",\"url\":\"https://www.semanticscholar.org/paper/5e754edf1328481474c69fb47a99e3ab41785c21\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47713710\",\"name\":\"Benjamin Z. Yao\"},{\"authorId\":\"47008378\",\"name\":\"Xiong Yang\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"},{\"authorId\":\"2649483\",\"name\":\"M. Lee\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"}],\"doi\":\"10.1109/JPROC.2010.2050411\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05e074abddd3fe987b9bebd46f6cf4bf8465c37e\",\"title\":\"I2T: Image Parsing to Text Description\",\"url\":\"https://www.semanticscholar.org/paper/05e074abddd3fe987b9bebd46f6cf4bf8465c37e\",\"venue\":\"Proceedings of the IEEE\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47080432\",\"name\":\"C. Zauner\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"635e1b5261ad1545aab7acde48efa267ae428fc3\",\"title\":\"Implementation and Benchmarking of Perceptual Image Hash Functions\",\"url\":\"https://www.semanticscholar.org/paper/635e1b5261ad1545aab7acde48efa267ae428fc3\",\"venue\":\"\",\"year\":2010},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017}],\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"topics\":[{\"topic\":\"GIF\",\"topicId\":\"84786\",\"url\":\"https://www.semanticscholar.org/topic/84786\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Crowdsourcing\",\"topicId\":\"85\",\"url\":\"https://www.semanticscholar.org/topic/85\"},{\"topic\":\"Flickr\",\"topicId\":\"67227\",\"url\":\"https://www.semanticscholar.org/topic/67227\"},{\"topic\":\"Computer animation\",\"topicId\":\"102448\",\"url\":\"https://www.semanticscholar.org/topic/102448\"},{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"Statistical machine translation\",\"topicId\":\"42996\",\"url\":\"https://www.semanticscholar.org/topic/42996\"},{\"topic\":\"Recurrent neural network\",\"topicId\":\"16115\",\"url\":\"https://www.semanticscholar.org/topic/16115\"},{\"topic\":\"Testbed\",\"topicId\":\"1705\",\"url\":\"https://www.semanticscholar.org/topic/1705\"},{\"topic\":\"Social media\",\"topicId\":\"6015\",\"url\":\"https://www.semanticscholar.org/topic/6015\"},{\"topic\":\"Display resolution\",\"topicId\":\"11387\",\"url\":\"https://www.semanticscholar.org/topic/11387\"},{\"topic\":\"Natural language generation\",\"topicId\":\"6196\",\"url\":\"https://www.semanticscholar.org/topic/6196\"},{\"topic\":\"Content-control software\",\"topicId\":\"194921\",\"url\":\"https://www.semanticscholar.org/topic/194921\"},{\"topic\":\"Data science\",\"topicId\":\"89193\",\"url\":\"https://www.semanticscholar.org/topic/89193\"},{\"topic\":\"Cipher\",\"topicId\":\"28252\",\"url\":\"https://www.semanticscholar.org/topic/28252\"},{\"topic\":\"Video clip\",\"topicId\":\"30493\",\"url\":\"https://www.semanticscholar.org/topic/30493\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"}],\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016}\n"