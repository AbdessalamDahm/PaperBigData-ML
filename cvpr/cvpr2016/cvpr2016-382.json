"{\"abstract\":\"In view of the advantages of deep networks in producing useful representation, the generated features of different modality data (such as image, audio) can be jointly learned using Multimodal Restricted Boltzmann Machines (MRB-M). Recently, audiovisual speech recognition based the M-RBM has attracted much attention, and the MRBM shows its effectiveness in learning the joint representation across audiovisual modalities. However, the built networks have weakness in modeling the multimodal sequence which is the natural property of speech signal. In this paper, we will introduce a novel temporal multimodal deep learning architecture, named as Recurrent Temporal Multimodal RB-M (RTMRBM), that models multimodal sequences by transforming the sequence of connected MRBMs into a probabilistic series model. Compared with existing multimodal networks, it's simple and efficient in learning temporal joint representation. We evaluate our model on audiovisual speech datasets, two public (AVLetters and AVLetters2) and one self-build. The experimental results demonstrate that our approach can obviously improve the accuracy of recognition compared with standard MRBM and the temporal model based on conditional RBM. In addition, RTMRBM still outperforms non-temporal multimodal deep networks in the presence of the weakness of long-term dependencies.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"48080348\",\"name\":\"D. Hu\",\"url\":\"https://www.semanticscholar.org/author/48080348\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\",\"url\":\"https://www.semanticscholar.org/author/1720243\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\",\"url\":\"https://www.semanticscholar.org/author/7828998\"}],\"citationVelocity\":16,\"citations\":[{\"arxivId\":\"1902.07473\",\"authors\":[{\"authorId\":\"2564871\",\"name\":\"Yan-Bo Lin\"},{\"authorId\":\"3312576\",\"name\":\"Yu-Jhe Li\"},{\"authorId\":\"2733735\",\"name\":\"Y. Wang\"}],\"doi\":\"10.1109/ICASSP.2019.8683226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6dad33f2e317cc23abeba8596a4c8a4a13117d54\",\"title\":\"Dual-modality Seq2Seq Network for Audio-visual Event Localization\",\"url\":\"https://www.semanticscholar.org/paper/6dad33f2e317cc23abeba8596a4c8a4a13117d54\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1704.03152\",\"authors\":[{\"authorId\":\"3042242\",\"name\":\"X. Yang\"},{\"authorId\":\"40327196\",\"name\":\"Palghat Ramesh\"},{\"authorId\":\"1776163\",\"name\":\"Radha Chitta\"},{\"authorId\":\"2234121\",\"name\":\"S. Madhvanath\"},{\"authorId\":\"2164140\",\"name\":\"E. Bernal\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2017.538\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"01f4e52ead817b83d65478d03f6a9d7e9074d889\",\"title\":\"Deep Multimodal Representation Learning from Temporal Data\",\"url\":\"https://www.semanticscholar.org/paper/01f4e52ead817b83d65478d03f6a9d7e9074d889\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51036510\",\"name\":\"Athma Narayanan\"},{\"authorId\":\"145608726\",\"name\":\"Avinash Siravuru\"},{\"authorId\":\"2086607\",\"name\":\"Behzad Dariush\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6f3e3703491e8e193dbe24b0dd832767c0a9fada\",\"title\":\"Temporal Multimodal Fusion for Driver Behavior Prediction Tasks using Gated Recurrent Fusion Units\",\"url\":\"https://www.semanticscholar.org/paper/6f3e3703491e8e193dbe24b0dd832767c0a9fada\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153211990\",\"name\":\"Di Hu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbc5f0dd73627fdf7d3d1b828fad6f663b30b286\",\"title\":\"Multimodal Learning via Exploring Deep Semantic Similarity\",\"url\":\"https://www.semanticscholar.org/paper/cbc5f0dd73627fdf7d3d1b828fad6f663b30b286\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"18059931\",\"name\":\"Mohammad Naim Rastgoo\"},{\"authorId\":\"20510630\",\"name\":\"Bahareh Nakisa\"},{\"authorId\":\"1737645\",\"name\":\"F. Maire\"},{\"authorId\":\"94815327\",\"name\":\"A. Rakotonirainy\"},{\"authorId\":\"46916715\",\"name\":\"V. Chandran\"}],\"doi\":\"10.1016/J.ESWA.2019.07.010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c3c762b92f0a67292cd3de5fdd83dda06078e3a6\",\"title\":\"Automatic driver stress level classification using multimodal deep learning\",\"url\":\"https://www.semanticscholar.org/paper/c3c762b92f0a67292cd3de5fdd83dda06078e3a6\",\"venue\":\"Expert Syst. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2317420\",\"name\":\"T. Saitoh\"},{\"authorId\":\"34849838\",\"name\":\"Ziheng Zhou\"},{\"authorId\":\"1757287\",\"name\":\"G. Zhao\"},{\"authorId\":\"145962204\",\"name\":\"M. Pietik\\u00e4inen\"}],\"doi\":\"10.1007/978-3-319-54427-4_21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"279e7334c580fdece25ffa280f0cbc0083a92da7\",\"title\":\"Concatenated Frame Image Based CNN for Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/279e7334c580fdece25ffa280f0cbc0083a92da7\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145377162\",\"name\":\"Y. Yuan\"},{\"authorId\":\"8788762\",\"name\":\"Chunlin Tian\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/ACCESS.2018.2796118\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"da01ca5564e3f554ccb345da89fd61c1e865490e\",\"title\":\"Auxiliary Loss Multimodal GRU Model in Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/da01ca5564e3f554ccb345da89fd61c1e865490e\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1611.01599\",\"authors\":[{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"291c0e453503a704c0fd932a067ca054cc7edad6\",\"title\":\"LipNet: End-to-End Sentence-level Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/291c0e453503a704c0fd932a067ca054cc7edad6\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410464604\",\"name\":\"A. Fernandez-Lopez\"},{\"authorId\":\"2012978\",\"name\":\"F. Sukno\"}],\"doi\":\"10.1016/j.imavis.2018.07.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"938717d66533d900abac7451e15c1f322e78db81\",\"title\":\"Survey on automatic lip-reading in the era of deep learning\",\"url\":\"https://www.semanticscholar.org/paper/938717d66533d900abac7451e15c1f322e78db81\",\"venue\":\"Image Vis. Comput.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48080348\",\"name\":\"D. Hu\"},{\"authorId\":\"144962210\",\"name\":\"F. Nie\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TMM.2018.2866771\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8fe7f2668c480dcae03de997ff7cc5ef69634053\",\"title\":\"Deep Binary Reconstruction for Cross-Modal Hashing\",\"url\":\"https://www.semanticscholar.org/paper/8fe7f2668c480dcae03de997ff7cc5ef69634053\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1709.04343\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"2563750\",\"name\":\"Yujiang Wang\"},{\"authorId\":\"8798628\",\"name\":\"Z. Li\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.21437/AVSP.2017-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"acd7bd7932e579ec3641a0191764dc349bfd6e8b\",\"title\":\"End-to-End Audiovisual Fusion with LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/acd7bd7932e579ec3641a0191764dc349bfd6e8b\",\"venue\":\"AVSP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29922964\",\"name\":\"Masaya Iwasaki\"},{\"authorId\":\"40099567\",\"name\":\"Michiko Kubokawa\"},{\"authorId\":\"2317420\",\"name\":\"T. Saitoh\"}],\"doi\":\"10.23919/MVA.2017.7986867\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"412b41f4a94192fc82cd63515f27dc3a755baa76\",\"title\":\"Two features combination with gated recurrent unit for visual speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/412b41f4a94192fc82cd63515f27dc3a755baa76\",\"venue\":\"2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49420307\",\"name\":\"Y. Liu\"},{\"authorId\":\"1390925314\",\"name\":\"Yue Yao\"},{\"authorId\":\"1390878200\",\"name\":\"Zhengjie Wang\"},{\"authorId\":\"1390011966\",\"name\":\"J. Plested\"},{\"authorId\":\"27011207\",\"name\":\"T. Gedeon\"}],\"doi\":\"10.1109/IJCNN.2019.8852216\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dc225b4fcb30c5c0a62c58bd64a4abdac3388f52\",\"title\":\"Generalized Alignment for Multimodal Physiological Signal Learning\",\"url\":\"https://www.semanticscholar.org/paper/dc225b4fcb30c5c0a62c58bd64a4abdac3388f52\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150073134\",\"name\":\"Dario Augusto Borges Oliveira\"},{\"authorId\":\"150007774\",\"name\":\"Andrea Britto Mattos\"},{\"authorId\":\"150095880\",\"name\":\"Edmilson da Silva Morais\"}],\"doi\":\"10.1109/FG.2019.8756589\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f5508afa55602265d6a648451089e5428b388fa\",\"title\":\"Improving Viseme Recognition with GAN-based Muti-view Mapping\",\"url\":\"https://www.semanticscholar.org/paper/6f5508afa55602265d6a648451089e5428b388fa\",\"venue\":\"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"7861176\",\"name\":\"Yi-Feng Wu\"},{\"authorId\":\"1721819\",\"name\":\"D. Zhan\"},{\"authorId\":\"2192443\",\"name\":\"Yuan Jiang\"}],\"doi\":\"10.1007/978-3-319-97310-4_8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"081b4f6f48a7c3927572c792858e55056a52e725\",\"title\":\"Deep Multi-modal Learning with Cascade Consensus\",\"url\":\"https://www.semanticscholar.org/paper/081b4f6f48a7c3927572c792858e55056a52e725\",\"venue\":\"PRICAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2163613\",\"name\":\"Alexandra Covaci\"},{\"authorId\":\"3165927\",\"name\":\"Est\\u00eav\\u00e3o Bissoli Saleme\"},{\"authorId\":\"2496596\",\"name\":\"G. Mesfin\"},{\"authorId\":\"143757259\",\"name\":\"N. Hussain\"},{\"authorId\":\"1403977935\",\"name\":\"Elahe Kani-Zabihi\"},{\"authorId\":\"119932788\",\"name\":\"Gheorghita Ghinea\"}],\"doi\":\"10.1109/TMM.2019.2941274\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0fb615dcebdc124a6f314fd4d142234cbf41e222\",\"title\":\"How Do We Experience Crossmodal Correspondent Mulsemedia Content?\",\"url\":\"https://www.semanticscholar.org/paper/0fb615dcebdc124a6f314fd4d142234cbf41e222\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1776581\",\"name\":\"S. Zhang\"},{\"authorId\":\"144554625\",\"name\":\"M. Lei\"},{\"authorId\":\"144720333\",\"name\":\"B. Ma\"},{\"authorId\":\"144206962\",\"name\":\"Lei Xie\"}],\"doi\":\"10.1109/ICASSP.2019.8682566\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6b488224f6abb21089aae826503a020fb59f02e5\",\"title\":\"Robust Audio-visual Speech Recognition Using Bimodal Dfsmn with Multi-condition Training and Dropout Regularization\",\"url\":\"https://www.semanticscholar.org/paper/6b488224f6abb21089aae826503a020fb59f02e5\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933398\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"7943876\",\"name\":\"Feipeng Cai\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aadcf6136b9eaa0c96358e4351a070258a3bcc40\",\"title\":\"C V ] 2 2 Fe b 20 18 END-TO-END AUDIOVISUAL SPEECH RECOGNITION\",\"url\":\"https://www.semanticscholar.org/paper/aadcf6136b9eaa0c96358e4351a070258a3bcc40\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807587\",\"name\":\"S. Essid\"},{\"authorId\":\"40217426\",\"name\":\"S. Parekh\"},{\"authorId\":\"1756744\",\"name\":\"Ngoc Q. K. Duong\"},{\"authorId\":\"2954162\",\"name\":\"R. Serizel\"},{\"authorId\":\"2889451\",\"name\":\"A. Ozerov\"},{\"authorId\":\"1772256\",\"name\":\"F. Antonacci\"},{\"authorId\":\"30136187\",\"name\":\"A. Sarti\"}],\"doi\":\"10.1007/978-3-319-63450-0_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ae031b519cbac99797320ad4bb405e8d07d62ed\",\"title\":\"Multiview Approaches to Event Detection and Scene Analysis\",\"url\":\"https://www.semanticscholar.org/paper/1ae031b519cbac99797320ad4bb405e8d07d62ed\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1934420\",\"name\":\"L. Terissi\"},{\"authorId\":\"2585807\",\"name\":\"Gonzalo D. Sad\"},{\"authorId\":\"144004546\",\"name\":\"J. C. G\\u00f3mez\"}],\"doi\":\"10.1007/s10772-018-9504-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"690c65461c940a6acc9cc807dbff038205ef2366\",\"title\":\"Robust front-end for audio, visual and audio\\u2013visual speech classification\",\"url\":\"https://www.semanticscholar.org/paper/690c65461c940a6acc9cc807dbff038205ef2366\",\"venue\":\"Int. J. Speech Technol.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8788762\",\"name\":\"Chunlin Tian\"},{\"authorId\":\"145377162\",\"name\":\"Y. Yuan\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1007/978-981-10-7299-4_54\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"164ebdd07c6fd85ca08812ffc0039ffc3db816ce\",\"title\":\"Deep Temporal Architecture for Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/164ebdd07c6fd85ca08812ffc0039ffc3db816ce\",\"venue\":\"CCCV\",\"year\":2017},{\"arxivId\":\"2001.04758\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"100602595\",\"name\":\"Mandi Luo\"},{\"authorId\":\"77886607\",\"name\":\"Rui Wang\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"title\":\"Deep Audio-Visual Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1802.06424\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"7943876\",\"name\":\"Feipeng Cai\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP.2018.8461326\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43cf4abfe1bc6e38b845740f7ee00715ce9d5a39\",\"title\":\"End-to-End Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/43cf4abfe1bc6e38b845740f7ee00715ce9d5a39\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mehmet Altinkaya\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a649c32fe74411a46dd8a310a1612630ca398ea\",\"title\":\"A Dynamically Expandable, Weakly Supervised, Audio-Visual Database of Stuttered Speech\",\"url\":\"https://www.semanticscholar.org/paper/1a649c32fe74411a46dd8a310a1612630ca398ea\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2802283\",\"name\":\"H. Liu\"},{\"authorId\":\"48354992\",\"name\":\"Zhengyan Chen\"},{\"authorId\":\"114896414\",\"name\":\"W. Shi\"}],\"doi\":\"10.1109/ICIP40778.2020.9190894\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0c6adad4daa66127059f114ac7fe880f2cdf1d3\",\"title\":\"Robust Audio-Visual Mandarin Speech Recognition Based On Adaptive Decision Fusion And Tone Features\",\"url\":\"https://www.semanticscholar.org/paper/b0c6adad4daa66127059f114ac7fe880f2cdf1d3\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":\"1912.13077\",\"authors\":[{\"authorId\":\"48240322\",\"name\":\"Changhao Chen\"},{\"authorId\":\"145895122\",\"name\":\"S. Rosa\"},{\"authorId\":\"11614724\",\"name\":\"Chris Xiaoxuan Lu\"},{\"authorId\":\"3641238\",\"name\":\"A. Trigoni\"},{\"authorId\":\"1404852450\",\"name\":\"Andrew Markham\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b7160308d4193a4594d37f8292aa476f3ce2847\",\"title\":\"SelectFusion: A Generic Framework to Selectively Learn Multisensory Fusion\",\"url\":\"https://www.semanticscholar.org/paper/0b7160308d4193a4594d37f8292aa476f3ce2847\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"145420448\",\"name\":\"E. Morais\"}],\"doi\":\"10.1109/CVPRW.2018.00289\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74349b71eeade9e11d43d4e394864fe65da630ca\",\"title\":\"Improving Viseme Recognition Using GAN-Based Frontal View Mapping\",\"url\":\"https://www.semanticscholar.org/paper/74349b71eeade9e11d43d4e394864fe65da630ca\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2415466\",\"name\":\"Weijiang Feng\"},{\"authorId\":\"145542203\",\"name\":\"Naiyang Guan\"},{\"authorId\":\"47003002\",\"name\":\"Yuan Li\"},{\"authorId\":\"46447502\",\"name\":\"X. Zhang\"},{\"authorId\":\"145254061\",\"name\":\"Z. Luo\"}],\"doi\":\"10.1109/IJCNN.2017.7965918\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"24b9ebb686b6af8e7920dccccce3f01011f58862\",\"title\":\"Audio visual speech recognition with multimodal recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/24b9ebb686b6af8e7920dccccce3f01011f58862\",\"venue\":\"2017 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2017},{\"arxivId\":\"1910.00628\",\"authors\":[{\"authorId\":\"51036510\",\"name\":\"A. Narayanan\"},{\"authorId\":\"71124982\",\"name\":\"Avinash Siravuru\"},{\"authorId\":\"2086607\",\"name\":\"B. Dariush\"}],\"doi\":\"10.1109/LRA.2020.2967738\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"997a31a9a32a1769adccbf4a5f77d69ac29fe229\",\"title\":\"Gated Recurrent Fusion to Learn Driving Behavior from Temporal Multimodal Data\",\"url\":\"https://www.semanticscholar.org/paper/997a31a9a32a1769adccbf4a5f77d69ac29fe229\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3442611\",\"name\":\"David Semedo\"},{\"authorId\":\"32552576\",\"name\":\"J. Magalh\\u00e3es\"}],\"doi\":\"10.1145/3394171.3413540\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d211dbf630834c025cb29fc82fe6aee4d0eabce\",\"title\":\"Adaptive Temporal Triplet-loss for Cross-modal Embedding Learning\",\"url\":\"https://www.semanticscholar.org/paper/0d211dbf630834c025cb29fc82fe6aee4d0eabce\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1708.05127\",\"authors\":[{\"authorId\":\"48080348\",\"name\":\"D. Hu\"},{\"authorId\":\"144962210\",\"name\":\"F. Nie\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1145/3123266.3123355\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2c0a7a7de693d7c24189d4a3c1cc668fab9ab38\",\"title\":\"Deep Binary Reconstruction for Cross-Modal Hashing\",\"url\":\"https://www.semanticscholar.org/paper/e2c0a7a7de693d7c24189d4a3c1cc668fab9ab38\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31719224\",\"name\":\"Y. Wen\"},{\"authorId\":\"144865366\",\"name\":\"K. Yao\"},{\"authorId\":\"8788762\",\"name\":\"Chunlin Tian\"},{\"authorId\":\"1699018\",\"name\":\"Y. Wu\"},{\"authorId\":\"47294000\",\"name\":\"Zhongmin Zhang\"},{\"authorId\":\"46571676\",\"name\":\"Y. Shi\"},{\"authorId\":\"49902152\",\"name\":\"Y. Tian\"},{\"authorId\":\"1686631\",\"name\":\"J. Yang\"},{\"authorId\":\"46808681\",\"name\":\"Peiqi Wang\"}],\"doi\":\"10.1007/978-3-030-00021-9_35\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6695f376d1e33e6eb556352ccac7633f6927300b\",\"title\":\"Aggregated Multimodal Bidirectional Recurrent Model for Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6695f376d1e33e6eb556352ccac7633f6927300b\",\"venue\":\"ICCCS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"116252480\",\"name\":\"M. Heydari\"},{\"authorId\":\"14256060\",\"name\":\"S. Shiry Ghidary\"}],\"doi\":\"10.1109/ACCESS.2019.2904117\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3e9f6c214c672299347f99cfa88094b98348f963\",\"title\":\"3D Motion Reconstruction From 2D Motion Data Using Multimodal Conditional Deep Belief Network\",\"url\":\"https://www.semanticscholar.org/paper/3e9f6c214c672299347f99cfa88094b98348f963\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2027003826\",\"name\":\"Mingfeng Hao\"},{\"authorId\":\"26957200\",\"name\":\"Mutallip Mamut\"},{\"authorId\":\"2027003608\",\"name\":\"Nurbiya Yadikar\"},{\"authorId\":\"2119595\",\"name\":\"A. Aysa\"},{\"authorId\":\"1869945\",\"name\":\"K. Ubul\"}],\"doi\":\"10.1109/ACCESS.2020.3036865\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3014cc7546ad2c35d47fd6627233d92d7ae40a85\",\"title\":\"A Survey of Research on Lipreading Technology\",\"url\":\"https://www.semanticscholar.org/paper/3014cc7546ad2c35d47fd6627233d92d7ae40a85\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2585807\",\"name\":\"Gonzalo D. Sad\"},{\"authorId\":\"66006510\",\"name\":\"Juan Carlos G\\u00f3mez\"}],\"doi\":\"10.23919/Eusipco47968.2020.9287615\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ab71c1bb7900f595de9167e08b7e9ddfd031e6b\",\"title\":\"Audio-Visual Speech Classification based on Absent Class Detection\",\"url\":\"https://www.semanticscholar.org/paper/9ab71c1bb7900f595de9167e08b7e9ddfd031e6b\",\"venue\":\"2020 28th European Signal Processing Conference (EUSIPCO)\",\"year\":2021},{\"arxivId\":\"2005.08449\",\"authors\":[{\"authorId\":\"153211990\",\"name\":\"Di Hu\"},{\"authorId\":\"48568841\",\"name\":\"Xuhong Li\"},{\"authorId\":\"35041003\",\"name\":\"Lichao Mou\"},{\"authorId\":\"36352940\",\"name\":\"P. Jin\"},{\"authorId\":\"144230587\",\"name\":\"D. Chen\"},{\"authorId\":\"104530500\",\"name\":\"L. Jing\"},{\"authorId\":\"1761972\",\"name\":\"X. Zhu\"},{\"authorId\":\"1721158\",\"name\":\"D. Dou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f82e61342920bbbedab956df50fa64eda1a00c32\",\"title\":\"Cross-Task Transfer for Multimodal Aerial Scene Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f82e61342920bbbedab956df50fa64eda1a00c32\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151104038\",\"name\":\"Michalis Angelou\"},{\"authorId\":\"39894135\",\"name\":\"Vassilis Solachidis\"},{\"authorId\":\"1738865\",\"name\":\"N. Vretos\"},{\"authorId\":\"1747572\",\"name\":\"P. Daras\"}],\"doi\":\"10.1016/J.PATCOG.2019.06.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2609821b209198d97a016d9f4e95d8aad6b5cf6a\",\"title\":\"Graph-based multimodal fusion with metric learning for multimodal classification\",\"url\":\"https://www.semanticscholar.org/paper/2609821b209198d97a016d9f4e95d8aad6b5cf6a\",\"venue\":\"Pattern Recognit.\",\"year\":2019},{\"arxivId\":\"2005.07097\",\"authors\":[{\"authorId\":\"153211990\",\"name\":\"Di Hu\"},{\"authorId\":\"35041003\",\"name\":\"Lichao Mou\"},{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"46930271\",\"name\":\"Junyu Gao\"},{\"authorId\":\"51151222\",\"name\":\"Yuansheng Hua\"},{\"authorId\":\"1721158\",\"name\":\"D. Dou\"},{\"authorId\":\"40049070\",\"name\":\"X. Zhu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd0438b43d63605356bf5bcfedb8bd1e99803cdc\",\"title\":\"Ambient Sound Helps: Audiovisual Crowd Counting in Extreme Conditions\",\"url\":\"https://www.semanticscholar.org/paper/bd0438b43d63605356bf5bcfedb8bd1e99803cdc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.12867\",\"authors\":[{\"authorId\":\"66066175\",\"name\":\"D. U. Jo\"},{\"authorId\":\"3206837\",\"name\":\"Byeongju Lee\"},{\"authorId\":\"47819455\",\"name\":\"J. Choi\"},{\"authorId\":\"47111186\",\"name\":\"H. Yoo\"},{\"authorId\":\"46174575\",\"name\":\"J. Choi\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3591a27e24c3197512331d8a8cc2f481b36d58fa\",\"title\":\"Cross-modal Variational Auto-encoder with Distributed Latent Spaces and Associators\",\"url\":\"https://www.semanticscholar.org/paper/3591a27e24c3197512331d8a8cc2f481b36d58fa\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144679565\",\"name\":\"N. Zhang\"},{\"authorId\":\"145613107\",\"name\":\"Shifei Ding\"},{\"authorId\":\"26433217\",\"name\":\"J. Zhang\"},{\"authorId\":\"153447505\",\"name\":\"Y. Xue\"}],\"doi\":\"10.1016/j.neucom.2017.09.065\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"874bf9750efab1b7d65dfb1cd1d6c6576e360ddb\",\"title\":\"An overview on Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/874bf9750efab1b7d65dfb1cd1d6c6576e360ddb\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1803.08842\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"145458657\",\"name\":\"Jing Shi\"},{\"authorId\":\"2868721\",\"name\":\"Bochen Li\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01216-8_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"title\":\"Audio-Visual Event Localization in Unconstrained Videos\",\"url\":\"https://www.semanticscholar.org/paper/a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2007.09902\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"48670507\",\"name\":\"Xudong Xu\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"}],\"doi\":\"10.1007/978-3-030-58610-2_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"73aa926dad010a3f1bb89faa31241f97a89cc461\",\"title\":\"Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/73aa926dad010a3f1bb89faa31241f97a89cc461\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.03983\",\"authors\":[{\"authorId\":\"1557300901\",\"name\":\"Mingshuang Luo\"},{\"authorId\":\"7389074\",\"name\":\"S. Yang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0872f4a6e14c6da98424e6daefe4d9b626ba4d3d\",\"title\":\"Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence Lip-Reading\",\"url\":\"https://www.semanticscholar.org/paper/0872f4a6e14c6da98424e6daefe4d9b626ba4d3d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"}],\"doi\":\"10.1109/TASLP.2017.2783545\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2294754f2e362814dcf030327f17a8a4749d00a6\",\"title\":\"Comparing Fusion Models for DNN-Based Audiovisual Continuous Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2294754f2e362814dcf030327f17a8a4749d00a6\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2208555\",\"name\":\"A. S. Saudi\"},{\"authorId\":\"145359879\",\"name\":\"M. Khalil\"},{\"authorId\":\"1706882\",\"name\":\"H. Abbas\"}],\"doi\":\"10.1016/J.DSP.2019.02.016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"773be5bb3cc77f2067718d59ff494e3e56858d5a\",\"title\":\"Improved features and dynamic stream weight adaption for robust Audio-Visual Speech Recognition framework\",\"url\":\"https://www.semanticscholar.org/paper/773be5bb3cc77f2067718d59ff494e3e56858d5a\",\"venue\":\"Digit. Signal Process.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144598359\",\"name\":\"Qianli Ma\"},{\"authorId\":\"48491340\",\"name\":\"L. Shen\"},{\"authorId\":\"28921865\",\"name\":\"Ruishi Su\"},{\"authorId\":\"47739563\",\"name\":\"Jieyu Chen\"}],\"doi\":\"10.1007/978-3-319-70096-0_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d52ebb85f484fe9fe40888f4455dc2c3d6ba6ae\",\"title\":\"Two-Stage Temporal Multimodal Learning for Speaker and Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6d52ebb85f484fe9fe40888f4455dc2c3d6ba6ae\",\"venue\":\"ICONIP\",\"year\":2017},{\"arxivId\":\"1812.09336\",\"authors\":[{\"authorId\":\"29951387\",\"name\":\"Devesh Walawalkar\"},{\"authorId\":\"39838894\",\"name\":\"Yihui He\"},{\"authorId\":\"26364117\",\"name\":\"Rohit Pillai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"120ce554a8de66d8f5b28bfbe8f0cb720f0abfa2\",\"title\":\"An Empirical Analysis of Deep Audio-Visual Models for Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/120ce554a8de66d8f5b28bfbe8f0cb720f0abfa2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1810.04547\",\"authors\":[{\"authorId\":\"3442611\",\"name\":\"David Semedo\"},{\"authorId\":\"144431718\",\"name\":\"J. Magalh\\u00e3es\"}],\"doi\":\"10.1145/3240508.3240665\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"243575d6baf3ba8e2b8783abc4a35d47bd194929\",\"title\":\"Temporal Cross-Media Retrieval with Soft-Smoothing\",\"url\":\"https://www.semanticscholar.org/paper/243575d6baf3ba8e2b8783abc4a35d47bd194929\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"145420448\",\"name\":\"E. Morais\"}],\"doi\":\"10.1109/ICME.2018.8486470\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0710d2df0200af259ea0f7c284f885b81fcb85b7\",\"title\":\"Improving CNN-Based Viseme Recognition Using Synthetic Data\",\"url\":\"https://www.semanticscholar.org/paper/0710d2df0200af259ea0f7c284f885b81fcb85b7\",\"venue\":\"2018 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2018},{\"arxivId\":\"2005.08449\",\"authors\":[{\"authorId\":\"153211990\",\"name\":\"Di Hu\"},{\"authorId\":\"48568841\",\"name\":\"Xuhong Li\"},{\"authorId\":\"35041003\",\"name\":\"Lichao Mou\"},{\"authorId\":\"36352940\",\"name\":\"P. Jin\"},{\"authorId\":\"144230587\",\"name\":\"D. Chen\"},{\"authorId\":\"104530500\",\"name\":\"L. Jing\"},{\"authorId\":\"1761972\",\"name\":\"X. Zhu\"},{\"authorId\":\"1721158\",\"name\":\"D. Dou\"}],\"doi\":\"10.1007/978-3-030-58586-0_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7fabb1ef96d2840834cfaf384408309bafc588d5\",\"title\":\"Cross-Task Transfer for Geotagged Audiovisual Aerial Scene Recognition\",\"url\":\"https://www.semanticscholar.org/paper/7fabb1ef96d2840834cfaf384408309bafc588d5\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1810.03414\",\"authors\":[{\"authorId\":\"153211990\",\"name\":\"Di Hu\"},{\"authorId\":\"144962210\",\"name\":\"F. Nie\"},{\"authorId\":\"40286455\",\"name\":\"Xuelong Li\"}],\"doi\":\"10.1109/ICASSP.2019.8683898\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97924ebc712b3739adfdadfd428247fce84af4b5\",\"title\":\"Dense Multimodal Fusion for Hierarchically Joint Representation\",\"url\":\"https://www.semanticscholar.org/paper/97924ebc712b3739adfdadfd428247fce84af4b5\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1904.01954\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"2563750\",\"name\":\"Yujiang Wang\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"8798628\",\"name\":\"Z. Li\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1016/j.patrec.2020.01.022\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4c785dcf80ae0f8d5fab1960ef2afc6cefcebd8a\",\"title\":\"End-to-End Visual Speech Recognition for Small-Scale Datasets\",\"url\":\"https://www.semanticscholar.org/paper/4c785dcf80ae0f8d5fab1960ef2afc6cefcebd8a\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":\"1611.06986\",\"authors\":[{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"},{\"authorId\":\"143867160\",\"name\":\"F. Torre\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f704e6f70729edeb1ff0087e9a87d1892f9be598\",\"title\":\"Robust end-to-end deep audiovisual speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/f704e6f70729edeb1ff0087e9a87d1892f9be598\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2008.03546\",\"authors\":[{\"authorId\":\"1657277123\",\"name\":\"J. Xia\"},{\"authorId\":\"36290866\",\"name\":\"Anyi Rao\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":\"150196512\",\"name\":\"Linning Xu\"},{\"authorId\":\"49320456\",\"name\":\"Jiangtao Wen\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-58610-2_11\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e71db960a43bf26d9de3515652f04579952ab6ff\",\"title\":\"Online Multi-modal Person Search in Videos\",\"url\":\"https://www.semanticscholar.org/paper/e71db960a43bf26d9de3515652f04579952ab6ff\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9936815\",\"name\":\"Olga Slizovskaia\"},{\"authorId\":\"145217215\",\"name\":\"E. G\\u00f3mez\"},{\"authorId\":\"1916387\",\"name\":\"G. Haro\"}],\"doi\":\"10.1145/3078971.3079002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"34912edb1cf0576ff36ca9c4f651237f9115deed\",\"title\":\"Musical Instrument Recognition in User-generated Videos using a Multimodal Convolutional Neural Network Architecture\",\"url\":\"https://www.semanticscholar.org/paper/34912edb1cf0576ff36ca9c4f651237f9115deed\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48080348\",\"name\":\"D. Hu\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1145/2964284.2967239\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aced38f6fde9ecee39b545a05d9370ea8b86b658\",\"title\":\"Multimodal Learning via Exploring Deep Semantic Similarity\",\"url\":\"https://www.semanticscholar.org/paper/aced38f6fde9ecee39b545a05d9370ea8b86b658\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1701.04224\",\"authors\":[{\"authorId\":\"8788762\",\"name\":\"Chunlin Tian\"},{\"authorId\":\"3341684\",\"name\":\"W. Ji\"},{\"authorId\":\"144156865\",\"name\":\"Y. Yuan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"490e1cf07f2dbfd05b2d93b619d64f0d21e9f77a\",\"title\":\"Auxiliary Multimodal LSTM for Audio-visual Speech Recognition and Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/490e1cf07f2dbfd05b2d93b619d64f0d21e9f77a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1810.00108\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/SLT.2018.8639643\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b553627247acd185fae1e2daff8b155528040be9\",\"title\":\"Audio-Visual Speech Recognition with a Hybrid CTC/Attention Architecture\",\"url\":\"https://www.semanticscholar.org/paper/b553627247acd185fae1e2daff8b155528040be9\",\"venue\":\"2018 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"150092714\",\"name\":\"Edmilson da Silva Morais\"}],\"doi\":\"10.1109/FG.2019.8756589\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9d06ca43fb061e3136d08b74dbe9ea081d17740\",\"title\":\"Improving Viseme Recognition with GAN-based Muti-view Mapping\",\"url\":\"https://www.semanticscholar.org/paper/c9d06ca43fb061e3136d08b74dbe9ea081d17740\",\"venue\":\"FG\",\"year\":2019},{\"arxivId\":\"1704.08619\",\"authors\":[{\"authorId\":\"2829366\",\"name\":\"Panagiotis Tzirakis\"},{\"authorId\":\"2814229\",\"name\":\"George Trigeorgis\"},{\"authorId\":\"1752913\",\"name\":\"Mihalis A. Nicolaou\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"}],\"doi\":\"10.1109/JSTSP.2017.2764438\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cf0c072eeaec310aff806bdb7a7931b81e9ed99\",\"title\":\"End-to-End Multimodal Emotion Recognition Using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/8cf0c072eeaec310aff806bdb7a7931b81e9ed99\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2719506\",\"name\":\"Andr\\u00e9a Britto Mattos\"},{\"authorId\":\"34475405\",\"name\":\"D. Oliveira\"},{\"authorId\":\"145420448\",\"name\":\"E. Morais\"}],\"doi\":\"10.1109/ICIP.2018.8451435\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db64d176ef22091a3249c3a3b95226ad4bc3e19e\",\"title\":\"Towards View-Independent Viseme Recognition Based on CNNS and Synthetic Data\",\"url\":\"https://www.semanticscholar.org/paper/db64d176ef22091a3249c3a3b95226ad4bc3e19e\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1996104375\",\"name\":\"Mehmet Altinkaya\"},{\"authorId\":\"144638781\",\"name\":\"A. Smeulders\"}],\"doi\":\"10.1145/3423325.3423733\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a649c32fe74411a46dd8a310a1612630ca398ea\",\"title\":\"A Dynamic, Self Supervised, Large Scale AudioVisual Dataset for Stuttered Speech\",\"url\":\"https://www.semanticscholar.org/paper/1a649c32fe74411a46dd8a310a1612630ca398ea\",\"venue\":\"\",\"year\":2020}],\"corpusId\":12177509,\"doi\":\"10.1109/CVPR.2016.389\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":5,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"51e0f0ebda30075940c9cd8b07047eddc2505663\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":\"10.1126/SCIENCE.1127647\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e\",\"title\":\"Reducing the Dimensionality of Data with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e\",\"venue\":\"Science\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"},{\"authorId\":\"7205190\",\"name\":\"T. Cootes\"},{\"authorId\":\"144427091\",\"name\":\"J. Bangham\"},{\"authorId\":\"35132323\",\"name\":\"S. Cox\"},{\"authorId\":\"144439756\",\"name\":\"R. Harvey\"}],\"doi\":\"10.1109/34.982900\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f78867834f7f6797ca6396f98edb10aad2a864fb\",\"title\":\"Extraction of Visual Features for Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/f78867834f7f6797ca6396f98edb10aad2a864fb\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2957517\",\"name\":\"Tijmen Tieleman\"}],\"doi\":\"10.1145/1390156.1390290\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"73d6a26f407db77506959fdf3f7b853e44f3844a\",\"title\":\"Training restricted Boltzmann machines using approximations to the likelihood gradient\",\"url\":\"https://www.semanticscholar.org/paper/73d6a26f407db77506959fdf3f7b853e44f3844a\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4f7476037408ac3d993f5088544aab427bc319c1\",\"title\":\"Information processing in dynamical systems: foundations of harmony theory\",\"url\":\"https://www.semanticscholar.org/paper/4f7476037408ac3d993f5088544aab427bc319c1\",\"venue\":\"\",\"year\":1986},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1731948\",\"name\":\"P. Viola\"},{\"authorId\":\"145319478\",\"name\":\"Michael J. Jones\"}],\"doi\":\"10.1109/CVPR.2001.990517\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63\",\"title\":\"Rapid object detection using a boosted cascade of simple features\",\"url\":\"https://www.semanticscholar.org/paper/dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63\",\"venue\":\"Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"9330607\",\"name\":\"S. Roweis\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"92e0f569d8fb17559d580d1c3b16a70e682b48b9\",\"title\":\"Two Distributed-State Models For Generating High-Dimensional Time Series\",\"url\":\"https://www.semanticscholar.org/paper/92e0f569d8fb17559d580d1c3b16a70e682b48b9\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1785530\",\"name\":\"Roni Mittelman\"},{\"authorId\":\"145585296\",\"name\":\"B. Kuipers\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5209f8559c97800ae06451cd34c14506c20ec23\",\"title\":\"Structured Recurrent Temporal Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/c5209f8559c97800ae06451cd34c14506c20ec23\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1206.6392\",\"authors\":[{\"authorId\":\"1395619597\",\"name\":\"Nicolas Boulanger-Lewandowski\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"120247189\",\"name\":\"Pascal Vincent\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8\",\"title\":\"Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription\",\"url\":\"https://www.semanticscholar.org/paper/18c82d4b6cf94fb84ba6ea230e80cb07ed9a9cf8\",\"venue\":\"ICML\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1162/089976602760128018\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9360e5ce9c98166bb179ad479a9d2919ff13d022\",\"title\":\"Training Products of Experts by Minimizing Contrastive Divergence\",\"url\":\"https://www.semanticscholar.org/paper/9360e5ce9c98166bb179ad479a9d2919ff13d022\",\"venue\":\"Neural Computation\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4599641\",\"name\":\"M. Amer\"},{\"authorId\":\"1832513\",\"name\":\"Behjat Siddiquie\"},{\"authorId\":\"144791561\",\"name\":\"Saad M. Khan\"},{\"authorId\":\"1696401\",\"name\":\"Ajay Divakaran\"},{\"authorId\":\"1733393\",\"name\":\"H. Sawhney\"}],\"doi\":\"10.1109/WACV.2014.6836053\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"cfd123e03844c32f8a25f6c4bfc0a93e145ecc2d\",\"title\":\"Multimodal fusion using dynamic hybrid models\",\"url\":\"https://www.semanticscholar.org/paper/cfd123e03844c32f8a25f6c4bfc0a93e145ecc2d\",\"venue\":\"IEEE Winter Conference on Applications of Computer Vision\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"},{\"authorId\":\"2264160\",\"name\":\"C. Neti\"},{\"authorId\":\"1678373\",\"name\":\"J. Luettin\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"afb50fe3d6490ad5cd0b624ac72e569fbf33f619\",\"title\":\"Audio-Visual Automatic Speech Recognition: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/afb50fe3d6490ad5cd0b624ac72e569fbf33f619\",\"venue\":\"\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2681887\",\"name\":\"D. Rumelhart\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1038/323533a0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"052b1d8ce63b07fec3de9dbb583772d860b7c769\",\"title\":\"Learning representations by back-propagating errors\",\"url\":\"https://www.semanticscholar.org/paper/052b1d8ce63b07fec3de9dbb583772d860b7c769\",\"venue\":\"Nature\",\"year\":1986},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"1727524\",\"name\":\"Michael L. Seltzer\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"},{\"authorId\":\"1723644\",\"name\":\"A. Acero\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e3c1bf806c325f306e5084c3bd332b83d2077e2a\",\"title\":\"Binary coding of speech spectrograms using a deep auto-encoder\",\"url\":\"https://www.semanticscholar.org/paper/e3c1bf806c325f306e5084c3bd332b83d2077e2a\",\"venue\":\"INTERSPEECH\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"2812486\",\"name\":\"P. Simard\"},{\"authorId\":\"1688235\",\"name\":\"P. Frasconi\"}],\"doi\":\"10.1109/72.279181\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d0be39ee052d246ae99c082a565aba25b811be2d\",\"title\":\"Learning long-term dependencies with gradient descent is difficult\",\"url\":\"https://www.semanticscholar.org/paper/d0be39ee052d246ae99c082a565aba25b811be2d\",\"venue\":\"IEEE Trans. Neural Networks\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1729571\",\"name\":\"Kihyuk Sohn\"},{\"authorId\":\"3163480\",\"name\":\"W. Shang\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"64bfeb1ddd35838706e4fffc469234cc2f215631\",\"title\":\"Improved Multimodal Deep Learning with Variation of Information\",\"url\":\"https://www.semanticscholar.org/paper/64bfeb1ddd35838706e4fffc469234cc2f215631\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"9330607\",\"name\":\"S. Roweis\"}],\"doi\":\"10.7551/mitpress/7503.003.0173\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"497a80b2813cffb17f46af50e621a71505094528\",\"title\":\"Modeling Human Motion Using Binary Latent Variables\",\"url\":\"https://www.semanticscholar.org/paper/497a80b2813cffb17f46af50e621a71505094528\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G Potamianos\"},{\"authorId\":null,\"name\":\"C Neti\"},{\"authorId\":null,\"name\":\"J Luettin\"},{\"authorId\":null,\"name\":\"I Matthews\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Audiovisual automatic speech recognition: An overview. Issues in visual and audio-visual speech processing\",\"url\":\"\",\"venue\":\"Audiovisual automatic speech recognition: An overview. Issues in visual and audio-visual speech processing\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49025046\",\"name\":\"Jing Huang\"},{\"authorId\":\"144707379\",\"name\":\"Brian Kingsbury\"}],\"doi\":\"10.1109/ICASSP.2013.6639140\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a039e33733ed88fecc5b353e0d7ab26817e122d\",\"title\":\"Audio-visual deep learning for noise robust speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/3a039e33733ed88fecc5b353e0d7ab26817e122d\",\"venue\":\"2013 IEEE International Conference on Acoustics, Speech and Signal Processing\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0228810a988f6b8f06337e14f564e2fd3f6e1056\",\"title\":\"The Recurrent Temporal Restricted Boltzmann Machine\",\"url\":\"https://www.semanticscholar.org/paper/0228810a988f6b8f06337e14f564e2fd3f6e1056\",\"venue\":\"NIPS\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35132323\",\"name\":\"S. Cox\"},{\"authorId\":\"144439756\",\"name\":\"R. Harvey\"},{\"authorId\":\"1934275\",\"name\":\"Yuxuan Lan\"},{\"authorId\":\"2537639\",\"name\":\"J. Newman\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3ccf2122a3890b062798790acc8d1d1c084a7b0f\",\"title\":\"The challenge of multispeaker lip-reading\",\"url\":\"https://www.semanticscholar.org/paper/3ccf2122a3890b062798790acc8d1d1c084a7b0f\",\"venue\":\"AVSP\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G Zhao\"},{\"authorId\":null,\"name\":\"M Barnard\"},{\"authorId\":null,\"name\":\"M Pietik\\u00e4inen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lipreading with local spatiotemporal descriptors. Multimedia\",\"url\":\"\",\"venue\":\"IEEE Transactions on\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2957517\",\"name\":\"Tijmen Tieleman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"52b5f395418dd3095f7bc9c7b5ade94f984c33c0\",\"title\":\"On the Convergence Properties of Contrastive Divergence\",\"url\":\"https://www.semanticscholar.org/paper/52b5f395418dd3095f7bc9c7b5ade94f984c33c0\",\"venue\":\"AISTATS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"85021c84383d18a7a4434d76dc8135fc6bdc0aa6\",\"title\":\"Deep Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/85021c84383d18a7a4434d76dc8135fc6bdc0aa6\",\"venue\":\"AISTATS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143967982\",\"name\":\"M. Cooke\"},{\"authorId\":\"32406400\",\"name\":\"J. Barker\"},{\"authorId\":\"75117630\",\"name\":\"S. Cunningham\"},{\"authorId\":\"48914950\",\"name\":\"X. Shao\"}],\"doi\":\"10.1121/1.2229005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"title\":\"An audio-visual corpus for speech perception and automatic speech recognition.\",\"url\":\"https://www.semanticscholar.org/paper/5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2020608\",\"name\":\"J. Ngiam\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"4738460\",\"name\":\"Mingyu Kim\"},{\"authorId\":\"145578392\",\"name\":\"Juhan Nam\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"80e9e3fc3670482c1fee16b2542061b779f47c4f\",\"title\":\"Multimodal Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/80e9e3fc3670482c1fee16b2542061b779f47c4f\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47854141\",\"name\":\"Sidney S. Simon\"}],\"doi\":\"10.3389/neuro.01.019.2008\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"320564bfe97930cff3f86dd90e2f01c73360e8ea\",\"title\":\"Merging of the Senses\",\"url\":\"https://www.semanticscholar.org/paper/320564bfe97930cff3f86dd90e2f01c73360e8ea\",\"venue\":\"Front. Neurosci.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1016/j.ijar.2008.11.006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a\",\"title\":\"Semantic hashing\",\"url\":\"https://www.semanticscholar.org/paper/cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a\",\"venue\":\"Int. J. Approx. Reason.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5726c7b40fcc454b77d989656c085520bf6c15fa\",\"title\":\"Multimodal learning with deep Boltzmann machines\",\"url\":\"https://www.semanticscholar.org/paper/5726c7b40fcc454b77d989656c085520bf6c15fa\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48198489\",\"name\":\"Di Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"44111392d9540097a4eb65f1f909675999b3871d\",\"title\":\"Human Action Recognition Using Deep Probabilistic Graphical Models\",\"url\":\"https://www.semanticscholar.org/paper/44111392d9540097a4eb65f1f909675999b3871d\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1757287\",\"name\":\"G. Zhao\"},{\"authorId\":\"144302675\",\"name\":\"M. Barnard\"},{\"authorId\":\"145962204\",\"name\":\"M. Pietik\\u00e4inen\"}],\"doi\":\"10.1109/TMM.2009.2030637\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c4a6a915a7fbb9af5beeff55bf7d8cef18bf93a\",\"title\":\"Lipreading With Local Spatiotemporal Descriptors\",\"url\":\"https://www.semanticscholar.org/paper/8c4a6a915a7fbb9af5beeff55bf7d8cef18bf93a\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2009},{\"arxivId\":\"1710.01122\",\"authors\":[{\"authorId\":\"3192282\",\"name\":\"Helen L. Bear\"},{\"authorId\":\"35132323\",\"name\":\"S. Cox\"},{\"authorId\":\"144439756\",\"name\":\"R. Harvey\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9133c54e2a5ba4b2fbb91d5c76368e7968da5cde\",\"title\":\"Speaker-independent machine lip-reading with speaker-dependent viseme classifiers\",\"url\":\"https://www.semanticscholar.org/paper/9133c54e2a5ba4b2fbb91d5c76368e7968da5cde\",\"venue\":\"AVSP\",\"year\":2015}],\"title\":\"Temporal Multimodal Learning in Audiovisual Speech Recognition\",\"topics\":[{\"topic\":\"Multimodal interaction\",\"topicId\":\"42592\",\"url\":\"https://www.semanticscholar.org/topic/42592\"},{\"topic\":\"Multimodal learning\",\"topicId\":\"406692\",\"url\":\"https://www.semanticscholar.org/topic/406692\"},{\"topic\":\"Restricted Boltzmann machine\",\"topicId\":\"298704\",\"url\":\"https://www.semanticscholar.org/topic/298704\"},{\"topic\":\"Audio-visual speech recognition\",\"topicId\":\"53783\",\"url\":\"https://www.semanticscholar.org/topic/53783\"},{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"},{\"topic\":\"Academy\",\"topicId\":\"2609\",\"url\":\"https://www.semanticscholar.org/topic/2609\"},{\"topic\":\"Open research\",\"topicId\":\"1298\",\"url\":\"https://www.semanticscholar.org/topic/1298\"},{\"topic\":\"Modality (human\\u2013computer interaction)\",\"topicId\":\"462\",\"url\":\"https://www.semanticscholar.org/topic/462\"},{\"topic\":\"Imaging technology\",\"topicId\":\"116453\",\"url\":\"https://www.semanticscholar.org/topic/116453\"}],\"url\":\"https://www.semanticscholar.org/paper/51e0f0ebda30075940c9cd8b07047eddc2505663\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016}\n"