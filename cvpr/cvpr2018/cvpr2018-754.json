"{\"abstract\":\"We introduce a novel framework for image captioning that can produce natural language explicitly grounded in entities that object detectors find in the image. Our approach reconciles classical slot filling approaches (that are generally better grounded in images) with modern neural captioning approaches (that are generally more natural sounding and accurate). Our approach first generates a sentence 'template' with slot locations explicitly tied to specific image regions. These slots are then filled in by visual concepts identified in the regions by object detectors. The entire architecture (sentence template generation and slot filling with object detectors) is end-to-end differentiable. We verify the effectiveness of our proposed model on different image captioning tasks. On standard image captioning and novel object captioning, our model reaches state-of-the-art on both COCO and Flickr30k datasets. We also demonstrate that our model has unique advantages when the train and test distributions of scene compositions - and hence language priors of associated captions - are different. Code has been made available at: https://github.com/jiasenlu/NeuralBabyTalk.\",\"arxivId\":\"1803.09845\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\",\"url\":\"https://www.semanticscholar.org/author/8553015\"},{\"authorId\":\"94908120\",\"name\":\"Jianwei Yang\",\"url\":\"https://www.semanticscholar.org/author/94908120\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\",\"url\":\"https://www.semanticscholar.org/author/1746610\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\",\"url\":\"https://www.semanticscholar.org/author/153432684\"}],\"citationVelocity\":67,\"citations\":[{\"arxivId\":\"1904.11251\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2019.01278\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"title\":\"Pointing Novel Objects in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1911.09753\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.1609/AAAI.V34I03.5655\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"title\":\"Reinforcing an Image Caption Generator Using Off-Line Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1812.08658\",\"authors\":[{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"46395829\",\"name\":\"Yufei Wang\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"145461380\",\"name\":\"Rishabh Jain\"},{\"authorId\":\"1607624548\",\"name\":\"Mark Johnson\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606363958\",\"name\":\"Devi Parikh\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"}],\"doi\":\"10.1109/ICCV.2019.00904\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"title\":\"nocaps: novel object captioning at scale\",\"url\":\"https://www.semanticscholar.org/paper/8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696589727\",\"name\":\"Dongming Zhou\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"48708659\",\"name\":\"Zhiwen Wang\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206932\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a32ac0995afcbc748fa6533b941e9834ec7bfc2e\",\"title\":\"Multi-level Visual Fusion Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a32ac0995afcbc748fa6533b941e9834ec7bfc2e\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1909.03169\",\"authors\":[{\"authorId\":\"32095408\",\"name\":\"Fawaz Sammani\"},{\"authorId\":\"67001969\",\"name\":\"Mahmoud Elsayed\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7af5c94fedc32f3104a08301a46c62f51b044a81\",\"title\":\"Look and Modify: Modification Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7af5c94fedc32f3104a08301a46c62f51b044a81\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1911.11010\",\"authors\":[{\"authorId\":\"35153729\",\"name\":\"A. V. Savchenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"804140158763a4e239f3dcda0d88c31eca632855\",\"title\":\"Event Recognition with Automatic Album Detection based on Sequential Processing, Neural Attention and Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/804140158763a4e239f3dcda0d88c31eca632855\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"153028537\",\"name\":\"X. Wu\"},{\"authorId\":\"36263371\",\"name\":\"Shen Ge\"},{\"authorId\":\"93249636\",\"name\":\"W. Fan\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1609/AAAI.V34I07.6824\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d40c5e2d42d19dd7fd85d45201c048e3768c740d\",\"title\":\"Federated Learning for Vision-and-Language Grounding Problems\",\"url\":\"https://www.semanticscholar.org/paper/d40c5e2d42d19dd7fd85d45201c048e3768c740d\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1812.02378\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/CVPR.2019.01094\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f6feb1af1809dfd872d868dfcc13021cc42f496c\",\"title\":\"Auto-Encoding Scene Graphs for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f6feb1af1809dfd872d868dfcc13021cc42f496c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"51491153\",\"name\":\"Takayuki Itamochi\"},{\"authorId\":\"3193466\",\"name\":\"K. Narioka\"},{\"authorId\":\"152599661\",\"name\":\"Ikuro Sato\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/ICCV.2019.00589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"949959250e487617faa3035b05b52aa03a8e9895\",\"title\":\"Generating Easy-to-Understand Referring Expressions for Target Identifications\",\"url\":\"https://www.semanticscholar.org/paper/949959250e487617faa3035b05b52aa03a8e9895\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2814633\",\"name\":\"M. Bustreo\"},{\"authorId\":\"3393678\",\"name\":\"Jacopo Cavazza\"},{\"authorId\":\"1727204\",\"name\":\"Vittorio Murino\"}],\"doi\":\"10.1109/ICCVW.2019.00165\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"68def1260e8040e6ff139e27ec101fc4911b8e04\",\"title\":\"Enhancing Visual Embeddings through Weakly Supervised Captioning for Zero-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/68def1260e8040e6ff139e27ec101fc4911b8e04\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"2010.03743\",\"authors\":[{\"authorId\":\"13941532\",\"name\":\"Fuxiao Liu\"},{\"authorId\":null,\"name\":\"Yinghan Wang\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"51f2eb8b16d132d3c403ed76d3ac816a8f58f25c\",\"title\":\"VisualNews : Benchmark and Challenges in Entity-aware Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/51f2eb8b16d132d3c403ed76d3ac816a8f58f25c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00785\",\"authors\":[{\"authorId\":\"51134926\",\"name\":\"Xisen Jin\"},{\"authorId\":\"3109653\",\"name\":\"Junyi Du\"},{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.158\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4f91d16d7d9c21e2d9fffeff6abc78619be4d133\",\"title\":\"Visually Grounded Continual Learning of Compositional Phrases\",\"url\":\"https://www.semanticscholar.org/paper/4f91d16d7d9c21e2d9fffeff6abc78619be4d133\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1912.02315\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"}],\"doi\":\"10.1109/cvpr42600.2020.01045\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b5f3fe42548216cd93816b1bf5c437cf47bc5fbf\",\"title\":\"12-in-1: Multi-Task Vision and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/b5f3fe42548216cd93816b1bf5c437cf47bc5fbf\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"50172036\",\"name\":\"X. Wu\"},{\"authorId\":\"1993634137\",\"name\":\"Shen Ge\"},{\"authorId\":\"47958349\",\"name\":\"X. Zhang\"},{\"authorId\":\"144934703\",\"name\":\"Wei Fan\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1145/3394171.3414004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"title\":\"Bridging the Gap between Vision and Language Domains for Improved Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35153729\",\"name\":\"A. V. Savchenko\"},{\"authorId\":\"1649989753\",\"name\":\"Evgeniy V. Miasnikov\"}],\"doi\":\"10.1007/978-3-030-44584-3_33\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"81665a0882095fdea7d708062f23f9b50600cf3e\",\"title\":\"Event Recognition Based on Classification of Generated Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/81665a0882095fdea7d708062f23f9b50600cf3e\",\"venue\":\"IDA\",\"year\":2020},{\"arxivId\":\"1810.02358\",\"authors\":[{\"authorId\":\"2018393\",\"name\":\"Hyeonwoo Noh\"},{\"authorId\":\"1837923\",\"name\":\"Taehoon Kim\"},{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00858\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b80f128830114896df94999b4104cb75408e657e\",\"title\":\"Transfer Learning via Unsupervised Task Discovery for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b80f128830114896df94999b4104cb75408e657e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34589688\",\"name\":\"Zhenghang Yuan\"},{\"authorId\":\"121856647\",\"name\":\"X. Li\"},{\"authorId\":\"39669401\",\"name\":\"Q. Wang\"}],\"doi\":\"10.1109/ACCESS.2019.2962195\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"52a221f10ec19fb3b20fda0271184b641c2ccc4b\",\"title\":\"Exploring Multi-Level Attention and Semantic Relationship for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/52a221f10ec19fb3b20fda0271184b641c2ccc4b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49933533\",\"name\":\"Jing Yun\"},{\"authorId\":\"49933533\",\"name\":\"Jing Yun\"},{\"authorId\":\"48559698\",\"name\":\"Zhiwei Xu\"},{\"authorId\":\"1807620\",\"name\":\"Guanglai Gao\"}],\"doi\":\"10.1155/2020/9562587\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a20a9b1345d8919b692f7f7fe919937bf823358\",\"title\":\"Gated Object-Attribute Matching Network for Detailed Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/8a20a9b1345d8919b692f7f7fe919937bf823358\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1805970305\",\"name\":\"Georgios Barlas\"},{\"authorId\":\"1859319\",\"name\":\"Christos Veinidis\"},{\"authorId\":\"35575984\",\"name\":\"A. Arampatzis\"}],\"doi\":\"10.1007/s00371-020-01867-9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a31c2b3a88719419cf679778846edbe3be9e81b3\",\"title\":\"What we see in a photograph: content selection for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/a31c2b3a88719419cf679778846edbe3be9e81b3\",\"venue\":\"The Visual Computer\",\"year\":2020},{\"arxivId\":\"1805.12589\",\"authors\":[{\"authorId\":\"31121723\",\"name\":\"A. Deshpande\"},{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"39709900\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1109/CVPR.2019.01095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e7fe886600399448f3282c8da8fd98ab7e50eae3\",\"title\":\"Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech\",\"url\":\"https://www.semanticscholar.org/paper/e7fe886600399448f3282c8da8fd98ab7e50eae3\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2007.02517\",\"authors\":[{\"authorId\":\"32583496\",\"name\":\"Y. Fu\"},{\"authorId\":\"46999477\",\"name\":\"Tingting Liu\"},{\"authorId\":\"31933517\",\"name\":\"M. Gao\"},{\"authorId\":\"145031578\",\"name\":\"Aoying Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2395f4b80e54d4b93bf425e207aa6d2236dd5f0b\",\"title\":\"EDSL: An Encoder-Decoder Architecture with Symbol-Level Features for Printed Mathematical Expression Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2395f4b80e54d4b93bf425e207aa6d2236dd5f0b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.09543\",\"authors\":[{\"authorId\":\"49051400\",\"name\":\"J. Zhang\"},{\"authorId\":\"143953573\",\"name\":\"K. Shih\"},{\"authorId\":\"29955511\",\"name\":\"A. Tao\"},{\"authorId\":\"2301680\",\"name\":\"Bryan Catanzaro\"},{\"authorId\":\"145159522\",\"name\":\"Ahmed Elgammal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fb4b4e95164c6ef46e0e46862c33b2a76687ee11\",\"title\":\"An Interpretable Model for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/fb4b4e95164c6ef46e0e46862c33b2a76687ee11\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48289232\",\"name\":\"Mohit Bajaj\"}],\"doi\":\"10.14288/1.0380482\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef8832a4cc4d1838763df8dc7580e14706547f5a\",\"title\":\"Graph-based language grounding\",\"url\":\"https://www.semanticscholar.org/paper/ef8832a4cc4d1838763df8dc7580e14706547f5a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.00249\",\"authors\":[{\"authorId\":\"144005516\",\"name\":\"Jing Wang\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.24963/ijcai.2019/132\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0b8f0e00f953ae51a7ea72ba51c699bb959cc948\",\"title\":\"Convolutional Auto-encoding of Sentence Topics for Image Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/0b8f0e00f953ae51a7ea72ba51c699bb959cc948\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s42979-020-00238-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f732495721356944027051bb14100436a5dbdf5\",\"title\":\"AACR: Feature Fusion Effects of Algebraic Amalgamation Composed Representation on (De)Compositional Network for Caption Generation for Images\",\"url\":\"https://www.semanticscholar.org/paper/3f732495721356944027051bb14100436a5dbdf5\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":\"2001.01037\",\"authors\":[{\"authorId\":\"46969089\",\"name\":\"J. Sun\"},{\"authorId\":\"3633358\",\"name\":\"S. Lapuschkin\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"},{\"authorId\":\"49345823\",\"name\":\"Alexander Binder\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"82e836be97e706dca7029ce6a0553b4890726593\",\"title\":\"Understanding Image Captioning Models beyond Visualizing Attention\",\"url\":\"https://www.semanticscholar.org/paper/82e836be97e706dca7029ce6a0553b4890726593\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.11824\",\"authors\":[{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"47731271\",\"name\":\"Ruiyu Li\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/ICCV.2019.00898\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0db903dd28a3be3e57f40033c16cce574231f78e\",\"title\":\"Reflective Decoding Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0db903dd28a3be3e57f40033c16cce574231f78e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2003.11743\",\"authors\":[{\"authorId\":\"1491233100\",\"name\":\"Pranav Agarwal\"},{\"authorId\":\"145086911\",\"name\":\"A. Betancourt\"},{\"authorId\":\"1594025086\",\"name\":\"V. Panagiotou\"},{\"authorId\":\"2251072\",\"name\":\"N. Rodr\\u00edguez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b430a5384c82beb6102106fbea0a134425a08c23\",\"title\":\"Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models\",\"url\":\"https://www.semanticscholar.org/paper/b430a5384c82beb6102106fbea0a134425a08c23\",\"venue\":\"ICLR 2020\",\"year\":2020},{\"arxivId\":\"1812.00500\",\"authors\":[{\"authorId\":\"41022273\",\"name\":\"Duy-Kien Nguyen\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1109/CVPR.2019.01074\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac359aac85ba5d05c8249bd7dfb5d71aa205db79\",\"title\":\"Multi-Task Learning of Hierarchical Vision-Language Representation\",\"url\":\"https://www.semanticscholar.org/paper/ac359aac85ba5d05c8249bd7dfb5d71aa205db79\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35212676\",\"name\":\"J. Peyre\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"caffa07ead18aae78bf654bc57023eef58e74faf\",\"title\":\"Learning to detect visual relations\",\"url\":\"https://www.semanticscholar.org/paper/caffa07ead18aae78bf654bc57023eef58e74faf\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2004.08744\",\"authors\":[{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43d77f0547f4a1bf4faf29e5e7548564b70e758a\",\"title\":\"Are we pretraining it right? Digging deeper into visio-linguistic pretraining\",\"url\":\"https://www.semanticscholar.org/paper/43d77f0547f4a1bf4faf29e5e7548564b70e758a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.02726\",\"authors\":[{\"authorId\":\"94845899\",\"name\":\"Qianyu Feng\"},{\"authorId\":\"98264517\",\"name\":\"Y. Wu\"},{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"3863922\",\"name\":\"C. Yan\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.1109/TCSVT.2020.2965966\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"daa87e86b469d975a9ad84bfb5eee230efa8af3e\",\"title\":\"Cascaded Revision Network for Novel Object Captioning\",\"url\":\"https://www.semanticscholar.org/paper/daa87e86b469d975a9ad84bfb5eee230efa8af3e\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"145078589\",\"name\":\"Xiaojun Wan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a05227df694c93b072eb9d041a7252aead410ce\",\"title\":\"Point Precisely: Towards Ensuring the Precision of Data in Generated Texts Using Delayed Copy Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/0a05227df694c93b072eb9d041a7252aead410ce\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":\"1812.06624\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f3c111425a1e2f6316f01b9b0f3d09fa146b134\",\"title\":\"Feature Fusion Effects of Tensor Product Representation on (De)Compositional Network for Caption Generation for Images\",\"url\":\"https://www.semanticscholar.org/paper/5f3c111425a1e2f6316f01b9b0f3d09fa146b134\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1908.07129\",\"authors\":[{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/ICCV.2019.00479\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"666d9a766de11c22d9fd7d8e3c36a80ecd6f0bd0\",\"title\":\"Zero-Shot Grounding of Objects From Natural Language Queries\",\"url\":\"https://www.semanticscholar.org/paper/666d9a766de11c22d9fd7d8e3c36a80ecd6f0bd0\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1903.10122\",\"authors\":[{\"authorId\":\"46194597\",\"name\":\"C. Y. Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.1609/AAAI.V33I01.33016666\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"adc998ac4fa71bdab19537c50e3d84bf982974c1\",\"title\":\"Knowledge-driven Encode, Retrieve, Paraphrase for Medical Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/adc998ac4fa71bdab19537c50e3d84bf982974c1\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1911.10115\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"50c6889b547cc08a203842d5cf5bcb4c58e052b5\",\"title\":\"TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet Representation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/50c6889b547cc08a203842d5cf5bcb4c58e052b5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145808756\",\"name\":\"Xuanyi Dong\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"145865760\",\"name\":\"D. Zhang\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3240508.3240527\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"81b3cfd55ca84802cdcc971410e633ed40e04980\",\"title\":\"Fast Parameter Adaptation for Few-shot Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/81b3cfd55ca84802cdcc971410e633ed40e04980\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"2004.00390\",\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"5332711\",\"name\":\"Meng Wang\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/cvpr42600.2020.00483\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c936e70cc1de52c5ad0ed5ec7a219bd25a46902c\",\"title\":\"More Grounded Image Captioning by Distilling Image-Text Matching Model\",\"url\":\"https://www.semanticscholar.org/paper/c936e70cc1de52c5ad0ed5ec7a219bd25a46902c\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1905.06139\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"title\":\"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations\",\"url\":\"https://www.semanticscholar.org/paper/cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1904.01766\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"49588480\",\"name\":\"A. Myers\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2019.00756\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c41a11c0e9b8b92b4faaf97749841170b760760a\",\"title\":\"VideoBERT: A Joint Model for Video and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1901.03035\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29e13746fa5aed13e51558a521a39aaeaa99c1b1\",\"title\":\"Self-Monitoring Navigation Agent via Auxiliary Progress Estimation\",\"url\":\"https://www.semanticscholar.org/paper/29e13746fa5aed13e51558a521a39aaeaa99c1b1\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143696833\",\"name\":\"Wenjie Cai\"},{\"authorId\":\"31126576\",\"name\":\"Zheng Xiong\"},{\"authorId\":\"2303260\",\"name\":\"Xianfang Sun\"},{\"authorId\":\"1734823\",\"name\":\"Paul L. Rosin\"},{\"authorId\":\"40534966\",\"name\":\"Longcun Jin\"},{\"authorId\":\"9245443\",\"name\":\"Xinyi Peng\"}],\"doi\":\"10.3390/app10010391\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"945ffeb90e538214e3063407db8e094469ec877a\",\"title\":\"Panoptic Segmentation-Based Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/945ffeb90e538214e3063407db8e094469ec877a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2002.06436\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4ad005019f7069a3aaff959fbaa657f2ef2143fc\",\"title\":\"MRRC: Multiple Role Representation Crossover Interpretation for Image Captioning With R-CNN Feature Distribution Composition (FDC)\",\"url\":\"https://www.semanticscholar.org/paper/4ad005019f7069a3aaff959fbaa657f2ef2143fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112858943\",\"name\":\"Z. Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"title\":\"Representation Learning for Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.12462\",\"authors\":[{\"authorId\":\"144656873\",\"name\":\"O. Sidorov\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"}],\"doi\":\"10.1007/978-3-030-58536-5_44\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7442eaaf453e63195cdee037f8e23830b4004027\",\"title\":\"TextCaps: a Dataset for Image Captioning with Reading Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7442eaaf453e63195cdee037f8e23830b4004027\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3238568\",\"name\":\"M. Liu\"},{\"authorId\":\"1485768948\",\"name\":\"Lingjun Li\"},{\"authorId\":\"146896370\",\"name\":\"H. Hu\"},{\"authorId\":\"15880069\",\"name\":\"W. Guan\"},{\"authorId\":\"153307124\",\"name\":\"J. Tian\"}],\"doi\":\"10.1016/j.ipm.2019.102178\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4ba3a265763677dfb68567aa6b62cd5fe4a633b\",\"title\":\"Image caption generation with dual attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/e4ba3a265763677dfb68567aa6b62cd5fe4a633b\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1978802390\",\"name\":\"Haolei Pei\"},{\"authorId\":\"8559954\",\"name\":\"Q. Chen\"},{\"authorId\":\"13257164\",\"name\":\"J. Wang\"},{\"authorId\":\"123555217\",\"name\":\"Q. Sun\"},{\"authorId\":\"1680030\",\"name\":\"Yubo Jia\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206815\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"title\":\"Visual Relational Reasoning for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"40186452\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/978-3-030-30645-8_66\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6f378be13df9e97e15ca240b60a0a0aa16d5eb64\",\"title\":\"Artpedia: A New Visual-Semantic Dataset with Visual and Contextual Sentences in the Artistic Domain\",\"url\":\"https://www.semanticscholar.org/paper/6f378be13df9e97e15ca240b60a0a0aa16d5eb64\",\"venue\":\"ICIAP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144060080\",\"name\":\"Giang T. Nguyen\"},{\"authorId\":\"8102722\",\"name\":\"Tae Joon Jun\"},{\"authorId\":\"145178790\",\"name\":\"Trung Tran\"},{\"authorId\":\"1729331675\",\"name\":\"Tolcha Yalew\"},{\"authorId\":\"40513516\",\"name\":\"Daeyoung Kim\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b76f2cba712972f068bfb0fffa9b467761fd273\",\"title\":\"ContCap: A scalable framework for continual image captioning.\",\"url\":\"https://www.semanticscholar.org/paper/7b76f2cba712972f068bfb0fffa9b467761fd273\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.06258\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/cvpr42600.2020.01001\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4fc2eb1b494a29c7cb39da3b180a302521e459a0\",\"title\":\"Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA\",\"url\":\"https://www.semanticscholar.org/paper/4fc2eb1b494a29c7cb39da3b180a302521e459a0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2007.11731\",\"authors\":[{\"authorId\":\"1828787912\",\"name\":\"Yiwu Zhong\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"101777533\",\"name\":\"J. Chen\"},{\"authorId\":null,\"name\":\"Dong Yu\"},{\"authorId\":\"47002659\",\"name\":\"Yanchao Li\"}],\"doi\":\"10.1007/978-3-030-58568-6_13\",\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"66ed8795eb6de5d2a6b204baac9378d6d28136cc\",\"title\":\"Comprehensive Image Captioning via Scene Graph Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/66ed8795eb6de5d2a6b204baac9378d6d28136cc\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1911.03705\",\"authors\":[{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"150341221\",\"name\":\"Wangchunshu Zhou\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.165\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"title\":\"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113001756\",\"name\":\"A. Deshpande\"},{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"24952249\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b2c60061ad32e28eb1e20aff42e062c9160786be\",\"title\":\"Diverse and Controllable Image Captioning with Part-of-Speech Guidance\",\"url\":\"https://www.semanticscholar.org/paper/b2c60061ad32e28eb1e20aff42e062c9160786be\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1909.08745\",\"authors\":[{\"authorId\":\"144060080\",\"name\":\"Giang T. Nguyen\"},{\"authorId\":\"8102722\",\"name\":\"T. Jun\"},{\"authorId\":\"144256269\",\"name\":\"Trung Tran\"},{\"authorId\":\"72657968\",\"name\":\"Daeyoung Kim\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b428979e3b67f40fc307b5b166f9c97adba6a63f\",\"title\":\"ContCap: A comprehensive framework for continual image captioning\",\"url\":\"https://www.semanticscholar.org/paper/b428979e3b67f40fc307b5b166f9c97adba6a63f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2012.07333\",\"authors\":[{\"authorId\":\"1733071048\",\"name\":\"Chao Zeng\"},{\"authorId\":\"1687386\",\"name\":\"S. Kwong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d7212df671c50beb567e3d3d608b0c14405c40e3\",\"title\":\"Intrinsic Image Captioning Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/d7212df671c50beb567e3d3d608b0c14405c40e3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.13693\",\"authors\":[{\"authorId\":\"2042334827\",\"name\":\"Sagar Gubbi Venkatesh\"},{\"authorId\":\"152417262\",\"name\":\"A. Biswas\"},{\"authorId\":\"2440327\",\"name\":\"Raviteja Upadrashta\"},{\"authorId\":\"123282453\",\"name\":\"V. Srinivasan\"},{\"authorId\":\"2408872\",\"name\":\"P. Talukdar\"},{\"authorId\":\"1756622\",\"name\":\"B. Amrutur\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1bc4e722f1b67d02a1a54ba222258f36062e050\",\"title\":\"Spatial Reasoning from Natural Language Instructions for Robot Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/f1bc4e722f1b67d02a1a54ba222258f36062e050\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.03923\",\"authors\":[{\"authorId\":\"1410097225\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e420efcd3fbace6f219c812a78d33cb64e25445\",\"title\":\"Deconfounded Image Captioning: A Causal Retrospect\",\"url\":\"https://www.semanticscholar.org/paper/1e420efcd3fbace6f219c812a78d33cb64e25445\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50844299\",\"name\":\"Marcus Klasson\"},{\"authorId\":\"2032187244\",\"name\":\"Cheng Zhang\"},{\"authorId\":\"1704879\",\"name\":\"H. Kjellstr\\u00f6m\"}],\"doi\":\"10.1016/j.patter.2020.100143\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"87130e6dd5d837880faa390c7c4006186c414676\",\"title\":\"Using Variational Multi-view Learning for Classification of Grocery Items\",\"url\":\"https://www.semanticscholar.org/paper/87130e6dd5d837880faa390c7c4006186c414676\",\"venue\":\"Patterns\",\"year\":2020},{\"arxivId\":\"1801.09041\",\"authors\":[{\"authorId\":\"48933900\",\"name\":\"Q. Li\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"2846025\",\"name\":\"D. Yu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.18653/v1/D18-1164\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd2f8bb5fa881797fad0448547e307a18bf897da\",\"title\":\"Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions\",\"url\":\"https://www.semanticscholar.org/paper/dd2f8bb5fa881797fad0448547e307a18bf897da\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":null,\"name\":\"Feng Wu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf2f9f9ebe16b2574ad6de69f5806e5d44c14217\",\"title\":\"Explainability by Parsing: Neural Module Tree Networks for Natural Language Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/bf2f9f9ebe16b2574ad6de69f5806e5d44c14217\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"L. Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47859297\",\"name\":\"D. Wang\"},{\"authorId\":\"143984297\",\"name\":\"Daniel Beck\"},{\"authorId\":\"143620680\",\"name\":\"Trevor Cohn\"}],\"doi\":\"10.18653/v1/D19-6405\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"93447c939657029b6305053599f51e78ba8a4c3d\",\"title\":\"On the Role of Scene Graphs in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93447c939657029b6305053599f51e78ba8a4c3d\",\"venue\":\"LANTERN@EMNLP-IJCNLP\",\"year\":2019},{\"arxivId\":\"2002.11701\",\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":\"10.1145/3366423.3380137\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a842fe8c25348627764462a57f0cd43d8cef103b\",\"title\":\"CLARA: Clinical Report Auto-completion\",\"url\":\"https://www.semanticscholar.org/paper/a842fe8c25348627764462a57f0cd43d8cef103b\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103382547\",\"name\":\"Junzhong Ji\"},{\"authorId\":\"49770078\",\"name\":\"C. Xu\"},{\"authorId\":\"46447188\",\"name\":\"X. Zhang\"},{\"authorId\":\"2692910\",\"name\":\"Boyue Wang\"},{\"authorId\":\"1688375\",\"name\":\"Xinhang Song\"}],\"doi\":\"10.1109/TIP.2020.3004729\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"63632ca4c1a6c16c9b6358624134880c3b23df90\",\"title\":\"Spatio-Temporal Memory Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/63632ca4c1a6c16c9b6358624134880c3b23df90\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2795882\",\"name\":\"Samuel Humeau\"},{\"authorId\":null,\"name\":\"Hexiang Hu\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c8ad080ccb3f5e3c999c2948029f0bd005d5635\",\"title\":\"ENGAGING IMAGE CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/5c8ad080ccb3f5e3c999c2948029f0bd005d5635\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1810.00912\",\"authors\":[{\"authorId\":\"120157233\",\"name\":\"J. Yang\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03625172605e0721db44ef107bc75e6ececdee25\",\"title\":\"Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/03625172605e0721db44ef107bc75e6ececdee25\",\"venue\":\"CoRL\",\"year\":2018},{\"arxivId\":\"1804.03803\",\"authors\":[{\"authorId\":\"1887625\",\"name\":\"Yuehua Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"39978626\",\"name\":\"Lu Jiang\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1145/3240508.3240640\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d64f52b94977b71976327eeb3db702b246ee39ce\",\"title\":\"Decoupled Novel Object Captioner\",\"url\":\"https://www.semanticscholar.org/paper/d64f52b94977b71976327eeb3db702b246ee39ce\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1911.10132\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s11042-020-09865-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f\",\"title\":\"CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation - A Generalization of Bi Directional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1802.00176\",\"authors\":[{\"authorId\":\"2033687\",\"name\":\"Xuemei Xie\"},{\"authorId\":\"144310853\",\"name\":\"Jiang Du\"},{\"authorId\":\"35367337\",\"name\":\"Guangming Shi\"},{\"authorId\":\"145872985\",\"name\":\"Chenye Wang\"},{\"authorId\":\"153030385\",\"name\":\"X. Xu\"}],\"doi\":\"10.1007/978-3-030-03338-5_23\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a45885b29e4d2efaf75759131fb8bb524cffbd94\",\"title\":\"Perceptual Compressive Sensing\",\"url\":\"https://www.semanticscholar.org/paper/a45885b29e4d2efaf75759131fb8bb524cffbd94\",\"venue\":\"PRCV\",\"year\":2018},{\"arxivId\":\"1905.02925\",\"authors\":[{\"authorId\":\"22199114\",\"name\":\"Panos Achlioptas\"},{\"authorId\":\"5586582\",\"name\":\"Judy Fan\"},{\"authorId\":\"8932668\",\"name\":\"Robert X. D. Hawkins\"},{\"authorId\":\"144002017\",\"name\":\"Noah D. Goodman\"},{\"authorId\":\"1744254\",\"name\":\"L. Guibas\"}],\"doi\":\"10.1109/ICCV.2019.00903\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9018a815a8a43ef8e27a9af55abdf6a270d7bc28\",\"title\":\"Shapeglot: Learning Language for Shape Differentiation\",\"url\":\"https://www.semanticscholar.org/paper/9018a815a8a43ef8e27a9af55abdf6a270d7bc28\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938051940\",\"name\":\"Dylan Flaute\"},{\"authorId\":\"2405109\",\"name\":\"B. Narayanan\"}],\"doi\":\"10.1117/12.2568016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"title\":\"Video captioning using weakly supervised convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48195668\",\"name\":\"Jin Yuan\"},{\"authorId\":\"47059067\",\"name\":\"L. Zhang\"},{\"authorId\":\"2836997\",\"name\":\"Songrui Guo\"},{\"authorId\":\"1406190317\",\"name\":\"Y. Xiao\"},{\"authorId\":\"152985786\",\"name\":\"Z. Li\"}],\"doi\":\"10.1145/3394955\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"title\":\"Image Captioning with a Joint Attention Mechanism by Visual Concept Samples\",\"url\":\"https://www.semanticscholar.org/paper/2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314360\",\"name\":\"Weixuan Wang\"},{\"authorId\":\"49865085\",\"name\":\"Zhihong Chen\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"}],\"doi\":\"10.1007/978-3-030-20876-9_37\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf3f2f2c1b88eb7b031d593c9d83641baea364e2\",\"title\":\"Multivariate Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf3f2f2c1b88eb7b031d593c9d83641baea364e2\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4237bdd7df710c08b58d02bb48f8dcecd2521a80\",\"title\":\"Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/4237bdd7df710c08b58d02bb48f8dcecd2521a80\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2002.05556\",\"authors\":[{\"authorId\":\"144869806\",\"name\":\"Pedro Henrique Martins\"},{\"authorId\":\"2114966\",\"name\":\"Vlad Niculae\"},{\"authorId\":\"2566656\",\"name\":\"Zita Marinho\"},{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e627eb9f17369770f2cef5d71360c93c8494785\",\"title\":\"Sparse and Structured Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4e627eb9f17369770f2cef5d71360c93c8494785\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152747612\",\"name\":\"Zhenyu Yang\"},{\"authorId\":\"153874396\",\"name\":\"Qiao Liu\"}],\"doi\":\"10.1109/ACCESS.2020.2980578\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"059093bc3bb44949e9754fb4366f0be7cea34bac\",\"title\":\"ATT-BM-SOM: A Framework of Effectively Choosing Image Information and Optimizing Syntax for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/059093bc3bb44949e9754fb4366f0be7cea34bac\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2706292\",\"name\":\"Xianhua Zeng\"},{\"authorId\":\"145117241\",\"name\":\"L. Wen\"},{\"authorId\":\"115986457\",\"name\":\"Yang Xu\"},{\"authorId\":\"1893927760\",\"name\":\"Conghui Ji\"}],\"doi\":\"10.1016/j.cmpb.2020.105700\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cbe08984fe9bb669e11e73b8fb14c9d43ab57017\",\"title\":\"Generating diagnostic report for medical image by high-middle-level visual information incorporation on double deep learning models\",\"url\":\"https://www.semanticscholar.org/paper/cbe08984fe9bb669e11e73b8fb14c9d43ab57017\",\"venue\":\"Comput. Methods Programs Biomed.\",\"year\":2020},{\"arxivId\":\"1905.11624\",\"authors\":[{\"authorId\":\"121038417\",\"name\":\"Zih-Siou Hung\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c47e1c608614ff624b6c20b51deee780482ee480\",\"title\":\"Union Visual Translation Embedding for Visual Relationship Detection and Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/c47e1c608614ff624b6c20b51deee780482ee480\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2012.14700\",\"authors\":[{\"authorId\":\"7748443\",\"name\":\"Sangwoong Yoon\"},{\"authorId\":\"21152168\",\"name\":\"Woo Young Kang\"},{\"authorId\":\"97519074\",\"name\":\"Sungwook Jeon\"},{\"authorId\":\"50112156\",\"name\":\"Seong-Eun Lee\"},{\"authorId\":\"118727697\",\"name\":\"Changjin Han\"},{\"authorId\":\"30664924\",\"name\":\"Jonghun Park\"},{\"authorId\":\"1845794808\",\"name\":\"Eun-Sol Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e0570df4e56d51be58b53166e853d848ef767af\",\"title\":\"Image-to-Image Retrieval by Learning Similarity between Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/2e0570df4e56d51be58b53166e853d848ef767af\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.08760\",\"authors\":[{\"authorId\":\"47824882\",\"name\":\"W. Wang\"},{\"authorId\":\"3373117\",\"name\":\"R. Wang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1007/978-3-030-58601-0_14\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c450ead5da6783d93e20e718f27f4475b16c7dc9\",\"title\":\"Sketching Image Gist: Human-Mimetic Hierarchical Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/c450ead5da6783d93e20e718f27f4475b16c7dc9\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1909.02097\",\"authors\":[{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"144865339\",\"name\":\"Bo Pang\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/D19-1155\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b6e6822eabe2f64192a1965c23e38043866319c\",\"title\":\"Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3b6e6822eabe2f64192a1965c23e38043866319c\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2009.12677\",\"authors\":[{\"authorId\":\"49291879\",\"name\":\"Y. Liu\"},{\"authorId\":\"1453924141\",\"name\":\"Yao Wan\"},{\"authorId\":\"40901818\",\"name\":\"Lifang He\"},{\"authorId\":\"1490937486\",\"name\":\"Hao Peng\"},{\"authorId\":\"144019071\",\"name\":\"Philip S. Yu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"baa8f524c82735f174b8d1ab512ac5750146d67e\",\"title\":\"KG-BART: Knowledge Graph-Augmented BART for Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/baa8f524c82735f174b8d1ab512ac5750146d67e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":\"10.1145/3372278.3390740\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60fb047e99e4ccdcd501767364db7bd04ac2743f\",\"title\":\"One Perceptron to Rule Them All: Language, Vision, Audio and Speech\",\"url\":\"https://www.semanticscholar.org/paper/60fb047e99e4ccdcd501767364db7bd04ac2743f\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"},{\"authorId\":\"88265392\",\"name\":\"P. Liu\"},{\"authorId\":\"3343198\",\"name\":\"Yingjie Zhou\"},{\"authorId\":\"144953181\",\"name\":\"D. Wu\"}],\"doi\":\"10.1109/BIGCOM.2019.00013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a203fb561497774689ff4bb80cb18c9e00bf47b6\",\"title\":\"Semantic Tensor Product for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a203fb561497774689ff4bb80cb18c9e00bf47b6\",\"venue\":\"2019 5th International Conference on Big Data Computing and Communications (BIGCOM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400258083\",\"name\":\"Chunpu Xu\"},{\"authorId\":\"47748857\",\"name\":\"Wei Zhao\"},{\"authorId\":\"31991405\",\"name\":\"Min Yang\"},{\"authorId\":\"2441161\",\"name\":\"Xiang Ao\"},{\"authorId\":\"1405918472\",\"name\":\"Wangrong Cheng\"},{\"authorId\":\"1936983\",\"name\":\"J. Tian\"}],\"doi\":\"10.1145/3357384.3358105\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e81c97c18cb4f4922e4442664350350536a71a13\",\"title\":\"A Unified Generation-Retrieval Framework for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e81c97c18cb4f4922e4442664350350536a71a13\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":\"1810.10126\",\"authors\":[{\"authorId\":\"1678662\",\"name\":\"Yang Li\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"3422911\",\"name\":\"S. Si\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"31b3b0a526683048f69e703d5f098aea0e8a0ce0\",\"title\":\"Area Attention\",\"url\":\"https://www.semanticscholar.org/paper/31b3b0a526683048f69e703d5f098aea0e8a0ce0\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"2005.00785\",\"authors\":[{\"authorId\":\"51134926\",\"name\":\"Xisen Jin\"},{\"authorId\":\"3109653\",\"name\":\"Junyi Du\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ca48348e0d00b5c3766b9a1ed8864ffce9285d96\",\"title\":\"Visually Grounded Continual Learning of Compositional Semantics\",\"url\":\"https://www.semanticscholar.org/paper/ca48348e0d00b5c3766b9a1ed8864ffce9285d96\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.13862\",\"authors\":[{\"authorId\":\"50232214\",\"name\":\"Wenjia Xu\"},{\"authorId\":\"51175635\",\"name\":\"Jiuniu Wang\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"3287827\",\"name\":\"Guangluan Xu\"},{\"authorId\":\"7413451\",\"name\":\"Daoyu Lin\"},{\"authorId\":\"1379498558\",\"name\":\"Wei Dai\"},{\"authorId\":\"48607717\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/JSTSP.2020.2987729\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c056c388f4deb0ff87932eb6e7789b23eb22d22\",\"title\":\"Where is the Model Looking At? \\u2013 Concentrate and Explain the Network Attention\",\"url\":\"https://www.semanticscholar.org/paper/7c056c388f4deb0ff87932eb6e7789b23eb22d22\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8ad97e877f55f7ce631fbce30d194c1912c22362\",\"title\":\"IARY PROGRESS ESTIMATION\",\"url\":\"https://www.semanticscholar.org/paper/8ad97e877f55f7ce631fbce30d194c1912c22362\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"18d48837a300b68822b4d2366600e9628c53718a\",\"title\":\"nocaps: novel object captioning at scale Supplementary Material\",\"url\":\"https://www.semanticscholar.org/paper/18d48837a300b68822b4d2366600e9628c53718a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.02339\",\"authors\":[{\"authorId\":\"2031911881\",\"name\":\"Edwin G. Ng\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0795ae19cae966a9f71d8da78ef09447ab9080a6\",\"title\":\"Understanding Guided Image Captioning Performance across Domains\",\"url\":\"https://www.semanticscholar.org/paper/0795ae19cae966a9f71d8da78ef09447ab9080a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.02206\",\"authors\":[{\"authorId\":\"73286206\",\"name\":\"Dave Zhenyu Chen\"},{\"authorId\":\"47621053\",\"name\":\"A. Gholami\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"},{\"authorId\":\"3317599\",\"name\":\"A. X. Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a4ba78d377eea9650e5e399a0878e30bd22f648\",\"title\":\"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans\",\"url\":\"https://www.semanticscholar.org/paper/7a4ba78d377eea9650e5e399a0878e30bd22f648\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.05710\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"title\":\"Look Before you Speak: Visually Contextualized Utterances\",\"url\":\"https://www.semanticscholar.org/paper/b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.18653/v1/d19-64\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55c9ec9ee9bcedafc502684623dfa799c6ce35e7\",\"title\":\"Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)\",\"url\":\"https://www.semanticscholar.org/paper/55c9ec9ee9bcedafc502684623dfa799c6ce35e7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405686623\",\"name\":\"Nils Murrugarra-Llerena\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1109/CVPR.2019.00659\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"title\":\"Cross-Modality Personalization for Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1908.00047\",\"authors\":[{\"authorId\":\"9424554\",\"name\":\"Berkan Demirel\"},{\"authorId\":\"1939006\",\"name\":\"Ramazan Gokberk Cinbis\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3d617b8cbf10b3fe776c16fa498e6420def3e000\",\"title\":\"Image Captioning with Unseen Objects\",\"url\":\"https://www.semanticscholar.org/paper/3d617b8cbf10b3fe776c16fa498e6420def3e000\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"2001.09545\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66a2eb540af6f47db177599bc793ab0c6a6aa47e\",\"title\":\"aiTPR: Attribute Interaction-Tensor Product Representation for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/66a2eb540af6f47db177599bc793ab0c6a6aa47e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"3374688\",\"name\":\"M. Tian\"},{\"authorId\":\"151118825\",\"name\":\"Mengmeng Jiang\"},{\"authorId\":null,\"name\":\"Heng Liu\"},{\"authorId\":\"30631999\",\"name\":\"Donghu Deng\"}],\"doi\":\"10.1016/J.PATREC.2019.07.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"749f376d0addb83569fcc7536e46308abbb232d4\",\"title\":\"How much do cross-modal related semantics benefit image captioning by weighting attributes and re-ranking sentences?\",\"url\":\"https://www.semanticscholar.org/paper/749f376d0addb83569fcc7536e46308abbb232d4\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":\"2003.10925\",\"authors\":[{\"authorId\":\"47577022\",\"name\":\"Nannan Li\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"16025ed0f452e62514d4705b7da1e9f78067d9e7\",\"title\":\"Learning Compact Reward for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16025ed0f452e62514d4705b7da1e9f78067d9e7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31465302\",\"name\":\"E. Wang\"},{\"authorId\":\"46182609\",\"name\":\"X. Zhang\"},{\"authorId\":\"39907479\",\"name\":\"F. Wang\"},{\"authorId\":\"1682589\",\"name\":\"T. Wu\"},{\"authorId\":\"144404748\",\"name\":\"Chien-Ming Chen\"}],\"doi\":\"10.1109/ACCESS.2019.2917771\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"751e871c12f70bf05ea012b21f1434849d6fe5ce\",\"title\":\"Multilayer Dense Attention Model for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/751e871c12f70bf05ea012b21f1434849d6fe5ce\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"49470127\",\"name\":\"X. Zhang\"},{\"authorId\":\"48593034\",\"name\":\"Guohua Dong\"},{\"authorId\":\"1406222945\",\"name\":\"Zhigang Luo\"}],\"doi\":\"10.1007/s11042-019-08209-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"311ee1284e6bdf035f8c3a1a20d00c5841b2e341\",\"title\":\"Object-aware semantics of attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/311ee1284e6bdf035f8c3a1a20d00c5841b2e341\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35399640\",\"name\":\"D\\u00eddac Sur\\u00eds\"},{\"authorId\":\"32486555\",\"name\":\"D. Epstein\"},{\"authorId\":\"153172090\",\"name\":\"Huai-zhong Ji\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"}],\"doi\":\"10.1007/978-3-030-58526-6_26\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"007ca8ca7a68451c32da034c72a06238434843c1\",\"title\":\"Learning to Learn Words from Visual Scenes\",\"url\":\"https://www.semanticscholar.org/paper/007ca8ca7a68451c32da034c72a06238434843c1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1808.08732\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"1781885\",\"name\":\"Houfeng Wang\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.18653/v1/D18-1013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8968072ad12bcb96c513ae1c01abf6abdae810df\",\"title\":\"simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/8968072ad12bcb96c513ae1c01abf6abdae810df\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"2011.09928\",\"authors\":[{\"authorId\":\"79439415\",\"name\":\"Haoyu Dong\"},{\"authorId\":\"152764048\",\"name\":\"Zidong Wang\"},{\"authorId\":\"83277545\",\"name\":\"Qiang Qiu\"},{\"authorId\":\"1699339\",\"name\":\"G. Sapiro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88b5d42753d2f3a2194c886169038fd9ae7ade22\",\"title\":\"Using Text to Teach Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/88b5d42753d2f3a2194c886169038fd9ae7ade22\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"46700004\",\"name\":\"J. Liu\"},{\"authorId\":\"116829059\",\"name\":\"Shi-chen Lu\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/TMM.2019.2951226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"837a513a43c7bcce903edbacbfc507cba6451e21\",\"title\":\"Show, Tell, and Polish: Ruminant Decoding for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/837a513a43c7bcce903edbacbfc507cba6451e21\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1901.02527\",\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17420047ab3c64ebbb63dcf21931d26fa77955d6\",\"title\":\"Viewpoint Invariant Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/17420047ab3c64ebbb63dcf21931d26fa77955d6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"92326466\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/cvpr42600.2020.01059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"title\":\"Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"145558284\",\"name\":\"Kai Lei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4eb0b7ced6c022eef4c6fb2ed3dcdbdccfc056dc\",\"title\":\"Aligning Visual Regions and Textual Concepts: Learning Fine-Grained Image Representations for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4eb0b7ced6c022eef4c6fb2ed3dcdbdccfc056dc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1903.11207\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"49624560\",\"name\":\"M. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2019.00211\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6258cb5ae8d15870e65c363efea319e012fb249\",\"title\":\"Information Maximizing Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/e6258cb5ae8d15870e65c363efea319e012fb249\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.08608\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/ICCV.2019.00435\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"45f9856d418527d23dc7c89197627fa1f3b215f9\",\"title\":\"Learning to Collocate Neural Modules for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/45f9856d418527d23dc7c89197627fa1f3b215f9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2006.08315\",\"authors\":[{\"authorId\":\"151195783\",\"name\":\"Ruixiang Tang\"},{\"authorId\":\"3432460\",\"name\":\"Mengnan Du\"},{\"authorId\":\"48513905\",\"name\":\"Yuening Li\"},{\"authorId\":\"47781070\",\"name\":\"Zirui Liu\"},{\"authorId\":\"1490483806\",\"name\":\"X. Hu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0b239fe63de5de0a1e0c08af97a30f79b5a2bdbc\",\"title\":\"Mitigating Gender Bias in Captioning Systems\",\"url\":\"https://www.semanticscholar.org/paper/0b239fe63de5de0a1e0c08af97a30f79b5a2bdbc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Jiang\"},{\"authorId\":null,\"name\":\"Qi Zhao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3977c0056755c2911811509ac38e0cef532004b0\",\"title\":\"Self-Distillation for Few-Shot Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3977c0056755c2911811509ac38e0cef532004b0\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1809.02156\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"40895688\",\"name\":\"Kaylee Burns\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D18-1437\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4921243268c81d0d6db99053a9d004852225a622\",\"title\":\"Object Hallucination in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4921243268c81d0d6db99053a9d004852225a622\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1903.02507\",\"authors\":[{\"authorId\":\"3259825\",\"name\":\"Jiayun Li\"},{\"authorId\":\"39367903\",\"name\":\"Mohammad K. Ebrahimpour\"},{\"authorId\":\"33129821\",\"name\":\"Azadeh Moghtaderi\"},{\"authorId\":\"1915432\",\"name\":\"Yen-Yun Yu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2ea752a837e24686a5774e6cbe6f787a76cd014d\",\"title\":\"Image captioning with weakly-supervised attention penalty\",\"url\":\"https://www.semanticscholar.org/paper/2ea752a837e24686a5774e6cbe6f787a76cd014d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47824553\",\"name\":\"W. Wang\"},{\"authorId\":\"3373117\",\"name\":\"R. Wang\"},{\"authorId\":\"145455919\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1109/CVPR.2019.00838\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"05bb6925dc3e9ab789bc672fee3e7003b8405b0a\",\"title\":\"Exploring Context and Visual Pattern of Relationship for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/05bb6925dc3e9ab789bc672fee3e7003b8405b0a\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2004.02435\",\"authors\":[{\"authorId\":\"2416001\",\"name\":\"Shashank Bujimalla\"},{\"authorId\":\"2536658\",\"name\":\"Mahesh Subedar\"},{\"authorId\":\"1798616\",\"name\":\"O. Tickoo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"81e7e74d0f5f200e575df1908a1b0a5f0500906b\",\"title\":\"B-SCST: Bayesian Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/81e7e74d0f5f200e575df1908a1b0a5f0500906b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1476813856\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1016/j.patrec.2019.11.018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2cc1a8014ab6de2e5ef45f7f2204ba7e73cb10ec\",\"title\":\"Explaining digital humanities by aligning images and textual descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2cc1a8014ab6de2e5ef45f7f2204ba7e73cb10ec\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":\"2004.14231\",\"authors\":[{\"authorId\":\"47287647\",\"name\":\"Sen He\"},{\"authorId\":\"3157379\",\"name\":\"Wentong Liao\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"1779035\",\"name\":\"B. Rosenhahn\"},{\"authorId\":\"3251678\",\"name\":\"N. Pugeault\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"657cce51f80e272373ab4fc0dabf5dc8b30c0070\",\"title\":\"Image Captioning through Image Transformer\",\"url\":\"https://www.semanticscholar.org/paper/657cce51f80e272373ab4fc0dabf5dc8b30c0070\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49418270\",\"name\":\"Yong Wang\"},{\"authorId\":\"1485232293\",\"name\":\"Wenkai Zhang\"},{\"authorId\":\"47362549\",\"name\":\"Qing Liu\"},{\"authorId\":\"9708577\",\"name\":\"Zhengyuan Zhang\"},{\"authorId\":\"11732382\",\"name\":\"X. Gao\"},{\"authorId\":\"9758599\",\"name\":\"Xi-an Sun\"}],\"doi\":\"10.1145/3394171.3413877\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a05a96d5d9bac88d7c8b34be2e57f58f17a6e53d\",\"title\":\"Improving Intra- and Inter-Modality Visual Relation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a05a96d5d9bac88d7c8b34be2e57f58f17a6e53d\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2003.00387\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"1490938689\",\"name\":\"Peng Wang\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/cvpr42600.2020.00998\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4916e339caef2d2a98e633e1f0b2144e2b0c9e2\",\"title\":\"Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/b4916e339caef2d2a98e633e1f0b2144e2b0c9e2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2012.05545\",\"authors\":[{\"authorId\":\"67333377\",\"name\":\"Zeliang Song\"},{\"authorId\":\"1727617\",\"name\":\"Xiaofei Zhou\"},{\"authorId\":\"1855978\",\"name\":\"Zhendong Mao\"},{\"authorId\":\"2573626\",\"name\":\"Jianlong Tan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f237f4ce10001a9b6cb47fae5333fa5ddb7e2d3\",\"title\":\"Image Captioning with Context-Aware Auxiliary Guidance\",\"url\":\"https://www.semanticscholar.org/paper/6f237f4ce10001a9b6cb47fae5333fa5ddb7e2d3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.03305\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"22d733f5d5a995469dc916102f1806253645ae60\",\"title\":\"Captioning Images with Novel Objects via Online Vocabulary Expansion\",\"url\":\"https://www.semanticscholar.org/paper/22d733f5d5a995469dc916102f1806253645ae60\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390626344\",\"name\":\"Zhiyuan Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"title\":\"Cross-Modal Representation\",\"url\":\"https://www.semanticscholar.org/paper/7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.03743\",\"authors\":[{\"authorId\":\"13941532\",\"name\":\"Fuxiao Liu\"},{\"authorId\":null,\"name\":\"Yinghan Wang\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7e5bcdd0366c3ddd1579159595d1990f515cf93b\",\"title\":\"VisualNews : A Large Multi-source News Image Dataset\",\"url\":\"https://www.semanticscholar.org/paper/7e5bcdd0366c3ddd1579159595d1990f515cf93b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1908.09317\",\"authors\":[{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"145587209\",\"name\":\"N. Navab\"}],\"doi\":\"10.1109/ICCV.2019.00751\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"title\":\"Towards Unsupervised Image Captioning With Shared Multimodal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32236286\",\"name\":\"Annika Lindh\"},{\"authorId\":\"144394786\",\"name\":\"R. Ross\"},{\"authorId\":\"34967075\",\"name\":\"John D. Kelleher\"}],\"doi\":\"10.21427/D7ZN6Q\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"418896d28c72b67311d522b39eb5871e7334a971\",\"title\":\"Entity-Grounded Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/418896d28c72b67311d522b39eb5871e7334a971\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1907.03950\",\"authors\":[{\"authorId\":\"152951058\",\"name\":\"Drew A. Hudson\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec\",\"title\":\"Learning by Abstraction: The Neural State Machine\",\"url\":\"https://www.semanticscholar.org/paper/136c05cb8dd359fb8e0dc7947172a9ecb74ccbec\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s13735-020-00198-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c85757cdd7de5f99ab1717a6355a09bb717013c\",\"title\":\"MRECN: mixed representation enhanced (de)compositional network for caption generation from visual features, modeling as pseudo tensor product representation\",\"url\":\"https://www.semanticscholar.org/paper/8c85757cdd7de5f99ab1717a6355a09bb717013c\",\"venue\":\"Int. J. Multim. Inf. Retr.\",\"year\":2020},{\"arxivId\":\"2010.13588\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e4ca3d95ffb83870661dd66deee143e782f0706\",\"title\":\"Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale\",\"url\":\"https://www.semanticscholar.org/paper/2e4ca3d95ffb83870661dd66deee143e782f0706\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1910.03291\",\"authors\":[{\"authorId\":\"1387993874\",\"name\":\"Alireza Mohammadshahi\"},{\"authorId\":\"2875254\",\"name\":\"R\\u00e9mi Lebret\"},{\"authorId\":\"1751802\",\"name\":\"K. Aberer\"}],\"doi\":\"10.18653/v1/D19-6605\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d880af464e477421e5cddc794e971c6db193b8c\",\"title\":\"Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task\",\"url\":\"https://www.semanticscholar.org/paper/2d880af464e477421e5cddc794e971c6db193b8c\",\"venue\":\"IJCNLP 2019\",\"year\":2019},{\"arxivId\":\"1808.00191\",\"authors\":[{\"authorId\":\"120157233\",\"name\":\"J. Yang\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1007/978-3-030-01246-5_41\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1fad7fe0a7a90a8470a0688ad26bab6ceb8a85b7\",\"title\":\"Graph R-CNN for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/1fad7fe0a7a90a8470a0688ad26bab6ceb8a85b7\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153297544\",\"name\":\"X. Yang\"},{\"authorId\":\"118565563\",\"name\":\"Chong-Yang Gao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1145/3394171.3413859\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96389251e4f0f8494dfc8dc67c05992eed8e192b\",\"title\":\"Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96389251e4f0f8494dfc8dc67c05992eed8e192b\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145840350\",\"name\":\"P. Anderson\"}],\"doi\":\"10.25911/5D00D4EC451CC\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0\",\"title\":\"Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents\",\"url\":\"https://www.semanticscholar.org/paper/c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48520780\",\"name\":\"X. Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"38bbace118817cd18677f169a8c8e6c8b005df18\",\"title\":\"Auto-Encoding Graphical Inductive Bias for Descriptive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/38bbace118817cd18677f169a8c8e6c8b005df18\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9073332\",\"name\":\"J. Li\"},{\"authorId\":\"1923156\",\"name\":\"P. Yao\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"49039585\",\"name\":\"Wei-Cun Zhang\"}],\"doi\":\"10.3390/APP9163260\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"47716ca06a3ad4226a6d8706b6f02793b9892fe4\",\"title\":\"Boosted Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/47716ca06a3ad4226a6d8706b6f02793b9892fe4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/ICCV.2019.00472\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5abe916562fad8306e3f4e571f83015047f0be1d\",\"title\":\"Robust Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5abe916562fad8306e3f4e571f83015047f0be1d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1810.10665\",\"authors\":[{\"authorId\":\"35752280\",\"name\":\"Kurt Shuster\"},{\"authorId\":\"2795882\",\"name\":\"Samuel Humeau\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":\"10.1109/CVPR.2019.01280\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c677000c9078fdff8622be15a37db7d4945f36c2\",\"title\":\"Engaging Image Captioning via Personality\",\"url\":\"https://www.semanticscholar.org/paper/c677000c9078fdff8622be15a37db7d4945f36c2\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1902.03751\",\"authors\":[{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1774780\",\"name\":\"Yilin Shen\"},{\"authorId\":\"1705713\",\"name\":\"Hongxia Jin\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/ICCV.2019.00268\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ab9ac4bea4ee49fd879337ac477dc09de4914fbf\",\"title\":\"Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded\",\"url\":\"https://www.semanticscholar.org/paper/ab9ac4bea4ee49fd879337ac477dc09de4914fbf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1812.03299\",\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1109/ICCV.2019.00477\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e504843bfd3ca81b859895439bbe19efc1f3dc61\",\"title\":\"Learning to Assemble Neural Module Tree Networks for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/e504843bfd3ca81b859895439bbe19efc1f3dc61\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2251072\",\"name\":\"N. Rodr\\u00edguez\"},{\"authorId\":\"1585231765\",\"name\":\"Galena Pisoni\"}],\"doi\":\"10.1145/3386392.3399276\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e217f502687c0af8ebe1fe70088e3e94a0db6830\",\"title\":\"Accessible Cultural Heritage through Explainable Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e217f502687c0af8ebe1fe70088e3e94a0db6830\",\"venue\":\"UMAP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144101175\",\"name\":\"Zhou Lei\"},{\"authorId\":\"1831825\",\"name\":\"Congcong Zhou\"},{\"authorId\":\"35155467\",\"name\":\"Shengbo Chen\"},{\"authorId\":\"1722738282\",\"name\":\"Yiyong Huang\"},{\"authorId\":\"1726027121\",\"name\":\"Xianrui Liu\"}],\"doi\":\"10.1109/ACCESS.2020.3024639\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e85fdae1b5e297f1d8925c9bd5cd7e243a305116\",\"title\":\"A Sparse Transformer-Based Approach for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e85fdae1b5e297f1d8925c9bd5cd7e243a305116\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2343931\",\"name\":\"Miao Ma\"},{\"authorId\":\"93119174\",\"name\":\"Ziang Gao\"}],\"doi\":\"10.1109/CCHI.2019.8901917\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ddcfc8f4aab75a96ba76f1275056f39f5595f2bc\",\"title\":\"Examinee Behavior Description Method Based on Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ddcfc8f4aab75a96ba76f1275056f39f5595f2bc\",\"venue\":\"2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144286903\",\"name\":\"Q. Yang\"},{\"authorId\":\"51145694\",\"name\":\"Gaosheng Wu\"},{\"authorId\":\"47003385\",\"name\":\"Yujun Li\"},{\"authorId\":\"1773599\",\"name\":\"R. Li\"},{\"authorId\":\"70387082\",\"name\":\"Xiwu Gu\"},{\"authorId\":\"1720853808\",\"name\":\"Huicai Deng\"},{\"authorId\":\"1720822983\",\"name\":\"Junzhuang Wu\"}],\"doi\":\"10.1109/TCSS.2020.2986778\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6f11d9836aa871891d40604e6a19fa8ed3dd2dc2\",\"title\":\"AMNN: Attention-Based Multimodal Neural Network Model for Hashtag Recommendation\",\"url\":\"https://www.semanticscholar.org/paper/6f11d9836aa871891d40604e6a19fa8ed3dd2dc2\",\"venue\":\"IEEE Transactions on Computational Social Systems\",\"year\":2020},{\"arxivId\":\"2001.05876\",\"authors\":[{\"authorId\":\"46659203\",\"name\":\"L. Wang\"},{\"authorId\":\"1486057423\",\"name\":\"Zechen Bai\"},{\"authorId\":\"48378975\",\"name\":\"Yonghua Zhang\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"}],\"doi\":\"10.1609/aaai.v34i07.6898\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"85079d64d6fdd0ba5318fda119d152f2d2946391\",\"title\":\"Show, Recall, and Tell: Image Captioning with Recall Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/85079d64d6fdd0ba5318fda119d152f2d2946391\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1909.04402\",\"authors\":[{\"authorId\":\"1387994359\",\"name\":\"Mitja Nikolaus\"},{\"authorId\":\"30671790\",\"name\":\"M. Abdou\"},{\"authorId\":\"48024953\",\"name\":\"Matthew Lamm\"},{\"authorId\":\"19509693\",\"name\":\"Rahul Aralikatte\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"}],\"doi\":\"10.18653/v1/K19-1009\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"11557ee241f6b5dc0a8c62bb8014c0111bbe45ba\",\"title\":\"Compositional Generalization in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/11557ee241f6b5dc0a8c62bb8014c0111bbe45ba\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"1811.07662\",\"authors\":[{\"authorId\":\"47833739\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2112160\",\"name\":\"Y. Li\"},{\"authorId\":\"1678689\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00859\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"23e943809c131c50dc90c1d308373febc60b9029\",\"title\":\"Intention Oriented Image Captions With Guiding Objects\",\"url\":\"https://www.semanticscholar.org/paper/23e943809c131c50dc90c1d308373febc60b9029\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2012.11587\",\"authors\":[{\"authorId\":\"120157233\",\"name\":\"J. Yang\"},{\"authorId\":\"13589371\",\"name\":\"Jiayuan Mao\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2042941\",\"name\":\"D. Cox\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"617b8d0715469dfdb9e44e0f0031ac99a0f333d9\",\"title\":\"Object-Centric Diagnosis of Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/617b8d0715469dfdb9e44e0f0031ac99a0f333d9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.11696\",\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"8350409\",\"name\":\"I. Padhi\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"},{\"authorId\":\"153598395\",\"name\":\"J. Ross\"},{\"authorId\":\"1999174380\",\"name\":\"Yair Schiff\"},{\"authorId\":\"49832828\",\"name\":\"R. A. Young\"},{\"authorId\":\"2679155\",\"name\":\"Brian M. Belgodere\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"title\":\"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042337560\",\"name\":\"Saketh Vishnubhatla\"},{\"authorId\":\"2303852\",\"name\":\"N. Sinha\"}],\"doi\":\"10.1145/3430984.3431059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38b047cd1373ccd67aa52c588068fd652d52b6e1\",\"title\":\"Image Captioning with Pretrained Language Generators\",\"url\":\"https://www.semanticscholar.org/paper/38b047cd1373ccd67aa52c588068fd652d52b6e1\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"1909.11059\",\"authors\":[{\"authorId\":\"48206987\",\"name\":\"L. Zhou\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1609/AAAI.V34I07.7005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"title\":\"Unified Vision-Language Pre-Training for Image Captioning and VQA\",\"url\":\"https://www.semanticscholar.org/paper/088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2009.12524\",\"authors\":[{\"authorId\":\"1972263989\",\"name\":\"Zanyar Zohourianshahzadi\"},{\"authorId\":\"34694214\",\"name\":\"J. Kalita\"}],\"doi\":\"10.1109/HCCAI49649.2020.00009\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d3cf77a7eb87bf9c85c4631b196c4eb10ff5b51e\",\"title\":\"Neural Twins Talk\",\"url\":\"https://www.semanticscholar.org/paper/d3cf77a7eb87bf9c85c4631b196c4eb10ff5b51e\",\"venue\":\"2020 IEEE International Conference on Humanized Computing and Communication with Artificial Intelligence (HCCAI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492164677\",\"name\":\"Min Yang\"},{\"authorId\":\"2948588\",\"name\":\"Junhao Liu\"},{\"authorId\":\"143822675\",\"name\":\"Ying Shen\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"40265331\",\"name\":\"X. Chen\"},{\"authorId\":\"31060469\",\"name\":\"Qingyao Wu\"},{\"authorId\":\"48161719\",\"name\":\"C. Li\"}],\"doi\":\"10.1109/TIP.2020.3028651\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f6a09cd467a2752e60a2766160a00c658667043e\",\"title\":\"An Ensemble of Generation- and Retrieval-Based Image Captioning With Dual Generator Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/f6a09cd467a2752e60a2766160a00c658667043e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2009.13682\",\"authors\":[{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"51188307\",\"name\":\"Kevin Lin\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f147279c9d1edddda57f1f21f23b3b58998bad74\",\"title\":\"VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training\",\"url\":\"https://www.semanticscholar.org/paper/f147279c9d1edddda57f1f21f23b3b58998bad74\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"46447554\",\"name\":\"Xiaodong Zhang\"},{\"authorId\":\"48593034\",\"name\":\"Guohua Dong\"},{\"authorId\":\"145254061\",\"name\":\"Z. Luo\"}],\"doi\":\"10.1109/ACCESS.2019.2917979\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5a759bbf042fe8852f23567cf14daf3d9f180bc\",\"title\":\"Cascade Semantic Fusion for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a759bbf042fe8852f23567cf14daf3d9f180bc\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48289232\",\"name\":\"Mohit Bajaj\"},{\"authorId\":\"49680751\",\"name\":\"Lanjun Wang\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00438\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a7bb3c251498a7c700c5f0563a53aea54345653\",\"title\":\"G3raphGround: Graph-Based Language Grounding\",\"url\":\"https://www.semanticscholar.org/paper/0a7bb3c251498a7c700c5f0563a53aea54345653\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443736039\",\"name\":\"Yuki Mori\"},{\"authorId\":\"46362679\",\"name\":\"Hiroshi Fukui\"},{\"authorId\":\"134790239\",\"name\":\"Tsubasa Hirakawa\"},{\"authorId\":\"1443785402\",\"name\":\"Jo Nishiyama\"},{\"authorId\":\"1687819\",\"name\":\"T. Yamashita\"},{\"authorId\":\"1687968\",\"name\":\"H. Fujiyoshi\"}],\"doi\":\"10.1109/ITSC.2019.8917187\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8fe7a6857906af4adb5b1e92d90279257811965\",\"title\":\"Attention Neural Baby Talk: Captioning of Risk Factors while Driving\",\"url\":\"https://www.semanticscholar.org/paper/b8fe7a6857906af4adb5b1e92d90279257811965\",\"venue\":\"2019 IEEE Intelligent Transportation Systems Conference (ITSC)\",\"year\":2019},{\"arxivId\":\"1912.08226\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"261570379fd841b426a4c51e8004f2cf9f1df771\",\"title\":\"M2: Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/261570379fd841b426a4c51e8004f2cf9f1df771\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29068663\",\"name\":\"Ruize Wang\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"2193560\",\"name\":\"Piji Li\"},{\"authorId\":\"1471463062\",\"name\":\"Qi Zhang\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"}],\"doi\":\"10.1609/AAAI.V34I05.6455\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1f676946d1585f802d21272e2b3899a53e7b8187\",\"title\":\"Storytelling from an Image Stream Using Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/1f676946d1585f802d21272e2b3899a53e7b8187\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"108358199\",\"name\":\"Viktar Atliha\"},{\"authorId\":\"1990294\",\"name\":\"D. Sesok\"}],\"doi\":\"10.1109/eStream50540.2020.9108880\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"08aa7d4d4e7fe6637ab83b8f8ae4144da517cdd6\",\"title\":\"Comparison of VGG and ResNet used as Encoders for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/08aa7d4d4e7fe6637ab83b8f8ae4144da517cdd6\",\"venue\":\"2020 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)\",\"year\":2020},{\"arxivId\":\"1906.00283\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"122175026\",\"name\":\"P\\u00e9ter Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"title\":\"Learning to Generate Grounded Image Captions without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410010039\",\"name\":\"Hao Zhao\"},{\"authorId\":\"123554573\",\"name\":\"M. Lu\"},{\"authorId\":\"1405606408\",\"name\":\"Anbang Yao\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"48571207\",\"name\":\"L. Zhang\"}],\"doi\":\"10.1007/s11263-019-01263-4\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"5b153c7410571339ff11b3b8d1fdbc54cc832d86\",\"title\":\"Learning to Draw Sight Lines\",\"url\":\"https://www.semanticscholar.org/paper/5b153c7410571339ff11b3b8d1fdbc54cc832d86\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797144\",\"name\":\"G. Friedland\"},{\"authorId\":\"1772549\",\"name\":\"D. Borth\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"df9a08016fa553a169d893ce2d3fca375bab4781\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/df9a08016fa553a169d893ce2d3fca375bab4781\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2011.00966\",\"authors\":[{\"authorId\":\"32370882\",\"name\":\"Shweta Mahajan\"},{\"authorId\":\"49863405\",\"name\":\"S. Roth\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d805e35f17dab7382748130a2ca1bda629cdceb\",\"title\":\"Diverse Image Captioning with Context-Object Split Latent Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7d805e35f17dab7382748130a2ca1bda629cdceb\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1811.12104\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"51491153\",\"name\":\"Takayuki Itamochi\"},{\"authorId\":\"3193466\",\"name\":\"K. Narioka\"},{\"authorId\":\"143973868\",\"name\":\"Ikuro Sato\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3c5907ba6b921841a49e77632c14db64e4db4fd4\",\"title\":\"Towards Human-Friendly Referring Expression Generation\",\"url\":\"https://www.semanticscholar.org/paper/3c5907ba6b921841a49e77632c14db64e4db4fd4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134654394\",\"name\":\"Ren C. Luo\"},{\"authorId\":\"34373093\",\"name\":\"Yu-Ting Hsu\"},{\"authorId\":\"151486060\",\"name\":\"Huan-Jun Ye\"}],\"doi\":\"10.1109/ISIE.2019.8781144\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"02695dca78cfe5c6c4fc7e4659b7ecac2b793bbc\",\"title\":\"Multi-Modal Human-Aware Image Caption System for Intelligent Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/02695dca78cfe5c6c4fc7e4659b7ecac2b793bbc\",\"venue\":\"2019 IEEE 28th International Symposium on Industrial Electronics (ISIE)\",\"year\":2019},{\"arxivId\":\"1910.06475\",\"authors\":[{\"authorId\":\"152325076\",\"name\":\"H. Ge\"},{\"authorId\":\"79403975\",\"name\":\"Zehang Yan\"},{\"authorId\":\"96280392\",\"name\":\"K. Zhang\"},{\"authorId\":\"145327825\",\"name\":\"Mingde Zhao\"},{\"authorId\":\"46732983\",\"name\":\"Liang Sun\"}],\"doi\":\"10.1109/ICCV.2019.00184\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b5231149f566fc8a78797b6fb448f9bca416380\",\"title\":\"Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style\",\"url\":\"https://www.semanticscholar.org/paper/4b5231149f566fc8a78797b6fb448f9bca416380\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1910.02974\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/ICRA40945.2020.9196653\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fe6f2a08cb8911d52533a413b071638d0463f10a\",\"title\":\"SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability\",\"url\":\"https://www.semanticscholar.org/paper/fe6f2a08cb8911d52533a413b071638d0463f10a\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":\"1806.06004\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ceabd7ff28ce2d501511da998252aeb938adc98b\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ceabd7ff28ce2d501511da998252aeb938adc98b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2006.11807\",\"authors\":[{\"authorId\":\"144910087\",\"name\":\"Zhan Shi\"},{\"authorId\":\"152482200\",\"name\":\"X. Zhou\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"150345740\",\"name\":\"Xiao-Dan Zhu\"}],\"doi\":\"10.18653/v1/2020.acl-main.664\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7e0f91e51ee372939c96714c7919dde6dc756849\",\"title\":\"Improving Image Captioning with Better Use of Captions\",\"url\":\"https://www.semanticscholar.org/paper/7e0f91e51ee372939c96714c7919dde6dc756849\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1912.03098\",\"authors\":[{\"authorId\":\"1403171438\",\"name\":\"J. Pont-Tuset\"},{\"authorId\":\"1823362\",\"name\":\"J. Uijlings\"},{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"}],\"doi\":\"10.1007/978-3-030-58558-7_38\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"title\":\"Connecting Vision and Language with Localized Narratives\",\"url\":\"https://www.semanticscholar.org/paper/439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1811.10652\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2019.00850\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8e59cf8c3becbedced0089028a1cddac8b19b251\",\"title\":\"Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions\",\"url\":\"https://www.semanticscholar.org/paper/8e59cf8c3becbedced0089028a1cddac8b19b251\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2005.08045\",\"authors\":[{\"authorId\":\"152516150\",\"name\":\"Aniket Agarwal\"},{\"authorId\":\"1703122502\",\"name\":\"Ayush Mangal\"},{\"authorId\":\"80465887\",\"name\":\"Vipul\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ac73d0abab68604b9c7a2b98635516eda899c7b3\",\"title\":\"Visual Relationship Detection using Scene Graphs: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/ac73d0abab68604b9c7a2b98635516eda899c7b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.08617\",\"authors\":[{\"authorId\":\"84267967\",\"name\":\"C. Thomas\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-58523-5_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"432921b7a2c782cedc2a7d87b6194b906e31086d\",\"title\":\"Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/432921b7a2c782cedc2a7d87b6194b906e31086d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41050714\",\"name\":\"Sandeep Narayan Parameswaran\"},{\"authorId\":\"144009889\",\"name\":\"Sukhendu Das\"}],\"doi\":\"10.1007/978-3-030-36718-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c940e8b66e822dbdd8d0dde9a6640ed01834e31\",\"title\":\"SACIC: A Semantics-Aware Convolutional Image Captioner Using Multi-level Pervasive Attention\",\"url\":\"https://www.semanticscholar.org/paper/9c940e8b66e822dbdd8d0dde9a6640ed01834e31\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":\"2003.10606\",\"authors\":[{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/CVPR42600.2020.01043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"70ea5d98141cd845d1c8131f25ddbc77f962f3d8\",\"title\":\"Video Object Grounding Using Semantic Roles in Language Description\",\"url\":\"https://www.semanticscholar.org/paper/70ea5d98141cd845d1c8131f25ddbc77f962f3d8\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48569838\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1109/TMM.2019.2896516\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bc73f0f7d895a089416bc8f6090f3f8707c6a12f\",\"title\":\"Know More Say Less: Image Captioning Based on Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/bc73f0f7d895a089416bc8f6090f3f8707c6a12f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40895688\",\"name\":\"Kaylee Burns\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9226675e413bb4b82ad419a10e4ac9ebd6aa7fef\",\"title\":\"Addressing and Understanding Shortcomings in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/9226675e413bb4b82ad419a10e4ac9ebd6aa7fef\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.07810\",\"authors\":[{\"authorId\":\"50118263\",\"name\":\"Yike Wu\"},{\"authorId\":\"2516425\",\"name\":\"Shiwan Zhao\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"48379958\",\"name\":\"Ying Zhang\"},{\"authorId\":\"1721029\",\"name\":\"Xiaojie Yuan\"},{\"authorId\":\"1703625\",\"name\":\"Zhong Su\"}],\"doi\":\"10.1109/ICME.2019.00070\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d4b29b999bc2d907d6d01ad30829058721d29394\",\"title\":\"Improving Captioning for Low-Resource Languages by Cycle Consistency\",\"url\":\"https://www.semanticscholar.org/paper/d4b29b999bc2d907d6d01ad30829058721d29394\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2002.06701\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19b00a2bd1624a0931069a539a971e40b65bfb3a\",\"title\":\"Gaussian Smoothen Semantic Features (GSSF) - Exploring the Linguistic Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO Framework\",\"url\":\"https://www.semanticscholar.org/paper/19b00a2bd1624a0931069a539a971e40b65bfb3a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s11042-019-08021-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ba03c16bf25d33efc0d977ab51392e1d9b5a0fb\",\"title\":\"Survey of deep learning and architectures for visual captioning\\u2014transitioning between media and natural languages\",\"url\":\"https://www.semanticscholar.org/paper/1ba03c16bf25d33efc0d977ab51392e1d9b5a0fb\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1907.10164\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"2365530\",\"name\":\"Mingda Zhang\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"},{\"authorId\":\"144838223\",\"name\":\"Wei Li\"},{\"authorId\":\"36690046\",\"name\":\"Danfeng Qin\"},{\"authorId\":\"6367313\",\"name\":\"J. Berent\"}],\"doi\":\"10.1109/ICCV.2019.00978\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a726e5a416da3d25e1127132e11f651de80eb76\",\"title\":\"Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/3a726e5a416da3d25e1127132e11f651de80eb76\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.13148\",\"authors\":[{\"authorId\":\"3110609\",\"name\":\"Zhennan Wang\"},{\"authorId\":\"2568383\",\"name\":\"Wenbin Zou\"},{\"authorId\":\"144282087\",\"name\":\"C. Xu\"}],\"doi\":\"10.1109/ICCV.2019.00611\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7c074e0fce985164989f0163cef8e7bb59a3612\",\"title\":\"PR Product: A Substitute for Inner Product in Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b7c074e0fce985164989f0163cef8e7bb59a3612\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1805.08298\",\"authors\":[{\"authorId\":\"46194597\",\"name\":\"C. Y. Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e2a2818ec251d947acd9c74c2040337e656946bc\",\"title\":\"Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/e2a2818ec251d947acd9c74c2040337e656946bc\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2009.12313\",\"authors\":[{\"authorId\":\"32556011\",\"name\":\"Victor Milewski\"},{\"authorId\":\"100781843\",\"name\":\"Marie-Francine Moens\"},{\"authorId\":\"2338197\",\"name\":\"Iacer Calixto\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b89e002625b0703f08b7590a1763ca7c8be66ad4\",\"title\":\"Are scene graphs good enough to improve Image Captioning?\",\"url\":\"https://www.semanticscholar.org/paper/b89e002625b0703f08b7590a1763ca7c8be66ad4\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2003.01473\",\"authors\":[{\"authorId\":\"10007273\",\"name\":\"Qiaolin Xia\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"144934143\",\"name\":\"Dongdong Zhang\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"49575302\",\"name\":\"Zhifang Sui\"},{\"authorId\":\"144530394\",\"name\":\"Edward Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":null,\"name\":\"Xin Liu\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"title\":\"XGPT: Cross-modal Generative Pre-Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35656420\",\"name\":\"Yoshito Nagaoka\"},{\"authorId\":\"3388392\",\"name\":\"T. Miyazaki\"},{\"authorId\":\"2137463\",\"name\":\"Y. Sugaya\"},{\"authorId\":\"1740235\",\"name\":\"S. Omachi\"}],\"doi\":\"10.1109/ACCESS.2019.2917554\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"220fbdc361ed6be65cbc365167450c4926357f6e\",\"title\":\"Automatic Mackerel Sorting Machine Using Global and Local Features\",\"url\":\"https://www.semanticscholar.org/paper/220fbdc361ed6be65cbc365167450c4926357f6e\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/ISM.2018.00021\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"95bba8c514f63c666e0a85bdf9c6e3b8113d1f8f\",\"title\":\"Improved Image Description Via Embedded Object Structure Graph and Semantic Feature Matching\",\"url\":\"https://www.semanticscholar.org/paper/95bba8c514f63c666e0a85bdf9c6e3b8113d1f8f\",\"venue\":\"2018 IEEE International Symposium on Multimedia (ISM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"1491236221\",\"name\":\"Meng Gao\"},{\"authorId\":\"1747773\",\"name\":\"T. Zhang\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1109/ICDM.2019.00054\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f04013efcdf606d65145859f4f9eb6c48908869\",\"title\":\"Exploring Semantic Relationships for Image Captioning without Parallel Data\",\"url\":\"https://www.semanticscholar.org/paper/8f04013efcdf606d65145859f4f9eb6c48908869\",\"venue\":\"2019 IEEE International Conference on Data Mining (ICDM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1774780\",\"name\":\"Yilin Shen\"},{\"authorId\":\"1705713\",\"name\":\"Hongxia Jin\"},{\"authorId\":\"46848045\",\"name\":\"S. Ghosh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"080a2805853cc0e5a8cba05af1fb7a65ace8f6d7\",\"title\":\"LANGUAGE MODELS MORE GROUNDED\",\"url\":\"https://www.semanticscholar.org/paper/080a2805853cc0e5a8cba05af1fb7a65ace8f6d7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"title\":\"CLARA: Dynamic Doctor Representation Learning for Clinical Trial Recruitment\",\"url\":\"https://www.semanticscholar.org/paper/e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eccfd6174709c285d62b11fb3da9b68d485ca883\",\"title\":\"Integrating Rule-based Entity Masking into Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/eccfd6174709c285d62b11fb3da9b68d485ca883\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7504183\",\"name\":\"B. Zhang\"},{\"authorId\":\"50703807\",\"name\":\"L. Li\"},{\"authorId\":\"153142919\",\"name\":\"Li Su\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"1994580036\",\"name\":\"Jincan Deng\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1145/3394171.3413885\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b0209f6fa41030e7e590f2cb5568f47563a759a\",\"title\":\"Structural Semantic Adversarial Active Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b0209f6fa41030e7e590f2cb5568f47563a759a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1905.11624\",\"authors\":[{\"authorId\":\"121038417\",\"name\":\"Zih-Siou Hung\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1109/TPAMI.2020.2992222\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19c17dd4e6929a7c9d198af2bd5207a20f1cd785\",\"title\":\"Contextual Translation Embedding for Visual Relationship Detection and Scene Graph Generation.\",\"url\":\"https://www.semanticscholar.org/paper/19c17dd4e6929a7c9d198af2bd5207a20f1cd785\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1912.11637\",\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"143725038\",\"name\":\"Qi Su\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"title\":\"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1904.11574\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.acl-main.730\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb1b368ca847846774ee41af6da906ab77013313\",\"title\":\"TVQA+: Spatio-Temporal Grounding for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/eb1b368ca847846774ee41af6da906ab77013313\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1911.04365\",\"authors\":[{\"authorId\":\"9457831\",\"name\":\"J. He\"},{\"authorId\":\"1405953764\",\"name\":\"Quan-Jie Cao\"},{\"authorId\":\"39844955\",\"name\":\"L. Zhang\"},{\"authorId\":\"93970041\",\"name\":\"Hui Tao\"}],\"doi\":\"10.1109/ACCESS.2020.2982571\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"587e1359d0a54ecf24e2577357abe79d0cd00ab2\",\"title\":\"Conditionally Learn to Pay Attention for Sequential Visual Task\",\"url\":\"https://www.semanticscholar.org/paper/587e1359d0a54ecf24e2577357abe79d0cd00ab2\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2012.15454\",\"authors\":[{\"authorId\":null,\"name\":\"Wei-Ning Hsu\"},{\"authorId\":null,\"name\":\"David Harwath\"},{\"authorId\":null,\"name\":\"Christopher Song\"},{\"authorId\":\"152450847\",\"name\":\"J. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5df12460d9a742b08f82e8b79cb102a8be5dd9b4\",\"title\":\"Text-Free Image-to-Speech Synthesis Using Learned Segmental Units\",\"url\":\"https://www.semanticscholar.org/paper/5df12460d9a742b08f82e8b79cb102a8be5dd9b4\",\"venue\":\"\",\"year\":2020}],\"corpusId\":4406645,\"doi\":\"10.1109/CVPR.2018.00754\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":29,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"references\":[{\"arxivId\":\"1511.02283\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"3317152\",\"name\":\"Oana-Maria Camburu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/CVPR.2016.9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e65142010431ffc089b272a1174214e00693e503\",\"title\":\"Generation and Comprehension of Unambiguous Object Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e65142010431ffc089b272a1174214e00693e503\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"34176020\",\"name\":\"Jesse Dodge\"},{\"authorId\":\"46479604\",\"name\":\"Amit Goyal\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"},{\"authorId\":\"1714215\",\"name\":\"K. Stratos\"},{\"authorId\":\"1682965\",\"name\":\"Xufeng Han\"},{\"authorId\":\"40614240\",\"name\":\"A. Mensch\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"355de7460120ddc1150d9ce3756f9848983f7ff4\",\"title\":\"Midge: Generating Image Descriptions From Computer Vision Detections\",\"url\":\"https://www.semanticscholar.org/paper/355de7460120ddc1150d9ce3756f9848983f7ff4\",\"venue\":\"EACL\",\"year\":2012},{\"arxivId\":\"1606.07770\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2017.130\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9aa3bafa9e8e21bb92908ae23b468fa248239b3\",\"title\":\"Captioning Images with Diverse Objects\",\"url\":\"https://www.semanticscholar.org/paper/b9aa3bafa9e8e21bb92908ae23b468fa248239b3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1506.03134\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"39067762\",\"name\":\"Meire Fortunato\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9653d5c2c7844347343d073bbedd96e05d52f69b\",\"title\":\"Pointer Networks\",\"url\":\"https://www.semanticscholar.org/paper/9653d5c2c7844347343d073bbedd96e05d52f69b\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50536468\",\"name\":\"Danqi Chen\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1082\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a14045a751f5d8ed387c8630a86a3a2861b90643\",\"title\":\"A Fast and Accurate Dependency Parser using Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/a14045a751f5d8ed387c8630a86a3a2861b90643\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143845796\",\"name\":\"Jeffrey Pennington\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1162\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"title\":\"Glove: Global Vectors for Word Representation\",\"url\":\"https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1511.03745\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-46448-0_49\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"14c2321851fb5ae580a19726dd2753a525d6ad76\",\"title\":\"Grounding of Textual Phrases in Images by Reconstruction\",\"url\":\"https://www.semanticscholar.org/paper/14c2321851fb5ae580a19726dd2753a525d6ad76\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1607.08822\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.1007/978-3-319-46454-1_24\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"title\":\"SPICE: Semantic Propositional Image Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"}],\"doi\":\"10.1109/TPAMI.2018.2844175\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a0912bb76777469295bb2c059faee907e7f3258\",\"title\":\"Mask R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/1a0912bb76777469295bb2c059faee907e7f3258\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3006072\",\"name\":\"Sahar Kazemzadeh\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"32215032\",\"name\":\"Mark Matten\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.3115/v1/D14-1086\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"92c141447f51b6732242376164ff961e464731c8\",\"title\":\"ReferItGame: Referring to Objects in Photographs of Natural Scenes\",\"url\":\"https://www.semanticscholar.org/paper/92c141447f51b6732242376164ff961e464731c8\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"},{\"authorId\":\"1760868\",\"name\":\"M. Surdeanu\"},{\"authorId\":\"144661918\",\"name\":\"John Bauer\"},{\"authorId\":\"2784228\",\"name\":\"Jenny Rose Finkel\"},{\"authorId\":\"2105138\",\"name\":\"Steven Bethard\"},{\"authorId\":\"2240597\",\"name\":\"D. McClosky\"}],\"doi\":\"10.3115/v1/P14-5010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f5102ec3f70d0dea98c957cc2cab4d15d83a2da\",\"title\":\"The Stanford CoreNLP Natural Language Processing Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/2f5102ec3f70d0dea98c957cc2cab4d15d83a2da\",\"venue\":\"ACL\",\"year\":2014},{\"arxivId\":\"1301.3781\",\"authors\":[{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"38644044\",\"name\":\"Kai Chen\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"330da625c15427c6e42ccfa3b747fb29e5835bf0\",\"title\":\"Efficient Estimation of Word Representations in Vector Space\",\"url\":\"https://www.semanticscholar.org/paper/330da625c15427c6e42ccfa3b747fb29e5835bf0\",\"venue\":\"ICLR\",\"year\":2013},{\"arxivId\":\"1612.00576\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.18653/v1/D17-1098\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c\",\"title\":\"Guided Open Vocabulary Image Captioning with Constrained Beam Search\",\"url\":\"https://www.semanticscholar.org/paper/086fa2fe3ee2a5b805aeaf9fbfe59ee8157dad5c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":\"1511.07571\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.494\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"title\":\"DenseCap: Fully Convolutional Localization Networks for Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1701.03439\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"2490189\",\"name\":\"Gregory Shakhnarovich\"}],\"doi\":\"10.1109/CVPR.2017.333\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b45e9a40313096abf530df3b98a1dfa1553f17b\",\"title\":\"Comprehension-Guided Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/9b45e9a40313096abf530df3b98a1dfa1553f17b\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1708.05271\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.559\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10480a42957a8e08e4c543185e135d7c254583a5\",\"title\":\"Incorporating Copying Mechanism in Image Captioning for Learning Novel Objects\",\"url\":\"https://www.semanticscholar.org/paper/10480a42957a8e08e4c543185e135d7c254583a5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/TPAMI.2012.162\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5cb6700d94c6118ee13f4f4fecac99f111189812\",\"title\":\"BabyTalk: Understanding and Generating Simple Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/5cb6700d94c6118ee13f4f4fecac99f111189812\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145592791\",\"name\":\"P. Kuznetsova\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2a0d0f6c5a69b264710df0230696f47c5918e2f2\",\"title\":\"Collective Generation of Natural Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2a0d0f6c5a69b264710df0230696f47c5918e2f2\",\"venue\":\"ACL\",\"year\":2012},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1611.09978\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2017.470\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ce264a4e1490e959d84ddd60edbb0edcbfb3af38\",\"title\":\"Modeling Relationships in Referential Expressions with Compositional Modular Networks\",\"url\":\"https://www.semanticscholar.org/paper/ce264a4e1490e959d84ddd60edbb0edcbfb3af38\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1606.03556\",\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1016/j.cviu.2017.10.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58cb0c24c936b8a14ca7b2d56ba80de733c545b3\",\"title\":\"Human Attention in Visual Question Answering: Do Humans and Deep Networks look at the same regions?\",\"url\":\"https://www.semanticscholar.org/paper/58cb0c24c936b8a14ca7b2d56ba80de733c545b3\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1888731\",\"name\":\"M. Hejrati\"},{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/978-3-642-15561-1_2\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"title\":\"Every Picture Tells a Story: Generating Sentences from Images\",\"url\":\"https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":\"1506.01497\",\"authors\":[{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"144716314\",\"name\":\"J. Sun\"}],\"doi\":\"10.1109/TPAMI.2016.2577031\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"title\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":\"1605.07912\",\"authors\":[{\"authorId\":\"47087291\",\"name\":\"Z. Yang\"},{\"authorId\":\"30556331\",\"name\":\"Y. Yuan\"},{\"authorId\":\"9287688\",\"name\":\"Yuexin Wu\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"90fbeb4c871d3916c2b428645a1e1482f05826e1\",\"title\":\"Encode, Review, and Decode: Reviewer Module for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/90fbeb4c871d3916c2b428645a1e1482f05826e1\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1505.04870\",\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2133220\",\"name\":\"C. Cervantes\"},{\"authorId\":\"145507543\",\"name\":\"Juan C. Caicedo\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"}],\"doi\":\"10.1007/s11263-016-0965-7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0612745dbd292fc0a548a16d39cd73e127faedde\",\"title\":\"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models\",\"url\":\"https://www.semanticscholar.org/paper/0612745dbd292fc0a548a16d39cd73e127faedde\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1707.07998\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"31790073\",\"name\":\"C. Buehler\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1109/CVPR.2018.00636\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"title\":\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1505.00468\",\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"118707418\",\"name\":\"M. Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/s11263-016-0966-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/CVPR.2011.5995466\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"169b847e69c35cfd475eb4dcc561a24de11762ca\",\"title\":\"Baby talk: Understanding and generating simple image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/169b847e69c35cfd475eb4dcc561a24de11762ca\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1109/CVPR.2015.7298856\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a72b8bbd039989db39769da836cdb287737deb92\",\"title\":\"Mind's eye: A recurrent visual representation for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/a72b8bbd039989db39769da836cdb287737deb92\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1603.03925\",\"authors\":[{\"authorId\":\"36610242\",\"name\":\"Quanzeng You\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.503\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf55591e09b58ea9ce8d66110d6d3000ee804bdd\",\"title\":\"Image Captioning with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1611.08669\",\"authors\":[{\"authorId\":\"145497716\",\"name\":\"A. Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"39855500\",\"name\":\"K. Gupta\"},{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"},{\"authorId\":\"24508084\",\"name\":\"Deshraj Yadav\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPR.2017.121\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2231f44be9a8472a46d8e8a628b4e52b9a8f44e0\",\"title\":\"Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/2231f44be9a8472a46d8e8a628b4e52b9a8f44e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fad611e35b3731740b4d8b754241e77add5a70b9\",\"title\":\"Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/fad611e35b3731740b4d8b754241e77add5a70b9\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1612.01887\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":\"10.1109/CVPR.2017.345\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9f4d7d622d1f7319cc511bfef661cd973e881a4c\",\"title\":\"Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9f4d7d622d1f7319cc511bfef661cd973e881a4c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Yang\"},{\"authorId\":null,\"name\":\"J. Lu\"},{\"authorId\":null,\"name\":\"D. Batra\"},{\"authorId\":null,\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A faster pytorch implementation of faster r-cnn\",\"url\":\"\",\"venue\":\"https://github.com/ jwyang/faster-rcnn.pytorch,\",\"year\":2017},{\"arxivId\":\"1608.00272\",\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"3451188\",\"name\":\"Patrick Poirson\"},{\"authorId\":\"144947353\",\"name\":\"S. Yang\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1007/978-3-319-46475-6_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29efbe391950ae438c63d86ad5c82b2942efb0b4\",\"title\":\"Modeling Context in Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/29efbe391950ae438c63d86ad5c82b2942efb0b4\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1511.04164\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"3286703\",\"name\":\"Huazhe Xu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPR.2016.493\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d696a1923288e6c15422660de9553f6fdb6a4fae\",\"title\":\"Natural Language Object Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d696a1923288e6c15422660de9553f6fdb6a4fae\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.05284\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPR.2016.8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"title\":\"Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data\",\"url\":\"https://www.semanticscholar.org/paper/e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1612.00563\",\"authors\":[{\"authorId\":\"2071376\",\"name\":\"Steven J. Rennie\"},{\"authorId\":\"2293163\",\"name\":\"E. Marcheret\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"39320489\",\"name\":\"J. Ross\"},{\"authorId\":\"1782589\",\"name\":\"V. Goel\"}],\"doi\":\"10.1109/CVPR.2017.131\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6c8353697cdbb98dfba4f493875778c4286d3e3a\",\"title\":\"Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c8353697cdbb98dfba4f493875778c4286d3e3a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}],\"title\":\"Neural Baby Talk\",\"topics\":[{\"topic\":\"Sensor\",\"topicId\":\"1117\",\"url\":\"https://www.semanticscholar.org/topic/1117\"},{\"topic\":\"Minimum bounding box\",\"topicId\":\"195508\",\"url\":\"https://www.semanticscholar.org/topic/195508\"},{\"topic\":\"Entity\",\"topicId\":\"6664\",\"url\":\"https://www.semanticscholar.org/topic/6664\"},{\"topic\":\"Lemmatisation\",\"topicId\":\"317987\",\"url\":\"https://www.semanticscholar.org/topic/317987\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Vocabulary\",\"topicId\":\"14901\",\"url\":\"https://www.semanticscholar.org/topic/14901\"},{\"topic\":\"Organizing (structure)\",\"topicId\":\"7824\",\"url\":\"https://www.semanticscholar.org/topic/7824\"},{\"topic\":\"Automatic sounding\",\"topicId\":\"3692259\",\"url\":\"https://www.semanticscholar.org/topic/3692259\"},{\"topic\":\"Map\",\"topicId\":\"2392\",\"url\":\"https://www.semanticscholar.org/topic/2392\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"}],\"url\":\"https://www.semanticscholar.org/paper/3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"