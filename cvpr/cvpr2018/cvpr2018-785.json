"{\"abstract\":\"Image sentiment influences visual perception. Emotion-eliciting stimuli such as happy faces and poisonous snakes are generally prioritized in human attention. However, little research has evaluated the interrelationships of image sentiment and visual saliency. In this paper, we present the first study to focus on the relation between emotional properties of an image and visual attention. We first create the EMOtional attention dataset (EMOd). It is a diverse set of emotion-eliciting images, and each image has (1) eye-tracking data collected from 16 subjects, (2) intensive image context labels including object contour, object sentiment, object semantic category, and high-level perceptual attributes such as image aesthetics and elicited emotions. We perform extensive analyses on EMOd to identify how image sentiment relates to human attention. We discover an emotion prioritization effect: for our images, emotion-eliciting content attracts human attention strongly, but such advantage diminishes dramatically after initial fixation. Aiming to model the human emotion prioritization computationally, we design a deep neural network for saliency prediction, which includes a novel subnetwork that learns the spatial and semantic context of the image scene. The proposed network outperforms the state-of-the-art on three benchmark datasets, by effectively capturing the relative importance of human attention within an image. The code, models, and dataset are available online at https://nus-sesame.top/emotionalattention/.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2087470\",\"name\":\"S. Fan\",\"url\":\"https://www.semanticscholar.org/author/2087470\"},{\"authorId\":\"144506018\",\"name\":\"Z. Shen\",\"url\":\"https://www.semanticscholar.org/author/144506018\"},{\"authorId\":\"144645142\",\"name\":\"M. Jiang\",\"url\":\"https://www.semanticscholar.org/author/144645142\"},{\"authorId\":\"34861255\",\"name\":\"Bryan L. Koenig\",\"url\":\"https://www.semanticscholar.org/author/34861255\"},{\"authorId\":\"1946538\",\"name\":\"J. Xu\",\"url\":\"https://www.semanticscholar.org/author/1946538\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\",\"url\":\"https://www.semanticscholar.org/author/1744045\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\",\"url\":\"https://www.semanticscholar.org/author/49033321\"}],\"citationVelocity\":13,\"citations\":[{\"arxivId\":\"1810.03716\",\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4df34e0194faa27078832cb5078a2af6c9d0ea9b\",\"title\":\"Saliency Prediction in the Deep Learning Era: An Empirical Investigation\",\"url\":\"https://www.semanticscholar.org/paper/4df34e0194faa27078832cb5078a2af6c9d0ea9b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1395583740\",\"name\":\"Alexis Nebout\"},{\"authorId\":\"67321830\",\"name\":\"Weijie Wei\"},{\"authorId\":\"49292977\",\"name\":\"Z. Liu\"},{\"authorId\":\"47033364\",\"name\":\"Lijin Huang\"},{\"authorId\":\"1789744\",\"name\":\"O. Meur\"}],\"doi\":\"10.1109/ICMEW.2019.00121\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8c90722b4b020f3a9a2e2e5fdc7bd24a291c8c3\",\"title\":\"Predicting Saliency Maps for ASD People\",\"url\":\"https://www.semanticscholar.org/paper/b8c90722b4b020f3a9a2e2e5fdc7bd24a291c8c3\",\"venue\":\"2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52164159\",\"name\":\"Yuya Moroto\"},{\"authorId\":\"47761580\",\"name\":\"K. Maeda\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.3390/s20082170\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f72a875502d619c16a78cef5c885cd781c15e45\",\"title\":\"Few-Shot Personalized Saliency Prediction Based on Adaptive Image Selection Considering Object and Visual Attention\\u2020\",\"url\":\"https://www.semanticscholar.org/paper/3f72a875502d619c16a78cef5c885cd781c15e45\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48341639\",\"name\":\"Masanao Matsumoto\"},{\"authorId\":\"3493752\",\"name\":\"Naoki Saito\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/GCCE46687.2019.9015464\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b33e05921e812924689e5240a0fc9baacd7f52e9\",\"title\":\"Interest Estimation for Images Based on Eye Gaze-based Visual and Text Features\",\"url\":\"https://www.semanticscholar.org/paper/b33e05921e812924689e5240a0fc9baacd7f52e9\",\"venue\":\"2019 IEEE 8th Global Conference on Consumer Electronics (GCCE)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1576676380\",\"name\":\"Maria Wahid\"},{\"authorId\":\"40989721\",\"name\":\"Asim Waris\"},{\"authorId\":\"2455342\",\"name\":\"S. Gilani\"},{\"authorId\":\"48236457\",\"name\":\"R. Subramanian\"}],\"doi\":\"10.3390/app9245378\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"95f802e62f6941d7569611c78ad97bcf8f5b9124\",\"title\":\"The Effect of Eye Movements in Response to Different Types of Scenes Using a Graph-Based Visual Saliency Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/95f802e62f6941d7569611c78ad97bcf8f5b9124\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49279229\",\"name\":\"Lifang Wu\"},{\"authorId\":\"35564789\",\"name\":\"Mingchao Qi\"},{\"authorId\":\"2980051\",\"name\":\"Meng Jian\"},{\"authorId\":\"40940525\",\"name\":\"H. Zhang\"}],\"doi\":\"10.1007/s11063-019-10027-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6f22922d5609f54710c4719dd3f28e34d82801a7\",\"title\":\"Visual Sentiment Analysis by Combining Global and Local Information\",\"url\":\"https://www.semanticscholar.org/paper/6f22922d5609f54710c4719dd3f28e34d82801a7\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144506018\",\"name\":\"Z. Shen\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"2475944\",\"name\":\"Tian-Tsong Ng\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1145/3343031.3350963\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3eab57ec72ee5a04250b5924df72dfbe6a533605\",\"title\":\"Human-imperceptible Privacy Protection Against Machines\",\"url\":\"https://www.semanticscholar.org/paper/3eab57ec72ee5a04250b5924df72dfbe6a533605\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49969564\",\"name\":\"Z. Li\"},{\"authorId\":\"30561276\",\"name\":\"Xiao-yan Zhang\"}],\"doi\":\"10.1109/ICME.2019.00052\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fc346eddd59f861d44ac08d428aae292b37809bc\",\"title\":\"Collaborative Deep Reinforcement Learning for Image Cropping\",\"url\":\"https://www.semanticscholar.org/paper/fc346eddd59f861d44ac08d428aae292b37809bc\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1842380883\",\"name\":\"Miroslav Laco\"},{\"authorId\":\"1842397131\",\"name\":\"Patrik Polatsek\"},{\"authorId\":\"1842403309\",\"name\":\"\\u0160imon Dekr\\u00e9t\"},{\"authorId\":\"2678977\",\"name\":\"W. Benesova\"},{\"authorId\":\"1842388686\",\"name\":\"Martina Bar\\u00e1nkov\\u00e1\"},{\"authorId\":\"1393827692\",\"name\":\"Bronislava Strn\\u00e1delov\\u00e1\"},{\"authorId\":\"114588830\",\"name\":\"Jana Kor\\u00f3niov\\u00e1\"},{\"authorId\":\"1842381266\",\"name\":\"M\\u00e1ria Gabl\\u00edkov\\u00e1\"}],\"doi\":\"10.1007/s00371-020-01912-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5cf2dfc8bce0d987b6696a7ffdb223ff3ba960d2\",\"title\":\"Effects of individual\\u2019s emotions on saliency and visual search\",\"url\":\"https://www.semanticscholar.org/paper/5cf2dfc8bce0d987b6696a7ffdb223ff3ba960d2\",\"venue\":\"The Visual Computer\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yujia Wang\"},{\"authorId\":\"14484252\",\"name\":\"W. Liang\"},{\"authorId\":\"1993490392\",\"name\":\"Wanwan Li\"},{\"authorId\":\"40580714\",\"name\":\"Dingzeyu Li\"},{\"authorId\":\"37397820\",\"name\":\"Lap-Fai Yu\"}],\"doi\":\"10.1145/3394171.3413894\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6363638e0bcbd2bc7fdf64467fe5cb52754f51c2\",\"title\":\"Scene-Aware Background Music Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/6363638e0bcbd2bc7fdf64467fe5cb52754f51c2\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52164159\",\"name\":\"Yuya Moroto\"},{\"authorId\":\"47761580\",\"name\":\"K. Maeda\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ACCESS.2020.3036908\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"004f7f01a3fe7db72b74a248a86493eb5163e7bf\",\"title\":\"Human-Centric Emotion Estimation Based on Correlation Maximization Considering Changes With Time in Visual Attention and Brain Activity\",\"url\":\"https://www.semanticscholar.org/paper/004f7f01a3fe7db72b74a248a86493eb5163e7bf\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46653600\",\"name\":\"A. Ali\"},{\"authorId\":\"2706430\",\"name\":\"Y. Kim\"}],\"doi\":\"10.1109/ACCESS.2020.2986815\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ae009483d9cc2fc0f6c11ca2415c362cea93e37\",\"title\":\"Deep Fusion for 3D Gaze Estimation From Natural Face Images Using Multi-Stream CNNs\",\"url\":\"https://www.semanticscholar.org/paper/7ae009483d9cc2fc0f6c11ca2415c362cea93e37\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35144077\",\"name\":\"A. Yadav\"},{\"authorId\":\"47731526\",\"name\":\"D. Vishwakarma\"}],\"doi\":\"10.1007/s00530-020-00656-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"764a6412c9ed41a06092bc6a908d25201029f0da\",\"title\":\"A deep learning architecture of RA-DLNet for visual sentiment analysis\",\"url\":\"https://www.semanticscholar.org/paper/764a6412c9ed41a06092bc6a908d25201029f0da\",\"venue\":\"Multimedia Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134797832\",\"name\":\"Taiga Matsui\"},{\"authorId\":\"3493752\",\"name\":\"Naoki Saito\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"1969677\",\"name\":\"Satoshi Asamizu\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/LifeTech.2019.8884060\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a4175d770ba598d32ac152b1ed8bb5d91102ae9\",\"title\":\"Estimation of Emotions Evoked by Images Based on Multiple Gaze-based CNN Features\",\"url\":\"https://www.semanticscholar.org/paper/8a4175d770ba598d32ac152b1ed8bb5d91102ae9\",\"venue\":\"2019 IEEE 1st Global Conference on Life Sciences and Technologies (LifeTech)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22631231\",\"name\":\"Dongyu She\"},{\"authorId\":\"152669122\",\"name\":\"Ming Sun\"},{\"authorId\":\"1755872\",\"name\":\"Jufeng Yang\"}],\"doi\":\"10.1145/3326335\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"524b93f761919d49db771931b31c53e6a8854473\",\"title\":\"Learning Discriminative Sentiment Representation from Strongly- and Weakly Supervised CNNs\",\"url\":\"https://www.semanticscholar.org/paper/524b93f761919d49db771931b31c53e6a8854473\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134797832\",\"name\":\"Taiga Matsui\"},{\"authorId\":\"3493752\",\"name\":\"Naoki Saito\"},{\"authorId\":\"144392699\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"1969677\",\"name\":\"Satoshi Asamizu\"},{\"authorId\":\"144029207\",\"name\":\"Miki Haseyama\"}],\"doi\":\"10.1117/12.2516885\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32260ad008f38aea3d1eb82edfa82af781ee1606\",\"title\":\"Gaze-based visual feature extraction via DLPCCA for visual sentiment estimation\",\"url\":\"https://www.semanticscholar.org/paper/32260ad008f38aea3d1eb82edfa82af781ee1606\",\"venue\":\"Other Conferences\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1725408802\",\"name\":\"Meng Cao\"},{\"authorId\":\"48270171\",\"name\":\"Y. Zhu\"},{\"authorId\":\"49896351\",\"name\":\"Wenjing Gao\"},{\"authorId\":\"47629198\",\"name\":\"Mengyao Li\"},{\"authorId\":\"1741081423\",\"name\":\"Shaoxiu Wang\"}],\"doi\":\"10.1002/cpe.5954\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fc3b95a6fea7bd7c5c424e60463fbca57dae6a7d\",\"title\":\"Various syncretic co\\u2010attention network for multimodal sentiment analysis\",\"url\":\"https://www.semanticscholar.org/paper/fc3b95a6fea7bd7c5c424e60463fbca57dae6a7d\",\"venue\":\"Concurr. Comput. Pract. Exp.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22631231\",\"name\":\"Dongyu She\"},{\"authorId\":\"1755872\",\"name\":\"Jufeng Yang\"},{\"authorId\":\"37535930\",\"name\":\"Ming-Ming Cheng\"},{\"authorId\":\"144891983\",\"name\":\"Yu-Kun Lai\"},{\"authorId\":\"1734823\",\"name\":\"Paul L. Rosin\"},{\"authorId\":\"144143336\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/TMM.2019.2939744\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f9e1ed189a2fef330b07953a6c20e9df01b89fab\",\"title\":\"WSCNet: Weakly Supervised Coupled Networks for Visual Sentiment Classification and Detection\",\"url\":\"https://www.semanticscholar.org/paper/f9e1ed189a2fef330b07953a6c20e9df01b89fab\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48084733\",\"name\":\"Macario O. Cordel\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"1700911\",\"name\":\"Zhiqi Shen\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/CVPR.2019.00415\",\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d61bd9bc9a09731bcdfb7c84ef57b71ebb06a1e3\",\"title\":\"Emotion-Aware Human Attention Prediction\",\"url\":\"https://www.semanticscholar.org/paper/d61bd9bc9a09731bcdfb7c84ef57b71ebb06a1e3\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2007.14419\",\"authors\":[{\"authorId\":\"94011352\",\"name\":\"S. Chen\"},{\"authorId\":\"1405907659\",\"name\":\"Ming Jiang\"},{\"authorId\":\"7788087\",\"name\":\"J. Yang\"},{\"authorId\":\"153386875\",\"name\":\"Q. Zhao\"}],\"doi\":\"10.1007/978-3-030-58452-8_6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96150d2fdbb72e11882ad06b346d6e0d2e819d51\",\"title\":\"AiR: Attention with Reasoning Capability\",\"url\":\"https://www.semanticscholar.org/paper/96150d2fdbb72e11882ad06b346d6e0d2e819d51\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d173893dd3f22fd73acc16b94f0ea98469c9855\",\"title\":\"Saliency Prediction in the Deep Learning Era: Successes, Limitations, and Future Challenges\",\"url\":\"https://www.semanticscholar.org/paper/3d173893dd3f22fd73acc16b94f0ea98469c9855\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2001.03960\",\"authors\":[{\"authorId\":\"24057388\",\"name\":\"\\u00d6mer S\\u00fcmer\"},{\"authorId\":\"87719752\",\"name\":\"Peter Gerjets\"},{\"authorId\":\"2446461\",\"name\":\"U. Trautwein\"},{\"authorId\":\"1884159\",\"name\":\"Enkelejda Kasneci\"}],\"doi\":\"10.1109/WACV45572.2020.9093515\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc9d5b5af6ec2b0746b0dbec0d18918182ec6cb6\",\"title\":\"Attention Flow: End-to-End Joint Attention Estimation\",\"url\":\"https://www.semanticscholar.org/paper/fc9d5b5af6ec2b0746b0dbec0d18918182ec6cb6\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48341639\",\"name\":\"Masanao Matsumoto\"},{\"authorId\":\"1423472470\",\"name\":\"Naoki Saito\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/LifeTech48969.2020.1570619124\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a883187453a21a0b27b3d27d33893d5341ace5b\",\"title\":\"Interest Estimation for Images Using Eye Gaze-based Visual and Text Features via DLPCCA\",\"url\":\"https://www.semanticscholar.org/paper/8a883187453a21a0b27b3d27d33893d5341ace5b\",\"venue\":\"2020 IEEE 2nd Global Conference on Life Sciences and Technologies (LifeTech)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52164159\",\"name\":\"Yuya Moroto\"},{\"authorId\":\"47761580\",\"name\":\"K. Maeda\"},{\"authorId\":\"144392692\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/ICIP.2019.8803446\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd48dfb1946abd2cb30a7afaa064f3221788aff3\",\"title\":\"Estimation of Emotion Labels via Tensor-Based Spatiotemporal Visual Attention Analysis\",\"url\":\"https://www.semanticscholar.org/paper/bd48dfb1946abd2cb30a7afaa064f3221788aff3\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2542064\",\"name\":\"Guoqin Peng\"},{\"authorId\":\"145223021\",\"name\":\"D. Xu\"}],\"doi\":\"10.1007/978-3-030-31456-9_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a2b475de05ab9d4c5befd2056fd3ea6de5cec65\",\"title\":\"Weakly Supervised Learning of Image Emotion Analysis Based on Cross-spatial Pooling\",\"url\":\"https://www.semanticscholar.org/paper/8a2b475de05ab9d4c5befd2056fd3ea6de5cec65\",\"venue\":\"CCBR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405686623\",\"name\":\"Nils Murrugarra-Llerena\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1109/CVPR.2019.00659\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"title\":\"Cross-Modality Personalization for Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52164159\",\"name\":\"Yuya Moroto\"},{\"authorId\":\"47761580\",\"name\":\"K. Maeda\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.3390/s20072146\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2087b63d4a3b88bb3225bd90f4912f4e0eb46bb\",\"title\":\"Tensor-Based Emotional Category Classification via Visual Attention-Based Heterogeneous CNN Feature Fusion\",\"url\":\"https://www.semanticscholar.org/paper/c2087b63d4a3b88bb3225bd90f4912f4e0eb46bb\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48084733\",\"name\":\"Macario O. Cordel\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"1700911\",\"name\":\"Zhiqi Shen\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"166aa31fc7b37b1a4c4abc64b1c56f34ceb81bdc\",\"title\":\"Supplementary Material for Paper \\u201c Emotion-Aware Human Attention Prediction \\u201d\",\"url\":\"https://www.semanticscholar.org/paper/166aa31fc7b37b1a4c4abc64b1c56f34ceb81bdc\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50543166\",\"name\":\"Anne-Flore Perrin\"},{\"authorId\":\"3163032\",\"name\":\"Vassilios Krassanakis\"},{\"authorId\":\"1429346658\",\"name\":\"Lu Zhang\"},{\"authorId\":\"1764950\",\"name\":\"V. Ricordel\"},{\"authorId\":\"21340060\",\"name\":\"Matthieu Perreira Da Silva\"},{\"authorId\":\"1789744\",\"name\":\"O. Meur\"}],\"doi\":\"10.3390/drones4010002\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd04ba0a3e183ee86221ecf98efb73c3495ec929\",\"title\":\"EyeTrackUAV2: a Large-Scale Binocular Eye-Tracking Dataset for UAV Videos\",\"url\":\"https://www.semanticscholar.org/paper/cd04ba0a3e183ee86221ecf98efb73c3495ec929\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"19269060\",\"name\":\"Huiyu Duan\"},{\"authorId\":\"2246414\",\"name\":\"Xiongkuo Min\"},{\"authorId\":\"47291418\",\"name\":\"Yi Fang\"},{\"authorId\":\"144509573\",\"name\":\"Lei Fan\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"},{\"authorId\":\"13903793\",\"name\":\"Guangtao Zhai\"}],\"doi\":\"10.1145/3337066\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e01f0475ac9a96da307b2b10778ab6c9283381fb\",\"title\":\"Visual Attention Analysis and Prediction on Human Faces for Children with Autism Spectrum Disorder\",\"url\":\"https://www.semanticscholar.org/paper/e01f0475ac9a96da307b2b10778ab6c9283381fb\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152229721\",\"name\":\"Olivier Le Meur\"},{\"authorId\":\"1395583740\",\"name\":\"Alexis Nebout\"},{\"authorId\":\"147436675\",\"name\":\"Myriam Ch\\u00e9rel\"},{\"authorId\":\"15190278\",\"name\":\"E. Etchamendy\"}],\"doi\":\"10.1109/ACCESS.2020.3020251\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4176f0c6f7b3b079c3a2a25efe9efdf143ca4f48\",\"title\":\"From Kanner Austim to Asperger Syndromes, the Difficult Task to Predict Where ASD People Look at\",\"url\":\"https://www.semanticscholar.org/paper/4176f0c6f7b3b079c3a2a25efe9efdf143ca4f48\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150171916\",\"name\":\"Katsuya Fujii\"},{\"authorId\":\"150083098\",\"name\":\"Daisuke Sugimura\"},{\"authorId\":\"34684870\",\"name\":\"T. Hamamoto\"}],\"doi\":\"10.1109/FG.2019.8756573\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5468bc23c087f504000908f52eea085742604743\",\"title\":\"Hierarchical Group-level Emotion Recognition in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/5468bc23c087f504000908f52eea085742604743\",\"venue\":\"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102300440\",\"name\":\"Jakob Wiesinger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"90d8bec9414f1c9dbca3be1ef9ff0fd0cd7b4f36\",\"title\":\"Video Saliency Detection Using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/90d8bec9414f1c9dbca3be1ef9ff0fd0cd7b4f36\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1482544048\",\"name\":\"Camilo Fosco\"},{\"authorId\":\"117232498\",\"name\":\"Anelise Newman\"},{\"authorId\":\"1482544051\",\"name\":\"Pat Sukhum\"},{\"authorId\":\"2204049\",\"name\":\"Y. Zhang\"},{\"authorId\":\"51150125\",\"name\":\"Nanxuan Zhao\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"1618896088\",\"name\":\"Hong Kong\"}],\"doi\":\"10.1109/cvpr42600.2020.00453\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cffc481c86d46ca812eff85030f812588bb20b80\",\"title\":\"How Much Time Do You Have? Modeling Multi-Duration Saliency\",\"url\":\"https://www.semanticscholar.org/paper/cffc481c86d46ca812eff85030f812588bb20b80\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2008.11151\",\"authors\":[{\"authorId\":\"3330863\",\"name\":\"F. Hu\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"205d3644e73c1e5cbce9ce08ea94bbfbc6c13a91\",\"title\":\"FastSal: a Computationally Efficient Network for Visual Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/205d3644e73c1e5cbce9ce08ea94bbfbc6c13a91\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67321830\",\"name\":\"Weijie Wei\"},{\"authorId\":\"3211706\",\"name\":\"Zhusong Liu\"},{\"authorId\":\"47033364\",\"name\":\"Lijin Huang\"},{\"authorId\":\"1395583740\",\"name\":\"Alexis Nebout\"},{\"authorId\":\"1789744\",\"name\":\"O. Meur\"},{\"authorId\":\"153913573\",\"name\":\"T. Zhang\"},{\"authorId\":\"7897836\",\"name\":\"J. Wang\"},{\"authorId\":\"119556705\",\"name\":\"Li-hua Xu\"}],\"doi\":\"10.1016/j.neucom.2020.06.125\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce2cf6e39d5c03d7d224eda614ad803a1347d145\",\"title\":\"Predicting atypical visual saliency for autism spectrum disorder via scale-adaptive inception module and discriminative region enhancement loss\",\"url\":\"https://www.semanticscholar.org/paper/ce2cf6e39d5c03d7d224eda614ad803a1347d145\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1808.04610\",\"authors\":[{\"authorId\":\"49218864\",\"name\":\"Abhinav Shukla\"},{\"authorId\":\"2478739\",\"name\":\"H. Katti\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"},{\"authorId\":\"1742936\",\"name\":\"S. Ramanathan\"}],\"doi\":\"10.1145/3242969.3242988\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f05ea1ba8179595c8540bd26d5bc8f6efff78c82\",\"title\":\"Looking Beyond a Clever Narrative: Visual Context and Attention are Primary Drivers of Affect in Video Advertisements\",\"url\":\"https://www.semanticscholar.org/paper/f05ea1ba8179595c8540bd26d5bc8f6efff78c82\",\"venue\":\"ICMI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48297070\",\"name\":\"Tetsuya Asakawa\"},{\"authorId\":\"96853476\",\"name\":\"Masaki Aono\"}],\"doi\":\"10.1109/APSIPAASC47483.2019.9023303\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0da8eb8bbb0e70d77957ebe8598986543094c7e\",\"title\":\"Median based Multi-label Prediction by Inflating Emotions with Dyads for Visual Sentiment Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e0da8eb8bbb0e70d77957ebe8598986543094c7e\",\"venue\":\"2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"34861255\",\"name\":\"Bryan L. Koenig\"},{\"authorId\":\"153386875\",\"name\":\"Q. Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1007/s42979-019-0061-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"be2e75a806c39bd6d4b8c3657097298e2cbac692\",\"title\":\"A Deeper Look at Human Visual Perception of Images\",\"url\":\"https://www.semanticscholar.org/paper/be2e75a806c39bd6d4b8c3657097298e2cbac692\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52164159\",\"name\":\"Yuya Moroto\"},{\"authorId\":\"47761580\",\"name\":\"K. Maeda\"},{\"authorId\":\"5057217\",\"name\":\"Takahiro Ogawa\"},{\"authorId\":\"144029207\",\"name\":\"M. Haseyama\"}],\"doi\":\"10.1109/GCCE50665.2020.9292048\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e92f32635b312c443faffe8a42009e7d9183c41\",\"title\":\"Estimation of User-Specific Visual Attention Considering Individual Tendency toward Gazed Objects\",\"url\":\"https://www.semanticscholar.org/paper/2e92f32635b312c443faffe8a42009e7d9183c41\",\"venue\":\"2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)\",\"year\":2020}],\"corpusId\":49318337,\"doi\":\"10.1109/CVPR.2018.00785\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":4,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"c46d02e64fbd70080c2f6ab6cc96c9c36f3107f1\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"},{\"authorId\":\"145624227\",\"name\":\"C. Koch\"}],\"doi\":\"10.1016/j.sigpro.2012.06.014\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"38652d59b73992f33ea21c7a2e54e9fd6c37389c\",\"title\":\"Learning saliency-based visual attention: A review\",\"url\":\"https://www.semanticscholar.org/paper/38652d59b73992f33ea21c7a2e54e9fd6c37389c\",\"venue\":\"Signal Process.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701293\",\"name\":\"J. Zhang\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"}],\"doi\":\"10.1109/ICCV.2013.26\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"75ee5cd7712b6ad13d64c3b4de24aa592a869fc1\",\"title\":\"Saliency Detection: A Boolean Map Approach\",\"url\":\"https://www.semanticscholar.org/paper/75ee5cd7712b6ad13d64c3b4de24aa592a869fc1\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145131953\",\"name\":\"L. Wang\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"},{\"authorId\":\"144526777\",\"name\":\"X. Ruan\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":\"10.1109/CVPR.2015.7298938\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"338e14e0c5e368efc79361adc5ba4957d1c17ef1\",\"title\":\"Deep networks for saliency detection via local estimation and global search\",\"url\":\"https://www.semanticscholar.org/paper/338e14e0c5e368efc79361adc5ba4957d1c17ef1\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1606.05814\",\"authors\":[{\"authorId\":\"34987921\",\"name\":\"Kyle Krafka\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"1712418\",\"name\":\"Petr Kellnhofer\"},{\"authorId\":\"143862402\",\"name\":\"Harini Kannan\"},{\"authorId\":\"3422895\",\"name\":\"S. Bhandarkar\"},{\"authorId\":\"1752521\",\"name\":\"W. Matusik\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2016.239\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0695751eb18cd138d7d9441378739882a8afc919\",\"title\":\"Eye Tracking for Everyone\",\"url\":\"https://www.semanticscholar.org/paper/0695751eb18cd138d7d9441378739882a8afc919\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27202682\",\"name\":\"Akanksha Das\"},{\"authorId\":\"25049311\",\"name\":\"R. Kumar\"},{\"authorId\":\"1810015\",\"name\":\"Dakshina Ranjan Kisku\"},{\"authorId\":\"2490911\",\"name\":\"G. Sanyal\"}],\"doi\":\"10.1145/2908446.2908487\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20c5708b5d60cf81d01c62a46dc10e9b7c1b7623\",\"title\":\"Attention Identification via Relative Saliency of Localized Crowd Faces\",\"url\":\"https://www.semanticscholar.org/paper/20c5708b5d60cf81d01c62a46dc10e9b7c1b7623\",\"venue\":\"INFOS '16\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144114624\",\"name\":\"Juan Xu\"},{\"authorId\":\"144889908\",\"name\":\"M. Jiang\"},{\"authorId\":\"40440632\",\"name\":\"Shuo Wang\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"},{\"authorId\":\"51027614\",\"name\":\"Q. Zhao\"}],\"doi\":\"10.1167/14.1.28\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"709e8791193cc3fe9fef9ad983afcd2891e6c680\",\"title\":\"Predicting human gaze beyond pixels.\",\"url\":\"https://www.semanticscholar.org/paper/709e8791193cc3fe9fef9ad983afcd2891e6c680\",\"venue\":\"Journal of vision\",\"year\":2014},{\"arxivId\":\"1312.6034\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71\",\"title\":\"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps\",\"url\":\"https://www.semanticscholar.org/paper/dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3139731\",\"name\":\"J. Machajdik\"},{\"authorId\":\"1699657\",\"name\":\"A. Hanbury\"}],\"doi\":\"10.1145/1873951.1873965\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c411a12f33f15451e1659a3435391962c0cc144\",\"title\":\"Affective image classification using features inspired by psychology and art theory\",\"url\":\"https://www.semanticscholar.org/paper/2c411a12f33f15451e1659a3435391962c0cc144\",\"venue\":\"ACM Multimedia\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32328080\",\"name\":\"J. M. Joyce\"}],\"doi\":\"10.1007/978-3-642-04898-2_327\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c7ab18a86779b2bfbb6c1fc86e9948acfb428d4\",\"title\":\"Kullback-Leibler Divergence\",\"url\":\"https://www.semanticscholar.org/paper/3c7ab18a86779b2bfbb6c1fc86e9948acfb428d4\",\"venue\":\"International Encyclopedia of Statistical Science\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38497468\",\"name\":\"M. Cerf\"},{\"authorId\":\"2356306\",\"name\":\"E. P. Frady\"},{\"authorId\":\"145624227\",\"name\":\"C. Koch\"}],\"doi\":\"10.1167/9.12.10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"14d146b099b12af9b6d7c13342d9ca9db9984275\",\"title\":\"Faces and text attract gaze independent of the task: Experimental data and computer model.\",\"url\":\"https://www.semanticscholar.org/paper/14d146b099b12af9b6d7c13342d9ca9db9984275\",\"venue\":\"Journal of vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2152501\",\"name\":\"P. Vuilleumier\"}],\"doi\":\"10.1016/j.tics.2005.10.011\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"efdcc6a400bab0a2eca571b5d431a38ee3d7d247\",\"title\":\"How brains beware: neural mechanisms of emotional attention\",\"url\":\"https://www.semanticscholar.org/paper/efdcc6a400bab0a2eca571b5d431a38ee3d7d247\",\"venue\":\"Trends in Cognitive Sciences\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145948905\",\"name\":\"R. Gupta\"}],\"doi\":\"10.3389/fpsyg.2016.01613\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"500d0c580012c8fdcc1de29fb51a79dbf987f563\",\"title\":\"Commentary: Neural Control of Vascular Reactions: Impact of Emotion and Attention\",\"url\":\"https://www.semanticscholar.org/paper/500d0c580012c8fdcc1de29fb51a79dbf987f563\",\"venue\":\"Front. Psychol.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2738838\",\"name\":\"R. Marois\"},{\"authorId\":\"49313374\",\"name\":\"Jason Ivanoff\"}],\"doi\":\"10.1016/j.tics.2005.04.010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d6477d6e12f30968993f063f41709c9dde7c28e\",\"title\":\"Capacity limits of information processing in the brain\",\"url\":\"https://www.semanticscholar.org/paper/5d6477d6e12f30968993f063f41709c9dde7c28e\",\"venue\":\"Trends in Cognitive Sciences\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2608733\",\"name\":\"Stas Goferman\"},{\"authorId\":\"1398327241\",\"name\":\"L. Zelnik-Manor\"},{\"authorId\":\"49653522\",\"name\":\"A. Tal\"}],\"doi\":\"10.1109/TPAMI.2011.272\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fbe18b90b9e3031bfb7044226882cfc763e0a1ee\",\"title\":\"Context-Aware Saliency Detection\",\"url\":\"https://www.semanticscholar.org/paper/fbe18b90b9e3031bfb7044226882cfc763e0a1ee\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Z. Bylinskii\"},{\"authorId\":null,\"name\":\"T. Judd\"},{\"authorId\":null,\"name\":\"A. Oliva\"},{\"authorId\":null,\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and F\",\"url\":\"\",\"venue\":\"Durand. What do different evaluation metrics tell us about saliency models? arXiv preprint arXiv:1604.03605\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Buhrmester\"},{\"authorId\":null,\"name\":\"T. Kwang\"},{\"authorId\":null,\"name\":\"S. D. Gosling\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Amazon\\u2019s mechanical turk a new source of inexpensive, yet high-quality, data\",\"url\":\"\",\"venue\":\"Perspectives on psychological science,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2007419\",\"name\":\"M. Bradley\"},{\"authorId\":\"143853826\",\"name\":\"P. Lang\"}],\"doi\":\"10.1016/0005-7916(94)90063-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9576d1573688a05e45ad30e783e7d0c10a10d6d9\",\"title\":\"Measuring emotion: the Self-Assessment Manikin and the Semantic Differential.\",\"url\":\"https://www.semanticscholar.org/paper/9576d1573688a05e45ad30e783e7d0c10a10d6d9\",\"venue\":\"Journal of behavior therapy and experimental psychiatry\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1719780\",\"name\":\"Yan Ke\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"144745185\",\"name\":\"Feng Jing\"}],\"doi\":\"10.1109/CVPR.2006.303\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6989f866a445bb880ed8663a05dc4cd71c87b1b7\",\"title\":\"The Design of High-Level Features for Photo Quality Assessment\",\"url\":\"https://www.semanticscholar.org/paper/6989f866a445bb880ed8663a05dc4cd71c87b1b7\",\"venue\":\"2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"2475944\",\"name\":\"Tian-Tsong Ng\"},{\"authorId\":\"34861255\",\"name\":\"Bryan L. Koenig\"},{\"authorId\":\"144645142\",\"name\":\"M. Jiang\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"}],\"doi\":\"10.1109/CVPR.2016.621\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a826e747e7baa478b228487883d769aaf88137c\",\"title\":\"A Paradigm for Building Generalized Models of Human Image Perception through Data Fusion\",\"url\":\"https://www.semanticscholar.org/paper/0a826e747e7baa478b228487883d769aaf88137c\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38163342\",\"name\":\"Merrielle Spain\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"}],\"doi\":\"10.1007/978-3-540-88682-2_40\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"554bf486046139b67881a23e5011fa2fcba85013\",\"title\":\"Some Objects Are More Equal Than Others: Measuring and Predicting Importance\",\"url\":\"https://www.semanticscholar.org/paper/554bf486046139b67881a23e5011fa2fcba85013\",\"venue\":\"ECCV\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1789744\",\"name\":\"O. Meur\"},{\"authorId\":\"1776651\",\"name\":\"P. Callet\"},{\"authorId\":\"144812989\",\"name\":\"D. Barba\"}],\"doi\":\"10.1016/j.visres.2007.06.015\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"122134f9242785383949caaaea4601861beebad8\",\"title\":\"Predicting visual fixations on video based on low-level visual features\",\"url\":\"https://www.semanticscholar.org/paper/122134f9242785383949caaaea4601861beebad8\",\"venue\":\"Vision Research\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1789744\",\"name\":\"O. Meur\"},{\"authorId\":\"1731790\",\"name\":\"T. Baccino\"}],\"doi\":\"10.3758/s13428-012-0226-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a98a0079fef4c434e5e1f775ce47f8151149bf47\",\"title\":\"Methods for comparing scanpaths and saliency maps: strengths and weaknesses\",\"url\":\"https://www.semanticscholar.org/paper/a98a0079fef4c434e5e1f775ce47f8151149bf47\",\"venue\":\"Behavior research methods\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7021441\",\"name\":\"Joseph A. Mikels\"},{\"authorId\":\"1892780\",\"name\":\"B. Fredrickson\"},{\"authorId\":\"153818150\",\"name\":\"G. R. Larkin\"},{\"authorId\":\"32035663\",\"name\":\"Casey M Lindberg\"},{\"authorId\":\"1860459\",\"name\":\"Sam J Maglio\"},{\"authorId\":\"1398013980\",\"name\":\"P. Reuter-Lorenz\"}],\"doi\":\"10.3758/BF03192732\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"11036a318e450effc32e6c0db5032cd8fd6a0674\",\"title\":\"Emotional category data on images from the international affective picture system\",\"url\":\"https://www.semanticscholar.org/paper/11036a318e450effc32e6c0db5032cd8fd6a0674\",\"venue\":\"Behavior research methods\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144645142\",\"name\":\"M. Jiang\"},{\"authorId\":\"1961257\",\"name\":\"Shengsheng Huang\"},{\"authorId\":\"2104164\",\"name\":\"Juanyong Duan\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"}],\"doi\":\"10.1109/CVPR.2015.7298710\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c71db5d3546e22227662ee0f0ce586495ef18899\",\"title\":\"SALICON: Saliency in Context\",\"url\":\"https://www.semanticscholar.org/paper/c71db5d3546e22227662ee0f0ce586495ef18899\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1606.00110\",\"authors\":[{\"authorId\":\"51285293\",\"name\":\"Christopher Thomas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"36541e5dcc7964f80bd622e6212fa5808730fe95\",\"title\":\"OpenSalicon: An Open Source Implementation of the Salicon Saliency Model\",\"url\":\"https://www.semanticscholar.org/paper/36541e5dcc7964f80bd622e6212fa5808730fe95\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47149107\",\"name\":\"Laura Chamberlain\"}],\"doi\":\"10.1108/13522750710740862\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f090e349f2e20f733baf5c88492c9b70e45141f0\",\"title\":\"Eye Tracking Methodology; Theory and Practice\",\"url\":\"https://www.semanticscholar.org/paper/f090e349f2e20f733baf5c88492c9b70e45141f0\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1630277553\",\"name\":\"M. Sankar\"}],\"doi\":\"10.1515/9783111548050-024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"517a45c35f9ad52ac75a27c5a5f190d41cbc5e65\",\"title\":\"M\",\"url\":\"https://www.semanticscholar.org/paper/517a45c35f9ad52ac75a27c5a5f190d41cbc5e65\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47138825\",\"name\":\"R. A. Bailey\"}],\"doi\":\"10.5860/choice.46-3308\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b92595be69798ece3213019a715f9517e91b18c2\",\"title\":\"Design of comparative experiments\",\"url\":\"https://www.semanticscholar.org/paper/b92595be69798ece3213019a715f9517e91b18c2\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34931109\",\"name\":\"R. Baddeley\"},{\"authorId\":\"3369864\",\"name\":\"B. Tatler\"}],\"doi\":\"10.1016/j.visres.2006.02.024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7432e012c196a63fc4e21d66b7f16db7fed2a3b1\",\"title\":\"High frequency edges (but not contrast) predict where we fixate: A Bayesian system identification analysis\",\"url\":\"https://www.semanticscholar.org/paper/7432e012c196a63fc4e21d66b7f16db7fed2a3b1\",\"venue\":\"Vision Research\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Q. Zhao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Salicon : Saliency in context Kullback - leibler divergence\",\"url\":\"\",\"venue\":\"International Encyclopedia of Statistical Science\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742936\",\"name\":\"S. Ramanathan\"},{\"authorId\":\"2478739\",\"name\":\"H. Katti\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1007/978-3-642-15561-1_3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ec80f411ef1c08eae586047d97977f9ef59dd6f5\",\"title\":\"An Eye Fixation Database for Saliency Detection in Images\",\"url\":\"https://www.semanticscholar.org/paper/ec80f411ef1c08eae586047d97977f9ef59dd6f5\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":\"1611.09571\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/TIP.2018.2851672\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7e336a7d3cd82ae8cd5e85d3cb62f0b6091a0e5\",\"title\":\"Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model\",\"url\":\"https://www.semanticscholar.org/paper/b7e336a7d3cd82ae8cd5e85d3cb62f0b6091a0e5\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2987811\",\"name\":\"M. Swain\"},{\"authorId\":\"1691804\",\"name\":\"D. Ballard\"}],\"doi\":\"10.1007/BF00130487\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b1e1696564e5a3021ac3a501c9deeb6c0fbc637\",\"title\":\"Color indexing\",\"url\":\"https://www.semanticscholar.org/paper/5b1e1696564e5a3021ac3a501c9deeb6c0fbc637\",\"venue\":\"International Journal of Computer Vision\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7326223\",\"name\":\"L. Itti\"},{\"authorId\":\"145624227\",\"name\":\"C. Koch\"},{\"authorId\":\"3271571\",\"name\":\"E. Niebur\"}],\"doi\":\"10.1109/34.730558\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4816f0b6f0d05da3901441bfa5cc7be044b4da8b\",\"title\":\"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis\",\"url\":\"https://www.semanticscholar.org/paper/4816f0b6f0d05da3901441bfa5cc7be044b4da8b\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2009},{\"arxivId\":\"1702.05150\",\"authors\":[{\"authorId\":\"2201320\",\"name\":\"N. W. Kim\"},{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"2203252\",\"name\":\"M. Borkin\"},{\"authorId\":\"1770992\",\"name\":\"Krzysztof Z Gajos\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"},{\"authorId\":\"143758231\",\"name\":\"H. Pfister\"}],\"doi\":\"10.1145/3131275\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc2e0b445dc1825c634768517de557ba4bf22e81\",\"title\":\"BubbleView\",\"url\":\"https://www.semanticscholar.org/paper/fc2e0b445dc1825c634768517de557ba4bf22e81\",\"venue\":\"ACM Trans. Comput. Hum. Interact.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24320514\",\"name\":\"Ronak Kosti\"},{\"authorId\":\"2974008\",\"name\":\"Jose M. Alvarez\"},{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"2677488\",\"name\":\"\\u00c0gata Lapedriza\"}],\"doi\":\"10.1109/CVPR.2017.212\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"382e9f1ed5e3e0e7dd5b1f27cdd14e4202044f9b\",\"title\":\"Emotion Recognition in Context\",\"url\":\"https://www.semanticscholar.org/paper/382e9f1ed5e3e0e7dd5b1f27cdd14e4202044f9b\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2256291\",\"name\":\"T. Brosch\"},{\"authorId\":\"3022479\",\"name\":\"G. Pourtois\"},{\"authorId\":\"143868107\",\"name\":\"D. Sander\"}],\"doi\":\"10.1080/02699930902975754\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"626c33ab9fef17c4089e05fe7074dea3236c6c38\",\"title\":\"The perception and categorisation of emotional stimuli: A review\",\"url\":\"https://www.semanticscholar.org/paper/626c33ab9fef17c4089e05fe7074dea3236c6c38\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1799204\",\"name\":\"R. Compton\"}],\"doi\":\"10.1177/1534582303002002003\",\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"9307c118725b3125b1ae491d96c94426a20919b7\",\"title\":\"The interface between emotion and attention: a review of evidence from psychology and neuroscience.\",\"url\":\"https://www.semanticscholar.org/paper/9307c118725b3125b1ae491d96c94426a20919b7\",\"venue\":\"Behavioral and cognitive neuroscience reviews\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21451088\",\"name\":\"P. Ekman\"}],\"doi\":\"10.1080/02699939208411068\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bfdbfe3bf703594b884ae69f505f94ce7e98141e\",\"title\":\"An argument for basic emotions\",\"url\":\"https://www.semanticscholar.org/paper/bfdbfe3bf703594b884ae69f505f94ce7e98141e\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2286630\",\"name\":\"E. Vig\"},{\"authorId\":\"1944405\",\"name\":\"M. Dorr\"},{\"authorId\":\"2042941\",\"name\":\"D. Cox\"}],\"doi\":\"10.1109/CVPR.2014.358\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8693c32dff29e851760fa0b6af464050ffc383d6\",\"title\":\"Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images\",\"url\":\"https://www.semanticscholar.org/paper/8693c32dff29e851760fa0b6af464050ffc383d6\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143800213\",\"name\":\"Charless C. Fowlkes\"},{\"authorId\":\"144891282\",\"name\":\"D. Martin\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1167/7.8.2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"532a5d06938907150adc67ce9894212070417ab4\",\"title\":\"Local figure-ground cues are valid for natural images.\",\"url\":\"https://www.semanticscholar.org/paper/532a5d06938907150adc67ce9894212070417ab4\",\"venue\":\"Journal of vision\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1917767\",\"name\":\"W. Einh\\u00e4user\"},{\"authorId\":\"38163342\",\"name\":\"Merrielle Spain\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"}],\"doi\":\"10.1167/8.14.18\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c032ef5e4ce9536de5669bbe487e1f618d3a12a6\",\"title\":\"Objects predict fixations better than early saliency.\",\"url\":\"https://www.semanticscholar.org/paper/c032ef5e4ce9536de5669bbe487e1f618d3a12a6\",\"venue\":\"Journal of vision\",\"year\":2008},{\"arxivId\":\"1701.01081\",\"authors\":[{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"1399086207\",\"name\":\"C. Canton-Ferrer\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"147166602\",\"name\":\"J. Torres\"},{\"authorId\":\"2470219\",\"name\":\"E. Sayrol\"},{\"authorId\":\"3100480\",\"name\":\"Xavier Giro-i-Nieto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"258fad95e709b6d0572ae6cc99efbbb14d32bdf2\",\"title\":\"SalGAN: Visual Saliency Prediction with Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/258fad95e709b6d0572ae6cc99efbbb14d32bdf2\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1604.03605\",\"authors\":[{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"152627906\",\"name\":\"T. Judd\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"}],\"doi\":\"10.1109/TPAMI.2018.2815601\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4\",\"title\":\"What Do Different Evaluation Metrics Tell Us About Saliency Models?\",\"url\":\"https://www.semanticscholar.org/paper/bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1610.01563\",\"authors\":[{\"authorId\":\"2997408\",\"name\":\"Matthias K\\u00fcmmerer\"},{\"authorId\":\"49737748\",\"name\":\"Thomas S. A. Wallis\"},{\"authorId\":\"1731199\",\"name\":\"M. Bethge\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d160d375a7c187dfe10c110e9e733f36accb87e6\",\"title\":\"DeepGaze II: Reading fixations from deep features trained on object recognition\",\"url\":\"https://www.semanticscholar.org/paper/d160d375a7c187dfe10c110e9e733f36accb87e6\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33182815\",\"name\":\"D. Hansen\"},{\"authorId\":\"50426357\",\"name\":\"Q. Ji\"}],\"doi\":\"10.1109/TPAMI.2009.30\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79ced39c474df5d944323a394dc0d5923c14b410\",\"title\":\"In the Eye of the Beholder: A Survey of Models for Eyes and Gaze\",\"url\":\"https://www.semanticscholar.org/paper/79ced39c474df5d944323a394dc0d5923c14b410\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Buhrmester\"},{\"authorId\":null,\"name\":\"T. Kwang\"},{\"authorId\":null,\"name\":\"S. D. Gosling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Amazon\\u2019s mechanical turk a new source of inexpensive\",\"url\":\"\",\"venue\":\"yet high-quality, data? Perspectives on psychological science, 6(1):3\\u20135\",\"year\":2011},{\"arxivId\":\"1505.03581\",\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"519feb1f3c23baea6960dfa204521f96a74b82bb\",\"title\":\"CAT2000: A Large Scale Fixation Dataset for Boosting Saliency Research\",\"url\":\"https://www.semanticscholar.org/paper/519feb1f3c23baea6960dfa204521f96a74b82bb\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144147221\",\"name\":\"Jianping Fan\"},{\"authorId\":\"2026151\",\"name\":\"Yuli Gao\"},{\"authorId\":\"1704919\",\"name\":\"H. Luo\"},{\"authorId\":\"1797002\",\"name\":\"G. Xu\"}],\"doi\":\"10.1145/1008992.1009055\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"909f91e1052279543d2e5b3164027d91d1db74be\",\"title\":\"Automatic image annotation by using concept-sensitive salient objects for image content representation\",\"url\":\"https://www.semanticscholar.org/paper/909f91e1052279543d2e5b3164027d91d1db74be\",\"venue\":\"SIGIR '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"49312774\",\"name\":\"K. Murphy\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1007/s11263-007-0090-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"092c275005ae49dc1303214f6d02d134457c7053\",\"title\":\"LabelMe: A Database and Web-Based Tool for Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/092c275005ae49dc1303214f6d02d134457c7053\",\"venue\":\"International Journal of Computer Vision\",\"year\":2007},{\"arxivId\":\"1510.02927\",\"authors\":[{\"authorId\":\"1784761\",\"name\":\"S. Kruthiventi\"},{\"authorId\":\"50430041\",\"name\":\"Kumar Ayush\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1109/TIP.2017.2710620\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1295cbaf3b03de2eb8c79530289f5939d7819e5c\",\"title\":\"DeepFix: A Fully Convolutional Neural Network for Predicting Human Eye Fixations\",\"url\":\"https://www.semanticscholar.org/paper/1295cbaf3b03de2eb8c79530289f5939d7819e5c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144866673\",\"name\":\"M. Calvo\"},{\"authorId\":\"143853826\",\"name\":\"P. Lang\"}],\"doi\":\"10.1023/B:MOEM.0000040153.26156.ED\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ee2514a4932d8948cbd99a11f337ed9668cdff3\",\"title\":\"Gaze Patterns When Looking at Emotional Pictures: Motivationally Biased Attention\",\"url\":\"https://www.semanticscholar.org/paper/3ee2514a4932d8948cbd99a11f337ed9668cdff3\",\"venue\":\"\",\"year\":2004},{\"arxivId\":\"1609.01064\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/ICPR.2016.7900174\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"57e143c96a41ba81d9d59120a1cd0dc04905d3f1\",\"title\":\"A deep multi-level network for saliency prediction\",\"url\":\"https://www.semanticscholar.org/paper/57e143c96a41ba81d9d59120a1cd0dc04905d3f1\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39810944\",\"name\":\"Jonathan Harel\"},{\"authorId\":\"145624227\",\"name\":\"C. Koch\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"}],\"doi\":\"10.7551/mitpress/7503.003.0073\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f412bb31ec9ef8bbef70eefc7ffd04420c1365d9\",\"title\":\"Graph-Based Visual Saliency\",\"url\":\"https://www.semanticscholar.org/paper/f412bb31ec9ef8bbef70eefc7ffd04420c1365d9\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"}],\"doi\":\"10.1007/978-3-319-46454-1_49\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6d6d0de32939a5d15d2abfb04d131884d2cadc4\",\"title\":\"Where Should Saliency Models Look Next?\",\"url\":\"https://www.semanticscholar.org/paper/a6d6d0de32939a5d15d2abfb04d131884d2cadc4\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40257358\",\"name\":\"He Tang\"},{\"authorId\":\"2500502\",\"name\":\"Chuanbo Chen\"},{\"authorId\":\"31564288\",\"name\":\"Xiaobing Pei\"}],\"doi\":\"10.1109/LSP.2016.2617340\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f09e22ab4ff7dfdca3a3843a1c0bb28c38a3b71\",\"title\":\"Visual Saliency Detection via Sparse Residual and Outlier Detection\",\"url\":\"https://www.semanticscholar.org/paper/1f09e22ab4ff7dfdca3a3843a1c0bb28c38a3b71\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144247007\",\"name\":\"X. Huang\"},{\"authorId\":\"3329744\",\"name\":\"Chengyao Shen\"},{\"authorId\":\"2343486\",\"name\":\"X. Boix\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"}],\"doi\":\"10.1109/ICCV.2015.38\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1281e443d2cf1c1dd71ed3b7b0376d408d0958af\",\"title\":\"SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/1281e443d2cf1c1dd71ed3b7b0376d408d0958af\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36424125\",\"name\":\"J. Fawcett\"},{\"authorId\":\"46805442\",\"name\":\"E. J. Russell\"},{\"authorId\":\"40000536\",\"name\":\"Kristine A. Peace\"},{\"authorId\":\"144614087\",\"name\":\"J. Christie\"}],\"doi\":\"10.1080/1068316X.2011.599325\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ecdbca104241b2f5dc46c7b8149d8c0ad0c87f4\",\"title\":\"Of guns and geese: a meta-analytic review of the \\u2018weapon focus\\u2019 literature\",\"url\":\"https://www.semanticscholar.org/paper/2ecdbca104241b2f5dc46c7b8149d8c0ad0c87f4\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40490812\",\"name\":\"R. Datta\"},{\"authorId\":\"40116905\",\"name\":\"Jia Li\"},{\"authorId\":\"48094094\",\"name\":\"James Ze Wang\"}],\"doi\":\"10.1109/ICIP.2008.4711702\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d55666687d3ef9f8306f1b53de43b63b15ca4b7\",\"title\":\"Algorithmic inferencing of aesthetics and emotion in natural images: An exposition\",\"url\":\"https://www.semanticscholar.org/paper/4d55666687d3ef9f8306f1b53de43b63b15ca4b7\",\"venue\":\"2008 15th IEEE International Conference on Image Processing\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47173549\",\"name\":\"Michael D. Buhrmester\"},{\"authorId\":\"6369977\",\"name\":\"T. Kwang\"},{\"authorId\":\"2705485\",\"name\":\"S. Gosling\"}],\"doi\":\"10.1177/1745691610393980\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce10854b47b1294e61e4b055a6c9a90a49ff3208\",\"title\":\"Amazon's Mechanical Turk\",\"url\":\"https://www.semanticscholar.org/paper/ce10854b47b1294e61e4b055a6c9a90a49ff3208\",\"venue\":\"Perspectives on psychological science : a journal of the Association for Psychological Science\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1719250\",\"name\":\"Xiaochun Cao\"},{\"authorId\":\"144704038\",\"name\":\"Changqing Zhang\"},{\"authorId\":\"1929093\",\"name\":\"Huazhu Fu\"},{\"authorId\":\"33465926\",\"name\":\"Xiaojie Guo\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TNNLS.2015.2488637\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3fbff11afa17f4c6f13f52a944ff9588be52ae41\",\"title\":\"Saliency-Aware Nonparametric Foreground Annotation Based on Weakly Labeled Data\",\"url\":\"https://www.semanticscholar.org/paper/3fbff11afa17f4c6f13f52a944ff9588be52ae41\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Z. Bylinskii\"},{\"authorId\":null,\"name\":\"M. A. Borkin\"},{\"authorId\":null,\"name\":\"K. Z. Gajos\"},{\"authorId\":null,\"name\":\"A. Oliva\"},{\"authorId\":null,\"name\":\"F. Durand\"},{\"authorId\":null,\"name\":\"H. Pfister\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Salicon : Saliency in context Kullback - leibler divergence\",\"url\":\"\",\"venue\":\"International Encyclopedia of Statistical Science\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"F. Chollet\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Keras\",\"url\":\"\",\"venue\":\"GitHub repository\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152172815\",\"name\":\"P. Lang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"788e2f5a24784ce952eec8a57902a6f03cd9318c\",\"title\":\"International affective picture system (IAPS) : affective ratings of pictures and instruction manual\",\"url\":\"https://www.semanticscholar.org/paper/788e2f5a24784ce952eec8a57902a6f03cd9318c\",\"venue\":\"\",\"year\":2005},{\"arxivId\":\"1501.02741\",\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"49712326\",\"name\":\"M. Cheng\"},{\"authorId\":\"40175280\",\"name\":\"Huaizu Jiang\"},{\"authorId\":\"97483166\",\"name\":\"J. Li\"}],\"doi\":\"10.1109/TIP.2015.2487833\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1260c42b86dcbe123ccc038857cd3b14e146032\",\"title\":\"Salient Object Detection: A Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/a1260c42b86dcbe123ccc038857cd3b14e146032\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Z. Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Amazon \\u2019 s mechanical turk a new source of inexpensive , yet high - quality , data ?\",\"url\":\"\",\"venue\":\"Perspectives on psychological science\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4078474\",\"name\":\"A. Ohman\"},{\"authorId\":\"4916712\",\"name\":\"A. Flykt\"},{\"authorId\":\"49277628\",\"name\":\"F. Esteves\"}],\"doi\":\"10.1037/0096-3445.130.3.466\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e35f63ac711e7253049a4811a354b93478d4959e\",\"title\":\"Emotion drives attention: detecting the snake in the grass.\",\"url\":\"https://www.semanticscholar.org/paper/e35f63ac711e7253049a4811a354b93478d4959e\",\"venue\":\"Journal of experimental psychology. General\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3369864\",\"name\":\"B. Tatler\"},{\"authorId\":\"34931109\",\"name\":\"R. Baddeley\"},{\"authorId\":\"2283822\",\"name\":\"I. Gilchrist\"}],\"doi\":\"10.1016/j.visres.2004.09.017\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ae38ed953333fb39eb671fce0247db65a09b3a80\",\"title\":\"Visual correlates of fixation selection: effects of scale and time\",\"url\":\"https://www.semanticscholar.org/paper/ae38ed953333fb39eb671fce0247db65a09b3a80\",\"venue\":\"Vision Research\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6776399\",\"name\":\"Sonja Engmann\"},{\"authorId\":\"8952593\",\"name\":\"B. M. \\u2019. Hart\"},{\"authorId\":\"148241187\",\"name\":\"Thomas Sieren\"},{\"authorId\":\"2469837\",\"name\":\"Selim Onat\"},{\"authorId\":\"40089171\",\"name\":\"P. K\\u00f6nig\"},{\"authorId\":\"1917767\",\"name\":\"W. Einh\\u00e4user\"}],\"doi\":\"10.3758/APP.71.6.1337\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f34791d07992dd0aa4a3f812e173bee8d0f5f40a\",\"title\":\"Saliency on a natural scene background: Effects of color and luminance contrast add linearly\",\"url\":\"https://www.semanticscholar.org/paper/f34791d07992dd0aa4a3f812e173bee8d0f5f40a\",\"venue\":\"Attention, perception & psychophysics\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40635259\",\"name\":\"D. Rudoy\"},{\"authorId\":\"1976171\",\"name\":\"D. Goldman\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1398327241\",\"name\":\"L. Zelnik-Manor\"}],\"doi\":\"10.1109/CVPR.2013.152\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d7924e07914a147d7f6b868050d4edd093b952d\",\"title\":\"Learning Video Saliency from Human Gaze Using Candidate Selection\",\"url\":\"https://www.semanticscholar.org/paper/3d7924e07914a147d7f6b868050d4edd093b952d\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Breuel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Salient object detection : A benchmark Borji and L . Itti . Cat 2000 : A large scale fixation dataset for boosting saliency research\",\"url\":\"\",\"venue\":\"CVPR 2015 workshop on \\u201c Future Datasets \\u201d\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3180583\",\"name\":\"Victoria Yanulevskaya\"},{\"authorId\":\"1738975\",\"name\":\"J. C. V. Gemert\"},{\"authorId\":\"30405671\",\"name\":\"Katharina Roth\"},{\"authorId\":\"35247697\",\"name\":\"A. Herbold\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"},{\"authorId\":\"1720149\",\"name\":\"Jan-Mark Geusebroek\"}],\"doi\":\"10.1109/ICIP.2008.4711701\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a3f78874e4f548bf719fb78011ebc9774491a498\",\"title\":\"Emotional valence categorization using holistic image features\",\"url\":\"https://www.semanticscholar.org/paper/a3f78874e4f548bf719fb78011ebc9774491a498\",\"venue\":\"2008 15th IEEE International Conference on Image Processing\",\"year\":2008},{\"arxivId\":\"1504.06755\",\"authors\":[{\"authorId\":\"2366042\",\"name\":\"Pingmei Xu\"},{\"authorId\":\"1865091\",\"name\":\"Krista A. Ehinger\"},{\"authorId\":\"2507239\",\"name\":\"Y. Zhang\"},{\"authorId\":\"37737599\",\"name\":\"Adam Finkelstein\"},{\"authorId\":\"1697413\",\"name\":\"S. Kulkarni\"},{\"authorId\":\"40599257\",\"name\":\"J. Xiao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3433627f803953280b66ae1576d083fc9a68385a\",\"title\":\"TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking\",\"url\":\"https://www.semanticscholar.org/paper/3433627f803953280b66ae1576d083fc9a68385a\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1610.01708\",\"authors\":[{\"authorId\":\"145021890\",\"name\":\"N. Liu\"},{\"authorId\":\"7181955\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/TIP.2018.2817047\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40bc3b9cfe74a4f164e53bf2e037e61256f8d354\",\"title\":\"A Deep Spatial Contextual Long-Term Recurrent Convolutional Network for Saliency Detection\",\"url\":\"https://www.semanticscholar.org/paper/40bc3b9cfe74a4f164e53bf2e037e61256f8d354\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Recasens\"},{\"authorId\":null,\"name\":\"A. Borji\"},{\"authorId\":null,\"name\":\"A. Oliva\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Mit saliency benchmark What do different evaluation metrics tell us about saliency models ? arXiv preprint arXiv : 1604\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"2475944\",\"name\":\"Tian-Tsong Ng\"},{\"authorId\":\"2645674\",\"name\":\"Jonathan S. Herberg\"},{\"authorId\":\"34861255\",\"name\":\"Bryan L. Koenig\"},{\"authorId\":\"1694051\",\"name\":\"Cheston Tan\"},{\"authorId\":\"8199233\",\"name\":\"R. Wang\"}],\"doi\":\"10.1109/CVPR.2014.535\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc472eb16f1b8623b4dc9d1468278409b99db0f8\",\"title\":\"An Automated Estimator of Image Visual Realism Based on Human Cognition\",\"url\":\"https://www.semanticscholar.org/paper/bc472eb16f1b8623b4dc9d1468278409b99db0f8\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"}],\"doi\":\"10.1167/12.9.1082\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"028916e62ddc516ff4e64fb0eb1e058eded48fcb\",\"title\":\"Understanding the Intrinsic Memorability of Images\",\"url\":\"https://www.semanticscholar.org/paper/028916e62ddc516ff4e64fb0eb1e058eded48fcb\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1524173324\",\"name\":\"\\u25cf. Pytorch\"}],\"doi\":\"10.1016/S1364-6613(97)82741-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"009e38c1166378ec6a349da0002161769676515b\",\"title\":\"Attention!\",\"url\":\"https://www.semanticscholar.org/paper/009e38c1166378ec6a349da0002161769676515b\",\"venue\":\"Trends in Cognitive Sciences\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24492532\",\"name\":\"Jian-xiong Xiao\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1865091\",\"name\":\"Krista A. Ehinger\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2010.5539970\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"908091b4a8757c3b2f7d9cfa2c4f616ee12c5157\",\"title\":\"SUN database: Large-scale scene recognition from abbey to zoo\",\"url\":\"https://www.semanticscholar.org/paper/908091b4a8757c3b2f7d9cfa2c4f616ee12c5157\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742936\",\"name\":\"S. Ramanathan\"},{\"authorId\":\"2478739\",\"name\":\"H. Katti\"},{\"authorId\":\"46730869\",\"name\":\"R. Huang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1145/1631272.1631399\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19d752477a8d331cce20984efd284bfa0a374ad8\",\"title\":\"Automated localization of affective objects and actions in images via caption text-cum-eye gaze analysis\",\"url\":\"https://www.semanticscholar.org/paper/19d752477a8d331cce20984efd284bfa0a374ad8\",\"venue\":\"MM '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-10593-2_34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b4738809317259d2b49017203da512b21ea51ed\",\"title\":\"Deep Features for Text Spotting\",\"url\":\"https://www.semanticscholar.org/paper/4b4738809317259d2b49017203da512b21ea51ed\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"1775497\",\"name\":\"Mengdi Xu\"},{\"authorId\":\"34646933\",\"name\":\"T. V. Nguyen\"},{\"authorId\":\"47446553\",\"name\":\"M. Wang\"},{\"authorId\":\"3350185\",\"name\":\"C. Lang\"},{\"authorId\":\"1748497\",\"name\":\"Z. Huang\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":\"10.1109/TMM.2014.2329275\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6b70bec7ec4b870b0501e0ff9d6a6430cb254159\",\"title\":\"Touch Saliency: Characteristics and Prediction\",\"url\":\"https://www.semanticscholar.org/paper/6b70bec7ec4b870b0501e0ff9d6a6430cb254159\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1772549\",\"name\":\"D. Borth\"},{\"authorId\":\"145592290\",\"name\":\"R. Ji\"},{\"authorId\":\"143726651\",\"name\":\"Tao Chen\"},{\"authorId\":\"1733858\",\"name\":\"T. Breuel\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/2502081.2502282\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed0ae70172a2c5aca9fa640972c2fa12420f463d\",\"title\":\"Large-scale visual sentiment ontology and detectors using adjective noun pairs\",\"url\":\"https://www.semanticscholar.org/paper/ed0ae70172a2c5aca9fa640972c2fa12420f463d\",\"venue\":\"MM '13\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1617780012\",\"name\":\"Citt\\u00e0 DI Torino\"},{\"authorId\":\"1620911036\",\"name\":\"Determinazione Dirigenziale\"},{\"authorId\":\"1620917356\",\"name\":\"N. Cronologico\"}],\"doi\":\"10.1515/9783111438443-006\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"444537108d9171272bfbb0eccd7a3855ada66b0d\",\"title\":\"N\",\"url\":\"https://www.semanticscholar.org/paper/444537108d9171272bfbb0eccd7a3855ada66b0d\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153010596\",\"name\":\"X. Lu\"},{\"authorId\":\"70116426\",\"name\":\"P. Suryanarayan\"},{\"authorId\":\"48460143\",\"name\":\"R. B. Adams\"},{\"authorId\":\"97483230\",\"name\":\"J. Li\"},{\"authorId\":\"134443957\",\"name\":\"M. G. Newman\"},{\"authorId\":\"46584105\",\"name\":\"J. Z. Wang\"}],\"doi\":\"10.1145/2393347.2393384\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8558e14bc0870fd1b208803b9154546c9042a17a\",\"title\":\"On shape and the computability of emotions\",\"url\":\"https://www.semanticscholar.org/paper/8558e14bc0870fd1b208803b9154546c9042a17a\",\"venue\":\"ACM Multimedia\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143615083\",\"name\":\"R. J. Peters\"},{\"authorId\":\"4292093\",\"name\":\"A. Iyer\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"},{\"authorId\":\"145624227\",\"name\":\"C. Koch\"}],\"doi\":\"10.1016/j.visres.2005.03.019\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a7447f7e6197c0875a28686efdffe4ba4beb5460\",\"title\":\"Components of bottom-up gaze allocation in natural images\",\"url\":\"https://www.semanticscholar.org/paper/a7447f7e6197c0875a28686efdffe4ba4beb5460\",\"venue\":\"Vision Research\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145090451\",\"name\":\"G. Krieger\"},{\"authorId\":\"2580914\",\"name\":\"I. Rentschler\"},{\"authorId\":\"3267783\",\"name\":\"G. Hauske\"},{\"authorId\":\"1755034\",\"name\":\"K. Schill\"},{\"authorId\":\"1683937\",\"name\":\"C. Zetzsche\"}],\"doi\":\"10.1163/156856800741216\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bfeecf1123829293384d5342324bc50943cdd24\",\"title\":\"Object and scene analysis by saccadic eye-movements: an investigation with higher-order statistics.\",\"url\":\"https://www.semanticscholar.org/paper/3bfeecf1123829293384d5342324bc50943cdd24\",\"venue\":\"Spatial vision\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3069252\",\"name\":\"C. Bundesen\"},{\"authorId\":\"3335230\",\"name\":\"S. Vangkilde\"},{\"authorId\":\"143843084\",\"name\":\"A. Petersen\"}],\"doi\":\"10.1016/j.visres.2014.11.005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db5a8ffeb92469a4d4766936478342f469228c90\",\"title\":\"Recent developments in a computational theory of visual attention (TVA)\",\"url\":\"https://www.semanticscholar.org/paper/db5a8ffeb92469a4d4766936478342f469228c90\",\"venue\":\"Vision Research\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2997408\",\"name\":\"Matthias K\\u00fcmmerer\"},{\"authorId\":\"49737748\",\"name\":\"Thomas S. A. Wallis\"},{\"authorId\":\"1731199\",\"name\":\"M. Bethge\"}],\"doi\":\"10.1073/pnas.1510393112\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"01058e4aabd61571da9e43a618834c90aabaf691\",\"title\":\"Information-theoretic model comparison unifies saliency metrics\",\"url\":\"https://www.semanticscholar.org/paper/01058e4aabd61571da9e43a618834c90aabaf691\",\"venue\":\"Proceedings of the National Academy of Sciences\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1829696\",\"name\":\"Y. Rubner\"},{\"authorId\":\"145086151\",\"name\":\"Carlo Tomasi\"},{\"authorId\":\"1744254\",\"name\":\"L. Guibas\"}],\"doi\":\"10.1023/A:1026543900054\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d13a04844e4a781e5180987118f732d93aa9f398\",\"title\":\"The Earth Mover's Distance as a Metric for Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d13a04844e4a781e5180987118f732d93aa9f398\",\"venue\":\"International Journal of Computer Vision\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3461088\",\"name\":\"L. Wang\"},{\"authorId\":\"145131953\",\"name\":\"L. Wang\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"},{\"authorId\":\"48754192\",\"name\":\"Pingping Zhang\"},{\"authorId\":\"144526777\",\"name\":\"X. Ruan\"}],\"doi\":\"10.1007/978-3-319-46493-0_50\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a7a18e1f3dddcc351403237ea5255099441d5d5\",\"title\":\"Saliency Detection with Recurrent Fully Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/7a7a18e1f3dddcc351403237ea5255099441d5d5\",\"venue\":\"ECCV\",\"year\":2016}],\"title\":\"Emotional Attention: A Study of Image Sentiment and Visual Attention\",\"topics\":[{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"},{\"topic\":\"Subnetwork\",\"topicId\":\"36979\",\"url\":\"https://www.semanticscholar.org/topic/36979\"},{\"topic\":\"Eye tracking\",\"topicId\":\"7621\",\"url\":\"https://www.semanticscholar.org/topic/7621\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"}],\"url\":\"https://www.semanticscholar.org/paper/c46d02e64fbd70080c2f6ab6cc96c9c36f3107f1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"