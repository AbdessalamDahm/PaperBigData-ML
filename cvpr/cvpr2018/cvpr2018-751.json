"{\"abstract\":\"Dense video captioning is a newly emerging task that aims at both localizing and describing all events in a video. We identify and tackle two challenges on this task, namely, (1) how to utilize both past and future contexts for accurate event proposal predictions, and (2) how to construct informative input to the decoder for generating natural event descriptions. First, previous works predominantly generate temporal event proposals in the forward direction, which neglects future video context. We propose a bidirectional proposal method that effectively exploits both past and future contexts to make proposal predictions. Second, different events ending at (nearly) the same time are indistinguishable in the previous works, resulting in the same captions. We solve this problem by representing each event with an attentive fusion of hidden states from the proposal module and video contents (e.g., C3D features). We further propose a novel context gating mechanism to balance the contributions from the current event and its surrounding contexts dynamically. We empirically show that our attentively fused event representation is superior to the proposal hidden states or video contents alone. By coupling proposal and captioning modules into one unified framework, our model outperforms the state-of-the-arts on the ActivityNet Captions dataset with a relative gain of over 100% (Meteor score increases from 4.82 to 9.65).\",\"arxivId\":\"1804.00100\",\"authors\":[{\"authorId\":null,\"name\":\"Jingwen Wang\",\"url\":null},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\",\"url\":\"https://www.semanticscholar.org/author/2093119\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\",\"url\":\"https://www.semanticscholar.org/author/145698310\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\",\"url\":\"https://www.semanticscholar.org/author/46641573\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\",\"url\":\"https://www.semanticscholar.org/author/144391096\"}],\"citationVelocity\":25,\"citations\":[{\"arxivId\":\"1808.03766\",\"authors\":[{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"8983218\",\"name\":\"S. Buch\"},{\"authorId\":\"3409955\",\"name\":\"C. D. Dao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5468c96e3846da23c26b59c28c313506bffbf7ce\",\"title\":\"The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary\",\"url\":\"https://www.semanticscholar.org/paper/5468c96e3846da23c26b59c28c313506bffbf7ce\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41078113\",\"name\":\"Abdullah Abuolaim\"},{\"authorId\":\"143955418\",\"name\":\"M. S. Brown\"}],\"doi\":\"10.1109/WACV45572.2020.9093558\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"19404d2b246e96134474b484e585d98ea9d6db7e\",\"title\":\"Online Lens Motion Smoothing for Video Autofocus\",\"url\":\"https://www.semanticscholar.org/paper/19404d2b246e96134474b484e585d98ea9d6db7e\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"},{\"authorId\":\"145952554\",\"name\":\"Michael Chan\"},{\"authorId\":\"28130078\",\"name\":\"P. P. Liang\"},{\"authorId\":\"31726556\",\"name\":\"Edmund Tong\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/CVPR.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"title\":\"Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1910.02930\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/K19-1039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"title\":\"A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"2007.14682\",\"authors\":[{\"authorId\":\"1840585237\",\"name\":\"Philipp Rimle\"},{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"143720818\",\"name\":\"M. Gro\\u00df\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"title\":\"Enriching Video Captions With Contextual Text\",\"url\":\"https://www.semanticscholar.org/paper/f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.13426\",\"authors\":[{\"authorId\":\"46584859\",\"name\":\"Jiangliu Wang\"},{\"authorId\":\"2840852\",\"name\":\"Jianbo Jiao\"},{\"authorId\":\"2780029\",\"name\":\"Linchao Bao\"},{\"authorId\":\"2548483\",\"name\":\"Shengfeng He\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"47908910\",\"name\":\"Y. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b38e482488359da26a25ed9ef5341cd38a2b6562\",\"title\":\"Self-supervised Video Representation Learning by Uncovering Spatio-temporal Statistics\",\"url\":\"https://www.semanticscholar.org/paper/b38e482488359da26a25ed9ef5341cd38a2b6562\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255213\",\"name\":\"Z. Zhang\"},{\"authorId\":\"38188040\",\"name\":\"Dong Xu\"},{\"authorId\":\"47337540\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"2597292\",\"name\":\"Chuanqi Tan\"}],\"doi\":\"10.1109/TCSVT.2019.2936526\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"title\":\"Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization\",\"url\":\"https://www.semanticscholar.org/paper/b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.08191\",\"authors\":[{\"authorId\":\"152923110\",\"name\":\"Kuan-Yen Lin\"},{\"authorId\":\"23608432\",\"name\":\"Chao-Chun Hsu\"},{\"authorId\":\"1725643\",\"name\":\"Yun-Nung (Vivian) Chen\"},{\"authorId\":\"1746959\",\"name\":\"Lun-Wei Ku\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee1add32f430fb6e9f82958dc431b635174ee9bd\",\"title\":\"Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation\",\"url\":\"https://www.semanticscholar.org/paper/ee1add32f430fb6e9f82958dc431b635174ee9bd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1527097122\",\"name\":\"Y. Li\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"9359893\",\"name\":\"Shengrong Gong\"}],\"doi\":\"10.1145/3378026\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"083e075864cdf19117f6b5f78db877347e2bca4f\",\"title\":\"Spatio-temporal Deep Residual Network with Hierarchical Attentions for Video Event Recognition\",\"url\":\"https://www.semanticscholar.org/paper/083e075864cdf19117f6b5f78db877347e2bca4f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"50272388\",\"name\":\"Ioana Croitoru\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"title\":\"A hierarchical approach to vision-based language generation: from simple sentences to complex natural language\",\"url\":\"https://www.semanticscholar.org/paper/9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1812.05634\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00676\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"title\":\"Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1978802390\",\"name\":\"Haolei Pei\"},{\"authorId\":\"8559954\",\"name\":\"Q. Chen\"},{\"authorId\":\"13257164\",\"name\":\"J. Wang\"},{\"authorId\":\"123555217\",\"name\":\"Q. Sun\"},{\"authorId\":\"1680030\",\"name\":\"Yubo Jia\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206815\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"title\":\"Visual Relational Reasoning for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1904.03597\",\"authors\":[{\"authorId\":\"46584859\",\"name\":\"Jiangliu Wang\"},{\"authorId\":\"2840852\",\"name\":\"Jianbo Jiao\"},{\"authorId\":\"2780029\",\"name\":\"Linchao Bao\"},{\"authorId\":\"2548483\",\"name\":\"Shengfeng He\"},{\"authorId\":\"46398631\",\"name\":\"Yunhui Liu\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2019.00413\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb2f5e00049eb5bd36bea402c2818fe4bbfe0a0e\",\"title\":\"Self-Supervised Spatio-Temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics\",\"url\":\"https://www.semanticscholar.org/paper/eb2f5e00049eb5bd36bea402c2818fe4bbfe0a0e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1581863540\",\"name\":\"Aidean Sharghi Karganroodi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"550b08b659d3b8e7f45bdc09602af2184791d082\",\"title\":\"Visual-Textual Video Synopsis Generation\",\"url\":\"https://www.semanticscholar.org/paper/550b08b659d3b8e7f45bdc09602af2184791d082\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1903.02155\",\"authors\":[{\"authorId\":\"144770282\",\"name\":\"D. Xie\"},{\"authorId\":\"145102437\",\"name\":\"Cheng Deng\"},{\"authorId\":\"49528465\",\"name\":\"Hao Wang\"},{\"authorId\":\"66140038\",\"name\":\"Chao Li\"},{\"authorId\":\"1701119\",\"name\":\"Dapeng Tao\"}],\"doi\":\"10.1609/AAAI.V33I01.33019030\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"19458fa0d29181cc3f72f5f1253b64ba0e02e5f3\",\"title\":\"Semantic Adversarial Network with Multi-scale Pyramid Attention for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/19458fa0d29181cc3f72f5f1253b64ba0e02e5f3\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1807.09986\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1007/978-3-030-01216-8_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"title\":\"Recurrent Fusion Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.24963/ijcai.2019/877\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"title\":\"Deep Learning for Video Captioning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2004.00760\",\"authors\":[{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.14288/1.0392691\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"title\":\"Consistent Multiple Sequence Decoding\",\"url\":\"https://www.semanticscholar.org/paper/5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740660\",\"name\":\"Jingyuan Chen\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"2750647\",\"name\":\"Zequn Jie\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"631d21ee2e0de4227be1dfe000006acf1e0fe783\",\"title\":\"Sentence Encoder Video Encoder Frame-Specific Sentence Representation Cross Gating Matching Aggregation Self Interactor Segment Localizer Cross Modal\",\"url\":\"https://www.semanticscholar.org/paper/631d21ee2e0de4227be1dfe000006acf1e0fe783\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1908.10072\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"67074535\",\"name\":\"W. Zhang\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"46641690\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/ICCV.2019.00273\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"title\":\"Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1903.05854\",\"authors\":[{\"authorId\":\"6002624\",\"name\":\"Tingting Qiao\"},{\"authorId\":\"49050705\",\"name\":\"J. Zhang\"},{\"authorId\":\"7471918\",\"name\":\"Duanqing Xu\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR.2019.00160\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5af2d424bb38db7e6a72f7bec2cfd8a5bb8af7fb\",\"title\":\"MirrorGAN: Learning Text-To-Image Generation by Redescription\",\"url\":\"https://www.semanticscholar.org/paper/5af2d424bb38db7e6a72f7bec2cfd8a5bb8af7fb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.01452\",\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474871\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/TPAMI.2019.2920899\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83a3fe38887880bccc15daa740d8d5041f826d91\",\"title\":\"Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/83a3fe38887880bccc15daa740d8d5041f826d91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390818869\",\"name\":\"Jinlei Xu\"},{\"authorId\":\"144546140\",\"name\":\"T. Xu\"},{\"authorId\":\"123432231\",\"name\":\"Xin Tian\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144911521\",\"name\":\"Y. Ji\"}],\"doi\":\"10.1109/IJCNN.2019.8851897\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf7b38dd24c20223e006066be4202d1da700af37\",\"title\":\"Context Gating with Short Temporal Information for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf7b38dd24c20223e006066be4202d1da700af37\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"2006.11693\",\"authors\":[{\"authorId\":\"1563987322\",\"name\":\"Teng Wang\"},{\"authorId\":\"39458374\",\"name\":\"H. Zheng\"},{\"authorId\":\"47730643\",\"name\":\"Mingjing Yu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d2383f16ecc732601af6c3929e8a3abfca193d87\",\"title\":\"Dense-Captioning Events in Videos: SYSU Submission to ActivityNet Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/d2383f16ecc732601af6c3929e8a3abfca193d87\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.09791\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-58589-1_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"title\":\"Identity-Aware Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":\"10.1007/978-3-030-31756-0_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"title\":\"The Encoder-Decoder Framework and Its Applications\",\"url\":\"https://www.semanticscholar.org/paper/0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1650071598\",\"name\":\"Ahmed Mazari\"},{\"authorId\":\"1692389\",\"name\":\"H. Sahbi\"}],\"doi\":\"10.1109/ICIP40778.2020.9191360\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a15dbfca55605bd3a5769ada375433c216822e0e\",\"title\":\"Coarse-to-Fine Aggregation for Cross-Granularity Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a15dbfca55605bd3a5769ada375433c216822e0e\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114276298\",\"name\":\"Soichiro Fujita\"},{\"authorId\":\"8351786\",\"name\":\"Tsutomu Hirao\"},{\"authorId\":\"2300756\",\"name\":\"Hidetaka Kamigaito\"},{\"authorId\":\"144859189\",\"name\":\"M. Okumura\"},{\"authorId\":\"2364073\",\"name\":\"M. Nagata\"}],\"doi\":\"10.1007/978-3-030-58539-6_31\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5a4c5fa5a25cff3c65e74f64504819683353ef1e\",\"title\":\"SODA: Story Oriented Dense Video Captioning Evaluation Framework\",\"url\":\"https://www.semanticscholar.org/paper/5a4c5fa5a25cff3c65e74f64504819683353ef1e\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1810.00207\",\"authors\":[{\"authorId\":\"7868449\",\"name\":\"Yongyi Tang\"},{\"authorId\":\"46447561\",\"name\":\"X. Zhang\"},{\"authorId\":null,\"name\":\"Jingwen Wang\"},{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-11018-5_20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"507f398b79e94a1ebde61c0a854c3954e7bb3d33\",\"title\":\"Non-local NetVLAD Encoding for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/507f398b79e94a1ebde61c0a854c3954e7bb3d33\",\"venue\":\"ECCV Workshops\",\"year\":2018},{\"arxivId\":\"2006.07896\",\"authors\":[{\"authorId\":\"40607664\",\"name\":\"Yuqing Song\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c9c256c33ed4db6b83321c516025b1feb62ddfb\",\"title\":\"Team RUC_AIM3 Technical Report at Activitynet 2020 Task 2: Exploring Sequential Events Detection for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c9c256c33ed4db6b83321c516025b1feb62ddfb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.04489\",\"authors\":[{\"authorId\":\"1650071598\",\"name\":\"Ahmed Mazari\"},{\"authorId\":\"1692389\",\"name\":\"H. Sahbi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe7915ca11fda06d678933e2f51ad26e4ce2c661\",\"title\":\"Action Recognition with Deep Multiple Aggregation Networks\",\"url\":\"https://www.semanticscholar.org/paper/fe7915ca11fda06d678933e2f51ad26e4ce2c661\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.00669\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"47190894\",\"name\":\"F. Zhang\"}],\"doi\":\"10.1609/aaai.v33i01.33018909\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fe33597affd4e99a5dc979ef4ed99ee6311fdc2b\",\"title\":\"Hierarchical Photo-Scene Encoder for Album Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/fe33597affd4e99a5dc979ef4ed99ee6311fdc2b\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1808.01575\",\"authors\":[{\"authorId\":\"144599697\",\"name\":\"Y. Feng\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1007/978-3-030-01264-9_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8619ce85b69e6164cb5bb32f225e510a0570bb37\",\"title\":\"Video Re-localization\",\"url\":\"https://www.semanticscholar.org/paper/8619ce85b69e6164cb5bb32f225e510a0570bb37\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46842113\",\"name\":\"S. Muralidharan\"},{\"authorId\":null,\"name\":\"smuralid\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"04d46aa5f75b7bee55f173cc76610643d755e154\",\"title\":\"Memory Augmented Recurrent Neural Networks for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04d46aa5f75b7bee55f173cc76610643d755e154\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.03922\",\"authors\":[{\"authorId\":\"144599697\",\"name\":\"Y. Feng\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"144416719\",\"name\":\"W. Liu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2019.00138\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c19659297ac67a29d7524fba60062558f2235f8a\",\"title\":\"Spatio-Temporal Video Re-Localization by Warp LSTM\",\"url\":\"https://www.semanticscholar.org/paper/c19659297ac67a29d7524fba60062558f2235f8a\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.02549\",\"authors\":[{\"authorId\":\"26907392\",\"name\":\"Zhenfang Chen\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\"},{\"authorId\":\"1698116\",\"name\":\"K. Wong\"}],\"doi\":\"10.18653/v1/P19-1183\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7e158a7619c6d0d583a8bbdf00658b7d4bbe0374\",\"title\":\"Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video\",\"url\":\"https://www.semanticscholar.org/paper/7e158a7619c6d0d583a8bbdf00658b7d4bbe0374\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1905.11799\",\"authors\":[{\"authorId\":\"7868449\",\"name\":\"Yongyi Tang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"26485115\",\"name\":\"Lianqiang Zhou\"}],\"doi\":\"10.24963/ijcai.2019/130\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"37252f8cd1324a972131fc6a92f778835ba2fac3\",\"title\":\"Hallucinating Optical Flow Features for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/37252f8cd1324a972131fc6a92f778835ba2fac3\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"2750647\",\"name\":\"Zequn Jie\"},{\"authorId\":\"38218192\",\"name\":\"X. Wang\"}],\"doi\":\"10.1016/J.NEUCOM.2018.11.089\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"02844808a10aa27fad397d1941ec24f5e546ca0b\",\"title\":\"Bidirectional image-sentence retrieval by local and global deep matching\",\"url\":\"https://www.semanticscholar.org/paper/02844808a10aa27fad397d1941ec24f5e546ca0b\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1997177\",\"name\":\"Ercheng Pei\"},{\"authorId\":\"153763368\",\"name\":\"Dong-mei Jiang\"},{\"authorId\":\"151486921\",\"name\":\"H. Sahli\"}],\"doi\":\"10.1016/j.neucom.2019.09.037\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"817f717412614675aee6e7e0e3e25af68a0b1219\",\"title\":\"An efficient model-level fusion approach for continuous affect recognition from audiovisual signals\",\"url\":\"https://www.semanticscholar.org/paper/817f717412614675aee6e7e0e3e25af68a0b1219\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"47381330\",\"name\":\"A. Dimou\"},{\"authorId\":\"33961149\",\"name\":\"G. Papadopoulos\"},{\"authorId\":\"1747572\",\"name\":\"P. Daras\"}],\"doi\":\"10.1109/ICE.2019.8792602\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"title\":\"Incorporating Textual Similarity in Video Captioning Schemes\",\"url\":\"https://www.semanticscholar.org/paper/2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"venue\":\"2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)\",\"year\":2019},{\"arxivId\":\"2005.03804\",\"authors\":[{\"authorId\":\"3377097\",\"name\":\"A. Sharghi\"},{\"authorId\":\"1700665\",\"name\":\"N. Lobo\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7cd871b42efb42f507444386e4317efd7dfc10c\",\"title\":\"Text Synopsis Generation for Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/d7cd871b42efb42f507444386e4317efd7dfc10c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"50688017\",\"name\":\"L. Ji\"},{\"authorId\":\"3887469\",\"name\":\"Yaobo Liang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"46915168\",\"name\":\"P. Chen\"},{\"authorId\":\"46764518\",\"name\":\"Zhendong Niu\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"}],\"doi\":\"10.18653/v1/P19-1641\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ea57ffa3e13400cad53dc061887a6fbbd45e7f12\",\"title\":\"Dense Procedure Captioning in Narrated Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/ea57ffa3e13400cad53dc061887a6fbbd45e7f12\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1909.09944\",\"authors\":[{\"authorId\":\"49172303\",\"name\":\"T. Rahman\"},{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00900\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7a2de516a4e628a30036193d71faac7240d553ef\",\"title\":\"Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a2de516a4e628a30036193d71faac7240d553ef\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48515249\",\"name\":\"Y. Li\"},{\"authorId\":\"1390462650\",\"name\":\"Juan Yang\"}],\"doi\":\"10.1145/3366750.3366756\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8462d4c88767ca26b70bcaa88da811d5f8303d1\",\"title\":\"Hydrological Time Series Prediction Model Based on Attention-LSTM Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/c8462d4c88767ca26b70bcaa88da811d5f8303d1\",\"venue\":\"ICML 2019\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144850642\",\"name\":\"W. Ren\"},{\"authorId\":\"47538816\",\"name\":\"Jingang Zhang\"},{\"authorId\":\"2291143\",\"name\":\"Xiangyu Xu\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1719250\",\"name\":\"Xiaochun Cao\"},{\"authorId\":\"3182192\",\"name\":\"Gaofeng Meng\"},{\"authorId\":\"143775741\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/TIP.2018.2876178\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"281b8c4e4ed6a9237a5b65a6f95048882340253c\",\"title\":\"Deep Video Dehazing With Semantic Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/281b8c4e4ed6a9237a5b65a6f95048882340253c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"1806.08854\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"10713620\",\"name\":\"J. Qiu\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a9c92a56e388997adb513305a4259798506b7f5\",\"title\":\"RUC+CMU: System Report for Dense Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/9a9c92a56e388997adb513305a4259798506b7f5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47569376\",\"name\":\"Shijie Yang\"},{\"authorId\":\"73596205\",\"name\":\"L. Li\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"49356099\",\"name\":\"Dechao Meng\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3343031.3350859\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"title\":\"Structured Stochastic Recurrent Network for Linguistic Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2007.08883\",\"authors\":[{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"72095125\",\"name\":\"Y. Zhang\"},{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"}],\"doi\":\"10.1007/978-3-030-58586-0_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fe573437cbd4069556348ad28dfeae2df46e22a0\",\"title\":\"Consensus-Aware Visual-Semantic Embedding for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/fe573437cbd4069556348ad28dfeae2df46e22a0\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1812.03849\",\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1688516\",\"name\":\"Jingdong Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"title\":\"Weakly Supervised Dense Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2006.11747\",\"authors\":[{\"authorId\":\"1500408667\",\"name\":\"Zhiyuan Fang\"},{\"authorId\":\"34362536\",\"name\":\"S. Kong\"},{\"authorId\":\"1521319166\",\"name\":\"Zhe Wang\"},{\"authorId\":\"143800213\",\"name\":\"Charless C. Fowlkes\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a792ff56eeed530fab1935168510cbb16b0f1b68\",\"title\":\"Weak Supervision and Referring Attention for Temporal-Textual Association Learning\",\"url\":\"https://www.semanticscholar.org/paper/a792ff56eeed530fab1935168510cbb16b0f1b68\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35041003\",\"name\":\"Lichao Mou\"},{\"authorId\":\"40049070\",\"name\":\"X. Zhu\"}],\"doi\":\"10.1109/TGRS.2019.2933609\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4c40aaa43b8edc4a4212207487bd74b32c23eb14\",\"title\":\"Learning to Pay Attention on Spectral Domain: A Spectral Attention Module-Based Convolutional Network for Hyperspectral Image Classification\",\"url\":\"https://www.semanticscholar.org/paper/4c40aaa43b8edc4a4212207487bd74b32c23eb14\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740660\",\"name\":\"Jingyuan Chen\"},{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"2750647\",\"name\":\"Zequn Jie\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.18653/v1/D18-1015\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"452aca244ef62a533d8b46a54c6212fe9fa3ce9a\",\"title\":\"Temporally Grounding Natural Sentence in Video\",\"url\":\"https://www.semanticscholar.org/paper/452aca244ef62a533d8b46a54c6212fe9fa3ce9a\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"2011.11479\",\"authors\":[{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"22314218\",\"name\":\"Silvio Giancola\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5388388db25f0d40ef6612333a0279373f8dddcf\",\"title\":\"TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks\",\"url\":\"https://www.semanticscholar.org/paper/5388388db25f0d40ef6612333a0279373f8dddcf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.12004\",\"authors\":[{\"authorId\":\"98243944\",\"name\":\"Chenglong Wang\"},{\"authorId\":\"3407947\",\"name\":\"R. Bunel\"},{\"authorId\":\"1729912\",\"name\":\"Krishnamurthy Dvijotham\"},{\"authorId\":\"2421691\",\"name\":\"Po-Sen Huang\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"}],\"doi\":\"10.1109/CVPR.2019.01254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59b439bde73d80dccf367d414e209d08d312c059\",\"title\":\"Knowing When to Stop: Evaluation and Verification of Conformity to Output-Size Specifications\",\"url\":\"https://www.semanticscholar.org/paper/59b439bde73d80dccf367d414e209d08d312c059\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1811.10698\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":\"10.1109/CVPR.2019.01019\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed78a2671ef61c031759c01434678c282f23faec\",\"title\":\"LSTA: Long Short-Term Attention for Egocentric Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ed78a2671ef61c031759c01434678c282f23faec\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2011.07735\",\"authors\":[{\"authorId\":\"40016108\",\"name\":\"Aman Chadha\"},{\"authorId\":\"2025073690\",\"name\":\"Gurneet Arora\"},{\"authorId\":\"2025065763\",\"name\":\"Navpreet Kaloty\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"title\":\"iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52203335\",\"name\":\"Yanlong Dong\"},{\"authorId\":\"72095125\",\"name\":\"Y. Zhang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"51434236\",\"name\":\"Zongguo Wang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1016/j.patcog.2020.107573\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"02b14bc694560b1ae5005136aa04e686e5690014\",\"title\":\"Unsupervised text-to-image synthesis\",\"url\":\"https://www.semanticscholar.org/paper/02b14bc694560b1ae5005136aa04e686e5690014\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":\"2006.04473\",\"authors\":[{\"authorId\":\"1650071598\",\"name\":\"Ahmed Mazari\"},{\"authorId\":\"1692389\",\"name\":\"H. Sahbi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efcd92bd79eab5d4eeabcf5da8710b04b5bf2d50\",\"title\":\"Deep hierarchical pooling design for cross-granularity action recognition\",\"url\":\"https://www.semanticscholar.org/paper/efcd92bd79eab5d4eeabcf5da8710b04b5bf2d50\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.10255\",\"authors\":[{\"authorId\":\"8809422\",\"name\":\"Vishwanath Sindagi\"},{\"authorId\":\"1741177\",\"name\":\"V. Patel\"}],\"doi\":\"10.1109/TIP.2019.2928634\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f8a1d07216259a87fe8968feb9379c14629ad9f\",\"title\":\"HA-CCN: Hierarchical Attention-Based Crowd Counting Network\",\"url\":\"https://www.semanticscholar.org/paper/8f8a1d07216259a87fe8968feb9379c14629ad9f\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1811.11524\",\"authors\":[{\"authorId\":\"46398922\",\"name\":\"Y. Liu\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"48380246\",\"name\":\"Yifeng Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2019.00372\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"990fb8c10628754e69fa8d0003d1fc0ed3e2027c\",\"title\":\"Multi-Granularity Generator for Temporal Action Proposal\",\"url\":\"https://www.semanticscholar.org/paper/990fb8c10628754e69fa8d0003d1fc0ed3e2027c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.05861\",\"authors\":[{\"authorId\":\"46584859\",\"name\":\"Jiangliu Wang\"},{\"authorId\":\"2840852\",\"name\":\"Jianbo Jiao\"},{\"authorId\":\"47908910\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/978-3-030-58520-4_30\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78ad3beec8cc6c331dfe491291c213214e798f45\",\"title\":\"Self-supervised Video Representation Learning by Pace Prediction\",\"url\":\"https://www.semanticscholar.org/paper/78ad3beec8cc6c331dfe491291c213214e798f45\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.04145\",\"authors\":[{\"authorId\":\"2004346846\",\"name\":\"Jialin Gao\"},{\"authorId\":\"49473137\",\"name\":\"Zhixiang Shi\"},{\"authorId\":\"1492113737\",\"name\":\"Jiani Li\"},{\"authorId\":\"50248679\",\"name\":\"Guanshuo Wang\"},{\"authorId\":\"46499930\",\"name\":\"Y. Yuan\"},{\"authorId\":\"39646508\",\"name\":\"Shiming Ge\"},{\"authorId\":\"50177639\",\"name\":\"Xiaoping Zhou\"}],\"doi\":\"10.1609/AAAI.V34I07.6711\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"36e38dbfee27f7a34d184dd58186944636de5258\",\"title\":\"Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid Network\",\"url\":\"https://www.semanticscholar.org/paper/36e38dbfee27f7a34d184dd58186944636de5258\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2010.08263\",\"authors\":[{\"authorId\":\"49602505\",\"name\":\"Xing Yan\"},{\"authorId\":\"47527753\",\"name\":\"Weizhong Zhang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"143604502\",\"name\":\"Q. Wu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a0a10d9c94e9c849d9004bdb077fb3274c82c6e\",\"title\":\"Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning\",\"url\":\"https://www.semanticscholar.org/paper/2a0a10d9c94e9c849d9004bdb077fb3274c82c6e\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1908.00707\",\"authors\":[{\"authorId\":\"3235708\",\"name\":\"Guoqiang Gong\"},{\"authorId\":\"9693996\",\"name\":\"Liangfeng Zheng\"},{\"authorId\":\"144654776\",\"name\":\"Kun Bai\"},{\"authorId\":\"145353089\",\"name\":\"Y. Mu\"}],\"doi\":\"10.1109/icme46284.2020.9102850\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bba21a50bdd896cc4ebddcb8b6ce807f4a26287\",\"title\":\"Scale Matters: Temporal Scale Aggregation Network For Precise Action Localization In Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/3bba21a50bdd896cc4ebddcb8b6ce807f4a26287\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.11566\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":null,\"name\":\"Bing Li\"},{\"authorId\":\"39397292\",\"name\":\"Peijin Wang\"},{\"authorId\":\"48594951\",\"name\":\"Weiming Hu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1109/cvpr42600.2020.01329\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1dd557a8839733a5ee06d19989a265e61f603c1\",\"title\":\"Object Relational Graph With Teacher-Recommended Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1dd557a8839733a5ee06d19989a265e61f603c1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1905.03966\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"49050519\",\"name\":\"Jiyuan Zhang\"},{\"authorId\":\"47119038\",\"name\":\"X. Wang\"},{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/CVPR.2019.00854\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"title\":\"Memory-Attended Recurrent Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"48093314\",\"name\":\"Jing-Wen Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3394171.3413908\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"title\":\"Controllable Video Captioning with an Exemplar Sentence\",\"url\":\"https://www.semanticscholar.org/paper/40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020}],\"corpusId\":4621662,\"doi\":\"10.1109/CVPR.2018.00751\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"references\":[{\"arxivId\":\"1705.02101\",\"authors\":[{\"authorId\":\"3029956\",\"name\":\"J. Gao\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"3469030\",\"name\":\"Zhenheng Yang\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/ICCV.2017.563\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e9bd6f0b04a0ddf9fcdf3a5fd1cfe87f8ae9cfff\",\"title\":\"TALL: Temporal Activity Localization via Language Query\",\"url\":\"https://www.semanticscholar.org/paper/e9bd6f0b04a0ddf9fcdf3a5fd1cfe87f8ae9cfff\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1403.6173\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"113090874\",\"name\":\"W. Qiu\"},{\"authorId\":\"33985877\",\"name\":\"Annemarie Friedrich\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-11752-2_15\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"889e723cd6d581e120ee6776b231fdf69707ab50\",\"title\":\"Coherent Multi-sentence Video Description with Variable Level of Detail\",\"url\":\"https://www.semanticscholar.org/paper/889e723cd6d581e120ee6776b231fdf69707ab50\",\"venue\":\"GCPR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":\"1611.05594\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1406012647\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"104757141\",\"name\":\"Jian Shao\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/CVPR.2017.667\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88513e738a95840de05a62f0e43d30a67b3c542e\",\"title\":\"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/88513e738a95840de05a62f0e43d30a67b3c542e\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1611.08002\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"143690259\",\"name\":\"K. Tran\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/CVPR.2017.127\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"778ce81457383bd5e3fdb11b145ded202ebb4970\",\"title\":\"Semantic Compositional Networks for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/778ce81457383bd5e3fdb11b145ded202ebb4970\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1804.00887\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc2c3be0796f65154d2106ed4442889c84546df\",\"title\":\"Learning to Guide Decoding for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3dc2c3be0796f65154d2106ed4442889c84546df\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"},{\"authorId\":\"1717056\",\"name\":\"M. Worring\"}],\"doi\":\"10.1561/1500000014\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"987dd3dd6079e5fa8a10a1c53b2580fd71e27ede\",\"title\":\"Concept-Based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/987dd3dd6079e5fa8a10a1c53b2580fd71e27ede\",\"venue\":\"Found. Trends Inf. Retr.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Y. Jiang\"},{\"authorId\":null,\"name\":\"J. Liu\"},{\"authorId\":null,\"name\":\"A. R. Zamir\"},{\"authorId\":null,\"name\":\"G. Toderici\"},{\"authorId\":null,\"name\":\"I. Laptev\"},{\"authorId\":null,\"name\":\"M. Shah\"},{\"authorId\":null,\"name\":\"R. Sukthankar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Thumos challenge: Action recognition with a large number of classes\",\"url\":\"\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3285716\",\"name\":\"Xinye Lin\"},{\"authorId\":\"9527255\",\"name\":\"Yixin Chen\"},{\"authorId\":\"1871246\",\"name\":\"X. Chang\"},{\"authorId\":\"1723807\",\"name\":\"X. Liu\"},{\"authorId\":\"47119262\",\"name\":\"X. Wang\"}],\"doi\":\"10.1145/3161412\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"299b07ec255398c7f9d147cb39d113e4926cc51b\",\"title\":\"SHOW\",\"url\":\"https://www.semanticscholar.org/paper/299b07ec255398c7f9d147cb39d113e4926cc51b\",\"venue\":\"IMWUT\",\"year\":2018},{\"arxivId\":\"1611.01646\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2017.524\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5785466bc14529e94e54baa4ed051f7037f3b1d3\",\"title\":\"Boosting Image Captioning with Attributes\",\"url\":\"https://www.semanticscholar.org/paper/5785466bc14529e94e54baa4ed051f7037f3b1d3\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3141511\",\"name\":\"S. Banerjee\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"title\":\"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments\",\"url\":\"https://www.semanticscholar.org/paper/0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"venue\":\"IEEvaluation@ACL\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1007/978-3-319-46487-9_47\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c24bbbc5139eb2f8c5d0579174dbeae5cbaedbfc\",\"title\":\"DAPs: Deep Action Proposals for Action Understanding\",\"url\":\"https://www.semanticscholar.org/paper/c24bbbc5139eb2f8c5d0579174dbeae5cbaedbfc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"48246959\",\"name\":\"Naiyan Wang\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"1739816\",\"name\":\"D. Yeung\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/CVPR.2015.7298872\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3de050d1707524512eeab99780df3cbdee09670c\",\"title\":\"DevNet: A Deep Event Network for multimedia event detection and evidence recounting\",\"url\":\"https://www.semanticscholar.org/paper/3de050d1707524512eeab99780df3cbdee09670c\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"},{\"authorId\":\"2704179\",\"name\":\"Dongang Wang\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2016.119\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"317eaf94573857bec786bbf030605ccdb0fd624d\",\"title\":\"Temporal Action Localization in Untrimmed Videos via Multi-stage CNNs\",\"url\":\"https://www.semanticscholar.org/paper/317eaf94573857bec786bbf030605ccdb0fd624d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jingwen Wang\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dde84862e2a2b1c11d27d695e47e41db0f2eb06d\",\"title\":\"Beyond Object Recognition: Visual Sentiment Analysis with Deep Coupled Adjective and Noun Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/dde84862e2a2b1c11d27d695e47e41db0f2eb06d\",\"venue\":\"IJCAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1504.06063\",\"authors\":[{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"11955007\",\"name\":\"Z. Lu\"},{\"authorId\":\"50812138\",\"name\":\"L. Shang\"},{\"authorId\":\"49404233\",\"name\":\"Hang Li\"}],\"doi\":\"10.1109/ICCV.2015.301\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"153d6feb7149e063b33e8ee437b74e4a2def8057\",\"title\":\"Multimodal Convolutional Neural Networks for Matching Image and Sentence\",\"url\":\"https://www.semanticscholar.org/paper/153d6feb7149e063b33e8ee437b74e4a2def8057\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1109/CVPR.2016.211\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bac994dda1385cd709e08e24170c711d8c573676\",\"title\":\"Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/bac994dda1385cd709e08e24170c711d8c573676\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1703.06189\",\"authors\":[{\"authorId\":\"3029956\",\"name\":\"J. Gao\"},{\"authorId\":\"3469030\",\"name\":\"Zhenheng Yang\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/ICCV.2017.392\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f94841ec597dcf6d1c23e7f40ba35e121f6a81c1\",\"title\":\"TURN TAP: Temporal Unit Regression Network for Temporal Action Proposals\",\"url\":\"https://www.semanticscholar.org/paper/f94841ec597dcf6d1c23e7f40ba35e121f6a81c1\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688587\",\"name\":\"Haojin Yang\"},{\"authorId\":\"1708312\",\"name\":\"C. Meinel\"}],\"doi\":\"10.1109/TLT.2014.2307305\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"afc05640074d494a45ebc7212d19b83bc263d9e8\",\"title\":\"Content Based Lecture Video Retrieval Using Speech and Video Text Information\",\"url\":\"https://www.semanticscholar.org/paper/afc05640074d494a45ebc7212d19b83bc263d9e8\",\"venue\":\"IEEE Transactions on Learning Technologies\",\"year\":2014},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"2069818\",\"name\":\"D. Zhang\"},{\"authorId\":\"1706774\",\"name\":\"J. Li\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/CVPR.2017.662\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b0b706fc94b35a1eddd830685e07870315b9565\",\"title\":\"Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description\",\"url\":\"https://www.semanticscholar.org/paper/3b0b706fc94b35a1eddd830685e07870315b9565\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1708.03423\",\"authors\":[{\"authorId\":\"144850642\",\"name\":\"W. Ren\"},{\"authorId\":\"2302673\",\"name\":\"Jin-shan Pan\"},{\"authorId\":\"1719250\",\"name\":\"Xiaochun Cao\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":\"10.1109/ICCV.2017.123\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"356591a161a227554ab27589fefa31d2a66993c4\",\"title\":\"Video Deblurring via Semantic Segmentation and Pixel-Wise Non-linear Kernel\",\"url\":\"https://www.semanticscholar.org/paper/356591a161a227554ab27589fefa31d2a66993c4\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"},{\"authorId\":\"144562060\",\"name\":\"Hui Ji\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"}],\"doi\":\"10.1007/s11263-009-0220-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"52d8f6f8b8cecb62019e18020e4495a354c0f2c9\",\"title\":\"Viewpoint Invariant Texture Description Using Fractal Analysis\",\"url\":\"https://www.semanticscholar.org/paper/52d8f6f8b8cecb62019e18020e4495a354c0f2c9\",\"venue\":\"International Journal of Computer Vision\",\"year\":2009},{\"arxivId\":\"1506.00333\",\"authors\":[{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"11955007\",\"name\":\"Z. Lu\"},{\"authorId\":\"49404233\",\"name\":\"Hang Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"98bd5dd1740f585bf25320ba504e2c1ae57f2e5f\",\"title\":\"Learning to Answer Questions from Image Using Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/98bd5dd1740f585bf25320ba504e2c1ae57f2e5f\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"X. Long\"},{\"authorId\":null,\"name\":\"C. Gan\"},{\"authorId\":null,\"name\":\"G. de Melo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Video captioning with multifaceted attention. Transactions of the Association for Computational Linguistics, 2018\",\"url\":\"\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":\"1708.00666\",\"authors\":[{\"authorId\":\"34567611\",\"name\":\"Yuan Yuan\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"1739816\",\"name\":\"D. Yeung\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1109/ICCV.2017.200\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c44b730bef287581ad1d70bf9a883787d5998d8d\",\"title\":\"Temporal Dynamic Graph LSTM for Action-Driven Video Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/c44b730bef287581ad1d70bf9a883787d5998d8d\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Potapov\"},{\"authorId\":null,\"name\":\"M. Douze\"},{\"authorId\":null,\"name\":\"Z. Harchaoui\"},{\"authorId\":null,\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Categoryspecific video summarization\",\"url\":\"\",\"venue\":\"In ECCV,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2015.7298698\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a28efacb92d16e6e0dd4d87b5aca91b28be8853\",\"title\":\"ActivityNet: A large-scale video benchmark for human activity understanding\",\"url\":\"https://www.semanticscholar.org/paper/0a28efacb92d16e6e0dd4d87b5aca91b28be8853\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144850642\",\"name\":\"W. Ren\"},{\"authorId\":\"144977051\",\"name\":\"S. Liu\"},{\"authorId\":\"31431896\",\"name\":\"Hua Zhang\"},{\"authorId\":\"2302673\",\"name\":\"Jin-shan Pan\"},{\"authorId\":\"1719250\",\"name\":\"Xiaochun Cao\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":\"10.1007/978-3-319-46475-6_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"392e419efe21b6e2cc1125f27482d22bf3c677be\",\"title\":\"Single Image Dehazing via Multi-scale Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/392e419efe21b6e2cc1125f27482d22bf3c677be\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1708.09666\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"}],\"doi\":\"10.1145/3078971.3079000\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"title\":\"Generating Video Descriptions with Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8983218\",\"name\":\"S. Buch\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"2263748\",\"name\":\"Chuanqi Shen\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2017.675\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"352b190acfe19406baee53a169a8732f9b2764d4\",\"title\":\"SST: Single-Stream Temporal Action Proposals\",\"url\":\"https://www.semanticscholar.org/paper/352b190acfe19406baee53a169a8732f9b2764d4\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"X. Long\"},{\"authorId\":null,\"name\":\"C. Gan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and G\",\"url\":\"\",\"venue\":\"de Melo. Video captioning with multifaceted attention. Transactions of the Association for Computational Linguistics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fad611e35b3731740b4d8b754241e77add5a70b9\",\"title\":\"Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/fad611e35b3731740b4d8b754241e77add5a70b9\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143616798\",\"name\":\"Zheng Lu\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2013.350\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"996e8c706cc8e726963d3bbc4e761fc2ab68d396\",\"title\":\"Story-Driven Summarization for Egocentric Video\",\"url\":\"https://www.semanticscholar.org/paper/996e8c706cc8e726963d3bbc4e761fc2ab68d396\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. Krishna\"},{\"authorId\":null,\"name\":\"K. Hata\"},{\"authorId\":null,\"name\":\"F. Ren\"},{\"authorId\":null,\"name\":\"L. Fei-Fei\"},{\"authorId\":null,\"name\":\"J. Carlos Niebles\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Densecaptioning events in videos\",\"url\":\"\",\"venue\":\"In ICCV, 2017\",\"year\":2017}],\"title\":\"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning\",\"topics\":[{\"topic\":\"Meteor\",\"topicId\":\"131649\",\"url\":\"https://www.semanticscholar.org/topic/131649\"},{\"topic\":\"Performance\",\"topicId\":\"3097\",\"url\":\"https://www.semanticscholar.org/topic/3097\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Internationalization and localization\",\"topicId\":\"69706\",\"url\":\"https://www.semanticscholar.org/topic/69706\"},{\"topic\":\"Recliner\",\"topicId\":\"457576\",\"url\":\"https://www.semanticscholar.org/topic/457576\"},{\"topic\":\"Unified Framework\",\"topicId\":\"105596\",\"url\":\"https://www.semanticscholar.org/topic/105596\"},{\"topic\":\"Information\",\"topicId\":\"185548\",\"url\":\"https://www.semanticscholar.org/topic/185548\"},{\"topic\":\"Surround sound\",\"topicId\":\"1380\",\"url\":\"https://www.semanticscholar.org/topic/1380\"},{\"topic\":\"ENCODE\",\"topicId\":\"365717\",\"url\":\"https://www.semanticscholar.org/topic/365717\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"},{\"topic\":\"Image fusion\",\"topicId\":\"41451\",\"url\":\"https://www.semanticscholar.org/topic/41451\"}],\"url\":\"https://www.semanticscholar.org/paper/bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"