"{\"abstract\":\"Visual Grounding (VG) aims to locate the most relevant object or region in an image, based on a natural language query. The query can be a phrase, a sentence or even a multi-round dialogue. There are three main challenges in VG: 1) what is the main focus in a query; 2) how to understand an image; 3) how to locate an object. Most existing methods combine all the information curtly, which may suffer from the problem of information redundancy (i.e. ambiguous query, complicated image and a large number of objects). In this paper, we formulate these challenges as three attention problems and propose an accumulated attention (A-ATT) mechanism to reason among them jointly. Our A-ATT mechanism can circularly accumulate the attention for useful information in image, query, and objects, while the noises are ignored gradually. We evaluate the performance of A-ATT on four popular datasets (namely Refer-COCO, ReferCOCO+, ReferCOCOg, and Guesswhat?!), and the experimental results show the superiority of the proposed method in term of accuracy.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"41036094\",\"name\":\"C. Deng\",\"url\":\"https://www.semanticscholar.org/author/41036094\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\",\"url\":\"https://www.semanticscholar.org/author/144663765\"},{\"authorId\":\"8277017\",\"name\":\"Q. Wu\",\"url\":\"https://www.semanticscholar.org/author/8277017\"},{\"authorId\":\"2348236\",\"name\":\"Fuyuan Hu\",\"url\":\"https://www.semanticscholar.org/author/2348236\"},{\"authorId\":\"12358136\",\"name\":\"Fan Lyu\",\"url\":\"https://www.semanticscholar.org/author/12358136\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\",\"url\":\"https://www.semanticscholar.org/author/2823637\"}],\"citationVelocity\":24,\"citations\":[{\"arxivId\":\"2003.08717\",\"authors\":[{\"authorId\":\"1388019217\",\"name\":\"Thierry Deruyttere\"},{\"authorId\":\"144481186\",\"name\":\"Guillem Collell\"},{\"authorId\":\"100781843\",\"name\":\"Marie-Francine Moens\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2bc42a9ebf099b84c02feac5b99e81ce5777eb07\",\"title\":\"Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/2bc42a9ebf099b84c02feac5b99e81ce5777eb07\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48043335\",\"name\":\"R. Hong\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"12086460\",\"name\":\"Xiaoyu Mo\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b31d3016349c164d7cad574b64f2bcb74eb2490\",\"title\":\"Recursive Grounding Pruning Input Language the skis of the man in the red jacket skis of man in red jacket RvG-Tree Constructor\",\"url\":\"https://www.semanticscholar.org/paper/8b31d3016349c164d7cad574b64f2bcb74eb2490\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143780023\",\"name\":\"Zhong Ji\"},{\"authorId\":\"11179553\",\"name\":\"Zhigang Lin\"},{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"101084939\",\"name\":\"Y. He\"}],\"doi\":\"10.1109/ACCESS.2020.2975594\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"284cd0512ecd5d7cec335b0038444085398ebaf5\",\"title\":\"Multi-Modal Memory Enhancement Attention Network for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/284cd0512ecd5d7cec335b0038444085398ebaf5\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121433901\",\"name\":\"Xiaoqian Guo\"},{\"authorId\":\"50080038\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1145/3394171.3413567\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1870b836377d41de6280f6291089d66ee8cf8e61\",\"title\":\"Expressional Region Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/1870b836377d41de6280f6291089d66ee8cf8e61\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2007.02036\",\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"103278467\",\"name\":\"Minuk Ma\"},{\"authorId\":\"48920375\",\"name\":\"T. Pham\"},{\"authorId\":\"97531942\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/cvpr42600.2020.01012\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7fa5fdaacc87165a0281babd96949e8ec74d3a41\",\"title\":\"Modality Shifting Attention Network for Multi-Modal Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7fa5fdaacc87165a0281babd96949e8ec74d3a41\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2010.00361\",\"authors\":[{\"authorId\":\"7814221\",\"name\":\"Zipeng Xu\"},{\"authorId\":\"39825530\",\"name\":\"Fangxiang Feng\"},{\"authorId\":\"38542466\",\"name\":\"X. Wang\"},{\"authorId\":\"2590300\",\"name\":\"Y. Yang\"},{\"authorId\":\"2309680\",\"name\":\"Huixing Jiang\"},{\"authorId\":\"1978074381\",\"name\":\"Zhongyuan Ouyang\"}],\"doi\":\"10.1145/3394171.3413668\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"998a4a5e3343e46c3b1ef8095bb29b7a85bf45ce\",\"title\":\"Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/998a4a5e3343e46c3b1ef8095bb29b7a85bf45ce\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2001.06891\",\"authors\":[{\"authorId\":\"1742291\",\"name\":\"Zixing Zhang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"90148415\",\"name\":\"Yang Zhao\"},{\"authorId\":\"50621207\",\"name\":\"Q. Wang\"},{\"authorId\":\"46936306\",\"name\":\"H. Liu\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1109/cvpr42600.2020.01068\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a709bb1b4bce51de5aa67a362ea69e51a43d1e5\",\"title\":\"Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences\",\"url\":\"https://www.semanticscholar.org/paper/0a709bb1b4bce51de5aa67a362ea69e51a43d1e5\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490935563\",\"name\":\"Feifei Zhang\"},{\"authorId\":\"49235537\",\"name\":\"M. Xu\"},{\"authorId\":\"3069077\",\"name\":\"Q. Mao\"},{\"authorId\":\"48258806\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3394171.3413917\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"50bd8474487073851b115385b9ded538d8825bc2\",\"title\":\"Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/50bd8474487073851b115385b9ded538d8825bc2\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2003.00403\",\"authors\":[{\"authorId\":\"26907392\",\"name\":\"Zhenfang Chen\"},{\"authorId\":\"97992296\",\"name\":\"P. Wang\"},{\"authorId\":\"152309770\",\"name\":\"Lin Ma\"},{\"authorId\":\"1698116\",\"name\":\"K. Wong\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/cvpr42600.2020.01010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d04e67e967c272d8d831bb3acaa75fb29607d4c7\",\"title\":\"Cops-Ref: A New Dataset and Task on Compositional Referring Expression Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/d04e67e967c272d8d831bb3acaa75fb29607d4c7\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1812.03299\",\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1109/ICCV.2019.00477\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e504843bfd3ca81b859895439bbe19efc1f3dc61\",\"title\":\"Learning to Assemble Neural Module Tree Networks for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/e504843bfd3ca81b859895439bbe19efc1f3dc61\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2011.02655\",\"authors\":[{\"authorId\":\"47297245\",\"name\":\"H. Zhu\"},{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"29962444\",\"name\":\"Zhao-Heng Zheng\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cb961c8bea36d5faf3c6011f15dc23832a3e8fc\",\"title\":\"Utilizing Every Image Object for Semi-supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/1cb961c8bea36d5faf3c6011f15dc23832a3e8fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35137594\",\"name\":\"Jinpeng Mi\"},{\"authorId\":\"1939598\",\"name\":\"Jian-Zhi Lyu\"},{\"authorId\":\"1739175813\",\"name\":\"Song Tang\"},{\"authorId\":\"153082688\",\"name\":\"Qingdu Li\"},{\"authorId\":\"50561627\",\"name\":\"J. Zhang\"}],\"doi\":\"10.3389/fnbot.2020.00043\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ceb666a47c683b0555f8976e7d2a0f15f9cf8be2\",\"title\":\"Interactive Natural Language Grounding via Referring Expression Comprehension and Scene Graph Parsing\",\"url\":\"https://www.semanticscholar.org/paper/ceb666a47c683b0555f8976e7d2a0f15f9cf8be2\",\"venue\":\"Frontiers in Neurorobotics\",\"year\":2020},{\"arxivId\":\"2002.10340\",\"authors\":[{\"authorId\":\"144052839\",\"name\":\"Wei Pang\"},{\"authorId\":\"50142157\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-58517-4_40\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b7ba12958cb670442236accd5e0579728821ad7d\",\"title\":\"Guessing State Tracking for Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/b7ba12958cb670442236accd5e0579728821ad7d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1906.03561\",\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"32222907\",\"name\":\"Qianru Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d84db9d0d21b0eae8cad741d16a74e7880c711c\",\"title\":\"Joint Visual Grounding with Language Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/3d84db9d0d21b0eae8cad741d16a74e7880c711c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32714395\",\"name\":\"Kennedy Hahn\"}],\"doi\":\"10.15760/HONORS.797\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e4763e53b80a9a047330d26363c916cde8927a3\",\"title\":\"Analyzing the Visual Grounding of \\\"Referring Relationships\\\"\",\"url\":\"https://www.semanticscholar.org/paper/0e4763e53b80a9a047330d26363c916cde8927a3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"8277017\",\"name\":\"Q. Wu\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1109/TIP.2019.2922108\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff4f13d4973cfe74f8ac7ef8384548c22284011e\",\"title\":\"Breaking Winner-Takes-All: Iterative-Winners-Out Networks for Weakly Supervised Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/ff4f13d4973cfe74f8ac7ef8384548c22284011e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":null,\"name\":\"Feng Wu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"bf2f9f9ebe16b2574ad6de69f5806e5d44c14217\",\"title\":\"Explainability by Parsing: Neural Module Tree Networks for Natural Language Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/bf2f9f9ebe16b2574ad6de69f5806e5d44c14217\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1912.04554\",\"authors\":[{\"authorId\":\"33305173\",\"name\":\"P. Purkait\"},{\"authorId\":\"72017275\",\"name\":\"C. Zach\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"}],\"doi\":\"10.1007/978-3-030-58586-0_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6a52fd650eecfc28948cce4dcf3e1d3aab70466\",\"title\":\"SG-VAE: Scene Grammar Variational Autoencoder to Generate New Indoor Scenes\",\"url\":\"https://www.semanticscholar.org/paper/b6a52fd650eecfc28948cce4dcf3e1d3aab70466\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66125335\",\"name\":\"Heqian Qiu\"},{\"authorId\":\"30548955\",\"name\":\"Hongliang Li\"},{\"authorId\":\"1702864\",\"name\":\"Q. Wu\"},{\"authorId\":\"1706784\",\"name\":\"Fanman Meng\"},{\"authorId\":\"3482320\",\"name\":\"Hengcan Shi\"},{\"authorId\":\"1993661016\",\"name\":\"Taijin Zhao\"},{\"authorId\":\"1684869\",\"name\":\"K. N. Ngan\"}],\"doi\":\"10.1145/3394171.3413850\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ee97b258b907a8859a67014d1bdb8bf3fcc4894\",\"title\":\"Language-Aware Fine-Grained Object Representation for Referring Expression Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7ee97b258b907a8859a67014d1bdb8bf3fcc4894\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48251796\",\"name\":\"Chenyun Wu\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"153500661\",\"name\":\"S. Cohen\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"}],\"doi\":\"10.1109/CVPR42600.2020.01023\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1ebc58f6c055eac8555d9850cb849a38c22c67d\",\"title\":\"PhraseCut: Language-Based Image Segmentation in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/a1ebc58f6c055eac8555d9850cb849a38c22c67d\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1812.04794\",\"authors\":[{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"1715610\",\"name\":\"Qi Wu\"},{\"authorId\":\"39767248\",\"name\":\"Jiewei Cao\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2019.00206\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ca91ad7763be4da05238aa17a9e5628f619dc0b\",\"title\":\"Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/8ca91ad7763be4da05238aa17a9e5628f619dc0b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3144952\",\"name\":\"Sibei Yang\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"66616425\",\"name\":\"Yizhou Yu\"}],\"doi\":\"10.1109/CVPR.2019.00427\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1db245d9d2873ed433774f55b74e5b0274a71bd8\",\"title\":\"Cross-Modal Relationship Inference for Grounding Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/1db245d9d2873ed433774f55b74e5b0274a71bd8\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48289232\",\"name\":\"Mohit Bajaj\"}],\"doi\":\"10.14288/1.0380482\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef8832a4cc4d1838763df8dc7580e14706547f5a\",\"title\":\"Graph-based language grounding\",\"url\":\"https://www.semanticscholar.org/paper/ef8832a4cc4d1838763df8dc7580e14706547f5a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.06941\",\"authors\":[{\"authorId\":\"47294375\",\"name\":\"Zhu Zhang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"49290494\",\"name\":\"Zhijie Lin\"},{\"authorId\":\"2422046\",\"name\":\"Baoxing Huai\"},{\"authorId\":\"145078771\",\"name\":\"J. Yuan\"}],\"doi\":\"10.24963/ijcai.2020/149\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2086bd08d3921bc1e83fdad306ec0c5ac3b43428\",\"title\":\"Object-Aware Multi-Branch Relation Networks for Spatio-Temporal Video Grounding\",\"url\":\"https://www.semanticscholar.org/paper/2086bd08d3921bc1e83fdad306ec0c5ac3b43428\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49941780\",\"name\":\"Zhiwei Hu\"},{\"authorId\":\"49081953\",\"name\":\"Guang Feng\"},{\"authorId\":\"1491083353\",\"name\":\"Jiayu Sun\"},{\"authorId\":\"47059008\",\"name\":\"L. Zhang\"},{\"authorId\":\"153176123\",\"name\":\"Huchuan Lu\"}],\"doi\":\"10.1109/cvpr42600.2020.00448\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"31422b7cbcabe22d24975129fafe9a3258051b2d\",\"title\":\"Bi-Directional Relationship Inferring Network for Referring Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/31422b7cbcabe22d24975129fafe9a3258051b2d\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1485630780\",\"name\":\"Yanbin Hao\"},{\"authorId\":\"145063759\",\"name\":\"Hao Zhang\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"},{\"authorId\":\"48874063\",\"name\":\"Q. Liu\"},{\"authorId\":\"144635784\",\"name\":\"X. Hu\"}],\"doi\":\"10.1145/3394171.3413595\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1e29826ff34cc39f04d23784ad8ec6649717e1ce\",\"title\":\"Compact Bilinear Augmented Query Structured Attention for Sport Highlights Classification\",\"url\":\"https://www.semanticscholar.org/paper/1e29826ff34cc39f04d23784ad8ec6649717e1ce\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2003.10663\",\"authors\":[{\"authorId\":\"2598007\",\"name\":\"Zhongguo Li\"},{\"authorId\":\"12358136\",\"name\":\"Fan Lyu\"},{\"authorId\":\"1409949029\",\"name\":\"W. Feng\"},{\"authorId\":\"40696794\",\"name\":\"Song Wang\"}],\"doi\":\"10.1109/ICME46284.2020.9102717\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d706c38381ec095c8e456b0b0387ae212e0581be\",\"title\":\"Modeling Cross-View Interaction Consistency for Paired Egocentric Interaction Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d706c38381ec095c8e456b0b0387ae212e0581be\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"2008.01403\",\"authors\":[{\"authorId\":\"98579574\",\"name\":\"Daizong Liu\"},{\"authorId\":\"51912474\",\"name\":\"Xiaoye Qu\"},{\"authorId\":\"4029028\",\"name\":\"Xiao-Yang Liu\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"},{\"authorId\":\"2956815\",\"name\":\"Zichuan Xu\"}],\"doi\":\"10.1145/3394171.3414026\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2b632209923bfe3452ac19f23b46c70455fae465\",\"title\":\"Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization\",\"url\":\"https://www.semanticscholar.org/paper/2b632209923bfe3452ac19f23b46c70455fae465\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"11179553\",\"name\":\"Zhigang Lin\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"},{\"authorId\":\"40286455\",\"name\":\"Xuelong Li\"}],\"doi\":\"10.1016/j.patcog.2020.107359\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"124f42e4787a81eba2fc6311b11a3afec60c4f57\",\"title\":\"Stacked squeeze-and-excitation recurrent residual network for visual-semantic matching\",\"url\":\"https://www.semanticscholar.org/paper/124f42e4787a81eba2fc6311b11a3afec60c4f57\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":\"2009.06066\",\"authors\":[{\"authorId\":\"24661938\",\"name\":\"N. Rufus\"},{\"authorId\":\"30900327\",\"name\":\"U. Nair\"},{\"authorId\":\"145211574\",\"name\":\"K. Krishna\"},{\"authorId\":\"145091336\",\"name\":\"V. Gandhi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f6887b5680d0041f3bea85bb85a2d9f77dd41067\",\"title\":\"Cosine meets Softmax: A tough-to-beat baseline for visual grounding\",\"url\":\"https://www.semanticscholar.org/paper/f6887b5680d0041f3bea85bb85a2d9f77dd41067\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.05010\",\"authors\":[{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"}],\"doi\":\"10.1609/AAAI.V34I07.6897\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa6c925c5f2fe61cdf8e09dd4ecbf1e7d220eac0\",\"title\":\"Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction\",\"url\":\"https://www.semanticscholar.org/paper/aa6c925c5f2fe61cdf8e09dd4ecbf1e7d220eac0\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1902.09326\",\"authors\":[{\"authorId\":\"49876189\",\"name\":\"T. Yang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d83f41d394cb23a87685d7a69bac42a6e86a4641\",\"title\":\"Making History Matter: Gold-Critic Sequence Training for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/d83f41d394cb23a87685d7a69bac42a6e86a4641\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98531843\",\"name\":\"Tianyu Yu\"},{\"authorId\":\"151475424\",\"name\":\"Tianrui Hui\"},{\"authorId\":\"30972874\",\"name\":\"Z. Yu\"},{\"authorId\":\"47303356\",\"name\":\"Yue Liao\"},{\"authorId\":\"1977587202\",\"name\":\"Sansi Yu\"},{\"authorId\":\"1977074324\",\"name\":\"Faxi Zhang\"},{\"authorId\":\"153318526\",\"name\":\"Si Liu\"}],\"doi\":\"10.1145/3394171.3413846\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"136de72fdb9498b20fbb438afb6567aa03cfa6a1\",\"title\":\"Cross-Modal Omni Interaction Modeling for Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/136de72fdb9498b20fbb438afb6567aa03cfa6a1\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1908.07129\",\"authors\":[{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/ICCV.2019.00479\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"666d9a766de11c22d9fd7d8e3c36a80ecd6f0bd0\",\"title\":\"Zero-Shot Grounding of Objects From Natural Language Queries\",\"url\":\"https://www.semanticscholar.org/paper/666d9a766de11c22d9fd7d8e3c36a80ecd6f0bd0\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1906.01784\",\"authors\":[{\"authorId\":\"48043335\",\"name\":\"R. Hong\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"12086460\",\"name\":\"Xiaoyu Mo\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/TPAMI.2019.2911066\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2db3da6c2279c5a7e8238a63d5e79681e25c97cb\",\"title\":\"Learning to Compose and Reason with Language Tree Structures for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/2db3da6c2279c5a7e8238a63d5e79681e25c97cb\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2019},{\"arxivId\":\"2009.05684\",\"authors\":[{\"authorId\":\"46662193\",\"name\":\"V. Mittal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd6df4febb7135007077f835f9c44d12514aa522\",\"title\":\"AttnGrounder: Talking to Cars with Attention\",\"url\":\"https://www.semanticscholar.org/paper/dd6df4febb7135007077f835f9c44d12514aa522\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1903.00839\",\"authors\":[{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"50218594\",\"name\":\"Z. Wang\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1109/CVPR.2019.00205\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"cd961afa9d75e1e7a657a9e11d6f6d3b968282a0\",\"title\":\"Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing\",\"url\":\"https://www.semanticscholar.org/paper/cd961afa9d75e1e7a657a9e11d6f6d3b968282a0\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.04745\",\"authors\":[{\"authorId\":\"2373631\",\"name\":\"L. Ye\"},{\"authorId\":\"2532612\",\"name\":\"Mrigank Rochan\"},{\"authorId\":\"143962643\",\"name\":\"Zhi Liu\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":\"10.1109/CVPR.2019.01075\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69455376f5ad52cac5b72d5e8c6cf03fb466b55c\",\"title\":\"Cross-Modal Self-Attention Network for Referring Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/69455376f5ad52cac5b72d5e8c6cf03fb466b55c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1908.04107\",\"authors\":[{\"authorId\":\"23165772\",\"name\":\"Z. Yu\"},{\"authorId\":\"30513139\",\"name\":\"Yuhao Cui\"},{\"authorId\":\"97583812\",\"name\":\"J. Yu\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a3d3df29fc98e0d674bf02bab69726795f4c7b78\",\"title\":\"Multimodal Unified Attention Networks for Vision-and-Language Interactions\",\"url\":\"https://www.semanticscholar.org/paper/a3d3df29fc98e0d674bf02bab69726795f4c7b78\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2003.08027\",\"authors\":[{\"authorId\":\"39024831\",\"name\":\"Shuai Wang\"},{\"authorId\":\"12358136\",\"name\":\"Fan Lyu\"},{\"authorId\":\"1409949029\",\"name\":\"W. Feng\"},{\"authorId\":\"40912079\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/ICME46284.2020.9102714\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b65ffe7ada51e34a0a8e3e46b90d71099141a8e\",\"title\":\"Mutatt: Visual-Textual Mutual Guidance For Referring Expression Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/3b65ffe7ada51e34a0a8e3e46b90d71099141a8e\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"1906.00283\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"122175026\",\"name\":\"P\\u00e9ter Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"title\":\"Learning to Generate Grounded Image Captions without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2006.01629\",\"authors\":[{\"authorId\":\"145498819\",\"name\":\"P. Wang\"},{\"authorId\":\"47417383\",\"name\":\"Dongyang Liu\"},{\"authorId\":\"144462039\",\"name\":\"H. Li\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1145/3394171.3413905\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a08274632296de2f98d11180ce3e2b06776a3a0\",\"title\":\"Give Me Something to Eat: Referring Expression Comprehension with Commonsense Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/3a08274632296de2f98d11180ce3e2b06776a3a0\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12358136\",\"name\":\"Fan Lyu\"},{\"authorId\":\"1409949029\",\"name\":\"W. Feng\"},{\"authorId\":\"40696794\",\"name\":\"Song Wang\"}],\"doi\":\"10.1016/j.neucom.2020.06.091\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"a26f46dec34e640b6a6ccb69cc28eda3c1eaac0d\",\"title\":\"vtGraphNet: Learning weakly-supervised scene graph for complex visual grounding\",\"url\":\"https://www.semanticscholar.org/paper/a26f46dec34e640b6a6ccb69cc28eda3c1eaac0d\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"8799308\",\"name\":\"U. Nadeem\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1007/978-3-030-49724-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"title\":\"Vision to Language: Methods, Metrics and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66353591\",\"name\":\"Amaia Salvador Aguilera\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4cb1819c27e207a2afeccfe450b931cc317de1e1\",\"title\":\"Computer vision beyond the visible : image understanding through language\",\"url\":\"https://www.semanticscholar.org/paper/4cb1819c27e207a2afeccfe450b931cc317de1e1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48289232\",\"name\":\"Mohit Bajaj\"},{\"authorId\":\"49680751\",\"name\":\"Lanjun Wang\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00438\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a7bb3c251498a7c700c5f0563a53aea54345653\",\"title\":\"G3raphGround: Graph-Based Language Grounding\",\"url\":\"https://www.semanticscholar.org/paper/0a7bb3c251498a7c700c5f0563a53aea54345653\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1902.04213\",\"authors\":[{\"authorId\":\"41036094\",\"name\":\"C. Deng\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"49560222\",\"name\":\"Guanghui Xu\"},{\"authorId\":\"153009573\",\"name\":\"Z. Yu\"},{\"authorId\":null,\"name\":\"Yanwu Xu\"},{\"authorId\":\"2370507\",\"name\":\"K. Jia\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8b54f188af9e71f5790f10c406d888ea006387d\",\"title\":\"You Only Look & Listen Once: Towards Fast and Accurate Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/d8b54f188af9e71f5790f10c406d888ea006387d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112897627\",\"name\":\"Taichi Iki\"},{\"authorId\":\"1705519\",\"name\":\"A. Aizawa\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.420\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d94ebc1c92870d6d21a2ad4a2402be86ecf36309\",\"title\":\"Language-Conditioned Feature Pyramids for Visual Selection Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d94ebc1c92870d6d21a2ad4a2402be86ecf36309\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1909.08164\",\"authors\":[{\"authorId\":\"3144952\",\"name\":\"Sibei Yang\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"66616425\",\"name\":\"Yizhou Yu\"}],\"doi\":\"10.1109/ICCV.2019.00474\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c8b8a2d61cdd9592faebb6aca262ec3a64e6d43\",\"title\":\"Dynamic Graph Attention for Referring Expression Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/6c8b8a2d61cdd9592faebb6aca262ec3a64e6d43\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1912.04554\",\"authors\":[{\"authorId\":\"33305173\",\"name\":\"P. Purkait\"},{\"authorId\":\"72017275\",\"name\":\"C. Zach\"},{\"authorId\":\"152729539\",\"name\":\"I. Reid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d3fb5edef50e11351459e6f989c70fa7b45c69f2\",\"title\":\"Learning to generate new indoor scenes\",\"url\":\"https://www.semanticscholar.org/paper/d3fb5edef50e11351459e6f989c70fa7b45c69f2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.04464\",\"authors\":[{\"authorId\":\"3144952\",\"name\":\"Sibei Yang\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"66616425\",\"name\":\"Yizhou Yu\"}],\"doi\":\"10.1109/tpami.2020.2973983\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"14204fd579ad2802490a8f4a781392ca7ba19c80\",\"title\":\"Relationship-Embedded Representation Learning for Grounding Referring Expressions.\",\"url\":\"https://www.semanticscholar.org/paper/14204fd579ad2802490a8f4a781392ca7ba19c80\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1811.11683\",\"authors\":[{\"authorId\":\"145559125\",\"name\":\"H. Akbari\"},{\"authorId\":\"35862299\",\"name\":\"S. Karaman\"},{\"authorId\":\"1754397\",\"name\":\"Surabhi Bhargava\"},{\"authorId\":\"143852868\",\"name\":\"B. Chen\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2019.01276\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4702bad26563a4e1d03b33c0af0b29b095438bc\",\"title\":\"Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/b4702bad26563a4e1d03b33c0af0b29b095438bc\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"39698901\",\"name\":\"H. Wang\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"48957872\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/TIP.2020.2992888\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"de847c41c0b6c52e5fc770f364a19fed04a7b3ae\",\"title\":\"Textual-Visual Reference-Aware Attention Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/de847c41c0b6c52e5fc770f364a19fed04a7b3ae\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100818163\",\"name\":\"Chen-chen Jing\"},{\"authorId\":\"145558278\",\"name\":\"Y. Wu\"},{\"authorId\":\"144315453\",\"name\":\"Mingtao Pei\"},{\"authorId\":\"46972595\",\"name\":\"Yao Hu\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"1720738301\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1145/3394171.3413902\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc24a5a8d6629eec5c8d87d48ab891bc17e862ff\",\"title\":\"Visual-Semantic Graph Matching for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/cc24a5a8d6629eec5c8d87d48ab891bc17e862ff\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1904.10151\",\"authors\":[{\"authorId\":\"35653798\",\"name\":\"Yuankai Qi\"},{\"authorId\":\"1715610\",\"name\":\"Qi Wu\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"39471238\",\"name\":\"M. Liu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3824bd35badf05e05cb6cbd22e2cf2b955a2734d\",\"title\":\"RERERE: Remote Embodied Referring Expressions in Real indoor Environments\",\"url\":\"https://www.semanticscholar.org/paper/3824bd35badf05e05cb6cbd22e2cf2b955a2734d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49876189\",\"name\":\"T. Yang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/ICCV.2019.00265\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ecc5cd01261cf9c396689121a3e8c1844c825775\",\"title\":\"Making History Matter: History-Advantage Sequence Training for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/ecc5cd01261cf9c396689121a3e8c1844c825775\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2008.01392\",\"authors\":[{\"authorId\":\"1413822807\",\"name\":\"Mert Bulent Sariyildiz\"},{\"authorId\":\"144781195\",\"name\":\"J. Perez\"},{\"authorId\":\"2295553\",\"name\":\"Diane Larlus\"}],\"doi\":\"10.1007/978-3-030-58598-3_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0181353f32b1ad3ac6bc59838c69b0e5c64137a\",\"title\":\"Learning Visual Representations with Caption Annotations\",\"url\":\"https://www.semanticscholar.org/paper/b0181353f32b1ad3ac6bc59838c69b0e5c64137a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35653798\",\"name\":\"Yuankai Qi\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"},{\"authorId\":\"153149395\",\"name\":\"P. Anderson\"},{\"authorId\":\"38218215\",\"name\":\"Xin Wang\"},{\"authorId\":\"49336516\",\"name\":\"W. Wang\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/cvpr42600.2020.01000\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd8ad5cc71f8108177206fa4844c8d06dd57cdc0\",\"title\":\"REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments\",\"url\":\"https://www.semanticscholar.org/paper/fd8ad5cc71f8108177206fa4844c8d06dd57cdc0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"34413657\",\"name\":\"G. Shen\"},{\"authorId\":\"51026885\",\"name\":\"W. Huang\"},{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1109/TMM.2019.2959977\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"320d212dd25dfdec58ac6d9f60198eb893d3744e\",\"title\":\"Relation Attention for Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/320d212dd25dfdec58ac6d9f60198eb893d3744e\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1910.11475\",\"authors\":[{\"authorId\":\"48729196\",\"name\":\"Weijiang Yu\"},{\"authorId\":\"150167685\",\"name\":\"Jingwen Zhou\"},{\"authorId\":\"23476952\",\"name\":\"Weihao Yu\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"1730284\",\"name\":\"N. Xiao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef318e7ff0883e72d853c75736d20cc123b556d5\",\"title\":\"Heterogeneous Graph Learning for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/ef318e7ff0883e72d853c75736d20cc123b556d5\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"title\":\"Efficient Attention Mechanism for Handling All the Interactions between Many Inputs with Application to Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.09554\",\"authors\":[{\"authorId\":\"80526284\",\"name\":\"Yanyuan Qiao\"},{\"authorId\":\"41036094\",\"name\":\"C. Deng\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/tmm.2020.3042066\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9d8398d9c36d25ad1363d9a5dd922dbbfa7be3be\",\"title\":\"Referring Expression Comprehension: A Survey of Methods and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/9d8398d9c36d25ad1363d9a5dd922dbbfa7be3be\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144288475\",\"name\":\"Yufei Feng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d037e105412a7edc93e97c4ea0abd209ec23795e\",\"title\":\"Learning Capsule Networks with Images and Text\",\"url\":\"https://www.semanticscholar.org/paper/d037e105412a7edc93e97c4ea0abd209ec23795e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1907.03609\",\"authors\":[{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1776220\",\"name\":\"Zhiwu Lu\"},{\"authorId\":\"70351911\",\"name\":\"Shih-Fu Chang\"}],\"doi\":\"10.1109/TPAMI.2019.2926266\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"df9c0a59bdbd54be299ca63b60773eed62e2c611\",\"title\":\"Variational Context: Exploiting Visual and Textual Context for Grounding Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/df9c0a59bdbd54be299ca63b60773eed62e2c611\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2021},{\"arxivId\":\"2011.10972\",\"authors\":[{\"authorId\":\"49039823\",\"name\":\"Weixia Zhang\"},{\"authorId\":\"1409866378\",\"name\":\"Chao Ma\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"15469693\",\"name\":\"Xiaokang Yang\"}],\"doi\":\"10.1109/TCSVT.2020.3039522\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"38bb24348dbcec08285a8670596ec7c9b3895603\",\"title\":\"Language-guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/38bb24348dbcec08285a8670596ec7c9b3895603\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.03252\",\"authors\":[{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"145913039\",\"name\":\"Y. Rong\"},{\"authorId\":\"144259957\",\"name\":\"P. Zhao\"},{\"authorId\":\"1768190\",\"name\":\"J. Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1109/ICCV.2019.00719\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd8e725159159ca2169d302f6cb510e3b1cc1a4b\",\"title\":\"Graph Convolutional Networks for Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/fd8e725159159ca2169d302f6cb510e3b1cc1a4b\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"48481929\",\"name\":\"Yi-jun Song\"},{\"authorId\":\"47891191\",\"name\":\"Jun Yu\"},{\"authorId\":\"48957872\",\"name\":\"Meng Wang\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1007/s11063-020-10205-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c1d607b58d31c582bee0cc78017909a1eed45c9\",\"title\":\"Intra- and Inter-modal Multilinear Pooling with Multitask Learning for Video Grounding\",\"url\":\"https://www.semanticscholar.org/paper/6c1d607b58d31c582bee0cc78017909a1eed45c9\",\"venue\":\"Neural Processing Letters\",\"year\":2020},{\"arxivId\":\"2012.05710\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"title\":\"Look Before you Speak: Visually Contextualized Utterances\",\"url\":\"https://www.semanticscholar.org/paper/b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.03776\",\"authors\":[{\"authorId\":\"3456962\",\"name\":\"Amar Shrestha\"},{\"authorId\":\"9091383\",\"name\":\"Krittaphat Pugdeethosapol\"},{\"authorId\":\"122851204\",\"name\":\"Haowen Fang\"},{\"authorId\":\"1862322\",\"name\":\"Q. Qiu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"032a517f0c7e27a5fa95522c49de423633036b5f\",\"title\":\"MAGNet: Multi-Region Attention-Assisted Grounding of Natural Language Queries at Phrase Level\",\"url\":\"https://www.semanticscholar.org/paper/032a517f0c7e27a5fa95522c49de423633036b5f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.07236\",\"authors\":[{\"authorId\":\"12358136\",\"name\":\"Fan Lyu\"},{\"authorId\":\"39024831\",\"name\":\"Shuai Wang\"},{\"authorId\":\"2031553111\",\"name\":\"Wei Feng\"},{\"authorId\":\"12068340\",\"name\":\"Zihan Ye\"},{\"authorId\":\"2348236\",\"name\":\"Fuyuan Hu\"},{\"authorId\":\"40912079\",\"name\":\"S. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22e06fa904f2022b63954a08543df4a2da63059e\",\"title\":\"Multi-Domain Multi-Task Rehearsal for Lifelong Learning\",\"url\":\"https://www.semanticscholar.org/paper/22e06fa904f2022b63954a08543df4a2da63059e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.11390\",\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1007/978-3-030-58586-0_14\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"468d5c15df63892ff06fb94c7b5cad0242685d02\",\"title\":\"Efficient Attention Mechanism for Visual Dialog that Can Handle All the Interactions Between Multiple Inputs\",\"url\":\"https://www.semanticscholar.org/paper/468d5c15df63892ff06fb94c7b5cad0242685d02\",\"venue\":\"ECCV\",\"year\":2020}],\"corpusId\":52837841,\"doi\":\"10.1109/CVPR.2018.00808\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":9,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"8d1c8dd0559642a3bdd5c7234d2ce4611e911e23\",\"references\":[{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1608.00525\",\"authors\":[{\"authorId\":\"3081378\",\"name\":\"Varun K. Nagaraja\"},{\"authorId\":\"2852035\",\"name\":\"V. Morariu\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1007/978-3-319-46493-0_48\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86eef3a1dff2bd2808847358cdb7f5ba2b7e0214\",\"title\":\"Modeling Context Between Objects for Referring Expression Understanding\",\"url\":\"https://www.semanticscholar.org/paper/86eef3a1dff2bd2808847358cdb7f5ba2b7e0214\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2365442\",\"name\":\"B. Alexe\"},{\"authorId\":\"1879646\",\"name\":\"Thomas Deselaers\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"}],\"doi\":\"10.1109/TPAMI.2012.28\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2eb6caace8296fd4dfd4947efa4fe911c8e133b2\",\"title\":\"Measuring the Objectness of Image Windows\",\"url\":\"https://www.semanticscholar.org/paper/2eb6caace8296fd4dfd4947efa4fe911c8e133b2\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"}],\"doi\":\"10.1007/978-3-319-10602-1_26\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b183947ee15718b45546eda6b01e179b9a95421f\",\"title\":\"Edge Boxes: Locating Object Proposals from Edges\",\"url\":\"https://www.semanticscholar.org/paper/b183947ee15718b45546eda6b01e179b9a95421f\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1705.01371\",\"authors\":[{\"authorId\":\"2299381\",\"name\":\"Fanyi Xiao\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144756076\",\"name\":\"Y. Lee\"}],\"doi\":\"10.1109/CVPR.2017.558\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9405a9180139f23f4dd9d90aa4e86944b35b8c88\",\"title\":\"Weakly-Supervised Visual Grounding of Phrases with Linguistic Structures\",\"url\":\"https://www.semanticscholar.org/paper/9405a9180139f23f4dd9d90aa4e86944b35b8c88\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":\"1506.01144\",\"authors\":[{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2016.29\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"title\":\"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?\",\"url\":\"https://www.semanticscholar.org/paper/00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/CVPR.2017.108\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"561ed7e47524fb3218e6a38f41cd877a9c33d3b9\",\"title\":\"StyleNet: Generating Attractive Visual Captions with Styles\",\"url\":\"https://www.semanticscholar.org/paper/561ed7e47524fb3218e6a38f41cd877a9c33d3b9\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1606.01847\",\"authors\":[{\"authorId\":\"50599725\",\"name\":\"A. Fukui\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"3422876\",\"name\":\"Daylen Yang\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D16-1044\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fddc15480d086629b960be5bff96232f967f2252\",\"title\":\"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/fddc15480d086629b960be5bff96232f967f2252\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1612.09542\",\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"47300698\",\"name\":\"Hao Tan\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/CVPR.2017.375\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a5b64709c677c131ec8b7846d3493df53987fa6f\",\"title\":\"A Joint Speaker-Listener-Reinforcer Model for Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/a5b64709c677c131ec8b7846d3493df53987fa6f\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1612.05386\",\"authors\":[{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2017.416\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7d52fa1a9021d3596930ad2d5121e9d125113ab2\",\"title\":\"The VQA-Machine: Learning How to Use Existing Vision Algorithms to Answer New Questions\",\"url\":\"https://www.semanticscholar.org/paper/7d52fa1a9021d3596930ad2d5121e9d125113ab2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1606.00061\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"145743311\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b\",\"title\":\"Hierarchical Question-Image Co-Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1511.04164\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"3286703\",\"name\":\"Huazhe Xu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPR.2016.493\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d696a1923288e6c15422660de9553f6fdb6a4fae\",\"title\":\"Natural Language Object Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d696a1923288e6c15422660de9553f6fdb6a4fae\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.03745\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-46448-0_49\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"14c2321851fb5ae580a19726dd2753a525d6ad76\",\"title\":\"Grounding of Textual Phrases in Images by Reconstruction\",\"url\":\"https://www.semanticscholar.org/paper/14c2321851fb5ae580a19726dd2753a525d6ad76\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1506.03340\",\"authors\":[{\"authorId\":\"2910877\",\"name\":\"K. Hermann\"},{\"authorId\":\"2367821\",\"name\":\"Tom\\u00e1s Kocisk\\u00fd\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"2311318\",\"name\":\"Lasse Espeholt\"},{\"authorId\":\"21028601\",\"name\":\"W. Kay\"},{\"authorId\":\"2573615\",\"name\":\"Mustafa Suleyman\"},{\"authorId\":\"1685771\",\"name\":\"P. Blunsom\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1505c6123c102e53eb19dff312cb25cea840b72\",\"title\":\"Teaching Machines to Read and Comprehend\",\"url\":\"https://www.semanticscholar.org/paper/d1505c6123c102e53eb19dff312cb25cea840b72\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1503.04069\",\"authors\":[{\"authorId\":\"3035541\",\"name\":\"Klaus Greff\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"2865775\",\"name\":\"J. Koutn\\u00edk\"},{\"authorId\":\"71009239\",\"name\":\"Bastiaan Steunebrink\"},{\"authorId\":\"48974230\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/tnnls.2016.2582924\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a7976c2bacfbb194ddbe7fd10c2e50a545cf4081\",\"title\":\"LSTM: A Search Space Odyssey\",\"url\":\"https://www.semanticscholar.org/paper/a7976c2bacfbb194ddbe7fd10c2e50a545cf4081\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2017},{\"arxivId\":\"1506.01497\",\"authors\":[{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"144716314\",\"name\":\"J. Sun\"}],\"doi\":\"10.1109/TPAMI.2016.2577031\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"title\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3006072\",\"name\":\"Sahar Kazemzadeh\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"32215032\",\"name\":\"Mark Matten\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.3115/v1/D14-1086\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"92c141447f51b6732242376164ff961e464731c8\",\"title\":\"ReferItGame: Referring to Objects in Photographs of Natural Scenes\",\"url\":\"https://www.semanticscholar.org/paper/92c141447f51b6732242376164ff961e464731c8\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1701.03439\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"2490189\",\"name\":\"Gregory Shakhnarovich\"}],\"doi\":\"10.1109/CVPR.2017.333\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9b45e9a40313096abf530df3b98a1dfa1553f17b\",\"title\":\"Comprehension-Guided Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/9b45e9a40313096abf530df3b98a1dfa1553f17b\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1505.01121\",\"authors\":[{\"authorId\":\"145478807\",\"name\":\"Mateusz Malinowski\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"}],\"doi\":\"10.1109/ICCV.2015.9\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bd7bd1d2945a58cdcc1797ba9698b8810fe68f60\",\"title\":\"Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images\",\"url\":\"https://www.semanticscholar.org/paper/bd7bd1d2945a58cdcc1797ba9698b8810fe68f60\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1603.02814\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"121177698\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/TPAMI.2017.2708709\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"1603.03925\",\"authors\":[{\"authorId\":\"36610242\",\"name\":\"Quanzeng You\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.503\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bf55591e09b58ea9ce8d66110d6d3000ee804bdd\",\"title\":\"Image Captioning with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1608.00272\",\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"3451188\",\"name\":\"Patrick Poirson\"},{\"authorId\":\"144947353\",\"name\":\"S. Yang\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1007/978-3-319-46475-6_5\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"29efbe391950ae438c63d86ad5c82b2942efb0b4\",\"title\":\"Modeling Context in Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/29efbe391950ae438c63d86ad5c82b2942efb0b4\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1711.07613\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00639\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9dde6ed569684356c46217fa53224272b668bae8\",\"title\":\"Are You Talking to Me? Reasoned Visual Dialog Generation Through Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/9dde6ed569684356c46217fa53224272b668bae8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1511.02283\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"3317152\",\"name\":\"Oana-Maria Camburu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/CVPR.2016.9\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e65142010431ffc089b272a1174214e00693e503\",\"title\":\"Generation and Comprehension of Unambiguous Object Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e65142010431ffc089b272a1174214e00693e503\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1611.08481\",\"authors\":[{\"authorId\":\"153559313\",\"name\":\"H. D. Vries\"},{\"authorId\":\"3367628\",\"name\":\"Florian Strub\"},{\"authorId\":\"144631588\",\"name\":\"A. Chandar\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/CVPR.2017.475\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"bed7834ae7d371171977a590872f60d137c2f951\",\"title\":\"GuessWhat?! Visual Object Discovery through Multi-modal Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/bed7834ae7d371171977a590872f60d137c2f951\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1301.3781\",\"authors\":[{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"38644044\",\"name\":\"Kai Chen\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"330da625c15427c6e42ccfa3b747fb29e5835bf0\",\"title\":\"Efficient Estimation of Word Representations in Vector Space\",\"url\":\"https://www.semanticscholar.org/paper/330da625c15427c6e42ccfa3b747fb29e5835bf0\",\"venue\":\"ICLR\",\"year\":2013}],\"title\":\"Visual Grounding via Accumulated Attention\",\"topics\":[{\"topic\":\"Natural language user interface\",\"topicId\":\"273007\",\"url\":\"https://www.semanticscholar.org/topic/273007\"},{\"topic\":\"Tree accumulation\",\"topicId\":\"2611275\",\"url\":\"https://www.semanticscholar.org/topic/2611275\"},{\"topic\":\"Redundancy (information theory)\",\"topicId\":\"325559\",\"url\":\"https://www.semanticscholar.org/topic/325559\"},{\"topic\":\"Information source\",\"topicId\":\"56773\",\"url\":\"https://www.semanticscholar.org/topic/56773\"},{\"topic\":\"Open research\",\"topicId\":\"1298\",\"url\":\"https://www.semanticscholar.org/topic/1298\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Database\",\"topicId\":\"1307\",\"url\":\"https://www.semanticscholar.org/topic/1307\"},{\"topic\":\"LVM\",\"topicId\":\"111895\",\"url\":\"https://www.semanticscholar.org/topic/111895\"},{\"topic\":\"Flight dynamics (fixed-wing aircraft)\",\"topicId\":\"426739\",\"url\":\"https://www.semanticscholar.org/topic/426739\"},{\"topic\":\"Microsoft Customer Care Framework\",\"topicId\":\"3901872\",\"url\":\"https://www.semanticscholar.org/topic/3901872\"},{\"topic\":\"Circular polarization\",\"topicId\":\"32217\",\"url\":\"https://www.semanticscholar.org/topic/32217\"},{\"topic\":\"Multiply\\u2013accumulate operation\",\"topicId\":\"408575\",\"url\":\"https://www.semanticscholar.org/topic/408575\"}],\"url\":\"https://www.semanticscholar.org/paper/8d1c8dd0559642a3bdd5c7234d2ce4611e911e23\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"