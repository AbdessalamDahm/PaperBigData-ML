"{\"abstract\":\"Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.\",\"arxivId\":\"1707.07998\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\",\"url\":\"https://www.semanticscholar.org/author/6965856\"},{\"authorId\":\"144137069\",\"name\":\"X. He\",\"url\":\"https://www.semanticscholar.org/author/144137069\"},{\"authorId\":\"31790073\",\"name\":\"C. Buehler\",\"url\":\"https://www.semanticscholar.org/author/31790073\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\",\"url\":\"https://www.semanticscholar.org/author/2406263\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\",\"url\":\"https://www.semanticscholar.org/author/145177220\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\",\"url\":\"https://www.semanticscholar.org/author/145273587\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\",\"url\":\"https://www.semanticscholar.org/author/39089563\"}],\"citationVelocity\":401,\"citations\":[{\"arxivId\":\"2007.04422\",\"authors\":[{\"authorId\":\"1802508687\",\"name\":\"Vatsal Goel\"},{\"authorId\":\"1802505447\",\"name\":\"Mohit Chandak\"},{\"authorId\":\"47583481\",\"name\":\"A. Anand\"},{\"authorId\":\"46401518\",\"name\":\"Prithwijit Guha\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9adb9a076ae1817fbac0dd258bb4a72027456e6e\",\"title\":\"IQ-VQA: Intelligent Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/9adb9a076ae1817fbac0dd258bb4a72027456e6e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"33969294\",\"name\":\"K. Ding\"},{\"authorId\":\"1683738\",\"name\":\"S. Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.1109/TMM.2019.2915033\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bbb9f5378eac3eb8245cbd0f998a95cef2954508\",\"title\":\"Deep Hierarchical Encoder\\u2013Decoder Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bbb9f5378eac3eb8245cbd0f998a95cef2954508\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"144547315\",\"name\":\"E. Hovy\"}],\"doi\":\"10.1145/3372278.3390674\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4d5018c8bdf61129ca06a333ddbbe32dd84e50c2\",\"title\":\"Forward and Backward Multimodal NMT for Improved Monolingual and Multilingual Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/4d5018c8bdf61129ca06a333ddbbe32dd84e50c2\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89996604\",\"name\":\"J. Zou\"},{\"authorId\":\"152341461\",\"name\":\"Guoli Wu\"},{\"authorId\":\"104792066\",\"name\":\"Taofeng Xue\"},{\"authorId\":\"50528654\",\"name\":\"Qingfeng Wu\"}],\"doi\":\"10.1109/ICME46284.2020.9102911\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"afa1fc2ece0cfbdccbee9fa548ba68de45f56781\",\"title\":\"An Affinity-Driven Relation Network for Figure Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/afa1fc2ece0cfbdccbee9fa548ba68de45f56781\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"144654777\",\"name\":\"Ke Bai\"},{\"authorId\":\"46387857\",\"name\":\"Chenyang Tao\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1700522\",\"name\":\"Guoyin Wang\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"51030446\",\"name\":\"R. Henao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.1609/AAAI.V34I05.6249\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"826db2e5f340a90fc9672279f9e921b596aba4b7\",\"title\":\"Sequence Generation with Optimal-Transport-Enhanced Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/826db2e5f340a90fc9672279f9e921b596aba4b7\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48569838\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1109/TMM.2019.2896516\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bc73f0f7d895a089416bc8f6090f3f8707c6a12f\",\"title\":\"Know More Say Less: Image Captioning Based on Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/bc73f0f7d895a089416bc8f6090f3f8707c6a12f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"2004.08814\",\"authors\":[{\"authorId\":\"3144952\",\"name\":\"Sibei Yang\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"66616425\",\"name\":\"Yizhou Yu\"}],\"doi\":\"10.1109/cvpr42600.2020.00997\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"434bccb2743fbb918b5666000c839b390cb209ae\",\"title\":\"Graph-Structured Referring Expression Reasoning in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/434bccb2743fbb918b5666000c839b390cb209ae\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2005.06035\",\"authors\":[{\"authorId\":\"46882405\",\"name\":\"Chen Zheng\"},{\"authorId\":\"144919537\",\"name\":\"Quan Guo\"},{\"authorId\":\"2190934\",\"name\":\"Parisa Kordjamshidi\"}],\"doi\":\"10.18653/v1/2020.acl-main.683\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b293e4fd06e8a0f3ea7e46c4dafb575b288515e2\",\"title\":\"Cross-Modality Relevance for Reasoning on Language and Vision\",\"url\":\"https://www.semanticscholar.org/paper/b293e4fd06e8a0f3ea7e46c4dafb575b288515e2\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"31081539\",\"name\":\"Pengpeng Zeng\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"6820648\",\"name\":\"Xianglong Liu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3240508.3240687\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97add9744ae63c5e7af9d9861ecc18a2734d3f0c\",\"title\":\"Examine before You Answer: Multi-task Learning with Adaptive-attentions for Multiple-choice VQA\",\"url\":\"https://www.semanticscholar.org/paper/97add9744ae63c5e7af9d9861ecc18a2734d3f0c\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9358850\",\"name\":\"Ruifan Li\"},{\"authorId\":\"4189987\",\"name\":\"Haoyu Liang\"},{\"authorId\":\"46571714\",\"name\":\"Yihui Shi\"},{\"authorId\":\"39825530\",\"name\":\"Fangxiang Feng\"},{\"authorId\":\"39527132\",\"name\":\"Xiaojie Wang\"}],\"doi\":\"10.1016/j.neucom.2020.02.041\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f85b7e09e60315d725b316ffc813d20535b21b2\",\"title\":\"Dual-CNN: A Convolutional language decoder for paragraph image captioning\",\"url\":\"https://www.semanticscholar.org/paper/7f85b7e09e60315d725b316ffc813d20535b21b2\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144894837\",\"name\":\"F. Wu\"},{\"authorId\":\"49737751\",\"name\":\"Xiaoyuan Jing\"},{\"authorId\":\"50061551\",\"name\":\"Zhiyong Wu\"},{\"authorId\":\"3040064\",\"name\":\"Yi-mu Ji\"},{\"authorId\":\"2542968\",\"name\":\"Xiwei Dong\"},{\"authorId\":\"1399901101\",\"name\":\"Xiaokai Luo\"},{\"authorId\":\"1500377508\",\"name\":\"Qinghua Huang\"},{\"authorId\":\"72122954\",\"name\":\"Ruchuan Wang\"}],\"doi\":\"10.1016/j.patcog.2020.107335\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0bd1f36db3c85a5ea60a50f494059ad674e71dfe\",\"title\":\"Modality-specific and shared generative adversarial network for cross-modal retrieval\",\"url\":\"https://www.semanticscholar.org/paper/0bd1f36db3c85a5ea60a50f494059ad674e71dfe\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":\"2007.09049\",\"authors\":[{\"authorId\":\"1810689822\",\"name\":\"Ganchao Tan\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.24963/ijcai.2020/104\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"title\":\"Learning to Discretely Compose Reasoning Module Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"1811.10092\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1109/CVPR.2019.00679\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c66b8e508718f4b7f14829e5c2cde0add31d2693\",\"title\":\"Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/c66b8e508718f4b7f14829e5c2cde0add31d2693\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1910.02509\",\"authors\":[{\"authorId\":\"31449728\",\"name\":\"T. Hayes\"},{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"47309247\",\"name\":\"Manoj Acharya\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.1007/978-3-030-58598-3_28\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f6c3e8b8e6d647ebb61833e9105d2147e472f534\",\"title\":\"REMIND Your Neural Network to Prevent Catastrophic Forgetting\",\"url\":\"https://www.semanticscholar.org/paper/f6c3e8b8e6d647ebb61833e9105d2147e472f534\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1908.05407\",\"authors\":[{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3343031.3350996\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c1056e6e84d52cf45017aad544fa0406441abda0\",\"title\":\"Unpaired Cross-lingual Image Caption Generation with Self-Supervised Rewards\",\"url\":\"https://www.semanticscholar.org/paper/c1056e6e84d52cf45017aad544fa0406441abda0\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1904.08324\",\"authors\":[{\"authorId\":\"66599917\",\"name\":\"Yanze Wu\"},{\"authorId\":\"145268319\",\"name\":\"Q. Sun\"},{\"authorId\":\"3493380\",\"name\":\"Jianqi Ma\"},{\"authorId\":\"49729707\",\"name\":\"B. Li\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"145589539\",\"name\":\"Y. Peng\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"914c3045b4e140a93eace23eded09317072d3f42\",\"title\":\"Question Guided Modular Routing Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/914c3045b4e140a93eace23eded09317072d3f42\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2006.06195\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"50580345\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1431754650\",\"name\":\"C. Zhu\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8dc49a2041a8a269fbf64911a4f2c8cef6738a5c\",\"title\":\"Large-Scale Adversarial Training for Vision-and-Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/8dc49a2041a8a269fbf64911a4f2c8cef6738a5c\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15565853\",\"name\":\"Y. Wu\"},{\"authorId\":\"1697925\",\"name\":\"Kun Chen\"},{\"authorId\":\"1515440027\",\"name\":\"Ziyue Wang\"},{\"authorId\":\"1391222888\",\"name\":\"Xuan Zhang\"},{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"2319426\",\"name\":\"Shengchen Li\"},{\"authorId\":\"50425717\",\"name\":\"Xi Shao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"33dcbaa50f22fa82d26f174300fd67600c58b90a\",\"title\":\"AUDIO CAPTIONING BASED ON TRANSFORMER AND PRE-TRAINING FOR 2020 DCASE AUDIO CAPTIONING CHALLENGE Technical Report\",\"url\":\"https://www.semanticscholar.org/paper/33dcbaa50f22fa82d26f174300fd67600c58b90a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314360\",\"name\":\"Weixuan Wang\"},{\"authorId\":\"49865085\",\"name\":\"Zhihong Chen\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"}],\"doi\":\"10.1007/978-3-030-20876-9_37\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf3f2f2c1b88eb7b031d593c9d83641baea364e2\",\"title\":\"Multivariate Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf3f2f2c1b88eb7b031d593c9d83641baea364e2\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":\"1812.06624\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f3c111425a1e2f6316f01b9b0f3d09fa146b134\",\"title\":\"Feature Fusion Effects of Tensor Product Representation on (De)Compositional Network for Caption Generation for Images\",\"url\":\"https://www.semanticscholar.org/paper/5f3c111425a1e2f6316f01b9b0f3d09fa146b134\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.07075\",\"authors\":[{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"123813779\",\"name\":\"Lucas Van Bramer\"},{\"authorId\":\"1696401\",\"name\":\"Ajay Divakaran\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"418003534843f0fdda7dfb1d41c35bef683b1dad\",\"title\":\"Deep Unified Multimodal Embeddings for Understanding both Content and Users in Social Media Networks\",\"url\":\"https://www.semanticscholar.org/paper/418003534843f0fdda7dfb1d41c35bef683b1dad\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1907.09815\",\"authors\":[{\"authorId\":\"145422343\",\"name\":\"Dalu Guo\"},{\"authorId\":\"145371954\",\"name\":\"Chang Xu\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"87b60156ac40e9b1f404e785cec7aa7b4365a489\",\"title\":\"Graph Reasoning Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/87b60156ac40e9b1f404e785cec7aa7b4365a489\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67159407\",\"name\":\"S. Amirian\"},{\"authorId\":\"1438623667\",\"name\":\"Khaled Rasheed\"},{\"authorId\":\"1784399\",\"name\":\"T. Taha\"},{\"authorId\":\"1712033\",\"name\":\"H. Arabnia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"afa8032f794011884be0b06f808540f36b404c0b\",\"title\":\"A Short Review on Image Caption Generation with Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/afa8032f794011884be0b06f808540f36b404c0b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"19268185\",\"name\":\"Niange Yu\"},{\"authorId\":\"1798067\",\"name\":\"Cornelius Weber\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.1007/978-3-030-30484-3_24\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b76967585b30e1418cd147e2f21b94e586db2b4\",\"title\":\"Learning Sparse Hidden States in Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/4b76967585b30e1418cd147e2f21b94e586db2b4\",\"venue\":\"ICANN\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121961512\",\"name\":\"Yi-Ling Wu\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"2847159\",\"name\":\"Guoli Song\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1145/3343031.3350940\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1830b5c741aa93b417e15ca1e26e56dfe83d3ce9\",\"title\":\"Learning Fragment Self-Attention Embeddings for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/1830b5c741aa93b417e15ca1e26e56dfe83d3ce9\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50086111\",\"name\":\"K. Niu\"},{\"authorId\":\"39937384\",\"name\":\"Yan Huang\"},{\"authorId\":\"145769446\",\"name\":\"L. Wang\"}],\"doi\":\"10.1145/3394171.3413895\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"98b354ddfa46ccf9fc604c751eb9deebf4c909f8\",\"title\":\"Textual Dependency Embedding for Person Search by Language\",\"url\":\"https://www.semanticscholar.org/paper/98b354ddfa46ccf9fc604c751eb9deebf4c909f8\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"Changyou Chen\"},{\"authorId\":\"49469303\",\"name\":\"Xin-yuan Zhang\"},{\"authorId\":\"1423652601\",\"name\":\"Ke Bai\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a32535fb09e35a0f8e24e4d4e9bd33222a5258e1\",\"title\":\"Semantic Matching for Sequence-to-Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/a32535fb09e35a0f8e24e4d4e9bd33222a5258e1\",\"venue\":\"EMNLP 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2764826\",\"name\":\"H. Degen\"},{\"authorId\":\"1399251774\",\"name\":\"L. Reinerman-Jones\"},{\"authorId\":\"1743774\",\"name\":\"E. Bertino\"}],\"doi\":\"10.1007/978-3-030-50334-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e82d446b3bfcbded4a212bf25db36bea9b62dad7\",\"title\":\"Artificial Intelligence in HCI: First International Conference, AI-HCI 2020, Held as Part of the 22nd HCI International Conference, HCII 2020, Copenhagen, Denmark, July 19\\u201324, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/e82d446b3bfcbded4a212bf25db36bea9b62dad7\",\"venue\":\"HCI\",\"year\":2020},{\"arxivId\":\"1811.07212\",\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"143953573\",\"name\":\"K. Shih\"},{\"authorId\":\"121704643\",\"name\":\"Yichen Li\"},{\"authorId\":\"145031845\",\"name\":\"Ke Xu\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/TPAMI.2020.3029008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e0fbead3da9e16b03894b63efc6816fa3ef6d0e\",\"title\":\"Revisiting Image-Language Networks for Open-ended Phrase Detection.\",\"url\":\"https://www.semanticscholar.org/paper/0e0fbead3da9e16b03894b63efc6816fa3ef6d0e\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"2009.14558\",\"authors\":[{\"authorId\":\"146096525\",\"name\":\"Achiya Jerbi\"},{\"authorId\":\"46796686\",\"name\":\"Roei Herzig\"},{\"authorId\":\"1750652\",\"name\":\"Jonathan Berant\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"},{\"authorId\":\"1786843\",\"name\":\"A. Globerson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fe840c573c2c0e6f76270e1054bb699627b0f470\",\"title\":\"Learning Object Detection from Captions via Textual Scene Attributes\",\"url\":\"https://www.semanticscholar.org/paper/fe840c573c2c0e6f76270e1054bb699627b0f470\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145778078\",\"name\":\"M. Hao\"},{\"authorId\":\"1492120634\",\"name\":\"Bo Xu\"},{\"authorId\":\"49174555\",\"name\":\"Jingyi Liang\"},{\"authorId\":\"7504195\",\"name\":\"B. Zhang\"},{\"authorId\":\"1682664\",\"name\":\"X. Yin\"}],\"doi\":\"10.1145/3388970\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32f75ede70e7a321e887eae75750251c009ba40d\",\"title\":\"Chinese Short Text Classification with Mutual-Attention Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/32f75ede70e7a321e887eae75750251c009ba40d\",\"venue\":\"ACM Trans. Asian Low Resour. Lang. Inf. Process.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993691002\",\"name\":\"Raksha Ramesh\"},{\"authorId\":\"144578530\",\"name\":\"Vishal Anand\"},{\"authorId\":\"1993659768\",\"name\":\"Ziyin Wang\"},{\"authorId\":\"1993695801\",\"name\":\"Tianle Zhu\"},{\"authorId\":\"1993695696\",\"name\":\"Wenfeng Lyu\"},{\"authorId\":\"1993660605\",\"name\":\"Serena Yuan\"},{\"authorId\":\"153903099\",\"name\":\"Ching-Yung Lin\"}],\"doi\":\"10.1145/3395035.3425641\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2667528b8e8301bfc095e9d51fb823125b6af62b\",\"title\":\"Kinetics and Scene Features for Intent Detection\",\"url\":\"https://www.semanticscholar.org/paper/2667528b8e8301bfc095e9d51fb823125b6af62b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.01725\",\"authors\":[{\"authorId\":\"6328564\",\"name\":\"M. Farazi\"},{\"authorId\":\"1859082176\",\"name\":\"Salman Khan\"},{\"authorId\":\"1712576\",\"name\":\"N. Barnes\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4e5ffb9d473b195a9f0f159dca8bdbebce531ff6\",\"title\":\"Attention Guided Semantic Relationship Parsing for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/4e5ffb9d473b195a9f0f159dca8bdbebce531ff6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.08322\",\"authors\":[{\"authorId\":\"47196880\",\"name\":\"Ziwei Wang\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"},{\"authorId\":\"49407982\",\"name\":\"Hui-min Lu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6c87ed5765c51f3ed294a399d9d2384dc296c585\",\"title\":\"ORD: Object Relationship Discovery for Visual Dialogue Generation\",\"url\":\"https://www.semanticscholar.org/paper/6c87ed5765c51f3ed294a399d9d2384dc296c585\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.12975\",\"authors\":[{\"authorId\":\"1410423255\",\"name\":\"R\\u0131za Velio\\u011flu\"},{\"authorId\":\"38444971\",\"name\":\"Jewgeni Rose\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d952d45eb6e7c0c564974e2caf1d86db36750b20\",\"title\":\"Detecting Hate Speech in Memes Using Multimodal Deep Learning Approaches: Prize-winning solution to Hateful Memes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/d952d45eb6e7c0c564974e2caf1d86db36750b20\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1935044135\",\"name\":\"Wenbo Zheng\"},{\"authorId\":\"1935044135\",\"name\":\"Wenbo Zheng\"},{\"authorId\":\"151486225\",\"name\":\"L. Yan\"},{\"authorId\":\"1491637173\",\"name\":\"Chao Gou\"},{\"authorId\":\"143754347\",\"name\":\"F. Wang\"}],\"doi\":\"10.1016/j.inffus.2020.10.007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03a8f5098e8ffeed7a38f1ae8705f71b18d24e0e\",\"title\":\"KM4: Visual reasoning via Knowledge Embedding Memory Model with Mutual Modulation\",\"url\":\"https://www.semanticscholar.org/paper/03a8f5098e8ffeed7a38f1ae8705f71b18d24e0e\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"2005.13118\",\"authors\":[{\"authorId\":\"38009032\",\"name\":\"P. Zhang\"},{\"authorId\":\"47103450\",\"name\":\"Yunlu Xu\"},{\"authorId\":\"2398015\",\"name\":\"Zhanzhan Cheng\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"97486095\",\"name\":\"J. Lu\"},{\"authorId\":null,\"name\":\"Liang Qiao\"},{\"authorId\":\"1490934795\",\"name\":\"Yi Niu\"},{\"authorId\":\"93192602\",\"name\":\"Fei Wu\"}],\"doi\":\"10.1145/3394171.3413900\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"617f5151f59848d24fe971cf1cf6bb0caec65ea4\",\"title\":\"TRIE: End-to-End Text Reading and Information Extraction for Document Understanding\",\"url\":\"https://www.semanticscholar.org/paper/617f5151f59848d24fe971cf1cf6bb0caec65ea4\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2005.09200\",\"authors\":[{\"authorId\":\"116614713\",\"name\":\"Tingle Li\"},{\"authorId\":\"151178158\",\"name\":\"Qingjian Lin\"},{\"authorId\":\"49955636\",\"name\":\"Yuanyuan Bao\"},{\"authorId\":\"49595665\",\"name\":\"Ming Li\"}],\"doi\":\"10.21437/interspeech.2020-1436\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69a2c9f5356db524fee6fb00c7016e84e82dd745\",\"title\":\"Atss-Net: Target Speaker Separation via Attention-based Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/69a2c9f5356db524fee6fb00c7016e84e82dd745\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2003.06576\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"1491414917\",\"name\":\"Xin Yan\"},{\"authorId\":\"153269968\",\"name\":\"Jun Xiao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1109/cvpr42600.2020.01081\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dc08d90005b12f66a12798fd79959a8f7f8c4885\",\"title\":\"Counterfactual Samples Synthesizing for Robust Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/dc08d90005b12f66a12798fd79959a8f7f8c4885\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.09971\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b57c9a293934b7434b99807c82d6146634cafa61\",\"title\":\"A Better Variant of Self-Critical Sequence Training\",\"url\":\"https://www.semanticscholar.org/paper/b57c9a293934b7434b99807c82d6146634cafa61\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.13198\",\"authors\":[{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"11882893\",\"name\":\"A. Yang\"},{\"authorId\":\"29343468\",\"name\":\"Yichang Zhang\"},{\"authorId\":\"1618186344\",\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9779ddeb6a8a9de0f7e104d8742728aa14578d6\",\"title\":\"InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction\",\"url\":\"https://www.semanticscholar.org/paper/b9779ddeb6a8a9de0f7e104d8742728aa14578d6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.06260\",\"authors\":[{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"1389646918\",\"name\":\"Edison Marrese-Taylor\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"71200893\",\"name\":\"H. Li\"},{\"authorId\":\"143685465\",\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a7177ae93ed91f3bf34875205396ec97f0873bb7\",\"title\":\"DORi: Discovering Object Relationship for Moment Localization of a Natural-Language Query in Video\",\"url\":\"https://www.semanticscholar.org/paper/a7177ae93ed91f3bf34875205396ec97f0873bb7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1733072724\",\"name\":\"Van-Luon Tran\"},{\"authorId\":\"93763734\",\"name\":\"Trong-Dat Phan\"},{\"authorId\":\"1733073249\",\"name\":\"Anh-Vu Mai-Nguyen\"},{\"authorId\":\"121206993\",\"name\":\"Anh-Khoa Vo\"},{\"authorId\":\"2756079\",\"name\":\"Minh-Son Dao\"},{\"authorId\":\"1831975\",\"name\":\"K. Zettsu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"197252ab532e6a8ba70c8e14a2201395a9481b2b\",\"title\":\"An Interactive Atomic-cluster Watershed-based System for Lifelog Moment Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/197252ab532e6a8ba70c8e14a2201395a9481b2b\",\"venue\":\"CLEF\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153297544\",\"name\":\"X. Yang\"},{\"authorId\":\"118565563\",\"name\":\"Chong-Yang Gao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1145/3394171.3413859\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"96389251e4f0f8494dfc8dc67c05992eed8e192b\",\"title\":\"Hierarchical Scene Graph Encoder-Decoder for Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96389251e4f0f8494dfc8dc67c05992eed8e192b\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yongzhuang Wang\"},{\"authorId\":\"49746133\",\"name\":\"Yangmei Shen\"},{\"authorId\":\"144045763\",\"name\":\"Hongkai Xiong\"},{\"authorId\":\"8131625\",\"name\":\"W. Lin\"}],\"doi\":\"10.1109/ICIP.2019.8803418\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"93368542c1774e2cbd12187a4ccc2c882c791d94\",\"title\":\"Adaptive Hard Example Mining for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93368542c1774e2cbd12187a4ccc2c882c791d94\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40295709\",\"name\":\"F. Liu\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"120313754\",\"name\":\"Richang Hong\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1145/3394171.3413649\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"063235546b312ee4cb5cc67578b6e879461c6d83\",\"title\":\"Dual Hierarchical Temporal Convolutional Network with QA-Aware Dynamic Normalization for Video Story Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/063235546b312ee4cb5cc67578b6e879461c6d83\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443435125\",\"name\":\"Zhengcong Fei\"}],\"doi\":\"10.1145/3394171.3413901\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0bcec12a99cfcd649ea36d6b7215d025bad12974\",\"title\":\"Iterative Back Modification for Faster Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0bcec12a99cfcd649ea36d6b7215d025bad12974\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2639734\",\"name\":\"Qingbao Huang\"},{\"authorId\":\"1754240987\",\"name\":\"Jielong Wei\"},{\"authorId\":\"1752876325\",\"name\":\"Yi Cai\"},{\"authorId\":\"150068355\",\"name\":\"Changmeng Zheng\"},{\"authorId\":\"47740571\",\"name\":\"J. Chen\"},{\"authorId\":\"1701688\",\"name\":\"Ho-fung Leung\"},{\"authorId\":\"145138436\",\"name\":\"Qing Li\"}],\"doi\":\"10.18653/v1/2020.acl-main.642\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ede49ec0dd27849e57152d5116770bcbe3e01874\",\"title\":\"Aligned Dual Channel Graph Convolutional Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ede49ec0dd27849e57152d5116770bcbe3e01874\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145779130\",\"name\":\"Peng Liu\"},{\"authorId\":\"7559523\",\"name\":\"Lemei Zhang\"},{\"authorId\":\"1755274\",\"name\":\"J. Gulla\"}],\"doi\":\"10.1016/J.IPM.2019.102099\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"799496915c96f44f0107c2cc4ea60a2c7b8df478\",\"title\":\"Dynamic attention-based explainable recommendation with textual and visual fusion\",\"url\":\"https://www.semanticscholar.org/paper/799496915c96f44f0107c2cc4ea60a2c7b8df478\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"2005.03119\",\"authors\":[{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"145919382\",\"name\":\"J. Hu\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.18653/v1/2020.acl-main.731\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4318f99804c5fd57fd7c0a1087b22bbe6268e276\",\"title\":\"Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting\",\"url\":\"https://www.semanticscholar.org/paper/4318f99804c5fd57fd7c0a1087b22bbe6268e276\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"3436470\",\"name\":\"Kimin Lee\"},{\"authorId\":\"143720148\",\"name\":\"Jinwoo Shin\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7adef3d0200207baec75e39bbb852cacfaf8268b\",\"title\":\"Learning to Specialize with Knowledge Distillation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7adef3d0200207baec75e39bbb852cacfaf8268b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97636424\",\"name\":\"G. Li\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"144303230\",\"name\":\"Ping Liu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/ICCV.2019.00902\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7c4530882cfcef1d2b4aa2996f494dfac626b5d9\",\"title\":\"Entangled Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7c4530882cfcef1d2b4aa2996f494dfac626b5d9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1476764875\",\"name\":\"Yatong Sun\"},{\"authorId\":\"38896551\",\"name\":\"G. Guo\"},{\"authorId\":\"48259019\",\"name\":\"Xu Chen\"},{\"authorId\":\"52221349\",\"name\":\"Penghai Zhang\"},{\"authorId\":\"144713897\",\"name\":\"Xingwei Wang\"}],\"doi\":\"10.1007/s10115-020-01447-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"99c19da486fe3e08ede5cfb7e7bd06a8e884fb0c\",\"title\":\"Exploiting review embedding and user attention for item recommendation\",\"url\":\"https://www.semanticscholar.org/paper/99c19da486fe3e08ede5cfb7e7bd06a8e884fb0c\",\"venue\":\"Knowledge and Information Systems\",\"year\":2020},{\"arxivId\":\"1807.03871\",\"authors\":[{\"authorId\":\"7934161\",\"name\":\"T. Chen\"},{\"authorId\":\"48805316\",\"name\":\"Z. Zhang\"},{\"authorId\":\"36610242\",\"name\":\"Quanzeng You\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1007/978-3-030-01249-6_32\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"abc7998326cc4fc3c9c0c3a9ede8ae2538439966\",\"title\":\"\\\"Factual\\\" or \\\"Emotional\\\": Stylized Image Captioning with Adaptive Learning and Attention\",\"url\":\"https://www.semanticscholar.org/paper/abc7998326cc4fc3c9c0c3a9ede8ae2538439966\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2011.02917\",\"authors\":[{\"authorId\":\"3444866\",\"name\":\"Alessandro Suglia\"},{\"authorId\":\"14341992\",\"name\":\"A. Vergari\"},{\"authorId\":\"2621022\",\"name\":\"Ioannis Konstas\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"2972920\",\"name\":\"E. Bastianelli\"},{\"authorId\":\"34742006\",\"name\":\"Andrea Vanzo\"},{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f837bf72e5b864e1c162e924fed59b778e946e23\",\"title\":\"Imagining Grounded Conceptual Representations from Perceptual Information in Situated Guessing Games\",\"url\":\"https://www.semanticscholar.org/paper/f837bf72e5b864e1c162e924fed59b778e946e23\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1419474253\",\"name\":\"Md Asifuzzaman Jishan\"},{\"authorId\":\"102857185\",\"name\":\"K. R. Mahmud\"},{\"authorId\":\"48058214\",\"name\":\"A. K. Azad\"},{\"authorId\":\"2868008\",\"name\":\"M. S. Alam\"},{\"authorId\":\"2004453191\",\"name\":\"Anif Minhaz Khan\"}],\"doi\":\"10.26555/ijain.v6i2.499\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"592664a1eb6209c58ab1056ad2e988ad4cd72559\",\"title\":\"Hybrid deep neural network for Bangla automated image descriptor\",\"url\":\"https://www.semanticscholar.org/paper/592664a1eb6209c58ab1056ad2e988ad4cd72559\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753617513\",\"name\":\"Tan Wang\"},{\"authorId\":\"50535545\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"32222907\",\"name\":\"Qianru Sun\"}],\"doi\":\"10.1109/CVPRW50498.2020.00197\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ff0236a4f4687b61155416eae8864bd9da234509\",\"title\":\"Visual Commonsense Representation Learning via Causal Inference\",\"url\":\"https://www.semanticscholar.org/paper/ff0236a4f4687b61155416eae8864bd9da234509\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1692269\",\"name\":\"Jiancheng Ni\"},{\"authorId\":\"1557381789\",\"name\":\"Susu Zhang\"},{\"authorId\":null,\"name\":\"Zili Zhou\"},{\"authorId\":\"47287130\",\"name\":\"J. Hou\"},{\"authorId\":\"152174467\",\"name\":\"Feng Gao\"}],\"doi\":\"10.1109/ACCESS.2020.2975841\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b7cddb9d7709cebcc615611930bedf93cd6caef3\",\"title\":\"Instance Mask Embedding and Attribute-Adaptive Generative Adversarial Network for Text-to-Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/b7cddb9d7709cebcc615611930bedf93cd6caef3\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32701193\",\"name\":\"Yuanzhi Liang\"},{\"authorId\":\"2643877\",\"name\":\"Y. Bai\"},{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"6468417\",\"name\":\"Xueming Qian\"},{\"authorId\":\"4096586\",\"name\":\"L. Zhu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2019.01050\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"db717d20dc699f4b402db0ddf923135108a9e686\",\"title\":\"VrR-VG: Refocusing Visually-Relevant Relationships\",\"url\":\"https://www.semanticscholar.org/paper/db717d20dc699f4b402db0ddf923135108a9e686\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736464\",\"name\":\"M. Khademi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4819ce65a0322a748c6d606d0678e8a0f78dafcc\",\"title\":\"Graph neural networks for multimodal learning and representation\",\"url\":\"https://www.semanticscholar.org/paper/4819ce65a0322a748c6d606d0678e8a0f78dafcc\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1802.00209\",\"authors\":[{\"authorId\":\"143757036\",\"name\":\"Ahmed Osman\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7b66dababebd800e95d23a1fde299d44a52e98ed\",\"title\":\"Dual Recurrent Attention Units for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7b66dababebd800e95d23a1fde299d44a52e98ed\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1527097122\",\"name\":\"Y. Li\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"9359893\",\"name\":\"Shengrong Gong\"}],\"doi\":\"10.1145/3378026\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"083e075864cdf19117f6b5f78db877347e2bca4f\",\"title\":\"Spatio-temporal Deep Residual Network with Hierarchical Attentions for Video Event Recognition\",\"url\":\"https://www.semanticscholar.org/paper/083e075864cdf19117f6b5f78db877347e2bca4f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1865719481\",\"name\":\"Deepak Gupta\"},{\"authorId\":\"27563697\",\"name\":\"Pabitra Lenka\"},{\"authorId\":\"1734904\",\"name\":\"Asif Ekbal\"},{\"authorId\":\"145532184\",\"name\":\"P. Bhattacharyya\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6ba2a128dd8dadc2925b3b9c01fb6db01bd75066\",\"title\":\"A Unified Framework for Multilingual and Code-Mixed Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/6ba2a128dd8dadc2925b3b9c01fb6db01bd75066\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2006.11807\",\"authors\":[{\"authorId\":\"144910087\",\"name\":\"Zhan Shi\"},{\"authorId\":\"152482200\",\"name\":\"X. Zhou\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"150345740\",\"name\":\"Xiao-Dan Zhu\"}],\"doi\":\"10.18653/v1/2020.acl-main.664\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7e0f91e51ee372939c96714c7919dde6dc756849\",\"title\":\"Improving Image Captioning with Better Use of Captions\",\"url\":\"https://www.semanticscholar.org/paper/7e0f91e51ee372939c96714c7919dde6dc756849\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1860938\",\"name\":\"M. Ghanimifard\"},{\"authorId\":\"2995275\",\"name\":\"Simon Dobnik\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"af258688503b48c1028680c67eb12cb351e9830a\",\"title\":\"Visual grounding of spatial relations in recurrent neural language models\",\"url\":\"https://www.semanticscholar.org/paper/af258688503b48c1028680c67eb12cb351e9830a\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2003.10925\",\"authors\":[{\"authorId\":\"47577022\",\"name\":\"Nannan Li\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"16025ed0f452e62514d4705b7da1e9f78067d9e7\",\"title\":\"Learning Compact Reward for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16025ed0f452e62514d4705b7da1e9f78067d9e7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.07804\",\"authors\":[{\"authorId\":\"51011359\",\"name\":\"S. Pramanik\"},{\"authorId\":\"7421228\",\"name\":\"Priyanka Agrawal\"},{\"authorId\":\"145374365\",\"name\":\"Aman Hussain\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"3fc815cebbd8948dfed0592a254d5d4bf82d3fcf\",\"title\":\"OmniNet: A unified architecture for multi-modal multi-task learning\",\"url\":\"https://www.semanticscholar.org/paper/3fc815cebbd8948dfed0592a254d5d4bf82d3fcf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1811.10830\",\"authors\":[{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1109/CVPR.2019.00688\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6dfc2ff03534a4325d06c6f88c3144831996629b\",\"title\":\"From Recognition to Cognition: Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/6dfc2ff03534a4325d06c6f88c3144831996629b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47817913\",\"name\":\"L. Chen\"},{\"authorId\":\"1397287686\",\"name\":\"Yifan Zhuo\"},{\"authorId\":\"48607932\",\"name\":\"Yingjie Wu\"},{\"authorId\":null,\"name\":\"Yilei Wang\"},{\"authorId\":\"1678585\",\"name\":\"Xianghan Zheng\"}],\"doi\":\"10.1007/978-3-030-31723-2_56\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bae56e688485148c1519ff8458ea2ba6b7fab3f2\",\"title\":\"Multi-modal Feature Fusion Based on Variational Autoencoder for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bae56e688485148c1519ff8458ea2ba6b7fab3f2\",\"venue\":\"PRCV\",\"year\":2019},{\"arxivId\":\"1911.10460\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143672098\",\"name\":\"Bei Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"35119829\",\"name\":\"R. Song\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"7474269\",\"name\":\"Ping-Ping Lin\"},{\"authorId\":\"47099153\",\"name\":\"Xiaoyu Qi\"},{\"authorId\":\"50096056\",\"name\":\"C. Wang\"},{\"authorId\":\"97807965\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1145/3343031.3350571\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e0779bca8faec33918338f98c0014e387993d388\",\"title\":\"Neural Storyboard Artist: Visualizing Stories with Coherent Image Sequences\",\"url\":\"https://www.semanticscholar.org/paper/e0779bca8faec33918338f98c0014e387993d388\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1907.03950\",\"authors\":[{\"authorId\":\"152951058\",\"name\":\"Drew A. Hudson\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec\",\"title\":\"Learning by Abstraction: The Neural State Machine\",\"url\":\"https://www.semanticscholar.org/paper/136c05cb8dd359fb8e0dc7947172a9ecb74ccbec\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"3256307\",\"name\":\"M. A. A. K. Jalwana\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"},{\"authorId\":\"2210661\",\"name\":\"S. Shah\"}],\"doi\":\"10.1109/IVCNZ51579.2020.9290719\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b88b5bc5fe9af08df2d953b1c14c6f5cacd9564\",\"title\":\"Leveraging Linguistically-aware Object Relations and NASNet for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8b88b5bc5fe9af08df2d953b1c14c6f5cacd9564\",\"venue\":\"2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ)\",\"year\":2020},{\"arxivId\":\"2012.11696\",\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"8350409\",\"name\":\"I. Padhi\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"},{\"authorId\":\"153598395\",\"name\":\"J. Ross\"},{\"authorId\":\"1999174380\",\"name\":\"Yair Schiff\"},{\"authorId\":\"49832828\",\"name\":\"R. A. Young\"},{\"authorId\":\"2679155\",\"name\":\"Brian M. Belgodere\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"title\":\"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.04365\",\"authors\":[{\"authorId\":\"9457831\",\"name\":\"J. He\"},{\"authorId\":\"1405953764\",\"name\":\"Quan-Jie Cao\"},{\"authorId\":\"39844955\",\"name\":\"L. Zhang\"},{\"authorId\":\"93970041\",\"name\":\"Hui Tao\"}],\"doi\":\"10.1109/ACCESS.2020.2982571\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"587e1359d0a54ecf24e2577357abe79d0cd00ab2\",\"title\":\"Conditionally Learn to Pay Attention for Sequential Visual Task\",\"url\":\"https://www.semanticscholar.org/paper/587e1359d0a54ecf24e2577357abe79d0cd00ab2\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/TIP.2020.3042086\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"title\":\"Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134892686\",\"name\":\"Yangfan Ni\"},{\"authorId\":\"70104819\",\"name\":\"Y. Yang\"},{\"authorId\":\"9976954\",\"name\":\"D. Zheng\"},{\"authorId\":\"134909173\",\"name\":\"Zhe Xie\"},{\"authorId\":\"48186495\",\"name\":\"Haozhe Huang\"},{\"authorId\":\"46314800\",\"name\":\"W. Wang\"}],\"doi\":\"10.1007/s10278-020-00355-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e8883b1762d5731a58c7357b1de6c512db40194e\",\"title\":\"The Invasiveness Classification of Ground-Glass Nodules Using 3D Attention Network and HRCT\",\"url\":\"https://www.semanticscholar.org/paper/e8883b1762d5731a58c7357b1de6c512db40194e\",\"venue\":\"Journal of Digital Imaging\",\"year\":2020},{\"arxivId\":\"2004.08744\",\"authors\":[{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43d77f0547f4a1bf4faf29e5e7548564b70e758a\",\"title\":\"Are we pretraining it right? Digging deeper into visio-linguistic pretraining\",\"url\":\"https://www.semanticscholar.org/paper/43d77f0547f4a1bf4faf29e5e7548564b70e758a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.14973\",\"authors\":[{\"authorId\":\"2905057\",\"name\":\"A. Majumdar\"},{\"authorId\":\"3445289\",\"name\":\"Ayush Shrivastava\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1606382599\",\"name\":\"Peter Anderson\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/978-3-030-58539-6_16\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1ac487f21829ef56c8ffdcd37ea414bce68c809\",\"title\":\"Improving Vision-and-Language Navigation with Image-Text Pairs from the Web\",\"url\":\"https://www.semanticscholar.org/paper/d1ac487f21829ef56c8ffdcd37ea414bce68c809\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.10638\",\"authors\":[{\"authorId\":\"3314779\",\"name\":\"Weituo Hao\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1109/cvpr42600.2020.01315\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"046d1e193faeeb404b4a7812d3c635aa5e1ecccd\",\"title\":\"Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training\",\"url\":\"https://www.semanticscholar.org/paper/046d1e193faeeb404b4a7812d3c635aa5e1ecccd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1908.04107\",\"authors\":[{\"authorId\":\"23165772\",\"name\":\"Z. Yu\"},{\"authorId\":\"30513139\",\"name\":\"Yuhao Cui\"},{\"authorId\":\"97583812\",\"name\":\"J. Yu\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a3d3df29fc98e0d674bf02bab69726795f4c7b78\",\"title\":\"Multimodal Unified Attention Networks for Vision-and-Language Interactions\",\"url\":\"https://www.semanticscholar.org/paper/a3d3df29fc98e0d674bf02bab69726795f4c7b78\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2009.02016\",\"authors\":[{\"authorId\":\"46933131\",\"name\":\"Huan Lin\"},{\"authorId\":\"33427918\",\"name\":\"Fandong Meng\"},{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":\"79701068\",\"name\":\"Yongjing Yin\"},{\"authorId\":\"21518096\",\"name\":\"Zhengyuan Yang\"},{\"authorId\":\"2425049\",\"name\":\"Y. Ge\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3394171.3413715\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c255d0978ce8791d99478a03546359118f350c5\",\"title\":\"Dynamic Context-guided Capsule Network for Multimodal Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/7c255d0978ce8791d99478a03546359118f350c5\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2012.05710\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"title\":\"Look Before you Speak: Visually Contextualized Utterances\",\"url\":\"https://www.semanticscholar.org/paper/b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1906.10169\",\"authors\":[{\"authorId\":\"7535126\",\"name\":\"R\\u00e9mi Cad\\u00e8ne\"},{\"authorId\":\"41020827\",\"name\":\"Corentin Dancette\"},{\"authorId\":\"1405301761\",\"name\":\"Hedi Ben-younes\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ab9cc2c6a8d35d7a145cf608ff9dd7af87213253\",\"title\":\"RUBi: Reducing Unimodal Biases in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ab9cc2c6a8d35d7a145cf608ff9dd7af87213253\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1909.11416\",\"authors\":[{\"authorId\":\"123266794\",\"name\":\"C. Liu\"},{\"authorId\":\"1855978\",\"name\":\"Zhendong Mao\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"152602127\",\"name\":\"Tianzhu Zhang\"},{\"authorId\":\"49292319\",\"name\":\"Bo Wang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.1145/3343031.3350869\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d149507610400ddc2f2b29d9a39f7688b613039\",\"title\":\"Focus Your Attention: A Bidirectional Focal Attention Network for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/2d149507610400ddc2f2b29d9a39f7688b613039\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1911.00147\",\"authors\":[{\"authorId\":\"153650159\",\"name\":\"C. Thomas\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8922345e2aaf4e4a88cf2c2e91b8545ef54c6d2\",\"title\":\"Predicting the Politics of an Image Using Webly Supervised Data\",\"url\":\"https://www.semanticscholar.org/paper/c8922345e2aaf4e4a88cf2c2e91b8545ef54c6d2\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22205368\",\"name\":\"Akshay Chaturvedi\"},{\"authorId\":\"48421321\",\"name\":\"U. Garain\"}],\"doi\":\"10.1109/TETCI.2020.2977695\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8d266a25416cfb8184c56c981beb5ca322b38ede\",\"title\":\"Attacking VQA Systems via Adversarial Background Noise\",\"url\":\"https://www.semanticscholar.org/paper/8d266a25416cfb8184c56c981beb5ca322b38ede\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2020},{\"arxivId\":\"1911.09753\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.1609/AAAI.V34I03.5655\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"title\":\"Reinforcing an Image Caption Generator Using Off-Line Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153228843\",\"name\":\"Chujie Lu\"},{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"71208047\",\"name\":\"C. Tan\"},{\"authorId\":\"47056890\",\"name\":\"Xiao-Lin Li\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.18653/v1/D19-1518\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"872091517b0bfad0e9bc1826d4668022d1d57953\",\"title\":\"DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization\",\"url\":\"https://www.semanticscholar.org/paper/872091517b0bfad0e9bc1826d4668022d1d57953\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1911.10115\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"50c6889b547cc08a203842d5cf5bcb4c58e052b5\",\"title\":\"TPsgtR: Neural-Symbolic Tensor Product Scene-Graph-Triplet Representation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/50c6889b547cc08a203842d5cf5bcb4c58e052b5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49066069\",\"name\":\"Fabian Frank\"},{\"authorId\":\"31199200\",\"name\":\"Lars Jebe\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e2873a2ea525507f5cd08e54ba363b06bc10e0a\",\"title\":\"Multi-Modal Information Extraction in a Question-Answer Framework\",\"url\":\"https://www.semanticscholar.org/paper/4e2873a2ea525507f5cd08e54ba363b06bc10e0a\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1904.00172\",\"authors\":[{\"authorId\":\"8572939\",\"name\":\"Jingcai Guo\"},{\"authorId\":\"144123447\",\"name\":\"Song Guo\"}],\"doi\":\"10.1109/ICASSP.2019.8682891\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"47b13d17d1c88ab30a63501dc0769bd0b22e9a4c\",\"title\":\"EE-AE: An Exclusivity Enhanced Unsupervised Feature Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/47b13d17d1c88ab30a63501dc0769bd0b22e9a4c\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"2001.05865\",\"authors\":[{\"authorId\":\"144992211\",\"name\":\"Shubham Agarwal\"},{\"authorId\":\"38962424\",\"name\":\"Raghav Goyal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4f9d6f092cce26c22404579157912973d71ec44e\",\"title\":\"Ensemble based discriminative models for Visual Dialog Challenge 2018\",\"url\":\"https://www.semanticscholar.org/paper/4f9d6f092cce26c22404579157912973d71ec44e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.01880\",\"authors\":[{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143905981\",\"name\":\"Baoyuan Wu\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2019.00678\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"88ec56eee6a787ccda0cf7fbfbec48c6b4ad68fe\",\"title\":\"Learning to Compose Dynamic Tree Structures for Visual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/88ec56eee6a787ccda0cf7fbfbec48c6b4ad68fe\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2001.07966\",\"authors\":[{\"authorId\":\"1380129958\",\"name\":\"Di Qi\"},{\"authorId\":\"143693093\",\"name\":\"L. Su\"},{\"authorId\":\"50185694\",\"name\":\"Jia Song\"},{\"authorId\":\"3474078\",\"name\":\"E. Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":\"35378689\",\"name\":\"Arun Sacheti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a9fd5511b42206a27748f373e0fdb7eb76a23055\",\"title\":\"ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data\",\"url\":\"https://www.semanticscholar.org/paper/a9fd5511b42206a27748f373e0fdb7eb76a23055\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1809.01810\",\"authors\":[{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"46708631\",\"name\":\"Bailin Li\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/tpami.2019.2943456\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97b93509f6c3c33dd3665d05b1878e36d58a1efb\",\"title\":\"Interpretable Visual Question Answering by Reasoning on Dependency Trees\",\"url\":\"https://www.semanticscholar.org/paper/97b93509f6c3c33dd3665d05b1878e36d58a1efb\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2019},{\"arxivId\":\"2007.04417\",\"authors\":[{\"authorId\":\"1796278527\",\"name\":\"Kevin Trebing\"},{\"authorId\":\"2950749\",\"name\":\"S. Mehrkanoon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b27f82b84ffd0894f649279946354acc760d2bb4\",\"title\":\"SmaAt-UNet: Precipitation Nowcasting using a Small Attention-UNet Architecture\",\"url\":\"https://www.semanticscholar.org/paper/b27f82b84ffd0894f649279946354acc760d2bb4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26339425\",\"name\":\"Valentin Vielzeuf\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0501b8a99270a20c7536ed2f6df6569413810f6d\",\"title\":\"Apprentissage neuronal profond pour l'analyse de contenus multimodaux et temporels. (Deep learning for multimodal and temporal contents analysis)\",\"url\":\"https://www.semanticscholar.org/paper/0501b8a99270a20c7536ed2f6df6569413810f6d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1807.00517\",\"authors\":[{\"authorId\":\"40895688\",\"name\":\"Kaylee Burns\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-01219-9_47\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b0c5dc3fa19a2bc97606ccb6f55226b913984395\",\"title\":\"Women also Snowboard: Overcoming Bias in Captioning Models\",\"url\":\"https://www.semanticscholar.org/paper/b0c5dc3fa19a2bc97606ccb6f55226b913984395\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41097744\",\"name\":\"Kai Shen\"},{\"authorId\":\"3008832\",\"name\":\"Lingfei Wu\"},{\"authorId\":\"11541418\",\"name\":\"F. Xu\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"1406012647\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.24963/ijcai.2020/131\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"41f10434eba89d27c83764619b067f9db492d98b\",\"title\":\"Hierarchical Attention Based Spatial-Temporal Graph-to-Sequence Learning for Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/41f10434eba89d27c83764619b067f9db492d98b\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1733072724\",\"name\":\"Van-Luon Tran\"},{\"authorId\":\"1733073249\",\"name\":\"Anh-Vu Mai-Nguyen\"},{\"authorId\":\"93763734\",\"name\":\"Trong-Dat Phan\"},{\"authorId\":\"121206993\",\"name\":\"Anh-Khoa Vo\"},{\"authorId\":\"50376884\",\"name\":\"Minh-Son Dao\"},{\"authorId\":\"48909205\",\"name\":\"Koji Zettsu\"}],\"doi\":\"10.1145/3372278.3391934\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"36d1bf050935729012bd1a13c60eb4b667027a85\",\"title\":\"An Interactive Multimodal Retrieval System for Memory Assistant and Life Organized Support\",\"url\":\"https://www.semanticscholar.org/paper/36d1bf050935729012bd1a13c60eb4b667027a85\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46701253\",\"name\":\"J. Liu\"},{\"authorId\":\"1739646\",\"name\":\"Christian Desrosiers\"},{\"authorId\":\"1746616\",\"name\":\"Yuanfeng Zhou\"}],\"doi\":\"10.1016/j.neucom.2020.06.017\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"707c8ff586686c148989610c2a2d3d887b63b955\",\"title\":\"Att-MoE: Attention-based Mixture of Experts for nuclear and cytoplasmic segmentation\",\"url\":\"https://www.semanticscholar.org/paper/707c8ff586686c148989610c2a2d3d887b63b955\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"2010.00361\",\"authors\":[{\"authorId\":\"7814221\",\"name\":\"Zipeng Xu\"},{\"authorId\":\"39825530\",\"name\":\"Fangxiang Feng\"},{\"authorId\":\"38542466\",\"name\":\"X. Wang\"},{\"authorId\":\"2590300\",\"name\":\"Y. Yang\"},{\"authorId\":\"2309680\",\"name\":\"Huixing Jiang\"},{\"authorId\":\"1978074381\",\"name\":\"Zhongyuan Ouyang\"}],\"doi\":\"10.1145/3394171.3413668\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"998a4a5e3343e46c3b1ef8095bb29b7a85bf45ce\",\"title\":\"Answer-Driven Visual State Estimator for Goal-Oriented Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/998a4a5e3343e46c3b1ef8095bb29b7a85bf45ce\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50974488\",\"name\":\"Mingrui Lao\"},{\"authorId\":\"49813983\",\"name\":\"Yanming Guo\"},{\"authorId\":\"49528566\",\"name\":\"Hui Wang\"},{\"authorId\":\"35742440\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1109/ACCESS.2018.2844789\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2609626519d8fa0ccc53bce49a3a21b928deeca6\",\"title\":\"Cross-Modal Multistep Fusion Network With Co-Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/2609626519d8fa0ccc53bce49a3a21b928deeca6\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"2011.08877\",\"authors\":[{\"authorId\":\"50180555\",\"name\":\"Xinyi Xu\"},{\"authorId\":\"2969311\",\"name\":\"Zhangyang Wang\"},{\"authorId\":\"145102438\",\"name\":\"Cheng Deng\"},{\"authorId\":\"1498527026\",\"name\":\"Hao Yuan\"},{\"authorId\":\"1743600\",\"name\":\"S. Ji\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"978df93d2d94d5cf9e9059761de356b360bfb317\",\"title\":\"Towards Improved and Interpretable Deep Metric Learning via Attentive Grouping\",\"url\":\"https://www.semanticscholar.org/paper/978df93d2d94d5cf9e9059761de356b360bfb317\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.04553\",\"authors\":[{\"authorId\":\"47267313\",\"name\":\"T. Le\"},{\"authorId\":\"144672395\",\"name\":\"Vuong Le\"},{\"authorId\":\"144162181\",\"name\":\"S. Venkatesh\"},{\"authorId\":\"6254479\",\"name\":\"T. Tran\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207580\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"45a733dab9614611567209628a770b5fe19ad41f\",\"title\":\"Neural Reasoning, Fast and Slow, for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/45a733dab9614611567209628a770b5fe19ad41f\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50498297\",\"name\":\"Liang Peng\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"145833207\",\"name\":\"Ning Xie\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"}],\"doi\":\"10.1007/s11042-018-6389-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"713dd629c183056202f31c2a98e5e37e0d83efa4\",\"title\":\"Word-to-region attention network for visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/713dd629c183056202f31c2a98e5e37e0d83efa4\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":\"1711.08105\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1007/978-3-030-01267-0_14\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a742c64f14b145b9e653ef30819520c5ce5e0123\",\"title\":\"Visual Question Answering as a Meta Learning Task\",\"url\":\"https://www.semanticscholar.org/paper/a742c64f14b145b9e653ef30819520c5ce5e0123\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1807.09986\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1007/978-3-030-01216-8_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"title\":\"Recurrent Fusion Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2037486210\",\"name\":\"Qiaoqiao Yang\"},{\"authorId\":\"1410201319\",\"name\":\"Guangxing Wang\"},{\"authorId\":\"1695975266\",\"name\":\"Xiaoyu Zhang\"},{\"authorId\":\"2037477833\",\"name\":\"Christos Grecos\"},{\"authorId\":\"143823414\",\"name\":\"Peng Ren\"}],\"doi\":\"10.2112/SI102-018.1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"556edd10f961c30fc523f291fe657db56a297850\",\"title\":\"Coastal Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/556edd10f961c30fc523f291fe657db56a297850\",\"venue\":\"Journal of Coastal Research\",\"year\":2020},{\"arxivId\":\"1811.09783\",\"authors\":[{\"authorId\":\"7671691\",\"name\":\"Song-Hai Zhang\"},{\"authorId\":\"48741571\",\"name\":\"Z. Zhou\"},{\"authorId\":null,\"name\":\"Bin Liu\"},{\"authorId\":null,\"name\":\"Xin Dong\"},{\"authorId\":\"3489912\",\"name\":\"Dun Liang\"},{\"authorId\":\"145200806\",\"name\":\"P. Hall\"},{\"authorId\":\"145140922\",\"name\":\"S. Hu\"}],\"doi\":\"10.1007/s41095-020-0158-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd896774b8e39873fc93757e1d9c12d33d31e1b4\",\"title\":\"What and where: A context-based recommendation system for object insertion\",\"url\":\"https://www.semanticscholar.org/paper/bd896774b8e39873fc93757e1d9c12d33d31e1b4\",\"venue\":\"Computational Visual Media\",\"year\":2020},{\"arxivId\":\"1911.03977\",\"authors\":[{\"authorId\":\"145282222\",\"name\":\"C. Zhang\"},{\"authorId\":\"8387085\",\"name\":\"Zichao Yang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718783\",\"name\":\"Li Deng\"}],\"doi\":\"10.1109/JSTSP.2020.2987728\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"415efb7b4d9d1e5b64dbaf3fe4229ad462acce71\",\"title\":\"Multimodal Intelligence: Representation Learning, Information Fusion, and Applications\",\"url\":\"https://www.semanticscholar.org/paper/415efb7b4d9d1e5b64dbaf3fe4229ad462acce71\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":\"1911.08855\",\"authors\":[{\"authorId\":\"71831900\",\"name\":\"C. Chen\"},{\"authorId\":\"47842072\",\"name\":\"M. Liu\"},{\"authorId\":\"3202418\",\"name\":\"Xiandong Meng\"},{\"authorId\":\"1410650653\",\"name\":\"Wanpeng Xiao\"},{\"authorId\":\"34974680\",\"name\":\"Q. Ju\"}],\"doi\":\"10.1109/CVPRW50498.2020.00358\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15ff599685fa30d3a5ca64ffc11413cc1a6ba386\",\"title\":\"RefineDetLite: A Lightweight One-stage Object Detection Framework for CPU-only Devices\",\"url\":\"https://www.semanticscholar.org/paper/15ff599685fa30d3a5ca64ffc11413cc1a6ba386\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1997177\",\"name\":\"Ercheng Pei\"},{\"authorId\":\"153763368\",\"name\":\"Dong-mei Jiang\"},{\"authorId\":\"151486921\",\"name\":\"H. Sahli\"}],\"doi\":\"10.1016/j.neucom.2019.09.037\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"817f717412614675aee6e7e0e3e25af68a0b1219\",\"title\":\"An efficient model-level fusion approach for continuous affect recognition from audiovisual signals\",\"url\":\"https://www.semanticscholar.org/paper/817f717412614675aee6e7e0e3e25af68a0b1219\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1911.06258\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/cvpr42600.2020.01001\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4fc2eb1b494a29c7cb39da3b180a302521e459a0\",\"title\":\"Iterative Answer Prediction With Pointer-Augmented Multimodal Transformers for TextVQA\",\"url\":\"https://www.semanticscholar.org/paper/4fc2eb1b494a29c7cb39da3b180a302521e459a0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89766800\",\"name\":\"Zhenru Li\"},{\"authorId\":\"121704343\",\"name\":\"Yaoyi Li\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"}],\"doi\":\"10.1007/978-3-030-36802-9_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c3a3cf7cbd5601d332de35a4c488da75b3a20047\",\"title\":\"Improve Image Captioning by Self-attention\",\"url\":\"https://www.semanticscholar.org/paper/c3a3cf7cbd5601d332de35a4c488da75b3a20047\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1879301834\",\"name\":\"Poornima Haridas\"},{\"authorId\":\"2530185\",\"name\":\"Gopinath Chennupati\"},{\"authorId\":\"119580605\",\"name\":\"N. Santhi\"},{\"authorId\":\"143741031\",\"name\":\"P. Romero\"},{\"authorId\":\"1707687\",\"name\":\"S. Eidenbenz\"}],\"doi\":\"10.1109/access.2020.3011909\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78792369f47928af2977c81c57e8f1c49eb119b7\",\"title\":\"Code Characterization With Graph Convolutions and Capsule Networks\",\"url\":\"https://www.semanticscholar.org/paper/78792369f47928af2977c81c57e8f1c49eb119b7\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2011.15124\",\"authors\":[{\"authorId\":\"83574123\",\"name\":\"Emanuele Bugliarello\"},{\"authorId\":\"1750769\",\"name\":\"Ryan Cotterell\"},{\"authorId\":\"102837708\",\"name\":\"N. Okazaki\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"82a6f9cce3e8acd0ef9e3ca5c7592bd7b9c058fd\",\"title\":\"Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs\",\"url\":\"https://www.semanticscholar.org/paper/82a6f9cce3e8acd0ef9e3ca5c7592bd7b9c058fd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49418270\",\"name\":\"Yong Wang\"},{\"authorId\":\"1485232293\",\"name\":\"Wenkai Zhang\"},{\"authorId\":\"47362549\",\"name\":\"Qing Liu\"},{\"authorId\":\"9708577\",\"name\":\"Zhengyuan Zhang\"},{\"authorId\":\"11732382\",\"name\":\"X. Gao\"},{\"authorId\":\"9758599\",\"name\":\"Xi-an Sun\"}],\"doi\":\"10.1145/3394171.3413877\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a05a96d5d9bac88d7c8b34be2e57f58f17a6e53d\",\"title\":\"Improving Intra- and Inter-Modality Visual Relation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a05a96d5d9bac88d7c8b34be2e57f58f17a6e53d\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2012.02356\",\"authors\":[{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"72be4e0750cf5591d527d7792aa861353526e311\",\"title\":\"Self-Supervised VQA: Answering Visual Questions using Images and Captions\",\"url\":\"https://www.semanticscholar.org/paper/72be4e0750cf5591d527d7792aa861353526e311\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.10496\",\"authors\":[{\"authorId\":\"3167894\",\"name\":\"Jiaxin Qi\"},{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"50560698\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/cvpr42600.2020.01087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ad99fef72abe84e6b5f194d4973ca824812dbb11\",\"title\":\"Two Causal Principles for Improving Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/ad99fef72abe84e6b5f194d4973ca824812dbb11\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6937593\",\"name\":\"Shih-Han Chou\"},{\"authorId\":\"150336480\",\"name\":\"Wei-Lun Chao\"},{\"authorId\":\"38784892\",\"name\":\"Wei-Lun Chao\"},{\"authorId\":\"30885811\",\"name\":\"Min Sun\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":\"10.1109/WACV45572.2020.9093452\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0e0c5d868d84908bb7a26c27c1aedea43fdc493d\",\"title\":\"Visual Question Answering on 360\\u00b0 Images\",\"url\":\"https://www.semanticscholar.org/paper/0e0c5d868d84908bb7a26c27c1aedea43fdc493d\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643877\",\"name\":\"Y. Bai\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"145382463\",\"name\":\"T. Zhao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1007/978-3-030-01258-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"380d50f3ccc07fa4f41282395a78c51e33985c39\",\"title\":\"Deep Attention Neural Tensor Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/380d50f3ccc07fa4f41282395a78c51e33985c39\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2004.06698\",\"authors\":[{\"authorId\":\"71119060\",\"name\":\"Gi-Cheon Kang\"},{\"authorId\":\"1755502\",\"name\":\"Junseok Park\"},{\"authorId\":\"2294014\",\"name\":\"Hwaran Lee\"},{\"authorId\":\"152705134\",\"name\":\"B. Zhang\"},{\"authorId\":\"153188145\",\"name\":\"J. Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"36642cb0c60047846ccc9bb670a63f6884e976d1\",\"title\":\"DialGraph: Sparse Graph Learning Networks for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/36642cb0c60047846ccc9bb670a63f6884e976d1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.02497\",\"authors\":[{\"authorId\":\"14248689\",\"name\":\"Xinrui Cui\"},{\"authorId\":\"47859596\",\"name\":\"D. Wang\"},{\"authorId\":\"1859192\",\"name\":\"Z. Wang\"}],\"doi\":\"10.1109/TNNLS.2019.2952322\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e5077b81edb6c2e6be89eec24de4cf4c48642eb\",\"title\":\"CHIP: Channel-Wise Disentangled Interpretation of Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/3e5077b81edb6c2e6be89eec24de4cf4c48642eb\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8088602\",\"name\":\"Ehsan Abbasnejad\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"113720743\",\"name\":\"Amin Parvaneh\"},{\"authorId\":\"31635758\",\"name\":\"Javen Shi\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR42600.2020.01006\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"046f1c5cc8c7b4f24ab62a536d8b0a989209824b\",\"title\":\"Counterfactual Vision and Language Learning\",\"url\":\"https://www.semanticscholar.org/paper/046f1c5cc8c7b4f24ab62a536d8b0a989209824b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.08889\",\"authors\":[{\"authorId\":\"1751482331\",\"name\":\"Zerun Feng\"},{\"authorId\":\"7801828\",\"name\":\"Zhimin Zeng\"},{\"authorId\":\"47932273\",\"name\":\"Caili Guo\"},{\"authorId\":\"2123874\",\"name\":\"Z. Li\"}],\"doi\":\"10.24963/ijcai.2020/140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ab464a59bbe790ffa33d028d72b3a20cfe89d9f\",\"title\":\"Exploiting Visual Semantic Reasoning for Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/1ab464a59bbe790ffa33d028d72b3a20cfe89d9f\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"120800460\",\"name\":\"L. Zhou\"},{\"authorId\":\"98110081\",\"name\":\"J. Gao\"},{\"authorId\":\"49620738\",\"name\":\"Di Li\"},{\"authorId\":\"70362337\",\"name\":\"Heung-Yeung Shum\"}],\"doi\":\"10.1162/coli_a_00368\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3737b57e8b739b595cd5b877769cbc719c42f2c7\",\"title\":\"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot\",\"url\":\"https://www.semanticscholar.org/paper/3737b57e8b739b595cd5b877769cbc719c42f2c7\",\"venue\":\"Computational Linguistics\",\"year\":2020},{\"arxivId\":\"2004.10966\",\"authors\":[{\"authorId\":\"1389550960\",\"name\":\"Tasmia Tasrin\"},{\"authorId\":\"1381931976\",\"name\":\"Md Sultan Al Nahian\"},{\"authorId\":\"34442699\",\"name\":\"B. Harrison\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"32b2ed6a8eba971cdcd343af0dc5171636a268b4\",\"title\":\"Visual Question Answering Using Semantic Information from Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/32b2ed6a8eba971cdcd343af0dc5171636a268b4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736464\",\"name\":\"M. Khademi\"}],\"doi\":\"10.18653/v1/2020.acl-main.643\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e600553fe2f21630f2a272c80de4ce0e0363cb9c\",\"title\":\"Multimodal Neural Graph Memory Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e600553fe2f21630f2a272c80de4ce0e0363cb9c\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400258083\",\"name\":\"Chunpu Xu\"},{\"authorId\":\"1940342\",\"name\":\"Y. Li\"},{\"authorId\":\"48161719\",\"name\":\"C. Li\"},{\"authorId\":\"1605170624\",\"name\":\"Xiang Ao\"},{\"authorId\":\"1492164677\",\"name\":\"Min Yang\"},{\"authorId\":\"39090191\",\"name\":\"Jin-Wen Tian\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"17c2549052978c3be85351541f69bf2b25e75f5f\",\"title\":\"Interactive Key-Value Memory-augmented Attention for Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/17c2549052978c3be85351541f69bf2b25e75f5f\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50031180\",\"name\":\"Xiaofeng Yang\"},{\"authorId\":\"2604251\",\"name\":\"Guosheng Lin\"},{\"authorId\":\"2753987\",\"name\":\"Fengmao Lv\"},{\"authorId\":\"2254178\",\"name\":\"Fayao Liu\"}],\"doi\":\"10.1007/978-3-030-58589-1_25\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ef765e3e0b7a2a0fc90322c13658480d2b1cb4d9\",\"title\":\"TRRNet: Tiered Relation Reasoning for Compositional Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ef765e3e0b7a2a0fc90322c13658480d2b1cb4d9\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.12126\",\"authors\":[{\"authorId\":\"35432059\",\"name\":\"L. Ren\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"1390771606\",\"name\":\"Liqiang Wang\"},{\"authorId\":\"1730455\",\"name\":\"K. Hua\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"065af7ecb52354be79f538dac3ca210bf57e7739\",\"title\":\"Beyond the Deep Metric Learning: Enhance the Cross-Modal Matching with Adversarial Discriminative Domain Regularization\",\"url\":\"https://www.semanticscholar.org/paper/065af7ecb52354be79f538dac3ca210bf57e7739\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1709.08693\",\"authors\":[{\"authorId\":\"48670486\",\"name\":\"X. Xu\"},{\"authorId\":\"2727656\",\"name\":\"X. Chen\"},{\"authorId\":\"28969396\",\"name\":\"C. Liu\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"143711382\",\"name\":\"D. Song\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dda1822872942f658b89e7e1c1ffe08c35e7b290\",\"title\":\"Can you fool AI with adversarial examples on a visual Turing test?\",\"url\":\"https://www.semanticscholar.org/paper/dda1822872942f658b89e7e1c1ffe08c35e7b290\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chi Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ad9e18e919b4452bd6f021fd7c52c0e361f62d4\",\"title\":\"Evolution of A Common Vector Space Approach to Multi-Modal Problems\",\"url\":\"https://www.semanticscholar.org/paper/5ad9e18e919b4452bd6f021fd7c52c0e361f62d4\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1909.10128\",\"authors\":[{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"51473867\",\"name\":\"Bailin Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4cede1c63336de84344922876e6ee23617e2afb3\",\"title\":\"Explainable High-order Visual Question Reasoning: A New Benchmark and Knowledge-routed Network\",\"url\":\"https://www.semanticscholar.org/paper/4cede1c63336de84344922876e6ee23617e2afb3\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49891089\",\"name\":\"Y. Zhang\"},{\"authorId\":\"9610896\",\"name\":\"Y. Ding\"},{\"authorId\":\"144265847\",\"name\":\"Rui Wu\"},{\"authorId\":\"50822330\",\"name\":\"F. Xue\"}],\"doi\":\"10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00151\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4a08403806ca61227b7a4780094fd8e652379362\",\"title\":\"A Denoising Framework for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/4a08403806ca61227b7a4780094fd8e652379362\",\"venue\":\"2019 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)\",\"year\":2019},{\"arxivId\":\"1910.09307\",\"authors\":[{\"authorId\":\"1524733293\",\"name\":\"Xueting Wang\"},{\"authorId\":\"1591133899\",\"name\":\"Yiwei Zhang\"},{\"authorId\":\"145572097\",\"name\":\"T. Yamasaki\"}],\"doi\":\"10.1145/3343031.3350920\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19ccd7817d76dfb107a533e40a1350c0349e4c91\",\"title\":\"User-Aware Folk Popularity Rank: User-Popularity-Based Tag Recommendation That Can Enhance Social Popularity\",\"url\":\"https://www.semanticscholar.org/paper/19ccd7817d76dfb107a533e40a1350c0349e4c91\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491093092\",\"name\":\"Caiyong Wang\"},{\"authorId\":null,\"name\":\"Yunlong Wang\"},{\"authorId\":\"1860829\",\"name\":\"Yunfan Liu\"},{\"authorId\":\"2510474\",\"name\":\"Zhaofeng He\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"},{\"authorId\":\"1757186\",\"name\":\"Z. Sun\"}],\"doi\":\"10.1109/TBIOM.2019.2962190\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0cae17826fbac5a1364349d9ffe0020b6b51622\",\"title\":\"ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/f0cae17826fbac5a1364349d9ffe0020b6b51622\",\"venue\":\"IEEE Transactions on Biometrics, Behavior, and Identity Science\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151500672\",\"name\":\"Mohammed Suhail\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.01046\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"129173467de277e538766a882af144a2857c3580\",\"title\":\"Mixture-Kernel Graph Attention Network for Situation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/129173467de277e538766a882af144a2857c3580\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46608477\",\"name\":\"Ningning Guo\"},{\"authorId\":\"31833173\",\"name\":\"H. Liu\"},{\"authorId\":\"47420475\",\"name\":\"Linhua Jiang\"}],\"doi\":\"10.1109/ICARM.2019.8834066\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02ac11a09db5d0f8a52d428e6d9ab64dba0535cf\",\"title\":\"Attention-based Visual-Audio Fusion for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/02ac11a09db5d0f8a52d428e6d9ab64dba0535cf\",\"venue\":\"2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2846025\",\"name\":\"D. Yu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"40434674\",\"name\":\"X. Tian\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3316767\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b0ce44270916fd6e254fd9e75dd77ee1cf9f212a\",\"title\":\"Multi-source Multi-level Attention Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b0ce44270916fd6e254fd9e75dd77ee1cf9f212a\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1906.06216\",\"authors\":[{\"authorId\":\"51270689\",\"name\":\"Hyounghun Kim\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P19-1351\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8501712706efa6f314438143de18507471781060\",\"title\":\"Improving Visual Question Answering by Referring to Generated Paragraph Captions\",\"url\":\"https://www.semanticscholar.org/paper/8501712706efa6f314438143de18507471781060\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"2006.03347\",\"authors\":[{\"authorId\":\"10752412\",\"name\":\"L. Cultrera\"},{\"authorId\":\"2831602\",\"name\":\"Lorenzo Seidenari\"},{\"authorId\":\"41172759\",\"name\":\"Federico Becattini\"},{\"authorId\":\"70892589\",\"name\":\"Pietro Pala\"},{\"authorId\":\"8196487\",\"name\":\"A. D. Bimbo\"}],\"doi\":\"10.1109/CVPRW50498.2020.00178\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8bf6619144d3fb32877794edf216b1da89632772\",\"title\":\"Explaining Autonomous Driving by Learning End-to-End Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/8bf6619144d3fb32877794edf216b1da89632772\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490935563\",\"name\":\"Feifei Zhang\"},{\"authorId\":\"49235537\",\"name\":\"M. Xu\"},{\"authorId\":\"3069077\",\"name\":\"Q. Mao\"},{\"authorId\":\"48258806\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3394171.3413917\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"50bd8474487073851b115385b9ded538d8825bc2\",\"title\":\"Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/50bd8474487073851b115385b9ded538d8825bc2\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8249814\",\"name\":\"Xujun Peng\"},{\"authorId\":\"80578043\",\"name\":\"C. Wang\"},{\"authorId\":\"39784761\",\"name\":\"H. Cao\"}],\"doi\":\"10.1109/ICDAR.2019.00017\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"56ebac51b0adb1cc6216c5f92ebc50ca4fbff652\",\"title\":\"Document Binarization via Multi-resolutional Attention Model with DRD Loss\",\"url\":\"https://www.semanticscholar.org/paper/56ebac51b0adb1cc6216c5f92ebc50ca4fbff652\",\"venue\":\"2019 International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49876189\",\"name\":\"T. Yang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/ICCV.2019.00265\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ecc5cd01261cf9c396689121a3e8c1844c825775\",\"title\":\"Making History Matter: History-Advantage Sequence Training for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/ecc5cd01261cf9c396689121a3e8c1844c825775\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"49470127\",\"name\":\"X. Zhang\"},{\"authorId\":\"48593034\",\"name\":\"Guohua Dong\"},{\"authorId\":\"1406222945\",\"name\":\"Zhigang Luo\"}],\"doi\":\"10.1007/s11042-019-08209-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"311ee1284e6bdf035f8c3a1a20d00c5841b2e341\",\"title\":\"Object-aware semantics of attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/311ee1284e6bdf035f8c3a1a20d00c5841b2e341\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"2010.03855\",\"authors\":[{\"authorId\":\"40622539\",\"name\":\"Dong-Jin Kim\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"49331178\",\"name\":\"Jinsoo Choi\"},{\"authorId\":\"145017149\",\"name\":\"In So Kweon\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0bda89ebb8cbec54afb013349c97a8264b36fee0\",\"title\":\"Dense Relational Image Captioning via Multi-task Triple-Stream Networks\",\"url\":\"https://www.semanticscholar.org/paper/0bda89ebb8cbec54afb013349c97a8264b36fee0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35212676\",\"name\":\"J. Peyre\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"caffa07ead18aae78bf654bc57023eef58e74faf\",\"title\":\"Learning to detect visual relations\",\"url\":\"https://www.semanticscholar.org/paper/caffa07ead18aae78bf654bc57023eef58e74faf\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38994364\",\"name\":\"Tu-Khiem Le\"},{\"authorId\":\"7736164\",\"name\":\"Van-Tu Ninh\"},{\"authorId\":\"1780348\",\"name\":\"M. Tran\"},{\"authorId\":\"7213584\",\"name\":\"Thanh-An Nguyen\"},{\"authorId\":\"1737836940\",\"name\":\"Hai-Dang Nguyen\"},{\"authorId\":\"1390638216\",\"name\":\"Liting Zhou\"},{\"authorId\":\"30978009\",\"name\":\"Graham Healy\"},{\"authorId\":\"1737981\",\"name\":\"C. Gurrin\"}],\"doi\":\"10.1145/3379172.3391724\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"46cbe0adff3c88ec1613871ddb0f75b895dc8912\",\"title\":\"LifeSeeker 2.0: Interactive Lifelog Search Engine at LSC 2020\",\"url\":\"https://www.semanticscholar.org/paper/46cbe0adff3c88ec1613871ddb0f75b895dc8912\",\"venue\":\"LSC@ICMR\",\"year\":2020},{\"arxivId\":\"1908.01323\",\"authors\":[{\"authorId\":\"143707205\",\"name\":\"B. Ding\"},{\"authorId\":\"48015811\",\"name\":\"Chengjiang Long\"},{\"authorId\":\"13800342\",\"name\":\"L. Zhang\"},{\"authorId\":\"2420700\",\"name\":\"Chunxia Xiao\"}],\"doi\":\"10.1109/ICCV.2019.01031\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1e9452b05457233e7761d36642dc8ca8a04dd8e\",\"title\":\"ARGAN: Attentive Recurrent Generative Adversarial Network for Shadow Detection and Removal\",\"url\":\"https://www.semanticscholar.org/paper/a1e9452b05457233e7761d36642dc8ca8a04dd8e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1906.08595\",\"authors\":[{\"authorId\":\"145045115\",\"name\":\"C. Otto\"},{\"authorId\":\"3430468\",\"name\":\"Matthias Springstein\"},{\"authorId\":\"39775488\",\"name\":\"Avishek Anand\"},{\"authorId\":\"1738703\",\"name\":\"R. Ewerth\"}],\"doi\":\"10.1145/3323873.3325049\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ca034c6005aa551063e68c50a3ba1bab4ff67ed\",\"title\":\"Understanding, Categorizing and Predicting Semantic Image-Text Relations\",\"url\":\"https://www.semanticscholar.org/paper/5ca034c6005aa551063e68c50a3ba1bab4ff67ed\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":\"1909.06023\",\"authors\":[{\"authorId\":\"50812963\",\"name\":\"Xinyu Zhang\"},{\"authorId\":\"1807746211\",\"name\":\"Rufeng Zhang\"},{\"authorId\":\"39767248\",\"name\":\"Jiewei Cao\"},{\"authorId\":\"145542268\",\"name\":\"Dong Gong\"},{\"authorId\":\"1810722839\",\"name\":\"Mingyu You\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"}],\"doi\":\"10.1109/tits.2020.3030301\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e1767c7281096035693afc07050bb7a909db230\",\"title\":\"Part-Guided Attention Learning for Vehicle Instance Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/2e1767c7281096035693afc07050bb7a909db230\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13867206\",\"name\":\"X. Wang\"},{\"authorId\":\"2099254\",\"name\":\"R. Hou\"},{\"authorId\":\"1561664268\",\"name\":\"Boyan Zhao\"},{\"authorId\":\"1563865309\",\"name\":\"Fengkai Yuan\"},{\"authorId\":\"50560888\",\"name\":\"Jun Zhang\"},{\"authorId\":\"144896516\",\"name\":\"D. Meng\"},{\"authorId\":\"2064331\",\"name\":\"Xuehai Qian\"}],\"doi\":\"10.1145/3373376.3378532\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19c61093e89be16073c68f74c42287177023f486\",\"title\":\"DNNGuard: An Elastic Heterogeneous DNN Accelerator Architecture against Adversarial Attacks\",\"url\":\"https://www.semanticscholar.org/paper/19c61093e89be16073c68f74c42287177023f486\",\"venue\":\"ASPLOS\",\"year\":2020},{\"arxivId\":\"2002.11848\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"2490189\",\"name\":\"Gregory Shakhnarovich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ed8c8ee62b0afc403e44c29f6deedf6632885d5\",\"title\":\"Analysis of diversity-accuracy tradeoff in image captioning\",\"url\":\"https://www.semanticscholar.org/paper/9ed8c8ee62b0afc403e44c29f6deedf6632885d5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.02664\",\"authors\":[{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"48985581\",\"name\":\"Manli Zhang\"},{\"authorId\":\"47538869\",\"name\":\"J. Zhang\"},{\"authorId\":\"1776220\",\"name\":\"Zhiwu Lu\"},{\"authorId\":\"2259699\",\"name\":\"Ji-Rong Wen\"}],\"doi\":\"10.1109/CVPR.2019.00684\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"41db31c451cd819d22f9c0b90be110edc4424911\",\"title\":\"Recursive Visual Attention in Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/41db31c451cd819d22f9c0b90be110edc4424911\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1905.10515\",\"authors\":[{\"authorId\":\"2153067\",\"name\":\"Baohua Sun\"},{\"authorId\":\"144890162\",\"name\":\"L. Yang\"},{\"authorId\":\"144485124\",\"name\":\"M. Lin\"},{\"authorId\":\"50674008\",\"name\":\"C. Young\"},{\"authorId\":\"46195424\",\"name\":\"P. Dong\"},{\"authorId\":\"47528094\",\"name\":\"Wenhan Zhang\"},{\"authorId\":\"35287113\",\"name\":\"Jason Dong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70471f7d38c875e6e2912737729043e67272c326\",\"title\":\"SuperCaptioning: Image Captioning Using Two-dimensional Word Embedding\",\"url\":\"https://www.semanticscholar.org/paper/70471f7d38c875e6e2912737729043e67272c326\",\"venue\":\"BigMine@KDD\",\"year\":2019},{\"arxivId\":\"1811.12772\",\"authors\":[{\"authorId\":\"6328564\",\"name\":\"M. Farazi\"},{\"authorId\":\"1931655\",\"name\":\"S. Khan\"},{\"authorId\":\"1712576\",\"name\":\"N. Barnes\"}],\"doi\":\"10.1016/j.imavis.2020.103985\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a678b68abd4047d5342f64725f57a04647a47711\",\"title\":\"From Known to the Unknown: Transferring Knowledge to Answer Questions about Novel Visual and Semantic Concepts\",\"url\":\"https://www.semanticscholar.org/paper/a678b68abd4047d5342f64725f57a04647a47711\",\"venue\":\"Image Vis. Comput.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9024867\",\"name\":\"Jongkwang Hong\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"2847986\",\"name\":\"Youngjung Uh\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"144036125\",\"name\":\"H. Byun\"}],\"doi\":\"10.1016/J.NEUCOM.2019.03.035\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b09f952de35e1ce98b01e14c2be036430ecace43\",\"title\":\"Exploiting hierarchical visual features for visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/b09f952de35e1ce98b01e14c2be036430ecace43\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042337560\",\"name\":\"Saketh Vishnubhatla\"},{\"authorId\":\"2303852\",\"name\":\"N. Sinha\"}],\"doi\":\"10.1145/3430984.3431059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38b047cd1373ccd67aa52c588068fd652d52b6e1\",\"title\":\"Image Captioning with Pretrained Language Generators\",\"url\":\"https://www.semanticscholar.org/paper/38b047cd1373ccd67aa52c588068fd652d52b6e1\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"2010.12852\",\"authors\":[{\"authorId\":\"34408936\",\"name\":\"R. Dua\"},{\"authorId\":\"2003624403\",\"name\":\"Sai Srinivas Kancheti\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fc80d27ca291823fc0be464a736cf72dbb4ac191\",\"title\":\"Beyond VQA: Generating Multi-word Answer and Rationale to Visual Questions\",\"url\":\"https://www.semanticscholar.org/paper/fc80d27ca291823fc0be464a736cf72dbb4ac191\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27751815\",\"name\":\"A. H. Mirza\"},{\"authorId\":\"51217540\",\"name\":\"Mine Kerpicci\"},{\"authorId\":\"1685178\",\"name\":\"S. Kozat\"}],\"doi\":\"10.1016/j.dsp.2020.102742\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e049de93cc1931bf4bbda8efad9666ae5cf75bf\",\"title\":\"Efficient online learning with improved LSTM neural networks\",\"url\":\"https://www.semanticscholar.org/paper/2e049de93cc1931bf4bbda8efad9666ae5cf75bf\",\"venue\":\"Digit. Signal Process.\",\"year\":2020},{\"arxivId\":\"2006.00753\",\"authors\":[{\"authorId\":\"2429278\",\"name\":\"Chenyu Gao\"},{\"authorId\":\"1476704317\",\"name\":\"Qi Zhu\"},{\"authorId\":\"1585288737\",\"name\":\"Peng Wang\"},{\"authorId\":null,\"name\":\"Hui Li\"},{\"authorId\":\"46398380\",\"name\":\"Y. Liu\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeab710608359ec5ada5fe6fe36da134291dd5cb\",\"title\":\"Structured Multimodal Attentions for TextVQA\",\"url\":\"https://www.semanticscholar.org/paper/eeab710608359ec5ada5fe6fe36da134291dd5cb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2452223\",\"name\":\"Kazuto Nakashima\"},{\"authorId\":\"1795530\",\"name\":\"Y. Iwashita\"},{\"authorId\":\"1801213\",\"name\":\"R. Kurazume\"}],\"doi\":\"10.1186/s40648-020-00181-2\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"76b4b7aa99a8434d80f3af553fbfa013f5cbfe6b\",\"title\":\"Lifelogging caption generation via fourth-person vision in a human\\u2013robot symbiotic environment\",\"url\":\"https://www.semanticscholar.org/paper/76b4b7aa99a8434d80f3af553fbfa013f5cbfe6b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.03743\",\"authors\":[{\"authorId\":\"13941532\",\"name\":\"Fuxiao Liu\"},{\"authorId\":null,\"name\":\"Yinghan Wang\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e5bcdd0366c3ddd1579159595d1990f515cf93b\",\"title\":\"VisualNews : A Large Multi-source News Image Dataset\",\"url\":\"https://www.semanticscholar.org/paper/7e5bcdd0366c3ddd1579159595d1990f515cf93b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1908.06354\",\"authors\":[{\"authorId\":\"21518096\",\"name\":\"Zhengyuan Yang\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"},{\"authorId\":\"46659782\",\"name\":\"L. Wang\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":null,\"name\":\"Dong Yu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/ICCV.2019.00478\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"46559d98a516a9520afa0638bba28f5fe7fb4ceb\",\"title\":\"A Fast and Accurate One-Stage Approach to Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/46559d98a516a9520afa0638bba28f5fe7fb4ceb\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993672539\",\"name\":\"Soumil Kanwal\"},{\"authorId\":\"1993398975\",\"name\":\"Vineet Mehta\"},{\"authorId\":\"1735697\",\"name\":\"Abhinav Dhall\"}],\"doi\":\"10.1145/3394171.3416302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d51986070dbf6b76db4e553faf7dc301655e3ce4\",\"title\":\"Large Scale Hierarchical Anomaly Detection and Temporal Localization\",\"url\":\"https://www.semanticscholar.org/paper/d51986070dbf6b76db4e553faf7dc301655e3ce4\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443435125\",\"name\":\"Zhengcong Fei\"}],\"doi\":\"10.24963/ijcai.2020/599\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be1b41ef45cd10a95b8ec10e827fb9ac79f44a85\",\"title\":\"Improving Tandem Mass Spectra Analysis with Hierarchical Learning\",\"url\":\"https://www.semanticscholar.org/paper/be1b41ef45cd10a95b8ec10e827fb9ac79f44a85\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49890682\",\"name\":\"Y. Zhang\"},{\"authorId\":\"9407393\",\"name\":\"Y. Chen\"},{\"authorId\":\"145544640\",\"name\":\"C. Huang\"},{\"authorId\":\"2135613\",\"name\":\"Ming-ke Gao\"}],\"doi\":\"10.3390/fi11010009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fa5b4b5ab5bda322c1c81ab2f1c4789eab4961db\",\"title\":\"Object Detection Network Based on Feature Fusion and Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/fa5b4b5ab5bda322c1c81ab2f1c4789eab4961db\",\"venue\":\"Future Internet\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"},{\"authorId\":\"145952554\",\"name\":\"Michael Chan\"},{\"authorId\":\"28130078\",\"name\":\"P. P. Liang\"},{\"authorId\":\"31726556\",\"name\":\"Edmund Tong\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/CVPR.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"title\":\"Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.08607\",\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"103278467\",\"name\":\"Minuk Ma\"},{\"authorId\":\"4604969\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"2561991\",\"name\":\"S. Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/CVPR.2019.00853\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b621afeef5888f8f71fd6ca97a62daa0d0cb6d69\",\"title\":\"Progressive Attention Memory Network for Movie Story Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b621afeef5888f8f71fd6ca97a62daa0d0cb6d69\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35141315\",\"name\":\"A. Garcia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7603c943fd03c22b534c20b1defc26a9b8ac2c9c\",\"title\":\"Pr\\u00e9diction de mod\\u00e8les structur\\u00e9s d'opinion : aspects th\\u00e9oriques et m\\u00e9thodologiques. (Prediction of structured opinion outputs : theoretical and methodological aspects)\",\"url\":\"https://www.semanticscholar.org/paper/7603c943fd03c22b534c20b1defc26a9b8ac2c9c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143912737\",\"name\":\"Heather Riley\"},{\"authorId\":\"1714890\",\"name\":\"M. Sridharan\"}],\"doi\":\"10.3389/frobt.2019.00125\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e9ae2f99dd2ae29f4bfd220446175bb854db2008\",\"title\":\"Integrating Non-monotonic Logical Reasoning and Inductive Learning With Deep Learning for Explainable Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e9ae2f99dd2ae29f4bfd220446175bb854db2008\",\"venue\":\"Front. Robot. AI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35935570\",\"name\":\"Jean-Benoit Delbrouck\"},{\"authorId\":\"1388031809\",\"name\":\"Antoine Maiorca\"},{\"authorId\":\"1388031811\",\"name\":\"Nathan Hubens\"},{\"authorId\":\"153352427\",\"name\":\"S. Dupont\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b2960fd63507253d1d6a19802068666de478bfc0\",\"title\":\"C V ] 8 O ct 2 01 9 Modulated Self-attention Convolutional Network for VQA\",\"url\":\"https://www.semanticscholar.org/paper/b2960fd63507253d1d6a19802068666de478bfc0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.1016/j.csl.2020.101095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"661f04ecc734ced906e16980a6143c814ce085ed\",\"title\":\"Hierarchical multimodal attention for end-to-end audio-visual scene-aware dialogue response generation\",\"url\":\"https://www.semanticscholar.org/paper/661f04ecc734ced906e16980a6143c814ce085ed\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":\"2002.08510\",\"authors\":[{\"authorId\":\"7934161\",\"name\":\"T. Chen\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1609/AAAI.V34I07.6631\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f1e3f995168b008637a049cbef6a5266986cb338\",\"title\":\"Expressing Objects just like Words: Recurrent Visual Embedding for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/f1e3f995168b008637a049cbef6a5266986cb338\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1910.04964\",\"authors\":[{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"72541452\",\"name\":\"X. Wang\"},{\"authorId\":\"1786871\",\"name\":\"Hongzhi Li\"}],\"doi\":\"10.1109/TCSVT.2019.2940647\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3d92574b3eb8a90736afb0d6a56f581eee79bdea\",\"title\":\"Multi-Modal Deep Analysis for Multimedia\",\"url\":\"https://www.semanticscholar.org/paper/3d92574b3eb8a90736afb0d6a56f581eee79bdea\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66073991\",\"name\":\"Edoardo Alati\"},{\"authorId\":\"65770249\",\"name\":\"L. Mauro\"},{\"authorId\":\"2368860\",\"name\":\"Valsamis Ntouskos\"},{\"authorId\":\"1695016\",\"name\":\"F. Pirri\"}],\"doi\":\"10.1007/978-3-030-29516-5_60\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"422910cd883a39a42e40d6630997c95cb1864d44\",\"title\":\"Anticipating Next Goal for Robot Plan Prediction\",\"url\":\"https://www.semanticscholar.org/paper/422910cd883a39a42e40d6630997c95cb1864d44\",\"venue\":\"IntelliSys\",\"year\":2019},{\"arxivId\":\"1905.10709\",\"authors\":[{\"authorId\":\"22380229\",\"name\":\"Doyup Lee\"},{\"authorId\":\"90068709\",\"name\":\"Suehun Jung\"},{\"authorId\":\"3144423\",\"name\":\"Yeongjae Cheon\"},{\"authorId\":\"90093230\",\"name\":\"Dong-il Kim\"},{\"authorId\":\"2303316\",\"name\":\"S. You\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c58734e1fb2c402d6cf543684d2db5b2e25960d8\",\"title\":\"Demand Forecasting from Spatiotemporal Data with Graph Networks and Temporal-Guided Embedding\",\"url\":\"https://www.semanticscholar.org/paper/c58734e1fb2c402d6cf543684d2db5b2e25960d8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2012.08673\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c8f6878dac0c5c99c192ded715d2e864eed7302\",\"title\":\"A Closer Look at the Robustness of Vision-and-Language Pre-trained Models\",\"url\":\"https://www.semanticscholar.org/paper/3c8f6878dac0c5c99c192ded715d2e864eed7302\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1803.09845\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"94908120\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2018.00754\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"title\":\"Neural Baby Talk\",\"url\":\"https://www.semanticscholar.org/paper/3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40055538\",\"name\":\"C. Yang\"},{\"authorId\":\"144889898\",\"name\":\"Mengqi Jiang\"},{\"authorId\":\"144069314\",\"name\":\"Bin Jiang\"},{\"authorId\":\"46351963\",\"name\":\"Weixin Zhou\"},{\"authorId\":\"2181606\",\"name\":\"K. Li\"}],\"doi\":\"10.1109/ACCESS.2019.2908035\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0734fd670605a578b9e4b908e58b63e4142625e\",\"title\":\"Co-Attention Network With Question Type for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f0734fd670605a578b9e4b908e58b63e4142625e\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"2003.04998\",\"authors\":[{\"authorId\":null,\"name\":\"Yitong Li\"},{\"authorId\":\"7379232\",\"name\":\"Dianqi Li\"},{\"authorId\":\"28612243\",\"name\":\"S. Prakash\"},{\"authorId\":\"49161455\",\"name\":\"P. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0fa920ea66b7642eb97df389b7e32faa9744b3e\",\"title\":\"Toward Interpretability of Dual-Encoder Models for Dialogue Response Suggestions\",\"url\":\"https://www.semanticscholar.org/paper/f0fa920ea66b7642eb97df389b7e32faa9744b3e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31696860\",\"name\":\"Shurong Sheng\"},{\"authorId\":\"35301620\",\"name\":\"Katrien Laenen\"},{\"authorId\":\"145446752\",\"name\":\"Marie-Francine Moens\"}],\"doi\":\"10.1007/978-3-030-15719-7_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d75b686c7d32680f5e5aeeefaffff6dab06d31ee\",\"title\":\"Can Image Captioning Help Passage Retrieval in Multimodal Question Answering?\",\"url\":\"https://www.semanticscholar.org/paper/d75b686c7d32680f5e5aeeefaffff6dab06d31ee\",\"venue\":\"ECIR\",\"year\":2019},{\"arxivId\":\"1908.05067\",\"authors\":[{\"authorId\":\"47999368\",\"name\":\"Yi-Ting Yeh\"},{\"authorId\":\"145514809\",\"name\":\"Tzu-Chuan Lin\"},{\"authorId\":\"152498628\",\"name\":\"Hsiao-Hua Cheng\"},{\"authorId\":\"152141374\",\"name\":\"Yu-Hsuan Deng\"},{\"authorId\":\"27629426\",\"name\":\"Shang-Yu Su\"},{\"authorId\":\"1725643\",\"name\":\"Yun-Nung (Vivian) Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f2539fad727e1def50b87fc88d3a499b4fe1f393\",\"title\":\"Reactive Multi-Stage Feature Fusion for Multimodal Dialogue Modeling\",\"url\":\"https://www.semanticscholar.org/paper/f2539fad727e1def50b87fc88d3a499b4fe1f393\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134518946\",\"name\":\"Nanxing Li\"},{\"authorId\":\"50678073\",\"name\":\"Bei Liu\"},{\"authorId\":\"2904100\",\"name\":\"Z. Han\"},{\"authorId\":\"46399275\",\"name\":\"Yu-Shen Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"}],\"doi\":\"10.1145/3323873.3325050\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"426ce52b0c4ac73e6226c532d4d8a2171f12e9fb\",\"title\":\"Emotion Reinforced Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/426ce52b0c4ac73e6226c532d4d8a2171f12e9fb\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144101175\",\"name\":\"Zhou Lei\"},{\"authorId\":\"1831825\",\"name\":\"Congcong Zhou\"},{\"authorId\":\"35155467\",\"name\":\"Shengbo Chen\"},{\"authorId\":\"1722738282\",\"name\":\"Yiyong Huang\"},{\"authorId\":\"1726027121\",\"name\":\"Xianrui Liu\"}],\"doi\":\"10.1109/ACCESS.2020.3024639\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e85fdae1b5e297f1d8925c9bd5cd7e243a305116\",\"title\":\"A Sparse Transformer-Based Approach for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e85fdae1b5e297f1d8925c9bd5cd7e243a305116\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144669071\",\"name\":\"Shah Nawaz\"},{\"authorId\":\"3457883\",\"name\":\"Alessandro Calefati\"},{\"authorId\":\"1486416823\",\"name\":\"Moreno Caraffini\"},{\"authorId\":\"1387994112\",\"name\":\"Nicola Landro\"},{\"authorId\":\"145116184\",\"name\":\"I. Gallo\"}],\"doi\":\"10.1109/IVCNZ48456.2019.8960960\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e69953361ce959aa85ff795bee593e589c1fd867\",\"title\":\"Are These Birds Similar: Learning Branched Networks for Fine-grained Representations\",\"url\":\"https://www.semanticscholar.org/paper/e69953361ce959aa85ff795bee593e589c1fd867\",\"venue\":\"2019 International Conference on Image and Vision Computing New Zealand (IVCNZ)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1519968024\",\"name\":\"X. Wang\"},{\"authorId\":\"48202088\",\"name\":\"Min Zhi\"}],\"doi\":\"10.1117/12.2557219\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e5be1b2baceb2a37b4cbf082e8d56f209c4574d\",\"title\":\"Summary of object detection based on convolutional neural network\",\"url\":\"https://www.semanticscholar.org/paper/2e5be1b2baceb2a37b4cbf082e8d56f209c4574d\",\"venue\":\"International Conference on Graphic and Image Processing\",\"year\":2020},{\"arxivId\":\"2009.12524\",\"authors\":[{\"authorId\":\"1972263989\",\"name\":\"Zanyar Zohourianshahzadi\"},{\"authorId\":\"34694214\",\"name\":\"J. Kalita\"}],\"doi\":\"10.1109/HCCAI49649.2020.00009\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d3cf77a7eb87bf9c85c4631b196c4eb10ff5b51e\",\"title\":\"Neural Twins Talk\",\"url\":\"https://www.semanticscholar.org/paper/d3cf77a7eb87bf9c85c4631b196c4eb10ff5b51e\",\"venue\":\"2020 IEEE International Conference on Humanized Computing and Communication with Artificial Intelligence (HCCAI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492164677\",\"name\":\"Min Yang\"},{\"authorId\":\"2948588\",\"name\":\"Junhao Liu\"},{\"authorId\":\"143822675\",\"name\":\"Ying Shen\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"40265331\",\"name\":\"X. Chen\"},{\"authorId\":\"31060469\",\"name\":\"Qingyao Wu\"},{\"authorId\":\"48161719\",\"name\":\"C. Li\"}],\"doi\":\"10.1109/TIP.2020.3028651\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f6a09cd467a2752e60a2766160a00c658667043e\",\"title\":\"An Ensemble of Generation- and Retrieval-Based Image Captioning With Dual Generator Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/f6a09cd467a2752e60a2766160a00c658667043e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2004.05704\",\"authors\":[{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.18653/v1/2020.acl-main.727\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2f4a012aa325cdee5a5c779fe2133e146616a5d5\",\"title\":\"A negative case analysis of visual grounding methods for VQA\",\"url\":\"https://www.semanticscholar.org/paper/2f4a012aa325cdee5a5c779fe2133e146616a5d5\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1908.08191\",\"authors\":[{\"authorId\":\"152923110\",\"name\":\"Kuan-Yen Lin\"},{\"authorId\":\"23608432\",\"name\":\"Chao-Chun Hsu\"},{\"authorId\":\"1725643\",\"name\":\"Yun-Nung (Vivian) Chen\"},{\"authorId\":\"1746959\",\"name\":\"Lun-Wei Ku\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee1add32f430fb6e9f82958dc431b635174ee9bd\",\"title\":\"Entropy-Enhanced Multimodal Attention Model for Scene-Aware Dialogue Generation\",\"url\":\"https://www.semanticscholar.org/paper/ee1add32f430fb6e9f82958dc431b635174ee9bd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49969428\",\"name\":\"Z. Li\"},{\"authorId\":\"1490937293\",\"name\":\"Feng Ling\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"}],\"doi\":\"10.1109/DSAA.2019.00029\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6cce8fc497e5c780bb3e8068541ae16b969b1042\",\"title\":\"Cross-Media Image-Text Retrieval Combined with Global Similarity and Local Similarity\",\"url\":\"https://www.semanticscholar.org/paper/6cce8fc497e5c780bb3e8068541ae16b969b1042\",\"venue\":\"2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"3374688\",\"name\":\"M. Tian\"},{\"authorId\":\"151118825\",\"name\":\"Mengmeng Jiang\"},{\"authorId\":null,\"name\":\"Heng Liu\"},{\"authorId\":\"30631999\",\"name\":\"Donghu Deng\"}],\"doi\":\"10.1016/J.PATREC.2019.07.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"749f376d0addb83569fcc7536e46308abbb232d4\",\"title\":\"How much do cross-modal related semantics benefit image captioning by weighting attributes and re-ranking sentences?\",\"url\":\"https://www.semanticscholar.org/paper/749f376d0addb83569fcc7536e46308abbb232d4\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":\"2010.12267\",\"authors\":[{\"authorId\":\"1519970315\",\"name\":\"Xinsheng Wang\"},{\"authorId\":\"1514891263\",\"name\":\"Siyuan Feng\"},{\"authorId\":\"2237811\",\"name\":\"Jihua Zhu\"},{\"authorId\":\"1399115926\",\"name\":\"M. Hasegawa-Johnson\"},{\"authorId\":\"144951859\",\"name\":\"O. Scharenborg\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d7b2a37fac581795101120989c0ff76147938a4a\",\"title\":\"Show and Speak: Directly Synthesize Spoken Description of Images\",\"url\":\"https://www.semanticscholar.org/paper/d7b2a37fac581795101120989c0ff76147938a4a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1998926555\",\"name\":\"Chenxi Yuan\"},{\"authorId\":\"2027167977\",\"name\":\"Yang Bai\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"68c11851ace525b233f05b985f8acf887d03d379\",\"title\":\"Bridge the Gap: High-level Semantic Planning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/68c11851ace525b233f05b985f8acf887d03d379\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2010.05379\",\"authors\":[{\"authorId\":\"27866536\",\"name\":\"Q. Wang\"},{\"authorId\":\"1725438390\",\"name\":\"H. Tan\"},{\"authorId\":\"2191455\",\"name\":\"Sheng Shen\"},{\"authorId\":\"1717098\",\"name\":\"M. W. Mahoney\"},{\"authorId\":\"9088433\",\"name\":\"Zhewei Yao\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.159\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f68429e4cb119d3ac6cfbc9be7476609878fd2fa\",\"title\":\"MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/f68429e4cb119d3ac6cfbc9be7476609878fd2fa\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.02384\",\"authors\":[{\"authorId\":\"150324514\",\"name\":\"Tejas Srinivasan\"},{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.242\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fc34c448b8b01be06716ec6f508ce262adbb5f53\",\"title\":\"Fine-Grained Grounding for Multimodal Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fc34c448b8b01be06716ec6f508ce262adbb5f53\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145098620\",\"name\":\"M. Ortiz\"},{\"authorId\":\"1683950\",\"name\":\"L. M. Bergasa\"},{\"authorId\":\"144347549\",\"name\":\"R. Arroyo\"},{\"authorId\":\"115023578\",\"name\":\"Sergio \\u00c1lvarez\"},{\"authorId\":\"153051245\",\"name\":\"Aitor Aller\"}],\"doi\":\"10.1007/978-3-030-62579-5_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"60c389da22f6a0ec3467e029842c7c5baf0893d4\",\"title\":\"Towards Fine-Tuning of VQA Models in Public Datasets\",\"url\":\"https://www.semanticscholar.org/paper/60c389da22f6a0ec3467e029842c7c5baf0893d4\",\"venue\":\"WAF\",\"year\":2020},{\"arxivId\":\"2012.07192\",\"authors\":[{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"51473867\",\"name\":\"Bailin Li\"},{\"authorId\":\"13246332\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"3170394\",\"name\":\"Keze Wang\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e452c1aa68173d6a1d3aefd9f08f70e43b0c59f4\",\"title\":\"Knowledge-Routed Visual Question Reasoning: Challenges for Deep Representation Embedding\",\"url\":\"https://www.semanticscholar.org/paper/e452c1aa68173d6a1d3aefd9f08f70e43b0c59f4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.14545\",\"authors\":[{\"authorId\":\"38907975\",\"name\":\"Ning Xie\"},{\"authorId\":\"46933964\",\"name\":\"Gabrielle Ras\"},{\"authorId\":\"143799386\",\"name\":\"M. V. Gerven\"},{\"authorId\":\"2514295\",\"name\":\"Derek Doran\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73055bab69d4f0d3b337bf83bae6ad5cc7604d7b\",\"title\":\"Explainable Deep Learning: A Field Guide for the Uninitiated\",\"url\":\"https://www.semanticscholar.org/paper/73055bab69d4f0d3b337bf83bae6ad5cc7604d7b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.09073\",\"authors\":[{\"authorId\":\"49659134\",\"name\":\"Zihao Zhu\"},{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"46394401\",\"name\":\"Yujing Wang\"},{\"authorId\":\"15087270\",\"name\":\"Yajing Sun\"},{\"authorId\":\"1581498565\",\"name\":\"Yue Hu\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.24963/ijcai.2020/153\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6b13065b4050800e30bb74e010b8aaba3355525d\",\"title\":\"Mucko: Multi-Layer Cross-Modal Knowledge Reasoning for Fact-based Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/6b13065b4050800e30bb74e010b8aaba3355525d\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2011.11479\",\"authors\":[{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"22314218\",\"name\":\"Silvio Giancola\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5388388db25f0d40ef6612333a0279373f8dddcf\",\"title\":\"TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks\",\"url\":\"https://www.semanticscholar.org/paper/5388388db25f0d40ef6612333a0279373f8dddcf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.10080\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"2365530\",\"name\":\"Mingda Zhang\"},{\"authorId\":\"144838223\",\"name\":\"Wei Li\"},{\"authorId\":\"36690046\",\"name\":\"Danfeng Qin\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"},{\"authorId\":\"6367313\",\"name\":\"J. Berent\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19f7724532c03b4a88ba5f0993f56b8832ee8d02\",\"title\":\"Learning to discover and localize visual objects with open vocabulary\",\"url\":\"https://www.semanticscholar.org/paper/19f7724532c03b4a88ba5f0993f56b8832ee8d02\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2003.11743\",\"authors\":[{\"authorId\":\"1491233100\",\"name\":\"Pranav Agarwal\"},{\"authorId\":\"145086911\",\"name\":\"A. Betancourt\"},{\"authorId\":\"1594025086\",\"name\":\"V. Panagiotou\"},{\"authorId\":\"2251072\",\"name\":\"N. Rodr\\u00edguez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b430a5384c82beb6102106fbea0a134425a08c23\",\"title\":\"Egoshots, an ego-vision life-logging dataset and semantic fidelity metric to evaluate diversity in image captioning models\",\"url\":\"https://www.semanticscholar.org/paper/b430a5384c82beb6102106fbea0a134425a08c23\",\"venue\":\"ICLR 2020\",\"year\":2020},{\"arxivId\":\"2011.12870\",\"authors\":[{\"authorId\":\"1960607091\",\"name\":\"Yi Zhou\"},{\"authorId\":\"8157338\",\"name\":\"Zhenhao Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"14e5d984af4ac965579644bd0ea4ae42587b6a00\",\"title\":\"Multimodal Learning for Hateful Memes Detection\",\"url\":\"https://www.semanticscholar.org/paper/14e5d984af4ac965579644bd0ea4ae42587b6a00\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.04264\",\"authors\":[{\"authorId\":\"32763968\",\"name\":\"A. Fisch\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"144797264\",\"name\":\"J. Clark\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8ca71f61139c69131ab200368a30a3dc72fa6785\",\"title\":\"CapWAP: Captioning with a Purpose\",\"url\":\"https://www.semanticscholar.org/paper/8ca71f61139c69131ab200368a30a3dc72fa6785\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2005.07486\",\"authors\":[{\"authorId\":\"51229603\",\"name\":\"Prajjwal Bhargava\"}],\"doi\":\"10.18653/v1/2020.acl-srw.1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"43ea1db66536f930bde2aa18d1224e52ec95418d\",\"title\":\"Adaptive Transformers for Learning Multimodal Representations\",\"url\":\"https://www.semanticscholar.org/paper/43ea1db66536f930bde2aa18d1224e52ec95418d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2548384\",\"name\":\"Hannaneh Hajishirzi\"},{\"authorId\":\"49617533\",\"name\":\"P. Allen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"189b518d70ad34c8de6f613bf3bd5051077608bc\",\"title\":\"Probing Language Models for Common Ground with Visual Representations\",\"url\":\"https://www.semanticscholar.org/paper/189b518d70ad34c8de6f613bf3bd5051077608bc\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1d660fdc8e7b23b5fa877a735744d1323a196cdb\",\"title\":\"A Simple Baseline for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/1d660fdc8e7b23b5fa877a735744d1323a196cdb\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102735962\",\"name\":\"Hongyu Liu\"},{\"authorId\":\"49892954\",\"name\":\"Qingyun Dai\"},{\"authorId\":\"98177768\",\"name\":\"Ya Li\"},{\"authorId\":\"90477013\",\"name\":\"Chuxin Zhang\"},{\"authorId\":\"104266220\",\"name\":\"Siyu Yi\"},{\"authorId\":null,\"name\":\"Tao Yuan\"}],\"doi\":\"10.1007/978-3-030-39431-8_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e5d6e4575aeeabd8141831bc5cecc4dca4fc620\",\"title\":\"The Design Patent Images Classification Based on Image Caption Model\",\"url\":\"https://www.semanticscholar.org/paper/4e5d6e4575aeeabd8141831bc5cecc4dca4fc620\",\"venue\":\"BICS\",\"year\":2019},{\"arxivId\":\"1908.06066\",\"authors\":[{\"authorId\":\"150112700\",\"name\":\"Gen Li\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143795948\",\"name\":\"Yuejian Fang\"},{\"authorId\":\"71790825\",\"name\":\"Daxin Jiang\"},{\"authorId\":\"143849622\",\"name\":\"M. Zhou\"}],\"doi\":\"10.1609/AAAI.V34I07.6795\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"title\":\"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2004.13073\",\"authors\":[{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e8d117d098ac59d90bf7814d889e814b52637f22\",\"title\":\"A Novel Attention-based Aggregation Function to Combine Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/e8d117d098ac59d90bf7814d889e814b52637f22\",\"venue\":\"ICPR 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471463062\",\"name\":\"Qi Zhang\"},{\"authorId\":\"1522102572\",\"name\":\"Zhen Lei\"},{\"authorId\":\"1415720379\",\"name\":\"Zhaoxiang Zhang\"},{\"authorId\":\"34679741\",\"name\":\"S. Li\"}],\"doi\":\"10.1109/cvpr42600.2020.00359\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4307dce5b1dd4a6bdcd06d61eed5aa7c85fb59f0\",\"title\":\"Context-Aware Attention Network for Image-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/4307dce5b1dd4a6bdcd06d61eed5aa7c85fb59f0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.06745\",\"authors\":[{\"authorId\":null,\"name\":\"Yi Zhu\"},{\"authorId\":\"94228656\",\"name\":\"Fengda Zhu\"},{\"authorId\":\"83374215\",\"name\":\"Zhaohuan Zhan\"},{\"authorId\":\"16208574\",\"name\":\"Bingqian Lin\"},{\"authorId\":\"73416694\",\"name\":\"Jianbin Jiao\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"13246332\",\"name\":\"Xiaodan Liang\"}],\"doi\":\"10.1109/cvpr42600.2020.01074\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"07de32d212a2d712b43557b4977b2ac76556e1d6\",\"title\":\"Vision-Dialog Navigation by Exploring Cross-Modal Memory\",\"url\":\"https://www.semanticscholar.org/paper/07de32d212a2d712b43557b4977b2ac76556e1d6\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1596808016\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"50118837\",\"name\":\"Y. Wu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/tpami.2020.3015894\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7401d3895e1cb78c34fa25db12409fdff56b1661\",\"title\":\"Symbiotic Attention for Egocentric Action Recognition with Object-centric Alignment.\",\"url\":\"https://www.semanticscholar.org/paper/7401d3895e1cb78c34fa25db12409fdff56b1661\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48195668\",\"name\":\"Jin Yuan\"},{\"authorId\":\"47059067\",\"name\":\"L. Zhang\"},{\"authorId\":\"2836997\",\"name\":\"Songrui Guo\"},{\"authorId\":\"1406190317\",\"name\":\"Y. Xiao\"},{\"authorId\":\"152985786\",\"name\":\"Z. Li\"}],\"doi\":\"10.1145/3394955\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"title\":\"Image Captioning with a Joint Attention Mechanism by Visual Concept Samples\",\"url\":\"https://www.semanticscholar.org/paper/2f977b1d34ad883483727a3c8c52e20b260ac0ad\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"1902.09487\",\"authors\":[{\"authorId\":\"7535126\",\"name\":\"R\\u00e9mi Cad\\u00e8ne\"},{\"authorId\":\"1405301761\",\"name\":\"Hedi Ben-younes\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"},{\"authorId\":\"1728523\",\"name\":\"N. Thome\"}],\"doi\":\"10.1109/CVPR.2019.00209\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0a9f1a1321958df7dfb2efce3e9d1e99b9f5ccb3\",\"title\":\"MUREL: Multimodal Relational Reasoning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/0a9f1a1321958df7dfb2efce3e9d1e99b9f5ccb3\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1804.05113\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1609/AAAI.V33I01.33019062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"title\":\"Multilevel Language and Vision Integration for Text-to-Clip Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"108605348\",\"name\":\"G. Chauhan\"},{\"authorId\":\"1491627924\",\"name\":\"D. Mishra\"}],\"doi\":\"10.1109/CSITSS47250.2019.9031059\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6709a6f80a0b1cf1a0aae19431ba8e02763eafb6\",\"title\":\"Evaluating deep learning based models for predicting click through rate\",\"url\":\"https://www.semanticscholar.org/paper/6709a6f80a0b1cf1a0aae19431ba8e02763eafb6\",\"venue\":\"2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)\",\"year\":2019},{\"arxivId\":\"1903.10658\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"7574699\",\"name\":\"Handong Zhao\"},{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/ICCV.2019.01042\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc\",\"title\":\"Unpaired Image Captioning via Scene Graph Alignments\",\"url\":\"https://www.semanticscholar.org/paper/f4c60c3ee4904d61ff84c9d4c15c9aecdcf04cdc\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.01356\",\"authors\":[{\"authorId\":\"89373414\",\"name\":\"Omer Arshad\"},{\"authorId\":\"145116184\",\"name\":\"I. Gallo\"},{\"authorId\":\"144669071\",\"name\":\"Shah Nawaz\"},{\"authorId\":\"3457883\",\"name\":\"Alessandro Calefati\"}],\"doi\":\"10.1109/ICDAR.2019.00061\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"add336dbad015f2f5ae0b3bd4525a9d7496adee5\",\"title\":\"Aiding Intra-Text Representations with Visual Context for Multimodal Named Entity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/add336dbad015f2f5ae0b3bd4525a9d7496adee5\",\"venue\":\"2019 International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2959229\",\"name\":\"Fran\\u00e7ois Plesse\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"108acfe359acf6875e3ab10fad0be45f6c777ecb\",\"title\":\"Int\\u00e9gration de Connaissances aux Mod\\u00e8les Neuronaux pour la D\\u00e9tection de Relations Visuelles Rares. (Knowledge Integration into Neural Networks for the purposes of Rare Visual Relation Detection)\",\"url\":\"https://www.semanticscholar.org/paper/108acfe359acf6875e3ab10fad0be45f6c777ecb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.09580\",\"authors\":[{\"authorId\":\"41036094\",\"name\":\"C. Deng\"},{\"authorId\":\"46649124\",\"name\":\"Ning Ding\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1007/978-3-030-58601-0_42\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2320f853059c29ce7e70409fa559074d727da5a2\",\"title\":\"Length-Controllable Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2320f853059c29ce7e70409fa559074d727da5a2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.06198\",\"authors\":[{\"authorId\":\"9748140\",\"name\":\"K. Gouthaman\"},{\"authorId\":\"50853059\",\"name\":\"Anurag Mittal\"}],\"doi\":\"10.1007/978-3-030-58601-0_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"06b869a92a22db711e4fbe8b141c83523c7c4604\",\"title\":\"Reducing Language Biases in Visual Question Answering with Visually-Grounded Question Encoder\",\"url\":\"https://www.semanticscholar.org/paper/06b869a92a22db711e4fbe8b141c83523c7c4604\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51933789\",\"name\":\"N. Ilinykh\"},{\"authorId\":\"2995275\",\"name\":\"Simon Dobnik\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4a8a490f8d3a7414444289b8802052eae3ebbe0c\",\"title\":\"When an Image Tells a Story: The Role of Visual and Semantic Information for Generating Paragraph Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/4a8a490f8d3a7414444289b8802052eae3ebbe0c\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51228129\",\"name\":\"Shailza Jolly\"},{\"authorId\":\"7413674\",\"name\":\"S. Kapoor\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.257\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"683b1230f53eddb0c03f4ad656579269eae2227d\",\"title\":\"Can Pre-training help VQA with Lexical Variations?\",\"url\":\"https://www.semanticscholar.org/paper/683b1230f53eddb0c03f4ad656579269eae2227d\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2007669688\",\"name\":\"Zujie Liang\"},{\"authorId\":\"49408562\",\"name\":\"Weitao Jiang\"},{\"authorId\":\"145442620\",\"name\":\"Haifeng Hu\"},{\"authorId\":\"47054782\",\"name\":\"Jiaying Zhu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.265\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f5dffcc47a08ef74e93077583b0e8a11662bf02\",\"title\":\"Learning to Contrast the Counterfactual Samples for Robust Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/5f5dffcc47a08ef74e93077583b0e8a11662bf02\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1455114388\",\"name\":\"Hrituraj Singh\"},{\"authorId\":\"35223379\",\"name\":\"Sumit Shekhar\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.264\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"baff151a5f1cb378d3627b88ff7d1b3afd29db45\",\"title\":\"STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/baff151a5f1cb378d3627b88ff7d1b3afd29db45\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2004.10151\",\"authors\":[{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"14487640\",\"name\":\"Ari Holtzman\"},{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"50730674\",\"name\":\"Joyce Chai\"},{\"authorId\":\"1412841514\",\"name\":\"M. Lapata\"},{\"authorId\":\"2672644\",\"name\":\"A. Lazaridou\"},{\"authorId\":\"143823227\",\"name\":\"Jonathan May\"},{\"authorId\":\"17109242\",\"name\":\"Aleksandr Nisnevich\"},{\"authorId\":\"30017846\",\"name\":\"Nicolas Pinto\"},{\"authorId\":\"153160559\",\"name\":\"Joseph P. Turian\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.703\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d\",\"title\":\"Experience Grounds Language\",\"url\":\"https://www.semanticscholar.org/paper/bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1907.10255\",\"authors\":[{\"authorId\":\"8809422\",\"name\":\"Vishwanath Sindagi\"},{\"authorId\":\"1741177\",\"name\":\"V. Patel\"}],\"doi\":\"10.1109/TIP.2019.2928634\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8f8a1d07216259a87fe8968feb9379c14629ad9f\",\"title\":\"HA-CCN: Hierarchical Attention-Based Crowd Counting Network\",\"url\":\"https://www.semanticscholar.org/paper/8f8a1d07216259a87fe8968feb9379c14629ad9f\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2007.03338\",\"authors\":[{\"authorId\":\"40651988\",\"name\":\"Marzieh Heidari\"},{\"authorId\":\"2862462\",\"name\":\"Mehdi Ghatee\"},{\"authorId\":\"1780566\",\"name\":\"A. Nickabadi\"},{\"authorId\":\"1796299590\",\"name\":\"Arash Pourhasan Nezhad\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20b332af8399233b5fc33a7e7f98538bd4a82ab0\",\"title\":\"Diverse and Styled Image Captioning Using SVD-Based Mixture of Recurrent Experts\",\"url\":\"https://www.semanticscholar.org/paper/20b332af8399233b5fc33a7e7f98538bd4a82ab0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.04649\",\"authors\":[{\"authorId\":\"1409874899\",\"name\":\"Yang Fan\"},{\"authorId\":\"2794096\",\"name\":\"Yingce Xia\"},{\"authorId\":\"47767550\",\"name\":\"Lijun Wu\"},{\"authorId\":\"1889683\",\"name\":\"Shufang Xie\"},{\"authorId\":\"1390517481\",\"name\":\"Weiqing Liu\"},{\"authorId\":\"152441498\",\"name\":\"Jiang Bian\"},{\"authorId\":\"82620854\",\"name\":\"Tao Qin\"},{\"authorId\":\"46218395\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"430707574a79e4ebfe0e2d120189a54cd7775299\",\"title\":\"Learning to Teach with Deep Interactions\",\"url\":\"https://www.semanticscholar.org/paper/430707574a79e4ebfe0e2d120189a54cd7775299\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8810290\",\"name\":\"J. Liang\"},{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"1855236018\",\"name\":\"Feng Lu\"}],\"doi\":\"10.1007/978-3-030-58548-8_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5b92d901f7242d40071f060841f3509d04e5b2eb\",\"title\":\"CPGAN: Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/5b92d901f7242d40071f060841f3509d04e5b2eb\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.10309\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"1382193868\",\"name\":\"Mayank Lunayach\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"01fffd4fcd33d83602f7bc5a600ce87317e72763\",\"title\":\"Uncertainty based Class Activation Maps for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/01fffd4fcd33d83602f7bc5a600ce87317e72763\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.00500\",\"authors\":[{\"authorId\":\"41022273\",\"name\":\"Duy-Kien Nguyen\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1109/CVPR.2019.01074\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac359aac85ba5d05c8249bd7dfb5d71aa205db79\",\"title\":\"Multi-Task Learning of Hierarchical Vision-Language Representation\",\"url\":\"https://www.semanticscholar.org/paper/ac359aac85ba5d05c8249bd7dfb5d71aa205db79\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67310661\",\"name\":\"Monica Haurilet\"},{\"authorId\":\"1390024605\",\"name\":\"Ziad Al-Halah\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1007/978-3-030-33676-9_30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d631106df52df0f47b5669e5b6547267a9756164\",\"title\":\"DynGraph: Visual Question Answering via Dynamic Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/d631106df52df0f47b5669e5b6547267a9756164\",\"venue\":\"GCPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143905586\",\"name\":\"Cuirong Long\"},{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1007/s11042-019-7441-7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f147a64982b25ea28a2b0737c68b0f65fdb46bd8\",\"title\":\"Cross-domain personalized image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f147a64982b25ea28a2b0737c68b0f65fdb46bd8\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"2011.11603\",\"authors\":[{\"authorId\":\"50219425\",\"name\":\"Zhonghao Wang\"},{\"authorId\":\"50753116\",\"name\":\"M. Yu\"},{\"authorId\":\"1557365681\",\"name\":\"Kai Wang\"},{\"authorId\":\"46590281\",\"name\":\"Jinjun Xiong\"},{\"authorId\":\"143668320\",\"name\":\"W. Hwu\"},{\"authorId\":\"1404424091\",\"name\":\"M. Hasegawa-Johnson\"},{\"authorId\":\"1733082596\",\"name\":\"Humphrey Shi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ceb7dad11d16c21c2743e8b418bee5260d90102d\",\"title\":\"Interpretable Visual Reasoning via Induced Symbolic Space\",\"url\":\"https://www.semanticscholar.org/paper/ceb7dad11d16c21c2743e8b418bee5260d90102d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40461583\",\"name\":\"Kun Zhang\"},{\"authorId\":\"2767360\",\"name\":\"Guangyi Lv\"},{\"authorId\":\"47767796\",\"name\":\"L. Wu\"},{\"authorId\":\"144378760\",\"name\":\"E. Chen\"},{\"authorId\":\"50383774\",\"name\":\"Q. Liu\"},{\"authorId\":\"46477167\",\"name\":\"H. Wu\"},{\"authorId\":\"153710346\",\"name\":\"Xing Xie\"},{\"authorId\":\"2397264\",\"name\":\"Fangzhao Wu\"}],\"doi\":\"10.1109/tsmc.2019.2932410\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da0c294c84ef6d48822eeb383d36015a5bbc96eb\",\"title\":\"Multilevel Image-Enhanced Sentence Representation Net for Natural Language Inference\",\"url\":\"https://www.semanticscholar.org/paper/da0c294c84ef6d48822eeb383d36015a5bbc96eb\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1904.08920\",\"authors\":[{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"2311222\",\"name\":\"V. Natarajan\"},{\"authorId\":\"144826412\",\"name\":\"Meet Shah\"},{\"authorId\":\"143804072\",\"name\":\"Y. Jiang\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00851\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"af1f7739283bdbd2b7a94903041f6d6afd991907\",\"title\":\"Towards VQA Models That Can Read\",\"url\":\"https://www.semanticscholar.org/paper/af1f7739283bdbd2b7a94903041f6d6afd991907\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"3374607\",\"name\":\"Vaibhav\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3323873.3325043\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ced9f3f05a780110a0a26549a45c4916cb16d3b1\",\"title\":\"Improving What Cross-Modal Retrieval Models Learn through Object-Oriented Inter- and Intra-Modal Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/ced9f3f05a780110a0a26549a45c4916cb16d3b1\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":\"1906.04464\",\"authors\":[{\"authorId\":\"3144952\",\"name\":\"Sibei Yang\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"66616425\",\"name\":\"Yizhou Yu\"}],\"doi\":\"10.1109/tpami.2020.2973983\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14204fd579ad2802490a8f4a781392ca7ba19c80\",\"title\":\"Relationship-Embedded Representation Learning for Grounding Referring Expressions.\",\"url\":\"https://www.semanticscholar.org/paper/14204fd579ad2802490a8f4a781392ca7ba19c80\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1811.11683\",\"authors\":[{\"authorId\":\"145559125\",\"name\":\"H. Akbari\"},{\"authorId\":\"35862299\",\"name\":\"S. Karaman\"},{\"authorId\":\"1754397\",\"name\":\"Surabhi Bhargava\"},{\"authorId\":\"143852868\",\"name\":\"B. Chen\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2019.01276\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4702bad26563a4e1d03b33c0af0b29b095438bc\",\"title\":\"Multi-Level Multimodal Common Semantic Space for Image-Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/b4702bad26563a4e1d03b33c0af0b29b095438bc\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1903.10831\",\"authors\":[{\"authorId\":\"3059957\",\"name\":\"L. Li\"},{\"authorId\":\"1743773\",\"name\":\"M. Xu\"},{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"47420847\",\"name\":\"Lai Jiang\"},{\"authorId\":\"3995100\",\"name\":\"H. Liu\"}],\"doi\":\"10.1109/CVPR.2019.01082\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97fb5a7a5ce79da0cdc5f2d28063373303ef4f8f\",\"title\":\"Attention Based Glaucoma Detection: A Large-Scale Database and CNN Model\",\"url\":\"https://www.semanticscholar.org/paper/97fb5a7a5ce79da0cdc5f2d28063373303ef4f8f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1711.06330\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":\"10.1109/CVPR.2018.00710\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66aebb3af16aaa78579344784212ae10f60ec27e\",\"title\":\"Attend and Interact: Higher-Order Object Interactions for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/66aebb3af16aaa78579344784212ae10f60ec27e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2007.08883\",\"authors\":[{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"72095125\",\"name\":\"Y. Zhang\"},{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"}],\"doi\":\"10.1007/978-3-030-58586-0_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe573437cbd4069556348ad28dfeae2df46e22a0\",\"title\":\"Consensus-Aware Visual-Semantic Embedding for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/fe573437cbd4069556348ad28dfeae2df46e22a0\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1879303113\",\"name\":\"Depeng Wang\"},{\"authorId\":\"7690231\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"1877859275\",\"name\":\"Yuanen Zhou\"},{\"authorId\":\"1390610633\",\"name\":\"Xueliang Liu\"},{\"authorId\":\"152318056\",\"name\":\"L. Wu\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"}],\"doi\":\"10.1109/icmew46912.2020.9106007\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"61cc97db488acc841cc31ffe046957829c366b53\",\"title\":\"A Text-Guided Graph Structure for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/61cc97db488acc841cc31ffe046957829c366b53\",\"venue\":\"2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)\",\"year\":2020},{\"arxivId\":\"2006.09920\",\"authors\":[{\"authorId\":\"1911972\",\"name\":\"Tanmay Gupta\"},{\"authorId\":\"3214848\",\"name\":\"Arash Vahdat\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"},{\"authorId\":\"47008546\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2433269\",\"name\":\"Derek Hoiem\"}],\"doi\":\"10.1007/978-3-030-58580-8_44\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14e6c84fd88badeaa969c27d4cdada764877afff\",\"title\":\"Contrastive Learning for Weakly Supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/14e6c84fd88badeaa969c27d4cdada764877afff\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.09928\",\"authors\":[{\"authorId\":\"79439415\",\"name\":\"Haoyu Dong\"},{\"authorId\":\"152764048\",\"name\":\"Zidong Wang\"},{\"authorId\":\"83277545\",\"name\":\"Qiang Qiu\"},{\"authorId\":\"1699339\",\"name\":\"G. Sapiro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88b5d42753d2f3a2194c886169038fd9ae7ade22\",\"title\":\"Using Text to Teach Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/88b5d42753d2f3a2194c886169038fd9ae7ade22\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1807.01448\",\"authors\":[{\"authorId\":\"40480894\",\"name\":\"Karuna Ahuja\"},{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"145149308\",\"name\":\"A. Roy\"},{\"authorId\":\"1696401\",\"name\":\"Ajay Divakaran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce407c2899ac955fa67dc7f6ba3a7f07ecd18855\",\"title\":\"Understanding Visual Ads by Aligning Symbols and Objects using Co-Attention\",\"url\":\"https://www.semanticscholar.org/paper/ce407c2899ac955fa67dc7f6ba3a7f07ecd18855\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143696833\",\"name\":\"Wenjie Cai\"},{\"authorId\":\"31126576\",\"name\":\"Zheng Xiong\"},{\"authorId\":\"2303260\",\"name\":\"Xianfang Sun\"},{\"authorId\":\"1734823\",\"name\":\"Paul L. Rosin\"},{\"authorId\":\"40534966\",\"name\":\"Longcun Jin\"},{\"authorId\":\"9245443\",\"name\":\"Xinyi Peng\"}],\"doi\":\"10.3390/app10010391\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"945ffeb90e538214e3063407db8e094469ec877a\",\"title\":\"Panoptic Segmentation-Based Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/945ffeb90e538214e3063407db8e094469ec877a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.14264\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b58adf956dfd71a756b8f98b726fdcd8e2552445\",\"title\":\"Self-Segregating and Coordinated-Segregating Transformer for Focused Deep Multi-Modular Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b58adf956dfd71a756b8f98b726fdcd8e2552445\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"49470127\",\"name\":\"X. Zhang\"},{\"authorId\":\"1406222945\",\"name\":\"Zhigang Luo\"}],\"doi\":\"10.1007/s11042-019-08567-0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"49280cc9fa8c9854a655afbfd213c1437c52f1e3\",\"title\":\"GateCap: Gated spatial and semantic attention model for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/49280cc9fa8c9854a655afbfd213c1437c52f1e3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153379960\",\"name\":\"Weilong Chen\"},{\"authorId\":\"47579283\",\"name\":\"Feng Hong\"},{\"authorId\":\"2460537\",\"name\":\"C. Huang\"},{\"authorId\":\"101266539\",\"name\":\"Shaoliang Zhang\"},{\"authorId\":\"145490524\",\"name\":\"R. Wang\"},{\"authorId\":\"3360722\",\"name\":\"Ruobing Xie\"},{\"authorId\":\"144956430\",\"name\":\"Fan Xia\"},{\"authorId\":\"46456215\",\"name\":\"L. Lin\"},{\"authorId\":\"2101148\",\"name\":\"Y. Zhang\"},{\"authorId\":null,\"name\":\"Yan Wang\"}],\"doi\":\"10.1145/3394171.3416275\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"022855e6cecd7ec64ca9faa7a3c66e4b21eaee8c\",\"title\":\"Curriculum Learning for Wide Multimedia-Based Transformer with Graph Target Detection\",\"url\":\"https://www.semanticscholar.org/paper/022855e6cecd7ec64ca9faa7a3c66e4b21eaee8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2005.08646\",\"authors\":[{\"authorId\":\"1947101\",\"name\":\"Shijie Geng\"},{\"authorId\":\"24263694\",\"name\":\"J. Zhang\"},{\"authorId\":\"2011378\",\"name\":\"Z. Fu\"},{\"authorId\":\"144740494\",\"name\":\"Peng Gao\"},{\"authorId\":\"49724425\",\"name\":\"H. Zhang\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"15fe6bb38c22a1d765bf57ffd7e6654edcfa4768\",\"title\":\"Character Matters: Video Story Understanding with Character-Aware Relations\",\"url\":\"https://www.semanticscholar.org/paper/15fe6bb38c22a1d765bf57ffd7e6654edcfa4768\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1510730409\",\"name\":\"Jia-Qi Yang\"},{\"authorId\":\"1721819\",\"name\":\"D. Zhan\"},{\"authorId\":\"50078941\",\"name\":\"Xin-Chun Li\"}],\"doi\":\"10.1007/978-3-030-47436-2_43\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb4b2b82b623e2dd52de1381903213e6a7c93073\",\"title\":\"Bottom-Up and Top-Down Graph Pooling\",\"url\":\"https://www.semanticscholar.org/paper/cb4b2b82b623e2dd52de1381903213e6a7c93073\",\"venue\":\"PAKDD\",\"year\":2020},{\"arxivId\":\"1911.10082\",\"authors\":[{\"authorId\":\"52150251\",\"name\":\"A. Goel\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"103192742\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"e4dcd3fa308b263cee8c7581e2d695dc15c2f51f\",\"title\":\"Injecting Prior Knowledge into Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/e4dcd3fa308b263cee8c7581e2d695dc15c2f51f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398104959\",\"name\":\"Justin R. Lovelace\"},{\"authorId\":\"144156074\",\"name\":\"Bobak Mortazavi\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.110\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a554243e6806ac419ffdb19c354686f77ef2b848\",\"title\":\"Learning to Generate Clinically Coherent Chest X-Ray Reports\",\"url\":\"https://www.semanticscholar.org/paper/a554243e6806ac419ffdb19c354686f77ef2b848\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1904.11141\",\"authors\":[{\"authorId\":\"47002847\",\"name\":\"Ya-Li Li\"},{\"authorId\":\"1678689\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/TIP.2019.2957850\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1e63c6436361bee3f37715c2bffb7affe9504a0d\",\"title\":\"HAR-Net: Joint Learning of Hybrid Attention for Single-Stage Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/1e63c6436361bee3f37715c2bffb7affe9504a0d\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58565-5_36\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"853069fa3f976fe368858ce4650b6348a17a3764\",\"title\":\"Hierarchical Visual-Textual Graph for Temporal Activity Localization via Language\",\"url\":\"https://www.semanticscholar.org/paper/853069fa3f976fe368858ce4650b6348a17a3764\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390626344\",\"name\":\"Zhiyuan Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"title\":\"Cross-Modal Representation\",\"url\":\"https://www.semanticscholar.org/paper/7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1907.02985\",\"authors\":[{\"authorId\":\"67344892\",\"name\":\"Federico Landi\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"40186452\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"64d1b545f586d930cbe67402e853ab26a8f6e18d\",\"title\":\"Embodied Vision-and-Language Navigation with Dynamic Convolutional Filters\",\"url\":\"https://www.semanticscholar.org/paper/64d1b545f586d930cbe67402e853ab26a8f6e18d\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1907.00490\",\"authors\":[{\"authorId\":\"35570245\",\"name\":\"Ali Furkan Biten\"},{\"authorId\":\"134682605\",\"name\":\"Ruben Tito\"},{\"authorId\":\"51238351\",\"name\":\"Andr\\u00e9s Mafla\"},{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"143823474\",\"name\":\"M. Rusi\\u00f1ol\"},{\"authorId\":\"34317896\",\"name\":\"M. Mathew\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"},{\"authorId\":\"2864362\",\"name\":\"Ernest Valveny\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":\"10.1109/ICDAR.2019.00251\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b3b0d8d5a24630c940049442a8d824534faf8b9\",\"title\":\"ICDAR 2019 Competition on Scene Text Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/4b3b0d8d5a24630c940049442a8d824534faf8b9\",\"venue\":\"2019 International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"10405058\",\"name\":\"Chenfei Wu\"},{\"authorId\":\"39527132\",\"name\":\"Xiaojie Wang\"},{\"authorId\":\"143672034\",\"name\":\"X. Dong\"}],\"doi\":\"10.1109/CCIS.2018.8691361\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e074ccab7b7c46b48d643c1026e71e563878885f\",\"title\":\"Sequential Visual Reasoning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e074ccab7b7c46b48d643c1026e71e563878885f\",\"venue\":\"2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems (CCIS)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33698309\",\"name\":\"Zhong-Qiu Zhao\"},{\"authorId\":\"49697077\",\"name\":\"Yue-Lin Sun\"},{\"authorId\":\"1977865899\",\"name\":\"Nan-Xun Wang\"},{\"authorId\":\"144963976\",\"name\":\"Weidong Tian\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206632\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03a53b48a2c869658f969856acd2830711dc9ba9\",\"title\":\"Extricating from GroundTruth: An Unpaired Learning Based Evaluation Metric for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/03a53b48a2c869658f969856acd2830711dc9ba9\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9610143\",\"name\":\"Zhihao Fan\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"50695111\",\"name\":\"Siyuan Wang\"},{\"authorId\":\"1790227\",\"name\":\"X. Huang\"}],\"doi\":\"10.18653/v1/P19-1652\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"aaf5e3afd61d0df6c483ca32faf8e7a9198b1557\",\"title\":\"Bridging by Word: Image Grounded Vocabulary Construction for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aaf5e3afd61d0df6c483ca32faf8e7a9198b1557\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"144663763\",\"name\":\"Q. Wu\"},{\"authorId\":\"41173169\",\"name\":\"W. Wang\"},{\"authorId\":\"49681016\",\"name\":\"L. Wang\"}],\"doi\":\"10.1109/TPAMI.2018.2883466\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c58fe4769e12bafa4e7dda5df9f1cc6111040c6d\",\"title\":\"Image and Sentence Matching via Semantic Concepts and Order Learning\",\"url\":\"https://www.semanticscholar.org/paper/c58fe4769e12bafa4e7dda5df9f1cc6111040c6d\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47267313\",\"name\":\"T. Le\"},{\"authorId\":\"144672395\",\"name\":\"Vuong Le\"},{\"authorId\":\"143761093\",\"name\":\"S. Venkatesh\"},{\"authorId\":\"6254479\",\"name\":\"T. Tran\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5d89c0c6a1d5bc5cf06b0927976eaca350653301\",\"title\":\"Learning to Reason with Relational Video Representation for Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/5d89c0c6a1d5bc5cf06b0927976eaca350653301\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.03743\",\"authors\":[{\"authorId\":\"13941532\",\"name\":\"Fuxiao Liu\"},{\"authorId\":null,\"name\":\"Yinghan Wang\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51f2eb8b16d132d3c403ed76d3ac816a8f58f25c\",\"title\":\"VisualNews : Benchmark and Challenges in Entity-aware Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/51f2eb8b16d132d3c403ed76d3ac816a8f58f25c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1628481326\",\"name\":\"Xuran Zhao\"},{\"authorId\":\"151484897\",\"name\":\"Luyang Yu\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054612\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f2ff54eaa0a3debb1436c369312f689d562852a1\",\"title\":\"Cross-View Attention Network for Breast Cancer Screening from Multi-View Mammograms\",\"url\":\"https://www.semanticscholar.org/paper/f2ff54eaa0a3debb1436c369312f689d562852a1\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443435125\",\"name\":\"Zhengcong Fei\"}],\"doi\":\"10.1145/3372278.3390679\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f1fd4006dd7edbdf6c28fb6db21c59dda8e16d08\",\"title\":\"Actor-Critic Sequence Generation for Relative Difference Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1fd4006dd7edbdf6c28fb6db21c59dda8e16d08\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"BiGRU BiGRU\"},{\"authorId\":null,\"name\":\"BiGRU\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd8a531ddff2ba788d02555af04632c46f31ecf0\",\"title\":\"Multi-Stage Cross-modal Interaction Module d ) Moment Retrieval Module q \\\" q # q $ q ) Query\",\"url\":\"https://www.semanticscholar.org/paper/dd8a531ddff2ba788d02555af04632c46f31ecf0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1910.05752\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":\"1387518609\",\"name\":\"Jiutong Wei\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"122094546\",\"name\":\"B. Li\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9dab1068a6e9f4696ea3687baf04f9098107c1f7\",\"title\":\"VATEX Captioning Challenge 2019: Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9dab1068a6e9f4696ea3687baf04f9098107c1f7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2002.06701\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"19b00a2bd1624a0931069a539a971e40b65bfb3a\",\"title\":\"Gaussian Smoothen Semantic Features (GSSF) - Exploring the Linguistic Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO Framework\",\"url\":\"https://www.semanticscholar.org/paper/19b00a2bd1624a0931069a539a971e40b65bfb3a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.14603\",\"authors\":[{\"authorId\":\"47267313\",\"name\":\"T. Le\"},{\"authorId\":\"144672395\",\"name\":\"Vuong Le\"},{\"authorId\":\"144162181\",\"name\":\"S. Venkatesh\"},{\"authorId\":\"6254479\",\"name\":\"T. Tran\"}],\"doi\":\"10.24963/ijcai.2020/114\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c6b2bd7d4c9535174788dca2e7e6b2fe26fe2b1e\",\"title\":\"Dynamic Language Binding in Relational Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/c6b2bd7d4c9535174788dca2e7e6b2fe26fe2b1e\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"1907.02065\",\"authors\":[{\"authorId\":\"150096351\",\"name\":\"Elaina Tan\"},{\"authorId\":\"30154736\",\"name\":\"Lakshay Sharma\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb2ee520ed9c8fc113ce55192217404492b71455\",\"title\":\"Neural Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bb2ee520ed9c8fc113ce55192217404492b71455\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1805.05062\",\"authors\":[{\"authorId\":\"46183659\",\"name\":\"Maha Elbayad\"},{\"authorId\":\"143823463\",\"name\":\"L. Besacier\"},{\"authorId\":\"34602236\",\"name\":\"Jakob Verbeek\"}],\"doi\":\"10.18653/v1/P18-1195\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"db3340742a56a4bbb9d7ee8dba1958fb132a3c0a\",\"title\":\"Token-level and sequence-level loss smoothing for RNN language models\",\"url\":\"https://www.semanticscholar.org/paper/db3340742a56a4bbb9d7ee8dba1958fb132a3c0a\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"2002.06436\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"4ad005019f7069a3aaff959fbaa657f2ef2143fc\",\"title\":\"MRRC: Multiple Role Representation Crossover Interpretation for Image Captioning With R-CNN Feature Distribution Composition (FDC)\",\"url\":\"https://www.semanticscholar.org/paper/4ad005019f7069a3aaff959fbaa657f2ef2143fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"1379526529\",\"name\":\"Qiankun Kong\"},{\"authorId\":null,\"name\":\"Haoran Wang\"},{\"authorId\":\"48278144\",\"name\":\"Y. Pang\"}],\"doi\":\"10.1145/3343031.3351064\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c54e587223099732a03c71bbe2b9b6e1b4ffc67\",\"title\":\"Small and Dense Commodity Object Detection with Multi-Scale Receptive Field Attention\",\"url\":\"https://www.semanticscholar.org/paper/6c54e587223099732a03c71bbe2b9b6e1b4ffc67\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500648244\",\"name\":\"Hongbiao Lu\"},{\"authorId\":\"1500658134\",\"name\":\"Xiaobao Liu\"},{\"authorId\":\"46524522\",\"name\":\"Yanchao Yin\"},{\"authorId\":\"46843124\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1109/ISCMI47871.2019.9004335\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1125c4543b07413c33f896434419e9cfc9a8297\",\"title\":\"A Patent Text Classification Model Based on Multivariate Neural Network Fusion\",\"url\":\"https://www.semanticscholar.org/paper/f1125c4543b07413c33f896434419e9cfc9a8297\",\"venue\":\"2019 6th International Conference on Soft Computing & Machine Intelligence (ISCMI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34656387\",\"name\":\"Matteo Tomei\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/978-3-030-30645-8_67\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"433bb6fe59b042c8a91a6df0cd0fbba101902638\",\"title\":\"Image-to-Image Translation to Unfold the Reality of Artworks: An Empirical Analysis\",\"url\":\"https://www.semanticscholar.org/paper/433bb6fe59b042c8a91a6df0cd0fbba101902638\",\"venue\":\"ICIAP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50974488\",\"name\":\"Mingrui Lao\"},{\"authorId\":\"49813983\",\"name\":\"Yanming Guo\"},{\"authorId\":\"49528566\",\"name\":\"Hui Wang\"},{\"authorId\":\"145863022\",\"name\":\"X. Zhang\"}],\"doi\":\"10.20944/PREPRINTS201804.0313.V1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2329335263b2911a05c36c8c3747d9ee96eb6fee\",\"title\":\"Cross-Modal Multistep Fusion Network With Co-Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/2329335263b2911a05c36c8c3747d9ee96eb6fee\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1910.03230\",\"authors\":[{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"153731335\",\"name\":\"Zhe Gan\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"152876475\",\"name\":\"William W. J. Wang\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d0bbae624efbfeee01bd38185c6d754c08417de7\",\"title\":\"Meta Module Network for Compositional Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/d0bbae624efbfeee01bd38185c6d754c08417de7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1905.04877\",\"authors\":[{\"authorId\":\"30921555\",\"name\":\"Yangyang Guo\"},{\"authorId\":\"13167100\",\"name\":\"Zhiyong Cheng\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"35435925\",\"name\":\"Y. Liu\"},{\"authorId\":\"49417788\",\"name\":\"Yinglong Wang\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1145/3331184.3331186\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d00fe8d117a3849d3085d4301d91c56b775ee8b1\",\"title\":\"Quantifying and Alleviating the Language Prior Problem in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/d00fe8d117a3849d3085d4301d91c56b775ee8b1\",\"venue\":\"SIGIR\",\"year\":2019},{\"arxivId\":\"2006.10598\",\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"2134146\",\"name\":\"Nikoli Dryden\"},{\"authorId\":\"1752594045\",\"name\":\"Julius Frost\"},{\"authorId\":\"1713648\",\"name\":\"Torsten Hoefler\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"235de6164e754400f82b77fb33be84fc43c79078\",\"title\":\"Shapeshifter Networks: Decoupling Layers from Parameters for Scalable and Effective Deep Learning.\",\"url\":\"https://www.semanticscholar.org/paper/235de6164e754400f82b77fb33be84fc43c79078\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.01410\",\"authors\":[{\"authorId\":\"4332039\",\"name\":\"Guojun Yin\"},{\"authorId\":\"37145669\",\"name\":\"Lu Sheng\"},{\"authorId\":null,\"name\":\"Bin Liu\"},{\"authorId\":\"1708598\",\"name\":\"N. Yu\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"}],\"doi\":\"10.1109/CVPR.2019.00640\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eba62fe8050e475ffe533b9f70db538074d8d0d1\",\"title\":\"Context and Attribute Grounded Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/eba62fe8050e475ffe533b9f70db538074d8d0d1\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2012.02339\",\"authors\":[{\"authorId\":\"2031911881\",\"name\":\"Edwin G. Ng\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0795ae19cae966a9f71d8da78ef09447ab9080a6\",\"title\":\"Understanding Guided Image Captioning Performance across Domains\",\"url\":\"https://www.semanticscholar.org/paper/0795ae19cae966a9f71d8da78ef09447ab9080a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1802.01433\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"46701859\",\"name\":\"H. Zhang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a4a90a2db209db2d5c49adfd2091ede2d4130f60\",\"title\":\"Interactive Grounded Language Acquisition and Generalization in a 2D World\",\"url\":\"https://www.semanticscholar.org/paper/a4a90a2db209db2d5c49adfd2091ede2d4130f60\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"2010.09304\",\"authors\":[{\"authorId\":\"1612421029\",\"name\":\"Yicong Hong\"},{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"35653798\",\"name\":\"Yuankai Qi\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"143685465\",\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f4e8c2b998e8714fe86ce5f1adafbab30402506f\",\"title\":\"Language and Visual Entity Relationship Graph for Agent Navigation\",\"url\":\"https://www.semanticscholar.org/paper/f4e8c2b998e8714fe86ce5f1adafbab30402506f\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1620218253\",\"name\":\"Sruthy Manmadhan\"},{\"authorId\":\"30588803\",\"name\":\"Binsu C. Kovoor\"}],\"doi\":\"10.1007/s10462-020-09832-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"667b88984df0c5c11ac07899ffb5509185abdf57\",\"title\":\"Visual question answering: a state-of-the-art review\",\"url\":\"https://www.semanticscholar.org/paper/667b88984df0c5c11ac07899ffb5509185abdf57\",\"venue\":\"Artificial Intelligence Review\",\"year\":2020},{\"arxivId\":\"2012.13235\",\"authors\":[{\"authorId\":\"34384001\",\"name\":\"Vlad Sandulescu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"29b30ccea4cfe517a1b9facd5c95b2a1071399cf\",\"title\":\"Detecting Hateful Memes Using a Multimodal Deep Ensemble\",\"url\":\"https://www.semanticscholar.org/paper/29b30ccea4cfe517a1b9facd5c95b2a1071399cf\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.00562\",\"authors\":[{\"authorId\":\"1403082631\",\"name\":\"J. G\\u00f3mez-P\\u00e9rez\"},{\"authorId\":\"144243879\",\"name\":\"R. Ortega\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.441\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"31988c840b07e00c588f7adf6e16c64e72811324\",\"title\":\"ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention\",\"url\":\"https://www.semanticscholar.org/paper/31988c840b07e00c588f7adf6e16c64e72811324\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.09413\",\"authors\":[{\"authorId\":\"1999185950\",\"name\":\"Duvsan Varivs\"},{\"authorId\":\"1790811\",\"name\":\"Katsuhito Sudoh\"},{\"authorId\":\"50068540\",\"name\":\"S. Nakamura\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"97b9a163be6fe1924a366129395177991e5eb83d\",\"title\":\"Image Captioning with Visual Object Representations Grounded in the Textual Modality\",\"url\":\"https://www.semanticscholar.org/paper/97b9a163be6fe1924a366129395177991e5eb83d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92716077\",\"name\":\"Shaokang Yang\"},{\"authorId\":\"122218340\",\"name\":\"J. Niu\"},{\"authorId\":\"1809483\",\"name\":\"Jiyan Wu\"},{\"authorId\":\"37305311\",\"name\":\"X. Liu\"}],\"doi\":\"10.1007/978-3-030-60248-2_48\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0e3f376cd8e08119a2f24821c667b7b9d6ec410\",\"title\":\"Automatic Medical Image Report Generation with Multi-view and Multi-modal Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/c0e3f376cd8e08119a2f24821c667b7b9d6ec410\",\"venue\":\"ICA3PP\",\"year\":2020},{\"arxivId\":\"2004.06165\",\"authors\":[{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"145307652\",\"name\":\"Li Dong\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1007/978-3-030-58577-8_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57\",\"title\":\"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ahmed Shehab Khan\"},{\"authorId\":null,\"name\":\"Zhiyuan Li\"},{\"authorId\":null,\"name\":\"Jie Cai\"},{\"authorId\":null,\"name\":\"Yan Tong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"170f403ca462bf88513197f8e499e31dd7701d1a\",\"title\":\"Regional Attention Networks with Context-aware Fusion for Group Emotion Recognition\",\"url\":\"https://www.semanticscholar.org/paper/170f403ca462bf88513197f8e499e31dd7701d1a\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1905.10671\",\"authors\":[{\"authorId\":\"12318532\",\"name\":\"Z. Huang\"},{\"authorId\":\"116746634\",\"name\":\"Senwei Liang\"},{\"authorId\":\"73445023\",\"name\":\"Mingfu Liang\"},{\"authorId\":\"2906086\",\"name\":\"H. Yang\"}],\"doi\":\"10.1609/AAAI.V34I04.5842\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"257e48098d564e26f021f596bea5c98ff6398fd1\",\"title\":\"DIANet: Dense-and-Implicit Attention Network\",\"url\":\"https://www.semanticscholar.org/paper/257e48098d564e26f021f596bea5c98ff6398fd1\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1904.08279\",\"authors\":[{\"authorId\":\"19248639\",\"name\":\"Sadaf Gulshad\"},{\"authorId\":\"2708564\",\"name\":\"J. H. Metzen\"},{\"authorId\":\"144638781\",\"name\":\"A. Smeulders\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d39d2fda54d287003adb133350178212fee6b60\",\"title\":\"Interpreting Adversarial Examples with Attributes\",\"url\":\"https://www.semanticscholar.org/paper/8d39d2fda54d287003adb133350178212fee6b60\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2004.02194\",\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"46507139\",\"name\":\"Haibo Wang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/CVPR42600.2020.01007\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a0d2ea210c9bd21676605682a76cec1a4004320a\",\"title\":\"Iterative Context-Aware Graph Inference for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1909.02217\",\"authors\":[{\"authorId\":\"144889895\",\"name\":\"Ming Jiang\"},{\"authorId\":\"145919382\",\"name\":\"J. Hu\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"3207378\",\"name\":\"J. Diesner\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.18653/v1/D19-1156\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"94c6e5ccd67be60a5ced11d0a5c59e0ab0f749d4\",\"title\":\"REO-Relevance, Extraness, Omission: A Fine-grained Evaluation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/94c6e5ccd67be60a5ced11d0a5c59e0ab0f749d4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1806.07243\",\"authors\":[{\"authorId\":\"1410126033\",\"name\":\"Will Norcliffe-Brown\"},{\"authorId\":\"2019087\",\"name\":\"Efstathios Vafeias\"},{\"authorId\":\"1398036715\",\"name\":\"Sarah Parisot\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6ac33d3dcecbed17580509a34bccdff2425f7ed8\",\"title\":\"Learning Conditioned Graph Structures for Interpretable Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/6ac33d3dcecbed17580509a34bccdff2425f7ed8\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51225721\",\"name\":\"Nikolaos Gkanatsios\"},{\"authorId\":\"1738119\",\"name\":\"Vassilis Pitsikalis\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/ICCVW.2019.00218\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d11b6028c1b29f57af506a3b1696a6569cc9542d\",\"title\":\"Attention-Translation-Relation Network for Scalable Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/d11b6028c1b29f57af506a3b1696a6569cc9542d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48831444\",\"name\":\"Shengdong Li\"},{\"authorId\":\"1759497\",\"name\":\"X. Lv\"}],\"doi\":\"10.1109/ICME.2019.00314\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2f22bd1a6dc157ec83a1f82c7ffa0887bef25a5\",\"title\":\"Momentum Based on Adaptive Bold Driver\",\"url\":\"https://www.semanticscholar.org/paper/c2f22bd1a6dc157ec83a1f82c7ffa0887bef25a5\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1912.03063\",\"authors\":[{\"authorId\":\"51149482\",\"name\":\"Corentin Kervadec\"},{\"authorId\":\"145664204\",\"name\":\"G. Antipov\"},{\"authorId\":\"2341854\",\"name\":\"M. Baccouche\"},{\"authorId\":\"144899680\",\"name\":\"C. Wolf\"}],\"doi\":\"10.3233/FAIA200412\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3e9336a1be4fc269a987656aab16d2791515917f\",\"title\":\"Weak Supervision helps Emergence of Word-Object Alignment and improves Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/3e9336a1be4fc269a987656aab16d2791515917f\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":\"1906.06632\",\"authors\":[{\"authorId\":\"145859178\",\"name\":\"Jian Zheng\"},{\"authorId\":\"33770363\",\"name\":\"Sudha Krishnamurthy\"},{\"authorId\":\"3090152\",\"name\":\"Ruxin Chen\"},{\"authorId\":\"50133145\",\"name\":\"Min-Hung Chen\"},{\"authorId\":\"11004494\",\"name\":\"Zhenhao Ge\"},{\"authorId\":\"47056922\",\"name\":\"X. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"854a20d09da5e412dbaf41513d97dca2f67b647b\",\"title\":\"Image Captioning with Integrated Bottom-Up and Multi-level Residual Top-Down Attention for Game Scene Understanding\",\"url\":\"https://www.semanticscholar.org/paper/854a20d09da5e412dbaf41513d97dca2f67b647b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1909.03169\",\"authors\":[{\"authorId\":\"32095408\",\"name\":\"Fawaz Sammani\"},{\"authorId\":\"67001969\",\"name\":\"Mahmoud Elsayed\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7af5c94fedc32f3104a08301a46c62f51b044a81\",\"title\":\"Look and Modify: Modification Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7af5c94fedc32f3104a08301a46c62f51b044a81\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"46651452\",\"name\":\"Cong Li\"},{\"authorId\":\"2641581\",\"name\":\"Tianpeng Li\"},{\"authorId\":\"47718901\",\"name\":\"Weitao Wan\"},{\"authorId\":\"47739902\",\"name\":\"J. Chen\"}],\"doi\":\"10.1109/ICIP.2019.8803108\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff24050374748529fa2a1fee6941af08296449f8\",\"title\":\"Image Captioning with Attribute Refinement\",\"url\":\"https://www.semanticscholar.org/paper/ff24050374748529fa2a1fee6941af08296449f8\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059218\",\"name\":\"Ankit Khare\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"01fe0c5f0d033141a29f4958f15520798022bbe7\",\"title\":\"ULTRA-CONTEXT: MAXIMIZING THE CONTEXT FOR BETTER IMAGE CAPTION GENERATION\",\"url\":\"https://www.semanticscholar.org/paper/01fe0c5f0d033141a29f4958f15520798022bbe7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Guohao Li\"},{\"authorId\":\"72541452\",\"name\":\"X. Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3350922\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35074d64a1d70172cab24a1343ae08ea3c078ce2\",\"title\":\"Perceptual Visual Reasoning with Knowledge Propagation\",\"url\":\"https://www.semanticscholar.org/paper/35074d64a1d70172cab24a1343ae08ea3c078ce2\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"151472634\",\"name\":\"Y. Hua\"},{\"authorId\":\"46406827\",\"name\":\"N. Robertson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9f964bc4f8405491bc69c9e03e6f466ec609b9dd\",\"title\":\"SC-RANK: Improving Convolutional Image Captioning with Self-Critical Learning and Ranking Metric-based Reward\",\"url\":\"https://www.semanticscholar.org/paper/9f964bc4f8405491bc69c9e03e6f466ec609b9dd\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50433099\",\"name\":\"Shaofeng Xu\"},{\"authorId\":\"33629364\",\"name\":\"Y. Xiong\"},{\"authorId\":\"1833914\",\"name\":\"Xiangnan Kong\"},{\"authorId\":\"8247706\",\"name\":\"Yangyong Zhu\"}],\"doi\":\"10.1007/978-3-030-18576-3_29\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1fcfa7c47623c1cb9630fadb3192fc4630956b4b\",\"title\":\"Net2Text: An Edge Labelling Language Model for Personalized Review Generation\",\"url\":\"https://www.semanticscholar.org/paper/1fcfa7c47623c1cb9630fadb3192fc4630956b4b\",\"venue\":\"DASFAA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144469308\",\"name\":\"Jian Wang\"},{\"authorId\":\"145534714\",\"name\":\"Jie Feng\"}],\"doi\":\"10.1109/ACCESS.2020.3018546\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"123361e4769f2f8a17742197aa52cc676a4caa9a\",\"title\":\"Hybrid Attention Distribution and Factorized Embedding Matrix in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/123361e4769f2f8a17742197aa52cc676a4caa9a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2004.03744\",\"authors\":[{\"authorId\":\"48615916\",\"name\":\"V. Do\"},{\"authorId\":\"3317152\",\"name\":\"Oana-Maria Camburu\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"1690572\",\"name\":\"Thomas Lukasiewicz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"889910db53995e18f8506820a765b8c0306f664b\",\"title\":\"e-SNLI-VE-2.0: Corrected Visual-Textual Entailment with Natural Language Explanations\",\"url\":\"https://www.semanticscholar.org/paper/889910db53995e18f8506820a765b8c0306f664b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1803.04376\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"31844147\",\"name\":\"Brian L. Price\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"2490189\",\"name\":\"Gregory Shakhnarovich\"}],\"doi\":\"10.1109/CVPR.2018.00728\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c1802d8d43dfe783650a03f03d41609fa5ae91e\",\"title\":\"Discriminability Objective for Training Descriptive Captions\",\"url\":\"https://www.semanticscholar.org/paper/7c1802d8d43dfe783650a03f03d41609fa5ae91e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2873524\",\"name\":\"Z. Ma\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"},{\"authorId\":\"150347046\",\"name\":\"Yangyang Cheng\"},{\"authorId\":\"51000590\",\"name\":\"Xinrui Zhu\"}],\"doi\":\"10.1109/ICME.2019.00225\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"daad11aee75bcf597602e654c33a12de61343dda\",\"title\":\"Image-to-Tree: A Tree-Structured Decoder for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/daad11aee75bcf597602e654c33a12de61343dda\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1510838844\",\"name\":\"Christian Otto\"},{\"authorId\":\"3430468\",\"name\":\"Matthias Springstein\"},{\"authorId\":\"39775488\",\"name\":\"Avishek Anand\"},{\"authorId\":\"1738703\",\"name\":\"R. Ewerth\"}],\"doi\":\"10.1007/s13735-019-00187-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc67b271a05b7b40ad23f3006375d094e094604f\",\"title\":\"Characterization and classification of semantic image-text relations\",\"url\":\"https://www.semanticscholar.org/paper/cc67b271a05b7b40ad23f3006375d094e094604f\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9432180\",\"name\":\"R. Kumar\"}],\"doi\":\"10.1007/s42979-020-00135-w\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9b3a8a857467fd389d835f4f6c9fcf5777ed026a\",\"title\":\"Visual Linguistic Model and Its Applications in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9b3a8a857467fd389d835f4f6c9fcf5777ed026a\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"144762952\",\"name\":\"J. Han\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"}],\"doi\":\"10.1109/tcyb.2020.2985716\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"758890bef9a1a85a25a1f6831a58f00a462476af\",\"title\":\"SMAN: Stacked Multimodal Attention Network for Cross-Modal Image-Text Retrieval.\",\"url\":\"https://www.semanticscholar.org/paper/758890bef9a1a85a25a1f6831a58f00a462476af\",\"venue\":\"IEEE transactions on cybernetics\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1805970305\",\"name\":\"Georgios Barlas\"},{\"authorId\":\"1859319\",\"name\":\"Christos Veinidis\"},{\"authorId\":\"35575984\",\"name\":\"A. Arampatzis\"}],\"doi\":\"10.1007/s00371-020-01867-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a31c2b3a88719419cf679778846edbe3be9e81b3\",\"title\":\"What we see in a photograph: content selection for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/a31c2b3a88719419cf679778846edbe3be9e81b3\",\"venue\":\"The Visual Computer\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yanan Wang\"},{\"authorId\":\"119837541\",\"name\":\"Jian-Ming Wu\"},{\"authorId\":\"47664350\",\"name\":\"Panikos Heracleous\"},{\"authorId\":\"2000114316\",\"name\":\"Shinya Wada\"},{\"authorId\":\"1809845071\",\"name\":\"Rui Kimura\"},{\"authorId\":\"1610841750\",\"name\":\"Satoshi Kurihara\"}],\"doi\":\"10.1145/3382507.3417960\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f8e51672cd32ec57536f689165d3030ff63f1e7d\",\"title\":\"Implicit Knowledge Injectable Cross Attention Audiovisual Model for Group Emotion Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f8e51672cd32ec57536f689165d3030ff63f1e7d\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92187979\",\"name\":\"C. Xu\"},{\"authorId\":\"103382547\",\"name\":\"Junzhong Ji\"},{\"authorId\":\"16003095\",\"name\":\"Meng-long Zhang\"},{\"authorId\":\"49470161\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1109/ICUSAI47366.2019.9124779\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"72364b3cd61221a99fb6be65e34a10c53db531cd\",\"title\":\"Attention-gated LSTM for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72364b3cd61221a99fb6be65e34a10c53db531cd\",\"venue\":\"2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46958543\",\"name\":\"T. Wang\"},{\"authorId\":\"49298611\",\"name\":\"J. Li\"},{\"authorId\":\"153501119\",\"name\":\"Zhaoning Kong\"},{\"authorId\":\"46333170\",\"name\":\"Liu Xin\"},{\"authorId\":\"2103629\",\"name\":\"H. Snoussi\"},{\"authorId\":\"2199558\",\"name\":\"H. Lv\"}],\"doi\":\"10.1016/j.jmsy.2020.07.011\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"13ce2e837c067f759d0596037e7bdeec80397553\",\"title\":\"Digital twin improved via visual question answering for vision-language interactive mode in human\\u2013machine collaboration\",\"url\":\"https://www.semanticscholar.org/paper/13ce2e837c067f759d0596037e7bdeec80397553\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.16267\",\"authors\":[{\"authorId\":\"145242715\",\"name\":\"Hao Cheng\"},{\"authorId\":\"3157379\",\"name\":\"Wentong Liao\"},{\"authorId\":\"1485719213\",\"name\":\"Xuejiao Tang\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"34946374\",\"name\":\"Monika Sester\"},{\"authorId\":\"1779035\",\"name\":\"B. Rosenhahn\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a81ff65038b8d5cbbe847a1ccc2a4ed2a415d50c\",\"title\":\"Exploring Dynamic Context for Multi-path Trajectory Prediction\",\"url\":\"https://www.semanticscholar.org/paper/a81ff65038b8d5cbbe847a1ccc2a4ed2a415d50c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.00299\",\"authors\":[{\"authorId\":\"1504364089\",\"name\":\"Mohammed Bany Muhammad\"},{\"authorId\":\"1828610\",\"name\":\"M. Yeasin\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206626\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8541d37973ec4c8a63b9b7b62dd6c1ec2d069d5\",\"title\":\"Eigen-CAM: Class Activation Map using Principal Components\",\"url\":\"https://www.semanticscholar.org/paper/d8541d37973ec4c8a63b9b7b62dd6c1ec2d069d5\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1812.05634\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00676\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"title\":\"Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.09317\",\"authors\":[{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.3389/frai.2019.00028\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"78f36f2acb0c88cfe74572933cb52c9cc75a1d50\",\"title\":\"Challenges and Prospects in Vision and Language Research\",\"url\":\"https://www.semanticscholar.org/paper/78f36f2acb0c88cfe74572933cb52c9cc75a1d50\",\"venue\":\"Front. Artif. Intell.\",\"year\":2019},{\"arxivId\":\"1907.12133\",\"authors\":[{\"authorId\":\"50445724\",\"name\":\"C. Zhang\"},{\"authorId\":\"150336480\",\"name\":\"Wei-Lun Chao\"},{\"authorId\":\"113737386\",\"name\":\"D. Xuan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"55a881988f757ff6fdac74429e39cb5b46aa3f47\",\"title\":\"An Empirical Study on Leveraging Scene Graphs for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/55a881988f757ff6fdac74429e39cb5b46aa3f47\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1807.08556\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1007/978-3-030-01234-2_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c611b9c82e234b344a232bcbbe5436e06da69f0b\",\"title\":\"Explainable Neural Computation via Stack Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/c611b9c82e234b344a232bcbbe5436e06da69f0b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1708.03619\",\"authors\":[{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"50812077\",\"name\":\"J. Yu\"},{\"authorId\":\"24071345\",\"name\":\"Chenchao Xiang\"},{\"authorId\":\"144147221\",\"name\":\"Jianping Fan\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/TNNLS.2018.2817340\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c0f41d3162e76500d4639557ff4463bd246e395\",\"title\":\"Beyond Bilinear: Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/0c0f41d3162e76500d4639557ff4463bd246e395\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2018},{\"arxivId\":\"1810.02358\",\"authors\":[{\"authorId\":\"2018393\",\"name\":\"Hyeonwoo Noh\"},{\"authorId\":\"1837923\",\"name\":\"Taehoon Kim\"},{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00858\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b80f128830114896df94999b4104cb75408e657e\",\"title\":\"Transfer Learning via Unsupervised Task Discovery for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b80f128830114896df94999b4104cb75408e657e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1811.09789\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"1795294\",\"name\":\"M. Dras\"},{\"authorId\":\"3093086\",\"name\":\"Stephen Wan\"},{\"authorId\":\"145212976\",\"name\":\"C\\u00e9cile Paris\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9347ee91bf90129582e7ed414d23ad3495180235\",\"title\":\"Senti-Attend: Image Captioning using Sentiment and Attention\",\"url\":\"https://www.semanticscholar.org/paper/9347ee91bf90129582e7ed414d23ad3495180235\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2008.06884\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1753617513\",\"name\":\"Tan Wang\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1704030\",\"name\":\"J. Zhu\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413518\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"080ee4e93438f8b8cbdd894eef15af71f0c30097\",\"title\":\"DeVLBert: Learning Deconfounded Visio-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/080ee4e93438f8b8cbdd894eef15af71f0c30097\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1808.09648\",\"authors\":[{\"authorId\":\"25309225\",\"name\":\"Avikalp Srivastava\"},{\"authorId\":\"153803927\",\"name\":\"H. Liu\"},{\"authorId\":\"33208854\",\"name\":\"S. Fujita\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"216c6d29a6f57c37ef8f26f88b6ec9be5b855a66\",\"title\":\"From VQA to Multimodal CQA: Adapting Visual QA Models for Community QA Tasks\",\"url\":\"https://www.semanticscholar.org/paper/216c6d29a6f57c37ef8f26f88b6ec9be5b855a66\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2003.04641\",\"authors\":[{\"authorId\":\"1491142121\",\"name\":\"Yuhong Deng\"},{\"authorId\":\"31513628\",\"name\":\"N. Zhang\"},{\"authorId\":\"144393479\",\"name\":\"D. Guo\"},{\"authorId\":\"2641547\",\"name\":\"H. Liu\"},{\"authorId\":\"2323566\",\"name\":\"Fu-Chun Sun\"},{\"authorId\":\"152806333\",\"name\":\"Chen Pang\"},{\"authorId\":\"1490867745\",\"name\":\"Jing Pang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88e8f7636a2a8d5d3d0a99708bea332770471eb3\",\"title\":\"MQA: Answering the Question via Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/88e8f7636a2a8d5d3d0a99708bea332770471eb3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144544920\",\"name\":\"Fei Liu\"},{\"authorId\":\"1749850\",\"name\":\"J. Liu\"},{\"authorId\":\"49597404\",\"name\":\"Z. Fang\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/ICIP.2019.8803670\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cd71651126f11162d262ff0dcb9ae094f51dcffd\",\"title\":\"Language and Visual Relations Encoding for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/cd71651126f11162d262ff0dcb9ae094f51dcffd\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672526\",\"name\":\"Shiwei Wang\"},{\"authorId\":\"2410125\",\"name\":\"Long Lan\"},{\"authorId\":\"46447554\",\"name\":\"Xiaodong Zhang\"},{\"authorId\":\"48593034\",\"name\":\"Guohua Dong\"},{\"authorId\":\"145254061\",\"name\":\"Z. Luo\"}],\"doi\":\"10.1109/ACCESS.2019.2917979\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5a759bbf042fe8852f23567cf14daf3d9f180bc\",\"title\":\"Cascade Semantic Fusion for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a759bbf042fe8852f23567cf14daf3d9f180bc\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"2007.09592\",\"authors\":[{\"authorId\":\"47043894\",\"name\":\"Ruixue Tang\"},{\"authorId\":\"144905344\",\"name\":\"Chifeng Ma\"},{\"authorId\":\"32794831\",\"name\":\"W. Zhang\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"15469693\",\"name\":\"Xiaokang Yang\"}],\"doi\":\"10.1007/978-3-030-58529-7_26\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ff560bbf5c11894379d7e808683d553e3d1f08c2\",\"title\":\"Semantic Equivalent Adversarial Data Augmentation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ff560bbf5c11894379d7e808683d553e3d1f08c2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.08565\",\"authors\":[{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"},{\"authorId\":\"150353841\",\"name\":\"Yinan Zhao\"},{\"authorId\":\"1409765557\",\"name\":\"Meng Zhang\"},{\"authorId\":\"23364558\",\"name\":\"Nilavra Bhattacharya\"}],\"doi\":\"10.1007/978-3-030-58520-4_25\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c936d878003254cdab662a966cecd29e8be652d0\",\"title\":\"Captioning Images Taken by People Who Are Blind\",\"url\":\"https://www.semanticscholar.org/paper/c936d878003254cdab662a966cecd29e8be652d0\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2012.14700\",\"authors\":[{\"authorId\":\"7748443\",\"name\":\"Sangwoong Yoon\"},{\"authorId\":\"21152168\",\"name\":\"Woo Young Kang\"},{\"authorId\":\"97519074\",\"name\":\"Sungwook Jeon\"},{\"authorId\":\"50112156\",\"name\":\"Seong-Eun Lee\"},{\"authorId\":\"118727697\",\"name\":\"Changjin Han\"},{\"authorId\":\"30664924\",\"name\":\"Jonghun Park\"},{\"authorId\":\"1845794808\",\"name\":\"Eun-Sol Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e0570df4e56d51be58b53166e853d848ef767af\",\"title\":\"Image-to-Image Retrieval by Learning Similarity between Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/2e0570df4e56d51be58b53166e853d848ef767af\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.08189\",\"authors\":[{\"authorId\":\"2283009\",\"name\":\"W. Chen\"},{\"authorId\":\"46315247\",\"name\":\"W. Wang\"},{\"authorId\":\"87109212\",\"name\":\"Li Liu\"},{\"authorId\":\"1731570\",\"name\":\"M. Lew\"}],\"doi\":\"10.1016/j.neucom.2020.10.042\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"61ba6969078b7358c61f7eb93b4150ab0e4f329b\",\"title\":\"New Ideas and Trends in Deep Multimodal Content Understanding: A Review\",\"url\":\"https://www.semanticscholar.org/paper/61ba6969078b7358c61f7eb93b4150ab0e4f329b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.01801\",\"authors\":[{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"46328947\",\"name\":\"B. Price\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.1109/WACV45572.2020.9093494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f59ae732612ce8c42035adfb47bd5739c6288ad6\",\"title\":\"Answering Questions about Data Visualizations using Efficient Bimodal Fusion\",\"url\":\"https://www.semanticscholar.org/paper/f59ae732612ce8c42035adfb47bd5739c6288ad6\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409786102\",\"name\":\"Yirui Wu\"},{\"authorId\":\"72405228\",\"name\":\"Xiaozhong Ji\"},{\"authorId\":\"29790429\",\"name\":\"Wanting Ji\"},{\"authorId\":\"121339486\",\"name\":\"Y. Tian\"},{\"authorId\":\"1418476321\",\"name\":\"Helen Zhou\"}],\"doi\":\"10.1007/s00521-019-04609-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b0c23c22a8f1468fbc0ad2117915d191e4048f1e\",\"title\":\"CASR: a context-aware residual network for single-image super-resolution\",\"url\":\"https://www.semanticscholar.org/paper/b0c23c22a8f1468fbc0ad2117915d191e4048f1e\",\"venue\":\"Neural Computing and Applications\",\"year\":2019},{\"arxivId\":\"2004.11883\",\"authors\":[{\"authorId\":\"41022273\",\"name\":\"Duy-Kien Nguyen\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6c7faa44bde54f017e164ad8e687bd1963005988\",\"title\":\"Revisiting Modulated Convolutions for Visual Counting and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/6c7faa44bde54f017e164ad8e687bd1963005988\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.01067\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"title\":\"Video Captioning Using Weak Annotation\",\"url\":\"https://www.semanticscholar.org/paper/aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.11475\",\"authors\":[{\"authorId\":\"48729196\",\"name\":\"Weijiang Yu\"},{\"authorId\":\"150167685\",\"name\":\"Jingwen Zhou\"},{\"authorId\":\"23476952\",\"name\":\"Weihao Yu\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"1730284\",\"name\":\"N. Xiao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef318e7ff0883e72d853c75736d20cc123b556d5\",\"title\":\"Heterogeneous Graph Learning for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/ef318e7ff0883e72d853c75736d20cc123b556d5\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1910.02974\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/ICRA40945.2020.9196653\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fe6f2a08cb8911d52533a413b071638d0463f10a\",\"title\":\"SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability\",\"url\":\"https://www.semanticscholar.org/paper/fe6f2a08cb8911d52533a413b071638d0463f10a\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48428476\",\"name\":\"Yu Quan\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"}],\"doi\":\"10.1007/978-3-030-29551-6_40\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1fb7748cda4741cd410b76687b1614df16dc821c\",\"title\":\"Object Detection by Combining Deep Dilated Convolutions Network and Light-Weight Network\",\"url\":\"https://www.semanticscholar.org/paper/1fb7748cda4741cd410b76687b1614df16dc821c\",\"venue\":\"KSEM\",\"year\":2019},{\"arxivId\":\"1911.09655\",\"authors\":[{\"authorId\":\"1822214\",\"name\":\"Haytham M. Fayek\"},{\"authorId\":\"31039758\",\"name\":\"J. Johnson\"}],\"doi\":\"10.1109/TASLP.2020.3010650\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bfa437129daef8533708924c395c3fd0c5c3bf81\",\"title\":\"Temporal Reasoning via Audio Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bfa437129daef8533708924c395c3fd0c5c3bf81\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"NATURAL SUPERVISION\"},{\"authorId\":\"13589371\",\"name\":\"Jiayuan Mao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"119837545\",\"name\":\"Jia-jun Wu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c48f5af4fc9b858d25316fb04ca5b50b3090fa44\",\"title\":\"Obj 1 Obj 2 Obj 3 Obj 4 Sphere Concept Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/c48f5af4fc9b858d25316fb04ca5b50b3090fa44\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"48325104\",\"name\":\"Z. Bai\"},{\"authorId\":\"46758870\",\"name\":\"Y. Zhu\"}],\"doi\":\"10.1145/3330393.3330410\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd0b66c3fdee9aebdb56d5998e955fd7cd5cd6a6\",\"title\":\"An Improved Approach Based on CNN-RNNs for Mathematical Expression Recognition\",\"url\":\"https://www.semanticscholar.org/paper/cd0b66c3fdee9aebdb56d5998e955fd7cd5cd6a6\",\"venue\":\"ICMSSP 2019\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ffaa0cb4e257d4d6886bd65af5a46e3a7755f067\",\"title\":\"Supplementary Material Cycle-Consistency for Robust Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ffaa0cb4e257d4d6886bd65af5a46e3a7755f067\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"51305348\",\"name\":\"Zhu Zhang\"},{\"authorId\":\"51055350\",\"name\":\"Shuwen Xiao\"},{\"authorId\":\"123034558\",\"name\":\"Z. Xiao\"},{\"authorId\":\"145477645\",\"name\":\"X. Yan\"},{\"authorId\":\"50812077\",\"name\":\"J. Yu\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TIP.2019.2922062\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80c67ece4198e3dd1ef88e6ddb81eb71bee5f3fa\",\"title\":\"Long-Form Video Question Answering via Dynamic Hierarchical Reinforced Networks\",\"url\":\"https://www.semanticscholar.org/paper/80c67ece4198e3dd1ef88e6ddb81eb71bee5f3fa\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151500672\",\"name\":\"Mohammed Suhail\"}],\"doi\":\"10.14288/1.0384601\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"575b1aee68dca07267ceacb4db6298d6cedc2d1e\",\"title\":\"Graph neural network for situation recognition\",\"url\":\"https://www.semanticscholar.org/paper/575b1aee68dca07267ceacb4db6298d6cedc2d1e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1912.04554\",\"authors\":[{\"authorId\":\"33305173\",\"name\":\"P. Purkait\"},{\"authorId\":\"72017275\",\"name\":\"C. Zach\"},{\"authorId\":\"152729539\",\"name\":\"I. Reid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d3fb5edef50e11351459e6f989c70fa7b45c69f2\",\"title\":\"Learning to generate new indoor scenes\",\"url\":\"https://www.semanticscholar.org/paper/d3fb5edef50e11351459e6f989c70fa7b45c69f2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fb1a989e98b79fdeb8ff9175710b0d9699ceff9c\",\"title\":\"Exploring Human-like Learning Capabilities of Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/fb1a989e98b79fdeb8ff9175710b0d9699ceff9c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"1585288737\",\"name\":\"Peng Wang\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4161d64430d0c283cdd10ba834301e68a772051c\",\"title\":\"Supplementary Material for \\u201cSay As You Wish: Fine-grained Control of Image Caption Generation with Abstract Scene Graph\\u201d\",\"url\":\"https://www.semanticscholar.org/paper/4161d64430d0c283cdd10ba834301e68a772051c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.09626\",\"authors\":[{\"authorId\":\"8363301\",\"name\":\"Sungyeon Kim\"},{\"authorId\":\"103550598\",\"name\":\"Minkyo Seo\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"72643925\",\"name\":\"Minsu Cho\"},{\"authorId\":\"2483916\",\"name\":\"Suha Kwak\"}],\"doi\":\"10.1109/CVPR.2019.00239\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"249f0a2ae3540dbe4f2a11806d2ac38581b9ad6b\",\"title\":\"Deep Metric Learning Beyond Binary Supervision\",\"url\":\"https://www.semanticscholar.org/paper/249f0a2ae3540dbe4f2a11806d2ac38581b9ad6b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1803.07724\",\"authors\":[{\"authorId\":\"34271280\",\"name\":\"J. Singh\"},{\"authorId\":\"40699843\",\"name\":\"Vincent Ying\"},{\"authorId\":\"46386672\",\"name\":\"Alex Nutkiewicz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"90d855f22d324f40230832a47e32f958a24b4aac\",\"title\":\"Attention on Attention: Architectures for Visual Question Answering (VQA)\",\"url\":\"https://www.semanticscholar.org/paper/90d855f22d324f40230832a47e32f958a24b4aac\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jos\\u00e9 Miguel Cano Sant\\u00edn\"},{\"authorId\":null,\"name\":\"Simon Dobnik\"},{\"authorId\":null,\"name\":\"Mehdi Ghanimifard\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"769cf57f538cb50c4f5bdec6bb64470d19e372ff\",\"title\":\"Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot\",\"url\":\"https://www.semanticscholar.org/paper/769cf57f538cb50c4f5bdec6bb64470d19e372ff\",\"venue\":\"PAM\",\"year\":2020},{\"arxivId\":\"1907.05092\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"50678073\",\"name\":\"Bei Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b7bf64b2c7372aa82d32424aacc6f4a86215433\",\"title\":\"Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8b7bf64b2c7372aa82d32424aacc6f4a86215433\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.04554\",\"authors\":[{\"authorId\":\"33305173\",\"name\":\"P. Purkait\"},{\"authorId\":\"72017275\",\"name\":\"C. Zach\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"}],\"doi\":\"10.1007/978-3-030-58586-0_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6a52fd650eecfc28948cce4dcf3e1d3aab70466\",\"title\":\"SG-VAE: Scene Grammar Variational Autoencoder to Generate New Indoor Scenes\",\"url\":\"https://www.semanticscholar.org/paper/b6a52fd650eecfc28948cce4dcf3e1d3aab70466\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"47957770\",\"name\":\"Xuying Zhang\"},{\"authorId\":\"40626740\",\"name\":\"Liujuan Cao\"},{\"authorId\":\"47096329\",\"name\":\"Yongjian Wu\"},{\"authorId\":\"1835006\",\"name\":\"Feiyue Huang\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"}],\"doi\":\"10.1145/3394171.3414008\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c19f8380410181249fdca70ec14f6b5f38ae0846\",\"title\":\"Exploring Language Prior for Mode-Sensitive Visual Attention Modeling\",\"url\":\"https://www.semanticscholar.org/paper/c19f8380410181249fdca70ec14f6b5f38ae0846\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2006.09238\",\"authors\":[{\"authorId\":\"1391202077\",\"name\":\"Joya Chen\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"50439333\",\"name\":\"DongSheng Liu\"},{\"authorId\":\"41157498\",\"name\":\"T. Xu\"}],\"doi\":\"10.1109/MIPR49039.2020.00066\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0adc3d917b1e832ae5adba570477ea2cbd92194a\",\"title\":\"Foreground-Background Imbalance Problem in Deep Object Detectors: A Review\",\"url\":\"https://www.semanticscholar.org/paper/0adc3d917b1e832ae5adba570477ea2cbd92194a\",\"venue\":\"2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)\",\"year\":2020},{\"arxivId\":\"2002.02649\",\"authors\":[{\"authorId\":\"9202187\",\"name\":\"Chaoqun Duan\"},{\"authorId\":\"145500846\",\"name\":\"Lei Cui\"},{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2675365\",\"name\":\"Conghui Zhu\"},{\"authorId\":\"1856039\",\"name\":\"T. Zhao\"}],\"doi\":\"10.3233/FAIA200320\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bf0bf048b88c2bb5a7d53e34ec912907295be81\",\"title\":\"Multimodal Matching Transformer for Live Commenting\",\"url\":\"https://www.semanticscholar.org/paper/7bf0bf048b88c2bb5a7d53e34ec912907295be81\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":\"2004.14638\",\"authors\":[{\"authorId\":\"7475040\",\"name\":\"S. Tan\"},{\"authorId\":\"2641547\",\"name\":\"H. Liu\"},{\"authorId\":\"144393479\",\"name\":\"D. Guo\"},{\"authorId\":\"2323566\",\"name\":\"Fu-Chun Sun\"}],\"doi\":\"10.15607/rss.2020.xvi.038\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"be6f591cfa15a23a13ea09b336f794601ba5eeca\",\"title\":\"Towards Embodied Scene Description\",\"url\":\"https://www.semanticscholar.org/paper/be6f591cfa15a23a13ea09b336f794601ba5eeca\",\"venue\":\"RSS 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1754689111\",\"name\":\"Simeon Sch\\u00fcz\"},{\"authorId\":\"1721364\",\"name\":\"Sina Zarrie\\u00df\"}],\"doi\":\"10.18653/v1/2020.acl-main.584\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b7cdac2d8e017b8ba1c612b47cd427727526e302\",\"title\":\"Knowledge Supports Visual Language Grounding: A Case Study on Colour Terms\",\"url\":\"https://www.semanticscholar.org/paper/b7cdac2d8e017b8ba1c612b47cd427727526e302\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48520182\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/3313873\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1a756ae8457454120ee2e067d4936d801f56ed62\",\"title\":\"Image Captioning by Asking Questions\",\"url\":\"https://www.semanticscholar.org/paper/1a756ae8457454120ee2e067d4936d801f56ed62\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1904.01475\",\"authors\":[{\"authorId\":\"35570245\",\"name\":\"Ali Furkan Biten\"},{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"143823474\",\"name\":\"M. Rusi\\u00f1ol\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":\"10.1109/CVPR.2019.01275\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"908c6b1577a1f5309ae183daf2e24363039f22a8\",\"title\":\"Good News, Everyone! Context Driven Entity-Aware Captioning for News Images\",\"url\":\"https://www.semanticscholar.org/paper/908c6b1577a1f5309ae183daf2e24363039f22a8\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1909.04696\",\"authors\":[{\"authorId\":\"20686092\",\"name\":\"Arijit Ray\"},{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"1696401\",\"name\":\"Ajay Divakaran\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"},{\"authorId\":\"69919463\",\"name\":\"Giedrius Burachas\"}],\"doi\":\"10.18653/v1/D19-1596\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"87d18100012d8e3bd85bfde93b34dadce4653fc6\",\"title\":\"Sunny and Dark Outside?! Improving Answer Consistency in VQA through Entailed Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/87d18100012d8e3bd85bfde93b34dadce4653fc6\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1908.07094\",\"authors\":[{\"authorId\":\"145798572\",\"name\":\"S. Ma\"},{\"authorId\":\"1801452\",\"name\":\"D. McDuff\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":\"10.1109/ICCV.2019.00769\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1e1feac96004866052787115ea08a4dcdd888b9\",\"title\":\"Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck\",\"url\":\"https://www.semanticscholar.org/paper/e1e1feac96004866052787115ea08a4dcdd888b9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1711.09151\",\"authors\":[{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"113001756\",\"name\":\"A. Deshpande\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":\"10.1109/CVPR.2018.00583\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"9fb5e3db385588f671b11cfc8bf18efb90ee7b19\",\"title\":\"Convolutional Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fb5e3db385588f671b11cfc8bf18efb90ee7b19\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1803.08024\",\"authors\":[{\"authorId\":\"1863953\",\"name\":\"Kuang-Huei Lee\"},{\"authorId\":\"1683647\",\"name\":\"X. Chen\"},{\"authorId\":\"144988571\",\"name\":\"Gang Hua\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"}],\"doi\":\"10.1007/978-3-030-01225-0_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45dd2a3cd7c27f2e9509b023d702408f5ac11c9d\",\"title\":\"Stacked Cross Attention for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/45dd2a3cd7c27f2e9509b023d702408f5ac11c9d\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143696833\",\"name\":\"Wenjie Cai\"},{\"authorId\":\"47362438\",\"name\":\"Q. Liu\"}],\"doi\":\"10.1016/j.neucom.2020.06.112\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c218331cf31c32c46eb660c1b2ce3e506c99b3b0\",\"title\":\"Image captioning with semantic-enhanced features and extremely hard negative examples\",\"url\":\"https://www.semanticscholar.org/paper/c218331cf31c32c46eb660c1b2ce3e506c99b3b0\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1908.02265\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"title\":\"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2006.00923\",\"authors\":[{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"35570245\",\"name\":\"Ali Furkan Biten\"},{\"authorId\":\"134682605\",\"name\":\"Ruben Tito\"},{\"authorId\":\"51238351\",\"name\":\"Andr\\u00e9s Mafla\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"16fd23b6c6661cfba01c3b9624e6a2617e45bee9\",\"title\":\"Multimodal grid features and cell pointers for Scene Text Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/16fd23b6c6661cfba01c3b9624e6a2617e45bee9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.11740\",\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":\"10.1007/978-3-030-58577-8_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8a305b9366608d54452ac30459ee57b4f5cf1c9\",\"title\":\"UNITER: UNiversal Image-TExt Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/d8a305b9366608d54452ac30459ee57b4f5cf1c9\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2001.01037\",\"authors\":[{\"authorId\":\"46969089\",\"name\":\"J. Sun\"},{\"authorId\":\"3633358\",\"name\":\"S. Lapuschkin\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"},{\"authorId\":\"49345823\",\"name\":\"Alexander Binder\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"82e836be97e706dca7029ce6a0553b4890726593\",\"title\":\"Understanding Image Captioning Models beyond Visualizing Attention\",\"url\":\"https://www.semanticscholar.org/paper/82e836be97e706dca7029ce6a0553b4890726593\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144613170\",\"name\":\"S. Cao\"},{\"authorId\":\"2896701\",\"name\":\"G. An\"},{\"authorId\":\"4464686\",\"name\":\"ZhenXing Zheng\"},{\"authorId\":\"2383779\",\"name\":\"Qiu-Qi Ruan\"}],\"doi\":\"10.1016/j.neucom.2020.08.019\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f235982f603a740dcee9cff7c5de234878f80a3f\",\"title\":\"Interactions Guided Generative Adversarial Network for unsupervised image captioning\",\"url\":\"https://www.semanticscholar.org/paper/f235982f603a740dcee9cff7c5de234878f80a3f\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1907.09408\",\"authors\":[{\"authorId\":\"144125122\",\"name\":\"L. Jiao\"},{\"authorId\":\"70450696\",\"name\":\"Fan Zhang\"},{\"authorId\":\"47185755\",\"name\":\"F. Liu\"},{\"authorId\":\"1702138\",\"name\":\"Shuyuan Yang\"},{\"authorId\":\"47681309\",\"name\":\"L. Li\"},{\"authorId\":\"1897949\",\"name\":\"Zhixi Feng\"},{\"authorId\":\"145036586\",\"name\":\"R. Qu\"}],\"doi\":\"10.1109/ACCESS.2019.2939201\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dfbeb3ca7a01fe80c49b76baa50bf092f71eef4a\",\"title\":\"A Survey of Deep Learning-Based Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/dfbeb3ca7a01fe80c49b76baa50bf092f71eef4a\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1805.07112\",\"authors\":[{\"authorId\":\"1828568\",\"name\":\"Chen Chen\"},{\"authorId\":\"39108991\",\"name\":\"Shuai Mu\"},{\"authorId\":\"1410650653\",\"name\":\"Wanpeng Xiao\"},{\"authorId\":\"1410066883\",\"name\":\"Zexiong Ye\"},{\"authorId\":\"1410052649\",\"name\":\"Liesi Wu\"},{\"authorId\":\"102396462\",\"name\":\"Fuming Ma\"},{\"authorId\":\"34974680\",\"name\":\"Q. Ju\"}],\"doi\":\"10.1609/aaai.v33i01.33018142\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"0daa3a4118e00b9f63b2d014a16ff1bc3ca9ff7e\",\"title\":\"Improving Image Captioning with Conditional Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/0daa3a4118e00b9f63b2d014a16ff1bc3ca9ff7e\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"42afa9a0166eeb45cb2e9b37e0a8eec482f78fe0\",\"title\":\"Visual Question Answering and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/42afa9a0166eeb45cb2e9b37e0a8eec482f78fe0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2013546\",\"name\":\"B. Wang\"},{\"authorId\":\"2263014\",\"name\":\"Xiangtao Zheng\"},{\"authorId\":\"1930660\",\"name\":\"Bo Qu\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/JSTARS.2019.2959208\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fbdb53c100005ac890989beb3d78e208ba9acda\",\"title\":\"Retrieval Topic Recurrent Memory Network for Remote Sensing Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fbdb53c100005ac890989beb3d78e208ba9acda\",\"venue\":\"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing\",\"year\":2020},{\"arxivId\":\"2012.02033\",\"authors\":[{\"authorId\":\"152332057\",\"name\":\"Baohua Sun\"},{\"authorId\":\"1999579263\",\"name\":\"Michael Lin\"},{\"authorId\":\"1505825326\",\"name\":\"Hao Sha\"},{\"authorId\":\"1986616718\",\"name\":\"Lin Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b366c17a5a0daf851ec4a5d94b29accb4c643b73\",\"title\":\"SuperOCR: A Conversion from Optical Character Recognition to Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b366c17a5a0daf851ec4a5d94b29accb4c643b73\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.07517\",\"authors\":[{\"authorId\":\"49606678\",\"name\":\"Jianan Wang\"},{\"authorId\":\"49730189\",\"name\":\"Boyang Li\"},{\"authorId\":\"46515715\",\"name\":\"Xiangyu Fan\"},{\"authorId\":\"1845592115\",\"name\":\"J. Lin\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f5cbd60f07a19afc3566376e404a490865e5def\",\"title\":\"Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions\",\"url\":\"https://www.semanticscholar.org/paper/5f5cbd60f07a19afc3566376e404a490865e5def\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.00451\",\"authors\":[{\"authorId\":\"153276988\",\"name\":\"Antoine Yang\"},{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"title\":\"Just Ask: Learning to Answer Questions from Millions of Narrated Videos\",\"url\":\"https://www.semanticscholar.org/paper/b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.01565\",\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"1920933367\",\"name\":\"Jing Li\"},{\"authorId\":\"145609003\",\"name\":\"Michael R. Lyu\"},{\"authorId\":\"145310663\",\"name\":\"Irwin King\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.268\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6dd1dd0f2b52b3b37be7229363101fa4fbbf1a50\",\"title\":\"Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings\",\"url\":\"https://www.semanticscholar.org/paper/6dd1dd0f2b52b3b37be7229363101fa4fbbf1a50\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2007.13135\",\"authors\":[{\"authorId\":\"144861502\",\"name\":\"Lei Shi\"},{\"authorId\":\"2198730\",\"name\":\"K. Shuang\"},{\"authorId\":\"1947101\",\"name\":\"Shijie Geng\"},{\"authorId\":\"47527626\",\"name\":\"Peng Su\"},{\"authorId\":\"50676465\",\"name\":\"Zhengkai Jiang\"},{\"authorId\":\"144740494\",\"name\":\"Peng Gao\"},{\"authorId\":\"2011378\",\"name\":\"Z. Fu\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"47374777\",\"name\":\"S. Su\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"71ff8e0194d1cc4f810f825a569263fea4056ecd\",\"title\":\"Contrastive Visual-Linguistic Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/71ff8e0194d1cc4f810f825a569263fea4056ecd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405907659\",\"name\":\"Ming Jiang\"},{\"authorId\":\"94011352\",\"name\":\"S. Chen\"},{\"authorId\":\"7788087\",\"name\":\"J. Yang\"},{\"authorId\":\"153386875\",\"name\":\"Q. Zhao\"}],\"doi\":\"10.1109/cvpr42600.2020.00305\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b3739681eb93295d05e9d6716f2ba94c48f047f3\",\"title\":\"Fantastic Answers and Where to Find Them: Immersive Question-Directed Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/b3739681eb93295d05e9d6716f2ba94c48f047f3\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1909.12911\",\"authors\":[{\"authorId\":\"145607944\",\"name\":\"Xin Guo\"},{\"authorId\":\"2454625\",\"name\":\"L. Polan\\u00eda\"},{\"authorId\":\"143734835\",\"name\":\"Bin Zhu\"},{\"authorId\":\"35020996\",\"name\":\"Charles Boncelet\"},{\"authorId\":\"1800783\",\"name\":\"K. Barner\"}],\"doi\":\"10.1109/WACV45572.2020.9093547\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41262b8a62ca1f28ca711ab6897759fc08379833\",\"title\":\"Graph Neural Networks for Image Understanding Based on Multiple Cues: Group Emotion Recognition and Event Recognition as Use Cases\",\"url\":\"https://www.semanticscholar.org/paper/41262b8a62ca1f28ca711ab6897759fc08379833\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145366409\",\"name\":\"F. Ren\"},{\"authorId\":\"1596808138\",\"name\":\"Yangyang Zhou\"}],\"doi\":\"10.1109/ACCESS.2020.2980024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed6ce80789889c0fd56c8117f85079c1c31fe426\",\"title\":\"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ed6ce80789889c0fd56c8117f85079c1c31fe426\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2880073\",\"name\":\"Gege Zhang\"},{\"authorId\":\"47428247\",\"name\":\"Qinghua Ma\"},{\"authorId\":\"1734497\",\"name\":\"Licheng Jiao\"},{\"authorId\":\"6906561\",\"name\":\"F. Liu\"},{\"authorId\":\"51136683\",\"name\":\"Qigong Sun\"}],\"doi\":\"10.24963/ijcai.2020/110\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c8dd8de02e03d57790b6696ba7f8e5d078cb943\",\"title\":\"AttAN: Attention Adversarial Networks for 3D Point Cloud Semantic Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/8c8dd8de02e03d57790b6696ba7f8e5d078cb943\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40362476\",\"name\":\"Liming Zhan\"},{\"authorId\":\"73548014\",\"name\":\"Bo Liu\"},{\"authorId\":\"145451510\",\"name\":\"L. Fan\"},{\"authorId\":\"145905368\",\"name\":\"Jiaxin Chen\"},{\"authorId\":\"1772198\",\"name\":\"X. Wu\"}],\"doi\":\"10.1145/3394171.3413761\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b8d5b853f2212cbb48a43f1edec9b96d76d388ec\",\"title\":\"Medical Visual Question Answering via Conditional Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/b8d5b853f2212cbb48a43f1edec9b96d76d388ec\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1908.08530\",\"authors\":[{\"authorId\":\"145499378\",\"name\":\"Weijie Su\"},{\"authorId\":\"2578924\",\"name\":\"X. Zhu\"},{\"authorId\":\"47746274\",\"name\":\"Y. Cao\"},{\"authorId\":\"48218753\",\"name\":\"B. Li\"},{\"authorId\":\"152309485\",\"name\":\"Lewei Lu\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"3304536\",\"name\":\"Jifeng Dai\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2527626c11a84f15709e943fbfa2356e19930e3b\",\"title\":\"VL-BERT: Pre-training of Generic Visual-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/2527626c11a84f15709e943fbfa2356e19930e3b\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1820939478\",\"name\":\"Preethi Vaidyanathan\"},{\"authorId\":\"113057658\",\"name\":\"Emily Prudhommeaux\"},{\"authorId\":\"144648940\",\"name\":\"Cecilia Ovesdotter Alm\"},{\"authorId\":\"2899726\",\"name\":\"J. Pelz\"}],\"doi\":\"10.1167/jov.20.7.13\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"36f47de2c0d8c8fa6d18423030d31e426586997c\",\"title\":\"Computational framework for fusing eye movements and spoken narratives for image annotation\",\"url\":\"https://www.semanticscholar.org/paper/36f47de2c0d8c8fa6d18423030d31e426586997c\",\"venue\":\"Journal of vision\",\"year\":2020},{\"arxivId\":\"2007.00222\",\"authors\":[{\"authorId\":\"67318326\",\"name\":\"Y. Koizumi\"},{\"authorId\":\"1423768681\",\"name\":\"Ryo Masumura\"},{\"authorId\":\"2963420\",\"name\":\"Kyosuke Nishida\"},{\"authorId\":\"50131290\",\"name\":\"M. Yasuda\"},{\"authorId\":\"2880610\",\"name\":\"S. Saito\"}],\"doi\":\"10.21437/interspeech.2020-2087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd1b307eff4e0a72e8c975e90bea6ec6be286718\",\"title\":\"A Transformer-based Audio Captioning Model with Keyword Estimation\",\"url\":\"https://www.semanticscholar.org/paper/cd1b307eff4e0a72e8c975e90bea6ec6be286718\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2065332\",\"name\":\"H. Lee\"},{\"authorId\":\"152333274\",\"name\":\"Seunghyun Yoon\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"2007775508\",\"name\":\"Doo Soon Kim\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"1731707\",\"name\":\"K. Jung\"}],\"doi\":\"10.18653/v1/2020.eval4nlp-1.4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2be4e374800a0db69695eb4c558a6653dd258fcd\",\"title\":\"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT\",\"url\":\"https://www.semanticscholar.org/paper/2be4e374800a0db69695eb4c558a6653dd258fcd\",\"venue\":\"EVAL4NLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51934339\",\"name\":\"Nuzhat Naqvi\"},{\"authorId\":\"83256875\",\"name\":\"Z. Ye\"}],\"doi\":\"10.1007/s11042-020-09128-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"12b53d372b723e201a234786792c6de002244386\",\"title\":\"Image captions: global-local and joint signals attention model (GL-JSAM)\",\"url\":\"https://www.semanticscholar.org/paper/12b53d372b723e201a234786792c6de002244386\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"1809.07041\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1007/978-3-030-01264-9_42\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"0000fcfd467a19cf0e59169c2f07d730a0f3a8b9\",\"title\":\"Exploring Visual Relationship for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0000fcfd467a19cf0e59169c2f07d730a0f3a8b9\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1903.05854\",\"authors\":[{\"authorId\":\"6002624\",\"name\":\"Tingting Qiao\"},{\"authorId\":\"49050705\",\"name\":\"J. Zhang\"},{\"authorId\":\"7471918\",\"name\":\"Duanqing Xu\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR.2019.00160\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5af2d424bb38db7e6a72f7bec2cfd8a5bb8af7fb\",\"title\":\"MirrorGAN: Learning Text-To-Image Generation by Redescription\",\"url\":\"https://www.semanticscholar.org/paper/5af2d424bb38db7e6a72f7bec2cfd8a5bb8af7fb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1811.12104\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"51491153\",\"name\":\"Takayuki Itamochi\"},{\"authorId\":\"3193466\",\"name\":\"K. Narioka\"},{\"authorId\":\"143973868\",\"name\":\"Ikuro Sato\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3c5907ba6b921841a49e77632c14db64e4db4fd4\",\"title\":\"Towards Human-Friendly Referring Expression Generation\",\"url\":\"https://www.semanticscholar.org/paper/3c5907ba6b921841a49e77632c14db64e4db4fd4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49336560\",\"name\":\"Weining Wang\"},{\"authorId\":\"144368926\",\"name\":\"Yan Huang\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/CVPR.2019.00042\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fa68cde4db12779adacb70a24961cf09b1adf73\",\"title\":\"Language-Driven Temporal Activity Localization: A Semantic Matching Reinforcement Learning Model\",\"url\":\"https://www.semanticscholar.org/paper/0fa68cde4db12779adacb70a24961cf09b1adf73\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2006.10079\",\"authors\":[{\"authorId\":\"41020827\",\"name\":\"Corentin Dancette\"},{\"authorId\":\"7535126\",\"name\":\"R\\u00e9mi Cad\\u00e8ne\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5cb23c3c09bcd9f0e12a864b9f4ac3fa0f2dfbd\",\"title\":\"Overcoming Statistical Shortcuts for Open-ended Visual Counting\",\"url\":\"https://www.semanticscholar.org/paper/b5cb23c3c09bcd9f0e12a864b9f4ac3fa0f2dfbd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112995179\",\"name\":\"L. Chen\"},{\"authorId\":\"1936990\",\"name\":\"Mengyao Zhai\"},{\"authorId\":\"50775044\",\"name\":\"Jiawei He\"},{\"authorId\":\"10771328\",\"name\":\"G. Mori\"}],\"doi\":\"10.1109/ICCVW.2019.00177\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"347748443be69ffceb2fb345df9b41448f2974ad\",\"title\":\"Object Grounding via Iterative Context Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/347748443be69ffceb2fb345df9b41448f2974ad\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3238568\",\"name\":\"M. Liu\"},{\"authorId\":\"1485768948\",\"name\":\"Lingjun Li\"},{\"authorId\":\"146896370\",\"name\":\"H. Hu\"},{\"authorId\":\"15880069\",\"name\":\"W. Guan\"},{\"authorId\":\"153307124\",\"name\":\"J. Tian\"}],\"doi\":\"10.1016/j.ipm.2019.102178\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4ba3a265763677dfb68567aa6b62cd5fe4a633b\",\"title\":\"Image caption generation with dual attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/e4ba3a265763677dfb68567aa6b62cd5fe4a633b\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"2006.08264\",\"authors\":[{\"authorId\":\"145242715\",\"name\":\"Hao Cheng\"},{\"authorId\":\"3157379\",\"name\":\"Wentong Liao\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"1779035\",\"name\":\"B. Rosenhahn\"},{\"authorId\":\"34946374\",\"name\":\"Monika Sester\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7cd21fa0174f4bf08b7cbd36372dffe4774369b6\",\"title\":\"AMENet: Attentive Maps Encoder Network for Trajectory Prediction\",\"url\":\"https://www.semanticscholar.org/paper/7cd21fa0174f4bf08b7cbd36372dffe4774369b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.05104\",\"authors\":[{\"authorId\":\"1396871443\",\"name\":\"Camila Kolling\"},{\"authorId\":\"8599825\",\"name\":\"Jonatas Wehrmann\"},{\"authorId\":\"1380051745\",\"name\":\"Rodrigo C. Barros\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206679\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e4bd198ec47697ed1442af0babd35b88451fd205\",\"title\":\"Component Analysis for Visual Question Answering Architectures\",\"url\":\"https://www.semanticscholar.org/paper/e4bd198ec47697ed1442af0babd35b88451fd205\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1909.03683\",\"authors\":[{\"authorId\":\"143997772\",\"name\":\"Christopher Clark\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":\"10.18653/v1/D19-1418\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba783d92d0eaf6a7bff6ced7660150ce38016bbc\",\"title\":\"Don't Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases\",\"url\":\"https://www.semanticscholar.org/paper/ba783d92d0eaf6a7bff6ced7660150ce38016bbc\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144544920\",\"name\":\"Fei Liu\"},{\"authorId\":\"1749850\",\"name\":\"J. Liu\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1145/3343031.3350993\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6834deeabfbc3ca5989ca7a3bd76dc5a84c5a346\",\"title\":\"Erasing-based Attention Learning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/6834deeabfbc3ca5989ca7a3bd76dc5a84c5a346\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2010.10802\",\"authors\":[{\"authorId\":\"2239880\",\"name\":\"I. Gat\"},{\"authorId\":\"38211837\",\"name\":\"Idan Schwartz\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"1918412\",\"name\":\"T. Hazan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da5dde64865d7620079e0f50ef27b32bbebef7af\",\"title\":\"Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies\",\"url\":\"https://www.semanticscholar.org/paper/da5dde64865d7620079e0f50ef27b32bbebef7af\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.18653/v1/d19-64\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"55c9ec9ee9bcedafc502684623dfa799c6ce35e7\",\"title\":\"Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)\",\"url\":\"https://www.semanticscholar.org/paper/55c9ec9ee9bcedafc502684623dfa799c6ce35e7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2011.10731\",\"authors\":[{\"authorId\":\"151253861\",\"name\":\"Weixin Liang\"},{\"authorId\":\"7650020\",\"name\":\"Feiyang Niu\"},{\"authorId\":\"8856206\",\"name\":\"Aishwarya N. Reganti\"},{\"authorId\":\"2028300167\",\"name\":\"Govind Thattai\"},{\"authorId\":\"1748051\",\"name\":\"G. T\\u00fcr\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"86fdbc4540d146b0a2d7d61bf9f0109fa1331dac\",\"title\":\"LRTA: A Transparent Neural-Symbolic Reasoning Framework with Modular Supervision for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/86fdbc4540d146b0a2d7d61bf9f0109fa1331dac\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.04963\",\"authors\":[{\"authorId\":\"1576818877\",\"name\":\"Kento Terao\"},{\"authorId\":\"47668008\",\"name\":\"T. Tamaki\"},{\"authorId\":\"1688940\",\"name\":\"B. Raytchev\"},{\"authorId\":\"1686272\",\"name\":\"K. Kaneda\"},{\"authorId\":\"49969107\",\"name\":\"S. Satoh\"}],\"doi\":\"10.1587/transinf.2020EDP7089\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d44be3e1158eb1b04e311479b17c0ff6f3fc06ab\",\"title\":\"Rephrasing visual questions by specifying the entropy of the answer distribution\",\"url\":\"https://www.semanticscholar.org/paper/d44be3e1158eb1b04e311479b17c0ff6f3fc06ab\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.02467\",\"authors\":[{\"authorId\":\"48567197\",\"name\":\"Zhou Yu\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1720236\",\"name\":\"J. Yu\"},{\"authorId\":\"144478231\",\"name\":\"T. Yu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1609/aaai.v33i01.33019127\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f2c1af57c056102806a184517313804f66e7447\",\"title\":\"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/4f2c1af57c056102806a184517313804f66e7447\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2004.13780\",\"authors\":[{\"authorId\":\"49590274\",\"name\":\"M. Saeed\"},{\"authorId\":\"153756236\",\"name\":\"S. Nawaz\"},{\"authorId\":\"1389596256\",\"name\":\"Pietro Morerio\"},{\"authorId\":\"38243491\",\"name\":\"A. Mahmood\"},{\"authorId\":\"145116184\",\"name\":\"I. Gallo\"},{\"authorId\":\"1697680\",\"name\":\"M. Yousaf\"},{\"authorId\":\"8955013\",\"name\":\"A. D. Bue\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"98ae5913e9b5de1225076bbdf27bb7173cd3a07e\",\"title\":\"Cross-modal Speaker Verification and Recognition: A Multilingual Perspective\",\"url\":\"https://www.semanticscholar.org/paper/98ae5913e9b5de1225076bbdf27bb7173cd3a07e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.08658\",\"authors\":[{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"46395829\",\"name\":\"Yufei Wang\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"145461380\",\"name\":\"Rishabh Jain\"},{\"authorId\":\"1607624548\",\"name\":\"Mark Johnson\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606363958\",\"name\":\"Devi Parikh\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"}],\"doi\":\"10.1109/ICCV.2019.00904\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"title\":\"nocaps: novel object captioning at scale\",\"url\":\"https://www.semanticscholar.org/paper/8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.08608\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/ICCV.2019.00435\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"45f9856d418527d23dc7c89197627fa1f3b215f9\",\"title\":\"Learning to Collocate Neural Modules for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/45f9856d418527d23dc7c89197627fa1f3b215f9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1908.07490\",\"authors\":[{\"authorId\":\"3218666\",\"name\":\"Hao Hao Tan\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D19-1514\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"79c93274429d6355959f1e4374c2147bb81ea649\",\"title\":\"LXMERT: Learning Cross-Modality Encoder Representations from Transformers\",\"url\":\"https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1909.04402\",\"authors\":[{\"authorId\":\"1387994359\",\"name\":\"Mitja Nikolaus\"},{\"authorId\":\"30671790\",\"name\":\"M. Abdou\"},{\"authorId\":\"48024953\",\"name\":\"Matthew Lamm\"},{\"authorId\":\"19509693\",\"name\":\"Rahul Aralikatte\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"}],\"doi\":\"10.18653/v1/K19-1009\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"11557ee241f6b5dc0a8c62bb8014c0111bbe45ba\",\"title\":\"Compositional Generalization in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/11557ee241f6b5dc0a8c62bb8014c0111bbe45ba\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"2003.12058\",\"authors\":[{\"authorId\":\"4055152\",\"name\":\"Sarah Pratt\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"20745881\",\"name\":\"Luca Weihs\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2684226\",\"name\":\"Aniruddha Kembhavi\"}],\"doi\":\"10.1007/978-3-030-58548-8_19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc261c0efb5f9ce82581932d1440630b861fb85f\",\"title\":\"Grounded Situation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fc261c0efb5f9ce82581932d1440630b861fb85f\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67159407\",\"name\":\"S. Amirian\"},{\"authorId\":\"1799121\",\"name\":\"K. Rasheed\"},{\"authorId\":\"1784399\",\"name\":\"T. Taha\"},{\"authorId\":\"1712033\",\"name\":\"H. Arabnia\"}],\"doi\":\"10.1109/ACCESS.2020.3042484\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"832aafb4989c24211a8377f82228c31f7a90ef81\",\"title\":\"Automatic Image and Video Caption Generation With Deep Learning: A Concise Review and Algorithmic Overlap\",\"url\":\"https://www.semanticscholar.org/paper/832aafb4989c24211a8377f82228c31f7a90ef81\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"49039449\",\"name\":\"Weifeng Zhang\"},{\"authorId\":\"7774960\",\"name\":\"Yuhang Lu\"},{\"authorId\":\"31055300\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"1703234\",\"name\":\"Yue Hu\"},{\"authorId\":\"2573626\",\"name\":\"Jianlong Tan\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/TMM.2020.2972830\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9ebcac11090f3c5a7b987c668d91c3e5fec0718b\",\"title\":\"Reasoning on the Relation: Enhancing Visual Representation for Visual Question Answering and Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/9ebcac11090f3c5a7b987c668d91c3e5fec0718b\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"2005.09801\",\"authors\":[{\"authorId\":\"7642720\",\"name\":\"Dehong Gao\"},{\"authorId\":\"81235644\",\"name\":\"Linbo Jin\"},{\"authorId\":\"50462177\",\"name\":\"B. Chen\"},{\"authorId\":\"2642333\",\"name\":\"Minghui Qiu\"},{\"authorId\":\"1491201634\",\"name\":\"Yi Wei\"},{\"authorId\":\"46972251\",\"name\":\"Y. Hu\"},{\"authorId\":\"3705643\",\"name\":\"H. Wang\"}],\"doi\":\"10.1145/3397271.3401430\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"efe4f7ca1a9e533a3a96cca0bca556f8e153e9b3\",\"title\":\"FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/efe4f7ca1a9e533a3a96cca0bca556f8e153e9b3\",\"venue\":\"SIGIR\",\"year\":2020},{\"arxivId\":\"2007.00145\",\"authors\":[{\"authorId\":\"3381900\",\"name\":\"E. Dodds\"},{\"authorId\":\"31922487\",\"name\":\"J. Culpepper\"},{\"authorId\":\"80236158\",\"name\":\"Simao Herdade\"},{\"authorId\":\"29969244\",\"name\":\"Y. Zhang\"},{\"authorId\":\"145908678\",\"name\":\"K. Boakye\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e09df55b9aaf6e81b210815106d5ea075e3aaad0\",\"title\":\"Modality-Agnostic Attention Fusion for visual search with text feedback\",\"url\":\"https://www.semanticscholar.org/paper/e09df55b9aaf6e81b210815106d5ea075e3aaad0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.03615\",\"authors\":[{\"authorId\":\"40175280\",\"name\":\"Huaizu Jiang\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1389846455\",\"name\":\"E. Learned-Miller\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"}],\"doi\":\"10.1109/cvpr42600.2020.01028\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8e4e57e0f7030f2c71ff7e9e3583d8ec908ca16e\",\"title\":\"In Defense of Grid Features for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8e4e57e0f7030f2c71ff7e9e3583d8ec908ca16e\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.03669\",\"authors\":[{\"authorId\":\"7934161\",\"name\":\"T. Chen\"},{\"authorId\":\"48550129\",\"name\":\"Jiajun Deng\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1007/978-3-030-58601-0_33\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"638a9e0ee63031d13193bf2f67483ffbaf44e674\",\"title\":\"Adaptive Offline Quintuplet Loss for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/638a9e0ee63031d13193bf2f67483ffbaf44e674\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2303858\",\"name\":\"N. Zaidi\"},{\"authorId\":\"143620680\",\"name\":\"Trevor Cohn\"},{\"authorId\":\"2561045\",\"name\":\"Gholamreza Haffari\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d354c100b51c458b5de4e7f285486c0019b128b\",\"title\":\"Decoding As Dynamic Programming For Recurrent Autoregressive Models\",\"url\":\"https://www.semanticscholar.org/paper/8d354c100b51c458b5de4e7f285486c0019b128b\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2004.04312\",\"authors\":[{\"authorId\":\"38727845\",\"name\":\"A. Burns\"},{\"authorId\":\"31494849\",\"name\":\"D. Kim\"},{\"authorId\":\"2129412\",\"name\":\"D. Wijaya\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"}],\"doi\":\"10.1007/978-3-030-58548-8_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8e9b6ecb592269836bcdc46d0d0d001e883c9ee\",\"title\":\"Learning to Scale Multilingual Representations for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/b8e9b6ecb592269836bcdc46d0d0d001e883c9ee\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"2134146\",\"name\":\"Nikoli Dryden\"},{\"authorId\":\"1752594045\",\"name\":\"Julius Frost\"},{\"authorId\":\"1713648\",\"name\":\"Torsten Hoefler\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2457396ebdf83f7f81a048863f4a0aa38bfb39f\",\"title\":\"Shapeshifter Networks: Cross-layer Parameter Sharing for Scalable and Effective Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/e2457396ebdf83f7f81a048863f4a0aa38bfb39f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.06666\",\"authors\":[{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"31039758\",\"name\":\"J. Johnson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1932edb4e5b2aae30ec1e7344b16d6110f52ef4\",\"title\":\"VirTex: Learning Visual Representations from Textual Annotations\",\"url\":\"https://www.semanticscholar.org/paper/b1932edb4e5b2aae30ec1e7344b16d6110f52ef4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49289638\",\"name\":\"Y. Xia\"},{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"46315174\",\"name\":\"Wenmin Wang\"},{\"authorId\":\"144539992\",\"name\":\"Xiao-Yong Wei\"},{\"authorId\":\"46669153\",\"name\":\"J. Chen\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054758\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"517efc27e303d408e36bad4d376884ae87fbbf93\",\"title\":\"Exploring Entity-Level Spatial Relationships for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/517efc27e303d408e36bad4d376884ae87fbbf93\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"1712.00733\",\"authors\":[{\"authorId\":\"6051714\",\"name\":\"G. Li\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"23ed7f18100717ba814b2859196e10c5d4fed216\",\"title\":\"Incorporating External Knowledge to Answer Open-Domain Visual Questions with Dynamic Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/23ed7f18100717ba814b2859196e10c5d4fed216\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134881509\",\"name\":\"Xiucong Shi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e75b28c77c83b3281541b6b9a0222de615ab78b\",\"title\":\"Image Description Generation in Chinese Based on Keywords Guidance\",\"url\":\"https://www.semanticscholar.org/paper/7e75b28c77c83b3281541b6b9a0222de615ab78b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1902.09368\",\"authors\":[{\"authorId\":\"71119060\",\"name\":\"Gi-Cheon Kang\"},{\"authorId\":\"70262116\",\"name\":\"Jaeseo Lim\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":\"10.18653/v1/D19-1209\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"86754c8a22d5df5636aa5603db01835b5d4ee32c\",\"title\":\"Dual Attention Networks for Visual Reference Resolution in Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/86754c8a22d5df5636aa5603db01835b5d4ee32c\",\"venue\":\"EMNLP\",\"year\":2019},{\"arxivId\":\"2004.08385\",\"authors\":[{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"2427516\",\"name\":\"Chenhui Chu\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab9b53307081ac7897fb84b646510907035be409\",\"title\":\"Knowledge-Based Visual Question Answering in Videos\",\"url\":\"https://www.semanticscholar.org/paper/ab9b53307081ac7897fb84b646510907035be409\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145093110\",\"name\":\"W. Zhou\"},{\"authorId\":\"49070204\",\"name\":\"Yuzhen Chen\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"1410141881\",\"name\":\"Lu Yu\"}],\"doi\":\"10.1109/LSP.2020.2993471\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"904964f02158a19b0e2afdee08c60e6715a9933c\",\"title\":\"GFNet: Gate Fusion Network With Res2Net for Detecting Salient Objects in RGB-D Images\",\"url\":\"https://www.semanticscholar.org/paper/904964f02158a19b0e2afdee08c60e6715a9933c\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2326243\",\"name\":\"Arun Balajee Vasudevan\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"392d258c7c49a8d22bd6b323b2fb67bbca780a5b\",\"title\":\"Talk2Nav: Long-Range Vision-and-Language Navigation in Cities\",\"url\":\"https://www.semanticscholar.org/paper/392d258c7c49a8d22bd6b323b2fb67bbca780a5b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1812.09681\",\"authors\":[{\"authorId\":\"51126032\",\"name\":\"Zhuoqian Yang\"},{\"authorId\":\"49402458\",\"name\":\"J. Yu\"},{\"authorId\":null,\"name\":\"Chenghao Yang\"},{\"authorId\":\"70565757\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"48483709\",\"name\":\"Y. Hu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dd2f6fe2cd8e96ca62a9c1c9e12973b8e13d5609\",\"title\":\"Multi-modal Learning with Prior Visual Relation Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/dd2f6fe2cd8e96ca62a9c1c9e12973b8e13d5609\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1909.00121\",\"authors\":[{\"authorId\":\"49178142\",\"name\":\"H. Chen\"},{\"authorId\":\"145468578\",\"name\":\"Ke Lin\"},{\"authorId\":\"1772128\",\"name\":\"A. Maye\"},{\"authorId\":\"47786863\",\"name\":\"J. Li\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.3389/frobt.2020.475767\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"304f94dbe2ed228309e86298766ad24d9b6c6747\",\"title\":\"A Semantics-Assisted Video Captioning Model Trained With Scheduled Sampling\",\"url\":\"https://www.semanticscholar.org/paper/304f94dbe2ed228309e86298766ad24d9b6c6747\",\"venue\":\"Frontiers in Robotics and AI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7535126\",\"name\":\"R\\u00e9mi Cad\\u00e8ne\"},{\"authorId\":\"1405301761\",\"name\":\"Hedi Ben-younes\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"},{\"authorId\":\"1728523\",\"name\":\"N. Thome\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cfc9ef5c7ef8056cff7bf1f1cfdd75e120f28231\",\"title\":\"Multimodal Relational Reasoning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/cfc9ef5c7ef8056cff7bf1f1cfdd75e120f28231\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1910.06475\",\"authors\":[{\"authorId\":\"152325076\",\"name\":\"H. Ge\"},{\"authorId\":\"79403975\",\"name\":\"Zehang Yan\"},{\"authorId\":\"96280392\",\"name\":\"K. Zhang\"},{\"authorId\":\"145327825\",\"name\":\"Mingde Zhao\"},{\"authorId\":\"46732983\",\"name\":\"Liang Sun\"}],\"doi\":\"10.1109/ICCV.2019.00184\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b5231149f566fc8a78797b6fb448f9bca416380\",\"title\":\"Exploring Overall Contextual Information for Image Captioning in Human-Like Cognitive Style\",\"url\":\"https://www.semanticscholar.org/paper/4b5231149f566fc8a78797b6fb448f9bca416380\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47666606\",\"name\":\"Hui Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1818920\",\"name\":\"Zijia Lin\"},{\"authorId\":\"34811036\",\"name\":\"Yuchen Guo\"},{\"authorId\":\"10795229\",\"name\":\"Caifeng Shan\"},{\"authorId\":\"144762952\",\"name\":\"J. Han\"}],\"doi\":\"10.1007/s12559-019-09656-w\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e99a3163815a0c40705ffd6347c6cdbf19fa5237\",\"title\":\"Image Captioning with Memorized Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/e99a3163815a0c40705ffd6347c6cdbf19fa5237\",\"venue\":\"Cognitive Computation\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113519586\",\"name\":\"T. X. Dang\"},{\"authorId\":\"31704596\",\"name\":\"A. Oh\"},{\"authorId\":\"9483271\",\"name\":\"In-Seop Na\"},{\"authorId\":\"2183069\",\"name\":\"S. Kim\"}],\"doi\":\"10.1145/3310986.3311002\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eeb782dcf29651caca5a2bcb6019b54c3ddee1a0\",\"title\":\"The Role of Attention Mechanism and Multi-Feature in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/eeb782dcf29651caca5a2bcb6019b54c3ddee1a0\",\"venue\":\"ICMLSC 2019\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144454465\",\"name\":\"L. Peng\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"32518385\",\"name\":\"Z. Wang\"},{\"authorId\":\"153028349\",\"name\":\"Xiao Wu\"},{\"authorId\":\"83672162\",\"name\":\"Zi Huang\"}],\"doi\":\"10.1145/3343031.3350925\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb488ee07fc078eb0200c3a4ca119bc67303e507\",\"title\":\"CRA-Net: Composed Relation Attention Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/eb488ee07fc078eb0200c3a4ca119bc67303e507\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1912.03478\",\"authors\":[{\"authorId\":\"2223252\",\"name\":\"Y. Zhou\"},{\"authorId\":\"145592288\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"51230543\",\"name\":\"G. Luo\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":\"2713947\",\"name\":\"Xinghao Ding\"},{\"authorId\":\"46246806\",\"name\":\"Chia-Wen Lin\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3165d8c9a3466ecac5f6e9dc7e61b65c62c1decb\",\"title\":\"A Real-time Global Inference Network for One-stage Referring Expression Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/3165d8c9a3466ecac5f6e9dc7e61b65c62c1decb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97713340\",\"name\":\"X. Liu\"},{\"authorId\":\"1943870\",\"name\":\"Weibin Liu\"},{\"authorId\":\"145767616\",\"name\":\"Weiwei Xing\"}],\"doi\":\"10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00152\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f55a588eef043cbb72ee548714d623b573c21e9b\",\"title\":\"Image Caption Generation with Local Semantic Information and Global Information\",\"url\":\"https://www.semanticscholar.org/paper/f55a588eef043cbb72ee548714d623b573c21e9b\",\"venue\":\"2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49597404\",\"name\":\"Z. Fang\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"49544292\",\"name\":\"Xueliang Liu\"},{\"authorId\":null,\"name\":\"Qu Tang\"},{\"authorId\":\"50025104\",\"name\":\"Yonghong Li\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1145/3282469\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2c74311cc5e18a120bebf331b0dd85c72426380d\",\"title\":\"BTDP: Toward Sparse Fusion with Block Term Decomposition Pooling for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/2c74311cc5e18a120bebf331b0dd85c72426380d\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993695002\",\"name\":\"Hongshuo Tian\"},{\"authorId\":\"145857587\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.1145/3394171.3413501\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9b93808e8c70410d1cec437ab79a7a0286ba4edb\",\"title\":\"Part-Aware Interactive Learning for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/9b93808e8c70410d1cec437ab79a7a0286ba4edb\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1904.12584\",\"authors\":[{\"authorId\":\"13589371\",\"name\":\"Jiayuan Mao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ec9b27d019fefadb5e97c8174ac889e831f483d7\",\"title\":\"The Neuro-Symbolic Concept Learner: Interpreting Scenes Words and Sentences from Natural Supervision\",\"url\":\"https://www.semanticscholar.org/paper/ec9b27d019fefadb5e97c8174ac889e831f483d7\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1903.04104\",\"authors\":[{\"authorId\":\"47003565\",\"name\":\"Yixin Li\"},{\"authorId\":\"79384643\",\"name\":\"Shengqin Tang\"},{\"authorId\":\"72522575\",\"name\":\"Y. Ye\"},{\"authorId\":\"1685259\",\"name\":\"Jinwen Ma\"}],\"doi\":\"10.1109/ICME.2019.00146\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"099eeeed8dc5a53603e4f739a3335fcfb94fc1db\",\"title\":\"Spatial-Aware Non-Local Attention for Fashion Landmark Detection\",\"url\":\"https://www.semanticscholar.org/paper/099eeeed8dc5a53603e4f739a3335fcfb94fc1db\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1902.05660\",\"authors\":[{\"authorId\":\"144826412\",\"name\":\"Meet Shah\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2019.00681\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"735a63b58349e07b84c2e31927ce1b1cfaf09980\",\"title\":\"Cycle-Consistency for Robust Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/735a63b58349e07b84c2e31927ce1b1cfaf09980\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a7501eb6d58b1f347140402171df7b3291496ab2\",\"title\":\"Connective Cognition Network for Directional Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/a7501eb6d58b1f347140402171df7b3291496ab2\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22603654\",\"name\":\"Xuecheng Ning\"},{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"48258806\",\"name\":\"C. Xu\"}],\"doi\":\"10.1007/978-3-030-37734-2_55\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"120b849abbfcaefe0e212c38141e86958118d1d7\",\"title\":\"Multi-hop Interactive Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/120b849abbfcaefe0e212c38141e86958118d1d7\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":\"1909.05506\",\"authors\":[{\"authorId\":\"1390879204\",\"name\":\"Zihao Wang\"},{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"},{\"authorId\":\"37145669\",\"name\":\"Lu Sheng\"},{\"authorId\":\"1721677\",\"name\":\"J. Yan\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"145965208\",\"name\":\"J. Shao\"}],\"doi\":\"10.1109/ICCV.2019.00586\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"19c630ad5a9de227f6357479fc95c62667be17f6\",\"title\":\"CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/19c630ad5a9de227f6357479fc95c62667be17f6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1911.09042\",\"authors\":[{\"authorId\":\"46398531\",\"name\":\"Yongfei Liu\"},{\"authorId\":\"47241555\",\"name\":\"Bo Wan\"},{\"authorId\":\"150345740\",\"name\":\"Xiao-Dan Zhu\"},{\"authorId\":\"33913193\",\"name\":\"Xuming He\"}],\"doi\":\"10.1609/aaai.v34i07.6833\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c6d410891bef95ce4240eaa6d4908feb493527c\",\"title\":\"Learning Cross-modal Context Graph for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/7c6d410891bef95ce4240eaa6d4908feb493527c\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1911.07883\",\"authors\":[{\"authorId\":\"94228656\",\"name\":\"Fengda Zhu\"},{\"authorId\":\"144899462\",\"name\":\"Yi Zhu\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"13246332\",\"name\":\"Xiaodan Liang\"}],\"doi\":\"10.1109/cvpr42600.2020.01003\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e7c78a3a0ebafad671ada06235fbebc34b097cd6\",\"title\":\"Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks\",\"url\":\"https://www.semanticscholar.org/paper/e7c78a3a0ebafad671ada06235fbebc34b097cd6\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"50172036\",\"name\":\"X. Wu\"},{\"authorId\":\"1993634137\",\"name\":\"Shen Ge\"},{\"authorId\":\"47958349\",\"name\":\"X. Zhang\"},{\"authorId\":\"144934703\",\"name\":\"Wei Fan\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1145/3394171.3414004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"title\":\"Bridging the Gap between Vision and Language Domains for Improved Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1809.04144\",\"authors\":[{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6610cecb59fdd108c8fcb73cac3562fa10d5e845\",\"title\":\"End-to-end Image Captioning Exploits Multimodal Distributional Similarity\",\"url\":\"https://www.semanticscholar.org/paper/6610cecb59fdd108c8fcb73cac3562fa10d5e845\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2006.03744\",\"authors\":[{\"authorId\":\"50651835\",\"name\":\"Mingjie Li\"},{\"authorId\":\"46429484\",\"name\":\"F. Wang\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"13246332\",\"name\":\"Xiaodan Liang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"461420c80d3bdc156e5db7af13264a955a6a2010\",\"title\":\"Auxiliary Signal-Guided Knowledge Encoder-Decoder for Medical Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/461420c80d3bdc156e5db7af13264a955a6a2010\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.09105\",\"authors\":[{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"150270181\",\"name\":\"Qing Du\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1609/AAAI.V34I07.6737\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"982192e82f17876bf3f8703fbd90ba7e8e3a6e5c\",\"title\":\"Location-Aware Graph Convolutional Networks for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/982192e82f17876bf3f8703fbd90ba7e8e3a6e5c\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2006.08335\",\"authors\":[{\"authorId\":\"1750913684\",\"name\":\"Bofan Xue\"},{\"authorId\":\"1774825\",\"name\":\"D. Chan\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5ca80f5097b6ad4f4537d913c48abc470fb37342\",\"title\":\"A Dataset and Benchmarks for Multimedia Social Analysis\",\"url\":\"https://www.semanticscholar.org/paper/5ca80f5097b6ad4f4537d913c48abc470fb37342\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1596812933\",\"name\":\"Zhiwei Wu\"},{\"authorId\":\"150068355\",\"name\":\"Changmeng Zheng\"},{\"authorId\":\"143997941\",\"name\":\"Y. Cai\"},{\"authorId\":\"153425774\",\"name\":\"Junying Chen\"},{\"authorId\":\"1701688\",\"name\":\"Ho-fung Leung\"},{\"authorId\":\"152283947\",\"name\":\"Q. Li\"}],\"doi\":\"10.1145/3394171.3413650\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"05f7078218eecd1bb1795aaa681418c6d3c1aef7\",\"title\":\"Multimodal Representation with Embedded Visual Guiding Objects for Named Entity Recognition in Social Media Posts\",\"url\":\"https://www.semanticscholar.org/paper/05f7078218eecd1bb1795aaa681418c6d3c1aef7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8772234\",\"name\":\"Hyeryun Park\"},{\"authorId\":\"113066066\",\"name\":\"Kyungmo Kim\"},{\"authorId\":\"72062486\",\"name\":\"J. Yoon\"},{\"authorId\":\"31171717\",\"name\":\"Seongkeun Park\"},{\"authorId\":\"153439158\",\"name\":\"Jinwook Choi\"}],\"doi\":\"10.18653/v1/2020.acl-srw.14\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b85f64b0a7c8eabd11476fa870b14b4ec696166\",\"title\":\"Feature Difference Makes Sense: A medical image captioning model exploiting feature difference and tag information\",\"url\":\"https://www.semanticscholar.org/paper/3b85f64b0a7c8eabd11476fa870b14b4ec696166\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48064309\",\"name\":\"L. Yang\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"},{\"authorId\":\"150311018\",\"name\":\"Songlong Xing\"},{\"authorId\":\"150344317\",\"name\":\"Xinlong Lu\"}],\"doi\":\"10.1145/3386725\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1546133a67bfab09c64a9e2875266b1286ccab55\",\"title\":\"Constrained LSTM and Residual Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1546133a67bfab09c64a9e2875266b1286ccab55\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"1711.11118\",\"authors\":[{\"authorId\":\"30083905\",\"name\":\"Robert L Logan IV\"},{\"authorId\":\"2795882\",\"name\":\"Samuel Humeau\"},{\"authorId\":\"34650964\",\"name\":\"Sameer Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce57cc478421adf85a9058a0cc8fad8ebfd81c52\",\"title\":\"Multimodal Attribute Extraction\",\"url\":\"https://www.semanticscholar.org/paper/ce57cc478421adf85a9058a0cc8fad8ebfd81c52\",\"venue\":\"AKBC@NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"46700004\",\"name\":\"J. Liu\"},{\"authorId\":\"116829059\",\"name\":\"Shi-chen Lu\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/TMM.2019.2951226\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"837a513a43c7bcce903edbacbfc507cba6451e21\",\"title\":\"Show, Tell, and Polish: Ruminant Decoding for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/837a513a43c7bcce903edbacbfc507cba6451e21\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1399904149\",\"name\":\"Tzu-Jui Julius Wang\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"1766204\",\"name\":\"M. Sj\\u00f6berg\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1145/3347450.3357656\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b45118c19c2c90dd8bad32ffeb5b6f7c69eb3ba9\",\"title\":\"Geometry-aware Relational Exemplar Attention for Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b45118c19c2c90dd8bad32ffeb5b6f7c69eb3ba9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3180068\",\"name\":\"D. C. Wyld\"},{\"authorId\":\"1805607\",\"name\":\"D. Nagamalai\"}],\"doi\":\"10.5121/csit.2018.80600\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb3f306c2529c5aea9b10dbe47c9269e04574e00\",\"title\":\"Computer Science & Information Technology\",\"url\":\"https://www.semanticscholar.org/paper/eb3f306c2529c5aea9b10dbe47c9269e04574e00\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"2011.08543\",\"authors\":[{\"authorId\":\"152601809\",\"name\":\"Minh Thu Nguyen\"},{\"authorId\":\"6195410\",\"name\":\"D. Phung\"},{\"authorId\":\"2356016\",\"name\":\"Minh Hoai\"},{\"authorId\":\"2008200586\",\"name\":\"Thien Huu Nguyen\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.411\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"28ffdee5f50398202f236fd088fbc0c624b9f9ce\",\"title\":\"Structural and Functional Decomposition for Personality Image Captioning in a Communication Game\",\"url\":\"https://www.semanticscholar.org/paper/28ffdee5f50398202f236fd088fbc0c624b9f9ce\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.04913\",\"authors\":[{\"authorId\":\"47043894\",\"name\":\"Ruixue Tang\"},{\"authorId\":\"1684762080\",\"name\":\"Chao Ma\"}],\"doi\":\"10.1007/978-3-030-60636-7_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f65f90c41aafd40449edc8e2c2c80a63bc767e6d\",\"title\":\"Interpretable Neural Computation for Real-World Compositional Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f65f90c41aafd40449edc8e2c2c80a63bc767e6d\",\"venue\":\"PRCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1590802267\",\"name\":\"J. Kim\"},{\"authorId\":\"1580654340\",\"name\":\"Hanbin Ko\"},{\"authorId\":\"9095876\",\"name\":\"Jipeng Wu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9480a1a403f3c3e0be22e08e63716ef4d5b38217\",\"title\":\"CoNAN: A Complementary Neighboring-based Attention Network for Referring Expression Generation\",\"url\":\"https://www.semanticscholar.org/paper/9480a1a403f3c3e0be22e08e63716ef4d5b38217\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2003.07333\",\"authors\":[{\"authorId\":\"7754251\",\"name\":\"Sylvain Lobry\"},{\"authorId\":\"144173388\",\"name\":\"D. Marcos\"},{\"authorId\":\"1409495574\",\"name\":\"J. Murray\"},{\"authorId\":\"1404577763\",\"name\":\"D. Tuia\"}],\"doi\":\"10.1109/TGRS.2020.2988782\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0b3ddddefee26c686da2e1088d21e69c594f1c3d\",\"title\":\"RSVQA: Visual Question Answering for Remote Sensing Data\",\"url\":\"https://www.semanticscholar.org/paper/0b3ddddefee26c686da2e1088d21e69c594f1c3d\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2020},{\"arxivId\":\"2010.13110\",\"authors\":[{\"authorId\":\"150166903\",\"name\":\"J. Xu\"},{\"authorId\":\"27093563\",\"name\":\"Fangwei Zhong\"},{\"authorId\":null,\"name\":\"Yizhou Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5e10cce7bc1a480bf9ce0f2fb19ffc8d8a800e77\",\"title\":\"Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks\",\"url\":\"https://www.semanticscholar.org/paper/5e10cce7bc1a480bf9ce0f2fb19ffc8d8a800e77\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48079662\",\"name\":\"Sungho Park\"},{\"authorId\":\"145864562\",\"name\":\"Sunhee Hwang\"},{\"authorId\":\"9024867\",\"name\":\"Jongkwang Hong\"},{\"authorId\":\"1703310\",\"name\":\"Hyeran Byun\"}],\"doi\":\"10.1109/ACCESS.2020.3041503\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"958284df0d0eca1b9cb234411aa15940c99e7ce6\",\"title\":\"Fair-VQA: Fairness-Aware Visual Question Answering Through Sensitive Attribute Prediction\",\"url\":\"https://www.semanticscholar.org/paper/958284df0d0eca1b9cb234411aa15940c99e7ce6\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1901.00484\",\"authors\":[{\"authorId\":\"29132542\",\"name\":\"Meera Hahn\"},{\"authorId\":\"49915485\",\"name\":\"Andrew Silva\"},{\"authorId\":\"144177248\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"797243368d6ccde3b70bab9e1265f4e1d4e1cc43\",\"title\":\"Action2Vec: A Crossmodal Embedding Approach to Action Learning\",\"url\":\"https://www.semanticscholar.org/paper/797243368d6ccde3b70bab9e1265f4e1d4e1cc43\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1808.10792\",\"authors\":[{\"authorId\":\"3159346\",\"name\":\"Sebastian Gehrmann\"},{\"authorId\":\"2505751\",\"name\":\"Y. Deng\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":\"10.18653/v1/D18-1443\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7af89df3691d8c33aaf1858f7cc51da1bc9549a9\",\"title\":\"Bottom-Up Abstractive Summarization\",\"url\":\"https://www.semanticscholar.org/paper/7af89df3691d8c33aaf1858f7cc51da1bc9549a9\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1726159226\",\"name\":\"Xiangqing Shen\"},{\"authorId\":\"49167055\",\"name\":\"B. Liu\"},{\"authorId\":\"1697439\",\"name\":\"Yong Zhou\"},{\"authorId\":\"1491078664\",\"name\":\"Jiaqi Zhao\"}],\"doi\":\"10.1007/s11042-020-09294-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"20d2de48968f3ed1dc00be6ded0e2271d0f1a1a4\",\"title\":\"Remote sensing image caption generation via transformer and reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/20d2de48968f3ed1dc00be6ded0e2271d0f1a1a4\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dzmitry Bahdanau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2eb710b446570f48377b25eb279295648d05f65d\",\"title\":\"On sample efficiency and systematic generalization of grounded language understanding with deep learning\",\"url\":\"https://www.semanticscholar.org/paper/2eb710b446570f48377b25eb279295648d05f65d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.11528\",\"authors\":[{\"authorId\":\"46875376\",\"name\":\"Xi Zhu\"},{\"authorId\":\"1855978\",\"name\":\"Zhendong Mao\"},{\"authorId\":\"123266794\",\"name\":\"C. Liu\"},{\"authorId\":\"9228892\",\"name\":\"P. Zhang\"},{\"authorId\":null,\"name\":\"Bin Wang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.24963/ijcai.2020/151\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0714f88deda344c87bf78569de68d9e1f0b377a7\",\"title\":\"Overcoming Language Priors with Self-supervised Learning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/0714f88deda344c87bf78569de68d9e1f0b377a7\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042704741\",\"name\":\"Xiaohan Zou\"},{\"authorId\":\"144289788\",\"name\":\"C. Lin\"},{\"authorId\":\"2042741172\",\"name\":\"Yinjia Zhang\"},{\"authorId\":\"1729695\",\"name\":\"Qinpei Zhao\"}],\"doi\":\"10.1109/ICTAI50040.2020.00124\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f8465dc22e66853636edce1cd537317120ecfcbb\",\"title\":\"To be an Artist: Automatic Generation on Food Image Aesthetic Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f8465dc22e66853636edce1cd537317120ecfcbb\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38889850\",\"name\":\"Pengfei Xia\"},{\"authorId\":\"50774917\",\"name\":\"Jingsong He\"},{\"authorId\":\"153781930\",\"name\":\"Jin Yin\"}],\"doi\":\"10.1007/s11042-020-09110-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"17cefd99f2d04fc01663ba261d6afee54e8408d5\",\"title\":\"Boosting image caption generation with feature fusion module\",\"url\":\"https://www.semanticscholar.org/paper/17cefd99f2d04fc01663ba261d6afee54e8408d5\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"80526284\",\"name\":\"Yanyuan Qiao\"},{\"authorId\":\"48567083\",\"name\":\"Zheng Yu\"},{\"authorId\":\"1749850\",\"name\":\"J. Liu\"}],\"doi\":\"10.1109/icme46284.2020.9102814\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fef6c56e474fa99e907b478f17f1becd50900f21\",\"title\":\"Rankvqa: Answer Re-Ranking For Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/fef6c56e474fa99e907b478f17f1becd50900f21\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"40186452\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/978-3-030-30645-8_66\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6f378be13df9e97e15ca240b60a0a0aa16d5eb64\",\"title\":\"Artpedia: A New Visual-Semantic Dataset with Visual and Contextual Sentences in the Artistic Domain\",\"url\":\"https://www.semanticscholar.org/paper/6f378be13df9e97e15ca240b60a0a0aa16d5eb64\",\"venue\":\"ICIAP\",\"year\":2019},{\"arxivId\":\"2005.03784\",\"authors\":[{\"authorId\":\"39092446\",\"name\":\"Arianna Yuan\"},{\"authorId\":null,\"name\":\"Yang Li\"}],\"doi\":\"10.1145/3313831.3376870\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"202fa272d3979ae06f27a565eefb2df3d09cc544\",\"title\":\"Modeling Human Visual Search Performance on Realistic Webpages Using Analytical and Deep Learning Methods\",\"url\":\"https://www.semanticscholar.org/paper/202fa272d3979ae06f27a565eefb2df3d09cc544\",\"venue\":\"CHI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"1791344388\",\"name\":\"Lei Ji\"},{\"authorId\":\"1783553\",\"name\":\"Zhen-dong Niu\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/3394171.3413498\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"title\":\"Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning\",\"url\":\"https://www.semanticscholar.org/paper/de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145511765\",\"name\":\"Tong Wu\"},{\"authorId\":\"38338059\",\"name\":\"T. Ku\"},{\"authorId\":\"153504695\",\"name\":\"Hao Zhang\"}],\"doi\":\"10.1117/12.2552711\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b37b3cdcd6d839aaee661324c9d4a5411de20a11\",\"title\":\"Research for image caption based on global attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/b37b3cdcd6d839aaee661324c9d4a5411de20a11\",\"venue\":\"Target Recognition and Artificial Intelligence Summit Forum\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34714992\",\"name\":\"Yi Qin\"},{\"authorId\":\"145069303\",\"name\":\"S. Xiang\"},{\"authorId\":\"1747476\",\"name\":\"Y. Chai\"},{\"authorId\":\"2141371\",\"name\":\"Haizhou Chen\"}],\"doi\":\"10.1109/TIE.2019.2959492\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"37f7a227df465c5362bf3df54ca90262a38e83e2\",\"title\":\"Macroscopic\\u2013Microscopic Attention in LSTM Networks Based on Fusion Features for Gear Remaining Life Prediction\",\"url\":\"https://www.semanticscholar.org/paper/37f7a227df465c5362bf3df54ca90262a38e83e2\",\"venue\":\"IEEE Transactions on Industrial Electronics\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29545186\",\"name\":\"M. Hosseinzadeh\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":\"10.1109/cvpr42600.2020.00365\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ef6db51cb736116266025eb1eab2fb4f36b75310\",\"title\":\"Composed Query Image Retrieval Using Locally Bounded Features\",\"url\":\"https://www.semanticscholar.org/paper/ef6db51cb736116266025eb1eab2fb4f36b75310\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49303626\",\"name\":\"K. Su\"},{\"authorId\":\"144904233\",\"name\":\"Hang Su\"},{\"authorId\":\"104545113\",\"name\":\"J. Li\"},{\"authorId\":\"1557387379\",\"name\":\"Jun Zhu\"}],\"doi\":\"10.3389/frobt.2020.00109\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"22e2a19bef88c30a37402da5b54fac34655d77ea\",\"title\":\"Toward Accurate Visual Reasoning With Dual-Path Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/22e2a19bef88c30a37402da5b54fac34655d77ea\",\"venue\":\"Frontiers in Robotics and AI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46583706\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"},{\"authorId\":\"50857536\",\"name\":\"Z. Wang\"},{\"authorId\":\"145855523\",\"name\":\"D. Feng\"},{\"authorId\":\"145808910\",\"name\":\"Tieniu Tan\"}],\"doi\":\"10.1016/j.patcog.2019.107075\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"70b03dfb947ca136e3476c9f49b8f6b044d0c1d9\",\"title\":\"Learning visual relationship and context-aware attention for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/70b03dfb947ca136e3476c9f49b8f6b044d0c1d9\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46432859\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"144585905\",\"name\":\"L. Huang\"},{\"authorId\":\"46458156\",\"name\":\"L. Liu\"},{\"authorId\":\"35550884\",\"name\":\"F. Zhu\"},{\"authorId\":\"8041153\",\"name\":\"S. Cui\"},{\"authorId\":\"144082425\",\"name\":\"L. Shao\"}],\"doi\":\"10.1109/CVPR.2019.00218\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b3637c9379d57479f3d2b63a8d7dc26a2cc6237\",\"title\":\"Collaborative Learning of Semi-Supervised Segmentation and Classification for Medical Images\",\"url\":\"https://www.semanticscholar.org/paper/9b3637c9379d57479f3d2b63a8d7dc26a2cc6237\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47011474\",\"name\":\"Kun Xiong\"},{\"authorId\":\"1807459832\",\"name\":\"Liu Jiang\"},{\"authorId\":\"39056715\",\"name\":\"Xuan Dang\"},{\"authorId\":\"50248791\",\"name\":\"G. Wang\"},{\"authorId\":\"1830569732\",\"name\":\"Wenwen Ye\"},{\"authorId\":\"1489386471\",\"name\":\"Zheng Qin\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206953\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dafb451fc3a51032c825eea6cc2037911089d47c\",\"title\":\"Towards Personalized Aesthetic Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/dafb451fc3a51032c825eea6cc2037911089d47c\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"2001.11782\",\"authors\":[{\"authorId\":\"152584142\",\"name\":\"Zhengxiong Jia\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":\"10.1145/3372278.3390697\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"87c7ec86e37178720686eba4d00ea53b2aca93d7\",\"title\":\"iCap: Interactive Image Captioning with Predictive Text\",\"url\":\"https://www.semanticscholar.org/paper/87c7ec86e37178720686eba4d00ea53b2aca93d7\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":\"1912.08360\",\"authors\":[{\"authorId\":\"49102717\",\"name\":\"Feilong Chen\"},{\"authorId\":\"33427918\",\"name\":\"Fandong Meng\"},{\"authorId\":\"46372563\",\"name\":\"Jiaming Xu\"},{\"authorId\":\"144326610\",\"name\":\"Peng Li\"},{\"authorId\":\"153260119\",\"name\":\"Bo Xu\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1609/AAAI.V34I05.6248\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7ae9c15c90b7d3e3c90c7f6d83743d4a0e07416b\",\"title\":\"DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/7ae9c15c90b7d3e3c90c7f6d83743d4a0e07416b\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2003.11844\",\"authors\":[{\"authorId\":\"51228129\",\"name\":\"Shailza Jolly\"},{\"authorId\":\"145491338\",\"name\":\"S. Palacio\"},{\"authorId\":\"144553243\",\"name\":\"J. Folz\"},{\"authorId\":\"48258541\",\"name\":\"Federico Raue\"},{\"authorId\":\"120996558\",\"name\":\"J. Hees\"},{\"authorId\":\"1384499125\",\"name\":\"Andreas Dengel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"625ec4698251cd181818bfd12f1cd5e63c7bcef0\",\"title\":\"P $\\\\approx$ NP, at least in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/625ec4698251cd181818bfd12f1cd5e63c7bcef0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48428476\",\"name\":\"Yu Quan\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"47190726\",\"name\":\"F. Zhang\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"}],\"doi\":\"10.1007/978-3-030-29894-4_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f66ae3e8ec524d13b1d15eced23df710c56498b7\",\"title\":\"D_dNet-65 R-CNN: Object Detection Model Fusing Deep Dilated Convolutions and Light-Weight Networks\",\"url\":\"https://www.semanticscholar.org/paper/f66ae3e8ec524d13b1d15eced23df710c56498b7\",\"venue\":\"PRICAI\",\"year\":2019},{\"arxivId\":\"2001.10857\",\"authors\":[{\"authorId\":\"2897459\",\"name\":\"Sebastian Stabinger\"},{\"authorId\":\"38185901\",\"name\":\"Justus Piater\"},{\"authorId\":\"1410954364\",\"name\":\"A. Rodr\\u00edguez-S\\u00e1nchez\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c36304005e6c6864111a3e803f34683808ebb8e\",\"title\":\"Evaluating the Progress of Deep Learning for Visual Relational Concepts\",\"url\":\"https://www.semanticscholar.org/paper/5c36304005e6c6864111a3e803f34683808ebb8e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.06023\",\"authors\":[{\"authorId\":\"120070100\",\"name\":\"Xinyu Zhang\"},{\"authorId\":\"48263835\",\"name\":\"Rufeng Zhang\"},{\"authorId\":\"39767248\",\"name\":\"Jiewei Cao\"},{\"authorId\":\"145542268\",\"name\":\"Dong Gong\"},{\"authorId\":\"34647494\",\"name\":\"Mingyu You\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a46c9e97c13c1c84267ea14bfcd2999abb7cd1c\",\"title\":\"Part-Guided Attention Learning for Vehicle Re-Identification\",\"url\":\"https://www.semanticscholar.org/paper/9a46c9e97c13c1c84267ea14bfcd2999abb7cd1c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46307991\",\"name\":\"Liyan Chen\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6af347d516b1975bafa59abb34a0bfb2bb09b364\",\"title\":\"Learning Latent Graph Representations for Relational VQA\",\"url\":\"https://www.semanticscholar.org/paper/6af347d516b1975bafa59abb34a0bfb2bb09b364\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.03366\",\"authors\":[{\"authorId\":\"93872817\",\"name\":\"Yatri Modi\"},{\"authorId\":\"2326758\",\"name\":\"Natalie Parde\"}],\"doi\":\"10.18653/v1/W19-1805\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a1269edf443cce55ad8de22822ec65a88a9db49\",\"title\":\"The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models\",\"url\":\"https://www.semanticscholar.org/paper/8a1269edf443cce55ad8de22822ec65a88a9db49\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"title\":\"Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1906.05963\",\"authors\":[{\"authorId\":\"80236158\",\"name\":\"Simao Herdade\"},{\"authorId\":\"40441990\",\"name\":\"Armin Kappeler\"},{\"authorId\":\"145908678\",\"name\":\"K. Boakye\"},{\"authorId\":\"145730823\",\"name\":\"J. Soares\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b499228aa74b59be32711c3926e44de208d6b636\",\"title\":\"Image Captioning: Transforming Objects into Words\",\"url\":\"https://www.semanticscholar.org/paper/b499228aa74b59be32711c3926e44de208d6b636\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Lukas Zbinden\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf095a26a34757616b29215617a99af8dc634e52\",\"title\":\"Learning Object Representations by Mixing Scenes Master Thesis\",\"url\":\"https://www.semanticscholar.org/paper/bf095a26a34757616b29215617a99af8dc634e52\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.06954\",\"authors\":[{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"46315174\",\"name\":\"Wenmin Wang\"},{\"authorId\":\"40445654\",\"name\":\"J. Chen\"},{\"authorId\":\"144539992\",\"name\":\"Xiao-Yong Wei\"}],\"doi\":\"10.1109/ICCV.2019.00473\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4c163d4942117179d3e97182e1b280027d7d60a9\",\"title\":\"Attention on Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4c163d4942117179d3e97182e1b280027d7d60a9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"153731335\",\"name\":\"Zhe Gan\"},{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"33c10383189c118465f8b40e8dba9213f57fa570\",\"title\":\"UNITER: Learning UNiversal Image-TExt Representations\",\"url\":\"https://www.semanticscholar.org/paper/33c10383189c118465f8b40e8dba9213f57fa570\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.05728\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"152264213\",\"name\":\"Shivansh Patel\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2cdd7267c95f00728e214439d41ad3efb6457ab6\",\"title\":\"Granular Multimodal Attention Networks for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/2cdd7267c95f00728e214439d41ad3efb6457ab6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.08226\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"261570379fd841b426a4c51e8004f2cf9f1df771\",\"title\":\"M2: Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/261570379fd841b426a4c51e8004f2cf9f1df771\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145764583\",\"name\":\"F. Liu\"},{\"authorId\":\"40628473\",\"name\":\"Jing Liu\"},{\"authorId\":\"49597404\",\"name\":\"Z. Fang\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.24963/ijcai.2019/122\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"99dc8f4c6be75b32a584608cea2c0da0de47db8d\",\"title\":\"Densely Connected Attention Flow for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/99dc8f4c6be75b32a584608cea2c0da0de47db8d\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2002.05556\",\"authors\":[{\"authorId\":\"144869806\",\"name\":\"Pedro Henrique Martins\"},{\"authorId\":\"2114966\",\"name\":\"Vlad Niculae\"},{\"authorId\":\"2566656\",\"name\":\"Zita Marinho\"},{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"4e627eb9f17369770f2cef5d71360c93c8494785\",\"title\":\"Sparse and Structured Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4e627eb9f17369770f2cef5d71360c93c8494785\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.03805\",\"authors\":[{\"authorId\":\"2848320\",\"name\":\"Dilin Wang\"},{\"authorId\":\"29777869\",\"name\":\"Chengyue Gong\"},{\"authorId\":\"47362268\",\"name\":\"Qiang Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e84d754564c9e2ce993596370e0a1493c9c6e4b1\",\"title\":\"Improving Neural Language Modeling via Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/e84d754564c9e2ce993596370e0a1493c9c6e4b1\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31465302\",\"name\":\"E. Wang\"},{\"authorId\":\"46182609\",\"name\":\"X. Zhang\"},{\"authorId\":\"39907479\",\"name\":\"F. Wang\"},{\"authorId\":\"1682589\",\"name\":\"T. Wu\"},{\"authorId\":\"144404748\",\"name\":\"Chien-Ming Chen\"}],\"doi\":\"10.1109/ACCESS.2019.2917771\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"751e871c12f70bf05ea012b21f1434849d6fe5ce\",\"title\":\"Multilayer Dense Attention Model for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/751e871c12f70bf05ea012b21f1434849d6fe5ce\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1908.02726\",\"authors\":[{\"authorId\":\"94845899\",\"name\":\"Qianyu Feng\"},{\"authorId\":\"98264517\",\"name\":\"Y. Wu\"},{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"3863922\",\"name\":\"C. Yan\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.1109/TCSVT.2020.2965966\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"daa87e86b469d975a9ad84bfb5eee230efa8af3e\",\"title\":\"Cascaded Revision Network for Novel Object Captioning\",\"url\":\"https://www.semanticscholar.org/paper/daa87e86b469d975a9ad84bfb5eee230efa8af3e\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"16358330\",\"name\":\"S. Liu\"},{\"authorId\":\"51130608\",\"name\":\"Tongzhen Si\"},{\"authorId\":\"2711226\",\"name\":\"Xiaolong Hao\"},{\"authorId\":\"34539206\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1109/ACCESS.2019.2958126\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f67da6d51d8aa038ad5993b2c921170053858f7f\",\"title\":\"Semantic Constraint GAN for Person Re-Identification in Camera Sensor Networks\",\"url\":\"https://www.semanticscholar.org/paper/f67da6d51d8aa038ad5993b2c921170053858f7f\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115145773\",\"name\":\"Seohyeon Kim\"},{\"authorId\":\"1871958\",\"name\":\"Gunpil Hwang\"},{\"authorId\":\"9383774\",\"name\":\"Hyeon-Min Bae\"}],\"doi\":\"10.1109/ACCESS.2020.3031297\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6bea31ec0066ad60650d5017fd5c79af9044e00a\",\"title\":\"Bat-G2 Net: Bat-Inspired Graphical Visualization Network Guided by Radiated Ultrasonic Call\",\"url\":\"https://www.semanticscholar.org/paper/6bea31ec0066ad60650d5017fd5c79af9044e00a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1796706\",\"name\":\"Daojian Zeng\"},{\"authorId\":\"1400347434\",\"name\":\"Guanhong Zhou\"},{\"authorId\":\"9491882\",\"name\":\"J. Wang\"}],\"doi\":\"10.1109/ICECIE47765.2019.8974765\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"01163764edf888ded242e992845badaaf6c6ec6e\",\"title\":\"Residual Self-Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/01163764edf888ded242e992845badaaf6c6ec6e\",\"venue\":\"2019 1st International Conference on Electrical, Control and Instrumentation Engineering (ICECIE)\",\"year\":2019},{\"arxivId\":\"1906.01290\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"097981245eb3c66cc10a3164275d0bd52f5ae22a\",\"title\":\"Relational Reasoning using Prior Knowledge for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/097981245eb3c66cc10a3164275d0bd52f5ae22a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490966079\",\"name\":\"A. Calabrese\"},{\"authorId\":\"143802044\",\"name\":\"Michele Bevilacqua\"},{\"authorId\":\"1733928\",\"name\":\"R. Navigli\"}],\"doi\":\"10.18653/v1/2020.acl-main.425\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"36d7e8c618bbc5e59ba3d13d7cce7e94d829eea5\",\"title\":\"Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts\",\"url\":\"https://www.semanticscholar.org/paper/36d7e8c618bbc5e59ba3d13d7cce7e94d829eea5\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"1380065125\",\"name\":\"Narges Honarvar Nazari\"},{\"authorId\":\"90323489\",\"name\":\"J. Hahn\"},{\"authorId\":\"1996796\",\"name\":\"Zaeem Hussain\"},{\"authorId\":\"2365530\",\"name\":\"Mingda Zhang\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1109/tpami.2019.2947440\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"86df22f8dbec3489432063ef569a4793dc232c70\",\"title\":\"Interpreting the Rhetoric of Visual Advertisements.\",\"url\":\"https://www.semanticscholar.org/paper/86df22f8dbec3489432063ef569a4793dc232c70\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2019},{\"arxivId\":\"2007.10662\",\"authors\":[{\"authorId\":\"69544685\",\"name\":\"Jie Wu\"},{\"authorId\":\"1765674\",\"name\":\"Tianshui Chen\"},{\"authorId\":\"1721715\",\"name\":\"Hefeng Wu\"},{\"authorId\":\"10665619\",\"name\":\"Z. Yang\"},{\"authorId\":\"1773818\",\"name\":\"Guangchun Luo\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/tmm.2020.3011317\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f0edc8eb2d2b53482faf34906dacf996bc4127f6\",\"title\":\"Fine-Grained Image Captioning with Global-Local Discriminative Objective\",\"url\":\"https://www.semanticscholar.org/paper/f0edc8eb2d2b53482faf34906dacf996bc4127f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.00266\",\"authors\":[{\"authorId\":\"50757607\",\"name\":\"Ben Bogin\"},{\"authorId\":\"17097887\",\"name\":\"Sanjay Subramanian\"},{\"authorId\":\"40642935\",\"name\":\"Matt Gardner\"},{\"authorId\":\"1750652\",\"name\":\"Jonathan Berant\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9af882a8629087ac4529bec6f6e9ea71b2ae772\",\"title\":\"Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b9af882a8629087ac4529bec6f6e9ea71b2ae772\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.13280\",\"authors\":[{\"authorId\":\"2531558\",\"name\":\"G. Chao\"},{\"authorId\":\"2188497\",\"name\":\"Abhinav Rastogi\"},{\"authorId\":\"3014143\",\"name\":\"Semih Yavuz\"},{\"authorId\":\"152325757\",\"name\":\"D. Hakkani-T\\u00fcr\"},{\"authorId\":\"47740493\",\"name\":\"Jindong Chen\"},{\"authorId\":\"5347612\",\"name\":\"I. Lane\"}],\"doi\":\"10.18653/v1/W19-5926\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fb5af7b7d5ebf9e8ff4164233a73b8fe4fe737a3\",\"title\":\"Learning Question-Guided Video Representation for Multi-Turn Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/fb5af7b7d5ebf9e8ff4164233a73b8fe4fe737a3\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144069314\",\"name\":\"Bin Jiang\"},{\"authorId\":\"47932618\",\"name\":\"X. Huang\"},{\"authorId\":\"143702931\",\"name\":\"C. Yang\"},{\"authorId\":\"48837492\",\"name\":\"J. Yuan\"}],\"doi\":\"10.1016/j.ipm.2019.102104\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bbc7306c513c42dff502dd0ce3850aab54096216\",\"title\":\"SLTFNet: A spatial and language-temporal tensor fusion network for video moment retrieval\",\"url\":\"https://www.semanticscholar.org/paper/bbc7306c513c42dff502dd0ce3850aab54096216\",\"venue\":\"Inf. Process. Manag.\",\"year\":2019},{\"arxivId\":\"1907.10164\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"2365530\",\"name\":\"Mingda Zhang\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"},{\"authorId\":\"144838223\",\"name\":\"Wei Li\"},{\"authorId\":\"36690046\",\"name\":\"Danfeng Qin\"},{\"authorId\":\"6367313\",\"name\":\"J. Berent\"}],\"doi\":\"10.1109/ICCV.2019.00978\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a726e5a416da3d25e1127132e11f651de80eb76\",\"title\":\"Cap2Det: Learning to Amplify Weak Caption Supervision for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/3a726e5a416da3d25e1127132e11f651de80eb76\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.07424\",\"authors\":[{\"authorId\":\"52144028\",\"name\":\"Huangyue Yu\"},{\"authorId\":\"3172280\",\"name\":\"Minjie Cai\"},{\"authorId\":\"47909479\",\"name\":\"Yunfei Liu\"},{\"authorId\":\"145053996\",\"name\":\"Feng Lu\"}],\"doi\":\"10.1145/3343031.3350896\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc6e6b1ca5b06e1b2b0ffe3ae3489473503c95f8\",\"title\":\"What I See Is What You See: Joint Attention Learning for First and Third Person Video Co-analysis\",\"url\":\"https://www.semanticscholar.org/paper/fc6e6b1ca5b06e1b2b0ffe3ae3489473503c95f8\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1903.11649\",\"authors\":[{\"authorId\":\"19200118\",\"name\":\"Samyak Datta\"},{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"145149308\",\"name\":\"A. Roy\"},{\"authorId\":\"40480894\",\"name\":\"Karuna Ahuja\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1696401\",\"name\":\"Ajay Divakaran\"}],\"doi\":\"10.1109/ICCV.2019.00269\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a89cd9056c0fb037d659215b121686ff3b454fd5\",\"title\":\"Align2Ground: Weakly Supervised Phrase Grounding Guided by Image-Caption Alignment\",\"url\":\"https://www.semanticscholar.org/paper/a89cd9056c0fb037d659215b121686ff3b454fd5\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2010.05468\",\"authors\":[{\"authorId\":\"2981509\",\"name\":\"Dongxu Li\"},{\"authorId\":\"49770180\",\"name\":\"Chenchen Xu\"},{\"authorId\":\"1490933487\",\"name\":\"Xin Yu\"},{\"authorId\":\"3397429\",\"name\":\"K. Zhang\"},{\"authorId\":\"1402604708\",\"name\":\"Ben Swift\"},{\"authorId\":\"1387047136\",\"name\":\"Hanna Suominen\"},{\"authorId\":\"71200893\",\"name\":\"H. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16091f0821502b70294ef66671183dadd1afcdc0\",\"title\":\"TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/16091f0821502b70294ef66671183dadd1afcdc0\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47748186\",\"name\":\"Wei Wei\"},{\"authorId\":\"1974207\",\"name\":\"J. Liu\"},{\"authorId\":\"2089102\",\"name\":\"X. Mao\"},{\"authorId\":\"38896551\",\"name\":\"G. Guo\"},{\"authorId\":\"143663410\",\"name\":\"F. Zhu\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"},{\"authorId\":\"2691094\",\"name\":\"Y. Hu\"}],\"doi\":\"10.1145/3357384.3357937\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3d9baf7e87ec43f0ad486e2077824a346a58118e\",\"title\":\"Emotion-aware Chat Machine: Automatic Emotional Response Generation for Human-like Emotional Interaction\",\"url\":\"https://www.semanticscholar.org/paper/3d9baf7e87ec43f0ad486e2077824a346a58118e\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":\"2010.12083\",\"authors\":[{\"authorId\":\"8083127\",\"name\":\"Simon Stepputtis\"},{\"authorId\":\"39860257\",\"name\":\"J. Campbell\"},{\"authorId\":\"2482400\",\"name\":\"Mariano Phielipp\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"},{\"authorId\":\"2207330\",\"name\":\"H. B. Amor\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7ca4abace88db259faed67686ed7bba02b46eb82\",\"title\":\"Language-Conditioned Imitation Learning for Robot Manipulation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/7ca4abace88db259faed67686ed7bba02b46eb82\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1908.05054\",\"authors\":[{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"50602231\",\"name\":\"Jeffrey Ling\"},{\"authorId\":\"123052390\",\"name\":\"Michael Collins\"},{\"authorId\":\"1781409\",\"name\":\"D. Reitter\"}],\"doi\":\"10.18653/v1/D19-1219\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b82153bf85d5d1edd3f170aace830e5328ca9ed0\",\"title\":\"Fusion of Detected Objects in Text for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b82153bf85d5d1edd3f170aace830e5328ca9ed0\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1806.05645\",\"authors\":[{\"authorId\":\"2112133\",\"name\":\"H. T. Vu\"},{\"authorId\":\"5975291\",\"name\":\"C. Greco\"},{\"authorId\":\"145095497\",\"name\":\"A. Erofeeva\"},{\"authorId\":\"51030589\",\"name\":\"Somayeh Jafaritazehjan\"},{\"authorId\":\"50816019\",\"name\":\"Guido Linders\"},{\"authorId\":\"32227979\",\"name\":\"M. Tanti\"},{\"authorId\":\"50829868\",\"name\":\"A. Testoni\"},{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1235dd37312cb20aced0e97d953f6379d8a0c7d4\",\"title\":\"Grounded Textual Entailment\",\"url\":\"https://www.semanticscholar.org/paper/1235dd37312cb20aced0e97d953f6379d8a0c7d4\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":\"2010.07526\",\"authors\":[{\"authorId\":\"3451494\",\"name\":\"Ana Marasovi\\u0107\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"39227408\",\"name\":\"Ronan Le Bras\"},{\"authorId\":\"1685669\",\"name\":\"N. A. Smith\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.253\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9940a17504a3b83bd1e9d613b095ddb204d2ad0\",\"title\":\"Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs\",\"url\":\"https://www.semanticscholar.org/paper/c9940a17504a3b83bd1e9d613b095ddb204d2ad0\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.11587\",\"authors\":[{\"authorId\":\"120157233\",\"name\":\"J. Yang\"},{\"authorId\":\"13589371\",\"name\":\"Jiayuan Mao\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2042941\",\"name\":\"D. Cox\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"617b8d0715469dfdb9e44e0f0031ac99a0f333d9\",\"title\":\"Object-Centric Diagnosis of Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/617b8d0715469dfdb9e44e0f0031ac99a0f333d9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1902.00579\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145215470\",\"name\":\"Yu Cheng\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"38079056\",\"name\":\"Jingjing Liu\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.18653/v1/P19-1648\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8d9f1eaac344b7e6cc76396f576c5dc0bd0b34f4\",\"title\":\"Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/8d9f1eaac344b7e6cc76396f576c5dc0bd0b34f4\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1423621769\",\"name\":\"B. T. Nguyen\"},{\"authorId\":\"50259366\",\"name\":\"O. Prakash\"},{\"authorId\":\"1580282435\",\"name\":\"A. H. Vo\"}],\"doi\":\"10.1007/978-3-030-62324-1_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21a742ee840b4a063deee66028409f9cf7f3829d\",\"title\":\"Attention Mechanism for Fashion Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/21a742ee840b4a063deee66028409f9cf7f3829d\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"2009.00145\",\"authors\":[{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"49659134\",\"name\":\"Zihao Zhu\"},{\"authorId\":null,\"name\":\"Yujing Wang\"},{\"authorId\":\"1735739\",\"name\":\"Weifeng Zhang\"},{\"authorId\":\"1581498565\",\"name\":\"Yue Hu\"},{\"authorId\":\"2573626\",\"name\":\"Jianlong Tan\"}],\"doi\":\"10.1016/j.patcog.2020.107563\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e526624783b3b5687da54b8cd4a7190a26a0b5e8\",\"title\":\"Cross-modal Knowledge Reasoning for Knowledge-based Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e526624783b3b5687da54b8cd4a7190a26a0b5e8\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":\"1811.08481\",\"authors\":[{\"authorId\":\"2909186\",\"name\":\"Ben Zion Vatashsky\"},{\"authorId\":\"1743045\",\"name\":\"S. Ullman\"}],\"doi\":\"10.1109/CVPR42600.2020.01039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66ccd32be901da217799c78af229676418c7a882\",\"title\":\"VQA With No Questions-Answers Training\",\"url\":\"https://www.semanticscholar.org/paper/66ccd32be901da217799c78af229676418c7a882\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2002.12114\",\"authors\":[{\"authorId\":\"46317139\",\"name\":\"Yunhan Zhao\"},{\"authorId\":\"34362536\",\"name\":\"S. Kong\"},{\"authorId\":\"2959087\",\"name\":\"Daeyun Shin\"},{\"authorId\":\"143800213\",\"name\":\"Charless C. Fowlkes\"}],\"doi\":\"10.1109/cvpr42600.2020.00339\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4eb5244b650e5f51cf77ad648cfdcabe24fc7313\",\"title\":\"Domain Decluttering: Simplifying Images to Mitigate Synthetic-Real Domain Shift and Improve Depth Estimation\",\"url\":\"https://www.semanticscholar.org/paper/4eb5244b650e5f51cf77ad648cfdcabe24fc7313\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.00387\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"1490938689\",\"name\":\"Peng Wang\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/cvpr42600.2020.00998\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b4916e339caef2d2a98e633e1f0b2144e2b0c9e2\",\"title\":\"Say As You Wish: Fine-Grained Control of Image Caption Generation With Abstract Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/b4916e339caef2d2a98e633e1f0b2144e2b0c9e2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2166593\",\"name\":\"G. Melfi\"},{\"authorId\":\"144141453\",\"name\":\"K. M\\u00fcller\"},{\"authorId\":\"49973058\",\"name\":\"T. Schwarz\"},{\"authorId\":\"3065761\",\"name\":\"Gerhard Jaworek\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1145/3313831.3376508\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"933c5c96280063dcfa74fa02b149b83af44c9994\",\"title\":\"Understanding what you feel: A Mobile Audio-Tactile System for Graphics Used at Schools with Students with Visual Impairment\",\"url\":\"https://www.semanticscholar.org/paper/933c5c96280063dcfa74fa02b149b83af44c9994\",\"venue\":\"CHI\",\"year\":2020},{\"arxivId\":\"2012.04329\",\"authors\":[{\"authorId\":\"51238351\",\"name\":\"Andr\\u00e9s Mafla\"},{\"authorId\":\"147961332\",\"name\":\"R. S. Rezende\"},{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"2295553\",\"name\":\"Diane Larlus\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23f74eff7453a31f75cd34abc04167c4b1ac9acb\",\"title\":\"StacMR: Scene-Text Aware Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/23f74eff7453a31f75cd34abc04167c4b1ac9acb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7504183\",\"name\":\"B. Zhang\"},{\"authorId\":\"50703807\",\"name\":\"L. Li\"},{\"authorId\":\"153142919\",\"name\":\"Li Su\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"1994580036\",\"name\":\"Jincan Deng\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1145/3394171.3413885\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b0209f6fa41030e7e590f2cb5568f47563a759a\",\"title\":\"Structural Semantic Adversarial Active Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b0209f6fa41030e7e590f2cb5568f47563a759a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1807.02250\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"1795294\",\"name\":\"M. Dras\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"119899233\",\"name\":\"Len Hamey\"}],\"doi\":\"10.1007/978-3-030-10925-7_14\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"036c29c2c2a2494faae205c0f270ce4d076cc3f2\",\"title\":\"Face-Cap: Image Captioning using Facial Expression Analysis\",\"url\":\"https://www.semanticscholar.org/paper/036c29c2c2a2494faae205c0f270ce4d076cc3f2\",\"venue\":\"ECML/PKDD\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"153576783\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"48693981\",\"name\":\"J. Nie\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.1109/TMM.2019.2941820\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aff890f20d28a13b9fb89d192fad35d92381c410\",\"title\":\"Multi-Level Policy and Reward-Based Deep Reinforcement Learning Framework for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aff890f20d28a13b9fb89d192fad35d92381c410\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2505751\",\"name\":\"Y. Deng\"},{\"authorId\":\"30819111\",\"name\":\"Justin T Chiu\"},{\"authorId\":\"35578711\",\"name\":\"Demi Guo\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"27a591ca871b22dfd6dd0c7d59fed69cbe6d96da\",\"title\":\"2 Background : Latent Alignment and Neural Attention\",\"url\":\"https://www.semanticscholar.org/paper/27a591ca871b22dfd6dd0c7d59fed69cbe6d96da\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2006.11714\",\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"1491357190\",\"name\":\"Yang Hua\"},{\"authorId\":\"1696052\",\"name\":\"N. Robertson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f8934f5e17a1e9d7592c641305477fe630a0fbb\",\"title\":\"Off-Policy Self-Critical Training for Transformer in Visual Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/0f8934f5e17a1e9d7592c641305477fe630a0fbb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1901.06706\",\"authors\":[{\"authorId\":\"38907975\",\"name\":\"Ning Xie\"},{\"authorId\":\"1868193\",\"name\":\"Farley Lai\"},{\"authorId\":\"2514295\",\"name\":\"Derek Doran\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3c54b796cc10cb530f77caa4d18e1c80ac863822\",\"title\":\"Visual Entailment: A Novel Task for Fine-Grained Image Understanding\",\"url\":\"https://www.semanticscholar.org/paper/3c54b796cc10cb530f77caa4d18e1c80ac863822\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121961846\",\"name\":\"Y. Wu\"},{\"authorId\":\"32435488\",\"name\":\"Huiyi Gao\"},{\"authorId\":\"47818720\",\"name\":\"L. Chen\"}],\"doi\":\"10.1117/12.2574575\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9a5a567cc381c97c73144a1c36ba0b0c27df78ec\",\"title\":\"Improving visual question answering with pre-trained language modeling\",\"url\":\"https://www.semanticscholar.org/paper/9a5a567cc381c97c73144a1c36ba0b0c27df78ec\",\"venue\":\"International Workshop on Pattern Recognition\",\"year\":2020},{\"arxivId\":\"1902.09818\",\"authors\":[{\"authorId\":\"49724488\",\"name\":\"H. Zhang\"},{\"authorId\":\"1703558\",\"name\":\"S. Ghosh\"},{\"authorId\":\"46819684\",\"name\":\"Larry Heck\"},{\"authorId\":\"145109280\",\"name\":\"S. Walsh\"},{\"authorId\":\"49051223\",\"name\":\"Junting Zhang\"},{\"authorId\":\"38791445\",\"name\":\"J. Zhang\"},{\"authorId\":\"9363144\",\"name\":\"C.-C. Jay Kuo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cb08a5d1ae9985ae3867780094c9a5e7074817d8\",\"title\":\"Generative Visual Dialogue System via Adaptive Reasoning and Weighted Likelihood Estimation\",\"url\":\"https://www.semanticscholar.org/paper/cb08a5d1ae9985ae3867780094c9a5e7074817d8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1801.09718\",\"authors\":[{\"authorId\":\"35358246\",\"name\":\"Mikyas T. Desta\"},{\"authorId\":\"2230576\",\"name\":\"Larry Chen\"},{\"authorId\":\"2725083\",\"name\":\"T. Kornuta\"}],\"doi\":\"10.1109/WACV.2018.00201\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"23d6bb8edcd86f8439072f932f414329b393473b\",\"title\":\"Object-Based Reasoning in VQA\",\"url\":\"https://www.semanticscholar.org/paper/23d6bb8edcd86f8439072f932f414329b393473b\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"1711.07280\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"12139064\",\"name\":\"Jake Bruce\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"1771913\",\"name\":\"Niko S\\u00fcnderhauf\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00387\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6bd9642470ff8c2089427f7a6392cd17d213a334\",\"title\":\"Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments\",\"url\":\"https://www.semanticscholar.org/paper/6bd9642470ff8c2089427f7a6392cd17d213a334\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2006.04315\",\"authors\":[{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1776220\",\"name\":\"Zhiwu Lu\"},{\"authorId\":\"143863244\",\"name\":\"Xian-Sheng Hua\"},{\"authorId\":\"112957699\",\"name\":\"Ji-Rong Wen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3e3f55cb25b919c4e8158195fd3ce2f23cfa7723\",\"title\":\"Counterfactual VQA: A Cause-Effect Look at Language Bias\",\"url\":\"https://www.semanticscholar.org/paper/3e3f55cb25b919c4e8158195fd3ce2f23cfa7723\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.06306\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"1382193868\",\"name\":\"Mayank Lunayach\"},{\"authorId\":\"152264213\",\"name\":\"Shivansh Patel\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":\"10.1109/ICCV.2019.00754\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c1d1d341f7d29ae078940fad184b7b182fa0cd1\",\"title\":\"U-CAM: Visual Explanation Using Uncertainty Based Class Activation Maps\",\"url\":\"https://www.semanticscholar.org/paper/5c1d1d341f7d29ae078940fad184b7b182fa0cd1\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"48210950\",\"name\":\"Jiawei Liu\"},{\"authorId\":\"49876189\",\"name\":\"T. Yang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.1145/3320061\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"86ce76f54a7bfc6047f83877408f789449f28df4\",\"title\":\"Spatiotemporal-Textual Co-Attention Network for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/86ce76f54a7bfc6047f83877408f789449f28df4\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50826496\",\"name\":\"Jiannan Fang\"},{\"authorId\":\"74213550\",\"name\":\"Lingling Sun\"},{\"authorId\":null,\"name\":\"Yaqi Wang\"}],\"doi\":\"10.1117/12.2539615\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6e077e28cadfc36eda163a01702deb795824d939\",\"title\":\"Video question answering by frame attention\",\"url\":\"https://www.semanticscholar.org/paper/6e077e28cadfc36eda163a01702deb795824d939\",\"venue\":\"International Conference on Digital Image Processing\",\"year\":2019},{\"arxivId\":\"1909.12936\",\"authors\":[{\"authorId\":\"94288059\",\"name\":\"Y. Cheng\"},{\"authorId\":\"7296648\",\"name\":\"Hongyuan Zhu\"},{\"authorId\":\"31276375\",\"name\":\"Cihan Acar\"},{\"authorId\":\"152405492\",\"name\":\"Wei Jing\"},{\"authorId\":\"48607277\",\"name\":\"Y. Wu\"},{\"authorId\":\"35718875\",\"name\":\"L. Li\"},{\"authorId\":\"1694051\",\"name\":\"Cheston Tan\"},{\"authorId\":\"9183286\",\"name\":\"Joo-Hwee Lim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d1000642158d91e28b720e152c0c1875909464a\",\"title\":\"6D Pose Estimation with Correlation Fusion\",\"url\":\"https://www.semanticscholar.org/paper/7d1000642158d91e28b720e152c0c1875909464a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.13077\",\"authors\":[{\"authorId\":\"143672098\",\"name\":\"Bei Liu\"},{\"authorId\":\"47272083\",\"name\":\"Zhicheng Huang\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"46842344\",\"name\":\"Z. Chen\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1e282859d38e97397920f66dd2df5ea6dd09f00\",\"title\":\"Learning Rich Image Region Representation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b1e282859d38e97397920f66dd2df5ea6dd09f00\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51116041\",\"name\":\"Xingxu Yao\"},{\"authorId\":\"22631231\",\"name\":\"Dongyu She\"},{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"152421153\",\"name\":\"Jie Liang\"},{\"authorId\":\"144891983\",\"name\":\"Yu-Kun Lai\"},{\"authorId\":\"1755872\",\"name\":\"Jufeng Yang\"}],\"doi\":\"10.1109/ICCV.2019.00123\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6cdae5b9bf479f37b263e96406eae36deb98db18\",\"title\":\"Attention-Aware Polarity Sensitive Embedding for Affective Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6cdae5b9bf479f37b263e96406eae36deb98db18\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1901.02527\",\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17420047ab3c64ebbb63dcf21931d26fa77955d6\",\"title\":\"Viewpoint Invariant Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/17420047ab3c64ebbb63dcf21931d26fa77955d6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71107995\",\"name\":\"Dong Zhang\"},{\"authorId\":\"51183249\",\"name\":\"Liangqing Wu\"},{\"authorId\":\"2988614\",\"name\":\"S. Li\"},{\"authorId\":\"7703092\",\"name\":\"Qiaoming Zhu\"},{\"authorId\":\"143740945\",\"name\":\"Guodong Zhou\"}],\"doi\":\"10.1109/ICME.2019.00130\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"39688ac1d78baba35a3ecc8bb12df5d59952b998\",\"title\":\"Multi-Modal Language Analysis with Hierarchical Interaction-Level and Selection-Level Attentions\",\"url\":\"https://www.semanticscholar.org/paper/39688ac1d78baba35a3ecc8bb12df5d59952b998\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1711.11543\",\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"19200118\",\"name\":\"Samyak Datta\"},{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPRW.2018.00279\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e5790afc079c6f36d6fe9235d6d253f3da631f51\",\"title\":\"Embodied Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e5790afc079c6f36d6fe9235d6d253f3da631f51\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50974488\",\"name\":\"Mingrui Lao\"},{\"authorId\":\"49813983\",\"name\":\"Yanming Guo\"},{\"authorId\":\"49528152\",\"name\":\"H. Wang\"},{\"authorId\":\"49468999\",\"name\":\"Xin Zhang\"}],\"doi\":\"10.1109/ACCESS.2018.2873570\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"161876ad06d5a349ccde4b4db3d3759ef43268a8\",\"title\":\"Multimodal Local Perception Bilinear Pooling for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/161876ad06d5a349ccde4b4db3d3759ef43268a8\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1901.06283\",\"authors\":[{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1940556\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"46387857\",\"name\":\"Chenyang Tao\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"46701859\",\"name\":\"H. Zhang\"},{\"authorId\":null,\"name\":\"Bai Li\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"2624978\",\"name\":\"C. Chen\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"717daba98eb57b898687fc013b705f763eb2916b\",\"title\":\"Improving Sequence-to-Sequence Learning via Optimal Transport\",\"url\":\"https://www.semanticscholar.org/paper/717daba98eb57b898687fc013b705f763eb2916b\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"2001.05876\",\"authors\":[{\"authorId\":\"46659203\",\"name\":\"L. Wang\"},{\"authorId\":\"1486057423\",\"name\":\"Zechen Bai\"},{\"authorId\":\"48378975\",\"name\":\"Yonghua Zhang\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"}],\"doi\":\"10.1609/aaai.v34i07.6898\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"85079d64d6fdd0ba5318fda119d152f2d2946391\",\"title\":\"Show, Recall, and Tell: Image Captioning with Recall Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/85079d64d6fdd0ba5318fda119d152f2d2946391\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2004.10796\",\"authors\":[{\"authorId\":\"4868335\",\"name\":\"J. Park\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1007/978-3-030-58558-7_30\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4aacd623a46adc6a03c925fe3ac007c271c9c6ab\",\"title\":\"VisualCOMET: Reasoning About the Dynamic Context of a Still Image\",\"url\":\"https://www.semanticscholar.org/paper/4aacd623a46adc6a03c925fe3ac007c271c9c6ab\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92057141\",\"name\":\"H. Wei\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"}],\"doi\":\"10.1007/978-3-030-37731-1_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d98f43eaf8678edbfabe50e4761c7aab550584b9\",\"title\":\"Image Captioning Based on Visual and Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/d98f43eaf8678edbfabe50e4761c7aab550584b9\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":\"2011.12662\",\"authors\":[{\"authorId\":\"47792675\",\"name\":\"Jie Ma\"},{\"authorId\":\"120809631\",\"name\":\"Jiwang Liu\"},{\"authorId\":\"49297808\",\"name\":\"Junjun Li\"},{\"authorId\":\"50320297\",\"name\":\"Qinghua Zheng\"},{\"authorId\":\"3393799\",\"name\":\"Qingyu Yin\"},{\"authorId\":\"1708169071\",\"name\":\"Jianlong Zhou\"},{\"authorId\":\"121240779\",\"name\":\"Y. Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ec666fc061c3e5276dfc1afd9b7a1ae39da69ba6\",\"title\":\"XTQA: Span-Level Explanations of the Textbook Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ec666fc061c3e5276dfc1afd9b7a1ae39da69ba6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.03289\",\"authors\":[{\"authorId\":\"6328564\",\"name\":\"M. Farazi\"},{\"authorId\":\"1931655\",\"name\":\"S. Khan\"},{\"authorId\":\"1712576\",\"name\":\"N. Barnes\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e2a185828cad6b8ea5133a9e41d1ab4b75c9f718\",\"title\":\"Question-Agnostic Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e2a185828cad6b8ea5133a9e41d1ab4b75c9f718\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"25309225\",\"name\":\"Avikalp Srivastava\"},{\"authorId\":\"153803927\",\"name\":\"H. Liu\"},{\"authorId\":\"33208854\",\"name\":\"S. Fujita\"}],\"doi\":\"10.1145/3357384.3358000\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6fb13d374dc32a55ab454fe9b8dd05527b553af9\",\"title\":\"Adapting Visual Question Answering Models for Enhancing Multimodal Community Q&A Platforms\",\"url\":\"https://www.semanticscholar.org/paper/6fb13d374dc32a55ab454fe9b8dd05527b553af9\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":\"150164718\",\"name\":\"Jialong Tang\"},{\"authorId\":null,\"name\":\"Ziyao Lu\"},{\"authorId\":\"3194601\",\"name\":\"Xianpei Han\"},{\"authorId\":\"2999092\",\"name\":\"H. Zhang\"}],\"doi\":\"10.1016/J.NEUCOM.2019.08.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2fb096118ebbe6615afe98c733c80f55bb40d2cf\",\"title\":\"A neural image captioning model with caption-to-images semantic constructor\",\"url\":\"https://www.semanticscholar.org/paper/2fb096118ebbe6615afe98c733c80f55bb40d2cf\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eefddfa610243968135726f9fddf4f69696863ed\",\"title\":\"TAB-VCR: Tags and Attributes based VCR Baselines\",\"url\":\"https://www.semanticscholar.org/paper/eefddfa610243968135726f9fddf4f69696863ed\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1902.09774\",\"authors\":[{\"authorId\":\"145422343\",\"name\":\"Dalu Guo\"},{\"authorId\":\"48258751\",\"name\":\"Chang Xu\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR.2019.01068\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5eb953608ab5eb95dd054a44980b5258fd7b8d7\",\"title\":\"Image-Question-Answer Synergistic Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/e5eb953608ab5eb95dd054a44980b5258fd7b8d7\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1476816820\",\"name\":\"Adriano Lucieri\"},{\"authorId\":\"1729241946\",\"name\":\"Huzaifa Sabir\"},{\"authorId\":\"29005173\",\"name\":\"S. Siddiqui\"},{\"authorId\":\"40795206\",\"name\":\"Syed Tahseen Raza Rizvi\"},{\"authorId\":\"2183320\",\"name\":\"Brian Kenji Iwana\"},{\"authorId\":\"48272784\",\"name\":\"S. Uchida\"},{\"authorId\":\"1384499125\",\"name\":\"Andreas Dengel\"},{\"authorId\":\"144723875\",\"name\":\"S. Ahmed\"}],\"doi\":\"10.1007/s42979-020-00132-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7fe90ec57430be821cd34b990077d00225e5cb17\",\"title\":\"Benchmarking Deep Learning Models for Classification of Book Covers\",\"url\":\"https://www.semanticscholar.org/paper/7fe90ec57430be821cd34b990077d00225e5cb17\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":\"2002.12204\",\"authors\":[{\"authorId\":\"134814700\",\"name\":\"T. Wang\"},{\"authorId\":\"50560698\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"32222907\",\"name\":\"Qianru Sun\"}],\"doi\":\"10.1109/cvpr42600.2020.01077\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d9d0197c6d9304d6bc179ac21370c49a33fc2388\",\"title\":\"Visual Commonsense R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/d9d0197c6d9304d6bc179ac21370c49a33fc2388\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1906.08430\",\"authors\":[{\"authorId\":\"35748708\",\"name\":\"Gabriel Grand\"},{\"authorId\":\"2083259\",\"name\":\"Yonatan Belinkov\"}],\"doi\":\"10.18653/v1/W19-1801\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a25800880aa6905f9cf2fb4e6aef54164f999cbd\",\"title\":\"Adversarial Regularization for Visual Question Answering: Strengths, Shortcomings, and Side Effects\",\"url\":\"https://www.semanticscholar.org/paper/a25800880aa6905f9cf2fb4e6aef54164f999cbd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1812.00733\",\"authors\":[{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"40134540\",\"name\":\"X. Liu\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1109/CVPR.2019.00925\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c4e6b18b5573329a5b62f297f6ab9565f841e11b\",\"title\":\"Attention-Based Adaptive Selection of Operations for Image Restoration in the Presence of Unknown Combined Distortions\",\"url\":\"https://www.semanticscholar.org/paper/c4e6b18b5573329a5b62f297f6ab9565f841e11b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1811.10652\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2019.00850\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8e59cf8c3becbedced0089028a1cddac8b19b251\",\"title\":\"Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions\",\"url\":\"https://www.semanticscholar.org/paper/8e59cf8c3becbedced0089028a1cddac8b19b251\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2012.07061\",\"authors\":[{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"46491945\",\"name\":\"Yunpeng Luo\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"1993645531\",\"name\":\"Gen Luo\"},{\"authorId\":\"47096329\",\"name\":\"Yongjian Wu\"},{\"authorId\":\"40366236\",\"name\":\"Yue Gao\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7217b5d8d0fb753532026cc36b0aaa056960c6f8\",\"title\":\"Improving Image Captioning by Leveraging Intra- and Inter-layer Global Representation in Transformer Network\",\"url\":\"https://www.semanticscholar.org/paper/7217b5d8d0fb753532026cc36b0aaa056960c6f8\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.12712\",\"authors\":[{\"authorId\":\"50358089\",\"name\":\"Shuguang Chen\"},{\"authorId\":\"46402847\",\"name\":\"Gustavo Aguilar\"},{\"authorId\":\"152842060\",\"name\":\"Leonardo Neves\"},{\"authorId\":\"1794626\",\"name\":\"T. Solorio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fba82586cc9a3a6a37df7447fa9727a14986e23a\",\"title\":\"A Caption Is Worth A Thousand Images: Investigating Image Captions for Multimodal Named Entity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fba82586cc9a3a6a37df7447fa9727a14986e23a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.03826\",\"authors\":[{\"authorId\":\"2218741\",\"name\":\"Fuwen Tan\"},{\"authorId\":\"1399431057\",\"name\":\"Paola Cascante-Bonilla\"},{\"authorId\":\"121433787\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"47987329\",\"name\":\"H. Wu\"},{\"authorId\":\"145480862\",\"name\":\"S. Feng\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5ec1906fbd2f34af88a987d381294e22a10c9165\",\"title\":\"Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries\",\"url\":\"https://www.semanticscholar.org/paper/5ec1906fbd2f34af88a987d381294e22a10c9165\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1908.02923\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"143899054\",\"name\":\"Mark Dras\"},{\"authorId\":\"3093086\",\"name\":\"Stephen Wan\"},{\"authorId\":\"145212976\",\"name\":\"C\\u00e9cile Paris\"}],\"doi\":\"10.1613/jair.1.12025\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"29f5da6a626c32506a1ef93300a16d1f2c812a11\",\"title\":\"Image Captioning using Facial Expression and Attention\",\"url\":\"https://www.semanticscholar.org/paper/29f5da6a626c32506a1ef93300a16d1f2c812a11\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2020},{\"arxivId\":\"2007.00900\",\"authors\":[{\"authorId\":\"46650151\",\"name\":\"Kamran Alipour\"},{\"authorId\":\"20686092\",\"name\":\"Arijit Ray\"},{\"authorId\":\"148376021\",\"name\":\"Xiao Lin\"},{\"authorId\":\"32330143\",\"name\":\"Jurgen P. Schulze\"},{\"authorId\":\"1400198856\",\"name\":\"Yi Yao\"},{\"authorId\":\"69919463\",\"name\":\"Giedrius Burachas\"}],\"doi\":\"10.1109/HCCAI49649.2020.00010\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f45cc95e7412bf8989a6c8f043d5fc69eecb910c\",\"title\":\"The Impact of Explanations on AI Competency Prediction in VQA\",\"url\":\"https://www.semanticscholar.org/paper/f45cc95e7412bf8989a6c8f043d5fc69eecb910c\",\"venue\":\"2020 IEEE International Conference on Humanized Computing and Communication with Artificial Intelligence (HCCAI)\",\"year\":2020},{\"arxivId\":\"2010.01288\",\"authors\":[{\"authorId\":\"144407296\",\"name\":\"J. Gao\"},{\"authorId\":\"1960607091\",\"name\":\"Yi Zhou\"},{\"authorId\":\"2721708\",\"name\":\"P. Yu\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8405576136062ea661febeeedbc5076235d5eb42\",\"title\":\"Unsupervised Cross-lingual Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8405576136062ea661febeeedbc5076235d5eb42\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.14744\",\"authors\":[{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2a8e5670a5ffdb72344f626ca06bb98a4a0209af\",\"title\":\"Graph Optimal Transport for Cross-Domain Alignment\",\"url\":\"https://www.semanticscholar.org/paper/2a8e5670a5ffdb72344f626ca06bb98a4a0209af\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"2004.14451\",\"authors\":[{\"authorId\":\"21771052\",\"name\":\"Allen Nie\"},{\"authorId\":\"1398108805\",\"name\":\"Reuben Cohn-Gordon\"},{\"authorId\":\"144922861\",\"name\":\"Christopher Potts\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.173\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"88c86523d500d636f453647385ddaa04085b5f1b\",\"title\":\"Pragmatic Issue-Sensitive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/88c86523d500d636f453647385ddaa04085b5f1b\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1804.03999\",\"authors\":[{\"authorId\":\"2941969\",\"name\":\"O. Oktay\"},{\"authorId\":\"9952952\",\"name\":\"Jo Schlemper\"},{\"authorId\":\"2411551\",\"name\":\"Lo\\u00efc Le Folgoc\"},{\"authorId\":\"47804190\",\"name\":\"M. J. Lee\"},{\"authorId\":\"1825371\",\"name\":\"M. Heinrich\"},{\"authorId\":\"38989230\",\"name\":\"K. Misawa\"},{\"authorId\":\"144070506\",\"name\":\"K. Mori\"},{\"authorId\":\"40513205\",\"name\":\"Steven G. McDonagh\"},{\"authorId\":\"3128867\",\"name\":\"N. Hammerla\"},{\"authorId\":\"2015193\",\"name\":\"Bernhard Kainz\"},{\"authorId\":\"1709824\",\"name\":\"Ben Glocker\"},{\"authorId\":\"1717710\",\"name\":\"D. Rueckert\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ae1c89817a3a239e5344293138bdd80293983460\",\"title\":\"Attention U-Net: Learning Where to Look for the Pancreas\",\"url\":\"https://www.semanticscholar.org/paper/ae1c89817a3a239e5344293138bdd80293983460\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152899351\",\"name\":\"X. Zhang\"},{\"authorId\":\"40485842\",\"name\":\"X. Sun\"},{\"authorId\":\"20637018\",\"name\":\"Chunjie Xie\"},{\"authorId\":\"1381365135\",\"name\":\"Bing Lun\"}],\"doi\":\"10.1109/ACCESS.2019.2933370\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"003ff75e4dbca1f2f87432399251c9d1d2a316c2\",\"title\":\"From Vision to Content: Construction of Domain-Specific Multi-Modal Knowledge Graph\",\"url\":\"https://www.semanticscholar.org/paper/003ff75e4dbca1f2f87432399251c9d1d2a316c2\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"2006.05121\",\"authors\":[{\"authorId\":\"51149482\",\"name\":\"Corentin Kervadec\"},{\"authorId\":\"145664204\",\"name\":\"G. Antipov\"},{\"authorId\":\"2341854\",\"name\":\"M. Baccouche\"},{\"authorId\":\"144899680\",\"name\":\"C. Wolf\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"95cb3e6b628a3a0220b8c80fde4f9f4a3d2e7221\",\"title\":\"Roses Are Red, Violets Are Blue... but Should Vqa Expect Them To?\",\"url\":\"https://www.semanticscholar.org/paper/95cb3e6b628a3a0220b8c80fde4f9f4a3d2e7221\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.00975\",\"authors\":[{\"authorId\":\"24799325\",\"name\":\"Zhizhe Liu\"},{\"authorId\":\"47958013\",\"name\":\"Xingxing Zhang\"},{\"authorId\":\"1749780\",\"name\":\"Zhenfeng Zhu\"},{\"authorId\":\"144147900\",\"name\":\"Shuai Zheng\"},{\"authorId\":\"1517826311\",\"name\":\"Yao Zhao\"},{\"authorId\":\"48265260\",\"name\":\"Jian Cheng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bf20e8dc79df9fd6e395e9b6ae1b960ff4519e1\",\"title\":\"Taking Modality-free Human Identification as Zero-shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/6bf20e8dc79df9fd6e395e9b6ae1b960ff4519e1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1997954506\",\"name\":\"Yevhen Romaniak\"},{\"authorId\":\"1997944494\",\"name\":\"Anastasiia Smielova\"},{\"authorId\":\"1380224734\",\"name\":\"Yevhenii Yakishyn\"},{\"authorId\":\"1380224706\",\"name\":\"Valerii Dziubliuk\"},{\"authorId\":\"1380224713\",\"name\":\"Mykhailo Zlotnyk\"},{\"authorId\":\"1380224917\",\"name\":\"Oleksandr Viatchaninov\"}],\"doi\":\"10.1145/3379350.3416153\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7444f34dd62067823dcf58b1ffdf03a7047e1a96\",\"title\":\"Nimble: Mobile Interface for a Visual Question Answering Augmented by Gestures\",\"url\":\"https://www.semanticscholar.org/paper/7444f34dd62067823dcf58b1ffdf03a7047e1a96\",\"venue\":\"UIST\",\"year\":2020},{\"arxivId\":\"2004.14025\",\"authors\":[{\"authorId\":\"79778234\",\"name\":\"Sungjin Park\"},{\"authorId\":\"89016637\",\"name\":\"T. Whang\"},{\"authorId\":\"3037023\",\"name\":\"Y. Yoon\"},{\"authorId\":\"1450703435\",\"name\":\"Hueiseok Lim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6a7c8371f6a0646a1ce04bc0c42f56ff1438f9ab\",\"title\":\"Multi-View Attention Networks for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/6a7c8371f6a0646a1ce04bc0c42f56ff1438f9ab\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.05099\",\"authors\":[{\"authorId\":\"143809690\",\"name\":\"X. Ma\"},{\"authorId\":\"151481034\",\"name\":\"Jingda Guo\"},{\"authorId\":\"47832015\",\"name\":\"Sihai Tang\"},{\"authorId\":\"1808399758\",\"name\":\"Zhinan Qiao\"},{\"authorId\":\"145123674\",\"name\":\"Q. Chen\"},{\"authorId\":\"1719352\",\"name\":\"Qing Yang\"},{\"authorId\":\"153052604\",\"name\":\"S. Fu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"52070f9d2b56330fc20bccd30f2dc168148fce25\",\"title\":\"DCANet: Learning Connected Attentions for Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/52070f9d2b56330fc20bccd30f2dc168148fce25\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.09610\",\"authors\":[{\"authorId\":\"50086111\",\"name\":\"K. Niu\"},{\"authorId\":\"48356084\",\"name\":\"Y. Huang\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":null,\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/TIP.2020.2984883\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7f2512bea65bcc298e8258f8aaccb13dbf59f2d9\",\"title\":\"Improving Description-Based Person Re-Identification by Multi-Granularity Image-Text Alignments\",\"url\":\"https://www.semanticscholar.org/paper/7f2512bea65bcc298e8258f8aaccb13dbf59f2d9\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2008.09884\",\"authors\":[{\"authorId\":\"46245587\",\"name\":\"Geeticka Chauhan\"},{\"authorId\":\"1742480112\",\"name\":\"Ruizhi Liao\"},{\"authorId\":\"153671052\",\"name\":\"W. Wells\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"35140763\",\"name\":\"S. Berkowitz\"},{\"authorId\":\"2986398\",\"name\":\"S. Horng\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"},{\"authorId\":\"1729630\",\"name\":\"P. Golland\"}],\"doi\":\"10.1007/978-3-030-59713-9_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aac5ce0dfd22f8d713e74b607ce849f632260023\",\"title\":\"Joint Modeling of Chest Radiographs and Radiology Reports for Pulmonary Edema Assessment\",\"url\":\"https://www.semanticscholar.org/paper/aac5ce0dfd22f8d713e74b607ce849f632260023\",\"venue\":\"MICCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"8799308\",\"name\":\"U. Nadeem\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1007/978-3-030-49724-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"title\":\"Vision to Language: Methods, Metrics and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.07162\",\"authors\":[{\"authorId\":\"7356928\",\"name\":\"T. Zhu\"},{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"47893252\",\"name\":\"Haoran Li\"},{\"authorId\":\"1768799\",\"name\":\"Youzheng Wu\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"145218984\",\"name\":\"Bowen Zhou\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.166\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97679b9616045bfa3723db15f360cf9f2c8b52ad\",\"title\":\"Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product\",\"url\":\"https://www.semanticscholar.org/paper/97679b9616045bfa3723db15f360cf9f2c8b52ad\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1905.04405\",\"authors\":[{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2019.01039\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2dc698077cb178286c737484dcf67c5ab19314d0\",\"title\":\"Language-Conditioned Graph Networks for Relational Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/2dc698077cb178286c737484dcf67c5ab19314d0\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"3374337\",\"name\":\"Guoliang Kang\"},{\"authorId\":\"3446043\",\"name\":\"Wenhe Liu\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3343031.3350894\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b8fd269ab69a974eaf057c0e4bcaf0be2b8ee55f\",\"title\":\"Annotation Efficient Cross-Modal Retrieval with Adversarial Attentive Alignment\",\"url\":\"https://www.semanticscholar.org/paper/b8fd269ab69a974eaf057c0e4bcaf0be2b8ee55f\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37226164\",\"name\":\"Yash Goyal\"},{\"authorId\":\"7595427\",\"name\":\"Tejas Khot\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1007/s11263-018-1116-0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e\",\"title\":\"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7e232313a59d735ef7c8a9f4cc7bc980a29deb5e\",\"venue\":\"International Journal of Computer Vision\",\"year\":2018},{\"arxivId\":\"1904.02865\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2019.00204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51932dc1148566040fdb0df6ed66d8d2a0712933\",\"title\":\"Actively Seeking and Learning From Live Data\",\"url\":\"https://www.semanticscholar.org/paper/51932dc1148566040fdb0df6ed66d8d2a0712933\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3055959\",\"name\":\"Veena Thenkanidiyoor\"},{\"authorId\":\"47798961\",\"name\":\"R. Prasath\"},{\"authorId\":\"150255310\",\"name\":\"Odelu Vanga\"},{\"authorId\":\"145960032\",\"name\":\"R. Goebel\"},{\"authorId\":\"144865865\",\"name\":\"Y. Tanaka\"}],\"doi\":\"10.1007/978-3-030-66187-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b326e950ca86368056d2846444c25c61fedbc111\",\"title\":\"Mining Intelligence and Knowledge Exploration: 7th International Conference, MIKE 2019, Goa, India, December 19\\u201322, 2019, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/b326e950ca86368056d2846444c25c61fedbc111\",\"venue\":\"MIKE\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35137594\",\"name\":\"Jinpeng Mi\"},{\"authorId\":\"1939598\",\"name\":\"Jian-Zhi Lyu\"},{\"authorId\":\"1739175813\",\"name\":\"Song Tang\"},{\"authorId\":\"153082688\",\"name\":\"Qingdu Li\"},{\"authorId\":\"50561627\",\"name\":\"J. Zhang\"}],\"doi\":\"10.3389/fnbot.2020.00043\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ceb666a47c683b0555f8976e7d2a0f15f9cf8be2\",\"title\":\"Interactive Natural Language Grounding via Referring Expression Comprehension and Scene Graph Parsing\",\"url\":\"https://www.semanticscholar.org/paper/ceb666a47c683b0555f8976e7d2a0f15f9cf8be2\",\"venue\":\"Frontiers in Neurorobotics\",\"year\":2020},{\"arxivId\":\"1809.04344\",\"authors\":[{\"authorId\":\"51228129\",\"name\":\"Shailza Jolly\"},{\"authorId\":\"3422247\",\"name\":\"Sandro Pezzelle\"},{\"authorId\":\"35660331\",\"name\":\"T. Klein\"},{\"authorId\":\"145279674\",\"name\":\"A. Dengel\"},{\"authorId\":\"1848946\",\"name\":\"Moin Nabi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"610e0bee525a6573932e077f091505f54a5c4ede\",\"title\":\"The Wisdom of MaSSeS: Majority, Subjectivity, and Semantic Similarity in the Evaluation of VQA\",\"url\":\"https://www.semanticscholar.org/paper/610e0bee525a6573932e077f091505f54a5c4ede\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1904.11251\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2019.01278\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"title\":\"Pointing Novel Objects in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6dc67482ee0530e9ff535775891481ed9fd5f6ad\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1907.05006\",\"authors\":[{\"authorId\":\"150065958\",\"name\":\"Chiwan Song\"},{\"authorId\":\"40506942\",\"name\":\"Woobin Im\"},{\"authorId\":\"144182454\",\"name\":\"Sung-eui Yoon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b48cf57d41e81f201a756c6b280fa2ebfe52f9d3\",\"title\":\"Two-stream Spatiotemporal Feature for Video QA Task\",\"url\":\"https://www.semanticscholar.org/paper/b48cf57d41e81f201a756c6b280fa2ebfe52f9d3\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.04289\",\"authors\":[{\"authorId\":\"144740494\",\"name\":\"Peng Gao\"},{\"authorId\":\"30156979\",\"name\":\"Haoxuan You\"},{\"authorId\":\"3152448\",\"name\":\"Zhanpeng Zhang\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1109/ICCV.2019.00592\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cf5a0d3b67cf03c2441f7aa20f0ea499bd02acf6\",\"title\":\"Multi-Modality Latent Interaction Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/cf5a0d3b67cf03c2441f7aa20f0ea499bd02acf6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1909.05663\",\"authors\":[{\"authorId\":\"145116184\",\"name\":\"I. Gallo\"},{\"authorId\":\"144669071\",\"name\":\"Shah Nawaz\"},{\"authorId\":\"3457883\",\"name\":\"Alessandro Calefati\"},{\"authorId\":\"51475116\",\"name\":\"R. L. Grassa\"},{\"authorId\":\"1387994112\",\"name\":\"Nicola Landro\"}],\"doi\":\"10.1109/DICTA47822.2019.8946033\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7334f6af3080b3ec7145c7f6f2e54cd75f87c905\",\"title\":\"Picture What You Read\",\"url\":\"https://www.semanticscholar.org/paper/7334f6af3080b3ec7145c7f6f2e54cd75f87c905\",\"venue\":\"2019 Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"51055350\",\"name\":\"Shuwen Xiao\"},{\"authorId\":\"1478185914\",\"name\":\"Zehan Song\"},{\"authorId\":\"153228843\",\"name\":\"Chujie Lu\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1109/TIP.2020.2963950\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b5de122d508518ecaae7c9e4cc627c36c96f2a9\",\"title\":\"Open-Ended Video Question Answering via Multi-Modal Conditional Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/5b5de122d508518ecaae7c9e4cc627c36c96f2a9\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1952857\",\"name\":\"K. Zheng\"},{\"authorId\":\"144469723\",\"name\":\"C. Zhu\"},{\"authorId\":\"2478555\",\"name\":\"Shaopeng Lu\"},{\"authorId\":\"40457369\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/978-3-030-00776-8_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9808d89c32f661d9fd797a97cc7c3ff2ed2cd260\",\"title\":\"Multiple-Level Feature-Based Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9808d89c32f661d9fd797a97cc7c3ff2ed2cd260\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1999410\",\"name\":\"Zhibin Hu\"},{\"authorId\":\"40132308\",\"name\":\"Yongsheng Luo\"},{\"authorId\":\"46698321\",\"name\":\"Jiong Lin\"},{\"authorId\":\"144761066\",\"name\":\"Yan Yan\"},{\"authorId\":\"5869774\",\"name\":\"J. Chen\"}],\"doi\":\"10.24963/ijcai.2019/111\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"10012e6a7a0ad10391533326b95bd1291df6f199\",\"title\":\"Multi-Level Visual-Semantic Alignments with Relation-Wise Dual Attention Network for Image and Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/10012e6a7a0ad10391533326b95bd1291df6f199\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2003.10606\",\"authors\":[{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/CVPR42600.2020.01043\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70ea5d98141cd845d1c8131f25ddbc77f962f3d8\",\"title\":\"Video Object Grounding Using Semantic Roles in Language Description\",\"url\":\"https://www.semanticscholar.org/paper/70ea5d98141cd845d1c8131f25ddbc77f962f3d8\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2005.01678\",\"authors\":[{\"authorId\":\"31686229\",\"name\":\"Noriyuki Kojima\"},{\"authorId\":\"1388323535\",\"name\":\"Hadar Averbuch-Elor\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"},{\"authorId\":\"3167681\",\"name\":\"Yoav Artzi\"}],\"doi\":\"10.18653/v1/2020.acl-main.234\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ff80f56fe0977836fdb232a058fbebc1c2d5bbac\",\"title\":\"What is Learned in Visually Grounded Neural Syntax Acquisition\",\"url\":\"https://www.semanticscholar.org/paper/ff80f56fe0977836fdb232a058fbebc1c2d5bbac\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32791167\",\"name\":\"Chenchen Jing\"},{\"authorId\":\"150352923\",\"name\":\"Yuwei Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1609/AAAI.V34I07.6776\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5ac43583eedebd895fcfd45e9670a542c4c4070f\",\"title\":\"Overcoming Language Priors in VQA via Decomposed Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/5ac43583eedebd895fcfd45e9670a542c4c4070f\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2003.12511\",\"authors\":[{\"authorId\":\"3428457\",\"name\":\"Tai-Yin Chiu\"},{\"authorId\":\"31812669\",\"name\":\"Y. Zhao\"},{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"}],\"doi\":\"10.1109/cvpr42600.2020.00370\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"795435da0de2cb9772e8ebec9a4242de7e677b30\",\"title\":\"Assessing Image Quality Issues for Real-World Problems\",\"url\":\"https://www.semanticscholar.org/paper/795435da0de2cb9772e8ebec9a4242de7e677b30\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48303991\",\"name\":\"Kanika Kalra\"},{\"authorId\":\"1754532926\",\"name\":\"Bhargav Kurma\"},{\"authorId\":\"1754580394\",\"name\":\"Silpa Vadakkeeveetil Sreelatha\"},{\"authorId\":\"34326205\",\"name\":\"Manasi S. Patwardhan\"},{\"authorId\":\"40151143\",\"name\":\"S. Karande\"}],\"doi\":\"10.18653/v1/2020.acl-main.674\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c194e971c89aedbf74ddc28e023d606b5ed657ff\",\"title\":\"Understanding Advertisements with BERT\",\"url\":\"https://www.semanticscholar.org/paper/c194e971c89aedbf74ddc28e023d606b5ed657ff\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9119184\",\"name\":\"Huazhong Jin\"},{\"authorId\":\"48608098\",\"name\":\"Yuxia Wu\"},{\"authorId\":\"1510781142\",\"name\":\"Fang Wan\"},{\"authorId\":\"1510781771\",\"name\":\"M. Hu\"},{\"authorId\":\"40201664\",\"name\":\"Q. Li\"}],\"doi\":\"10.1117/12.2539338\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf4a051a57b7bd22ab0f416f162fc72d5640d17b\",\"title\":\"Image caption generation method based on adaptive attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/cf4a051a57b7bd22ab0f416f162fc72d5640d17b\",\"venue\":\"International Symposium on Multispectral Image Processing and Pattern Recognition\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897228\",\"name\":\"Jie Shao\"},{\"authorId\":\"48569045\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/LSP.2020.2977498\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5c39af3eefc61c44255f09f8181579e5d6a15f8\",\"title\":\"Generalized Zero-Shot Learning With Multi-Channel Gaussian Mixture VAE\",\"url\":\"https://www.semanticscholar.org/paper/e5c39af3eefc61c44255f09f8181579e5d6a15f8\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2020},{\"arxivId\":\"2004.05573\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"47824843\",\"name\":\"W. Wang\"},{\"authorId\":\"1630359492\",\"name\":\"Ludan Ruan\"},{\"authorId\":\"49539732\",\"name\":\"Linli Yao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78605f537f6c4a8c5206bdd4f57cea22d9750703\",\"title\":\"YouMakeup VQA Challenge: Towards Fine-grained Action Understanding in Domain-Specific Videos\",\"url\":\"https://www.semanticscholar.org/paper/78605f537f6c4a8c5206bdd4f57cea22d9750703\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51126032\",\"name\":\"Zhuoqian Yang\"},{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"119883554\",\"name\":\"J. Yu\"},{\"authorId\":null,\"name\":\"Zengchang Qin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"490a9ee7c995140136d2c5054081c08429ebc171\",\"title\":\"Scene Graph Reasoning with Prior Visual Relationship for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/490a9ee7c995140136d2c5054081c08429ebc171\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47859297\",\"name\":\"D. Wang\"},{\"authorId\":\"143984297\",\"name\":\"Daniel Beck\"},{\"authorId\":\"143620680\",\"name\":\"Trevor Cohn\"}],\"doi\":\"10.18653/v1/D19-6405\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"93447c939657029b6305053599f51e78ba8a4c3d\",\"title\":\"On the Role of Scene Graphs in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93447c939657029b6305053599f51e78ba8a4c3d\",\"venue\":\"LANTERN@EMNLP-IJCNLP\",\"year\":2019},{\"arxivId\":\"2010.11701\",\"authors\":[{\"authorId\":\"84039136\",\"name\":\"P. Sadler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b60d6e35891baa34282433546df0449d7422ce98\",\"title\":\"Spatial Attention as an Interface for Image Captioning Models\",\"url\":\"https://www.semanticscholar.org/paper/b60d6e35891baa34282433546df0449d7422ce98\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"84742701\",\"name\":\"R. Bai\"},{\"authorId\":\"72682778\",\"name\":\"Zhongqing Wang\"},{\"authorId\":\"1500530440\",\"name\":\"Fang Kong\"},{\"authorId\":\"2988614\",\"name\":\"S. Li\"},{\"authorId\":\"143740949\",\"name\":\"G. Zhou\"}],\"doi\":\"10.1145/3394113\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"377fa7874ec24e229309c3643a7e44fa1d26ea2b\",\"title\":\"Neural Co-training for Sentiment Classification with Product Attributes\",\"url\":\"https://www.semanticscholar.org/paper/377fa7874ec24e229309c3643a7e44fa1d26ea2b\",\"venue\":\"ACM Trans. Asian Low Resour. Lang. Inf. Process.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23865980\",\"name\":\"R. Gupta\"},{\"authorId\":\"1753738514\",\"name\":\"Parikshit Hooda\"},{\"authorId\":\"1753737639\",\"name\":\"Sanjeev\"},{\"authorId\":\"1753738106\",\"name\":\"Nikhil Kumar Chikkara\"}],\"doi\":\"10.1109/ICICCS48265.2020.9121068\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6788a018647840cfac2c9af608c5a1a7903d8778\",\"title\":\"Natural Language Processing based Visual Question Answering Efficient: an EfficientDet Approach\",\"url\":\"https://www.semanticscholar.org/paper/6788a018647840cfac2c9af608c5a1a7903d8778\",\"venue\":\"2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS)\",\"year\":2020},{\"arxivId\":\"1909.11059\",\"authors\":[{\"authorId\":\"48206987\",\"name\":\"L. Zhou\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1609/AAAI.V34I07.7005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"title\":\"Unified Vision-Language Pre-Training for Image Captioning and VQA\",\"url\":\"https://www.semanticscholar.org/paper/088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3381668\",\"name\":\"Jinwon An\"},{\"authorId\":\"97642229\",\"name\":\"S. Cho\"}],\"doi\":\"10.1109/ACCESS.2020.3035463\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e6fa446ebd0540b70727f935cda245b42c4d64fd\",\"title\":\"Hierarchical Transformer Encoder With Structured Representation for Abstract Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/e6fa446ebd0540b70727f935cda245b42c4d64fd\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51126032\",\"name\":\"Zhuoqian Yang\"},{\"authorId\":\"31055300\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"1482482570\",\"name\":\"Jing Yu\"},{\"authorId\":\"153482287\",\"name\":\"Tao Wan\"}],\"doi\":\"10.1109/ICIP40778.2020.9190771\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ce990e1eb9879f71c4af25381d7a5949e538c019\",\"title\":\"Prior Visual Relationship Reasoning For Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ce990e1eb9879f71c4af25381d7a5949e538c019\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eli Pugh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2864ea38ac22ebeddd8044c09a5a750ac2c7f36a\",\"title\":\"A Meta Learning Approach to Novel Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2864ea38ac22ebeddd8044c09a5a750ac2c7f36a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.08110\",\"authors\":[{\"authorId\":\"47904580\",\"name\":\"Yiyu Wang\"},{\"authorId\":\"2073589\",\"name\":\"Jungang Xu\"},{\"authorId\":\"46676156\",\"name\":\"Yingfei Sun\"},{\"authorId\":\"40368776\",\"name\":\"Ben He\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ef4a09eb86e150b08a80d59c2c092d1a56be780\",\"title\":\"Image Captioning based on Deep Learning Methods: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/8ef4a09eb86e150b08a80d59c2c092d1a56be780\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1783064\",\"name\":\"A. Bouju\"},{\"authorId\":\"1732746\",\"name\":\"Micka\\u00ebl Coustaty\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c69e2c5227478527cfab3c6b9eda86bdb2894154\",\"title\":\"Ontology-based management of ancient \\\"lettrines\\\"\",\"url\":\"https://www.semanticscholar.org/paper/c69e2c5227478527cfab3c6b9eda86bdb2894154\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1804.02088\",\"authors\":[{\"authorId\":\"145659406\",\"name\":\"Y. Shi\"},{\"authorId\":\"2426872\",\"name\":\"T. Furlanello\"},{\"authorId\":\"40881843\",\"name\":\"Sheng Zha\"},{\"authorId\":\"2047844\",\"name\":\"Anima Anandkumar\"}],\"doi\":\"10.1007/978-3-030-01225-0_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"80fc9efde5bb28550d17363d882fd5bc6d805c26\",\"title\":\"Question Type Guided Attention in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/80fc9efde5bb28550d17363d882fd5bc6d805c26\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37310105\",\"name\":\"L. Zhao\"},{\"authorId\":\"47423527\",\"name\":\"Chunxia Zhang\"},{\"authorId\":null,\"name\":\"Xi Zhang\"},{\"authorId\":\"3034224\",\"name\":\"Yating Hu\"},{\"authorId\":\"8253080\",\"name\":\"Z. Niu\"}],\"doi\":\"10.1007/978-3-319-97304-3_67\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f472524ad1a5989aa1d66634a477041cd6ec9b64\",\"title\":\"A Deep Reinforced Training Method for Location-Based Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f472524ad1a5989aa1d66634a477041cd6ec9b64\",\"venue\":\"PRICAI\",\"year\":2018},{\"arxivId\":\"2011.13406\",\"authors\":[{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"97671685\",\"name\":\"H. Wu\"},{\"authorId\":\"51135899\",\"name\":\"Yi Ren Fung\"},{\"authorId\":\"153172090\",\"name\":\"Huai-zhong Ji\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1904c5389a70a905019d5429f09bc7f669bdc898\",\"title\":\"Learning from Lexical Perturbations for Consistent Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1904c5389a70a905019d5429f09bc7f669bdc898\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.10132\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s11042-020-09865-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f\",\"title\":\"CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation - A Generalization of Bi Directional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2008.08290\",\"authors\":[{\"authorId\":\"50232214\",\"name\":\"Wenjia Xu\"},{\"authorId\":\"3370667\",\"name\":\"Yongqin Xian\"},{\"authorId\":\"51175635\",\"name\":\"Jiuniu Wang\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f76025280f8e597e31d8dcad22301af9f2e6de7a\",\"title\":\"Attribute Prototype Network for Zero-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/f76025280f8e597e31d8dcad22301af9f2e6de7a\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2006.08315\",\"authors\":[{\"authorId\":\"151195783\",\"name\":\"Ruixiang Tang\"},{\"authorId\":\"3432460\",\"name\":\"Mengnan Du\"},{\"authorId\":\"48513905\",\"name\":\"Yuening Li\"},{\"authorId\":\"47781070\",\"name\":\"Zirui Liu\"},{\"authorId\":\"1490483806\",\"name\":\"X. Hu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b239fe63de5de0a1e0c08af97a30f79b5a2bdbc\",\"title\":\"Mitigating Gender Bias in Captioning Systems\",\"url\":\"https://www.semanticscholar.org/paper/0b239fe63de5de0a1e0c08af97a30f79b5a2bdbc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.03339\",\"authors\":[{\"authorId\":\"6937593\",\"name\":\"Shih-Han Chou\"},{\"authorId\":\"150336480\",\"name\":\"Wei-Lun Chao\"},{\"authorId\":\"2268189\",\"name\":\"Wei-Sheng Lai\"},{\"authorId\":\"30885811\",\"name\":\"Min Sun\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8d27640ce75557156de13fb827b64446ef9cc0e4\",\"title\":\"Visual Question Answering on 360{\\\\deg} Images.\",\"url\":\"https://www.semanticscholar.org/paper/8d27640ce75557156de13fb827b64446ef9cc0e4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.05175\",\"authors\":[{\"authorId\":\"37619618\",\"name\":\"Khyathi Raghavi Chandu\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"87784755\",\"name\":\"A. Thapliyal\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d325b31c8ccffea52c25e8586bdb9d4dde26151\",\"title\":\"Weakly Supervised Content Selection for Improved Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9d325b31c8ccffea52c25e8586bdb9d4dde26151\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1808.10544\",\"authors\":[{\"authorId\":\"2876552\",\"name\":\"Changqing Zou\"},{\"authorId\":\"51208710\",\"name\":\"Haoran Mo\"},{\"authorId\":\"36485086\",\"name\":\"Ruofei Du\"},{\"authorId\":\"144534218\",\"name\":\"Xing Wu\"},{\"authorId\":\"2971945\",\"name\":\"Chengying Gao\"},{\"authorId\":\"3169698\",\"name\":\"Hongbo Fu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6d143d81596db42ef7eb4f9a687f8a589a95dae\",\"title\":\"LUCSS: Language-based User-customized Colourization of Scene Sketches\",\"url\":\"https://www.semanticscholar.org/paper/d6d143d81596db42ef7eb4f9a687f8a589a95dae\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1774780\",\"name\":\"Yilin Shen\"},{\"authorId\":\"1705713\",\"name\":\"Hongxia Jin\"},{\"authorId\":\"46848045\",\"name\":\"S. Ghosh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"080a2805853cc0e5a8cba05af1fb7a65ace8f6d7\",\"title\":\"LANGUAGE MODELS MORE GROUNDED\",\"url\":\"https://www.semanticscholar.org/paper/080a2805853cc0e5a8cba05af1fb7a65ace8f6d7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2003.10065\",\"authors\":[{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"3170394\",\"name\":\"Keze Wang\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4e90d80721fc24ac40f4711f8692aba08dc14e0\",\"title\":\"Linguistically Driven Graph Capsule Network for Visual Question Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/e4e90d80721fc24ac40f4711f8692aba08dc14e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"VGG\"},{\"authorId\":null,\"name\":\"RCNN\"},{\"authorId\":null,\"name\":\"LSTM\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"adf9243e6166df170fa1653dafe2d952ad1bfe9f\",\"title\":\"Dual Visual Attention Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/adf9243e6166df170fa1653dafe2d952ad1bfe9f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1576818877\",\"name\":\"Kento Terao\"},{\"authorId\":\"134811419\",\"name\":\"Toru Tamaki\"},{\"authorId\":\"1688940\",\"name\":\"B. Raytchev\"},{\"authorId\":\"1686272\",\"name\":\"K. Kaneda\"},{\"authorId\":\"144404414\",\"name\":\"S. Satoh\"}],\"doi\":\"10.1109/ACCESS.2020.3022063\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c7583d58aec662313bd5b0082b656d9f44956dc3\",\"title\":\"An Entropy Clustering Approach for Assessing Visual Question Difficulty\",\"url\":\"https://www.semanticscholar.org/paper/c7583d58aec662313bd5b0082b656d9f44956dc3\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100818163\",\"name\":\"Chen-chen Jing\"},{\"authorId\":\"145558278\",\"name\":\"Y. Wu\"},{\"authorId\":\"144315453\",\"name\":\"Mingtao Pei\"},{\"authorId\":\"46972595\",\"name\":\"Yao Hu\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"1720738301\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1145/3394171.3413902\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc24a5a8d6629eec5c8d87d48ab891bc17e862ff\",\"title\":\"Visual-Semantic Graph Matching for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/cc24a5a8d6629eec5c8d87d48ab891bc17e862ff\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993611266\",\"name\":\"Guohao Li\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3394171.3413943\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22\",\"title\":\"Boosting Visual Question Answering with Context-aware Knowledge Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41052836\",\"name\":\"Anya Belz\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"}],\"doi\":\"10.1017/S1351324918000086\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d740802aa22dbc187fe5df39108ba493b34d2839\",\"title\":\"From image to language and back again\",\"url\":\"https://www.semanticscholar.org/paper/d740802aa22dbc187fe5df39108ba493b34d2839\",\"venue\":\"Nat. Lang. Eng.\",\"year\":2018},{\"arxivId\":\"1808.01571\",\"authors\":[{\"authorId\":\"1679279\",\"name\":\"Dapeng Chen\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"},{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"2371221\",\"name\":\"Y. Shen\"},{\"authorId\":\"33762094\",\"name\":\"Zejian Yuan\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-01270-0_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a808a17f5c86413bd552a324ee6ba180a12f46d\",\"title\":\"Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association\",\"url\":\"https://www.semanticscholar.org/paper/0a808a17f5c86413bd552a324ee6ba180a12f46d\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"title\":\"Image Input OR Video Hierarchical LSTMs with Adaptive Attention ( hLSTMat ) Feature Extraction Generated Captions Losses\",\"url\":\"https://www.semanticscholar.org/paper/e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1907.11117\",\"authors\":[{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b73ff5846772da8575262925aa7709b5e64079a0\",\"title\":\"Learning Visual Actions Using Multiple Verb-Only Labels\",\"url\":\"https://www.semanticscholar.org/paper/b73ff5846772da8575262925aa7709b5e64079a0\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1904.04166\",\"authors\":[{\"authorId\":\"1887625\",\"name\":\"Yuehua Wu\"},{\"authorId\":\"39978626\",\"name\":\"Lu Jiang\"},{\"authorId\":\"143699996\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/TIP.2020.2967584\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"866908141e1db5d6b278984072303a0e14423bcc\",\"title\":\"Revisiting EmbodiedQA: A Simple Baseline and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/866908141e1db5d6b278984072303a0e14423bcc\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"90745780\",\"name\":\"Thomas Scialom\"},{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"1502219404\",\"name\":\"Paul-Alexis Dray\"},{\"authorId\":\"1767493\",\"name\":\"Jacopo Staiano\"},{\"authorId\":\"67192547\",\"name\":\"P. Gallinari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b32d02d7f6fa954e7ae500f94f6a5efd113751a4\",\"title\":\"BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations\",\"url\":\"https://www.semanticscholar.org/paper/b32d02d7f6fa954e7ae500f94f6a5efd113751a4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30670181\",\"name\":\"Thilini Cooray\"},{\"authorId\":\"143770929\",\"name\":\"N. Cheung\"},{\"authorId\":\"153022029\",\"name\":\"W. Lu\"}],\"doi\":\"10.1109/cvpr42600.2020.00479\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb8aea99913099a3496dcbc8af49d7f99edf77d2\",\"title\":\"Attention-Based Context Aware Reasoning for Situation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb8aea99913099a3496dcbc8af49d7f99edf77d2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1109/CVPR42600.2020.01390\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"954d4027839ff8d3aacc8c963ef2a3be69c8a1f9\",\"title\":\"Hierarchical Graph Attention Network for Visual Relationship Detection\",\"url\":\"https://www.semanticscholar.org/paper/954d4027839ff8d3aacc8c963ef2a3be69c8a1f9\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144963976\",\"name\":\"Weidong Tian\"},{\"authorId\":\"145380213\",\"name\":\"B. He\"},{\"authorId\":\"1977865899\",\"name\":\"Nan-Xun Wang\"},{\"authorId\":\"33698309\",\"name\":\"Zhong-Qiu Zhao\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207058\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"99a01a687f7959de4c86102342d4bcfec6382aa8\",\"title\":\"Multi-Channel Co-Attention Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/99a01a687f7959de4c86102342d4bcfec6382aa8\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1912.01496\",\"authors\":[{\"authorId\":\"23608432\",\"name\":\"Chao-Chun Hsu\"},{\"authorId\":\"8157332\",\"name\":\"Zi-Yuan Chen\"},{\"authorId\":\"48162772\",\"name\":\"Chi-Yang Hsu\"},{\"authorId\":\"1441050875\",\"name\":\"Chih-Chia Li\"},{\"authorId\":\"9261587\",\"name\":\"Tzu-Yuan Lin\"},{\"authorId\":\"144188081\",\"name\":\"Ting-Hao Kenneth Huang\"},{\"authorId\":\"1746959\",\"name\":\"Lun-Wei Ku\"}],\"doi\":\"10.1609/AAAI.V34I05.6303\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e69e4bb78f812ad252771b9600e1cfeb88d36722\",\"title\":\"Knowledge-Enriched Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/e69e4bb78f812ad252771b9600e1cfeb88d36722\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1912.13151\",\"authors\":[{\"authorId\":\"8010189\",\"name\":\"Xinjie Fan\"},{\"authorId\":\"3272356\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"1471411319\",\"name\":\"Zhendong Wang\"},{\"authorId\":\"38026572\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa950171b25c05466d9d3cc58dff3b9ea9882e4a\",\"title\":\"Adaptive Correlated Monte Carlo for Contextual Categorical Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/aa950171b25c05466d9d3cc58dff3b9ea9882e4a\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993656298\",\"name\":\"Jingwen Hou\"},{\"authorId\":\"144545118\",\"name\":\"Sheng Yang\"},{\"authorId\":\"144968899\",\"name\":\"W. Lin\"}],\"doi\":\"10.1145/3394171.3413695\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3be8cfb1299582b8ac6eca28350537c8b6f3777f\",\"title\":\"Object-level Attention for Aesthetic Rating Distribution Prediction\",\"url\":\"https://www.semanticscholar.org/paper/3be8cfb1299582b8ac6eca28350537c8b6f3777f\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2001.09545\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"66a2eb540af6f47db177599bc793ab0c6a6aa47e\",\"title\":\"aiTPR: Attribute Interaction-Tensor Product Representation for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/66a2eb540af6f47db177599bc793ab0c6a6aa47e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797144\",\"name\":\"G. Friedland\"},{\"authorId\":\"1772549\",\"name\":\"D. Borth\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"df9a08016fa553a169d893ce2d3fca375bab4781\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/df9a08016fa553a169d893ce2d3fca375bab4781\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.1145/3347450.3357660\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fdab9c73255869d9c681e41aa3963991159d946d\",\"title\":\"Connecting Language and Vision: From Captioning towards Embodied Learning\",\"url\":\"https://www.semanticscholar.org/paper/fdab9c73255869d9c681e41aa3963991159d946d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.00822\",\"authors\":[{\"authorId\":\"1387720883\",\"name\":\"Haozheng Luo\"},{\"authorId\":\"1443782482\",\"name\":\"Ruiyang Qin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1fe97e5fd1b7151d5ab6565bfe3ca6efe034575\",\"title\":\"Open-Ended Multi-Modal Relational Reason for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b1fe97e5fd1b7151d5ab6565bfe3ca6efe034575\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47267313\",\"name\":\"T. Le\"},{\"authorId\":\"144672395\",\"name\":\"Vuong Le\"},{\"authorId\":\"143761093\",\"name\":\"S. Venkatesh\"},{\"authorId\":\"6254479\",\"name\":\"T. Tran\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"68ea8690a6e55c00bdf6df50af6d14a0ad08914f\",\"title\":\"Input Clips CNN Features Clip Representation How many times eyes ?\",\"url\":\"https://www.semanticscholar.org/paper/68ea8690a6e55c00bdf6df50af6d14a0ad08914f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"94011352\",\"name\":\"S. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"456108dc68bc5d2a2e39cbeb69f51c65d0acf53e\",\"title\":\"AiR: Attention with Reasoning Capability (Supplementary Materials)\",\"url\":\"https://www.semanticscholar.org/paper/456108dc68bc5d2a2e39cbeb69f51c65d0acf53e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31760501\",\"name\":\"K. Yoo\"},{\"authorId\":\"32021996\",\"name\":\"H. S. Jo\"},{\"authorId\":\"3305330\",\"name\":\"Hanbit Lee\"},{\"authorId\":\"122200203\",\"name\":\"Jeeseung Han\"},{\"authorId\":\"3013044\",\"name\":\"Sanggoo Lee\"}],\"doi\":\"10.1109/ICCVW.2019.00105\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93c7e48355ff86a79d95682e3d97acabaccd3928\",\"title\":\"Stochastic Relational Network\",\"url\":\"https://www.semanticscholar.org/paper/93c7e48355ff86a79d95682e3d97acabaccd3928\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1810.04101\",\"authors\":[{\"authorId\":\"1809420\",\"name\":\"Loris Bazzani\"},{\"authorId\":\"2120874\",\"name\":\"Tobias Domhan\"},{\"authorId\":\"2521764\",\"name\":\"F. Hieber\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d76c73ab62d960f5417dfa8aff9e63d357c5fd5d\",\"title\":\"Image Captioning as Neural Machine Translation Task in SOCKEYE\",\"url\":\"https://www.semanticscholar.org/paper/d76c73ab62d960f5417dfa8aff9e63d357c5fd5d\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2126585\",\"name\":\"Byungkook Oh\"},{\"authorId\":\"2034634\",\"name\":\"Seungmin Seo\"},{\"authorId\":\"1379884035\",\"name\":\"Cheolheon Shin\"},{\"authorId\":\"91170516\",\"name\":\"Eun-Ju Jo\"},{\"authorId\":\"14934343\",\"name\":\"Kyong-Ho Lee\"}],\"doi\":\"10.18653/v1/D19-1232\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3565d761c81335978d8bd935150f322a5e668d01\",\"title\":\"Topic-Guided Coherence Modeling for Sentence Ordering by Preserving Global and Local Information\",\"url\":\"https://www.semanticscholar.org/paper/3565d761c81335978d8bd935150f322a5e668d01\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391204924\",\"name\":\"Zongjian Zhang\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"50528721\",\"name\":\"Qiuyun Wu\"},{\"authorId\":\"94294263\",\"name\":\"F. Chen\"}],\"doi\":\"10.1109/IJCNN.2019.8851832\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a273df2ac9fc4e796c532feb76f5e676a0773b1c\",\"title\":\"Visual Relationship Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a273df2ac9fc4e796c532feb76f5e676a0773b1c\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47557007\",\"name\":\"Himanshu Sharma\"},{\"authorId\":\"2704856\",\"name\":\"A. S. Jalal\"}],\"doi\":\"10.1142/s0217984920503157\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b16e790764d0169e06501125ce946123d67f7c30\",\"title\":\"Incorporating external knowledge for image captioning using CNN and LSTM\",\"url\":\"https://www.semanticscholar.org/paper/b16e790764d0169e06501125ce946123d67f7c30\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41050714\",\"name\":\"Sandeep Narayan Parameswaran\"},{\"authorId\":\"144009889\",\"name\":\"Sukhendu Das\"}],\"doi\":\"10.1007/978-3-030-36718-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c940e8b66e822dbdd8d0dde9a6640ed01834e31\",\"title\":\"SACIC: A Semantics-Aware Convolutional Image Captioner Using Multi-level Pervasive Attention\",\"url\":\"https://www.semanticscholar.org/paper/9c940e8b66e822dbdd8d0dde9a6640ed01834e31\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145522783\",\"name\":\"Z. Sun\"},{\"authorId\":\"144361839\",\"name\":\"X. Lin\"},{\"authorId\":\"50218964\",\"name\":\"Zhaohui Wang\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"}],\"doi\":\"10.1007/978-3-030-00767-6_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8b600d520f95b811857052d864ee54567064dd9\",\"title\":\"Multi-decoder Based Co-attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b8b600d520f95b811857052d864ee54567064dd9\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72928123\",\"name\":\"Zhu Zi-hao\"},{\"authorId\":\"150353001\",\"name\":\"Yang Xiao\"},{\"authorId\":\"49619470\",\"name\":\"Li Shuai\"},{\"authorId\":\"1795646\",\"name\":\"Z. Cao\"},{\"authorId\":\"39271955\",\"name\":\"Zhiwen Fang\"},{\"authorId\":\"10638646\",\"name\":\"Joey Tianyi Zhou\"}],\"doi\":\"10.1016/j.ins.2020.10.006\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d1cab07d22becf1629b8939cf58cdf8724afcf6\",\"title\":\"LPQ++: A discriminative blur-insensitive textural descriptor with spatial-channel interaction\",\"url\":\"https://www.semanticscholar.org/paper/4d1cab07d22becf1629b8939cf58cdf8724afcf6\",\"venue\":\"Inf. Sci.\",\"year\":2021},{\"arxivId\":\"2011.07680\",\"authors\":[{\"authorId\":\"1477956290\",\"name\":\"Wenting Xu\"},{\"authorId\":\"2023765018\",\"name\":\"C. Qi\"},{\"authorId\":\"50070382\",\"name\":\"Zhenghua Xu\"},{\"authorId\":\"1690572\",\"name\":\"Thomas Lukasiewicz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"504e7bba81083ebbc4e114e48938ebaffb5ba03c\",\"title\":\"Reinforced Medical Report Generation with X-Linear Attention and Repetition Penalty\",\"url\":\"https://www.semanticscholar.org/paper/504e7bba81083ebbc4e114e48938ebaffb5ba03c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144638992\",\"name\":\"Xinzhe Han\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"145422145\",\"name\":\"Chi Su\"},{\"authorId\":\"153645460\",\"name\":\"W. Zhang\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1007/978-3-030-58545-7_32\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a315d4e5db42ae5d45f7c1702998b41ac26babe\",\"title\":\"Interpretable Visual Reasoning via Probabilistic Formulation Under Natural Supervision\",\"url\":\"https://www.semanticscholar.org/paper/2a315d4e5db42ae5d45f7c1702998b41ac26babe\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145986708\",\"name\":\"Bo Sun\"},{\"authorId\":\"14701865\",\"name\":\"Z. Yao\"},{\"authorId\":\"48380350\",\"name\":\"Yinghui Zhang\"},{\"authorId\":\"8834504\",\"name\":\"Lejun Yu\"}],\"doi\":\"10.1016/j.jvcir.2020.102762\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86475be7965eebb5edba838788d26c9272f14a3b\",\"title\":\"Local relation network with multilevel attention for visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/86475be7965eebb5edba838788d26c9272f14a3b\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/s11042-020-09251-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"af92381f95f28701396abeecaf715383b26ca354\",\"title\":\"A unified cycle-consistent neural model for text and image retrieval\",\"url\":\"https://www.semanticscholar.org/paper/af92381f95f28701396abeecaf715383b26ca354\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"2223252\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"46700604\",\"name\":\"J. Liu\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3394171.3414009\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0f480dadb895975a3921f5fcd03fdbf4085f8e9b\",\"title\":\"Attacking Image Captioning Towards Accuracy-Preserving Target Words Removal\",\"url\":\"https://www.semanticscholar.org/paper/0f480dadb895975a3921f5fcd03fdbf4085f8e9b\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47220354\",\"name\":\"Pingping Huang\"},{\"authorId\":\"47513298\",\"name\":\"J. Huang\"},{\"authorId\":\"46791647\",\"name\":\"Y. Guo\"},{\"authorId\":\"49605671\",\"name\":\"M. Qiao\"},{\"authorId\":\"143756111\",\"name\":\"Y. Zhu\"}],\"doi\":\"10.18653/v1/P19-1349\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"144f4d5dcd0b13935ff0d0890c2ec37aa40039b1\",\"title\":\"Multi-grained Attention with Object-level Grounding for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/144f4d5dcd0b13935ff0d0890c2ec37aa40039b1\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1904.13148\",\"authors\":[{\"authorId\":\"3110609\",\"name\":\"Zhennan Wang\"},{\"authorId\":\"2568383\",\"name\":\"Wenbin Zou\"},{\"authorId\":\"144282087\",\"name\":\"C. Xu\"}],\"doi\":\"10.1109/ICCV.2019.00611\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b7c074e0fce985164989f0163cef8e7bb59a3612\",\"title\":\"PR Product: A Substitute for Inner Product in Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b7c074e0fce985164989f0163cef8e7bb59a3612\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1708.02711\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00444\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81\",\"title\":\"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1804.00105\",\"authors\":[{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"46708223\",\"name\":\"B. Li\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/CVPR.2018.00757\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eff328e0ecfb9a7a2d6664ee38aa32a61c7b9f42\",\"title\":\"Visual Question Reasoning on General Dependency Tree\",\"url\":\"https://www.semanticscholar.org/paper/eff328e0ecfb9a7a2d6664ee38aa32a61c7b9f42\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1903.00839\",\"authors\":[{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"50218594\",\"name\":\"Z. Wang\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1109/CVPR.2019.00205\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd961afa9d75e1e7a657a9e11d6f6d3b968282a0\",\"title\":\"Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing\",\"url\":\"https://www.semanticscholar.org/paper/cd961afa9d75e1e7a657a9e11d6f6d3b968282a0\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1901.02579\",\"authors\":[{\"authorId\":\"46947776\",\"name\":\"Zhenqiang Li\"},{\"authorId\":\"48355461\",\"name\":\"Yifei Huang\"},{\"authorId\":\"3172280\",\"name\":\"Minjie Cai\"},{\"authorId\":\"9467266\",\"name\":\"Y. Sato\"}],\"doi\":\"10.1109/ICCVW.2019.00539\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70681bc4d3facb1839c3459ee1c68b048d46bca8\",\"title\":\"Manipulation-Skill Assessment from Videos with Spatial Attention Network\",\"url\":\"https://www.semanticscholar.org/paper/70681bc4d3facb1839c3459ee1c68b048d46bca8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152951058\",\"name\":\"Drew A. Hudson\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.1109/CVPR.2019.00686\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1\",\"title\":\"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3144952\",\"name\":\"Sibei Yang\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"66616425\",\"name\":\"Yizhou Yu\"}],\"doi\":\"10.1109/CVPR.2019.00427\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1db245d9d2873ed433774f55b74e5b0274a71bd8\",\"title\":\"Cross-Modal Relationship Inference for Grounding Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/1db245d9d2873ed433774f55b74e5b0274a71bd8\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.06619\",\"authors\":[{\"authorId\":\"2251827\",\"name\":\"Gil Sadeh\"},{\"authorId\":\"46397904\",\"name\":\"L. Fritz\"},{\"authorId\":\"36004650\",\"name\":\"Gabi Shalev\"},{\"authorId\":\"40135367\",\"name\":\"E. Oks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b4708bce76d496e0a1083d057cad6e1562a302d\",\"title\":\"Generating Diverse and Informative Natural Language Fashion Feedback\",\"url\":\"https://www.semanticscholar.org/paper/7b4708bce76d496e0a1083d057cad6e1562a302d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1803.07464\",\"authors\":[{\"authorId\":\"48933900\",\"name\":\"Q. Li\"},{\"authorId\":\"3392051\",\"name\":\"Qingyi Tao\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1007/978-3-030-01234-2_34\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"06ba3492e3a9a2e98df2c81b91ec94787e3f97fb\",\"title\":\"VQA-E: Explaining, Elaborating, and Enhancing Your Answers for Visual Questions\",\"url\":\"https://www.semanticscholar.org/paper/06ba3492e3a9a2e98df2c81b91ec94787e3f97fb\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32763968\",\"name\":\"A. Fisch\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"144797264\",\"name\":\"J. Clark\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.705\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b01bc15919f72929d36fc0443395e97b632c81b8\",\"title\":\"CapWAP: Image Captioning with a Purpose\",\"url\":\"https://www.semanticscholar.org/paper/b01bc15919f72929d36fc0443395e97b632c81b8\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2008.00397\",\"authors\":[{\"authorId\":\"51115516\",\"name\":\"L. Yang\"},{\"authorId\":\"49435166\",\"name\":\"Fanqi Meng\"},{\"authorId\":\"31395194\",\"name\":\"Ming-Kuang Daniel Wu\"},{\"authorId\":\"1850625173\",\"name\":\"Vicent Ying\"},{\"authorId\":\"150345115\",\"name\":\"Xianchao Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c711f0b0f9e214d6ee462cffcac733221bf07026\",\"title\":\"SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space\",\"url\":\"https://www.semanticscholar.org/paper/c711f0b0f9e214d6ee462cffcac733221bf07026\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.06597\",\"authors\":[{\"authorId\":\"47773127\",\"name\":\"S. Yuan\"},{\"authorId\":\"1423652601\",\"name\":\"Ke Bai\"},{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"48378494\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"46387857\",\"name\":\"Chenyang Tao\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"1807489212\",\"name\":\"Guoyin Wang\"},{\"authorId\":\"51030446\",\"name\":\"R. Henao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1af06d2c4a129f9335159db8bb1455414705bed1\",\"title\":\"Weakly supervised cross-domain alignment with optimal transport\",\"url\":\"https://www.semanticscholar.org/paper/1af06d2c4a129f9335159db8bb1455414705bed1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.06800\",\"authors\":[{\"authorId\":\"9208016\",\"name\":\"Aakansha Mishra\"},{\"authorId\":\"47583423\",\"name\":\"A. Anand\"},{\"authorId\":\"46401518\",\"name\":\"Prithwijit Guha\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206913\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"212c2d71f9f404bcbb3cfe8bc35e5e1ea25076c5\",\"title\":\"CQ-VQA: Visual Question Answering on Categorized Questions\",\"url\":\"https://www.semanticscholar.org/paper/212c2d71f9f404bcbb3cfe8bc35e5e1ea25076c5\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"2004.12770\",\"authors\":[{\"authorId\":\"1659695668\",\"name\":\"Cristobal Eyzaguirre\"},{\"authorId\":\"144106940\",\"name\":\"A. Soto\"}],\"doi\":\"10.1109/cvpr42600.2020.01283\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2732b5a501870e20e77cd6cde81b33ae017ba206\",\"title\":\"Differentiable Adaptive Computation Time for Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/2732b5a501870e20e77cd6cde81b33ae017ba206\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2589441\",\"name\":\"Shiqi Wu\"},{\"authorId\":\"152899497\",\"name\":\"Xiangrong Zhang\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"1713590317\",\"name\":\"Chen Li\"},{\"authorId\":\"1734497\",\"name\":\"Licheng Jiao\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207381\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f497460633cf0a61c5749aa02f11bc95a599dc4\",\"title\":\"Scene Attention Mechanism for Remote Sensing Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/8f497460633cf0a61c5749aa02f11bc95a599dc4\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1978802390\",\"name\":\"Haolei Pei\"},{\"authorId\":\"8559954\",\"name\":\"Q. Chen\"},{\"authorId\":\"13257164\",\"name\":\"J. Wang\"},{\"authorId\":\"123555217\",\"name\":\"Q. Sun\"},{\"authorId\":\"1680030\",\"name\":\"Yubo Jia\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206815\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"title\":\"Visual Relational Reasoning for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/129f71acdcec4b171a4f11beec4f10463d5ffa38\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"2003.08897\",\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"49051904\",\"name\":\"P. Yao\"},{\"authorId\":\"116829059\",\"name\":\"Shi-chen Lu\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/cvpr42600.2020.01034\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"833560cd68a3e3d1be1bc650756dd6c679798551\",\"title\":\"Normalized and Geometry-Aware Self-Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/833560cd68a3e3d1be1bc650756dd6c679798551\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49251978\",\"name\":\"J. Chen\"},{\"authorId\":\"41079034\",\"name\":\"Hong-Yang Chao\"}],\"doi\":\"10.1145/3394171.3416291\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"598ad06c164043c45c952dbde37e0c75991e66aa\",\"title\":\"VideoTRM: Pre-training for Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/598ad06c164043c45c952dbde37e0c75991e66aa\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40175280\",\"name\":\"Huaizu Jiang\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1389846455\",\"name\":\"E. Learned-Miller\"},{\"authorId\":\"1639441927\",\"name\":\"Xinlei Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a929fb67bae52cfd01d7d3a6b22e27ece62bfa94\",\"title\":\"Supplementary Material: In Defense of Grid Features for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a929fb67bae52cfd01d7d3a6b22e27ece62bfa94\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49597404\",\"name\":\"Z. Fang\"},{\"authorId\":\"7187373\",\"name\":\"J. Liu\"},{\"authorId\":null,\"name\":\"Qu Tang\"},{\"authorId\":\"49112842\",\"name\":\"Y. Li\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1007/978-3-030-20887-5_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8463ff93a6725017bd1875eeb7ae4d0f0e7df568\",\"title\":\"Answer Distillation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8463ff93a6725017bd1875eeb7ae4d0f0e7df568\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":\"2012.07248\",\"authors\":[{\"authorId\":\"1560385163\",\"name\":\"Bo Pang\"},{\"authorId\":\"1527112562\",\"name\":\"Yizhuo Li\"},{\"authorId\":\"49299169\",\"name\":\"J. Li\"},{\"authorId\":\"1807805250\",\"name\":\"Muchen Li\"},{\"authorId\":\"50732737\",\"name\":\"Hanwen Cao\"},{\"authorId\":\"1830034\",\"name\":\"Cewu Lu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"965571810fcb79fdaaed7329ff57b3720508a241\",\"title\":\"TDAF: Top-Down Attention Framework for Vision Tasks\",\"url\":\"https://www.semanticscholar.org/paper/965571810fcb79fdaaed7329ff57b3720508a241\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.12313\",\"authors\":[{\"authorId\":\"32556011\",\"name\":\"Victor Milewski\"},{\"authorId\":\"100781843\",\"name\":\"Marie-Francine Moens\"},{\"authorId\":\"2338197\",\"name\":\"Iacer Calixto\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"b89e002625b0703f08b7590a1763ca7c8be66ad4\",\"title\":\"Are scene graphs good enough to improve Image Captioning?\",\"url\":\"https://www.semanticscholar.org/paper/b89e002625b0703f08b7590a1763ca7c8be66ad4\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30658665\",\"name\":\"Z. Li\"},{\"authorId\":\"108085542\",\"name\":\"Rui Wang\"},{\"authorId\":\"2849740\",\"name\":\"Kehai Chen\"},{\"authorId\":\"1583166440\",\"name\":\"Masso Utiyama\"},{\"authorId\":\"1698363\",\"name\":\"Eiichiro Sumita\"},{\"authorId\":\"3322871\",\"name\":\"Zhuosheng Zhang\"},{\"authorId\":\"47941144\",\"name\":\"Hai Zhao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69e4dabf1f140915878a365c1adb86ceb2362ab6\",\"title\":\"Data-dependent Gaussian Prior Objective for Language Generation\",\"url\":\"https://www.semanticscholar.org/paper/69e4dabf1f140915878a365c1adb86ceb2362ab6\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1902.03751\",\"authors\":[{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1774780\",\"name\":\"Yilin Shen\"},{\"authorId\":\"1705713\",\"name\":\"Hongxia Jin\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/ICCV.2019.00268\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ab9ac4bea4ee49fd879337ac477dc09de4914fbf\",\"title\":\"Taking a HINT: Leveraging Explanations to Make Vision and Language Models More Grounded\",\"url\":\"https://www.semanticscholar.org/paper/ab9ac4bea4ee49fd879337ac477dc09de4914fbf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49597404\",\"name\":\"Z. Fang\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"80526284\",\"name\":\"Yanyuan Qiao\"},{\"authorId\":null,\"name\":\"Qu Tang\"},{\"authorId\":\"47002702\",\"name\":\"Y. Li\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1145/3240508.3240662\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a1f1a06b840558c4433f0e06a4e9172539469e21\",\"title\":\"Enhancing Visual Question Answering Using Dropout\",\"url\":\"https://www.semanticscholar.org/paper/a1f1a06b840558c4433f0e06a4e9172539469e21\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1811.10582\",\"authors\":[{\"authorId\":\"38907975\",\"name\":\"Ning Xie\"},{\"authorId\":\"1868193\",\"name\":\"Farley Lai\"},{\"authorId\":\"2514295\",\"name\":\"Derek Doran\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0955252cd57db8503a2ed9e56f195fa44b1bc0d4\",\"title\":\"Visual Entailment Task for Visually-Grounded Language Learning\",\"url\":\"https://www.semanticscholar.org/paper/0955252cd57db8503a2ed9e56f195fa44b1bc0d4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1812.05252\",\"authors\":[{\"authorId\":\"144579865\",\"name\":\"P. Gao\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"},{\"authorId\":\"30156979\",\"name\":\"Haoxuan You\"},{\"authorId\":\"50676465\",\"name\":\"Zhengkai Jiang\"},{\"authorId\":\"2887562\",\"name\":\"Pan Lu\"},{\"authorId\":\"49212307\",\"name\":\"Steven C. H. Hoi\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00680\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e9b13731027418ed38103d1dfc8a70f6881bc684\",\"title\":\"Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e9b13731027418ed38103d1dfc8a70f6881bc684\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.11903\",\"authors\":[{\"authorId\":\"33970300\",\"name\":\"Bor-Chun Chen\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"},{\"authorId\":\"38760573\",\"name\":\"Ser-Nam Lim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c00d9cae4974c4e8744b81dd18daa0eeb08974d8\",\"title\":\"An Analysis of Object Embeddings for Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/c00d9cae4974c4e8744b81dd18daa0eeb08974d8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73b49bcb79810a3b4f53222a4698bc1766fa9d6f\",\"title\":\"Grounded Video Description : Supplementary Document\",\"url\":\"https://www.semanticscholar.org/paper/73b49bcb79810a3b4f53222a4698bc1766fa9d6f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"104501424\",\"name\":\"Muhammad Rameez Ur Rahman\"},{\"authorId\":\"2522828\",\"name\":\"Haiyong Chen\"}],\"doi\":\"10.1109/ACCESS.2020.2976843\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd661fcc8b47c3d1dae2a56ece20ae68c3c25b10\",\"title\":\"Defects Inspection in Polycrystalline Solar Cells Electroluminescence Images Using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/fd661fcc8b47c3d1dae2a56ece20ae68c3c25b10\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1807.09956\",\"authors\":[{\"authorId\":\"143804072\",\"name\":\"Y. Jiang\"},{\"authorId\":\"2311222\",\"name\":\"V. Natarajan\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"36c3972569a6949ecca90bfa6f8e99883e092845\",\"title\":\"Pythia v0.1: the Winning Entry to the VQA Challenge 2018\",\"url\":\"https://www.semanticscholar.org/paper/36c3972569a6949ecca90bfa6f8e99883e092845\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2009.14405\",\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"47739902\",\"name\":\"J. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6caaf71d18f093aebd0cffdcf246b86400092ab5\",\"title\":\"Teacher-Critical Training Strategies for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6caaf71d18f093aebd0cffdcf246b86400092ab5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2548483\",\"name\":\"Shengfeng He\"},{\"authorId\":\"14898006\",\"name\":\"Chu Han\"},{\"authorId\":\"2513505\",\"name\":\"Guoqiang Han\"},{\"authorId\":\"145947071\",\"name\":\"Jing Qin\"}],\"doi\":\"10.1109/TNNLS.2019.2933439\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d3c31f077c002d476d7eb85e94d43830243e4bf\",\"title\":\"Exploring Duality in Visual Question-Driven Top-Down Saliency\",\"url\":\"https://www.semanticscholar.org/paper/3d3c31f077c002d476d7eb85e94d43830243e4bf\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":\"2004.10258\",\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"1491357190\",\"name\":\"Yang Hua\"},{\"authorId\":\"46406827\",\"name\":\"N. Robertson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"f882bc53ded32b2a32b291078c9454d82f6f108b\",\"title\":\"ParaCNN: Visual Paragraph Generation via Adversarial Twin Contextual CNNs\",\"url\":\"https://www.semanticscholar.org/paper/f882bc53ded32b2a32b291078c9454d82f6f108b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1571504713\",\"name\":\"Ana Luiza Favar\\u00e3o Le\\u00e3o\"},{\"authorId\":\"1394470211\",\"name\":\"Hugo Queiroz Abonizio\"},{\"authorId\":\"1713629\",\"name\":\"Sylvio Barbon Junior\"},{\"authorId\":\"87617788\",\"name\":\"M. Kanashiro\"}],\"doi\":\"10.47235/rmu.v8i1.140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bdde545f748816dbe0f0eb5d19a9c3a8b55c4664\",\"title\":\"Identifica\\u00e7\\u00e3o de Composi\\u00e7\\u00f5es da Paisagem Urbana: Uma abordagem de Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/bdde545f748816dbe0f0eb5d19a9c3a8b55c4664\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1909.10650\",\"authors\":[{\"authorId\":\"143912737\",\"name\":\"Heather Riley\"},{\"authorId\":\"1714890\",\"name\":\"M. Sridharan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb2fb0462a1e8cf01e2ddf4ff7029d1edb3a8d58\",\"title\":\"Non-monotonic Logical Reasoning Guiding Deep Learning for Explainable Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bb2fb0462a1e8cf01e2ddf4ff7029d1edb3a8d58\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47056886\",\"name\":\"Xiangpeng Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"9764377\",\"name\":\"Xuanhan Wang\"},{\"authorId\":\"51347989\",\"name\":\"W. Liu\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"152555512\",\"name\":\"Heng Tao Shen\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1145/3343031.3350971\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f133f3daed70c50d1828cc6a46bc2e8dbd7880de\",\"title\":\"Learnable Aggregating Net with Diversity Learning for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f133f3daed70c50d1828cc6a46bc2e8dbd7880de\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"143953573\",\"name\":\"K. Shih\"},{\"authorId\":\"1798489\",\"name\":\"Yichen Li\"},{\"authorId\":\"144968844\",\"name\":\"K. Xu\"},{\"authorId\":\"1749609\",\"name\":\"S. Lazebnik\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"155dbe88e444b18ad1fd78dc9b655eb9b9d7fd43\",\"title\":\"Open-vocabulary Phrase Detection\",\"url\":\"https://www.semanticscholar.org/paper/155dbe88e444b18ad1fd78dc9b655eb9b9d7fd43\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1431726865\",\"name\":\"R. SreelaS.\"},{\"authorId\":\"1984257\",\"name\":\"S. M. Idicula\"}],\"doi\":\"10.3390/info10110354\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f3b1964968f2dd1a065a43c4cafae6a6de6dac1c\",\"title\":\"Dense Model for Automatic Image Description Generation with Game Theoretic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/f3b1964968f2dd1a065a43c4cafae6a6de6dac1c\",\"venue\":\"Inf.\",\"year\":2019},{\"arxivId\":\"1910.00058\",\"authors\":[{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.18653/v1/D19-1154\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99b3b563f6611f60af8ca96624191ff66b27a8f9\",\"title\":\"Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations\",\"url\":\"https://www.semanticscholar.org/paper/99b3b563f6611f60af8ca96624191ff66b27a8f9\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2011.02164\",\"authors\":[{\"authorId\":\"49172303\",\"name\":\"T. Rahman\"},{\"authorId\":\"6937593\",\"name\":\"Shih-Han Chou\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1387254703\",\"name\":\"Giuseppe Carenini\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e1cf7e279abc4301729c24cb7f888b2df42f7644\",\"title\":\"An Improved Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e1cf7e279abc4301729c24cb7f888b2df42f7644\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3178736\",\"name\":\"R. Pierdicca\"},{\"authorId\":\"3413670\",\"name\":\"M. Paolanti\"},{\"authorId\":\"1721130\",\"name\":\"E. Frontoni\"},{\"authorId\":\"92326466\",\"name\":\"L. Baraldi\"}],\"doi\":\"10.1007/978-3-030-58465-8_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f66c2761eec464281bccf2a11df755f23443ff7\",\"title\":\"AI4AR: An AI-Based Mobile Application for the Automatic Generation of AR Contents\",\"url\":\"https://www.semanticscholar.org/paper/1f66c2761eec464281bccf2a11df755f23443ff7\",\"venue\":\"AVR\",\"year\":2020},{\"arxivId\":\"2012.02206\",\"authors\":[{\"authorId\":\"73286206\",\"name\":\"Dave Zhenyu Chen\"},{\"authorId\":\"47621053\",\"name\":\"A. Gholami\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"},{\"authorId\":\"3317599\",\"name\":\"A. X. Chang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7a4ba78d377eea9650e5e399a0878e30bd22f648\",\"title\":\"Scan2Cap: Context-aware Dense Captioning in RGB-D Scans\",\"url\":\"https://www.semanticscholar.org/paper/7a4ba78d377eea9650e5e399a0878e30bd22f648\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\"},{\"authorId\":\"1380374156\",\"name\":\"C V Jawahar\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":\"10.1145/3343031.3350939\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eeacc1da20db16e2a2bcf4a5902bcff556c7a0ed\",\"title\":\"Towards Increased Accessibility of Meme Images with the Help of Rich Face Emotion Captions\",\"url\":\"https://www.semanticscholar.org/paper/eeacc1da20db16e2a2bcf4a5902bcff556c7a0ed\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2223252\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"1993645531\",\"name\":\"Gen Luo\"},{\"authorId\":\"46761465\",\"name\":\"Xiaopeng Hong\"},{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":\"2713947\",\"name\":\"Xinghao Ding\"},{\"authorId\":\"40799321\",\"name\":\"Ling Shao\"}],\"doi\":\"10.1145/3394171.3413998\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"72687e467b4ab4d4799cca976013c5936ccb74b1\",\"title\":\"K-armed Bandit based Multi-Modal Network Architecture Search for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/72687e467b4ab4d4799cca976013c5936ccb74b1\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32543309\",\"name\":\"Tianling Jiang\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"49046633\",\"name\":\"Chunping Liu\"},{\"authorId\":\"21633777\",\"name\":\"Hailin Shao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"78c2f7520becde5e3bcd9b952791d67c33a48612\",\"title\":\"Visual-Textual Alignment for Graph Inference in Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/78c2f7520becde5e3bcd9b952791d67c33a48612\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2012.10210\",\"authors\":[{\"authorId\":\"108630954\",\"name\":\"T. Winterbottom\"},{\"authorId\":\"2455565\",\"name\":\"S. Xiao\"},{\"authorId\":\"115718758\",\"name\":\"A. Mclean\"},{\"authorId\":\"1875235\",\"name\":\"N. A. Moubayed\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6ebabf0479d3d6ccbd2febfb194fe1df75358190\",\"title\":\"On modality bias in the TVQA dataset.\",\"url\":\"https://www.semanticscholar.org/paper/6ebabf0479d3d6ccbd2febfb194fe1df75358190\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.02707\",\"authors\":[{\"authorId\":\"1612421029\",\"name\":\"Yicong Hong\"},{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.271\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c71cf890f1e6169c28407a985dcfd2b0b156d9e2\",\"title\":\"Sub-Instruction Aware Vision-and-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/c71cf890f1e6169c28407a985dcfd2b0b156d9e2\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1911.10354\",\"authors\":[{\"authorId\":\"51215319\",\"name\":\"Kohei Uehara\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.18653/v1/2020.nlpbt-1.6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc4598d636b599c4752a376cc074541c5a0ec97a\",\"title\":\"Unsupervised Keyword Extraction for Full-sentence VQA\",\"url\":\"https://www.semanticscholar.org/paper/fc4598d636b599c4752a376cc074541c5a0ec97a\",\"venue\":\"NLPBT\",\"year\":2020},{\"arxivId\":\"2009.01039\",\"authors\":[{\"authorId\":\"40974493\",\"name\":\"Alessio Sarullo\"},{\"authorId\":\"47279171\",\"name\":\"Tingting Mu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c945cff9a378aca8ebe5659afc8a0338010770df\",\"title\":\"Zero-Shot Human-Object Interaction Recognition via Affordance Graphs\",\"url\":\"https://www.semanticscholar.org/paper/c945cff9a378aca8ebe5659afc8a0338010770df\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.06225\",\"authors\":[{\"authorId\":\"7412048\",\"name\":\"Moloud Abdar\"},{\"authorId\":\"1866603\",\"name\":\"Farhad Pourpanah\"},{\"authorId\":\"1833049320\",\"name\":\"Sadiq Hussain\"},{\"authorId\":\"1404229235\",\"name\":\"D. Rezazadegan\"},{\"authorId\":\"144073922\",\"name\":\"Li Liu\"},{\"authorId\":\"103809454\",\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":\"93660405\",\"name\":\"P. Fieguth\"},{\"authorId\":\"1719250\",\"name\":\"Xiaochun Cao\"},{\"authorId\":\"145434104\",\"name\":\"Abbas Khosravi\"},{\"authorId\":\"144076869\",\"name\":\"U. Acharya\"},{\"authorId\":\"144531494\",\"name\":\"V. Makarenkov\"},{\"authorId\":\"98613453\",\"name\":\"S. Nahavandi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"497d5002f41feb2e4729a171cdc5c9f22ee403df\",\"title\":\"A Review of Uncertainty Quantification in Deep Learning: Techniques, Applications and Challenges\",\"url\":\"https://www.semanticscholar.org/paper/497d5002f41feb2e4729a171cdc5c9f22ee403df\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.13862\",\"authors\":[{\"authorId\":\"50232214\",\"name\":\"Wenjia Xu\"},{\"authorId\":\"51175635\",\"name\":\"Jiuniu Wang\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"3287827\",\"name\":\"Guangluan Xu\"},{\"authorId\":\"7413451\",\"name\":\"Daoyu Lin\"},{\"authorId\":\"1379498558\",\"name\":\"Wei Dai\"},{\"authorId\":\"48607717\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/JSTSP.2020.2987729\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c056c388f4deb0ff87932eb6e7789b23eb22d22\",\"title\":\"Where is the Model Looking At? \\u2013 Concentrate and Explain the Network Attention\",\"url\":\"https://www.semanticscholar.org/paper/7c056c388f4deb0ff87932eb6e7789b23eb22d22\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":\"1912.10248\",\"authors\":[{\"authorId\":\"40960698\",\"name\":\"Huaizheng Zhang\"},{\"authorId\":\"84065133\",\"name\":\"Y. Luo\"},{\"authorId\":\"1470571795\",\"name\":\"Qiming Ai\"},{\"authorId\":\"35324425\",\"name\":\"Nana Hou\"},{\"authorId\":\"145868453\",\"name\":\"Yonggang Wen\"}],\"doi\":\"10.1145/3394171.3413582\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"570ec4424ca7c2607c446109f8890eaf36f97e16\",\"title\":\"Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning\",\"url\":\"https://www.semanticscholar.org/paper/570ec4424ca7c2607c446109f8890eaf36f97e16\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2006.11524\",\"authors\":[{\"authorId\":\"1961237\",\"name\":\"S. Amizadeh\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"2636739\",\"name\":\"Oleksandr Polozov\"},{\"authorId\":\"153268415\",\"name\":\"Y. Huang\"},{\"authorId\":\"145733034\",\"name\":\"K. Koishida\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fb62c82e469a265d986a164ba56d96d130937fd7\",\"title\":\"Neuro-Symbolic Visual Reasoning: Disentangling \\\"Visual\\\" from \\\"Reasoning\\\"\",\"url\":\"https://www.semanticscholar.org/paper/fb62c82e469a265d986a164ba56d96d130937fd7\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"1906.05168\",\"authors\":[{\"authorId\":\"146922818\",\"name\":\"H. N. Pham\"},{\"authorId\":\"145301586\",\"name\":\"Trung Le\"}],\"doi\":\"10.1109/KSE.2019.8919265\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"724dbeb78f2c7b29f77cef29ecd6adff11bce1fa\",\"title\":\"Attention-based Multi-Input Deep Learning Architecture for Biological Activity Prediction: An Application in EGFR Inhibitors\",\"url\":\"https://www.semanticscholar.org/paper/724dbeb78f2c7b29f77cef29ecd6adff11bce1fa\",\"venue\":\"2019 11th International Conference on Knowledge and Systems Engineering (KSE)\",\"year\":2019},{\"arxivId\":\"2006.07214\",\"authors\":[{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"},{\"authorId\":\"48374479\",\"name\":\"Marcos Treviso\"},{\"authorId\":\"1748971692\",\"name\":\"Ant'onio Farinhas\"},{\"authorId\":\"2114966\",\"name\":\"Vlad Niculae\"},{\"authorId\":\"35129010\",\"name\":\"M. A. Figueiredo\"},{\"authorId\":\"35537344\",\"name\":\"P. Aguiar\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"09e69bf0926e55cd277a3ef5b1450ba083719cb9\",\"title\":\"Sparse and Continuous Attention Mechanisms\",\"url\":\"https://www.semanticscholar.org/paper/09e69bf0926e55cd277a3ef5b1450ba083719cb9\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d115b0e8ba14641c26d5e38fd2b5f4c226888729\",\"title\":\"Research Statement Visual Question Answering and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/d115b0e8ba14641c26d5e38fd2b5f4c226888729\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2520427\",\"name\":\"Yulei Niu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"48985581\",\"name\":\"Manli Zhang\"},{\"authorId\":\"1684276\",\"name\":\"Jianhong Zhang\"},{\"authorId\":\"1776220\",\"name\":\"Zhiwu Lu\"},{\"authorId\":\"2259699\",\"name\":\"Ji-Rong Wen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"14644cc31aec3b9aa7d785e3c1a007bda0c7824d\",\"title\":\"Recursive Visual Attention Algorithm 1 Recursive Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/14644cc31aec3b9aa7d785e3c1a007bda0c7824d\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89813962\",\"name\":\"Shivangi Modi\"},{\"authorId\":\"153279050\",\"name\":\"Dhatri Pandya\"}],\"doi\":\"10.1109/ICCMC.2019.8819803\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c99654c738cf9a426fac40251e277282a8ee86a7\",\"title\":\"VQAR: Review on Information Retrieval Techniques based on Computer Vision and Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/c99654c738cf9a426fac40251e277282a8ee86a7\",\"venue\":\"2019 3rd International Conference on Computing Methodologies and Communication (ICCMC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2343931\",\"name\":\"Miao Ma\"},{\"authorId\":\"93119174\",\"name\":\"Ziang Gao\"}],\"doi\":\"10.1109/CCHI.2019.8901917\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ddcfc8f4aab75a96ba76f1275056f39f5595f2bc\",\"title\":\"Examinee Behavior Description Method Based on Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ddcfc8f4aab75a96ba76f1275056f39f5595f2bc\",\"venue\":\"2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI)\",\"year\":2019},{\"arxivId\":\"2002.09237\",\"authors\":[{\"authorId\":\"82384337\",\"name\":\"Karim Huesmann\"},{\"authorId\":\"40589394\",\"name\":\"Soeren Klemm\"},{\"authorId\":\"151092867\",\"name\":\"Lars Linsen\"},{\"authorId\":\"1726617\",\"name\":\"Benjamin Risse\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a1b77bba6130e1fe3650be5730c1ab253bf6065\",\"title\":\"Exploiting the Full Capacity of Deep Neural Networks while Avoiding Overfitting by Targeted Sparsity Regularization\",\"url\":\"https://www.semanticscholar.org/paper/6a1b77bba6130e1fe3650be5730c1ab253bf6065\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1624658620\",\"name\":\"Chunlei Wu\"},{\"authorId\":\"1429199889\",\"name\":\"Shaozu Yuan\"},{\"authorId\":\"51172982\",\"name\":\"Haiwen Cao\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"}],\"doi\":\"10.1109/ACCESS.2020.2981513\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"711be95a04da16c93b6bc880169532b68cdca37a\",\"title\":\"Hierarchical Attention-Based Fusion for Image Caption With Multi-Grained Rewards\",\"url\":\"https://www.semanticscholar.org/paper/711be95a04da16c93b6bc880169532b68cdca37a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2004.06193\",\"authors\":[{\"authorId\":\"10774714\",\"name\":\"Rajat Koner\"},{\"authorId\":\"98755209\",\"name\":\"Poulami Sinhamahapatra\"},{\"authorId\":\"1700754\",\"name\":\"Volker Tresp\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1fc60ce53d8fb9711c5c2cd9206f7578ebc5d9df\",\"title\":\"Relation Transformer Network\",\"url\":\"https://www.semanticscholar.org/paper/1fc60ce53d8fb9711c5c2cd9206f7578ebc5d9df\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.09144\",\"authors\":[{\"authorId\":\"67213349\",\"name\":\"N. Messina\"},{\"authorId\":\"1846129\",\"name\":\"F. Falchi\"},{\"authorId\":\"1699411\",\"name\":\"Andrea Esuli\"},{\"authorId\":\"144514869\",\"name\":\"G. Amato\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8185538174d9f751125407cad3687994ff08fadb\",\"title\":\"Transformer Reasoning Network for Image-Text Matching and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8185538174d9f751125407cad3687994ff08fadb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"40454588\",\"name\":\"J. Frahm\"}],\"doi\":\"10.1007/978-3-030-58577-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"title\":\"Computer Vision \\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\\u201328, 2020, Proceedings, Part XXX\",\"url\":\"https://www.semanticscholar.org/paper/08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10405058\",\"name\":\"Chenfei Wu\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"38542466\",\"name\":\"X. Wang\"},{\"authorId\":\"143672034\",\"name\":\"X. Dong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"184002001b3b514f432e538f872aebce3c7db060\",\"title\":\"Chain of Reasoning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/184002001b3b514f432e538f872aebce3c7db060\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1805.04247\",\"authors\":[{\"authorId\":\"6328564\",\"name\":\"M. Farazi\"},{\"authorId\":\"144812766\",\"name\":\"Salman Khan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7605857f551d128e7c3babfc019950250f81bca9\",\"title\":\"Reciprocal Attention Fusion for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7605857f551d128e7c3babfc019950250f81bca9\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":\"1812.01855\",\"authors\":[{\"authorId\":\"2522647\",\"name\":\"Jiaxin Shi\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"8549842\",\"name\":\"Juan-Zi Li\"}],\"doi\":\"10.1109/CVPR.2019.00857\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca1a2b86d39495be5524a0e39b663f7c423a0397\",\"title\":\"Explainable and Explicit Visual Reasoning Over Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/ca1a2b86d39495be5524a0e39b663f7c423a0397\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38994364\",\"name\":\"Tu-Khiem Le\"},{\"authorId\":\"7736164\",\"name\":\"Van-Tu Ninh\"},{\"authorId\":\"1381816609\",\"name\":\"Duc-Tien Dang-Nguyen\"},{\"authorId\":\"1780348\",\"name\":\"M. Tran\"},{\"authorId\":\"1829790\",\"name\":\"L. Zhou\"},{\"authorId\":\"104064634\",\"name\":\"Pablo Redondo\"},{\"authorId\":\"49074444\",\"name\":\"S. Smyth\"},{\"authorId\":\"1737981\",\"name\":\"C. Gurrin\"}],\"doi\":\"10.1145/3326460.3329162\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"057f139409e4a3bc89f1c19de4d8ca6ca5133385\",\"title\":\"LifeSeeker: Interactive Lifelog Search Engine at LSC 2019\",\"url\":\"https://www.semanticscholar.org/paper/057f139409e4a3bc89f1c19de4d8ca6ca5133385\",\"venue\":\"LSC '19\",\"year\":2019},{\"arxivId\":\"1811.07662\",\"authors\":[{\"authorId\":\"47833739\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2112160\",\"name\":\"Y. Li\"},{\"authorId\":\"1678689\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00859\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"23e943809c131c50dc90c1d308373febc60b9029\",\"title\":\"Intention Oriented Image Captions With Guiding Objects\",\"url\":\"https://www.semanticscholar.org/paper/23e943809c131c50dc90c1d308373febc60b9029\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144792753\",\"name\":\"Y. Qin\"},{\"authorId\":\"151046769\",\"name\":\"Jiajun Du\"},{\"authorId\":\"48379418\",\"name\":\"Yonghua Zhang\"},{\"authorId\":\"37514286\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/CVPR.2019.00856\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e0c0ac3bb66203c32be81193fabeee44c3585582\",\"title\":\"Look Back and Predict Forward in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e0c0ac3bb66203c32be81193fabeee44c3585582\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.07689\",\"authors\":[{\"authorId\":\"47300698\",\"name\":\"Hao Tan\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"145527705\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P19-1182\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157\",\"title\":\"Expressing Visual Relationships via Language\",\"url\":\"https://www.semanticscholar.org/paper/4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"2012.11014\",\"authors\":[{\"authorId\":\"35789996\",\"name\":\"Kenneth Marino\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1726095131\",\"name\":\"Abhinav Gupta\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a9015e511ec3da873f6114eeb542905a92d7d62\",\"title\":\"KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA\",\"url\":\"https://www.semanticscholar.org/paper/1a9015e511ec3da873f6114eeb542905a92d7d62\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37670557\",\"name\":\"Xiangyang Li\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"},{\"authorId\":\"1783847\",\"name\":\"J. Han\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"65885b53d54e52b8bbf31b1714be740eeb544c7f\",\"title\":\"Region Features ... ... Input Image Contextual Object Features ... ... ROI pooling C o n v Conv 5 _ 3 feature map Object Detection Region Proposal Network s Proposals Object Context Encoding\",\"url\":\"https://www.semanticscholar.org/paper/65885b53d54e52b8bbf31b1714be740eeb544c7f\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"145592288\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"1382537904\",\"name\":\"Chengpeng Dai\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"46246806\",\"name\":\"Chia-Wen Lin\"},{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"1740430\",\"name\":\"B. Zhang\"},{\"authorId\":\"1835006\",\"name\":\"Feiyue Huang\"},{\"authorId\":\"40626740\",\"name\":\"Liujuan Cao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"efb774eb43950267853b0387416f8c19dfa859a4\",\"title\":\"C V ] 9 O ct 2 01 9 Semantic-aware Image Deblurring\",\"url\":\"https://www.semanticscholar.org/paper/efb774eb43950267853b0387416f8c19dfa859a4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"},{\"authorId\":\"3431037\",\"name\":\"J. Qi\"},{\"authorId\":\"79633139\",\"name\":\"Yunkan Zhuo\"}],\"doi\":\"10.1109/TIP.2019.2952085\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f03ed79925004dcc28b3c7af33759463d08884e\",\"title\":\"MAVA: Multi-Level Adaptive Visual-Textual Alignment by Cross-Media Bi-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/3f03ed79925004dcc28b3c7af33759463d08884e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2004.10796\",\"authors\":[{\"authorId\":\"4868335\",\"name\":\"J. Park\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6966b0018daffa49eb2c38e68eb8964d56440233\",\"title\":\"Visual Commonsense Graphs: Reasoning about the Dynamic Context of a Still Image\",\"url\":\"https://www.semanticscholar.org/paper/6966b0018daffa49eb2c38e68eb8964d56440233\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.07770\",\"authors\":[{\"authorId\":\"47482125\",\"name\":\"E. Nowara\"},{\"authorId\":\"1381192475\",\"name\":\"Daniel McDuff\"},{\"authorId\":\"93554569\",\"name\":\"A. Veeraraghavan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6c00cd9b39e435775519033d737d82c5ad6dfad\",\"title\":\"The Benefit of Distraction: Denoising Remote Vitals Measurements using Inverse Attention\",\"url\":\"https://www.semanticscholar.org/paper/c6c00cd9b39e435775519033d737d82c5ad6dfad\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144069314\",\"name\":\"Bin Jiang\"},{\"authorId\":\"49444962\",\"name\":\"X. Huang\"},{\"authorId\":\"143702931\",\"name\":\"C. Yang\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3323873.3325019\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4e1ad4e3eeded53acf7514029ec556ee0ea42c45\",\"title\":\"Cross-Modal Video Moment Retrieval with Spatial and Language-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/4e1ad4e3eeded53acf7514029ec556ee0ea42c45\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":\"1812.04794\",\"authors\":[{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"1715610\",\"name\":\"Qi Wu\"},{\"authorId\":\"39767248\",\"name\":\"Jiewei Cao\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2019.00206\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8ca91ad7763be4da05238aa17a9e5628f619dc0b\",\"title\":\"Neighbourhood Watch: Referring Expression Comprehension via Language-Guided Graph Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/8ca91ad7763be4da05238aa17a9e5628f619dc0b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"16726627\",\"name\":\"C. Liu\"},{\"authorId\":\"2785372\",\"name\":\"Ding-Jie Chen\"},{\"authorId\":\"1803730\",\"name\":\"Hwann-Tzong Chen\"},{\"authorId\":\"1805102\",\"name\":\"Tyng-Luh Liu\"}],\"doi\":\"10.1007/978-3-030-20876-9_26\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1687d0120e937d5efe2022cbeab19b38edba0608\",\"title\":\"A2A: Attention to Attention Reasoning for Movie Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1687d0120e937d5efe2022cbeab19b38edba0608\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":\"1906.00283\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"122175026\",\"name\":\"P\\u00e9ter Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"title\":\"Learning to Generate Grounded Image Captions without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443736039\",\"name\":\"Yuki Mori\"},{\"authorId\":\"46362679\",\"name\":\"Hiroshi Fukui\"},{\"authorId\":\"134790239\",\"name\":\"Tsubasa Hirakawa\"},{\"authorId\":\"1443785402\",\"name\":\"Jo Nishiyama\"},{\"authorId\":\"1687819\",\"name\":\"T. Yamashita\"},{\"authorId\":\"1687968\",\"name\":\"H. Fujiyoshi\"}],\"doi\":\"10.1109/ITSC.2019.8917187\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8fe7a6857906af4adb5b1e92d90279257811965\",\"title\":\"Attention Neural Baby Talk: Captioning of Risk Factors while Driving\",\"url\":\"https://www.semanticscholar.org/paper/b8fe7a6857906af4adb5b1e92d90279257811965\",\"venue\":\"2019 IEEE Intelligent Transportation Systems Conference (ITSC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52150251\",\"name\":\"A. Goel\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"103192742\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"3d807fed82f61bc48aea76f191d2e19e2fc38f5d\",\"title\":\"Learning to Caption Images with Two-Stream Attention and Sentence Auto-Encoder\",\"url\":\"https://www.semanticscholar.org/paper/3d807fed82f61bc48aea76f191d2e19e2fc38f5d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2002.04355\",\"authors\":[{\"authorId\":\"1471439125\",\"name\":\"\\u015eeymanur Akt\\u0131\"},{\"authorId\":\"1471431070\",\"name\":\"G\\u00f6zde Ay\\u015fe Tataro\\u011flu\"},{\"authorId\":\"32365318\",\"name\":\"H. K. Ekenel\"}],\"doi\":\"10.1109/IPTA.2019.8936070\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41a8954131f313d8526b2f8fc0506405ec519819\",\"title\":\"Vision-based Fight Detection from Surveillance Cameras\",\"url\":\"https://www.semanticscholar.org/paper/41a8954131f313d8526b2f8fc0506405ec519819\",\"venue\":\"2019 Ninth International Conference on Image Processing Theory, Tools and Applications (IPTA)\",\"year\":2019},{\"arxivId\":\"1909.09060\",\"authors\":[{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"1788029\",\"name\":\"W. Wang\"},{\"authorId\":\"49289638\",\"name\":\"Y. Xia\"},{\"authorId\":\"40445654\",\"name\":\"J. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"312799645adfafb886f156708a7a36f2db459c62\",\"title\":\"Adaptively Aligned Image Captioning via Adaptive Attention Time\",\"url\":\"https://www.semanticscholar.org/paper/312799645adfafb886f156708a7a36f2db459c62\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1805.03508\",\"authors\":[{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"50812077\",\"name\":\"J. Yu\"},{\"authorId\":\"24071345\",\"name\":\"Chenchao Xiang\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.24963/ijcai.2018/155\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"7102c13c3a98d872d31369c778261736808aa32f\",\"title\":\"Rethinking Diversified and Discriminative Proposal Generation for Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/7102c13c3a98d872d31369c778261736808aa32f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1909.02097\",\"authors\":[{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"144865339\",\"name\":\"Bo Pang\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/D19-1155\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3b6e6822eabe2f64192a1965c23e38043866319c\",\"title\":\"Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3b6e6822eabe2f64192a1965c23e38043866319c\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1982492\",\"name\":\"Seungho Han\"},{\"authorId\":\"145530103\",\"name\":\"H. Choi\"}],\"doi\":\"10.1109/BigComp48618.2020.00-12\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ba1575071c2e928dfc35380ef7c4cb1e849771f5\",\"title\":\"Domain-Specific Image Caption Generator with Semantic Ontology\",\"url\":\"https://www.semanticscholar.org/paper/ba1575071c2e928dfc35380ef7c4cb1e849771f5\",\"venue\":\"2020 IEEE International Conference on Big Data and Smart Computing (BigComp)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49116303\",\"name\":\"Yue Qiu\"},{\"authorId\":\"1732705\",\"name\":\"Y. Satoh\"},{\"authorId\":\"144410256\",\"name\":\"Ryota Suzuki\"},{\"authorId\":\"35206224\",\"name\":\"K. Iwata\"}],\"doi\":\"10.1007/978-3-030-50334-5_26\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee2dc587ff68e353fc47e0b2ad25e402fe87c57f\",\"title\":\"Multi-view Visual Question Answering Dataset for Real Environment Applications\",\"url\":\"https://www.semanticscholar.org/paper/ee2dc587ff68e353fc47e0b2ad25e402fe87c57f\",\"venue\":\"HCI\",\"year\":2020},{\"arxivId\":\"1907.01166\",\"authors\":[{\"authorId\":\"143725625\",\"name\":\"Hung Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/P19-1564\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"594ad264d6b92afb9d13cb56ad8ffadba94a9f7a\",\"title\":\"Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/594ad264d6b92afb9d13cb56ad8ffadba94a9f7a\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1812.08989\",\"authors\":[{\"authorId\":\"49718206\",\"name\":\"L. Zhou\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"49620738\",\"name\":\"Di Li\"},{\"authorId\":\"144154486\",\"name\":\"H. Shum\"}],\"doi\":\"10.1162/coli_a_00368\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"66b7d31527f980bb2eecc23629f08ba6037facc4\",\"title\":\"The Design and Implementation of XiaoIce, an Empathetic Social Chatbot\",\"url\":\"https://www.semanticscholar.org/paper/66b7d31527f980bb2eecc23629f08ba6037facc4\",\"venue\":\"Computational Linguistics\",\"year\":2018},{\"arxivId\":\"2007.01072\",\"authors\":[{\"authorId\":\"1491246679\",\"name\":\"M. Hildebrandt\"},{\"authorId\":\"47892918\",\"name\":\"H. Li\"},{\"authorId\":\"10774714\",\"name\":\"Rajat Koner\"},{\"authorId\":\"1742501819\",\"name\":\"Volker Tresp\"},{\"authorId\":\"3075189\",\"name\":\"Stephan G\\u00fcnnemann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59f14747ef7ba486fd45664f3902e3ad7b395a0d\",\"title\":\"Scene Graph Reasoning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/59f14747ef7ba486fd45664f3902e3ad7b395a0d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.04672\",\"authors\":[{\"authorId\":\"40857101\",\"name\":\"Yuewei Yang\"},{\"authorId\":\"144130492\",\"name\":\"Kevin J Liang\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a8acb61589352819b96e0642d5dcb2c78bac03b\",\"title\":\"Object Detection as a Positive-Unlabeled Problem\",\"url\":\"https://www.semanticscholar.org/paper/3a8acb61589352819b96e0642d5dcb2c78bac03b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"}],\"doi\":\"10.1109/ICCVW.2019.00536\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"title\":\"EgoVQA - An Egocentric Video Question Answering Benchmark Dataset\",\"url\":\"https://www.semanticscholar.org/paper/71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"2012.02951\",\"authors\":[{\"authorId\":\"1935410\",\"name\":\"M. Rahnemoonfar\"},{\"authorId\":\"39734189\",\"name\":\"Tashnim J. S. Chowdhury\"},{\"authorId\":\"1381535116\",\"name\":\"Argho Sarkar\"},{\"authorId\":\"1411385478\",\"name\":\"D. Varshney\"},{\"authorId\":\"1505798397\",\"name\":\"Masoud Yari\"},{\"authorId\":\"1789429\",\"name\":\"R. Murphy\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"af2550268c34fb0376a3f18f150bf4f9ce3023b9\",\"title\":\"FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding\",\"url\":\"https://www.semanticscholar.org/paper/af2550268c34fb0376a3f18f150bf4f9ce3023b9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72211602\",\"name\":\"Kai Rannenberg\"}],\"doi\":\"10.1007/978-3-030-46931-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d37509fc390147f2ab8e3746c3abb208b3cfc36f\",\"title\":\"Intelligent Information Processing X: 11th IFIP TC 12 International Conference, IIP 2020, Hangzhou, China, July 3\\u20136, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/d37509fc390147f2ab8e3746c3abb208b3cfc36f\",\"venue\":\"Intelligent Information Processing\",\"year\":2020},{\"arxivId\":\"2007.00398\",\"authors\":[{\"authorId\":\"34317896\",\"name\":\"M. Mathew\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b40bfcf339de3f0dba08fabb2b58b9368ff4c51a\",\"title\":\"DocVQA: A Dataset for VQA on Document Images\",\"url\":\"https://www.semanticscholar.org/paper/b40bfcf339de3f0dba08fabb2b58b9368ff4c51a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47002278\",\"name\":\"Yikuan Li\"},{\"authorId\":\"51464971\",\"name\":\"Hanyin Wang\"},{\"authorId\":\"1830568527\",\"name\":\"Yuan Luo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff554f6228cf1f939a0e9e44ada06ef9cd28be15\",\"title\":\"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports\",\"url\":\"https://www.semanticscholar.org/paper/ff554f6228cf1f939a0e9e44ada06ef9cd28be15\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.08617\",\"authors\":[{\"authorId\":\"84267967\",\"name\":\"C. Thomas\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-58523-5_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"432921b7a2c782cedc2a7d87b6194b906e31086d\",\"title\":\"Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/432921b7a2c782cedc2a7d87b6194b906e31086d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.02709\",\"authors\":[{\"authorId\":\"48806403\",\"name\":\"Zhenxing Zhang\"},{\"authorId\":\"52643489\",\"name\":\"L. Schomaker\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6fe68c8c970ba707e8767e5010c32b3cb1033063\",\"title\":\"DTGAN: Dual Attention Generative Adversarial Networks for Text-to-Image Generation\",\"url\":\"https://www.semanticscholar.org/paper/6fe68c8c970ba707e8767e5010c32b3cb1033063\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.00966\",\"authors\":[{\"authorId\":\"32370882\",\"name\":\"Shweta Mahajan\"},{\"authorId\":\"49863405\",\"name\":\"S. Roth\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d805e35f17dab7382748130a2ca1bda629cdceb\",\"title\":\"Diverse Image Captioning with Context-Object Split Latent Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7d805e35f17dab7382748130a2ca1bda629cdceb\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2011.10972\",\"authors\":[{\"authorId\":\"49039823\",\"name\":\"Weixia Zhang\"},{\"authorId\":\"1409866378\",\"name\":\"Chao Ma\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"15469693\",\"name\":\"Xiaokang Yang\"}],\"doi\":\"10.1109/TCSVT.2020.3039522\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"38bb24348dbcec08285a8670596ec7c9b3895603\",\"title\":\"Language-guided Navigation via Cross-Modal Grounding and Alternate Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/38bb24348dbcec08285a8670596ec7c9b3895603\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"104150443\",\"name\":\"Zeqin Huang\"},{\"authorId\":\"9118491\",\"name\":\"Zhongzhi Shi\"}],\"doi\":\"10.1007/978-3-030-46931-3_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f08ac1aa8bfc804414b1e5a2bb98203337368807\",\"title\":\"Image Caption Combined with GAN Training Method\",\"url\":\"https://www.semanticscholar.org/paper/f08ac1aa8bfc804414b1e5a2bb98203337368807\",\"venue\":\"Intelligent Information Processing\",\"year\":2020},{\"arxivId\":\"2005.03492\",\"authors\":[{\"authorId\":\"66273773\",\"name\":\"Xiaoxue Chen\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"},{\"authorId\":\"49780704\",\"name\":\"Yuanzhi Zhu\"},{\"authorId\":\"30099960\",\"name\":\"Canjie Luo\"},{\"authorId\":\"3008041\",\"name\":\"T. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"708ed0964e998dc12ffb37d93719c481c417d87a\",\"title\":\"Text Recognition in the Wild: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/708ed0964e998dc12ffb37d93719c481c417d87a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00200\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1664725279\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1520007550\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.161\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"title\":\"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1911.11390\",\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1007/978-3-030-58586-0_14\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"468d5c15df63892ff06fb94c7b5cad0242685d02\",\"title\":\"Efficient Attention Mechanism for Visual Dialog that Can Handle All the Interactions Between Multiple Inputs\",\"url\":\"https://www.semanticscholar.org/paper/468d5c15df63892ff06fb94c7b5cad0242685d02\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.03708\",\"authors\":[{\"authorId\":\"80389349\",\"name\":\"Zhuowan Li\"},{\"authorId\":\"2536742\",\"name\":\"Quan Hung Tran\"},{\"authorId\":\"2712573\",\"name\":\"Long Mai\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/CVPR42600.2020.00350\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"title\":\"Context-Aware Group Captioning via Self-Attention and Contrastive Features\",\"url\":\"https://www.semanticscholar.org/paper/477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2434622\",\"name\":\"Z. Seymour\"},{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"35260743\",\"name\":\"Han-Pang Chiu\"},{\"authorId\":\"1789477\",\"name\":\"S. Samarasekera\"},{\"authorId\":\"145539777\",\"name\":\"R. Kumar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aed34b3687cebbba987640d5888f34c0a025b996\",\"title\":\"Semantically-Aware Attentive Neural Embeddings for 2D Long-Term Visual Localization\",\"url\":\"https://www.semanticscholar.org/paper/aed34b3687cebbba987640d5888f34c0a025b996\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1707938\",\"name\":\"K. Shirahama\"},{\"authorId\":\"46476910\",\"name\":\"Daichi Sakurai\"},{\"authorId\":\"144872058\",\"name\":\"Takashi Matsubara\"},{\"authorId\":\"70339089\",\"name\":\"K. Uehara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66b39cb0fe6bdb33273b8e737a6fa9a3c9850656\",\"title\":\"Kindai University and Kobe University at TRECVID 2019 AVS Task\",\"url\":\"https://www.semanticscholar.org/paper/66b39cb0fe6bdb33273b8e737a6fa9a3c9850656\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23364558\",\"name\":\"Nilavra Bhattacharya\"},{\"authorId\":\"101489041\",\"name\":\"Q. Li\"},{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6a9293145a9a3a7ca42b18a437e0bce67a359127\",\"title\":\"Reason Label Description Issues with the Question-Image ( QI ) pair Low Quality\",\"url\":\"https://www.semanticscholar.org/paper/6a9293145a9a3a7ca42b18a437e0bce67a359127\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.12679\",\"authors\":[{\"authorId\":\"1500734790\",\"name\":\"Weidong Yin\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"472353d4642f96b2f713e50a2d882b50d0d2dda1\",\"title\":\"Person-in-Context Synthesiswith Compositional Structural Space\",\"url\":\"https://www.semanticscholar.org/paper/472353d4642f96b2f713e50a2d882b50d0d2dda1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kushal Kafle\"},{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e3bb5773205477ae4711524a9d4ae739bee40349\",\"title\":\"Visual semantic role labeling requires recognizing activities and semantic context in images\",\"url\":\"https://www.semanticscholar.org/paper/e3bb5773205477ae4711524a9d4ae739bee40349\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.10797\",\"authors\":[{\"authorId\":\"47060391\",\"name\":\"Jia Huei Tan\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"},{\"authorId\":\"38900275\",\"name\":\"Joon Huang Chuah\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"04a49009833179d29c541d5775d2dc2f2a0adf4a\",\"title\":\"Image Captioning with Sparse Recurrent Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/04a49009833179d29c541d5775d2dc2f2a0adf4a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47440120\",\"name\":\"B. Chen\"},{\"authorId\":\"46389392\",\"name\":\"Hanzhi Ma\"},{\"authorId\":\"49264154\",\"name\":\"Junjie He\"},{\"authorId\":\"51286297\",\"name\":\"Yinzhang Ding\"},{\"authorId\":\"7974850\",\"name\":\"Liang-Hao Wang\"},{\"authorId\":\"7379096\",\"name\":\"Dongxiao Li\"},{\"authorId\":\"73212130\",\"name\":\"M. Zhang\"}],\"doi\":\"10.1109/ACCESS.2019.2944925\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f91e4b99bd615bf3b78d2274e74f82dcaf8b26a\",\"title\":\"PAM: Pyramid Attention Mechanism Based on Contextual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/4f91e4b99bd615bf3b78d2274e74f82dcaf8b26a\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1910.07416\",\"authors\":[{\"authorId\":\"19248639\",\"name\":\"Sadaf Gulshad\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"2708564\",\"name\":\"Jan Hendrik Metzen\"},{\"authorId\":\"144638781\",\"name\":\"Arnold W. M. Smeulders\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3dfbffec4c4d47e3111d48e5f6ab38eb2e1b06b\",\"title\":\"Understanding Misclassifications by Attributes\",\"url\":\"https://www.semanticscholar.org/paper/c3dfbffec4c4d47e3111d48e5f6ab38eb2e1b06b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400258083\",\"name\":\"Chunpu Xu\"},{\"authorId\":\"47748857\",\"name\":\"Wei Zhao\"},{\"authorId\":\"31991405\",\"name\":\"Min Yang\"},{\"authorId\":\"2441161\",\"name\":\"Xiang Ao\"},{\"authorId\":\"1405918472\",\"name\":\"Wangrong Cheng\"},{\"authorId\":\"1936983\",\"name\":\"J. Tian\"}],\"doi\":\"10.1145/3357384.3358105\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e81c97c18cb4f4922e4442664350350536a71a13\",\"title\":\"A Unified Generation-Retrieval Framework for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e81c97c18cb4f4922e4442664350350536a71a13\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1583849717\",\"name\":\"Sabarish Gopalarishnan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8bf6fa5dc3bfcf1974bd07a2f89e5b8811f1dec3\",\"title\":\"Vector Spaces for Multiple Modal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/8bf6fa5dc3bfcf1974bd07a2f89e5b8811f1dec3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47111044\",\"name\":\"Mingqin Chen\"},{\"authorId\":null,\"name\":\"Yilei Wang\"},{\"authorId\":\"145675052\",\"name\":\"Shan Chen\"},{\"authorId\":\"48607932\",\"name\":\"Yingjie Wu\"}],\"doi\":\"10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00167\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f69228901c1bb6a139a96fab87e1a5827ba08a2a\",\"title\":\"Counting Attention Based on Classification Confidence for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f69228901c1bb6a139a96fab87e1a5827ba08a2a\",\"venue\":\"2019 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c1693e1defad1cc8ec36b061add2afcd564013ff\",\"title\":\"Advancing Multi-Modal Deep Learning: Towards Language-Grounded Visual Understanding\",\"url\":\"https://www.semanticscholar.org/paper/c1693e1defad1cc8ec36b061add2afcd564013ff\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52433641\",\"name\":\"Pilin Dai\"},{\"authorId\":\"3313556\",\"name\":\"Jinna Lv\"},{\"authorId\":\"49814386\",\"name\":\"B. Wu\"}],\"doi\":\"10.1109/ICME.2019.00198\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"959dc25edc722b395fbdfd3b77ec670d81d9c6c3\",\"title\":\"Two-Stage Model for Social Relationship Understanding from Videos\",\"url\":\"https://www.semanticscholar.org/paper/959dc25edc722b395fbdfd3b77ec670d81d9c6c3\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1802.08218\",\"authors\":[{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"},{\"authorId\":\"48933900\",\"name\":\"Q. Li\"},{\"authorId\":\"32027456\",\"name\":\"A. J. Stangl\"},{\"authorId\":\"2582404\",\"name\":\"Anhong Guo\"},{\"authorId\":\"47532530\",\"name\":\"Chi Lin\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1744846\",\"name\":\"Jeffrey P. Bigham\"}],\"doi\":\"10.1109/CVPR.2018.00380\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c\",\"title\":\"VizWiz Grand Challenge: Answering Visual Questions from Blind People\",\"url\":\"https://www.semanticscholar.org/paper/a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4237bdd7df710c08b58d02bb48f8dcecd2521a80\",\"title\":\"Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/4237bdd7df710c08b58d02bb48f8dcecd2521a80\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1965909970\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"1755773\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930562\",\"name\":\"G. Chen\"},{\"authorId\":\"153016830\",\"name\":\"J. Guo\"}],\"doi\":\"10.1109/ACCESS.2020.3021857\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"title\":\"Understanding Objects in Video: Object-Oriented Video Captioning via Structured Trajectory and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2008.01593\",\"authors\":[{\"authorId\":\"2118630\",\"name\":\"Junchi Liang\"},{\"authorId\":\"2209847\",\"name\":\"Abdeslam Boularias\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6b5e13841d1c5fbfedf9e4f886dbd3e81aed8100\",\"title\":\"Learning Transition Models with Time-delayed Causal Relations\",\"url\":\"https://www.semanticscholar.org/paper/6b5e13841d1c5fbfedf9e4f886dbd3e81aed8100\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03749\",\"authors\":[{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1109/CVPR42600.2020.01090\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"96485bda4f4118da249cc8a898230281ac8040a7\",\"title\":\"Better Captioning With Sequence-Level Exploration\",\"url\":\"https://www.semanticscholar.org/paper/96485bda4f4118da249cc8a898230281ac8040a7\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51907635\",\"name\":\"I. Khurram\"},{\"authorId\":\"1756409\",\"name\":\"M. M. Fraz\"},{\"authorId\":\"1380493605\",\"name\":\"M. Shahzad\"},{\"authorId\":\"2229652\",\"name\":\"N. Rajpoot\"}],\"doi\":\"10.1007/s12559-019-09697-1\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"e6b2728a7d0677e045a50203b574ea569a20cdf2\",\"title\":\"Dense-CaptionNet: a Sentence Generation Architecture for Fine-grained Description of Image Semantics\",\"url\":\"https://www.semanticscholar.org/paper/e6b2728a7d0677e045a50203b574ea569a20cdf2\",\"venue\":\"Cognitive Computation\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yiqi Yan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"25102a9c3f6aab9d2b1c990f8a917872dabcaae9\",\"title\":\"Attention-based skin lesion recognition\",\"url\":\"https://www.semanticscholar.org/paper/25102a9c3f6aab9d2b1c990f8a917872dabcaae9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1453661830\",\"name\":\"Keyu Wen\"},{\"authorId\":\"27698074\",\"name\":\"X. Gu\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206782\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1a5d86ff2012a868a23c0defb83c90400e30706\",\"title\":\"Dual Semantic Relationship Attention Network for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/b1a5d86ff2012a868a23c0defb83c90400e30706\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"2007.06877\",\"authors\":[{\"authorId\":\"51175635\",\"name\":\"Jiuniu Wang\"},{\"authorId\":\"50232214\",\"name\":\"Wenjia Xu\"},{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":\"10.1007/978-3-030-58452-8_22\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e38274d1d544af5fad2cb814d62a028e2ff7437b\",\"title\":\"Compare and Reweight: Distinctive Image Captioning Using Similar Images Sets\",\"url\":\"https://www.semanticscholar.org/paper/e38274d1d544af5fad2cb814d62a028e2ff7437b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2490430\",\"name\":\"Carina Silberer\"},{\"authorId\":\"1721364\",\"name\":\"Sina Zarrie\\u00df\"},{\"authorId\":\"2746394\",\"name\":\"M. Westera\"},{\"authorId\":\"1807810\",\"name\":\"Gemma Boleda\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e7155b150abf033bc3a0c24e16202777c9d41367\",\"title\":\"Humans Meet Models on Object Naming: A New Dataset and Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e7155b150abf033bc3a0c24e16202777c9d41367\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49116303\",\"name\":\"Yue Qiu\"},{\"authorId\":\"1732705\",\"name\":\"Y. Satoh\"},{\"authorId\":\"144410256\",\"name\":\"Ryota Suzuki\"},{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"}],\"doi\":\"10.1109/3DV.2019.00088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c03f85185134c3c5c0f0874dc6d5538ae529059\",\"title\":\"Incorporating 3D Information Into Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3c03f85185134c3c5c0f0874dc6d5538ae529059\",\"venue\":\"2019 International Conference on 3D Vision (3DV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1582059215\",\"name\":\"Austin Kershaw\"},{\"authorId\":\"144048413\",\"name\":\"M. Bober\"}],\"doi\":\"10.18653/v1/W19-0603\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"25cf62b795a84896f2d2dc2e4f770ed8edc874a6\",\"title\":\"The Lexical Gap: An Improved Measure of Automated Image Description Quality\",\"url\":\"https://www.semanticscholar.org/paper/25cf62b795a84896f2d2dc2e4f770ed8edc874a6\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144382769\",\"name\":\"Viviana Beltr\\u00e1n\"},{\"authorId\":\"1748667\",\"name\":\"N. Journet\"},{\"authorId\":\"1732746\",\"name\":\"Micka\\u00ebl Coustaty\"},{\"authorId\":\"34796546\",\"name\":\"A. Doucet\"}],\"doi\":\"10.1109/ICDARW.2019.40088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb38a28c53dc97b2b95a7fb2915a39a3f997e4fd\",\"title\":\"Semantic Text Recognition via Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/cb38a28c53dc97b2b95a7fb2915a39a3f997e4fd\",\"venue\":\"2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153504695\",\"name\":\"Hao Zhang\"},{\"authorId\":\"47835286\",\"name\":\"Qing-fang Zhang\"},{\"authorId\":\"9122533\",\"name\":\"Siyu Shao\"},{\"authorId\":\"67022579\",\"name\":\"T. Niu\"},{\"authorId\":\"2595119\",\"name\":\"X. Yang\"}],\"doi\":\"10.1109/ACCESS.2020.3010066\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d909e28f89fd4b3e7f70b80882056847df3b13b1\",\"title\":\"Attention-Based LSTM Network for Rotatory Machine Remaining Useful Life Prediction\",\"url\":\"https://www.semanticscholar.org/paper/d909e28f89fd4b3e7f70b80882056847df3b13b1\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47056886\",\"name\":\"Xiangpeng Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"6820648\",\"name\":\"Xianglong Liu\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1609/AAAI.V33I01.33018658\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"565359aac8914505e6b02db05822ee63d3ffd03a\",\"title\":\"Beyond RNNs: Positional Self-Attention with Co-Attention for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/565359aac8914505e6b02db05822ee63d3ffd03a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"51491153\",\"name\":\"Takayuki Itamochi\"},{\"authorId\":\"3193466\",\"name\":\"K. Narioka\"},{\"authorId\":\"152599661\",\"name\":\"Ikuro Sato\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":\"10.1109/ICCV.2019.00589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"949959250e487617faa3035b05b52aa03a8e9895\",\"title\":\"Generating Easy-to-Understand Referring Expressions for Target Identifications\",\"url\":\"https://www.semanticscholar.org/paper/949959250e487617faa3035b05b52aa03a8e9895\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1812.09041\",\"authors\":[{\"authorId\":\"153198570\",\"name\":\"Guoyun Tu\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"11004839\",\"name\":\"Jiarui Gao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/TMM.2019.2922129\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41b76703b03ecb40dbcc00e9fbf6a73b0b808778\",\"title\":\"A Multi-Task Neural Approach for Emotion Attribution, Classification, and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/41b76703b03ecb40dbcc00e9fbf6a73b0b808778\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144196376\",\"name\":\"Z. Fang\"},{\"authorId\":\"2505157\",\"name\":\"Dezhi Hong\"},{\"authorId\":\"145170139\",\"name\":\"R. Gupta\"}],\"doi\":\"10.1145/3304109.3306221\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"01fa5eb436db99529af2ab69722416afcb03d03e\",\"title\":\"Serving deep neural networks at the cloud edge for vision applications on mobile platforms\",\"url\":\"https://www.semanticscholar.org/paper/01fa5eb436db99529af2ab69722416afcb03d03e\",\"venue\":\"MMSys\",\"year\":2019},{\"arxivId\":\"2002.11863\",\"authors\":[{\"authorId\":\"51431831\",\"name\":\"Chuang Niu\"},{\"authorId\":null,\"name\":\"Jun Zhang\"},{\"authorId\":\"30712173\",\"name\":\"G. Wang\"},{\"authorId\":\"90847237\",\"name\":\"Jimin Liang\"}],\"doi\":\"10.1007/978-3-030-58595-2_44\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c8b7687f74880814d0969d52b7f056dae1850b9d\",\"title\":\"GATCluster: Self-Supervised Gaussian-Attention Network for Image Clustering\",\"url\":\"https://www.semanticscholar.org/paper/c8b7687f74880814d0969d52b7f056dae1850b9d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2034270322\",\"name\":\"Liyana Sahir Kallooriyakath\"},{\"authorId\":\"2034269084\",\"name\":\"Jithin M V\"},{\"authorId\":\"81431088\",\"name\":\"B. V\"},{\"authorId\":\"2034269088\",\"name\":\"Adith P P\"}],\"doi\":\"10.1109/ICSTCEE49637.2020.9277374\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ea0ab46474037363b0a52b758538e61ccb90ecec\",\"title\":\"Visual Question Answering: Methodologies and Challenges\",\"url\":\"https://www.semanticscholar.org/paper/ea0ab46474037363b0a52b758538e61ccb90ecec\",\"venue\":\"2020 International Conference on Smart Technologies in Computing, Electrical and Electronics (ICSTCEE)\",\"year\":2020},{\"arxivId\":\"2012.03662\",\"authors\":[{\"authorId\":\"50218156\",\"name\":\"Zhaokai Wang\"},{\"authorId\":\"7760591\",\"name\":\"Renda Bao\"},{\"authorId\":\"1720738301\",\"name\":\"Qi Wu\"},{\"authorId\":\"101219260\",\"name\":\"Si Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"b1fe7a16266cf2316f436688e0df6c6350c885ef\",\"title\":\"Confidence-aware Non-repetitive Multimodal Transformers for TextCaps\",\"url\":\"https://www.semanticscholar.org/paper/b1fe7a16266cf2316f436688e0df6c6350c885ef\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144485908\",\"name\":\"Y. Long\"},{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"47390553\",\"name\":\"Zhihua Wei\"},{\"authorId\":\"73723234\",\"name\":\"Jinjing Gu\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"}],\"doi\":\"10.1016/j.ins.2020.04.034\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb8f7cb96782c422fdf67c48bcfef10b635cd6f0\",\"title\":\"RepeatPadding: Balancing words and sentence length for language comprehension in visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/eb8f7cb96782c422fdf67c48bcfef10b635cd6f0\",\"venue\":\"Inf. Sci.\",\"year\":2020},{\"arxivId\":\"1910.05134\",\"authors\":[{\"authorId\":\"49184936\",\"name\":\"S. Wang\"},{\"authorId\":\"3373117\",\"name\":\"R. Wang\"},{\"authorId\":\"12977859\",\"name\":\"Z. Yao\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1109/WACV45572.2020.9093614\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9f1b9e51c1d16c346e1ef679c243d61702a8a80\",\"title\":\"Cross-modal Scene Graph Matching for Relationship-aware Image-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/c9f1b9e51c1d16c346e1ef679c243d61702a8a80\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2003.03107\",\"authors\":[{\"authorId\":\"32095408\",\"name\":\"Fawaz Sammani\"},{\"authorId\":\"1411260673\",\"name\":\"Luke Melas-Kyriazi\"}],\"doi\":\"10.1109/cvpr42600.2020.00486\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e9419436682726232e1b37a04c53bba919b12025\",\"title\":\"Show, Edit and Tell: A Framework for Editing Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/e9419436682726232e1b37a04c53bba919b12025\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1909.05693\",\"authors\":[{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"101377061\",\"name\":\"Zizhou Jia\"},{\"authorId\":\"144600412\",\"name\":\"H. Chen\"},{\"authorId\":\"2091623\",\"name\":\"L. Li\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1732330\",\"name\":\"K. Keutzer\"}],\"doi\":\"10.1145/3343031.3351062\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12c258108ab59ef0189a6deb47222a99269da212\",\"title\":\"PDANet: Polarity-consistent Deep Attention Network for Fine-grained Visual Emotion Regression\",\"url\":\"https://www.semanticscholar.org/paper/12c258108ab59ef0189a6deb47222a99269da212\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1908.11216\",\"authors\":[{\"authorId\":\"1384082144\",\"name\":\"A. Garcia\"},{\"authorId\":\"46985469\",\"name\":\"Pierre Colombo\"},{\"authorId\":\"1807587\",\"name\":\"S. Essid\"},{\"authorId\":\"1389671466\",\"name\":\"Florence d'Alch\\u00e9-Buc\"},{\"authorId\":\"2049106\",\"name\":\"C. Clavel\"}],\"doi\":\"10.18653/v1/D19-1556\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"901b18363a1673cb9cf14a7fd86b023dc8450a6b\",\"title\":\"From the Token to the Review: A Hierarchical Multimodal approach to Opinion Mining\",\"url\":\"https://www.semanticscholar.org/paper/901b18363a1673cb9cf14a7fd86b023dc8450a6b\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1908.06327\",\"authors\":[{\"authorId\":\"38727845\",\"name\":\"A. Burns\"},{\"authorId\":\"73441526\",\"name\":\"R. Tan\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"}],\"doi\":\"10.1109/ICCV.2019.00757\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"71634edd80ce000b3d1e462137fcfa8c2b377943\",\"title\":\"Language Features Matter: Effective Language Representations for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/71634edd80ce000b3d1e462137fcfa8c2b377943\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51432978\",\"name\":\"Zhongke Liao\"},{\"authorId\":\"46354059\",\"name\":\"Haifeng Hu\"},{\"authorId\":\"49050918\",\"name\":\"J. Zhang\"},{\"authorId\":\"7650248\",\"name\":\"C. Yin\"}],\"doi\":\"10.1016/j.cviu.2019.102821\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fedf56f95e5e80464254573ce2d9648606899ccb\",\"title\":\"Residual attention unit for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/fedf56f95e5e80464254573ce2d9648606899ccb\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144600412\",\"name\":\"H. Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1387712541\",\"name\":\"Zijin Lin\"},{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"144762952\",\"name\":\"J. Han\"}],\"doi\":\"10.1145/3343031.3351055\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42001225313e0f5376a8f4b1759e687225cd9d00\",\"title\":\"Cross-Modal Image-Text Retrieval with Semantic Consistency\",\"url\":\"https://www.semanticscholar.org/paper/42001225313e0f5376a8f4b1759e687225cd9d00\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"147331977\",\"name\":\"Lu Liang\"},{\"authorId\":\"117412308\",\"name\":\"J. Cao\"},{\"authorId\":\"145087456\",\"name\":\"X. Li\"},{\"authorId\":\"144329386\",\"name\":\"J. You\"}],\"doi\":\"10.1007/978-3-030-36189-1_44\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a64d4b1fa2e6f8012d1deb28cd5f8a4e493ea3e3\",\"title\":\"Improvement of Residual Attention Network for Image Classification\",\"url\":\"https://www.semanticscholar.org/paper/a64d4b1fa2e6f8012d1deb28cd5f8a4e493ea3e3\",\"venue\":\"IScIDE\",\"year\":2019},{\"arxivId\":\"1912.02379\",\"authors\":[{\"authorId\":\"46258988\",\"name\":\"Vishvak S. Murahari\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"}],\"doi\":\"10.1007/978-3-030-58523-5_20\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"604d7678235f5bb6039794e382d12058cecf8070\",\"title\":\"Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline\",\"url\":\"https://www.semanticscholar.org/paper/604d7678235f5bb6039794e382d12058cecf8070\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1810.09630\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"345a222fef6f5c1415056319ae7e87a369940d3f\",\"title\":\"A Neural Compositional Paradigm for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/345a222fef6f5c1415056319ae7e87a369940d3f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1903.12314\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145215470\",\"name\":\"Yu Cheng\"},{\"authorId\":\"38079056\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.1109/ICCV.2019.01041\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d379ba96b8f400b23b2cd72c428af67e578959ea\",\"title\":\"Relation-Aware Graph Attention Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/d379ba96b8f400b23b2cd72c428af67e578959ea\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2012.09742\",\"authors\":[{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"49336560\",\"name\":\"Weining Wang\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb3f19de52d330ded5b3eebb79b89876648f67cb\",\"title\":\"AutoCaption: Image Captioning with Neural Architecture Search\",\"url\":\"https://www.semanticscholar.org/paper/eb3f19de52d330ded5b3eebb79b89876648f67cb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.03000\",\"authors\":[{\"authorId\":\"7229001\",\"name\":\"Johann Sawatzky\"},{\"authorId\":\"1889486\",\"name\":\"Yaser Souri\"},{\"authorId\":\"144306299\",\"name\":\"C. Grund\"},{\"authorId\":\"145689714\",\"name\":\"Juergen Gall\"}],\"doi\":\"10.1109/CVPR.2019.00779\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7b36024652d99cdf1f75a1daa891729ef0aa0cb\",\"title\":\"What Object Should I Use? - Task Driven Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/e7b36024652d99cdf1f75a1daa891729ef0aa0cb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2005.07310\",\"authors\":[{\"authorId\":\"1701219797\",\"name\":\"Jize Cao\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"46700583\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.1007/978-3-030-58539-6_34\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"26cfb57a9722599b361858d454ec816420723e36\",\"title\":\"Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\",\"url\":\"https://www.semanticscholar.org/paper/26cfb57a9722599b361858d454ec816420723e36\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.14095\",\"authors\":[{\"authorId\":\"25263842\",\"name\":\"Aisha Urooj Khan\"},{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"1700665\",\"name\":\"N. Lobo\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.417\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf8a0cda4f26c889d1b6d16fe070fa3d907a8686\",\"title\":\"MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bf8a0cda4f26c889d1b6d16fe070fa3d907a8686\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.08708\",\"authors\":[{\"authorId\":\"2728646\",\"name\":\"Hantao Huang\"},{\"authorId\":\"145421604\",\"name\":\"T. Han\"},{\"authorId\":\"72549949\",\"name\":\"Wei Han\"},{\"authorId\":\"48986588\",\"name\":\"D. Yap\"},{\"authorId\":\"32312400\",\"name\":\"C. Chiang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"91550d1490caa39f723d3efb5d1b78b6f8b7c6cf\",\"title\":\"Answer-checking in Context: A Multi-modal FullyAttention Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/91550d1490caa39f723d3efb5d1b78b6f8b7c6cf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.05608\",\"authors\":[{\"authorId\":\"11007025\",\"name\":\"Junjiao Tian\"},{\"authorId\":\"143904954\",\"name\":\"Jean Oh\"}],\"doi\":\"10.24963/ijcai.2019/496\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"18403a06a67b7060645e137a36ad15122ee2c2f9\",\"title\":\"Image Captioning with Compositional Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/18403a06a67b7060645e137a36ad15122ee2c2f9\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2005.01239\",\"authors\":[{\"authorId\":\"114180826\",\"name\":\"Violetta Shevchenko\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b8a3c338551f9c512868bc89217f95bbce69b1a8\",\"title\":\"Visual Question Answering with Prior Class Semantics\",\"url\":\"https://www.semanticscholar.org/paper/b8a3c338551f9c512868bc89217f95bbce69b1a8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.08325\",\"authors\":[{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.1007/978-3-030-58589-1_23\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"558984f821c4b9fb9aacf7441cf9ca6d303be836\",\"title\":\"VQA-LOL: Visual Question Answering under the Lens of Logic\",\"url\":\"https://www.semanticscholar.org/paper/558984f821c4b9fb9aacf7441cf9ca6d303be836\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.01473\",\"authors\":[{\"authorId\":\"10007273\",\"name\":\"Qiaolin Xia\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"144934143\",\"name\":\"Dongdong Zhang\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"49575302\",\"name\":\"Zhifang Sui\"},{\"authorId\":\"144530394\",\"name\":\"Edward Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":null,\"name\":\"Xin Liu\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"title\":\"XGPT: Cross-modal Generative Pre-Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46255000\",\"name\":\"N. Ramachandran\"},{\"authorId\":null,\"name\":\"Emmie Kehoe\"},{\"authorId\":\"145933680\",\"name\":\"V. Sriram\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6b948b4c3c0e3b04ce161684185fa4450246bb22\",\"title\":\"Visual Question Answering via Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6b948b4c3c0e3b04ce161684185fa4450246bb22\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1954481\",\"name\":\"D. Huynh\"},{\"authorId\":\"47126776\",\"name\":\"E. Elhamifar\"}],\"doi\":\"10.1109/cvpr42600.2020.00880\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7541e6e97c5d94184922f7db5b7fe185063941ef\",\"title\":\"A Shared Multi-Attention Framework for Multi-Label Zero-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/7541e6e97c5d94184922f7db5b7fe185063941ef\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741390809\",\"name\":\"Weidong Tian\"},{\"authorId\":\"1657469716\",\"name\":\"Rencai Zhou\"},{\"authorId\":\"151481257\",\"name\":\"Zhong-Qiu Zhao\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207390\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ff3ccd867a94b77f519e73cf2de9772aa8ae905\",\"title\":\"Cascading Top-Down Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/0ff3ccd867a94b77f519e73cf2de9772aa8ae905\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14248689\",\"name\":\"Xinrui Cui\"},{\"authorId\":\"1399963334\",\"name\":\"D. Wang\"},{\"authorId\":\"38106845\",\"name\":\"Z. Wang\"}],\"doi\":\"10.1109/TMM.2020.2976985\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5053d81303af9b029732396cd3fbe23a08be786d\",\"title\":\"Feature-Flow Interpretation of Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/5053d81303af9b029732396cd3fbe23a08be786d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"2007.14419\",\"authors\":[{\"authorId\":\"94011352\",\"name\":\"S. Chen\"},{\"authorId\":\"1405907659\",\"name\":\"Ming Jiang\"},{\"authorId\":\"7788087\",\"name\":\"J. Yang\"},{\"authorId\":\"153386875\",\"name\":\"Q. Zhao\"}],\"doi\":\"10.1007/978-3-030-58452-8_6\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"96150d2fdbb72e11882ad06b346d6e0d2e819d51\",\"title\":\"AiR: Attention with Reasoning Capability\",\"url\":\"https://www.semanticscholar.org/paper/96150d2fdbb72e11882ad06b346d6e0d2e819d51\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1912.12014\",\"authors\":[{\"authorId\":\"46709826\",\"name\":\"Pengcheng Yang\"},{\"authorId\":\"2229601\",\"name\":\"Boxing Chen\"},{\"authorId\":\"47242878\",\"name\":\"Pei Zhang\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":\"10.1609/AAAI.V34I05.6484\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6e8d72eefbea5dc744efc08302992803adc01ca5\",\"title\":\"Visual Agreement Regularized Training for Multi-Modal Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/6e8d72eefbea5dc744efc08302992803adc01ca5\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1908.03557\",\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"144508458\",\"name\":\"Da Yin\"},{\"authorId\":\"1793529\",\"name\":\"C. Hsieh\"},{\"authorId\":\"2782886\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\"title\":\"VisualBERT: A Simple and Performant Baseline for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98727476\",\"name\":\"Tsan-Hwei Huang\"},{\"authorId\":\"2024357948\",\"name\":\"Hunter Hsieh\"},{\"authorId\":\"2025282857\",\"name\":\"Jiaqi Qin\"},{\"authorId\":\"2024727250\",\"name\":\"Hsien-Fung Liu\"},{\"authorId\":\"2377003\",\"name\":\"M. Eirinaki\"}],\"doi\":\"10.1109/TransAI49837.2020.00008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"30855a62d640c70429a81eec1b22bcc2a63c9c30\",\"title\":\"Play it again IMuCo! Music Composition to Match your Mood\",\"url\":\"https://www.semanticscholar.org/paper/30855a62d640c70429a81eec1b22bcc2a63c9c30\",\"venue\":\"2020 Second International Conference on Transdisciplinary AI (TransAI)\",\"year\":2020},{\"arxivId\":\"2012.07788\",\"authors\":[{\"authorId\":\"2037383772\",\"name\":\"Niklas Muennighoff\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54e4f2ef7410de9f94683cc570cb82257d27c0ff\",\"title\":\"Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes\",\"url\":\"https://www.semanticscholar.org/paper/54e4f2ef7410de9f94683cc570cb82257d27c0ff\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1912.03098\",\"authors\":[{\"authorId\":\"1403171438\",\"name\":\"J. Pont-Tuset\"},{\"authorId\":\"1823362\",\"name\":\"J. Uijlings\"},{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"}],\"doi\":\"10.1007/978-3-030-58558-7_38\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"title\":\"Connecting Vision and Language with Localized Narratives\",\"url\":\"https://www.semanticscholar.org/paper/439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145269477\",\"name\":\"Y. Zhao\"},{\"authorId\":\"1492126621\",\"name\":\"Y. Rao\"},{\"authorId\":\"49279687\",\"name\":\"Lianwei Wu\"},{\"authorId\":\"1978658363\",\"name\":\"Cong Feng\"}],\"doi\":\"10.1007/978-3-030-63823-8_69\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b686455c59af63ccd45b73b5445465bbd282bfd\",\"title\":\"Image Captioning Algorithm Based on Sufficient Visual Information and Text Information\",\"url\":\"https://www.semanticscholar.org/paper/0b686455c59af63ccd45b73b5445465bbd282bfd\",\"venue\":\"ICONIP\",\"year\":2020},{\"arxivId\":\"2010.12831\",\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"30156979\",\"name\":\"Haoxuan You\"},{\"authorId\":\"2513111\",\"name\":\"Zhecan Wang\"},{\"authorId\":\"2778637\",\"name\":\"Alireza Zareian\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"101751639\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"16f82c1dae2f6e5149c1be95165d7081f08298b6\",\"title\":\"Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions\",\"url\":\"https://www.semanticscholar.org/paper/16f82c1dae2f6e5149c1be95165d7081f08298b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.06139\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"title\":\"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations\",\"url\":\"https://www.semanticscholar.org/paper/cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1909.04800\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"1380338049\",\"name\":\"Anupriy\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":\"10.1016/j.patcog.2020.107586\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e281d8af3ec87fd893743afd6fb9d5ec3eaca924\",\"title\":\"Probabilistic framework for solving Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/e281d8af3ec87fd893743afd6fb9d5ec3eaca924\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":\"1909.02072\",\"authors\":[{\"authorId\":\"7934161\",\"name\":\"T. Chen\"},{\"authorId\":\"50218411\",\"name\":\"Z. Wang\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/ICCV.2019.00921\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"22d07f5eb536877aafbad9d790b56c51bb1c4a42\",\"title\":\"Large-Scale Tag-Based Font Retrieval With Generative Feature Learning\",\"url\":\"https://www.semanticscholar.org/paper/22d07f5eb536877aafbad9d790b56c51bb1c4a42\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1805.08174\",\"authors\":[{\"authorId\":\"2462516\",\"name\":\"Shagun Sodhani\"},{\"authorId\":\"7591930\",\"name\":\"Vardaan Pahuja\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0dd42c0d049b05b5d3c37cb2e64d8a307862b196\",\"title\":\"Reproducibility Report for \\\"Learning To Count Objects In Natural Images For Visual Question Answering\\\"\",\"url\":\"https://www.semanticscholar.org/paper/0dd42c0d049b05b5d3c37cb2e64d8a307862b196\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10405058\",\"name\":\"Chenfei Wu\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"1680068\",\"name\":\"X. Wang\"},{\"authorId\":\"143672034\",\"name\":\"X. Dong\"}],\"doi\":\"10.1145/3240508.3240513\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"34c9753893fe4713568542e7d96dc9a9e6545ec8\",\"title\":\"Object-Difference Attention: A Simple Relational Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/34c9753893fe4713568542e7d96dc9a9e6545ec8\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1903.02507\",\"authors\":[{\"authorId\":\"3259825\",\"name\":\"Jiayun Li\"},{\"authorId\":\"39367903\",\"name\":\"Mohammad K. Ebrahimpour\"},{\"authorId\":\"33129821\",\"name\":\"Azadeh Moghtaderi\"},{\"authorId\":\"1915432\",\"name\":\"Yen-Yun Yu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2ea752a837e24686a5774e6cbe6f787a76cd014d\",\"title\":\"Image captioning with weakly-supervised attention penalty\",\"url\":\"https://www.semanticscholar.org/paper/2ea752a837e24686a5774e6cbe6f787a76cd014d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145268319\",\"name\":\"Qiang Sun\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"}],\"doi\":\"10.1145/3323873.3325044\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"33c6f05eac12622146fec4868735daa78f79f80a\",\"title\":\"Stacked Self-Attention Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/33c6f05eac12622146fec4868735daa78f79f80a\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40393034\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f373756a5d99acdbf0b7cbe884ebba87293624f\",\"title\":\"Material for \\u201c Auto-Encoding Scene Graphs for Image Captioning \\u201d\",\"url\":\"https://www.semanticscholar.org/paper/9f373756a5d99acdbf0b7cbe884ebba87293624f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.02632\",\"authors\":[{\"authorId\":\"151482698\",\"name\":\"Chen Shen\"},{\"authorId\":\"145592288\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"2642638\",\"name\":\"Fuhai Chen\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"48569526\",\"name\":\"Xiangming Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"c7f60b69ccafc9fb1aabc0aaac9942d68d9166cf\",\"title\":\"Scene-based Factored Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c7f60b69ccafc9fb1aabc0aaac9942d68d9166cf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.02943\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"143899054\",\"name\":\"Mark Dras\"},{\"authorId\":\"3093086\",\"name\":\"Stephen Wan\"},{\"authorId\":\"145212976\",\"name\":\"C\\u00e9cile Paris\"},{\"authorId\":\"119899233\",\"name\":\"Len Hamey\"}],\"doi\":\"10.1007/978-3-030-29908-8_22\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d236d1415ac9a52ec599f0c100e15589cfceefd9\",\"title\":\"Towards Generating Stylized Image Captions via Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/d236d1415ac9a52ec599f0c100e15589cfceefd9\",\"venue\":\"PRICAI\",\"year\":2019},{\"arxivId\":\"1908.09317\",\"authors\":[{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"145587209\",\"name\":\"N. Navab\"}],\"doi\":\"10.1109/ICCV.2019.00751\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"title\":\"Towards Unsupervised Image Captioning With Shared Multimodal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1908.07810\",\"authors\":[{\"authorId\":\"50118263\",\"name\":\"Yike Wu\"},{\"authorId\":\"2516425\",\"name\":\"Shiwan Zhao\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"48379958\",\"name\":\"Ying Zhang\"},{\"authorId\":\"1721029\",\"name\":\"Xiaojie Yuan\"},{\"authorId\":\"1703625\",\"name\":\"Zhong Su\"}],\"doi\":\"10.1109/ICME.2019.00070\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d4b29b999bc2d907d6d01ad30829058721d29394\",\"title\":\"Improving Captioning for Low-Resource Languages by Cycle Consistency\",\"url\":\"https://www.semanticscholar.org/paper/d4b29b999bc2d907d6d01ad30829058721d29394\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35466168\",\"name\":\"Jian Han Lim\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"}],\"doi\":\"10.1109/ICIP.2019.8803004\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ae7bdb4c9b55080ba781b4737730127c80d04cca\",\"title\":\"Mask Captioning Network\",\"url\":\"https://www.semanticscholar.org/paper/ae7bdb4c9b55080ba781b4737730127c80d04cca\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"93763734\",\"name\":\"Trong-Dat Phan\"},{\"authorId\":\"50376884\",\"name\":\"Minh-Son Dao\"},{\"authorId\":\"48909205\",\"name\":\"Koji Zettsu\"}],\"doi\":\"10.1109/BigMM.2019.00-10\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab2891fc63658d55156545ec2b38e32c332fa387\",\"title\":\"An Interactive Watershed-Based Approach for Lifelog Moment Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/ab2891fc63658d55156545ec2b38e32c332fa387\",\"venue\":\"2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)\",\"year\":2019},{\"arxivId\":\"1804.00861\",\"authors\":[{\"authorId\":\"7379232\",\"name\":\"Dianqi Li\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"144463557\",\"name\":\"M. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"39144e468bde0424d38a3f20a6b62ddec4b459ae\",\"title\":\"Generating Diverse and Accurate Visual Captions by Comparative Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/39144e468bde0424d38a3f20a6b62ddec4b459ae\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2011.13681\",\"authors\":[{\"authorId\":\"20657367\",\"name\":\"A. Mani\"},{\"authorId\":\"116122080\",\"name\":\"William Hinthorn\"},{\"authorId\":\"2029244392\",\"name\":\"Nobline Yoo\"},{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f20439db6285ae9812fb27e33d9e9d0fc8c20b4e\",\"title\":\"Point and Ask: Incorporating Pointing into Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f20439db6285ae9812fb27e33d9e9d0fc8c20b4e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.01755\",\"authors\":[{\"authorId\":\"2472349\",\"name\":\"B. Gao\"},{\"authorId\":\"48053972\",\"name\":\"Hongyu Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ff2c3fcfde02cfd2d2c684f022981d91eb43fb3\",\"title\":\"Multi-Label Image Recognition with Multi-Class Attentional Regions\",\"url\":\"https://www.semanticscholar.org/paper/2ff2c3fcfde02cfd2d2c684f022981d91eb43fb3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1903.05942\",\"authors\":[{\"authorId\":\"40622539\",\"name\":\"Dong-Jin Kim\"},{\"authorId\":\"49331178\",\"name\":\"Jinsoo Choi\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"}],\"doi\":\"10.1109/CVPR.2019.00643\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79de42d6ca8d1bf2952c46eb74e6e0561f979257\",\"title\":\"Dense Relational Captioning: Triple-Stream Networks for Relationship-Based Captioning\",\"url\":\"https://www.semanticscholar.org/paper/79de42d6ca8d1bf2952c46eb74e6e0561f979257\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51021265\",\"name\":\"T. Yu\"},{\"authorId\":\"97583812\",\"name\":\"J. Yu\"},{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/TIP.2019.2940677\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ef33aedbab68b0771f6dd7ca8ec2492f12d7ea51\",\"title\":\"Compositional Attention Networks With Two-Stream Fusion for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ef33aedbab68b0771f6dd7ca8ec2492f12d7ea51\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2003689892\",\"name\":\"Hideo Umada\"},{\"authorId\":\"96853476\",\"name\":\"Masaki Aono\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20a52008ecd5a1396d1501545f96fe3c17f63863\",\"title\":\"kdevqa at VQA-Med 2020: Focusing on GLU-based Classification\",\"url\":\"https://www.semanticscholar.org/paper/20a52008ecd5a1396d1501545f96fe3c17f63863\",\"venue\":\"CLEF\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2151048\",\"name\":\"Taiki Miyanishi\"},{\"authorId\":\"34772057\",\"name\":\"Takuya Maekawa\"},{\"authorId\":\"1716788\",\"name\":\"M. Kawanabe\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3fc587b83d8c6a28b2bc4f9a1f8770f42fb5658\",\"title\":\"Two-Stream Spatiotemporal Compositional Attention Network for VideoQA\",\"url\":\"https://www.semanticscholar.org/paper/b3fc587b83d8c6a28b2bc4f9a1f8770f42fb5658\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"145497716\",\"name\":\"A. Das\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"}],\"doi\":\"10.18653/v1/P18-5004\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"532837c431617d37c03361ba5a7d5fdb082c55f4\",\"title\":\"Connecting Language and Vision to Actions\",\"url\":\"https://www.semanticscholar.org/paper/532837c431617d37c03361ba5a7d5fdb082c55f4\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1909.03396\",\"authors\":[{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"2904055\",\"name\":\"Ashish V. Thapliyal\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd153dbee4f4b86962ddacbc47010785f9cdec3c\",\"title\":\"Quality Estimation for Image Captions Based on Large-scale Human Evaluations\",\"url\":\"https://www.semanticscholar.org/paper/bd153dbee4f4b86962ddacbc47010785f9cdec3c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.04342\",\"authors\":[{\"authorId\":\"23364558\",\"name\":\"Nilavra Bhattacharya\"},{\"authorId\":\"48933740\",\"name\":\"Q. Li\"},{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"}],\"doi\":\"10.1109/ICCV.2019.00437\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4cae43b28757e0c37a05156ed063dcc3bb652809\",\"title\":\"Why Does a Visual Question Have Different Answers?\",\"url\":\"https://www.semanticscholar.org/paper/4cae43b28757e0c37a05156ed063dcc3bb652809\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143971671\",\"name\":\"Yu Sun\"},{\"authorId\":\"115516475\",\"name\":\"Huibin Ruan\"},{\"authorId\":\"1391027239\",\"name\":\"Yu Hong\"},{\"authorId\":\"7513481\",\"name\":\"C. Wu\"},{\"authorId\":\"50495554\",\"name\":\"Min Zhang\"},{\"authorId\":\"143740945\",\"name\":\"Guodong Zhou\"}],\"doi\":\"10.1007/978-3-030-32233-5_56\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da579218c973a43acd0e6bb02b9ecc66a19692b9\",\"title\":\"Multi-grain Representation Learning for Implicit Discourse Relation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/da579218c973a43acd0e6bb02b9ecc66a19692b9\",\"venue\":\"NLPCC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000125058\",\"name\":\"Feicheng Huang\"},{\"authorId\":\"49969428\",\"name\":\"Z. Li\"},{\"authorId\":\"1856671082\",\"name\":\"Shengjia Chen\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"1998838209\",\"name\":\"Huifang Ma\"}],\"doi\":\"10.1145/3340531.3411948\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3428bac9141f6db961fc4665db1cbc9a196152da\",\"title\":\"Image Captioning with Internal and External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/3428bac9141f6db961fc4665db1cbc9a196152da\",\"venue\":\"CIKM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73312190\",\"name\":\"W. Zhang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"1591412916\",\"name\":\"Jiajie Su\"},{\"authorId\":\"145974114\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1007/s11042-020-08832-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"814cfb898aa0bd011a97ebcf1774e42c32cf469e\",\"title\":\"Tell and guess: cooperative learning for natural image caption generation with hierarchical refined attention\",\"url\":\"https://www.semanticscholar.org/paper/814cfb898aa0bd011a97ebcf1774e42c32cf469e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"2003.03305\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"22d733f5d5a995469dc916102f1806253645ae60\",\"title\":\"Captioning Images with Novel Objects via Online Vocabulary Expansion\",\"url\":\"https://www.semanticscholar.org/paper/22d733f5d5a995469dc916102f1806253645ae60\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145617963\",\"name\":\"W. Li\"},{\"authorId\":\"2430445\",\"name\":\"Zhengxia Zou\"},{\"authorId\":\"49473026\",\"name\":\"Z. Shi\"}],\"doi\":\"10.1109/TGRS.2020.2988265\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5940c33e0a0602c250e0845819692c02b851c85f\",\"title\":\"Deep Matting for Cloud Detection in Remote Sensing Images\",\"url\":\"https://www.semanticscholar.org/paper/5940c33e0a0602c250e0845819692c02b851c85f\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2020},{\"arxivId\":\"1811.10666\",\"authors\":[{\"authorId\":\"34656387\",\"name\":\"Matteo Tomei\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2019.00600\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b430af071a523b03911e35a23bc12ae96e86bd4c\",\"title\":\"Art2Real: Unfolding the Reality of Artworks via Semantically-Aware Image-To-Image Translation\",\"url\":\"https://www.semanticscholar.org/paper/b430af071a523b03911e35a23bc12ae96e86bd4c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1812.05038\",\"authors\":[{\"authorId\":\"2978413\",\"name\":\"Chao-Yuan Wu\"},{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"2681569\",\"name\":\"Haoqi Fan\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2562966\",\"name\":\"Philipp Kr\\u00e4henb\\u00fchl\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"}],\"doi\":\"10.1109/CVPR.2019.00037\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45cb2cbae5c0b4cc52e524cc191a2f8db674ed42\",\"title\":\"Long-Term Feature Banks for Detailed Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/45cb2cbae5c0b4cc52e524cc191a2f8db674ed42\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1764508\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2440041\",\"name\":\"X. Jiang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2019.2902106\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"65bd828c05cac7eabe791e10d4d3c0f2da2b798c\",\"title\":\"Multi-Turn Video Question Answering via Hierarchical Attention Context Reinforced Networks\",\"url\":\"https://www.semanticscholar.org/paper/65bd828c05cac7eabe791e10d4d3c0f2da2b798c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49304790\",\"name\":\"Jun-hong Ma\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"980120c6f94fda8ee8b18082b5df12d72e1d4388\",\"title\":\"Research on Application of Deep Learning in Text Generation and Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/980120c6f94fda8ee8b18082b5df12d72e1d4388\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":\"10.1145/3347449.3357484\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"989730c00381805543baa470a2d6490cc5354a13\",\"title\":\"L-STAP: Learned Spatio-Temporal Adaptive Pooling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/989730c00381805543baa470a2d6490cc5354a13\",\"venue\":\"AI4TV@MM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2036790405\",\"name\":\"Kiyohiko Iwamura\"},{\"authorId\":\"34769384\",\"name\":\"Jun Younes Louhi Kasahara\"},{\"authorId\":\"24316406\",\"name\":\"A. Moro\"},{\"authorId\":\"152521159\",\"name\":\"A. Yamashita\"},{\"authorId\":\"50631807\",\"name\":\"H. Asama\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b6bea157580146a416540b467fa3002ccd044b7d\",\"title\":\"Potential of Incorporating Motion Estimation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b6bea157580146a416540b467fa3002ccd044b7d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2008.01180\",\"authors\":[{\"authorId\":\"48251796\",\"name\":\"Chenyun Wu\"},{\"authorId\":\"143903550\",\"name\":\"M. Timm\"},{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"}],\"doi\":\"10.1007/978-3-030-58452-8_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"86f0fb3791761cdb5e9108721a536d752641d4bf\",\"title\":\"Describing Textures using Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/86f0fb3791761cdb5e9108721a536d752641d4bf\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2003205110\",\"name\":\"Artur Podtikhov\"},{\"authorId\":\"2000598263\",\"name\":\"Makhmud Shaban\"},{\"authorId\":\"153780959\",\"name\":\"A. Kovalev\"},{\"authorId\":\"35234816\",\"name\":\"A. Panov\"}],\"doi\":\"10.1007/978-3-030-60577-3_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c3d7635cd2d65c69c586375c34f4b45479563087\",\"title\":\"Error Analysis for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/c3d7635cd2d65c69c586375c34f4b45479563087\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"2010.10604\",\"authors\":[{\"authorId\":\"8010189\",\"name\":\"Xinjie Fan\"},{\"authorId\":\"1515867113\",\"name\":\"Shujian Zhang\"},{\"authorId\":\"1409955695\",\"name\":\"B. Chen\"},{\"authorId\":\"38026572\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"83243b3639bbb42566bc300b4999db9a2b7a93c3\",\"title\":\"Bayesian Attention Modules\",\"url\":\"https://www.semanticscholar.org/paper/83243b3639bbb42566bc300b4999db9a2b7a93c3\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2007.11690\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"2562211\",\"name\":\"Xiaoyu Shen\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54a42098b34c2602305b03fe07b7db82b789f5db\",\"title\":\"Integrating Image Captioning with Rule-based Entity Masking\",\"url\":\"https://www.semanticscholar.org/paper/54a42098b34c2602305b03fe07b7db82b789f5db\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"120897486\",\"name\":\"Anwen Hu\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3394171.3413576\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6c0abdf9a9c47e8ec2606b06a5324ec7d2e7ebe7\",\"title\":\"ICECAP: Information Concentrated Entity-aware Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c0abdf9a9c47e8ec2606b06a5324ec7d2e7ebe7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xuanwen Luo\"},{\"authorId\":\"1761635\",\"name\":\"M. Villani\"},{\"authorId\":\"47631888\",\"name\":\"N. Agarwal\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e6734af10c532eec815ff2bb48dff0f68fc9b024\",\"title\":\"IMMM 2019 Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/e6734af10c532eec815ff2bb48dff0f68fc9b024\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2110665\",\"name\":\"A. Padmakumar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5593141856543502260007a7e46d0812bbd77f98\",\"title\":\"Improved Models and Queries for Grounded Human-Robot Dialog\",\"url\":\"https://www.semanticscholar.org/paper/5593141856543502260007a7e46d0812bbd77f98\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"title\":\"LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1899511699\",\"name\":\"Zhigang Wei\"},{\"authorId\":\"1901601357\",\"name\":\"Xinwei Song\"},{\"authorId\":\"1902693062\",\"name\":\"Yecheng Lin\"}],\"doi\":\"10.1109/ICISC47916.2020.9171207\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"01b057a14aa67a3a3c15535f1e9970e4320a9106\",\"title\":\"Image Feature Recognition Algorithm for Rural Revitalization Product Design based on Visual Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/01b057a14aa67a3a3c15535f1e9970e4320a9106\",\"venue\":\"2020 Fourth International Conference on Inventive Systems and Control (ICISC)\",\"year\":2020},{\"arxivId\":\"2008.08012\",\"authors\":[{\"authorId\":\"9748140\",\"name\":\"K. Gouthaman\"},{\"authorId\":\"3265714\",\"name\":\"A. Nambiar\"},{\"authorId\":\"1882516497\",\"name\":\"Kancheti Sai Srinivas\"},{\"authorId\":\"50853059\",\"name\":\"Anurag Mittal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b46ce08535c728cc690852a7c63a9bb211ff25ab\",\"title\":\"Linguistically-aware Attention for Reducing the Semantic-Gap in Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/b46ce08535c728cc690852a7c63a9bb211ff25ab\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Arka Sadhu\"},{\"authorId\":\"1716207091\",\"name\":\"Xuefeng Hu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9147121ab65d3ace2d32afe123500f1b4ee18edd\",\"title\":\"Joint Learning of Scene Graph Generation and Reasoning for Visual Question Answering Mid-term report\",\"url\":\"https://www.semanticscholar.org/paper/9147121ab65d3ace2d32afe123500f1b4ee18edd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bcec2fc7db5df44eee4e84cd15bf6aa537d524c0\",\"title\":\"Meshed-Memory Transformer for Image Captioning Supplementary Material\",\"url\":\"https://www.semanticscholar.org/paper/bcec2fc7db5df44eee4e84cd15bf6aa537d524c0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113001756\",\"name\":\"A. Deshpande\"},{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"24952249\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b2c60061ad32e28eb1e20aff42e062c9160786be\",\"title\":\"Diverse and Controllable Image Captioning with Part-of-Speech Guidance\",\"url\":\"https://www.semanticscholar.org/paper/b2c60061ad32e28eb1e20aff42e062c9160786be\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1906.10770\",\"authors\":[{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"9919436\",\"name\":\"J. Yu\"},{\"authorId\":\"30513139\",\"name\":\"Yuhao Cui\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/CVPR.2019.00644\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8a1744da011375d711ed75fc2d160c6fdca2cf89\",\"title\":\"Deep Modular Co-Attention Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8a1744da011375d711ed75fc2d160c6fdca2cf89\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9073332\",\"name\":\"J. Li\"},{\"authorId\":\"1923156\",\"name\":\"P. Yao\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"49039585\",\"name\":\"Wei-Cun Zhang\"}],\"doi\":\"10.3390/APP9163260\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"47716ca06a3ad4226a6d8706b6f02793b9892fe4\",\"title\":\"Boosted Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/47716ca06a3ad4226a6d8706b6f02793b9892fe4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144081629\",\"name\":\"Mario G\\u00f3mez Mart\\u00ednez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"faa8314259e9de1af8e841c265b0251531b32e04\",\"title\":\"Deep learning for image captioning: an encoder-decoder architecture with soft attention\",\"url\":\"https://www.semanticscholar.org/paper/faa8314259e9de1af8e841c265b0251531b32e04\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1901.02273\",\"authors\":[{\"authorId\":\"82237332\",\"name\":\"Shiyang Feng\"},{\"authorId\":\"82438735\",\"name\":\"Tianyue Chen\"},{\"authorId\":\"48728626\",\"name\":\"Hao Sun\"}],\"doi\":\"10.1109/ITAIC.2019.8785574\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cfaa60a691e9243001f990e2c43beb8fc49ac377\",\"title\":\"Long Short-Term Memory Spatial Transformer Network\",\"url\":\"https://www.semanticscholar.org/paper/cfaa60a691e9243001f990e2c43beb8fc49ac377\",\"venue\":\"2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)\",\"year\":2019},{\"arxivId\":\"2011.07735\",\"authors\":[{\"authorId\":\"40016108\",\"name\":\"Aman Chadha\"},{\"authorId\":\"2025073690\",\"name\":\"Gurneet Arora\"},{\"authorId\":\"2025065763\",\"name\":\"Navpreet Kaloty\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"title\":\"iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1470727651\",\"name\":\"Ariyo Oluwasanmi\"},{\"authorId\":\"4043033\",\"name\":\"E. Frimpong\"},{\"authorId\":\"32593111\",\"name\":\"Muhammad Umar Aftab\"},{\"authorId\":\"46352756\",\"name\":\"Edward Y. Baagyere\"},{\"authorId\":\"152179239\",\"name\":\"Zhiquang Qin\"},{\"authorId\":\"1470727785\",\"name\":\"Kifayat Ullah\"}],\"doi\":\"10.1109/ACCESS.2019.2957513\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78396f9e33eaada2a84dc12a59a3deceac05c526\",\"title\":\"Fully Convolutional CaptionNet: Siamese Difference Captioning Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/78396f9e33eaada2a84dc12a59a3deceac05c526\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389373080\",\"name\":\"Ruoyu Chen\"},{\"authorId\":\"2607225\",\"name\":\"Zhongnian Li\"},{\"authorId\":\"1772283\",\"name\":\"D. Zhang\"}],\"doi\":\"10.1007/978-981-15-1398-5_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bab70faa1e36b525766f85b5ff87bff4566d0294\",\"title\":\"Adaptive Joint Attention with Reinforcement Training for Convolutional Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/bab70faa1e36b525766f85b5ff87bff4566d0294\",\"venue\":\"HBAI@IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"31081539\",\"name\":\"Pengpeng Zeng\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.24963/ijcai.2018/126\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b18d7ce3e7514fdae89ff410e2e122382c3d10a9\",\"title\":\"From Pixels to Objects: Cubic Visual Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b18d7ce3e7514fdae89ff410e2e122382c3d10a9\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nannan Li\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.24963/ijcai.2018/110\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"47921216d4758a16cf7b5bfc85cc7b9178bffe6a\",\"title\":\"Image Cationing with Visual-Semantic LSTM\",\"url\":\"https://www.semanticscholar.org/paper/47921216d4758a16cf7b5bfc85cc7b9178bffe6a\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1812.08126\",\"authors\":[{\"authorId\":\"32236286\",\"name\":\"Annika Lindh\"},{\"authorId\":\"144394786\",\"name\":\"R. Ross\"},{\"authorId\":\"31179447\",\"name\":\"Abhijit Mahalunkar\"},{\"authorId\":\"31071031\",\"name\":\"Giancarlo Salton\"},{\"authorId\":\"34967075\",\"name\":\"John D. Kelleher\"}],\"doi\":\"10.1007/978-3-030-01418-6_18\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"93a10137a41f9e1c8f939517bfc75379b47cb1a1\",\"title\":\"Generating Diverse and Meaningful Captions - Unsupervised Specificity Optimization for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93a10137a41f9e1c8f939517bfc75379b47cb1a1\",\"venue\":\"ICANN\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391051492\",\"name\":\"Y. Quan\"},{\"authorId\":\"49969428\",\"name\":\"Z. Li\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"46389488\",\"name\":\"H. Ma\"}],\"doi\":\"10.1109/ACCESS.2019.2958817\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2dcf080a5e8cc3c8b999be79a44dfaea61fd878a\",\"title\":\"Object Detection Model Based on Deep Dilated Convolutional Networks by Fusing Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/2dcf080a5e8cc3c8b999be79a44dfaea61fd878a\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150337817\",\"name\":\"Weike Jin\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"67144160\",\"name\":\"Mao Gu\"},{\"authorId\":\"97583812\",\"name\":\"J. Yu\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1145/3343031.3351065\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a162f189c9c553438b83a8a8ec7de4a6fa59069\",\"title\":\"Multi-interaction Network with Object Relation for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7a162f189c9c553438b83a8a8ec7de4a6fa59069\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152553572\",\"name\":\"Huan Shao\"},{\"authorId\":\"1965723\",\"name\":\"Y. Xu\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"7788280\",\"name\":\"J. Yang\"},{\"authorId\":\"47535378\",\"name\":\"Chunping Liu\"}],\"doi\":\"10.1007/978-3-030-36802-9_24\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f21cba46085eba47b299fbff283515284bed7189\",\"title\":\"Intra-Modality Feature Interaction Using Self-attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f21cba46085eba47b299fbff283515284bed7189\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":\"1911.08618\",\"authors\":[{\"authorId\":\"48494603\",\"name\":\"Badri N. Patro\"},{\"authorId\":\"1380338049\",\"name\":\"Anupriy\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"}],\"doi\":\"10.1609/AAAI.V34I07.6858\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dabe557eaad9e326a5b44c04fa619b2118f4bda5\",\"title\":\"Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA\",\"url\":\"https://www.semanticscholar.org/paper/dabe557eaad9e326a5b44c04fa619b2118f4bda5\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29379814\",\"name\":\"Adela Pu\\u015fca\\u015fiu\"},{\"authorId\":\"9443907\",\"name\":\"Alexandra Fanca\"},{\"authorId\":\"123464438\",\"name\":\"Dan-Ioan Gota\"},{\"authorId\":\"29345333\",\"name\":\"Honoriu V\\u0103lean\"}],\"doi\":\"10.1109/AQTR49680.2020.9129930\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf52c8dca018391239f190f2ebc1f7f7a8f762ca\",\"title\":\"Automated image captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf52c8dca018391239f190f2ebc1f7f7a8f762ca\",\"venue\":\"2020 IEEE International Conference on Automation, Quality and Testing, Robotics (AQTR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819450790\",\"name\":\"Qi Chen\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"49251914\",\"name\":\"Jun-kai Chen\"},{\"authorId\":\"8277017\",\"name\":\"Q. Wu\"},{\"authorId\":\"133678193\",\"name\":\"A. van den Hengel\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1109/TIP.2020.3003227\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a2418fd9c453492f1ca833d5595571249a3ee55\",\"title\":\"Scripted Video Generation With a Bottom-Up Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/5a2418fd9c453492f1ca833d5595571249a3ee55\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1910.11124\",\"authors\":[{\"authorId\":\"1381855534\",\"name\":\"Hammad A. Ayyubi\"},{\"authorId\":\"35631602\",\"name\":\"Md. Mehrab Tanjim\"},{\"authorId\":\"1765887\",\"name\":\"D. Kriegman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9e5b9536ab2d87eb45d0185d89a1164e0b7f75d\",\"title\":\"Enforcing Reasoning in Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/c9e5b9536ab2d87eb45d0185d89a1164e0b7f75d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7314814\",\"name\":\"H. Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1818920\",\"name\":\"Zijia Lin\"},{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"1435766877\",\"name\":\"Gu Xiao-peng\"}],\"doi\":\"10.1145/3362065\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d39fb70393e9a17e5556708852829743380eeed\",\"title\":\"ACMNet: Adaptive Confidence Matching Network for Human Behavior Analysis via Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/0d39fb70393e9a17e5556708852829743380eeed\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.24963/ijcai.2018/114\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2b5a95ec7882bf329c4203513e606aa91d4174c5\",\"title\":\"Multi-Level Policy and Reward Reinforcement Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2b5a95ec7882bf329c4203513e606aa91d4174c5\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1910.10706\",\"authors\":[{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"2427516\",\"name\":\"Chenhui Chu\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"}],\"doi\":\"10.1609/AAAI.V34I07.6713\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"12f11d56a83516bfdd1f32f60e3695ab92a0f819\",\"title\":\"KnowIT VQA: Answering Knowledge-Based Questions about Videos\",\"url\":\"https://www.semanticscholar.org/paper/12f11d56a83516bfdd1f32f60e3695ab92a0f819\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1802.05766\",\"authors\":[{\"authorId\":\"49889702\",\"name\":\"Y. Zhang\"},{\"authorId\":\"3724810\",\"name\":\"J. Hare\"},{\"authorId\":\"1398417301\",\"name\":\"A. Pr\\u00fcgel-Bennett\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"30a3eee5e9302108416f6234d739373dde68d373\",\"title\":\"Learning to Count Objects in Natural Images for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/30a3eee5e9302108416f6234d739373dde68d373\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1902.00313\",\"authors\":[{\"authorId\":\"32701193\",\"name\":\"Yuanzhi Liang\"},{\"authorId\":\"2643877\",\"name\":\"Y. Bai\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"6468417\",\"name\":\"Xueming Qian\"},{\"authorId\":\"4096586\",\"name\":\"L. Zhu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ab88c48e7829029c47ac6deea5c0ccbff3614c5c\",\"title\":\"Rethinking Visual Relationships for High-level Image Understanding\",\"url\":\"https://www.semanticscholar.org/paper/ab88c48e7829029c47ac6deea5c0ccbff3614c5c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1939569\",\"name\":\"Feiran Huang\"},{\"authorId\":\"1863856\",\"name\":\"Kaimin Wei\"},{\"authorId\":\"1729395\",\"name\":\"J. Weng\"},{\"authorId\":\"1707275\",\"name\":\"Zhoujun Li\"}],\"doi\":\"10.1145/3388861\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ddd604d33793517b171a28ccbd1e3ed40d543cc\",\"title\":\"Attention-Based Modality-Gated Networks for Image-Text Sentiment Analysis\",\"url\":\"https://www.semanticscholar.org/paper/1ddd604d33793517b171a28ccbd1e3ed40d543cc\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"2010.03403\",\"authors\":[{\"authorId\":\"1490652152\",\"name\":\"Jiwei Wei\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1524912498\",\"name\":\"Yang Yang\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":null,\"name\":\"Zheng Wang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/CVPR42600.2020.01302\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"title\":\"Universal Weighting Metric Learning for Cross-Modal Matching\",\"url\":\"https://www.semanticscholar.org/paper/dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993645531\",\"name\":\"Gen Luo\"},{\"authorId\":\"2223252\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1572139630\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"34739384\",\"name\":\"Jinsong Su\"},{\"authorId\":\"46246806\",\"name\":\"Chia-Wen Lin\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3394171.3414006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"24f2a18c54ab017d00ce1f714600b6bede6c0820\",\"title\":\"Cascade Grouped Attention Network for Referring Expression Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/24f2a18c54ab017d00ce1f714600b6bede6c0820\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152281902\",\"name\":\"S. Wu\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"1700911\",\"name\":\"Zhiqi Shen\"},{\"authorId\":\"145977143\",\"name\":\"Mohan S. Kankanhalli\"},{\"authorId\":\"1699730\",\"name\":\"Anthony K. H. Tung\"}],\"doi\":\"10.1145/3394171.3413589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35ba6ed07ef68db187674498e684de7f3e160716\",\"title\":\"Who You Are Decides How You Tell\",\"url\":\"https://www.semanticscholar.org/paper/35ba6ed07ef68db187674498e684de7f3e160716\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22228139\",\"name\":\"Yunan Ye\"},{\"authorId\":\"101266543\",\"name\":\"Shifeng Zhang\"},{\"authorId\":\"48515305\",\"name\":\"Yimeng Li\"},{\"authorId\":\"152519410\",\"name\":\"Xufeng Qian\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"153269968\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.1016/j.ipm.2020.102265\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32589847320c3c783e6eaf512e7723b8d5f5fb0a\",\"title\":\"Video question answering via grounded cross-attention network learning\",\"url\":\"https://www.semanticscholar.org/paper/32589847320c3c783e6eaf512e7723b8d5f5fb0a\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9254311\",\"name\":\"Yuansheng Song\"},{\"authorId\":\"2110377\",\"name\":\"Ping Jian\"}],\"doi\":\"10.1007/978-3-030-60450-9_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a81751ea4350cd67f9bd1d81b9410c64c22527e\",\"title\":\"Deep Hierarchical Attention Flow for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/5a81751ea4350cd67f9bd1d81b9410c64c22527e\",\"venue\":\"NLPCC\",\"year\":2020},{\"arxivId\":\"1910.02029\",\"authors\":[{\"authorId\":\"2326243\",\"name\":\"Arun Balajee Vasudevan\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/s11263-020-01374-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28766f2ecfb5dc19addf272bf25301030e8b1af9\",\"title\":\"Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory\",\"url\":\"https://www.semanticscholar.org/paper/28766f2ecfb5dc19addf272bf25301030e8b1af9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1912.08562\",\"authors\":[{\"authorId\":\"8810290\",\"name\":\"J. Liang\"},{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"1388864077\",\"name\":\"Feng Lu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dca1721cabdf92baf624dc0c29315146a6dbe364\",\"title\":\"CPGAN: Full-Spectrum Content-Parsing Generative Adversarial Networks for Text-to-Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/dca1721cabdf92baf624dc0c29315146a6dbe364\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1803.11544\",\"authors\":[{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"145587210\",\"name\":\"Nassir Navab\"},{\"authorId\":\"1678633\",\"name\":\"Gregory Hager\"},{\"authorId\":\"2266326\",\"name\":\"Federico Tombari\"}],\"doi\":\"10.1109/CVPR.2018.00892\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a334442b493501bb60a53dc3e689fc569965ad81\",\"title\":\"Guide Me: Interacting with Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/a334442b493501bb60a53dc3e689fc569965ad81\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"39882601\",\"name\":\"Yu Cheng\"},{\"authorId\":null,\"name\":\"Jingjing Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"54cf3aea22c9875eb6d02bcf41925da0d6376a23\",\"title\":\"Supplementary Material UNITER: UNiversal Image-TExt Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/54cf3aea22c9875eb6d02bcf41925da0d6376a23\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.05595\",\"authors\":[{\"authorId\":\"1576818877\",\"name\":\"Kento Terao\"},{\"authorId\":\"47668008\",\"name\":\"T. Tamaki\"},{\"authorId\":\"1688940\",\"name\":\"B. Raytchev\"},{\"authorId\":\"1686272\",\"name\":\"K. Kaneda\"},{\"authorId\":\"49969107\",\"name\":\"S. Satoh\"}],\"doi\":\"10.1109/ACCESS.2020.3022063\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"217d80b1425c24996dafbe539a1e976a1e790ac0\",\"title\":\"Which visual questions are difficult to answer? Analysis with Entropy of Answer Distributions\",\"url\":\"https://www.semanticscholar.org/paper/217d80b1425c24996dafbe539a1e976a1e790ac0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.16056\",\"authors\":[{\"authorId\":\"46843171\",\"name\":\"Zhihong Chen\"},{\"authorId\":\"1922182598\",\"name\":\"Yan Song\"},{\"authorId\":\"2678812\",\"name\":\"Tsung-Hui Chang\"},{\"authorId\":\"2005096276\",\"name\":\"Xiang Wan\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.112\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ebfc629ef2ed733e1c77df5e27c94f05eb012cab\",\"title\":\"Generating Radiology Reports via Memory-driven Transformer\",\"url\":\"https://www.semanticscholar.org/paper/ebfc629ef2ed733e1c77df5e27c94f05eb012cab\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2009.07310\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"3456894\",\"name\":\"J. Ive\"},{\"authorId\":\"1945450523\",\"name\":\"Veneta Haralampieva\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1389738309\",\"name\":\"Loic Barrault\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.184\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ae01da2d35fb9e63d548ef0f1351db8b1c63b03c\",\"title\":\"Simultaneous Machine Translation with Visual Context\",\"url\":\"https://www.semanticscholar.org/paper/ae01da2d35fb9e63d548ef0f1351db8b1c63b03c\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2006.05726\",\"authors\":[{\"authorId\":\"51149482\",\"name\":\"Corentin Kervadec\"},{\"authorId\":\"145664204\",\"name\":\"G. Antipov\"},{\"authorId\":\"2341854\",\"name\":\"M. Baccouche\"},{\"authorId\":\"144899680\",\"name\":\"C. Wolf\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7454abbee8f92dd0fd893c8039c03d66e9a5f032\",\"title\":\"Estimating semantic structure for the VQA answer space\",\"url\":\"https://www.semanticscholar.org/paper/7454abbee8f92dd0fd893c8039c03d66e9a5f032\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71222785\",\"name\":\"Yun Liu\"},{\"authorId\":\"46447759\",\"name\":\"X. Zhang\"},{\"authorId\":\"1939569\",\"name\":\"Feiran Huang\"},{\"authorId\":\"2548662\",\"name\":\"X. Tang\"},{\"authorId\":\"1707275\",\"name\":\"Zhoujun Li\"}],\"doi\":\"10.1016/J.ASOC.2019.105584\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a78477728a184a80cdfcfb97d5023f37961c3b6a\",\"title\":\"Visual question answering via Attention-based syntactic structure tree-LSTM\",\"url\":\"https://www.semanticscholar.org/paper/a78477728a184a80cdfcfb97d5023f37961c3b6a\",\"venue\":\"Appl. Soft Comput.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/ICCV.2019.00472\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5abe916562fad8306e3f4e571f83015047f0be1d\",\"title\":\"Robust Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5abe916562fad8306e3f4e571f83015047f0be1d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1905.04430\",\"authors\":[{\"authorId\":\"119352476\",\"name\":\"M. M. K. Moghaddam\"},{\"authorId\":\"8088602\",\"name\":\"Ehsan Abbasnejad\"},{\"authorId\":\"31635758\",\"name\":\"Javen Shi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6e889406f3f85dcb33e90eba2a6dfffbb3c51a1\",\"title\":\"Follow the Attention: Combining Partial Pose and Object Motion for Fine-Grained Action Detection\",\"url\":\"https://www.semanticscholar.org/paper/c6e889406f3f85dcb33e90eba2a6dfffbb3c51a1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1811.10698\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":\"10.1109/CVPR.2019.01019\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed78a2671ef61c031759c01434678c282f23faec\",\"title\":\"LSTA: Long Short-Term Attention for Egocentric Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ed78a2671ef61c031759c01434678c282f23faec\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1903.00366\",\"authors\":[{\"authorId\":\"153677280\",\"name\":\"Robik Shrestha\"},{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.1109/CVPR.2019.01072\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d9344534ab39544a3a3c173b27628e0d9c5d4dc5\",\"title\":\"Answer Them All! Toward Universal Visual Question Answering Models\",\"url\":\"https://www.semanticscholar.org/paper/d9344534ab39544a3a3c173b27628e0d9c5d4dc5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.06847\",\"authors\":[{\"authorId\":\"147383784\",\"name\":\"Liangjiang Wen\"},{\"authorId\":\"46447533\",\"name\":\"Xueyang Zhang\"},{\"authorId\":\"9583912\",\"name\":\"Haoli Bai\"},{\"authorId\":\"1683510\",\"name\":\"Zenglin Xu\"}],\"doi\":\"10.1016/j.neunet.2019.11.018\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f122bdec26a4fe0badb1234b8036b1023de3405\",\"title\":\"Structured Pruning of Recurrent Neural Networks through Neuron Selection\",\"url\":\"https://www.semanticscholar.org/paper/7f122bdec26a4fe0badb1234b8036b1023de3405\",\"venue\":\"Neural Networks\",\"year\":2020},{\"arxivId\":\"2009.08566\",\"authors\":[{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.63\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf4e64566252b3a342ba344e8a123c2b209766f2\",\"title\":\"MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/cf4e64566252b3a342ba344e8a123c2b209766f2\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2011.00927\",\"authors\":[{\"authorId\":\"2000125058\",\"name\":\"Feicheng Huang\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"92057141\",\"name\":\"H. Wei\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"1998838209\",\"name\":\"Huifang Ma\"}],\"doi\":\"10.1007/s10994-020-05919-y\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"899b365ce70207f1fd456e982583841d9e4701bf\",\"title\":\"Boost Image Captioning with Knowledge Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/899b365ce70207f1fd456e982583841d9e4701bf\",\"venue\":\"Mach. Learn.\",\"year\":2020},{\"arxivId\":\"2004.12070\",\"authors\":[{\"authorId\":\"1564034697\",\"name\":\"Zhou Yu\"},{\"authorId\":\"30513139\",\"name\":\"Yuhao Cui\"},{\"authorId\":null,\"name\":\"Jun Yu\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"1490934465\",\"name\":\"Dacheng Tao\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3394171.3413977\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f40ed567148f724028952e5fe20a34a0b671dd2e\",\"title\":\"Deep Multimodal Neural Architecture Search\",\"url\":\"https://www.semanticscholar.org/paper/f40ed567148f724028952e5fe20a34a0b671dd2e\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2007.01755\",\"authors\":[{\"authorId\":\"2472349\",\"name\":\"B. Gao\"},{\"authorId\":\"48053972\",\"name\":\"Hongyu Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6752ce6368210c13b9f9abb6ef8a8bc59fc4ca66\",\"title\":\"Learning to Discover Multi-Class Attentional Regions for Multi-Label Image Recognition.\",\"url\":\"https://www.semanticscholar.org/paper/6752ce6368210c13b9f9abb6ef8a8bc59fc4ca66\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.11134\",\"authors\":[{\"authorId\":\"2018700866\",\"name\":\"Chao Yang\"},{\"authorId\":\"145030306\",\"name\":\"S. Feng\"},{\"authorId\":\"144032853\",\"name\":\"Dong-sheng Li\"},{\"authorId\":\"2476503\",\"name\":\"H. Shen\"},{\"authorId\":\"50248868\",\"name\":\"Guoqing Wang\"},{\"authorId\":\"1796274181\",\"name\":\"Bin Jiang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e30790690231e7730eb6f903a722a54091fc4967\",\"title\":\"Learning content and context with language bias for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e30790690231e7730eb6f903a722a54091fc4967\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103864860\",\"name\":\"Praful Hambarde\"},{\"authorId\":\"1827383\",\"name\":\"S. Murala\"}],\"doi\":\"10.1109/tci.2020.2981761\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28fff082ccc1a974abf6d8c01f1583724326fb02\",\"title\":\"S2DNet: Depth Estimation From Single Image and Sparse Samples\",\"url\":\"https://www.semanticscholar.org/paper/28fff082ccc1a974abf6d8c01f1583724326fb02\",\"venue\":\"IEEE Transactions on Computational Imaging\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9733883\",\"name\":\"Qingrong Cheng\"},{\"authorId\":\"1649999106\",\"name\":\"Xiaodong Gu\"}],\"doi\":\"10.1007/s11042-020-09450-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f473ada1bad964dc701f9fd19e53a05db8f3b4e6\",\"title\":\"Deep attentional fine-grained similarity network with adversarial learning for cross-modal retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f473ada1bad964dc701f9fd19e53a05db8f3b4e6\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":\"2012.15454\",\"authors\":[{\"authorId\":null,\"name\":\"Wei-Ning Hsu\"},{\"authorId\":null,\"name\":\"David Harwath\"},{\"authorId\":null,\"name\":\"Christopher Song\"},{\"authorId\":\"152450847\",\"name\":\"J. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5df12460d9a742b08f82e8b79cb102a8be5dd9b4\",\"title\":\"Text-Free Image-to-Speech Synthesis Using Learned Segmental Units\",\"url\":\"https://www.semanticscholar.org/paper/5df12460d9a742b08f82e8b79cb102a8be5dd9b4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1909.03918\",\"authors\":[{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2019.00271\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"05106b86ec45914d1136719d311078182d437872\",\"title\":\"Hierarchy Parsing for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/05106b86ec45914d1136719d311078182d437872\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49290494\",\"name\":\"Zhijie Lin\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1742291\",\"name\":\"Zixing Zhang\"},{\"authorId\":\"48805561\",\"name\":\"Zijian Zhang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2020.2965987\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a68ef4d0c1a814dd0920f6824aaad8e15339f66\",\"title\":\"Moment Retrieval via Cross-Modal Interaction Networks With Query Reconstruction\",\"url\":\"https://www.semanticscholar.org/paper/5a68ef4d0c1a814dd0920f6824aaad8e15339f66\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1574421683\",\"name\":\"Xi Zhu\"},{\"authorId\":\"1855978\",\"name\":\"Zhendong Mao\"},{\"authorId\":\"8157255\",\"name\":\"Zhineng Chen\"},{\"authorId\":\"40282454\",\"name\":\"Y. Li\"},{\"authorId\":\"50218711\",\"name\":\"Z. Wang\"},{\"authorId\":\"15696552\",\"name\":\"Bin Wang\"}],\"doi\":\"10.1007/s11042-020-08790-0\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"49c9921b5df66b421b5e6432d84a8eb699c6443b\",\"title\":\"Object-difference drived graph convolutional networks for visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/49c9921b5df66b421b5e6432d84a8eb699c6443b\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41050714\",\"name\":\"Sandeep Narayan Parameswaran\"},{\"authorId\":\"144009889\",\"name\":\"Sukhendu Das\"}],\"doi\":\"10.1145/3293353.3293391\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"acc9221fbfbfe52a6a81032ea02f42449fc1eca9\",\"title\":\"A Bottom-Up and Top-Down Approach for Image Captioning using Transformer\",\"url\":\"https://www.semanticscholar.org/paper/acc9221fbfbfe52a6a81032ea02f42449fc1eca9\",\"venue\":\"ICVGIP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38590137\",\"name\":\"Z. Liu\"},{\"authorId\":\"48262929\",\"name\":\"Zeyu Zheng\"},{\"authorId\":\"3490004\",\"name\":\"Xiwang Guo\"},{\"authorId\":\"144851022\",\"name\":\"Liang Qi\"},{\"authorId\":\"121949855\",\"name\":\"Jun Gui\"},{\"authorId\":\"40758343\",\"name\":\"Dianzheng Fu\"},{\"authorId\":\"14216081\",\"name\":\"Qingfeng Yao\"},{\"authorId\":\"12555588\",\"name\":\"L. Jin\"}],\"doi\":\"10.1109/ACCESS.2019.2941503\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18c3d3929d5c54571da2d327dcbf77e2fd28dbc3\",\"title\":\"AttentiveHerb: A Novel Method for Traditional Medicine Prescription Generation\",\"url\":\"https://www.semanticscholar.org/paper/18c3d3929d5c54571da2d327dcbf77e2fd28dbc3\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"2008.12520\",\"authors\":[{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"82729121\",\"name\":\"Chentao Ye\"},{\"authorId\":\"9071958\",\"name\":\"Zihua Liu\"},{\"authorId\":\"32104754\",\"name\":\"Qingtao Hu\"},{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"2427516\",\"name\":\"Chenhui Chu\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"1706595\",\"name\":\"T. Mitamura\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fb106fdc2d02b077c100bd0a653395bd6cffcded\",\"title\":\"A Dataset and Baselines for Visual Question Answering on Art\",\"url\":\"https://www.semanticscholar.org/paper/fb106fdc2d02b077c100bd0a653395bd6cffcded\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.02489\",\"authors\":[{\"authorId\":\"47748186\",\"name\":\"Wei Wei\"},{\"authorId\":\"144996789\",\"name\":\"L. Cheng\"},{\"authorId\":\"2089102\",\"name\":\"X. Mao\"},{\"authorId\":\"143652253\",\"name\":\"G. Zhou\"},{\"authorId\":\"143663410\",\"name\":\"F. Zhu\"}],\"doi\":\"10.1109/ACCESS.2020.3018752\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1e8e509a62c424055ffc050dc7cb41329674ea9\",\"title\":\"Stack-VS: Stacked Visual-Semantic Attention for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/b1e8e509a62c424055ffc050dc7cb41329674ea9\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1812.00301\",\"authors\":[{\"authorId\":\"8241534\",\"name\":\"Yantian Zha\"},{\"authorId\":\"2180892\",\"name\":\"Yikang Li\"},{\"authorId\":\"144149597\",\"name\":\"Tianshu Yu\"},{\"authorId\":\"1740315\",\"name\":\"S. Kambhampati\"},{\"authorId\":\"2913552\",\"name\":\"Baoxin Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e2ec7e28345ff0346fbeaf151f1f98ed3a8a047\",\"title\":\"Plan-Recognition-Driven Attention Modeling for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/0e2ec7e28345ff0346fbeaf151f1f98ed3a8a047\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1809.01816\",\"authors\":[{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1007/978-3-030-01267-0_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"782bc02684de81f98c92475957501801bf91e023\",\"title\":\"Visual Coreference Resolution in Visual Dialog using Neural Module Networks\",\"url\":\"https://www.semanticscholar.org/paper/782bc02684de81f98c92475957501801bf91e023\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2004.00849\",\"authors\":[{\"authorId\":\"47272083\",\"name\":\"Zhicheng Huang\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"1453953482\",\"name\":\"Bei Liu\"},{\"authorId\":\"143890169\",\"name\":\"Dongmei Fu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5c188874316557d501369e611a96cafc8058dffa\",\"title\":\"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers\",\"url\":\"https://www.semanticscholar.org/paper/5c188874316557d501369e611a96cafc8058dffa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1944765097\",\"name\":\"Beigeng Zhao\"}],\"doi\":\"10.1109/ACCESS.2020.3021312\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"acbbb090caf30b6f8bf140f03ebe912c9e6a8c6d\",\"title\":\"DrunaliaCap: Image Captioning for Drug-Related Paraphernalia With Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/acbbb090caf30b6f8bf140f03ebe912c9e6a8c6d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1911.12377\",\"authors\":[{\"authorId\":\"67344892\",\"name\":\"Federico Landi\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"40186452\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb4e5d203efae1f0838bb59e31026ce3e7511f87\",\"title\":\"Perceive, Transform, and Act: Multi-Modal Attention Networks for Vision-and-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/bb4e5d203efae1f0838bb59e31026ce3e7511f87\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9455885\",\"name\":\"Ankit Rathi\"}],\"doi\":\"10.1109/ICCECE48148.2020.9223087\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a93629ec132628e86ff7ab6cd21099f5968d58bf\",\"title\":\"Deep learning apporach for image captioning in Hindi language\",\"url\":\"https://www.semanticscholar.org/paper/a93629ec132628e86ff7ab6cd21099f5968d58bf\",\"venue\":\"2020 International Conference on Computer, Electrical & Communication Engineering (ICCECE)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48544896\",\"name\":\"Lun Huang\"},{\"authorId\":\"1788029\",\"name\":\"W. Wang\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/ICASSP.2019.8682392\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4dee549ba6f09e59c6d6d13434a2b023d868467a\",\"title\":\"Image Captioning with Two Cascaded Agents\",\"url\":\"https://www.semanticscholar.org/paper/4dee549ba6f09e59c6d6d13434a2b023d868467a\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1905.10226\",\"authors\":[{\"authorId\":\"10405058\",\"name\":\"Chenfei Wu\"},{\"authorId\":\"46433486\",\"name\":\"Yanzhao Zhou\"},{\"authorId\":\"103515844\",\"name\":\"Gen Li\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"39483833\",\"name\":\"Duyu Tang\"},{\"authorId\":\"38542466\",\"name\":\"X. Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"cda975be6f9fb55813bb6813b8c7c417331de7bd\",\"title\":\"Deep Reason: A Strong Baseline for Real-World Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/cda975be6f9fb55813bb6813b8c7c417331de7bd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"32818833\",\"name\":\"Mingda Zhang\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a9a074894a788afb0decb055574ea29b4190d636\",\"title\":\"Breaking Shortcuts by Masking for Robust Visual Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/a9a074894a788afb0decb055574ea29b4190d636\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s42979-020-00238-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f732495721356944027051bb14100436a5dbdf5\",\"title\":\"AACR: Feature Fusion Effects of Algebraic Amalgamation Composed Representation on (De)Compositional Network for Caption Generation for Images\",\"url\":\"https://www.semanticscholar.org/paper/3f732495721356944027051bb14100436a5dbdf5\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":\"2004.02435\",\"authors\":[{\"authorId\":\"2416001\",\"name\":\"Shashank Bujimalla\"},{\"authorId\":\"2536658\",\"name\":\"Mahesh Subedar\"},{\"authorId\":\"1798616\",\"name\":\"O. Tickoo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"81e7e74d0f5f200e575df1908a1b0a5f0500906b\",\"title\":\"B-SCST: Bayesian Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/81e7e74d0f5f200e575df1908a1b0a5f0500906b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.00390\",\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"5332711\",\"name\":\"Meng Wang\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"}],\"doi\":\"10.1109/cvpr42600.2020.00483\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c936e70cc1de52c5ad0ed5ec7a219bd25a46902c\",\"title\":\"More Grounded Image Captioning by Distilling Image-Text Matching Model\",\"url\":\"https://www.semanticscholar.org/paper/c936e70cc1de52c5ad0ed5ec7a219bd25a46902c\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"1752427\",\"name\":\"Jiansheng Chen\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"47718901\",\"name\":\"Weitao Wan\"},{\"authorId\":\"153447481\",\"name\":\"Youze Xue\"}],\"doi\":\"10.1109/TIP.2020.2969330\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"ff5a1ec81f327b711a5433e4cd40467215a13f39\",\"title\":\"Image Captioning With End-to-End Attribute Detection and Subsequent Attributes Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ff5a1ec81f327b711a5433e4cd40467215a13f39\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2685285\",\"name\":\"P. Palasek\"},{\"authorId\":\"48955389\",\"name\":\"Nilli Lavie\"},{\"authorId\":\"153242913\",\"name\":\"Luke Palmer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd4c523e227fd326be5d9f37d8dbdd7c57ad6713\",\"title\":\"Attentional demand estimation with attentive driving models\",\"url\":\"https://www.semanticscholar.org/paper/cd4c523e227fd326be5d9f37d8dbdd7c57ad6713\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"108358199\",\"name\":\"Viktar Atliha\"},{\"authorId\":\"1990294\",\"name\":\"D. Sesok\"}],\"doi\":\"10.1109/eStream50540.2020.9108880\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"08aa7d4d4e7fe6637ab83b8f8ae4144da517cdd6\",\"title\":\"Comparison of VGG and ResNet used as Encoders for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/08aa7d4d4e7fe6637ab83b8f8ae4144da517cdd6\",\"venue\":\"2020 IEEE Open Conference of Electrical, Electronic and Information Sciences (eStream)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1865728407\",\"name\":\"Avishek Siris\"},{\"authorId\":\"2840852\",\"name\":\"Jianbo Jiao\"},{\"authorId\":\"1888880\",\"name\":\"G. Tam\"},{\"authorId\":\"1388037008\",\"name\":\"Xianghua Xie\"},{\"authorId\":\"1726262\",\"name\":\"R. Lau\"}],\"doi\":\"10.1109/cvpr42600.2020.01215\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"06348f0f10a9599fe4c9b18fb7a2a972c49ba806\",\"title\":\"Inferring Attention Shift Ranks of Objects for Image Saliency\",\"url\":\"https://www.semanticscholar.org/paper/06348f0f10a9599fe4c9b18fb7a2a972c49ba806\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118023258\",\"name\":\"X. Wei\"},{\"authorId\":\"152602127\",\"name\":\"Tianzhu Zhang\"},{\"authorId\":\"2694924\",\"name\":\"Y. Li\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"1684705122\",\"name\":\"Feng Wu\"}],\"doi\":\"10.1109/CVPR42600.2020.01095\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"caabcf61499e00c78d8ee692b8939caf98544a9c\",\"title\":\"Multi-Modality Cross Attention Network for Image and Sentence Matching\",\"url\":\"https://www.semanticscholar.org/paper/caabcf61499e00c78d8ee692b8939caf98544a9c\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103382547\",\"name\":\"Junzhong Ji\"},{\"authorId\":\"49770078\",\"name\":\"C. Xu\"},{\"authorId\":\"46447188\",\"name\":\"X. Zhang\"},{\"authorId\":\"2692910\",\"name\":\"Boyue Wang\"},{\"authorId\":\"1688375\",\"name\":\"Xinhang Song\"}],\"doi\":\"10.1109/TIP.2020.3004729\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"63632ca4c1a6c16c9b6358624134880c3b23df90\",\"title\":\"Spatio-Temporal Memory Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/63632ca4c1a6c16c9b6358624134880c3b23df90\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2008.07935\",\"authors\":[{\"authorId\":\"48269537\",\"name\":\"Ye Zhu\"},{\"authorId\":\"50118837\",\"name\":\"Y. Wu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"96519714\",\"name\":\"Yan Yan\"}],\"doi\":\"10.1007/978-3-030-58592-1_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29ef96b859c2ec55b67fca6f2e59ac27a6bc2cb1\",\"title\":\"Describing Unseen Videos via Multi-Modal Cooperative Dialog Agents\",\"url\":\"https://www.semanticscholar.org/paper/29ef96b859c2ec55b67fca6f2e59ac27a6bc2cb1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"}],\"doi\":\"10.1609/AAAI.V34I07.6998\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9421fa26257e6a8d59bb874cf3b376c6d4c4118b\",\"title\":\"MemCap: Memorizing Style Knowledge for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9421fa26257e6a8d59bb874cf3b376c6d4c4118b\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2011.14901\",\"authors\":[{\"authorId\":\"32236286\",\"name\":\"Annika Lindh\"},{\"authorId\":\"144394786\",\"name\":\"R. Ross\"},{\"authorId\":\"34967075\",\"name\":\"John D. Kelleher\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"027aba2214f6a199d7be75150e94075f3752e27f\",\"title\":\"Language-Driven Region Pointer Advancement for Controllable Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/027aba2214f6a199d7be75150e94075f3752e27f\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2012.05545\",\"authors\":[{\"authorId\":\"67333377\",\"name\":\"Zeliang Song\"},{\"authorId\":\"1727617\",\"name\":\"Xiaofei Zhou\"},{\"authorId\":\"1855978\",\"name\":\"Zhendong Mao\"},{\"authorId\":\"2573626\",\"name\":\"Jianlong Tan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6f237f4ce10001a9b6cb47fae5333fa5ddb7e2d3\",\"title\":\"Image Captioning with Context-Aware Auxiliary Guidance\",\"url\":\"https://www.semanticscholar.org/paper/6f237f4ce10001a9b6cb47fae5333fa5ddb7e2d3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.07841\",\"authors\":[{\"authorId\":\"9919436\",\"name\":\"J. Yu\"},{\"authorId\":\"31115234\",\"name\":\"Jing Li\"},{\"authorId\":\"144007938\",\"name\":\"Zhou Yu\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/TCSVT.2019.2947482\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"99b5153235b0ee583803bbd7cd6bd9da161d5348\",\"title\":\"Multimodal Transformer With Multi-View Visual Representation for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/99b5153235b0ee583803bbd7cd6bd9da161d5348\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"2002.10832\",\"authors\":[{\"authorId\":\"90745780\",\"name\":\"Thomas Scialom\"},{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"1502219404\",\"name\":\"Paul-Alexis Dray\"},{\"authorId\":\"1767493\",\"name\":\"Jacopo Staiano\"},{\"authorId\":\"67192547\",\"name\":\"P. Gallinari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6364653539facfdd70837f460b20c62f7ca8a6d\",\"title\":\"What BERT Sees: Cross-Modal Transfer for Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/d6364653539facfdd70837f460b20c62f7ca8a6d\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":\"1806.01873\",\"authors\":[{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"},{\"authorId\":\"39978626\",\"name\":\"Lu Jiang\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TPAMI.2018.2890628\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aee265f6a19f9774c65d296cf9ec0e169365dda5\",\"title\":\"Focal Visual-Text Attention for Memex Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/aee265f6a19f9774c65d296cf9ec0e169365dda5\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2011.13922\",\"authors\":[{\"authorId\":\"1612421029\",\"name\":\"Yicong Hong\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"35653798\",\"name\":\"Yuankai Qi\"},{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"47873182\",\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d16bce335338372c1927f69d4b1f667a330b59d2\",\"title\":\"A Recurrent Vision-and-Language BERT for Navigation\",\"url\":\"https://www.semanticscholar.org/paper/d16bce335338372c1927f69d4b1f667a330b59d2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48986542\",\"name\":\"L. Jin\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"1993658042\",\"name\":\"Y. Pan\"},{\"authorId\":\"145113946\",\"name\":\"J. Tang\"}],\"doi\":\"10.1145/3394171.3414022\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3481a5795ee234df36ca259b15cfeb6d39b96fa4\",\"title\":\"Weakly-Supervised Image Hashing through Masked Visual-Semantic Graph-based Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/3481a5795ee234df36ca259b15cfeb6d39b96fa4\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1803.08314\",\"authors\":[{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"49404547\",\"name\":\"Hongsheng Li\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"},{\"authorId\":\"143982372\",\"name\":\"Dapeng Chen\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-01267-0_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"caab1c1d53718315f54bc4df42eb9a727fa18483\",\"title\":\"Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data\",\"url\":\"https://www.semanticscholar.org/paper/caab1c1d53718315f54bc4df42eb9a727fa18483\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49597404\",\"name\":\"Z. Fang\"},{\"authorId\":\"46700426\",\"name\":\"J. Liu\"},{\"authorId\":\"47003032\",\"name\":\"Y. Li\"},{\"authorId\":\"80526284\",\"name\":\"Yanyuan Qiao\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1016/J.PATCOG.2019.01.038\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa31126aa9dc7665bf236a51c6de31c4bf6c59ad\",\"title\":\"Improving visual question answering using dropout and enhanced question encoder\",\"url\":\"https://www.semanticscholar.org/paper/aa31126aa9dc7665bf236a51c6de31c4bf6c59ad\",\"venue\":\"Pattern Recognit.\",\"year\":2019},{\"arxivId\":\"1907.12905\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":\"10.1145/3343031.3351060\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"title\":\"Watch It Twice: Video Captioning with a Refocused Video Encoder\",\"url\":\"https://www.semanticscholar.org/paper/5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1908.04919\",\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"58a77455b1c38afe1eab4bec664bd866eba1573d\",\"title\":\"Towards Diverse and Accurate Image Captions via Reinforcing Determinantal Point Process\",\"url\":\"https://www.semanticscholar.org/paper/58a77455b1c38afe1eab4bec664bd866eba1573d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.00311\",\"authors\":[{\"authorId\":\"31264049\",\"name\":\"Sarthak Garg\"},{\"authorId\":\"22272110\",\"name\":\"Joel Ruben Antony Moniz\"},{\"authorId\":\"144844721\",\"name\":\"Anshu Aviral\"},{\"authorId\":\"2660028\",\"name\":\"Priyatham Bollimpalli\"}],\"doi\":\"10.18653/v1/P19-1660\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"20e582dd2afab1778ad3fa9438a6451d5ebe1af2\",\"title\":\"Learning to Relate from Captions and Bounding Boxes\",\"url\":\"https://www.semanticscholar.org/paper/20e582dd2afab1778ad3fa9438a6451d5ebe1af2\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"2007.13262\",\"authors\":[{\"authorId\":\"39209233\",\"name\":\"Siwen Luo\"},{\"authorId\":\"2046142\",\"name\":\"S. Han\"},{\"authorId\":\"33053279\",\"name\":\"Kaiyuan Sun\"},{\"authorId\":\"48422087\",\"name\":\"Josiah Poon\"}],\"doi\":\"10.1007/978-3-030-63830-6_44\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5225c9ca12387634ad93d6ccc547f5b2347d88a3\",\"title\":\"REXUP: I REason, I EXtract, I UPdate with Structured Compositional Reasoning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/5225c9ca12387634ad93d6ccc547f5b2347d88a3\",\"venue\":\"ICONIP\",\"year\":2020},{\"arxivId\":\"1911.04058\",\"authors\":[{\"authorId\":\"48615049\",\"name\":\"Y. Xu\"},{\"authorId\":\"46308159\",\"name\":\"L. Chen\"},{\"authorId\":\"120235436\",\"name\":\"Zhongwei Cheng\"},{\"authorId\":\"71138167\",\"name\":\"Lixin Duan\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.34\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2fdcc61d9a02412f5eb5d596ee41c8504a83c20a\",\"title\":\"Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/2fdcc61d9a02412f5eb5d596ee41c8504a83c20a\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2007.12146\",\"authors\":[{\"authorId\":\"66536530\",\"name\":\"Yash Kant\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606382599\",\"name\":\"Peter Anderson\"},{\"authorId\":\"5153264\",\"name\":\"A. Schwing\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"}],\"doi\":\"10.1007/978-3-030-58545-7_41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f92dc7d9a61f7b8b5b7cb58f3895541e266ce057\",\"title\":\"Spatially Aware Multimodal Transformers for TextVQA\",\"url\":\"https://www.semanticscholar.org/paper/f92dc7d9a61f7b8b5b7cb58f3895541e266ce057\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152109174\",\"name\":\"Yuting Zhao\"},{\"authorId\":\"2936411\",\"name\":\"Mamoru Komachi\"},{\"authorId\":\"1981103\",\"name\":\"T. Kajiwara\"},{\"authorId\":\"2427516\",\"name\":\"Chenhui Chu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"36601455a41ac3d1eaae8b8fb4769e9356276154\",\"title\":\"Double Attention-based Multimodal Neural Machine Translation with Semantic Image Regions\",\"url\":\"https://www.semanticscholar.org/paper/36601455a41ac3d1eaae8b8fb4769e9356276154\",\"venue\":\"EAMT\",\"year\":2020},{\"arxivId\":\"2008.05231\",\"authors\":[{\"authorId\":\"67213349\",\"name\":\"N. Messina\"},{\"authorId\":\"144514869\",\"name\":\"G. Amato\"},{\"authorId\":\"1699411\",\"name\":\"Andrea Esuli\"},{\"authorId\":\"1846129\",\"name\":\"F. Falchi\"},{\"authorId\":\"2209975\",\"name\":\"C. Gennaro\"},{\"authorId\":\"1405499517\",\"name\":\"St\\u00e9phane Marchand-Maillet\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"52011033fb859c38bbcc82c311667feb38994ae3\",\"title\":\"Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders\",\"url\":\"https://www.semanticscholar.org/paper/52011033fb859c38bbcc82c311667feb38994ae3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.08045\",\"authors\":[{\"authorId\":\"152516150\",\"name\":\"Aniket Agarwal\"},{\"authorId\":\"1703122502\",\"name\":\"Ayush Mangal\"},{\"authorId\":\"80465887\",\"name\":\"Vipul\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac73d0abab68604b9c7a2b98635516eda899c7b3\",\"title\":\"Visual Relationship Detection using Scene Graphs: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/ac73d0abab68604b9c7a2b98635516eda899c7b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.09034\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"8088602\",\"name\":\"Ehsan Abbasnejad\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1007/978-3-030-58607-2_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"29121a31e4d684839cfd0bb358f33ea1266cece5\",\"title\":\"Learning What Makes a Difference from Counterfactual Examples and Gradient Supervision\",\"url\":\"https://www.semanticscholar.org/paper/29121a31e4d684839cfd0bb358f33ea1266cece5\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1805.12589\",\"authors\":[{\"authorId\":\"31121723\",\"name\":\"A. Deshpande\"},{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"39709900\",\"name\":\"Liwei Wang\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1109/CVPR.2019.01095\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e7fe886600399448f3282c8da8fd98ab7e50eae3\",\"title\":\"Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech\",\"url\":\"https://www.semanticscholar.org/paper/e7fe886600399448f3282c8da8fd98ab7e50eae3\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143694682\",\"name\":\"A. Bellini\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"168de54915fa45d8bd0d33b53ed54eb0cef33463\",\"title\":\"Towards open-ended VQA models using transformers\",\"url\":\"https://www.semanticscholar.org/paper/168de54915fa45d8bd0d33b53ed54eb0cef33463\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.13540\",\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"103278467\",\"name\":\"Minuk Ma\"},{\"authorId\":\"3549056\",\"name\":\"K. Kim\"},{\"authorId\":\"72108920\",\"name\":\"S. Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/IJCNN.2019.8852087\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1165b090e5ec6a9311ecba5be4f6aac0a1a4586b\",\"title\":\"Gaining Extra Supervision via Multi-task learning for Multi-Modal Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1165b090e5ec6a9311ecba5be4f6aac0a1a4586b\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696589727\",\"name\":\"Dongming Zhou\"},{\"authorId\":\"7924036\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"144111674\",\"name\":\"Zhixin Li\"},{\"authorId\":\"48708659\",\"name\":\"Zhiwen Wang\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206932\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a32ac0995afcbc748fa6533b941e9834ec7bfc2e\",\"title\":\"Multi-level Visual Fusion Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a32ac0995afcbc748fa6533b941e9834ec7bfc2e\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145523333\",\"name\":\"Peng Yao\"},{\"authorId\":\"102227937\",\"name\":\"J. Li\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"1749850\",\"name\":\"J. Liu\"}],\"doi\":\"10.1109/ICME46284.2020.9102935\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"59d65f0719287512f3f605615f64b7eda27db97b\",\"title\":\"Modeling Local and Global Contexts for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59d65f0719287512f3f605615f64b7eda27db97b\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming-Jen Huang\"},{\"authorId\":\"2028457\",\"name\":\"Chun-Fang Huang\"},{\"authorId\":null,\"name\":\"Chiching Wei\"}],\"doi\":\"10.5121/csit.2020.100917\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"157526b8b428cf220bb97b10f6a99230e06ab115\",\"title\":\"DOCPRO: A Framework for Building Document Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/157526b8b428cf220bb97b10f6a99230e06ab115\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2008.11009\",\"authors\":[{\"authorId\":\"35466168\",\"name\":\"Jian Han Lim\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"},{\"authorId\":\"145923164\",\"name\":\"K. Ng\"},{\"authorId\":\"2034793\",\"name\":\"Lixin Fan\"},{\"authorId\":\"153096457\",\"name\":\"Q. Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f37d3bd1f656e946ee7e0e99070ad7a6950f688\",\"title\":\"Protect, Show, Attend and Tell: Image Captioning Model with Ownership Protection\",\"url\":\"https://www.semanticscholar.org/paper/0f37d3bd1f656e946ee7e0e99070ad7a6950f688\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1805.08389\",\"authors\":[{\"authorId\":\"35585536\",\"name\":\"Jialin Wu\"},{\"authorId\":\"32193161\",\"name\":\"Zeyuan Hu\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"91956c41190231eefd2186f21b79d1ca1495a68e\",\"title\":\"Joint Image Captioning and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/91956c41190231eefd2186f21b79d1ca1495a68e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1906.06362\",\"authors\":[{\"authorId\":\"7975935\",\"name\":\"Daphne Ippolito\"},{\"authorId\":\"46218926\",\"name\":\"Reno Kriz\"},{\"authorId\":\"51119291\",\"name\":\"M. Kustikova\"},{\"authorId\":\"2662374\",\"name\":\"Jo\\u00e3o Sedoc\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"}],\"doi\":\"10.18653/v1/P19-1365\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fd846869e6f25d9b1a524aef8b54a08b81a1b1fa\",\"title\":\"Comparison of Diverse Decoding Methods from Conditional Language Models\",\"url\":\"https://www.semanticscholar.org/paper/fd846869e6f25d9b1a524aef8b54a08b81a1b1fa\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1810.03649\",\"authors\":[{\"authorId\":\"31448527\",\"name\":\"S. Ramakrishnan\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"45ec1446f42c0a7c7fe74319118335c76e0f7b19\",\"title\":\"Overcoming Language Priors in Visual Question Answering with Adversarial Regularization\",\"url\":\"https://www.semanticscholar.org/paper/45ec1446f42c0a7c7fe74319118335c76e0f7b19\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2009.09809\",\"authors\":[{\"authorId\":\"51238351\",\"name\":\"Andr\\u00e9s Mafla\"},{\"authorId\":\"151478793\",\"name\":\"S. Dey\"},{\"authorId\":\"35570245\",\"name\":\"Ali Furkan Biten\"},{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b67759f193e2c39877723424df0b3d5f91c0bf0b\",\"title\":\"Multi-Modal Reasoning Graph for Scene-Text Based Fine-Grained Image Classification and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/b67759f193e2c39877723424df0b3d5f91c0bf0b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.09103\",\"authors\":[{\"authorId\":\"40269075\",\"name\":\"R. Burt\"},{\"authorId\":\"3403588\",\"name\":\"Nina N Thigpen\"},{\"authorId\":\"144310558\",\"name\":\"A. Keil\"},{\"authorId\":\"143961030\",\"name\":\"J. Pr\\u00edncipe\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bb968777ae146b8534944764c6fabb117cc9161\",\"title\":\"Unsupervised Foveal Vision Neural Networks with Top-Down Attention\",\"url\":\"https://www.semanticscholar.org/paper/7bb968777ae146b8534944764c6fabb117cc9161\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"2005.08722\",\"authors\":[{\"authorId\":\"2512283\",\"name\":\"Shahin Amiriparian\"},{\"authorId\":\"1704366927\",\"name\":\"Pawel Winokurow\"},{\"authorId\":\"1423719032\",\"name\":\"Vincent Karas\"},{\"authorId\":\"31696419\",\"name\":\"Sandra Ottl\"},{\"authorId\":\"31766982\",\"name\":\"Maurice Gerczuk\"},{\"authorId\":\"123939577\",\"name\":\"B. Schuller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c1bb66863b10a3653c6e592dc31783e4a9945fe\",\"title\":\"A Novel Fusion of Attention and Sequence to Sequence Autoencoders to Predict Sleepiness From Speech\",\"url\":\"https://www.semanticscholar.org/paper/7c1bb66863b10a3653c6e592dc31783e4a9945fe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1801.09041\",\"authors\":[{\"authorId\":\"48933900\",\"name\":\"Q. Li\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"2846025\",\"name\":\"D. Yu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.18653/v1/D18-1164\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd2f8bb5fa881797fad0448547e307a18bf897da\",\"title\":\"Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions\",\"url\":\"https://www.semanticscholar.org/paper/dd2f8bb5fa881797fad0448547e307a18bf897da\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"2007.02036\",\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"103278467\",\"name\":\"Minuk Ma\"},{\"authorId\":\"48920375\",\"name\":\"T. Pham\"},{\"authorId\":\"97531942\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/cvpr42600.2020.01012\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7fa5fdaacc87165a0281babd96949e8ec74d3a41\",\"title\":\"Modality Shifting Attention Network for Multi-Modal Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7fa5fdaacc87165a0281babd96949e8ec74d3a41\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32102885\",\"name\":\"Rachel Gardner\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ad707fc8b36a8f3daf8742cf92fcf099de434cec\",\"title\":\"A Deep Learning Approach for Identification of Confusion in Unstructured Crowdsourced Annotations\",\"url\":\"https://www.semanticscholar.org/paper/ad707fc8b36a8f3daf8742cf92fcf099de434cec\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47968201\",\"name\":\"Lixin Liu\"},{\"authorId\":\"46741143\",\"name\":\"Jiajun Tang\"},{\"authorId\":\"145078589\",\"name\":\"Xiaojun Wan\"},{\"authorId\":\"35310979\",\"name\":\"Zongming Guo\"}],\"doi\":\"10.1109/ICCV.2019.00434\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b774d6cd89cb27e62f03f183b89b7cacc412e131\",\"title\":\"Generating Diverse and Descriptive Image Captions Using Visual Paraphrases\",\"url\":\"https://www.semanticscholar.org/paper/b774d6cd89cb27e62f03f183b89b7cacc412e131\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1505835104\",\"name\":\"Phuong-Binh Vo\"},{\"authorId\":\"93763734\",\"name\":\"Trong-Dat Phan\"},{\"authorId\":\"50376884\",\"name\":\"Minh-Son Dao\"},{\"authorId\":\"48909205\",\"name\":\"Koji Zettsu\"}],\"doi\":\"10.1109/BigData47090.2019.9005636\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b823158adfd36fd86d2a6e67147380828c7c2a23\",\"title\":\"Association Model between Visual Feature and AQI Rank Using Lifelog Data\",\"url\":\"https://www.semanticscholar.org/paper/b823158adfd36fd86d2a6e67147380828c7c2a23\",\"venue\":\"2019 IEEE International Conference on Big Data (Big Data)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152747612\",\"name\":\"Zhenyu Yang\"},{\"authorId\":\"153874396\",\"name\":\"Qiao Liu\"}],\"doi\":\"10.1109/ACCESS.2020.2980578\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"059093bc3bb44949e9754fb4366f0be7cea34bac\",\"title\":\"ATT-BM-SOM: A Framework of Effectively Choosing Image Information and Optimizing Syntax for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/059093bc3bb44949e9754fb4366f0be7cea34bac\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145422343\",\"name\":\"Dalu Guo\"},{\"authorId\":\"93374657\",\"name\":\"C. Xu\"},{\"authorId\":\"1490934465\",\"name\":\"Dacheng Tao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"22fa5a7f3f00737c1912cbd6b2cac248a7e734a4\",\"title\":\"Bilinear Graph Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/22fa5a7f3f00737c1912cbd6b2cac248a7e734a4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1904.06861\",\"authors\":[{\"authorId\":\"2903539\",\"name\":\"Junlong Gao\"},{\"authorId\":\"1705047\",\"name\":\"S. Wang\"},{\"authorId\":\"1755176\",\"name\":\"Shanshe Wang\"},{\"authorId\":\"10634370\",\"name\":\"S. Ma\"},{\"authorId\":\"48385803\",\"name\":\"W. Gao\"}],\"doi\":\"10.1109/CVPR.2019.00646\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6b3d6a37e7e77f5d5c763a4abeade256324268c\",\"title\":\"Self-Critical N-Step Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b6b3d6a37e7e77f5d5c763a4abeade256324268c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49970148\",\"name\":\"Zhenqiang Li\"},{\"authorId\":\"48355461\",\"name\":\"Yifei Huang\"},{\"authorId\":\"49678929\",\"name\":\"Y. Sato\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59e777069a4a24b47dfb27712bb3e5dcf3392f62\",\"title\":\"F \\\" , $ F % , $ conv Conv Fusion conv ReLU concat X $ Spatial Attention Submodule \\u03b1 $ \\u03a3 RNNtask Feature Encoding Attention Pooling Temporal Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/59e777069a4a24b47dfb27712bb3e5dcf3392f62\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2726914\",\"name\":\"Shuren Zhou\"},{\"authorId\":\"145782499\",\"name\":\"Jia Qiu\"}],\"doi\":\"10.1109/IICSPI48186.2019.9095895\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f5f720ebc0f5b5cbc10a526bfd7b0cf34de95ac\",\"title\":\"RSSD: Object Detection via Attention Regions in SSD Detector\",\"url\":\"https://www.semanticscholar.org/paper/1f5f720ebc0f5b5cbc10a526bfd7b0cf34de95ac\",\"venue\":\"2019 2nd International Conference on Safety Produce Informatization (IICSPI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2548384\",\"name\":\"Hannaneh Hajishirzi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c\",\"title\":\"Probing Text Models for Common Ground with Visual Representations\",\"url\":\"https://www.semanticscholar.org/paper/467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31696860\",\"name\":\"Shurong Sheng\"},{\"authorId\":\"145446752\",\"name\":\"Marie-Francine Moens\"}],\"doi\":\"10.1145/3343031.3350972\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d35c56b2a86d928fabe380f5380e83a7992c1d7a\",\"title\":\"Generating Captions for Images of Ancient Artworks\",\"url\":\"https://www.semanticscholar.org/paper/d35c56b2a86d928fabe380f5380e83a7992c1d7a\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1508389232\",\"name\":\"Junyi Feng\"},{\"authorId\":\"145197100\",\"name\":\"P. Gong\"},{\"authorId\":\"1508486471\",\"name\":\"Guanghui Qiu\"}],\"doi\":\"10.1145/3376067.3376082\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"732af43dac39f3ed15a43ab916eabb80dd67e72a\",\"title\":\"MDAnet: Multiple Fusion Network with Double Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/732af43dac39f3ed15a43ab916eabb80dd67e72a\",\"venue\":\"ICVIP\",\"year\":2019},{\"arxivId\":\"1909.00301\",\"authors\":[{\"authorId\":\"46700226\",\"name\":\"Jiacheng Liu\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.18653/v1/D19-1515\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9abeabd42883b55a1b01e812aa8856280abe0bad\",\"title\":\"Phrase Grounding by Soft-Label Chain Conditional Random Field\",\"url\":\"https://www.semanticscholar.org/paper/9abeabd42883b55a1b01e812aa8856280abe0bad\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990265392\",\"name\":\"Leigang Qu\"},{\"authorId\":\"38813990\",\"name\":\"M. Liu\"},{\"authorId\":\"145479818\",\"name\":\"D. Cao\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3394171.3413961\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"922d677867e1aa2a7cca05241af4746a0be04dd0\",\"title\":\"Context-Aware Multi-View Summarization Network for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/922d677867e1aa2a7cca05241af4746a0be04dd0\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9024867\",\"name\":\"Jongkwang Hong\"},{\"authorId\":\"48079221\",\"name\":\"Sungho Park\"},{\"authorId\":\"1703310\",\"name\":\"Hyeran Byun\"}],\"doi\":\"10.1016/j.neucom.2020.03.098\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72e48298519b5ff583e585a65eeea3ac10556adf\",\"title\":\"Selective residual learning for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/72e48298519b5ff583e585a65eeea3ac10556adf\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52152408\",\"name\":\"Zihan Guo\"},{\"authorId\":\"9100598\",\"name\":\"Dezhi Han\"}],\"doi\":\"10.3390/s20236758\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ea04b05c82087771b905fbdedd5ce6dfe48de097\",\"title\":\"Multi-Modal Explicit Sparse Attention Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ea04b05c82087771b905fbdedd5ce6dfe48de097\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":\"1810.04020\",\"authors\":[{\"authorId\":\"47412302\",\"name\":\"M. Z. Hossain\"},{\"authorId\":\"2470423\",\"name\":\"Ferdous Sohel\"},{\"authorId\":\"2222847\",\"name\":\"M. F. Shiratuddin\"},{\"authorId\":\"47028380\",\"name\":\"Hamid Laga\"}],\"doi\":\"10.1145/3295748\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e27d44e3fac723ccb703e0a83b22711bd42efe8\",\"title\":\"A Comprehensive Survey of Deep Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7e27d44e3fac723ccb703e0a83b22711bd42efe8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1906.12165\",\"authors\":[{\"authorId\":\"1764508\",\"name\":\"Z. Zhang\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"34064720\",\"name\":\"Zhijie Lin\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.24963/ijcai.2019/610\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d145292fd11ca274693ec9ea6941a9147ae5868\",\"title\":\"Localizing Unseen Activities in Video via Image Query\",\"url\":\"https://www.semanticscholar.org/paper/5d145292fd11ca274693ec9ea6941a9147ae5868\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1860938\",\"name\":\"M. Ghanimifard\"},{\"authorId\":\"2995275\",\"name\":\"Simon Dobnik\"}],\"doi\":\"10.18653/v1/W19-8668\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7c6b67838d895f08a177634f553b5dfc669f44c5\",\"title\":\"What goes into a word: generating image descriptions with top-down spatial knowledge\",\"url\":\"https://www.semanticscholar.org/paper/7c6b67838d895f08a177634f553b5dfc669f44c5\",\"venue\":\"INLG\",\"year\":2019},{\"arxivId\":\"1904.12004\",\"authors\":[{\"authorId\":\"98243944\",\"name\":\"Chenglong Wang\"},{\"authorId\":\"3407947\",\"name\":\"R. Bunel\"},{\"authorId\":\"1729912\",\"name\":\"Krishnamurthy Dvijotham\"},{\"authorId\":\"2421691\",\"name\":\"Po-Sen Huang\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"}],\"doi\":\"10.1109/CVPR.2019.01254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59b439bde73d80dccf367d414e209d08d312c059\",\"title\":\"Knowing When to Stop: Evaluation and Verification of Conformity to Output-Size Specifications\",\"url\":\"https://www.semanticscholar.org/paper/59b439bde73d80dccf367d414e209d08d312c059\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1476813856\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1016/j.patrec.2019.11.018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2cc1a8014ab6de2e5ef45f7f2204ba7e73cb10ec\",\"title\":\"Explaining digital humanities by aligning images and textual descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2cc1a8014ab6de2e5ef45f7f2204ba7e73cb10ec\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":\"1912.11637\",\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"143725038\",\"name\":\"Qi Su\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"title\":\"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1911.07928\",\"authors\":[{\"authorId\":\"144052839\",\"name\":\"Wei Pang\"},{\"authorId\":\"50142157\",\"name\":\"X. Wang\"}],\"doi\":\"10.1609/AAAI.V34I07.6856\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ef9eade43a21e4b628c977f608661d0988d01a9\",\"title\":\"Visual Dialogue State Tracking for Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/8ef9eade43a21e4b628c977f608661d0988d01a9\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394741222\",\"name\":\"Yuling Gui\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"97522088\",\"name\":\"Ye Zhao\"}],\"doi\":\"10.1145/3347319.3356839\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"title\":\"Semantic Enhanced Encoder-Decoder Network (SEN) for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2706292\",\"name\":\"Xianhua Zeng\"},{\"authorId\":\"145117241\",\"name\":\"L. Wen\"},{\"authorId\":\"115986457\",\"name\":\"Yang Xu\"},{\"authorId\":\"1893927760\",\"name\":\"Conghui Ji\"}],\"doi\":\"10.1016/j.cmpb.2020.105700\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cbe08984fe9bb669e11e73b8fb14c9d43ab57017\",\"title\":\"Generating diagnostic report for medical image by high-middle-level visual information incorporation on double deep learning models\",\"url\":\"https://www.semanticscholar.org/paper/cbe08984fe9bb669e11e73b8fb14c9d43ab57017\",\"venue\":\"Comput. Methods Programs Biomed.\",\"year\":2020},{\"arxivId\":\"2003.09853\",\"authors\":[{\"authorId\":\"150257726\",\"name\":\"P. Bongini\"},{\"authorId\":\"41172759\",\"name\":\"Federico Becattini\"},{\"authorId\":\"1749498\",\"name\":\"Andrew D. Bagdanov\"},{\"authorId\":\"8196487\",\"name\":\"A. D. Bimbo\"}],\"doi\":\"10.1088/1757-899X/949/1/012074\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a08cb38aa4a5a4bc2f6fa0c4d379d23e874c0b7\",\"title\":\"Visual Question Answering for Cultural Heritage\",\"url\":\"https://www.semanticscholar.org/paper/0a08cb38aa4a5a4bc2f6fa0c4d379d23e874c0b7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143647292\",\"name\":\"F. Liu\"},{\"authorId\":\"49560222\",\"name\":\"Guanghui Xu\"},{\"authorId\":\"1720738301\",\"name\":\"Qi Wu\"},{\"authorId\":\"150270181\",\"name\":\"Qing Du\"},{\"authorId\":\"145229535\",\"name\":\"W. Jia\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1145/3394171.3413924\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"29df8ed16d3787c710e0128dc1948a95990cc9fb\",\"title\":\"Cascade Reasoning Network for Text-based Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/29df8ed16d3787c710e0128dc1948a95990cc9fb\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1784897\",\"name\":\"Incheol Kim\"}],\"doi\":\"10.1155/2020/8567271\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3900d795a4914f0e8c346397c3dff4038d41591\",\"title\":\"Visual Experience-Based Question Answering with Complex Multimodal Environments\",\"url\":\"https://www.semanticscholar.org/paper/a3900d795a4914f0e8c346397c3dff4038d41591\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"49039966\",\"name\":\"W. Zhang\"},{\"authorId\":\"51126032\",\"name\":\"Zhuoqian Yang\"},{\"authorId\":\"31055300\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"1492131589\",\"name\":\"Yue Hu\"}],\"doi\":\"10.1016/j.knosys.2020.106150\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"14663f521f8c2590d94cb00094ae1353558f2585\",\"title\":\"Cross-modal learning with prior visual relation knowledge\",\"url\":\"https://www.semanticscholar.org/paper/14663f521f8c2590d94cb00094ae1353558f2585\",\"venue\":\"Knowl. Based Syst.\",\"year\":2020},{\"arxivId\":\"2007.11731\",\"authors\":[{\"authorId\":\"1828787912\",\"name\":\"Yiwu Zhong\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"},{\"authorId\":\"101777533\",\"name\":\"J. Chen\"},{\"authorId\":null,\"name\":\"Dong Yu\"},{\"authorId\":\"47002659\",\"name\":\"Yanchao Li\"}],\"doi\":\"10.1007/978-3-030-58568-6_13\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"66ed8795eb6de5d2a6b204baac9378d6d28136cc\",\"title\":\"Comprehensive Image Captioning via Scene Graph Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/66ed8795eb6de5d2a6b204baac9378d6d28136cc\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.14231\",\"authors\":[{\"authorId\":\"47287647\",\"name\":\"Sen He\"},{\"authorId\":\"3157379\",\"name\":\"Wentong Liao\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"1779035\",\"name\":\"B. Rosenhahn\"},{\"authorId\":\"3251678\",\"name\":\"N. Pugeault\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"657cce51f80e272373ab4fc0dabf5dc8b30c0070\",\"title\":\"Image Captioning through Image Transformer\",\"url\":\"https://www.semanticscholar.org/paper/657cce51f80e272373ab4fc0dabf5dc8b30c0070\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2326243\",\"name\":\"Arun Balajee Vasudevan\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.3929/ETHZ-B-000438408\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"53e77b526587b3c3bf7bb359590692a081b53260\",\"title\":\"Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory\",\"url\":\"https://www.semanticscholar.org/paper/53e77b526587b3c3bf7bb359590692a081b53260\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144493079\",\"name\":\"Z. Hu\"},{\"authorId\":\"1754240987\",\"name\":\"Jielong Wei\"},{\"authorId\":\"2639734\",\"name\":\"Qingbao Huang\"},{\"authorId\":\"1904290800\",\"name\":\"Hanyu Liang\"},{\"authorId\":\"1908173213\",\"name\":\"Xingmao Zhang\"},{\"authorId\":\"1901543027\",\"name\":\"Qingguang Liu\"}],\"doi\":\"10.1109/DSC50466.2020.00040\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ad3b719a1a0868c29dbf72e81c8c3d226196c28\",\"title\":\"Graph Convolutional Network for Visual Question Answering Based on Fine-grained Question Representation\",\"url\":\"https://www.semanticscholar.org/paper/2ad3b719a1a0868c29dbf72e81c8c3d226196c28\",\"venue\":\"2020 IEEE Fifth International Conference on Data Science in Cyberspace (DSC)\",\"year\":2020},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1733073249\",\"name\":\"Anh-Vu Mai-Nguyen\"},{\"authorId\":\"93763734\",\"name\":\"Trong-Dat Phan\"},{\"authorId\":\"121206993\",\"name\":\"Anh-Khoa Vo\"},{\"authorId\":\"1733072724\",\"name\":\"Van-Luon Tran\"},{\"authorId\":\"50376884\",\"name\":\"Minh-Son Dao\"},{\"authorId\":\"48909205\",\"name\":\"Koji Zettsu\"}],\"doi\":\"10.1145/3379172.3391722\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7bfe8f52018eb103de5e325fa39cf098c7b800a\",\"title\":\"BIDAL-HCMUS@LSC2020: An Interactive Multimodal Lifelog Retrieval with Query-to-Sample Attention-based Search Engine\",\"url\":\"https://www.semanticscholar.org/paper/a7bfe8f52018eb103de5e325fa39cf098c7b800a\",\"venue\":\"LSC@ICMR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50580169\",\"name\":\"Y. Chen\"},{\"authorId\":\"144784813\",\"name\":\"S. Gong\"},{\"authorId\":\"1809420\",\"name\":\"Loris Bazzani\"}],\"doi\":\"10.1109/CVPR42600.2020.00307\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78f69364531794550130389342b7bc0ff785b7e9\",\"title\":\"Image Search With Text Feedback by Visiolinguistic Attention Learning\",\"url\":\"https://www.semanticscholar.org/paper/78f69364531794550130389342b7bc0ff785b7e9\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"},{\"authorId\":\"51172982\",\"name\":\"Haiwen Cao\"},{\"authorId\":\"134473682\",\"name\":\"Mingwen Shao\"},{\"authorId\":\"46740305\",\"name\":\"Chunlei Wu\"}],\"doi\":\"10.1016/j.neucom.2019.12.073\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"50260b547a481a95a2346759da8dba9366e89348\",\"title\":\"Multi-Attention Generative Adversarial Network for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/50260b547a481a95a2346759da8dba9366e89348\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1910.03853\",\"authors\":[{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"145592288\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"1382537904\",\"name\":\"Chengpeng Dai\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"46246806\",\"name\":\"Chia-Wen Lin\"},{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"1740430\",\"name\":\"B. Zhang\"},{\"authorId\":\"1835006\",\"name\":\"Feiyue Huang\"},{\"authorId\":\"40626740\",\"name\":\"Liujuan Cao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7755545e3ed52ac13db79a844d631f4f435f4cf7\",\"title\":\"Semantic-aware Image Deblurring\",\"url\":\"https://www.semanticscholar.org/paper/7755545e3ed52ac13db79a844d631f4f435f4cf7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150156999\",\"name\":\"J. Park\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d78eaf500f06764f635da3924a252490232f451e\",\"title\":\"GRE: Evaluating computer vision models on Generalizability, Robustness, and Extensibility\",\"url\":\"https://www.semanticscholar.org/paper/d78eaf500f06764f635da3924a252490232f451e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1392406002\",\"name\":\"Arturs Polis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"4bfeae734bced5b2613af9f7d8271354b614e08e\",\"title\":\"Paragraph-length image captioning using hierarchical recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/4bfeae734bced5b2613af9f7d8271354b614e08e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1812.03402\",\"authors\":[{\"authorId\":\"2434622\",\"name\":\"Z. Seymour\"},{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"35260743\",\"name\":\"Han-Pang Chiu\"},{\"authorId\":\"1789477\",\"name\":\"S. Samarasekera\"},{\"authorId\":\"153411819\",\"name\":\"R. Kumar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"da5062a8b794445445058c00d6879d17c7510494\",\"title\":\"Semantically-Aware Attentive Neural Embeddings for Image-based Visual Localization\",\"url\":\"https://www.semanticscholar.org/paper/da5062a8b794445445058c00d6879d17c7510494\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1609/AAAI.V34I07.6731\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4df184d6a74f1ffd84b644735c9afb5060552770\",\"title\":\"Joint Commonsense and Relation Reasoning for Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4df184d6a74f1ffd84b644735c9afb5060552770\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"fc8df4ad35282ccf19261e02de87d8e35c956537\",\"title\":\"Binary Image Selection (BISON): Interpretable Evaluation of Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/fc8df4ad35282ccf19261e02de87d8e35c956537\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2005.07493\",\"authors\":[{\"authorId\":\"144992211\",\"name\":\"Shubham Agarwal\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"1576788264\",\"name\":\"Joon-Young Lee\"},{\"authorId\":\"2621022\",\"name\":\"Ioannis Konstas\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/2020.acl-main.728\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c3baa99376fac050160c25a9c6d92d9baa5846c\",\"title\":\"History for Visual Dialog: Do we really need it?\",\"url\":\"https://www.semanticscholar.org/paper/8c3baa99376fac050160c25a9c6d92d9baa5846c\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4009206\",\"name\":\"O. Kovaleva\"},{\"authorId\":\"1866532\",\"name\":\"Chaitanya Shivade\"},{\"authorId\":\"33201965\",\"name\":\"Satyananda Kashyap\"},{\"authorId\":\"1410148565\",\"name\":\"Karina Kanjaria\"},{\"authorId\":\"40346984\",\"name\":\"Joy T. Wu\"},{\"authorId\":\"13403287\",\"name\":\"D. Ballah\"},{\"authorId\":\"1388126424\",\"name\":\"Adam Coy\"},{\"authorId\":\"2308391\",\"name\":\"Alexandros Karargyris\"},{\"authorId\":\"2230103\",\"name\":\"Yufan Guo\"},{\"authorId\":\"1768272818\",\"name\":\"David James Beymer\"},{\"authorId\":\"1681193\",\"name\":\"Anna Rumshisky\"},{\"authorId\":\"80257800\",\"name\":\"Vandana Mukherjee\"}],\"doi\":\"10.18653/v1/2020.bionlp-1.6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6609489a0f800a9ef411efdcfca4c014c4e86aa8\",\"title\":\"Towards Visual Dialog for Radiology\",\"url\":\"https://www.semanticscholar.org/paper/6609489a0f800a9ef411efdcfca4c014c4e86aa8\",\"venue\":\"BioNLP\",\"year\":2020},{\"arxivId\":\"2010.16010\",\"authors\":[{\"authorId\":\"1390575046\",\"name\":\"Yangyang Guo\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"46504200\",\"name\":\"Zhiyong Cheng\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9b9e35875ad771961ad2d35f7b9cd645b96afd04\",\"title\":\"Loss-rescaling VQA: Revisiting Language Prior Problem from a Class-imbalance View\",\"url\":\"https://www.semanticscholar.org/paper/9b9e35875ad771961ad2d35f7b9cd645b96afd04\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.11976\",\"authors\":[{\"authorId\":\"2068427\",\"name\":\"Ankan Bansal\"},{\"authorId\":\"46867282\",\"name\":\"Y. Zhang\"},{\"authorId\":\"9215658\",\"name\":\"R. Chellappa\"}],\"doi\":\"10.1007/978-3-030-58589-1_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18ba4e542a5206a40e308f54ceffc6786b7d94d2\",\"title\":\"Visual Question Answering on Image Sets\",\"url\":\"https://www.semanticscholar.org/paper/18ba4e542a5206a40e308f54ceffc6786b7d94d2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2009.03949\",\"authors\":[{\"authorId\":\"1699605396\",\"name\":\"Zeyu Wang\"},{\"authorId\":\"114200472\",\"name\":\"Berthy Feng\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"}],\"doi\":\"10.1007/978-3-030-58571-6_37\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"81be56a5783552d5b32463b392ff0499dd86a5ab\",\"title\":\"Towards Unique and Informative Captioning of Images\",\"url\":\"https://www.semanticscholar.org/paper/81be56a5783552d5b32463b392ff0499dd86a5ab\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.03160\",\"authors\":[{\"authorId\":\"1557299630\",\"name\":\"Xiaoyu Zeng\"},{\"authorId\":null,\"name\":\"Yanan Wang\"},{\"authorId\":\"3428457\",\"name\":\"Tai-Yin Chiu\"},{\"authorId\":\"23364558\",\"name\":\"Nilavra Bhattacharya\"},{\"authorId\":\"2028946\",\"name\":\"D. Gurari\"}],\"doi\":\"10.1145/3415220\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72f5a58ac11e98a15e96c413178198b6f1b6e736\",\"title\":\"Vision Skills Needed to Answer Visual Questions\",\"url\":\"https://www.semanticscholar.org/paper/72f5a58ac11e98a15e96c413178198b6f1b6e736\",\"venue\":\"Proc. ACM Hum. Comput. Interact.\",\"year\":2020},{\"arxivId\":\"2004.00760\",\"authors\":[{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.14288/1.0392691\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"title\":\"Consistent Multiple Sequence Decoding\",\"url\":\"https://www.semanticscholar.org/paper/5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.07268\",\"authors\":[{\"authorId\":\"10098888\",\"name\":\"R. Bigazzi\"},{\"authorId\":\"67344892\",\"name\":\"Federico Landi\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9ffc4fa1db8372d763f990a1f0a6985260d693b0\",\"title\":\"Explore and Explain: Self-supervised Navigation and Recounting\",\"url\":\"https://www.semanticscholar.org/paper/9ffc4fa1db8372d763f990a1f0a6985260d693b0\",\"venue\":\"ICPR 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"144508458\",\"name\":\"Da Yin\"},{\"authorId\":\"1793529\",\"name\":\"C. Hsieh\"},{\"authorId\":\"101751639\",\"name\":\"Kai-Wei Chang\"}],\"doi\":\"10.18653/v1/2020.acl-main.469\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c\",\"title\":\"What Does BERT with Vision Look At?\",\"url\":\"https://www.semanticscholar.org/paper/cdcdc7ab1f5b6e86146b5c0224cba7d8cd35142c\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2642638\",\"name\":\"F. Chen\"},{\"authorId\":\"145592288\",\"name\":\"Rongrong Ji\"},{\"authorId\":\"9665187\",\"name\":\"Jiayi Ji\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"1740430\",\"name\":\"B. Zhang\"},{\"authorId\":\"1380224383\",\"name\":\"Xuri Ge\"},{\"authorId\":\"153017460\",\"name\":\"Yongjian Wu\"},{\"authorId\":\"1835006\",\"name\":\"Feiyue Huang\"},{\"authorId\":null,\"name\":\"Yan Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2eca0afa23ee2b9b0999c445b8dba44ad3039bd1\",\"title\":\"Variational Structured Semantic Inference for Diverse Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2eca0afa23ee2b9b0999c445b8dba44ad3039bd1\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1802.06960\",\"authors\":[{\"authorId\":\"48754192\",\"name\":\"Pingping Zhang\"},{\"authorId\":\"49681182\",\"name\":\"Luyao Wang\"},{\"authorId\":\"40562844\",\"name\":\"D. Wang\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8b958e1dafb14be6378f3938f150b6f0d2d308e\",\"title\":\"Agile Amulet: Real-Time Salient Object Detection with Contextual Attention\",\"url\":\"https://www.semanticscholar.org/paper/b8b958e1dafb14be6378f3938f150b6f0d2d308e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/ISM.2018.00021\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"95bba8c514f63c666e0a85bdf9c6e3b8113d1f8f\",\"title\":\"Improved Image Description Via Embedded Object Structure Graph and Semantic Feature Matching\",\"url\":\"https://www.semanticscholar.org/paper/95bba8c514f63c666e0a85bdf9c6e3b8113d1f8f\",\"venue\":\"2018 IEEE International Symposium on Multimedia (ISM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhu Zhang\"},{\"authorId\":\"144197770\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"49290494\",\"name\":\"Zhijie Lin\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"49710696\",\"name\":\"Dexia Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d145292fd11ca274693ec9ea6941a9147ae5868\",\"title\":\"4 Self-Attention Interaction Localizer 4 . 1 Problem Formulation\",\"url\":\"https://www.semanticscholar.org/paper/5d145292fd11ca274693ec9ea6941a9147ae5868\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145874777\",\"name\":\"Y. Lang\"},{\"authorId\":\"104002286\",\"name\":\"Qing Wang\"},{\"authorId\":\"46173234\",\"name\":\"Y. Yang\"},{\"authorId\":\"46620459\",\"name\":\"Chunping Hou\"},{\"authorId\":\"46936243\",\"name\":\"Haiping Liu\"},{\"authorId\":\"39624714\",\"name\":\"Yuan He\"}],\"doi\":\"10.1109/JIOT.2019.2929833\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1469a166643909001e2e78454ac380a9420c43e6\",\"title\":\"Joint Motion Classification and Person Identification via Multitask Learning for Smart Homes\",\"url\":\"https://www.semanticscholar.org/paper/1469a166643909001e2e78454ac380a9420c43e6\",\"venue\":\"IEEE Internet of Things Journal\",\"year\":2019},{\"arxivId\":\"1906.00717\",\"authors\":[{\"authorId\":\"2903539\",\"name\":\"Junlong Gao\"},{\"authorId\":\"2802555\",\"name\":\"Xi Meng\"},{\"authorId\":null,\"name\":\"Shiqi Wang\"},{\"authorId\":\"50078954\",\"name\":\"Xia Li\"},{\"authorId\":\"1755176\",\"name\":\"Shanshe Wang\"},{\"authorId\":\"51130683\",\"name\":\"S. Ma\"},{\"authorId\":\"48385803\",\"name\":\"W. Gao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"36a3cfa6ed48988e23cf68b3abcbe097b29e594a\",\"title\":\"Masked Non-Autoregressive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/36a3cfa6ed48988e23cf68b3abcbe097b29e594a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49969428\",\"name\":\"Z. Li\"},{\"authorId\":\"1490937293\",\"name\":\"Feng Ling\"},{\"authorId\":\"104269832\",\"name\":\"Canlong Zhang\"},{\"authorId\":\"46389488\",\"name\":\"H. Ma\"}],\"doi\":\"10.1109/ACCESS.2020.2969808\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b8b61c7ebe5aec472ee19962a4edb2b8b7e971a\",\"title\":\"Combining Global and Local Similarity for Cross-Media Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/3b8b61c7ebe5aec472ee19962a4edb2b8b7e971a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1804472\",\"name\":\"H. Chen\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"},{\"authorId\":\"1818920\",\"name\":\"Zijia Lin\"},{\"authorId\":\"34811036\",\"name\":\"Yuchen Guo\"},{\"authorId\":\"1783847\",\"name\":\"J. Han\"}],\"doi\":\"10.1007/978-3-030-00563-4_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18d99d29834ddd8c70878504998ca9f63f7b3ce1\",\"title\":\"Attend to Knowledge: Memory-Enhanced Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/18d99d29834ddd8c70878504998ca9f63f7b3ce1\",\"venue\":\"BICS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1514929766\",\"name\":\"Chongqing Chen\"},{\"authorId\":\"1840771\",\"name\":\"D. Han\"},{\"authorId\":\"71563119\",\"name\":\"Jun Wang\"}],\"doi\":\"10.1109/ACCESS.2020.2975093\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e75fa8852f5a4779cfdf2f22bd87e213f57b2d20\",\"title\":\"Multimodal Encoder-Decoder Attention Networks for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e75fa8852f5a4779cfdf2f22bd87e213f57b2d20\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50627194\",\"name\":\"Yue Qiu\"},{\"authorId\":\"1732705\",\"name\":\"Y. Satoh\"},{\"authorId\":\"143924106\",\"name\":\"R. Suzuki\"},{\"authorId\":\"35206224\",\"name\":\"K. Iwata\"},{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"}],\"doi\":\"10.3390/s20174761\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4f1217919ef803a0549822039f5328ea78f80ff\",\"title\":\"Indoor Scene Change Captioning Based on Multimodality Data\",\"url\":\"https://www.semanticscholar.org/paper/c4f1217919ef803a0549822039f5328ea78f80ff\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":\"1712.09532\",\"authors\":[{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"2763884\",\"name\":\"G. Henter\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"1700567\",\"name\":\"Shin'ichi Satoh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"96c866f07ff999ee11459519aa361fa4fdfc2139\",\"title\":\"Consensus-based Sequence Training for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/96c866f07ff999ee11459519aa361fa4fdfc2139\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1907.04983\",\"authors\":[{\"authorId\":\"145746402\",\"name\":\"X. Jin\"},{\"authorId\":\"2688093\",\"name\":\"Le Wu\"},{\"authorId\":\"143681906\",\"name\":\"Geng Zhao\"},{\"authorId\":\"47057319\",\"name\":\"X. Li\"},{\"authorId\":\"1391223326\",\"name\":\"Xiaokun Zhang\"},{\"authorId\":\"39646508\",\"name\":\"Shiming Ge\"},{\"authorId\":\"2780914\",\"name\":\"Dongqing Zou\"},{\"authorId\":\"145314008\",\"name\":\"Bin Zhou\"},{\"authorId\":\"51197465\",\"name\":\"Xinghui Zhou\"}],\"doi\":\"10.1145/3343031.3350970\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e79423b5e216151ce0cdf615a7098666d9d9c07e\",\"title\":\"Aesthetic Attributes Assessment of Images\",\"url\":\"https://www.semanticscholar.org/paper/e79423b5e216151ce0cdf615a7098666d9d9c07e\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152924581\",\"name\":\"Jicheng Wang\"},{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"1438588470\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"47957556\",\"name\":\"X. Zhang\"},{\"authorId\":\"31048669\",\"name\":\"M. Wang\"}],\"doi\":\"10.1007/s11042-019-08439-7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0c48de74a40736498d6443f84ecdddc08275359f\",\"title\":\"Sequential image encoding for vision-to-language problems\",\"url\":\"https://www.semanticscholar.org/paper/0c48de74a40736498d6443f84ecdddc08275359f\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"2008.02693\",\"authors\":[{\"authorId\":\"8314407\",\"name\":\"X. Yang\"},{\"authorId\":\"49724488\",\"name\":\"H. Zhang\"},{\"authorId\":\"145657309\",\"name\":\"D. Jin\"},{\"authorId\":\"49421744\",\"name\":\"Yingru Liu\"},{\"authorId\":\"120931191\",\"name\":\"Chi-Hao Wu\"},{\"authorId\":\"34331333\",\"name\":\"Jianchao Tan\"},{\"authorId\":\"47300385\",\"name\":\"Dongliang Xie\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":null,\"name\":\"Xin Wang\"}],\"doi\":\"10.1007/978-3-030-58601-0_1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf72a4a75b6ecd6dbc4c1af52a9592ec61abcfb5\",\"title\":\"Fashion Captioning: Towards Generating Accurate Descriptions with Semantic Rewards\",\"url\":\"https://www.semanticscholar.org/paper/bf72a4a75b6ecd6dbc4c1af52a9592ec61abcfb5\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2012.04638\",\"authors\":[{\"authorId\":\"21518096\",\"name\":\"Zhengyuan Yang\"},{\"authorId\":\"38534822\",\"name\":\"Y. Lu\"},{\"authorId\":\"46583994\",\"name\":\"Jianfeng Wang\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"1882479\",\"name\":\"D. Flor\\u00eancio\"},{\"authorId\":\"30602591\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"1706673\",\"name\":\"C. Zhang\"},{\"authorId\":\"145637095\",\"name\":\"Lei Zhang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8deceb13cb3afcfbaab06a2c655f1935445635fe\",\"title\":\"TAP: Text-Aware Pre-training for Text-VQA and Text-Caption\",\"url\":\"https://www.semanticscholar.org/paper/8deceb13cb3afcfbaab06a2c655f1935445635fe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"80526284\",\"name\":\"Yanyuan Qiao\"},{\"authorId\":\"48567083\",\"name\":\"Zheng Yu\"},{\"authorId\":\"3116943\",\"name\":\"Jiange Liu\"}],\"doi\":\"10.1109/ICIP40778.2020.9190828\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"316f057e36cf432ece4ba4e2d167a84aef700aee\",\"title\":\"VC-VQA: Visual Calibration Mechanism For Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/316f057e36cf432ece4ba4e2d167a84aef700aee\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":\"1910.06737\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ac87d8ef6da4be7d7822053355c0528c58d8ddf5\",\"title\":\"Integrating Temporal and Spatial Attentions for VATEX Video Captioning Challenge 2019\",\"url\":\"https://www.semanticscholar.org/paper/ac87d8ef6da4be7d7822053355c0528c58d8ddf5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2009.05684\",\"authors\":[{\"authorId\":\"46662193\",\"name\":\"V. Mittal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd6df4febb7135007077f835f9c44d12514aa522\",\"title\":\"AttnGrounder: Talking to Cars with Attention\",\"url\":\"https://www.semanticscholar.org/paper/dd6df4febb7135007077f835f9c44d12514aa522\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.11550\",\"authors\":[{\"authorId\":\"1453661830\",\"name\":\"Keyu Wen\"},{\"authorId\":\"1649999106\",\"name\":\"Xiaodong Gu\"},{\"authorId\":\"48561436\",\"name\":\"Q. Cheng\"}],\"doi\":\"10.1109/TCSVT.2020.3030656\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2740a2308d9f9b867cd54cdf04da82c82c417481\",\"title\":\"Learning Dual Semantic Relations with Graph Attention for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/2740a2308d9f9b867cd54cdf04da82c82c417481\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.04446\",\"authors\":[{\"authorId\":\"2018337565\",\"name\":\"Jia Guo\"},{\"authorId\":\"1431754650\",\"name\":\"C. Zhu\"},{\"authorId\":\"46316984\",\"name\":\"Yilun Zhao\"},{\"authorId\":\"3405101\",\"name\":\"He-Da Wang\"},{\"authorId\":\"72285615\",\"name\":\"Yao Hu\"},{\"authorId\":\"101131295\",\"name\":\"Xiaofei He\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6695d3b92e7cd7f2359f698a09c7b3dc37996329\",\"title\":\"LAMP: Label Augmented Multimodal Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/6695d3b92e7cd7f2359f698a09c7b3dc37996329\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150074679\",\"name\":\"Denis Dushi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d19ab1db4d8c60643bc2ce8334505acaadee84ff\",\"title\":\"Using Deep Learning to Answer Visual Questions from Blind People\",\"url\":\"https://www.semanticscholar.org/paper/d19ab1db4d8c60643bc2ce8334505acaadee84ff\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.07119\",\"authors\":[{\"authorId\":\"1500647323\",\"name\":\"Giang Dao\"},{\"authorId\":\"65756109\",\"name\":\"Minwoo Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f11a33dce7aea8bad599560ab20b88501eb206a\",\"title\":\"Demysifying Deep Neural Networks Through Interpretation: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/2f11a33dce7aea8bad599560ab20b88501eb206a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"134814700\",\"name\":\"T. Wang\"},{\"authorId\":\"46285717\",\"name\":\"Y. Yang\"},{\"authorId\":\"46911598\",\"name\":\"Lin Zuo\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TNNLS.2020.2967597\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a87295dbea1256b99e4539f18c5fb73cd8b317aa\",\"title\":\"Cross-Modal Attention With Semantic Consistence for Image\\u2013Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/a87295dbea1256b99e4539f18c5fb73cd8b317aa\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"11882893\",\"name\":\"A. Yang\"},{\"authorId\":\"29343468\",\"name\":\"Yichang Zhang\"},{\"authorId\":\"49723003\",\"name\":\"J. Liu\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee4918cc9b1dc28007454490fbe8366ec017b33d\",\"title\":\"InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/ee4918cc9b1dc28007454490fbe8366ec017b33d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.07698\",\"authors\":[{\"authorId\":\"73441526\",\"name\":\"R. Tan\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.163\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"444e2b250e0709be009736e8744287d69c854fb6\",\"title\":\"Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News\",\"url\":\"https://www.semanticscholar.org/paper/444e2b250e0709be009736e8744287d69c854fb6\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47540106\",\"name\":\"J. Zhang\"},{\"authorId\":\"50828249\",\"name\":\"Q. Wang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1016/j.ipm.2019.102152\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ff7b42d8cc37acfc08210cff20983090a968308\",\"title\":\"Multi-Modal fusion with multi-level attention for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/2ff7b42d8cc37acfc08210cff20983090a968308\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"2006.03184\",\"authors\":[{\"authorId\":\"2455191\",\"name\":\"Omid Mohamad Nezami\"},{\"authorId\":\"22205368\",\"name\":\"Akshay Chaturvedi\"},{\"authorId\":\"143899054\",\"name\":\"Mark Dras\"},{\"authorId\":\"48421321\",\"name\":\"U. Garain\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cf2f8c8686cff2643270ed2d61f12d4661cbb75a\",\"title\":\"Pick-Object-Attack: Type-Specific Adversarial Attack for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/cf2f8c8686cff2643270ed2d61f12d4661cbb75a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.04726\",\"authors\":[{\"authorId\":\"1380616323\",\"name\":\"Jeff Da\"},{\"authorId\":\"39191185\",\"name\":\"M. Forbes\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"90390316\",\"name\":\"Anthony Zheng\"},{\"authorId\":\"2012510\",\"name\":\"Jena D. Hwang\"},{\"authorId\":\"2691021\",\"name\":\"Antoine Bosselut\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c5d597e5f34a01809f1bf4fd6e0f3475f59fb4d\",\"title\":\"Edited Media Understanding: Reasoning About Implications of Manipulated Images\",\"url\":\"https://www.semanticscholar.org/paper/7c5d597e5f34a01809f1bf4fd6e0f3475f59fb4d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46702778\",\"name\":\"H. Zhang\"},{\"authorId\":\"2913523\",\"name\":\"Diedie Qiu\"},{\"authorId\":\"50477983\",\"name\":\"R. Wu\"},{\"authorId\":\"103624776\",\"name\":\"Dong-Hong Ji\"},{\"authorId\":\"49461429\",\"name\":\"Guangli Li\"},{\"authorId\":\"9201022\",\"name\":\"Zhenyu Niu\"},{\"authorId\":\"50289773\",\"name\":\"Tao Li\"}],\"doi\":\"10.1007/S00500-019-03973-W\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f7104056642c03263508957f20505a1dbba03ce\",\"title\":\"Novel model to integrate word embeddings and syntactic trees for automatic caption generation from images\",\"url\":\"https://www.semanticscholar.org/paper/7f7104056642c03263508957f20505a1dbba03ce\",\"venue\":\"Soft Comput.\",\"year\":2020},{\"arxivId\":\"1709.03376\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"},{\"authorId\":\"40894914\",\"name\":\"T. Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f14e73dade94b8b1f276dcd91257aa7de5f19d7\",\"title\":\"Stack-Captioning: Coarse-to-Fine Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7f14e73dade94b8b1f276dcd91257aa7de5f19d7\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1805.07932\",\"authors\":[{\"authorId\":\"2684967\",\"name\":\"J. Kim\"},{\"authorId\":\"29818400\",\"name\":\"Jaehyun Jun\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a5d10341717c0519cf63151b496a6d2ed67aa05f\",\"title\":\"Bilinear Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/a5d10341717c0519cf63151b496a6d2ed67aa05f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.09383\",\"authors\":[{\"authorId\":\"144652817\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"48607331\",\"name\":\"Yu Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"143699996\",\"name\":\"Y. Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f8762553f4a8674249e60eb1cac9289ef0547f4\",\"title\":\"Baidu-UTS Submission to the EPIC-Kitchens Action Recognition Challenge 2019\",\"url\":\"https://www.semanticscholar.org/paper/0f8762553f4a8674249e60eb1cac9289ef0547f4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1812.02378\",\"authors\":[{\"authorId\":\"47008946\",\"name\":\"X. Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"}],\"doi\":\"10.1109/CVPR.2019.01094\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f6feb1af1809dfd872d868dfcc13021cc42f496c\",\"title\":\"Auto-Encoding Scene Graphs for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f6feb1af1809dfd872d868dfcc13021cc42f496c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1604960419\",\"name\":\"Y. Liu\"},{\"authorId\":\"145684665\",\"name\":\"M. Shahid\"},{\"authorId\":\"2037014054\",\"name\":\"Wannaporn Sarapugdi\"},{\"authorId\":\"151476997\",\"name\":\"Yong-Xiang Lin\"},{\"authorId\":\"50762717\",\"name\":\"Jyh-cheng Chen\"},{\"authorId\":\"145525478\",\"name\":\"K. Hua\"}],\"doi\":\"10.1007/s11042-020-10078-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e8bd20c17455dbb171f01a6b48f9b587d4e9135c\",\"title\":\"Cascaded atrous dual attention U-Net for tumor segmentation\",\"url\":\"https://www.semanticscholar.org/paper/e8bd20c17455dbb171f01a6b48f9b587d4e9135c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2932516\",\"name\":\"J. Zhang\"},{\"authorId\":\"1853471\",\"name\":\"K. Mei\"},{\"authorId\":\"1626610869\",\"name\":\"Yu Zheng\"},{\"authorId\":\"1692580\",\"name\":\"Jianping Fan\"}],\"doi\":\"10.1109/TMM.2020.2976552\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"27d838ae5a89cd54ef1cb6599484c794bd544ad9\",\"title\":\"Integrating Part of Speech Guidance for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/27d838ae5a89cd54ef1cb6599484c794bd544ad9\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2021},{\"arxivId\":\"2003.09790\",\"authors\":[{\"authorId\":\"47039493\",\"name\":\"Zhonghua Wu\"},{\"authorId\":\"3392051\",\"name\":\"Qingyi Tao\"},{\"authorId\":\"2604251\",\"name\":\"Guosheng Lin\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":\"10.1109/cvpr42600.2020.01295\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"af77956223f213b04e9988c23851d1c4216060f3\",\"title\":\"Exploring Bottom-Up and Top-Down Cues With Attentive Learning for Webly Supervised Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/af77956223f213b04e9988c23851d1c4216060f3\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144524309\",\"name\":\"S. S. Behera\"},{\"authorId\":\"5141773\",\"name\":\"S. Mishra\"},{\"authorId\":\"1709001\",\"name\":\"B. Mandal\"},{\"authorId\":\"2156276\",\"name\":\"N. B. Puhan\"}],\"doi\":\"10.1016/j.imavis.2020.104016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd3bfce9b0e7082c97903f7db417ed43d9d576f9\",\"title\":\"Variance-guided attention-based twin deep network for cross-spectral periocular recognition\",\"url\":\"https://www.semanticscholar.org/paper/fd3bfce9b0e7082c97903f7db417ed43d9d576f9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.05552\",\"authors\":[{\"authorId\":\"83605661\",\"name\":\"T. Gao\"},{\"authorId\":\"144171288\",\"name\":\"Q. Huang\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ecc8f61418f6afdbb600d9f6fbb286143e56026\",\"title\":\"Systematic Generalization on gSCAN with Language Conditioned Embedding\",\"url\":\"https://www.semanticscholar.org/paper/3ecc8f61418f6afdbb600d9f6fbb286143e56026\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2005.00619\",\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2548384\",\"name\":\"Hannaneh Hajishirzi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf45a40d129e02079ba482d3b1bc742a1f6ef36b\",\"title\":\"Probing Contextual Language Models for Common Ground with Visual Representations\",\"url\":\"https://www.semanticscholar.org/paper/cf45a40d129e02079ba482d3b1bc742a1f6ef36b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1923372\",\"name\":\"L. D. Paolis\"},{\"authorId\":\"1782179\",\"name\":\"P. Bourdot\"}],\"doi\":\"10.1007/978-3-030-58465-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8177b45156f948c19c0686610ad26d0e50a6426f\",\"title\":\"Augmented Reality, Virtual Reality, and Computer Graphics: 7th International Conference, AVR 2020, Lecce, Italy, September 7\\u201310, 2020, Proceedings, Part I\",\"url\":\"https://www.semanticscholar.org/paper/8177b45156f948c19c0686610ad26d0e50a6426f\",\"venue\":\"AVR\",\"year\":2020},{\"arxivId\":\"1908.00169\",\"authors\":[{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"145527564\",\"name\":\"Z. Zhang\"},{\"authorId\":\"31115284\",\"name\":\"Jingjing Li\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"}],\"doi\":\"10.1145/3343031.3350961\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250\",\"title\":\"Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1582890834\",\"name\":\"Zekun Yang\"},{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"2427516\",\"name\":\"Chenhui Chu\"},{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"1748743\",\"name\":\"H. Takemura\"}],\"doi\":\"10.1109/WACV45572.2020.9093596\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"39b2d8b8233a53dc7eadb819c52213369dff8648\",\"title\":\"BERT Representations for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/39b2d8b8233a53dc7eadb819c52213369dff8648\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1711.06354\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"title\":\"Grounded Objects and Interactions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1912.11872\",\"authors\":[{\"authorId\":\"153040576\",\"name\":\"T. Mei\"},{\"authorId\":\"101586660\",\"name\":\"W. Zhang\"},{\"authorId\":\"48577275\",\"name\":\"Ting Yao\"}],\"doi\":\"10.1017/ATSIP.2020.10\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"title\":\"Vision and Language: from Visual Perception to Content Creation\",\"url\":\"https://www.semanticscholar.org/paper/3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50317425\",\"name\":\"Abhishek Das\"},{\"authorId\":\"19200118\",\"name\":\"Samyak Datta\"},{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPR.2018.00008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6868c233c2d0fe01ecf0eda01099f6c7a0f98fb9\",\"title\":\"Embodied Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/6868c233c2d0fe01ecf0eda01099f6c7a0f98fb9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1909.05746\",\"authors\":[{\"authorId\":\"116614713\",\"name\":\"Tingle Li\"},{\"authorId\":\"47739592\",\"name\":\"Jia-Wei Chen\"},{\"authorId\":\"9397636\",\"name\":\"H. Hou\"},{\"authorId\":\"49595665\",\"name\":\"Ming Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e840902d88a19180a82dfb2753f79ac245fb32c\",\"title\":\"Sams-Net: A Sliced Attention-based Neural Network for Music Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/9e840902d88a19180a82dfb2753f79ac245fb32c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1810.06245\",\"authors\":[{\"authorId\":\"35935570\",\"name\":\"Jean-Benoit Delbrouck\"},{\"authorId\":\"144436412\",\"name\":\"S. Dupont\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"58d16e23e1192be4acaf6a29c1f5995817146554\",\"title\":\"Bringing back simplicity and lightliness into neural image captioning\",\"url\":\"https://www.semanticscholar.org/paper/58d16e23e1192be4acaf6a29c1f5995817146554\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50627194\",\"name\":\"Yue Qiu\"},{\"authorId\":\"1732705\",\"name\":\"Y. Satoh\"},{\"authorId\":\"143924106\",\"name\":\"R. Suzuki\"},{\"authorId\":\"35206224\",\"name\":\"K. Iwata\"},{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"}],\"doi\":\"10.3390/s20082281\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"779a14591ce143820a3d8c7661e04454329e3abc\",\"title\":\"Multi-View Visual Question Answering with Active Viewpoint Selection\",\"url\":\"https://www.semanticscholar.org/paper/779a14591ce143820a3d8c7661e04454329e3abc\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":\"1910.03343\",\"authors\":[{\"authorId\":\"35935570\",\"name\":\"Jean-Benoit Delbrouck\"},{\"authorId\":\"1388031809\",\"name\":\"Antoine Maiorca\"},{\"authorId\":\"1388031811\",\"name\":\"Nathan Hubens\"},{\"authorId\":\"153352427\",\"name\":\"S. Dupont\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fd8e7ed46ca883e7e6f897f25715d50220eaca30\",\"title\":\"Modulated Self-attention Convolutional Network for VQA\",\"url\":\"https://www.semanticscholar.org/paper/fd8e7ed46ca883e7e6f897f25715d50220eaca30\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1954481\",\"name\":\"D. Huynh\"},{\"authorId\":\"47126776\",\"name\":\"E. Elhamifar\"}],\"doi\":\"10.1109/cvpr42600.2020.00454\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed0e117152196693a42fd845740ca08ae1d5ef8a\",\"title\":\"Fine-Grained Generalized Zero-Shot Learning via Dense Attribute-Based Attention\",\"url\":\"https://www.semanticscholar.org/paper/ed0e117152196693a42fd845740ca08ae1d5ef8a\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"},{\"authorId\":\"88265392\",\"name\":\"P. Liu\"},{\"authorId\":\"3343198\",\"name\":\"Yingjie Zhou\"},{\"authorId\":\"144953181\",\"name\":\"D. Wu\"}],\"doi\":\"10.1109/BIGCOM.2019.00013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a203fb561497774689ff4bb80cb18c9e00bf47b6\",\"title\":\"Semantic Tensor Product for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a203fb561497774689ff4bb80cb18c9e00bf47b6\",\"venue\":\"2019 5th International Conference on Big Data Computing and Communications (BIGCOM)\",\"year\":2019},{\"arxivId\":\"1806.06004\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ceabd7ff28ce2d501511da998252aeb938adc98b\",\"title\":\"Partially-Supervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ceabd7ff28ce2d501511da998252aeb938adc98b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3490541\",\"name\":\"Aaron Chan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b775774edc1c2edcc9782f37eb2682cd01cfd94a\",\"title\":\"A Survey of Image Captioning Methods\",\"url\":\"https://www.semanticscholar.org/paper/b775774edc1c2edcc9782f37eb2682cd01cfd94a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1810.06859\",\"authors\":[{\"authorId\":\"47666554\",\"name\":\"H. Chen\"},{\"authorId\":\"48355461\",\"name\":\"Yifei Huang\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":\"10.1007/978-3-030-20870-7_27\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5122fe2ece8157ff53870a59c6c842f21d6a8a34\",\"title\":\"Semantic Aware Attention Based Deep Object Co-segmentation\",\"url\":\"https://www.semanticscholar.org/paper/5122fe2ece8157ff53870a59c6c842f21d6a8a34\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2684967\",\"name\":\"J. Kim\"},{\"authorId\":\"49146706\",\"name\":\"Y. Choi\"},{\"authorId\":\"2487892\",\"name\":\"Sungeun Hong\"},{\"authorId\":\"29818400\",\"name\":\"Jaehyun Jun\"},{\"authorId\":\"1692756\",\"name\":\"B. Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d111dc672d48823c22594d28d9742c0b707959c6\",\"title\":\"Bilinear attention networks for VizWiz challenge\",\"url\":\"https://www.semanticscholar.org/paper/d111dc672d48823c22594d28d9742c0b707959c6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89373414\",\"name\":\"Omer Arshad\"},{\"authorId\":\"145116184\",\"name\":\"I. Gallo\"},{\"authorId\":\"144669071\",\"name\":\"Shah Nawaz\"},{\"authorId\":\"3457883\",\"name\":\"Alessandro Calefati\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1b49ea26f61527f897cf7ea2d5cf1a32de6cfc83\",\"title\":\"EasyChair Preprint No 1375 Aiding Intra-Text Representations with Visual Context for Multimodal Named Entity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1b49ea26f61527f897cf7ea2d5cf1a32de6cfc83\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144213485\",\"name\":\"Jie Wang\"},{\"authorId\":\"2005499\",\"name\":\"Hairong Lv\"},{\"authorId\":\"47125990\",\"name\":\"R. Jiang\"},{\"authorId\":\"144050010\",\"name\":\"Z. Xie\"}],\"doi\":\"10.1109/CBMS.2019.00016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cfbb3a4c294409eba1432bb3e072252cc66be80e\",\"title\":\"Rule-Based Method to Develop Question-Answer Dataset from Chest X-Ray Reports\",\"url\":\"https://www.semanticscholar.org/paper/cfbb3a4c294409eba1432bb3e072252cc66be80e\",\"venue\":\"2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144382769\",\"name\":\"Viviana Beltr\\u00e1n\"},{\"authorId\":\"1732746\",\"name\":\"Micka\\u00ebl Coustaty\"},{\"authorId\":\"1748667\",\"name\":\"N. Journet\"},{\"authorId\":\"145507543\",\"name\":\"Juan C. Caicedo\"},{\"authorId\":\"34796546\",\"name\":\"A. Doucet\"}],\"doi\":\"10.1007/978-3-030-59830-3_49\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a210b0d83ab985228c47013456dfdbbe49831980\",\"title\":\"An Extended Evaluation of the Impact of Different Modules in ST-VQA Systems\",\"url\":\"https://www.semanticscholar.org/paper/a210b0d83ab985228c47013456dfdbbe49831980\",\"venue\":\"ICPRAI\",\"year\":2020},{\"arxivId\":\"1711.06666\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-01267-0_51\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e1be2c16060974f66e5366872ebbee21325075e8\",\"title\":\"ADVISE: Symbolism and External Knowledge for Decoding Advertisements\",\"url\":\"https://www.semanticscholar.org/paper/e1be2c16060974f66e5366872ebbee21325075e8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1807.03756\",\"authors\":[{\"authorId\":\"2505751\",\"name\":\"Y. Deng\"},{\"authorId\":\"38367242\",\"name\":\"Yoon Kim\"},{\"authorId\":\"30819111\",\"name\":\"Justin T Chiu\"},{\"authorId\":\"35578711\",\"name\":\"Demi Guo\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b84405fb6e75d41ae35337b86916ca059201824\",\"title\":\"Latent Alignment and Variational Attention\",\"url\":\"https://www.semanticscholar.org/paper/8b84405fb6e75d41ae35337b86916ca059201824\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2009.08792\",\"authors\":[{\"authorId\":\"1388019217\",\"name\":\"Thierry Deruyttere\"},{\"authorId\":\"83754395\",\"name\":\"Simon Vandenhende\"},{\"authorId\":\"1388019229\",\"name\":\"Dusan Grujicic\"},{\"authorId\":\"30752055\",\"name\":\"Y. Liu\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"},{\"authorId\":\"1758219\",\"name\":\"Matthew B. Blaschko\"},{\"authorId\":\"1704728\",\"name\":\"T. Tuytelaars\"},{\"authorId\":\"100781843\",\"name\":\"Marie-Francine Moens\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a43ed50d4ae027cfd29ab3fdbb731f8439c49c5\",\"title\":\"Commands 4 Autonomous Vehicles (C4AV) Workshop Summary\",\"url\":\"https://www.semanticscholar.org/paper/9a43ed50d4ae027cfd29ab3fdbb731f8439c49c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.04357\",\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"49469577\",\"name\":\"X. Zhang\"},{\"authorId\":\"50202300\",\"name\":\"Shu Zhang\"},{\"authorId\":\"46314996\",\"name\":\"Wensheng Wang\"},{\"authorId\":null,\"name\":\"Chi Zhang\"},{\"authorId\":\"46675463\",\"name\":\"Heng Huang\"}],\"doi\":\"10.1109/CVPR.2019.00210\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5c5d99eff1377e141be293336a14ffddb323c364\",\"title\":\"Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/5c5d99eff1377e141be293336a14ffddb323c364\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sunny Katiyar\"},{\"authorId\":\"88294723\",\"name\":\"M. Wakode\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a0881b665590ac2aba58a5b3b7db93c1c4f6af15\",\"title\":\"A Survey On Visual Questioning Answering : Datasets, Approaches And Models\",\"url\":\"https://www.semanticscholar.org/paper/a0881b665590ac2aba58a5b3b7db93c1c4f6af15\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.02517\",\"authors\":[{\"authorId\":\"32583496\",\"name\":\"Y. Fu\"},{\"authorId\":\"46999477\",\"name\":\"Tingting Liu\"},{\"authorId\":\"31933517\",\"name\":\"M. Gao\"},{\"authorId\":\"145031578\",\"name\":\"Aoying Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2395f4b80e54d4b93bf425e207aa6d2236dd5f0b\",\"title\":\"EDSL: An Encoder-Decoder Architecture with Symbol-Level Features for Printed Mathematical Expression Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2395f4b80e54d4b93bf425e207aa6d2236dd5f0b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1783874\",\"name\":\"T. Wang\"},{\"authorId\":\"145030935\",\"name\":\"H. Hu\"},{\"authorId\":\"144751998\",\"name\":\"C. He\"}],\"doi\":\"10.1007/s11063-019-09979-7\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"a54a18073b4b4a788e106d540d26817c8c898a63\",\"title\":\"Image Caption with Endogenous\\u2013Exogenous Attention\",\"url\":\"https://www.semanticscholar.org/paper/a54a18073b4b4a788e106d540d26817c8c898a63\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":\"1906.02497\",\"authors\":[{\"authorId\":\"1764508\",\"name\":\"Z. Zhang\"},{\"authorId\":\"34064720\",\"name\":\"Zhijie Lin\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"123034558\",\"name\":\"Z. Xiao\"}],\"doi\":\"10.1145/3331184.3331235\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fb53803897d3df3e1f43a43a753ee88a64517c47\",\"title\":\"Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos\",\"url\":\"https://www.semanticscholar.org/paper/fb53803897d3df3e1f43a43a753ee88a64517c47\",\"venue\":\"SIGIR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Huang\"},{\"authorId\":\"51231229\",\"name\":\"Fengqi Yan\"},{\"authorId\":\"40515617\",\"name\":\"W. Xu\"},{\"authorId\":\"1716059\",\"name\":\"M. Li\"}],\"doi\":\"10.1109/ACCESS.2019.2947134\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"12e9fb63e5b46b241fed3ab2e9d95219db80e056\",\"title\":\"Multi-Attention and Incorporating Background Information Model for Chest X-Ray Image Report Generation\",\"url\":\"https://www.semanticscholar.org/paper/12e9fb63e5b46b241fed3ab2e9d95219db80e056\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1910.11102\",\"authors\":[{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"1923156\",\"name\":\"Peng Yao\"},{\"authorId\":\"1749850\",\"name\":\"Jing Liu\"},{\"authorId\":\"116829059\",\"name\":\"Shichen Lu\"},{\"authorId\":\"2125223\",\"name\":\"Zheng Gen Yu\"},{\"authorId\":\"46641690\",\"name\":\"Wei Liu\"},{\"authorId\":\"46386029\",\"name\":\"Hanqing Lu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1ae6fc431791e0daa8bfb3b195acdba4535cf07c\",\"title\":\"Multi-View Features and Hybrid Reward Strategies for Vatex Video Captioning Challenge 2019\",\"url\":\"https://www.semanticscholar.org/paper/1ae6fc431791e0daa8bfb3b195acdba4535cf07c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1810.12535\",\"authors\":[{\"authorId\":\"50621243\",\"name\":\"Qingzhong Wang\"},{\"authorId\":\"3651407\",\"name\":\"Antoni B. Chan\"}],\"doi\":\"10.1007/978-3-030-20870-7_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74eda5e2a4a34b9d4a737da755b136455c947339\",\"title\":\"Gated Hierarchical Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/74eda5e2a4a34b9d4a737da755b136455c947339\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":\"2009.13682\",\"authors\":[{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"51188307\",\"name\":\"Kevin Lin\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"f147279c9d1edddda57f1f21f23b3b58998bad74\",\"title\":\"VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training\",\"url\":\"https://www.semanticscholar.org/paper/f147279c9d1edddda57f1f21f23b3b58998bad74\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112858943\",\"name\":\"Z. Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"title\":\"Representation Learning for Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.05880\",\"authors\":[{\"authorId\":\"38211837\",\"name\":\"Idan Schwartz\"},{\"authorId\":\"1885974\",\"name\":\"Seunghak Yu\"},{\"authorId\":\"1918412\",\"name\":\"T. Hazan\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":\"10.1109/CVPR.2019.00214\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ecce0a7b0f9b2bfbc6ca2c99805bddd53178ac35\",\"title\":\"Factor Graph Attention\",\"url\":\"https://www.semanticscholar.org/paper/ecce0a7b0f9b2bfbc6ca2c99805bddd53178ac35\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s11042-019-08021-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ba03c16bf25d33efc0d977ab51392e1d9b5a0fb\",\"title\":\"Survey of deep learning and architectures for visual captioning\\u2014transitioning between media and natural languages\",\"url\":\"https://www.semanticscholar.org/paper/1ba03c16bf25d33efc0d977ab51392e1d9b5a0fb\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1912.02315\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"}],\"doi\":\"10.1109/cvpr42600.2020.01045\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b5f3fe42548216cd93816b1bf5c437cf47bc5fbf\",\"title\":\"12-in-1: Multi-Task Vision and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/b5f3fe42548216cd93816b1bf5c437cf47bc5fbf\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735739\",\"name\":\"Weifeng Zhang\"},{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"47864783\",\"name\":\"H. Hu\"},{\"authorId\":\"144645443\",\"name\":\"H. Hu\"},{\"authorId\":\"70565757\",\"name\":\"Zengchang Qin\"}],\"doi\":\"10.1016/J.INFFUS.2019.08.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04afa0417dc2a4555c15243a69e6a54ce44ecf63\",\"title\":\"Multimodal feature fusion by relational reasoning and attention for visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/04afa0417dc2a4555c15243a69e6a54ce44ecf63\",\"venue\":\"Inf. Fusion\",\"year\":2020},{\"arxivId\":\"2012.06946\",\"authors\":[{\"authorId\":\"46583603\",\"name\":\"J. Wang\"},{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"41171e9024d0082c2a57f4887bac93131669b881\",\"title\":\"MiniVLM: A Smaller and Faster Vision-Language Model\",\"url\":\"https://www.semanticscholar.org/paper/41171e9024d0082c2a57f4887bac93131669b881\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49356747\",\"name\":\"D. Zhang\"},{\"authorId\":\"145399329\",\"name\":\"J. Shao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3398685\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c118cab81c39b534dde58145c7cb0ea2d833de27\",\"title\":\"Kernel Attention Network for Single Image Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/c118cab81c39b534dde58145c7cb0ea2d833de27\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":\"2010.12917\",\"authors\":[{\"authorId\":\"3442255\",\"name\":\"Zan-Xia Jin\"},{\"authorId\":\"2003456770\",\"name\":\"Heran Wu\"},{\"authorId\":\"46962194\",\"name\":\"C. Yang\"},{\"authorId\":\"31679477\",\"name\":\"Fang Zhou\"},{\"authorId\":\"2005306\",\"name\":\"Jingyan Qin\"},{\"authorId\":\"145628818\",\"name\":\"Lei Xiao\"},{\"authorId\":\"120644708\",\"name\":\"XuCheng Yin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d209f0d94492447713541b28373d9b6cad5edeb9\",\"title\":\"RUArt: A Novel Text-Centered Solution for Text-Based Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/d209f0d94492447713541b28373d9b6cad5edeb9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.00808\",\"authors\":[{\"authorId\":\"32824146\",\"name\":\"Ammarah Farooq\"},{\"authorId\":\"144987296\",\"name\":\"M. Awais\"},{\"authorId\":\"144535339\",\"name\":\"Fei Yan\"},{\"authorId\":\"47091894\",\"name\":\"J. Kittler\"},{\"authorId\":\"144595173\",\"name\":\"A. Akbari\"},{\"authorId\":\"2610118\",\"name\":\"Syed Safwan Khalid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73d3b9d649923f87b18882232f68682a941b213a\",\"title\":\"A Convolutional Baseline for Person Re-Identification Using Vision and Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/73d3b9d649923f87b18882232f68682a941b213a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47196880\",\"name\":\"Ziwei Wang\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"}],\"doi\":\"10.1007/978-3-030-39469-1_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7fb87759fff098cbb487d74404ce8ca1098253a1\",\"title\":\"PAIC: Parallelised Attentive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7fb87759fff098cbb487d74404ce8ca1098253a1\",\"venue\":\"ADC\",\"year\":2020},{\"arxivId\":\"2009.14352\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"1410092701\",\"name\":\"Xu Yang\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":\"10.1007/978-3-030-58568-6_34\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b41882903384ef849688a325d747fdaad8ecee82\",\"title\":\"Finding It at Another Side: A Viewpoint-Adapted Matching Encoder for Change Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b41882903384ef849688a325d747fdaad8ecee82\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"92326466\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/cvpr42600.2020.01059\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"title\":\"Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40228165\",\"name\":\"A. Hani\"},{\"authorId\":\"2821832\",\"name\":\"Najiba Tagougui\"},{\"authorId\":\"2139481\",\"name\":\"M. Kherallah\"}],\"doi\":\"10.1109/ACIT47987.2019.8990998\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"82ecd67c8425c9756c873df42a6dadba2db04945\",\"title\":\"Image Caption Generation Using A Deep Architecture\",\"url\":\"https://www.semanticscholar.org/paper/82ecd67c8425c9756c873df42a6dadba2db04945\",\"venue\":\"2019 International Arab Conference on Information Technology (ACIT)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50627194\",\"name\":\"Yue Qiu\"},{\"authorId\":\"1732705\",\"name\":\"Y. Satoh\"},{\"authorId\":\"143924106\",\"name\":\"R. Suzuki\"},{\"authorId\":\"35206224\",\"name\":\"K. Iwata\"},{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"}],\"doi\":\"10.1109/LRA.2020.3003290\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"583d745ec6e1af705f695d899482996e4bc9d1b4\",\"title\":\"3D-Aware Scene Change Captioning From Multiview Images\",\"url\":\"https://www.semanticscholar.org/paper/583d745ec6e1af705f695d899482996e4bc9d1b4\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":\"2004.08070\",\"authors\":[{\"authorId\":\"51163002\",\"name\":\"Alasdair Tran\"},{\"authorId\":\"46953477\",\"name\":\"A. Mathews\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"}],\"doi\":\"10.1109/CVPR42600.2020.01305\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c9f19acb4a9d56085a6d2f8f1acef7514777345\",\"title\":\"Transform and Tell: Entity-Aware News Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c9f19acb4a9d56085a6d2f8f1acef7514777345\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"39698901\",\"name\":\"H. Wang\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"48957872\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/TIP.2020.2992888\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de847c41c0b6c52e5fc770f364a19fed04a7b3ae\",\"title\":\"Textual-Visual Reference-Aware Attention Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/de847c41c0b6c52e5fc770f364a19fed04a7b3ae\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48520780\",\"name\":\"X. Yang\"},{\"authorId\":\"10817432\",\"name\":\"Kaihua Tang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"38bbace118817cd18677f169a8c8e6c8b005df18\",\"title\":\"Auto-Encoding Graphical Inductive Bias for Descriptive Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/38bbace118817cd18677f169a8c8e6c8b005df18\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1901.06595\",\"authors\":[{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":\"10.1109/ICCVW.2019.00237\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5a97e9a253a60814f76dab86e8a4ec65fc86e261\",\"title\":\"Evaluating Text-to-Image Matching using Binary Image Selection (BISON)\",\"url\":\"https://www.semanticscholar.org/paper/5a97e9a253a60814f76dab86e8a4ec65fc86e261\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1910.14671\",\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1743587c272c36dbda0adc50496bf7f34b8148f1\",\"title\":\"TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning Baselines\",\"url\":\"https://www.semanticscholar.org/paper/1743587c272c36dbda0adc50496bf7f34b8148f1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1909.07583\",\"authors\":[{\"authorId\":\"1388780256\",\"name\":\"Yaser Alwatter\"},{\"authorId\":\"1798719\",\"name\":\"Yuhong Guo\"}],\"doi\":\"10.22215/etd/2019-13929\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"48795928eb87d1e8a038063b3aebee180e424d04\",\"title\":\"Inverse Visual Question Answering with Multi-Level Attentions\",\"url\":\"https://www.semanticscholar.org/paper/48795928eb87d1e8a038063b3aebee180e424d04\",\"venue\":\"ACML\",\"year\":2020},{\"arxivId\":\"2007.01499\",\"authors\":[{\"authorId\":\"90475778\",\"name\":\"Qing Li\"},{\"authorId\":\"51442394\",\"name\":\"Siyuan Huang\"},{\"authorId\":\"151261268\",\"name\":\"Yining Hong\"},{\"authorId\":\"12554898\",\"name\":\"S. Zhu\"}],\"doi\":\"10.1007/978-3-030-58536-5_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2d6fee1cc06354fdb3811aaa06910f4e34cd4a7\",\"title\":\"A Competence-aware Curriculum for Visual Concepts Learning via Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/c2d6fee1cc06354fdb3811aaa06910f4e34cd4a7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144729180\",\"name\":\"R. Su\"},{\"authorId\":\"39497371\",\"name\":\"J. Liu\"},{\"authorId\":\"2053638\",\"name\":\"D. Zhang\"},{\"authorId\":\"8357466\",\"name\":\"Chuandong Cheng\"},{\"authorId\":\"2815119\",\"name\":\"Mingquan Ye\"}],\"doi\":\"10.3389/fnins.2020.586197\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fba78579320f79c5948e85724b0e0e841618c03e\",\"title\":\"Multimodal Glioma Image Segmentation Using Dual Encoder Structure and Channel Spatial Attention Block\",\"url\":\"https://www.semanticscholar.org/paper/fba78579320f79c5948e85724b0e0e841618c03e\",\"venue\":\"Frontiers in Neuroscience\",\"year\":2020},{\"arxivId\":\"2001.07059\",\"authors\":[{\"authorId\":\"6328564\",\"name\":\"M. Farazi\"},{\"authorId\":\"1931655\",\"name\":\"S. Khan\"},{\"authorId\":\"1712576\",\"name\":\"N. Barnes\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8527a2f65eaaf0c96b2ac9430185b0e12d8973a9\",\"title\":\"Accuracy vs. Complexity: A Trade-off in Visual Question Answering Models\",\"url\":\"https://www.semanticscholar.org/paper/8527a2f65eaaf0c96b2ac9430185b0e12d8973a9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.14080\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":\"10.1109/cvpr42600.2020.01098\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0\",\"title\":\"X-Linear Attention Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.16934\",\"authors\":[{\"authorId\":\"40471592\",\"name\":\"Fei Yu\"},{\"authorId\":\"11713158\",\"name\":\"Jiji Tang\"},{\"authorId\":\"2318321\",\"name\":\"Weichong Yin\"},{\"authorId\":\"144825828\",\"name\":\"Y. Sun\"},{\"authorId\":null,\"name\":\"Hao Tian\"},{\"authorId\":\"120155201\",\"name\":\"Hua Wu\"},{\"authorId\":\"144270729\",\"name\":\"Haifeng Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e34cf702b9c90889e268380572bec782280b59c3\",\"title\":\"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph\",\"url\":\"https://www.semanticscholar.org/paper/e34cf702b9c90889e268380572bec782280b59c3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.02419\",\"authors\":[{\"authorId\":\"2672644\",\"name\":\"A. Lazaridou\"},{\"authorId\":\"152259557\",\"name\":\"Marco Baroni\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"90210f2a5331e2c6ad703b7c4f5bdc8dd29c9c94\",\"title\":\"Emergent Multi-Agent Communication in the Deep Learning Era\",\"url\":\"https://www.semanticscholar.org/paper/90210f2a5331e2c6ad703b7c4f5bdc8dd29c9c94\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.15015\",\"authors\":[{\"authorId\":\"65844131\",\"name\":\"Yuxian Meng\"},{\"authorId\":\"1845298604\",\"name\":\"Shuhe Wang\"},{\"authorId\":\"5439717\",\"name\":\"Qinghong Han\"},{\"authorId\":\"48304805\",\"name\":\"Xiaofei Sun\"},{\"authorId\":\"93192602\",\"name\":\"Fei Wu\"},{\"authorId\":null,\"name\":\"Rui Yan\"},{\"authorId\":\"5183779\",\"name\":\"J. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3470e15ae52baae8b8560dd59c616da9820cf43a\",\"title\":\"OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/3470e15ae52baae8b8560dd59c616da9820cf43a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2008.05242\",\"authors\":[{\"authorId\":\"1871552406\",\"name\":\"Myoungha Song\"},{\"authorId\":\"3566974\",\"name\":\"J. Lee\"},{\"authorId\":\"87918971\",\"name\":\"Donghwan Kim\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d619bbbac79d330523128cc58a0a5542d56896c7\",\"title\":\"PAM: Point-wise Attention Module for 6D Object Pose Estimation\",\"url\":\"https://www.semanticscholar.org/paper/d619bbbac79d330523128cc58a0a5542d56896c7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.08918\",\"authors\":[{\"authorId\":\"2027105\",\"name\":\"K. Maninis\"},{\"authorId\":\"30407997\",\"name\":\"Ilija Radosavovic\"},{\"authorId\":\"2010660\",\"name\":\"I. Kokkinos\"}],\"doi\":\"10.1109/CVPR.2019.00195\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8433be13db27f7c195cf54999dfd2dd2845b831e\",\"title\":\"Attentive Single-Tasking of Multiple Tasks\",\"url\":\"https://www.semanticscholar.org/paper/8433be13db27f7c195cf54999dfd2dd2845b831e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.05548\",\"authors\":[{\"authorId\":\"49774254\",\"name\":\"Zilong Zheng\"},{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"3390244\",\"name\":\"Siyuan Qi\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"}],\"doi\":\"10.1109/CVPR.2019.00683\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"edbab8c313fc6d07a2116ab78248ff8af7bd6f4b\",\"title\":\"Reasoning Visual Dialogs With Structural and Partial Observations\",\"url\":\"https://www.semanticscholar.org/paper/edbab8c313fc6d07a2116ab78248ff8af7bd6f4b\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2017491\",\"name\":\"J. Wu\"},{\"authorId\":\"1765674\",\"name\":\"Tianshui Chen\"},{\"authorId\":\"1721715\",\"name\":\"Hefeng Wu\"},{\"authorId\":\"20570336\",\"name\":\"Z. Yang\"},{\"authorId\":\"104002286\",\"name\":\"Qing Wang\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/ICME.2019.00227\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4e3b2d5e0dfffc088e75da297a77f63820cd3a31\",\"title\":\"Concrete Image Captioning by Integrating Content Sensitive and Global Discriminative Objective\",\"url\":\"https://www.semanticscholar.org/paper/4e3b2d5e0dfffc088e75da297a77f63820cd3a31\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1909.02701\",\"authors\":[{\"authorId\":\"49243413\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"2410227\",\"name\":\"Yulun Zhang\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"47003439\",\"name\":\"Yuanyuan Li\"},{\"authorId\":\"46956675\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/ICCV.2019.00475\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"26500af6bb97bf2dab1cc14dfb3b8b08fef67940\",\"title\":\"Visual Semantic Reasoning for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/26500af6bb97bf2dab1cc14dfb3b8b08fef67940\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd6a2c7bbb54ad5c1350eb02718211fe86e125e5\",\"title\":\"End-to-End Multimodal Dialog Systems with Hierarchical Multimodal Attention on Video Features\",\"url\":\"https://www.semanticscholar.org/paper/fd6a2c7bbb54ad5c1350eb02718211fe86e125e5\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8353931\",\"name\":\"Jiahe Shi\"},{\"authorId\":\"5550675\",\"name\":\"Y. Li\"},{\"authorId\":\"103307901\",\"name\":\"Shengjin Wang\"}],\"doi\":\"10.1109/ICIP.2019.8803149\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8c054cda5375018e902daab0b0875773a854d035\",\"title\":\"Cascade Attention: Multiple Feature Based Learning for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c054cda5375018e902daab0b0875773a854d035\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405686623\",\"name\":\"Nils Murrugarra-Llerena\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1109/CVPR.2019.00659\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"title\":\"Cross-Modality Personalization for Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f23a5c29f227c2653014450284b7aa15ad9f88eb\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2007.12750\",\"authors\":[{\"authorId\":\"144354133\",\"name\":\"Michael Cogswell\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1410551459\",\"name\":\"Rishabh Jain\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1606363958\",\"name\":\"Devi Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1005e3d5584fc25d1aa42922d78033c50719bfa\",\"title\":\"Dialog without Dialog Data: Learning Visual Dialog Agents from VQA Data\",\"url\":\"https://www.semanticscholar.org/paper/d1005e3d5584fc25d1aa42922d78033c50719bfa\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1803.06936\",\"authors\":[{\"authorId\":\"144238414\",\"name\":\"Feng Liu\"},{\"authorId\":\"145406421\",\"name\":\"T. Xiang\"},{\"authorId\":\"1697755\",\"name\":\"Timothy M. Hospedales\"},{\"authorId\":\"2345507\",\"name\":\"Wankou Yang\"},{\"authorId\":\"145928755\",\"name\":\"C. Sun\"}],\"doi\":\"10.1109/TPAMI.2018.2880185\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8927117cba0d82d59a35f099b47acb291c6567e3\",\"title\":\"Inverse Visual Question Answering: A New Benchmark and VQA Diagnosis Tool\",\"url\":\"https://www.semanticscholar.org/paper/8927117cba0d82d59a35f099b47acb291c6567e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"L. Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"15246869\",\"name\":\"X. Jiang\"},{\"authorId\":\"31055300\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"1735739\",\"name\":\"Weifeng Zhang\"},{\"authorId\":\"1581498565\",\"name\":\"Yue Hu\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1109/TIP.2020.3034494\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72c2a42e00131f63f4898e29f641c32f9cdd24a3\",\"title\":\"Learning Dual Encoding Model for Adaptive Visual Understanding in Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/72c2a42e00131f63f4898e29f641c32f9cdd24a3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.11797\",\"authors\":[{\"authorId\":\"48017396\",\"name\":\"Kai Qiao\"},{\"authorId\":null,\"name\":\"Chi Zhang\"},{\"authorId\":\"123878630\",\"name\":\"Jian Chen\"},{\"authorId\":\"82527663\",\"name\":\"Linyuan Wang\"},{\"authorId\":\"122221440\",\"name\":\"L. Tong\"},{\"authorId\":null,\"name\":\"Bin Yan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d960ae2c92a9c1224ca6ad403b449a2f5fc5ff9a\",\"title\":\"Neural encoding and interpretation for high-level visual cortices based on fMRI using image caption features\",\"url\":\"https://www.semanticscholar.org/paper/d960ae2c92a9c1224ca6ad403b449a2f5fc5ff9a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3225210\",\"name\":\"Shaohan Hu\"},{\"authorId\":\"15634170\",\"name\":\"S. Huang\"},{\"authorId\":\"50248791\",\"name\":\"G. Wang\"},{\"authorId\":\"49969968\",\"name\":\"Zhipeng Li\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"}],\"doi\":\"10.1007/978-3-030-36802-9_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"797089139d87aeda56bee0b0374bee71521ae169\",\"title\":\"Delving into Precise Attention in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/797089139d87aeda56bee0b0374bee71521ae169\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"title\":\"Efficient Attention Mechanism for Handling All the Interactions between Many Inputs with Application to Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2005.04690\",\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"153003010\",\"name\":\"Xingjian He\"},{\"authorId\":\"1575173011\",\"name\":\"Jie Jiang\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.24963/ijcai.2020/106\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9637ae1fbb8aebc8deb6e3c15dd93839e327e1c\",\"title\":\"Non-Autoregressive Image Captioning with Counterfactuals-Critical Multi-Agent Learning\",\"url\":\"https://www.semanticscholar.org/paper/b9637ae1fbb8aebc8deb6e3c15dd93839e327e1c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1803.09374\",\"authors\":[{\"authorId\":\"40807486\",\"name\":\"Brendan Duke\"},{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"}],\"doi\":\"10.1109/CRV.2018.00016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3d83aa3f1ed743d80b472a660102cb0ce21622ab\",\"title\":\"Generalized Hadamard-Product Fusion Operators for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3d83aa3f1ed743d80b472a660102cb0ce21622ab\",\"venue\":\"2018 15th Conference on Computer and Robot Vision (CRV)\",\"year\":2018},{\"arxivId\":\"2011.02655\",\"authors\":[{\"authorId\":\"47297245\",\"name\":\"H. Zhu\"},{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"29962444\",\"name\":\"Zhao-Heng Zheng\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1cb961c8bea36d5faf3c6011f15dc23832a3e8fc\",\"title\":\"Utilizing Every Image Object for Semi-supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/1cb961c8bea36d5faf3c6011f15dc23832a3e8fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2012673\",\"name\":\"H. Liu\"},{\"authorId\":\"118255837\",\"name\":\"Lisha Mo\"},{\"authorId\":\"46389698\",\"name\":\"Huimin Ma\"}],\"doi\":\"10.1007/978-3-030-34120-6_42\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f71755a896cd92edf1f479181a1e9775da4a188\",\"title\":\"Semantic Inference Network for Human-Object Interaction Detection\",\"url\":\"https://www.semanticscholar.org/paper/3f71755a896cd92edf1f479181a1e9775da4a188\",\"venue\":\"ICIG\",\"year\":2019},{\"arxivId\":\"1812.03928\",\"authors\":[{\"authorId\":\"49889702\",\"name\":\"Y. Zhang\"},{\"authorId\":\"3724810\",\"name\":\"J. Hare\"},{\"authorId\":\"1398417301\",\"name\":\"A. Pr\\u00fcgel-Bennett\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8083cfb0e76358ab54f92eedbe13ed6a874a48e5\",\"title\":\"Learning Representations of Sets through Optimized Permutations\",\"url\":\"https://www.semanticscholar.org/paper/8083cfb0e76358ab54f92eedbe13ed6a874a48e5\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1810.12440\",\"authors\":[{\"authorId\":\"47309247\",\"name\":\"Manoj Acharya\"},{\"authorId\":\"33315685\",\"name\":\"Kushal Kafle\"},{\"authorId\":\"3290098\",\"name\":\"Christopher Kanan\"}],\"doi\":\"10.1609/aaai.v33i01.33018076\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"634161e4759616dbe06f0b1465999d3df122f366\",\"title\":\"TallyQA: Answering Complex Counting Questions\",\"url\":\"https://www.semanticscholar.org/paper/634161e4759616dbe06f0b1465999d3df122f366\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1907.09340\",\"authors\":[{\"authorId\":\"3238408\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":\"10.18653/v1/P19-1654\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fbcea7715f62395fd5a83423d57556e8acca9a62\",\"title\":\"VIFIDEL: Evaluating the Visual Fidelity of Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/fbcea7715f62395fd5a83423d57556e8acca9a62\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yanan Wang\"},{\"authorId\":\"119837541\",\"name\":\"Jian-Ming Wu\"},{\"authorId\":\"2000011573\",\"name\":\"Jinfa Huang\"},{\"authorId\":\"31229419\",\"name\":\"G. Hattori\"},{\"authorId\":\"2466117\",\"name\":\"Y. Takishima\"},{\"authorId\":\"2000114316\",\"name\":\"Shinya Wada\"},{\"authorId\":\"1809845071\",\"name\":\"Rui Kimura\"},{\"authorId\":\"47740650\",\"name\":\"Jian Jhen Chen\"},{\"authorId\":\"1610841750\",\"name\":\"Satoshi Kurihara\"}],\"doi\":\"10.1145/3382507.3418830\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e1526a7ad07d90f1b53b63299bb1cb9c0ce01925\",\"title\":\"LDNN: Linguistic Knowledge Injectable Deep Neural Network for Group Cohesiveness Understanding\",\"url\":\"https://www.semanticscholar.org/paper/e1526a7ad07d90f1b53b63299bb1cb9c0ce01925\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":\"1912.01119\",\"authors\":[{\"authorId\":\"97634546\",\"name\":\"Khaled Jedoui\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e23a129f49fcd52258c14da28311cce3190d271f\",\"title\":\"Deep Bayesian Active Learning for Multiple Correct Outputs\",\"url\":\"https://www.semanticscholar.org/paper/e23a129f49fcd52258c14da28311cce3190d271f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40564040\",\"name\":\"J. Wu\"},{\"authorId\":\"37403439\",\"name\":\"H. Hu\"},{\"authorId\":\"40468514\",\"name\":\"Liang Yang\"}],\"doi\":\"10.1145/3336495\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"715fe60a8615d283c70d12db13857d8948baebca\",\"title\":\"Pseudo-3D Attention Transfer Network with Content-aware Strategy for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/715fe60a8615d283c70d12db13857d8948baebca\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1811.08075\",\"authors\":[{\"authorId\":\"22244104\",\"name\":\"Weilin Cong\"},{\"authorId\":\"145661817\",\"name\":\"William Wang\"},{\"authorId\":\"1686360\",\"name\":\"W. Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3dd504163d56303273697c76c5dc0f47c80cc4b6\",\"title\":\"Scene Graph Generation via Conditional Random Fields\",\"url\":\"https://www.semanticscholar.org/paper/3dd504163d56303273697c76c5dc0f47c80cc4b6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3149518\",\"name\":\"Volkan Cirik\"},{\"authorId\":\"1400419309\",\"name\":\"Taylor Berg-Kirkpatrick\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6d5fa3f69b521ab563cb24484049aeee13ea4b54\",\"title\":\"Refer360$^\\\\circ$: A Referring Expression Recognition Dataset in 360$^\\\\circ$ Images\",\"url\":\"https://www.semanticscholar.org/paper/6d5fa3f69b521ab563cb24484049aeee13ea4b54\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1911.02133\",\"authors\":[{\"authorId\":\"1868193\",\"name\":\"Farley Lai\"},{\"authorId\":\"38907975\",\"name\":\"Ning Xie\"},{\"authorId\":\"2514295\",\"name\":\"Derek Doran\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"75877b483f59cfd8d054652fcdca83f7be7e1d2e\",\"title\":\"Contextual Grounding of Natural Language Entities in Images\",\"url\":\"https://www.semanticscholar.org/paper/75877b483f59cfd8d054652fcdca83f7be7e1d2e\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":\"2005.14386\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"46208708\",\"name\":\"G. Shakhnarovich\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"57ace777e00c0df5d0b729990eed12ed6b0dcfe9\",\"title\":\"Controlling Length in Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/57ace777e00c0df5d0b729990eed12ed6b0dcfe9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1726159226\",\"name\":\"Xiangqing Shen\"},{\"authorId\":\"47655360\",\"name\":\"B. Liu\"},{\"authorId\":\"1697439\",\"name\":\"Yong Zhou\"},{\"authorId\":\"1445303213\",\"name\":\"Jiaqi Zhao\"},{\"authorId\":\"49353948\",\"name\":\"Mingming Liu\"}],\"doi\":\"10.1016/j.knosys.2020.105920\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"46fd9996cd945983f5acc9ed073dc244d6d2f32e\",\"title\":\"Remote sensing image captioning via Variational Autoencoder and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/46fd9996cd945983f5acc9ed073dc244d6d2f32e\",\"venue\":\"Knowl. Based Syst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3422247\",\"name\":\"Sandro Pezzelle\"},{\"authorId\":\"152741757\",\"name\":\"C. Greco\"},{\"authorId\":\"32113652\",\"name\":\"G. Gandolfi\"},{\"authorId\":\"2008208159\",\"name\":\"Eleonora Gualdoni\"},{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.248\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"caba4881b921c43f4562ed6b6aa8f07b971a6fa2\",\"title\":\"Be Different to Be Better! A Benchmark to Leverage the Complementarity of Language and Vision\",\"url\":\"https://www.semanticscholar.org/paper/caba4881b921c43f4562ed6b6aa8f07b971a6fa2\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49287373\",\"name\":\"Lele Xu\"},{\"authorId\":\"47001996\",\"name\":\"Yu-xiang Li\"},{\"authorId\":\"2341356\",\"name\":\"Jinzhong Xu\"},{\"authorId\":\"46846090\",\"name\":\"Lili Guo\"}],\"doi\":\"10.1016/j.compag.2020.105281\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f1981d60b797050daea8a2c44add7fdd28f36ca\",\"title\":\"Two-level attention and score consistency network for plant segmentation\",\"url\":\"https://www.semanticscholar.org/paper/5f1981d60b797050daea8a2c44add7fdd28f36ca\",\"venue\":\"Comput. Electron. Agric.\",\"year\":2020},{\"arxivId\":\"2005.06409\",\"authors\":[{\"authorId\":\"51270689\",\"name\":\"Hyounghun Kim\"},{\"authorId\":\"151270642\",\"name\":\"Zineng Tang\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.acl-main.435\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a87c1f04bae4ab507ed0e03bfd10d870d733367\",\"title\":\"Dense-Caption Matching and Frame-Selection Gating for Temporal Localization in VideoQA\",\"url\":\"https://www.semanticscholar.org/paper/9a87c1f04bae4ab507ed0e03bfd10d870d733367\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2003.05993\",\"authors\":[{\"authorId\":\"8405939\",\"name\":\"Erik Wijmans\"},{\"authorId\":\"20128275\",\"name\":\"J. Straub\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"145955800\",\"name\":\"Irfan Essa\"},{\"authorId\":\"50196944\",\"name\":\"Judy Hoffman\"},{\"authorId\":\"4690624\",\"name\":\"Ari S. Morcos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bc0af8a20212428bf8e70a97ad1f05460dbd2aec\",\"title\":\"Analyzing Visual Representations in Embodied Navigation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/bc0af8a20212428bf8e70a97ad1f05460dbd2aec\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.08034\",\"authors\":[{\"authorId\":\"153060461\",\"name\":\"Darryl Hannan\"},{\"authorId\":\"50658802\",\"name\":\"Akshay Jain\"},{\"authorId\":\"143977265\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.1609/AAAI.V34I05.6294\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6b578fe40abc9ec29424d342a8d676c88a98b921\",\"title\":\"ManyModalQA: Modality Disambiguation and QA over Diverse Inputs\",\"url\":\"https://www.semanticscholar.org/paper/6b578fe40abc9ec29424d342a8d676c88a98b921\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2001.06354\",\"authors\":[{\"authorId\":\"51270689\",\"name\":\"Hyounghun Kim\"},{\"authorId\":\"47300698\",\"name\":\"Hao Tan\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.1609/AAAI.V34I05.6320\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cad92e34323aa135d13d692c759246a8da54d05\",\"title\":\"Modality-Balanced Models for Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/0cad92e34323aa135d13d692c759246a8da54d05\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1906.12158\",\"authors\":[{\"authorId\":\"47294375\",\"name\":\"Zhu Zhang\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"145510896\",\"name\":\"Z. Lin\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"}],\"doi\":\"10.24963/ijcai.2019/609\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"1b2ae2e1f9148e6a05128773c16868cae7e99dc2\",\"title\":\"Open-Ended Long-Form Video Question Answering via Hierarchical Convolutional Self-Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/1b2ae2e1f9148e6a05128773c16868cae7e99dc2\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1909.08782\",\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"153533740\",\"name\":\"Yuan Zhang\"},{\"authorId\":\"1387994164\",\"name\":\"Jason Baldridge\"}],\"doi\":\"10.18653/v1/K19-1006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c937b4499c259835e4e0b2f6b983ca435005fea3\",\"title\":\"Large-scale representation learning from visually grounded untranscribed speech\",\"url\":\"https://www.semanticscholar.org/paper/c937b4499c259835e4e0b2f6b983ca435005fea3\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"1908.08529\",\"authors\":[{\"authorId\":\"29956361\",\"name\":\"Jyoti Aneja\"},{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":\"10.1109/ICCV.2019.00436\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"49d46b0245475067bb7192d9bb1538701ae1c014\",\"title\":\"Sequential Latent Spaces for Modeling the Intention During Diverse Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/49d46b0245475067bb7192d9bb1538701ae1c014\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1911.07251\",\"authors\":[{\"authorId\":\"15246869\",\"name\":\"X. Jiang\"},{\"authorId\":\"119883573\",\"name\":\"J. Yu\"},{\"authorId\":\"70565757\",\"name\":\"Zengchang Qin\"},{\"authorId\":\"15586721\",\"name\":\"Yingying Zhuang\"},{\"authorId\":\"47958013\",\"name\":\"Xingxing Zhang\"},{\"authorId\":null,\"name\":\"Yue Hu\"},{\"authorId\":\"49018095\",\"name\":\"Qi Wu\"}],\"doi\":\"10.1609/AAAI.V34I07.6769\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b42cb7889053f5c89380c82604aa33fd6270894\",\"title\":\"DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/0b42cb7889053f5c89380c82604aa33fd6270894\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1808.05864\",\"authors\":[{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":null,\"name\":\"Feng Wu\"}],\"doi\":\"10.1145/3240508.3240632\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0574dc64c8275b09ed587dc3977f4d3c990bd4df\",\"title\":\"Context-Aware Visual Policy Network for Sequence-Level Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0574dc64c8275b09ed587dc3977f4d3c990bd4df\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1809.02805\",\"authors\":[{\"authorId\":\"35585536\",\"name\":\"Jialin Wu\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":\"10.18653/v1/W19-4812\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"491d0101110fcacfad7c739d5fd807cf8b79de18\",\"title\":\"Faithful Multimodal Explanation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/491d0101110fcacfad7c739d5fd807cf8b79de18\",\"venue\":\"ACL 2019\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49640335\",\"name\":\"Jinfei Zhou\"},{\"authorId\":\"145350969\",\"name\":\"Yaping Zhu\"},{\"authorId\":\"49349128\",\"name\":\"H. Pan\"}],\"doi\":\"10.1145/3317640.3317660\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"c002b14a8bd3879858f2543102d5b8797297c801\",\"title\":\"Image caption based on Visual Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/c002b14a8bd3879858f2543102d5b8797297c801\",\"venue\":\"IVSP 2019\",\"year\":2019},{\"arxivId\":\"1612.00837\",\"authors\":[{\"authorId\":\"37226164\",\"name\":\"Yash Goyal\"},{\"authorId\":\"7595427\",\"name\":\"Tejas Khot\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2017.670\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c\",\"title\":\"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s13735-020-00198-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c85757cdd7de5f99ab1717a6355a09bb717013c\",\"title\":\"MRECN: mixed representation enhanced (de)compositional network for caption generation from visual features, modeling as pseudo tensor product representation\",\"url\":\"https://www.semanticscholar.org/paper/8c85757cdd7de5f99ab1717a6355a09bb717013c\",\"venue\":\"Int. J. Multim. Inf. Retr.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"292d6cbab27ba35c825d75130311a4b27f291af2\",\"title\":\"Visual Question Answering with Annotation-Efficient Zero Shot Learning under Linguistic Domain Shift\",\"url\":\"https://www.semanticscholar.org/paper/292d6cbab27ba35c825d75130311a4b27f291af2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112897627\",\"name\":\"Taichi Iki\"},{\"authorId\":\"1705519\",\"name\":\"A. Aizawa\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.420\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d94ebc1c92870d6d21a2ad4a2402be86ecf36309\",\"title\":\"Language-Conditioned Feature Pyramids for Visual Selection Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d94ebc1c92870d6d21a2ad4a2402be86ecf36309\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.02949\",\"authors\":[{\"authorId\":\"1490732820\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"39543696\",\"name\":\"Fei Sha\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.60\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"73068d13d6e53876c374ebd4c862ec01351c9f39\",\"title\":\"Learning to Represent Image and Text with Denotation Graph\",\"url\":\"https://www.semanticscholar.org/paper/73068d13d6e53876c374ebd4c862ec01351c9f39\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824699\",\"name\":\"Zheng Lian\"},{\"authorId\":\"49403723\",\"name\":\"H. Li\"},{\"authorId\":\"102696161\",\"name\":\"Rui Wang\"},{\"authorId\":\"38865491\",\"name\":\"Xiaohui Hu\"}],\"doi\":\"10.1109/ICTAI50040.2020.00119\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"45f9791def36fd6e2e701b0d5d5c44063a9da473\",\"title\":\"Enhanced soft attention mechanism with an inception-like module for image captioning\",\"url\":\"https://www.semanticscholar.org/paper/45f9791def36fd6e2e701b0d5d5c44063a9da473\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3149518\",\"name\":\"Volkan Cirik\"},{\"authorId\":\"1400419309\",\"name\":\"Taylor Berg-Kirkpatrick\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.18653/v1/2020.acl-main.644\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4887113b68c6b9dfda201019c99bcb99b18d642e\",\"title\":\"Refer360\\u2218: A Referring Expression Recognition Dataset in 360: A Referring Expression Recognition Dataset in 360\\u2218 Images Images\",\"url\":\"https://www.semanticscholar.org/paper/4887113b68c6b9dfda201019c99bcb99b18d642e\",\"venue\":\"ACL 2020\",\"year\":2020},{\"arxivId\":\"2003.12462\",\"authors\":[{\"authorId\":\"144656873\",\"name\":\"O. Sidorov\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"}],\"doi\":\"10.1007/978-3-030-58536-5_44\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7442eaaf453e63195cdee037f8e23830b4004027\",\"title\":\"TextCaps: a Dataset for Image Captioning with Reading Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7442eaaf453e63195cdee037f8e23830b4004027\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.03923\",\"authors\":[{\"authorId\":\"1410097225\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"1e420efcd3fbace6f219c812a78d33cb64e25445\",\"title\":\"Deconfounded Image Captioning: A Causal Retrospect\",\"url\":\"https://www.semanticscholar.org/paper/1e420efcd3fbace6f219c812a78d33cb64e25445\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.02127\",\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":\"1749850\",\"name\":\"J. Liu\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"49298906\",\"name\":\"Jiangwei Li\"},{\"authorId\":\"145851264\",\"name\":\"Wei Luo\"},{\"authorId\":\"46386029\",\"name\":\"H. Lu\"}],\"doi\":\"10.1145/3343031.3350943\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"133921bb5e559de464c0078f5fa67409aca27917\",\"title\":\"Aligning Linguistic Words and Visual Semantic Units for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/133921bb5e559de464c0078f5fa67409aca27917\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2001.09671\",\"authors\":[{\"authorId\":\"19248639\",\"name\":\"Sadaf Gulshad\"},{\"authorId\":\"144638781\",\"name\":\"A. Smeulders\"}],\"doi\":\"10.1145/3372278.3390672\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c2b3e14bb4a2c21d53bb5f4329e0df527ef2d1e\",\"title\":\"Explaining with Counter Visual Attributes and Examples\",\"url\":\"https://www.semanticscholar.org/paper/7c2b3e14bb4a2c21d53bb5f4329e0df527ef2d1e\",\"venue\":\"ICMR\",\"year\":2020},{\"arxivId\":\"2007.03848\",\"authors\":[{\"authorId\":\"1947101\",\"name\":\"Shijie Geng\"},{\"authorId\":\"144740494\",\"name\":\"Peng Gao\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"03598364626c419d3a2578b5c22403f0dd246e99\",\"title\":\"Spatio-Temporal Scene Graphs for Video Dialog\",\"url\":\"https://www.semanticscholar.org/paper/03598364626c419d3a2578b5c22403f0dd246e99\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.14891\",\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":null,\"name\":\"Japsimar Singh Wahi\"},{\"authorId\":\"1557382867\",\"name\":\"Siyao Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"32cdd9d0d4d462a2ca781f0c7cdbc053f6a07938\",\"title\":\"Detecting Hate Speech in Multi-modal Memes\",\"url\":\"https://www.semanticscholar.org/paper/32cdd9d0d4d462a2ca781f0c7cdbc053f6a07938\",\"venue\":\"\",\"year\":2020}],\"corpusId\":3753452,\"doi\":\"10.1109/CVPR.2018.00636\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":309,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"references\":[{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"\\u2022 (a) A young girl standing on top of a tennis court\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1511.07571\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"title\":\"DenseCap: Fully Convolutional Localization Networks for Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1612.01033\",\"authors\":[{\"authorId\":\"3048367\",\"name\":\"M. Pedersoli\"},{\"authorId\":\"144982160\",\"name\":\"T. Lucas\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"34602236\",\"name\":\"Jakob Verbeek\"}],\"doi\":\"10.1109/ICCV.2017.140\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d86f0e22fed5e065ecf54b273d540b2430f014d\",\"title\":\"Areas of Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6d86f0e22fed5e065ecf54b273d540b2430f014d\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1505.00387\",\"authors\":[{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"3035541\",\"name\":\"Klaus Greff\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0945081b5b87187a53d4329cf77cd8bff635795\",\"title\":\"Highway Networks\",\"url\":\"https://www.semanticscholar.org/paper/e0945081b5b87187a53d4329cf77cd8bff635795\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1506.02640\",\"authors\":[{\"authorId\":\"40497777\",\"name\":\"Joseph Redmon\"},{\"authorId\":\"2038685\",\"name\":\"S. Divvala\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2016.91\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8e79ac0ea341056ef20f2616628b3e964764cfd\",\"title\":\"You Only Look Once: Unified, Real-Time Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/f8e79ac0ea341056ef20f2616628b3e964764cfd\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1611.01646\",\"authors\":[{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2017.524\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5785466bc14529e94e54baa4ed051f7037f3b1d3\",\"title\":\"Boosting Image Captioning with Attributes\",\"url\":\"https://www.semanticscholar.org/paper/5785466bc14529e94e54baa4ed051f7037f3b1d3\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1506.01144\",\"authors\":[{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2016.29\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"title\":\"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?\",\"url\":\"https://www.semanticscholar.org/paper/00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1606.08390\",\"authors\":[{\"authorId\":\"14258597\",\"name\":\"A. Jabri\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":\"10.1007/978-3-319-46484-8_44\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c1bbd2672c11a796f1e6e6aa787257498ec8bec\",\"title\":\"Revisiting Visual Question Answering Baselines\",\"url\":\"https://www.semanticscholar.org/paper/3c1bbd2672c11a796f1e6e6aa787257498ec8bec\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1505.00468\",\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"118707418\",\"name\":\"M. Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/s11263-016-0966-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1512.02325\",\"authors\":[{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"144828948\",\"name\":\"S. Reed\"},{\"authorId\":\"2667317\",\"name\":\"Cheng-Yang Fu\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"}],\"doi\":\"10.1007/978-3-319-46448-0_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0\",\"title\":\"SSD: Single Shot MultiBox Detector\",\"url\":\"https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1923920\",\"name\":\"B. Scholl\"}],\"doi\":\"10.1016/S0010-0277(00)00152-9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e32be6db00da2d6ea9ff2be0482abac4daf7ecac\",\"title\":\"Objects and attention: the state of the art\",\"url\":\"https://www.semanticscholar.org/paper/e32be6db00da2d6ea9ff2be0482abac4daf7ecac\",\"venue\":\"Cognition\",\"year\":2001},{\"arxivId\":\"1603.05027\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1007/978-3-319-46493-0_38\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"77f0a39b8e02686fd85b01971f8feb7f60971f80\",\"title\":\"Identity Mappings in Deep Residual Networks\",\"url\":\"https://www.semanticscholar.org/paper/77f0a39b8e02686fd85b01971f8feb7f60971f80\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2502152\",\"name\":\"A. Treisman\"}],\"doi\":\"10.1037/0096-1523.8.2.194\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e543af6f1f29661a43a2cc7706e4a95639327d68\",\"title\":\"Perceptual grouping and attention in visual search for features and for objects.\",\"url\":\"https://www.semanticscholar.org/paper/e543af6f1f29661a43a2cc7706e4a95639327d68\",\"venue\":\"Journal of experimental psychology. Human perception and performance\",\"year\":1982},{\"arxivId\":\"1606.00061\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"145743311\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b\",\"title\":\"Hierarchical Question-Image Co-Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/fb9d253258d6b3beceb9d6cd7bba6e0a29ab875b\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1612.00837\",\"authors\":[{\"authorId\":\"37226164\",\"name\":\"Yash Goyal\"},{\"authorId\":\"7595427\",\"name\":\"Tejas Khot\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2017.670\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c\",\"title\":\"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a27ed1310b9c0832bde8f906e0fcd23d1f3aa79c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1506.01497\",\"authors\":[{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"144716314\",\"name\":\"J. Sun\"}],\"doi\":\"10.1109/TPAMI.2016.2577031\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"title\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":\"1612.00370\",\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"665a311c538fc021c27acd3953f171924cc5905c\",\"title\":\"Optimization of image description metrics using policy gradient methods\",\"url\":\"https://www.semanticscholar.org/paper/665a311c538fc021c27acd3953f171924cc5905c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6970078\",\"name\":\"R. Egly\"},{\"authorId\":\"144169526\",\"name\":\"J. Driver\"},{\"authorId\":\"2449974\",\"name\":\"R. Rafal\"}],\"doi\":\"10.1037/0096-3445.123.2.161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"75a7931e3de0a05a3a71021fde66d71a68247bcf\",\"title\":\"Shifting visual attention between objects and locations: evidence from normal and parietal lesion subjects.\",\"url\":\"https://www.semanticscholar.org/paper/75a7931e3de0a05a3a71021fde66d71a68247bcf\",\"venue\":\"Journal of experimental psychology. General\",\"year\":1994},{\"arxivId\":\"1612.08083\",\"authors\":[{\"authorId\":\"2921469\",\"name\":\"Yann Dauphin\"},{\"authorId\":\"144270981\",\"name\":\"Angela Fan\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"},{\"authorId\":\"2529182\",\"name\":\"David Grangier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"88caa4a0253a8b0076176745ebc072864eab66e1\",\"title\":\"Language Modeling with Gated Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/88caa4a0253a8b0076176745ebc072864eab66e1\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145167247\",\"name\":\"E. Miller\"},{\"authorId\":\"2738354\",\"name\":\"T. J. Buschman\"}],\"doi\":\"10.1126/SCIENCE.1145017\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9e75777ed654908d158642f27a5d24c7bbcfeb20\",\"title\":\"Response to Comment on \\\"Top-Down Versus Bottom-Up Control of Attention in the Prefrontal and Posterior Parietal Cortices\\\"\",\"url\":\"https://www.semanticscholar.org/paper/9e75777ed654908d158642f27a5d24c7bbcfeb20\",\"venue\":\"Science\",\"year\":2007},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"\\u2022 (b) A giraffe standing on top of a green field. High n-gram similarity\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1704.03162\",\"authors\":[{\"authorId\":\"2626422\",\"name\":\"V. Kazemi\"},{\"authorId\":\"2544590\",\"name\":\"Ali Elqursh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d674b540dcd968bc302ea4360df3f4e85e994b55\",\"title\":\"Show, Ask, Attend, and Answer: A Strong Baseline For Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/d674b540dcd968bc302ea4360df3f4e85e994b55\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1612.00563\",\"authors\":[{\"authorId\":\"2071376\",\"name\":\"Steven J. Rennie\"},{\"authorId\":\"2293163\",\"name\":\"E. Marcheret\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"39320489\",\"name\":\"J. Ross\"},{\"authorId\":\"1782589\",\"name\":\"V. Goel\"}],\"doi\":\"10.1109/CVPR.2017.131\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"6c8353697cdbb98dfba4f493875778c4286d3e3a\",\"title\":\"Self-Critical Sequence Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c8353697cdbb98dfba4f493875778c4286d3e3a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1511.05234\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1007/978-3-319-46478-7_28\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1cf6bc0866226c1f8e282463adc8b75d92fba9bb\",\"title\":\"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1cf6bc0866226c1f8e282463adc8b75d92fba9bb\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"25a4fb7025453ce73feef36eeaa45dbd0eb215e5\",\"title\":\"Maximum Expected BLEU Training of Phrase and Lexicon Translation Models\",\"url\":\"https://www.semanticscholar.org/paper/25a4fb7025453ce73feef36eeaa45dbd0eb215e5\",\"venue\":\"ACL\",\"year\":2012},{\"arxivId\":\"1708.02711\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00444\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81\",\"title\":\"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1606.01847\",\"authors\":[{\"authorId\":\"50599725\",\"name\":\"A. Fukui\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"3422876\",\"name\":\"Daylen Yang\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D16-1044\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fddc15480d086629b960be5bff96232f967f2252\",\"title\":\"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding\",\"url\":\"https://www.semanticscholar.org/paper/fddc15480d086629b960be5bff96232f967f2252\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1506.06272\",\"authors\":[{\"authorId\":\"2512850\",\"name\":\"Junqi Jin\"},{\"authorId\":\"2266415\",\"name\":\"K. Fu\"},{\"authorId\":\"2153397\",\"name\":\"Runpeng Cui\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"},{\"authorId\":\"1700883\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"56ffece2817a0363f551210733a611830ba1155d\",\"title\":\"Aligning where to see and what to tell: image caption with region-based attention and scene factorization\",\"url\":\"https://www.semanticscholar.org/paper/56ffece2817a0363f551210733a611830ba1155d\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"\\u2022 (c) A shiny metal pot filled with some diced veggies. \\u2022 (d) The pan on the stove has chopped vegetables in it\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Lu\"},{\"authorId\":null,\"name\":\"X. Lin\"},{\"authorId\":null,\"name\":\"D. Batra\"},{\"authorId\":null,\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deeper lstm and normalized cnn visual question answering model\",\"url\":\"\",\"venue\":\"https://github.com/VT-vision-lab/ VQA_LSTM_CNN,\",\"year\":2015},{\"arxivId\":\"1607.08822\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.1007/978-3-319-46454-1_24\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"title\":\"SPICE: Semantic Propositional Image Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1511.06361\",\"authors\":[{\"authorId\":\"2210865\",\"name\":\"Ivan Vendrov\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"46b8cbcdff87b842c2c1d4a003c831f845096ba7\",\"title\":\"Order-Embeddings of Images and Language\",\"url\":\"https://www.semanticscholar.org/paper/46b8cbcdff87b842c2c1d4a003c831f845096ba7\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Simonyan\"},{\"authorId\":null,\"name\":\"A. Zisserman\"},{\"authorId\":null,\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Joulin , and L . van der Maaten . Revisiting visual question answering baselines\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"163a474747fd63ab62ae586711fa5e5a2ac91bd8\",\"title\":\"Improved Image Captioning via Policy Gradient optimization of SPIDEr\",\"url\":\"https://www.semanticscholar.org/paper/163a474747fd63ab62ae586711fa5e5a2ac91bd8\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1511.03416\",\"authors\":[{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.540\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"def584565d05d6a8ba94de6621adab9e301d375d\",\"title\":\"Visual7W: Grounded Question Answering in Images\",\"url\":\"https://www.semanticscholar.org/paper/def584565d05d6a8ba94de6621adab9e301d375d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.02274\",\"authors\":[{\"authorId\":\"8387085\",\"name\":\"Zichao Yang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"46234526\",\"name\":\"Alex Smola\"}],\"doi\":\"10.1109/CVPR.2016.10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2c1890864c1c2b750f48316dc8b650ba4772adc5\",\"title\":\"Stacked Attention Networks for Image Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/2c1890864c1c2b750f48316dc8b650ba4772adc5\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1602.07332\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"40591424\",\"name\":\"J. Kravitz\"},{\"authorId\":\"6000646\",\"name\":\"Stephanie Chen\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"1388344504\",\"name\":\"David A. Shamma\"},{\"authorId\":\"1379506718\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-016-0981-7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"title\":\"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\",\"url\":\"https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1823362\",\"name\":\"J. Uijlings\"},{\"authorId\":\"1756979\",\"name\":\"K. V. D. Sande\"},{\"authorId\":\"1620219267\",\"name\":\"Theo Gevers\"},{\"authorId\":\"144638781\",\"name\":\"A. Smeulders\"}],\"doi\":\"10.1007/s11263-013-0620-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38b6540ddd5beebffd05047c78183f7575559fb2\",\"title\":\"Selective Search for Object Recognition\",\"url\":\"https://www.semanticscholar.org/paper/38b6540ddd5beebffd05047c78183f7575559fb2\",\"venue\":\"International Journal of Computer Vision\",\"year\":2013},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1611.05546\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a8224266b8ab1483f6548307ab96227147f34da\",\"title\":\"Zero-Shot Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8a8224266b8ab1483f6548307ab96227147f34da\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47087291\",\"name\":\"Z. Yang\"},{\"authorId\":\"30556331\",\"name\":\"Y. Yuan\"},{\"authorId\":\"9287688\",\"name\":\"Yuexin Wu\"},{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"61d2dda8d96a10a714636475c7589bd149bda053\",\"title\":\"Review Networks for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/61d2dda8d96a10a714636475c7589bd149bda053\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"}],\"doi\":\"10.1007/978-3-319-10602-1_26\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b183947ee15718b45546eda6b01e179b9a95421f\",\"title\":\"Edge Boxes: Locating Object Proposals from Edges\",\"url\":\"https://www.semanticscholar.org/paper/b183947ee15718b45546eda6b01e179b9a95421f\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Teney\"},{\"authorId\":null,\"name\":\"P. Anderson\"},{\"authorId\":null,\"name\":\"X. He\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and A\",\"url\":\"\",\"venue\":\"van den Hengel. Tips and tricks for visual question answering: Learnings from the 2017 challenge. In CVPR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2502152\",\"name\":\"A. Treisman\"},{\"authorId\":\"46475250\",\"name\":\"G. Gelade\"}],\"doi\":\"10.1016/0010-0285(80)90005-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"76361a44e145732a39dbc68d9418871038c83be2\",\"title\":\"A feature-integration theory of attention\",\"url\":\"https://www.semanticscholar.org/paper/76361a44e145732a39dbc68d9418871038c83be2\",\"venue\":\"Cognitive Psychology\",\"year\":1980},{\"arxivId\":\"1212.5701\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8729441d734782c3ed532a7d2d9611b438c0a09a\",\"title\":\"ADADELTA: An Adaptive Learning Rate Method\",\"url\":\"https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1603.03925\",\"authors\":[{\"authorId\":\"36610242\",\"name\":\"Quanzeng You\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.503\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bf55591e09b58ea9ce8d66110d6d3000ee804bdd\",\"title\":\"Image Captioning with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/bf55591e09b58ea9ce8d66110d6d3000ee804bdd\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1506.02025\",\"authors\":[{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe87ea16d5eb1c7509da9a0314bbf4c7b0676506\",\"title\":\"Spatial Transformer Networks\",\"url\":\"https://www.semanticscholar.org/paper/fe87ea16d5eb1c7509da9a0314bbf4c7b0676506\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1612.01887\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":\"10.1109/CVPR.2017.345\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9f4d7d622d1f7319cc511bfef661cd973e881a4c\",\"title\":\"Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9f4d7d622d1f7319cc511bfef661cd973e881a4c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143845796\",\"name\":\"Jeffrey Pennington\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1162\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"title\":\"Glove: Global Vectors for Word Representation\",\"url\":\"https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1723344\",\"name\":\"M. Corbetta\"},{\"authorId\":\"39269549\",\"name\":\"G. Shulman\"}],\"doi\":\"10.1038/nrn755\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"53e66b6934516a9859573f4866f81f04bce977ae\",\"title\":\"Control of goal-directed and stimulus-driven attention in the brain\",\"url\":\"https://www.semanticscholar.org/paper/53e66b6934516a9859573f4866f81f04bce977ae\",\"venue\":\"Nature Reviews Neuroscience\",\"year\":2002},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016}],\"title\":\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\"topics\":[{\"topic\":\"Question answering\",\"topicId\":\"18817\",\"url\":\"https://www.semanticscholar.org/topic/18817\"},{\"topic\":\"Top-down and bottom-up design\",\"topicId\":\"96387\",\"url\":\"https://www.semanticscholar.org/topic/96387\"},{\"topic\":\"SPICE\",\"topicId\":\"131946\",\"url\":\"https://www.semanticscholar.org/topic/131946\"},{\"topic\":\"Feature vector\",\"topicId\":\"4255\",\"url\":\"https://www.semanticscholar.org/topic/4255\"},{\"topic\":\"BLEU\",\"topicId\":\"250421\",\"url\":\"https://www.semanticscholar.org/topic/250421\"},{\"topic\":\"Bottom-up parsing\",\"topicId\":\"682974\",\"url\":\"https://www.semanticscholar.org/topic/682974\"},{\"topic\":\"Computer vision\",\"topicId\":\"5332\",\"url\":\"https://www.semanticscholar.org/topic/5332\"},{\"topic\":\"Server (computing)\",\"topicId\":\"6042\",\"url\":\"https://www.semanticscholar.org/topic/6042\"}],\"url\":\"https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"