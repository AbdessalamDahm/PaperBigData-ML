"{\"abstract\":\"Video generation and manipulation is an important yet challenging task in computer vision. Existing methods usually lack ways to explicitly control the synthesized motion. In this work, we present a conditional video generation model that allows detailed control over the motion of the generated video. Given the first frame and sparse motion trajectories specified by users, our model can synthesize a video with corresponding appearance and motion. We propose to combine the advantage of copying pixels from the given frame and hallucinating the lightness difference from scratch which help generate sharp video while keeping the model robust to occlusion and lightness change. We also propose a training paradigm that calculate trajectories from video clips, which eliminated the need of annotated training data. Experiments on several standard benchmarks demonstrate that our approach can generate realistic videos comparable to state-of-the-art video generation and video prediction methods while the motion of the generated videos can correspond well with user input.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"19235216\",\"name\":\"Zekun Hao\",\"url\":\"https://www.semanticscholar.org/author/19235216\"},{\"authorId\":\"47932904\",\"name\":\"Xun Huang\",\"url\":\"https://www.semanticscholar.org/author/47932904\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\",\"url\":\"https://www.semanticscholar.org/author/50172592\"}],\"citationVelocity\":11,\"citations\":[{\"arxivId\":\"2011.03864\",\"authors\":[{\"authorId\":\"2007745319\",\"name\":\"Cade Gordon\"},{\"authorId\":\"2326758\",\"name\":\"Natalie Parde\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac4e977aba1731ce1bd5ddeb9ea466fa86c10a5f\",\"title\":\"Latent Neural Differential Equations for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/ac4e977aba1731ce1bd5ddeb9ea466fa86c10a5f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.06699\",\"authors\":[{\"authorId\":\"37868227\",\"name\":\"C. D. Souza\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"3407519\",\"name\":\"Y. Cabon\"},{\"authorId\":\"26734366\",\"name\":\"N. Murray\"},{\"authorId\":\"3940956\",\"name\":\"A. Pe\\u00f1a\"}],\"doi\":\"10.1007/s11263-019-01222-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"099bdf9cf1c59c7f5ed201759157fb505f51a762\",\"title\":\"Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models\",\"url\":\"https://www.semanticscholar.org/paper/099bdf9cf1c59c7f5ed201759157fb505f51a762\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"1812.01261\",\"authors\":[{\"authorId\":\"48333400\",\"name\":\"S. Yamamoto\"},{\"authorId\":\"1399435786\",\"name\":\"Antonio Tejero-de-Pablos\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e8de3d97d14344d13d15701eea8d6a27c696e27a\",\"title\":\"Conditional Video Generation Using Action-Appearance Captions\",\"url\":\"https://www.semanticscholar.org/paper/e8de3d97d14344d13d15701eea8d6a27c696e27a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2004.14878\",\"authors\":[{\"authorId\":\"46207490\",\"name\":\"Zden\\u011bk Straka\"},{\"authorId\":\"51181107\",\"name\":\"Tom\\u00e1\\u0161 Svoboda\"},{\"authorId\":\"152703120\",\"name\":\"M. Hoffmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd70bf6e980b0b962c8e18a9ae9878da55072178\",\"title\":\"PreCNet: Next Frame Video Prediction Based on Predictive Coding\",\"url\":\"https://www.semanticscholar.org/paper/cd70bf6e980b0b962c8e18a9ae9878da55072178\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"2388085\",\"name\":\"M. K\\u00f6rner\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0fc9b202107bafa4b755c913c904d8ab046b8113\",\"title\":\"FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing Autoencoder GANs\",\"url\":\"https://www.semanticscholar.org/paper/0fc9b202107bafa4b755c913c904d8ab046b8113\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1808.02992\",\"authors\":[{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1768190\",\"name\":\"J. Huang\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1609/aaai.v33i01.33013510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1bb03b7c37bea7536eb62d5db7a8462dc992777\",\"title\":\"Controllable Image-to-Video Translation: A Case Study on Facial Expression Generation\",\"url\":\"https://www.semanticscholar.org/paper/e1bb03b7c37bea7536eb62d5db7a8462dc992777\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2004.02869\",\"authors\":[{\"authorId\":\"19235216\",\"name\":\"Zekun Hao\"},{\"authorId\":\"1388323535\",\"name\":\"Hadar Averbuch-Elor\"},{\"authorId\":\"1830653\",\"name\":\"Noah Snavely\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"}],\"doi\":\"10.1109/cvpr42600.2020.00765\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6563ab3ee5c0771056fd94f89723d891ce301206\",\"title\":\"DualSDF: Semantic Shape Manipulation Using a Two-Level Representation\",\"url\":\"https://www.semanticscholar.org/paper/6563ab3ee5c0771056fd94f89723d891ce301206\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1909.10833\",\"authors\":[{\"authorId\":\"144027562\",\"name\":\"P. K\\u00f6nig\"},{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"2388085\",\"name\":\"M. K\\u00f6rner\"}],\"doi\":\"10.1109/ITSC.2019.8917046\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5bacdc2068aa755016faf585f2d2f90b9046d4eb\",\"title\":\"Enhancing Traffic Scene Predictions with Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/5bacdc2068aa755016faf585f2d2f90b9046d4eb\",\"venue\":\"2019 IEEE Intelligent Transportation Systems Conference (ITSC)\",\"year\":2019},{\"arxivId\":\"1812.02784\",\"authors\":[{\"authorId\":\"50024168\",\"name\":\"Yitong Li\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1752875\",\"name\":\"Y. Shen\"},{\"authorId\":\"31617773\",\"name\":\"J. Liu\"},{\"authorId\":\"145215470\",\"name\":\"Yu Cheng\"},{\"authorId\":\"9287688\",\"name\":\"Yuexin Wu\"},{\"authorId\":\"145006559\",\"name\":\"L. Carin\"},{\"authorId\":\"144752689\",\"name\":\"David Edwin Carlson\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1109/CVPR.2019.00649\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b87e795f1f501843f7f99e83e38f125f6af8600\",\"title\":\"StoryGAN: A Sequential Conditional GAN for Story Visualization\",\"url\":\"https://www.semanticscholar.org/paper/3b87e795f1f501843f7f99e83e38f125f6af8600\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.02912\",\"authors\":[{\"authorId\":\"40897201\",\"name\":\"Tsun-Hsuan Wang\"},{\"authorId\":\"94288126\",\"name\":\"Y. Cheng\"},{\"authorId\":\"49044307\",\"name\":\"Chieh Hubert Lin\"},{\"authorId\":\"1803730\",\"name\":\"Hwann-Tzong Chen\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2019.01059\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb0a151cf8740c66b461ff809e086b91f14bd23b\",\"title\":\"Point-to-Point Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/cb0a151cf8740c66b461ff809e086b91f14bd23b\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.08379\",\"authors\":[{\"authorId\":\"90840812\",\"name\":\"Oran Gafni\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"},{\"authorId\":\"2188620\",\"name\":\"Yaniv Taigman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ca6e880e3163a1e799ceaf002563a6950188745\",\"title\":\"Vid2Game: Controllable Characters Extracted from Real-World Videos\",\"url\":\"https://www.semanticscholar.org/paper/3ca6e880e3163a1e799ceaf002563a6950188745\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1812.01874\",\"authors\":[{\"authorId\":\"5437547\",\"name\":\"Qiyang Hu\"},{\"authorId\":\"52148417\",\"name\":\"Adrian Waelchli\"},{\"authorId\":\"21529935\",\"name\":\"Tiziano Portenier\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"},{\"authorId\":\"145646305\",\"name\":\"P. Favaro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"949ec7afb546060a8c8929462b2eb7bdb468f30a\",\"title\":\"Video Synthesis from a Single Image and Motion Stroke\",\"url\":\"https://www.semanticscholar.org/paper/949ec7afb546060a8c8929462b2eb7bdb468f30a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1811.09245\",\"authors\":[{\"authorId\":\"1725418556\",\"name\":\"Masaki Saito\"},{\"authorId\":\"3083107\",\"name\":\"Shunta Saito\"},{\"authorId\":\"152400765\",\"name\":\"Masanori Koyama\"},{\"authorId\":\"3456592\",\"name\":\"S. Kobayashi\"}],\"doi\":\"10.1007/s11263-020-01333-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"14b175654024c6b57653239674305fe91bca89a1\",\"title\":\"Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN\",\"url\":\"https://www.semanticscholar.org/paper/14b175654024c6b57653239674305fe91bca89a1\",\"venue\":\"International Journal of Computer Vision\",\"year\":2020},{\"arxivId\":\"2007.08509\",\"authors\":[{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36775862\",\"name\":\"K. Sapra\"},{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"}],\"doi\":\"10.1007/978-3-030-58598-3_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"633a7a0c92dd8c254b65a11b759b99c157e115c7\",\"title\":\"World-Consistent Video-to-Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/633a7a0c92dd8c254b65a11b759b99c157e115c7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.04035\",\"authors\":[{\"authorId\":\"152831141\",\"name\":\"Pauline Luc\"},{\"authorId\":\"31993415\",\"name\":\"A. Clark\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"40550616\",\"name\":\"D. Casas\"},{\"authorId\":\"2895238\",\"name\":\"Yotam Doron\"},{\"authorId\":\"51042571\",\"name\":\"Albin Cassirer\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e93e16a1a9f060841956ae2eca735f5cce457c8c\",\"title\":\"Transformation-based Adversarial Video Prediction on Large-Scale Data\",\"url\":\"https://www.semanticscholar.org/paper/e93e16a1a9f060841956ae2eca735f5cce457c8c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.09245\",\"authors\":[{\"authorId\":\"144648787\",\"name\":\"M. Saito\"},{\"authorId\":\"3083107\",\"name\":\"Shunta Saito\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef29d5c85b70fd9dbe04a7b839fbc7d413b161e6\",\"title\":\"TGANv2: Efficient Training of Large Models for Video Generation with Multiple Subsampling Layers\",\"url\":\"https://www.semanticscholar.org/paper/ef29d5c85b70fd9dbe04a7b839fbc7d413b161e6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1910.07192\",\"authors\":[{\"authorId\":\"2420042\",\"name\":\"Y. Endo\"},{\"authorId\":\"2504432\",\"name\":\"Y. Kanamori\"},{\"authorId\":\"36375845\",\"name\":\"Shigeru Kuriyama\"}],\"doi\":\"10.1145/3355089.3356523\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf234eb0d6f4eae787be83c8d9756e8c34e2e80a\",\"title\":\"Animating landscape\",\"url\":\"https://www.semanticscholar.org/paper/bf234eb0d6f4eae787be83c8d9756e8c34e2e80a\",\"venue\":\"ACM Trans. Graph.\",\"year\":2019},{\"arxivId\":\"1812.00452\",\"authors\":[{\"authorId\":null,\"name\":\"Hang Gao\"},{\"authorId\":\"3286703\",\"name\":\"Huazhe Xu\"},{\"authorId\":\"46193391\",\"name\":\"Qi-Zhi Cai\"},{\"authorId\":\"46886239\",\"name\":\"R. Wang\"},{\"authorId\":\"1807197\",\"name\":\"F. Yu\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/ICCV.2019.00910\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ce71c5b4c959c34715503a5980e457e700db9e70\",\"title\":\"Disentangling Propagation and Generation for Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ce71c5b4c959c34715503a5980e457e700db9e70\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2002.09905\",\"authors\":[{\"authorId\":\"146270823\",\"name\":\"Beibei Jin\"},{\"authorId\":\"1943030\",\"name\":\"Y. Hu\"},{\"authorId\":\"31431435\",\"name\":\"Qiankun Tang\"},{\"authorId\":\"66692321\",\"name\":\"Jingyu Niu\"},{\"authorId\":\"144578811\",\"name\":\"Z. Shi\"},{\"authorId\":\"152713339\",\"name\":\"Yinhe Han\"},{\"authorId\":\"40613624\",\"name\":\"Xiaowei Li\"}],\"doi\":\"10.1109/cvpr42600.2020.00461\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a280048e69d41750c42d6f96e451e75c52c07741\",\"title\":\"Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/a280048e69d41750c42d6f96e451e75c52c07741\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1810.01325\",\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"119567230\",\"name\":\"Marco Korner\"}],\"doi\":\"10.5194/isprs-archives-xlii-2-w16-3-2019\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947\",\"title\":\"FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing GANs\",\"url\":\"https://www.semanticscholar.org/paper/b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1638059605\",\"name\":\"Yuki Endo\"},{\"authorId\":\"2504432\",\"name\":\"Y. Kanamori\"},{\"authorId\":\"36375845\",\"name\":\"Shigeru Kuriyama\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab351fa2be0a268742905b04f9ce0f0369139a41\",\"title\":\"Animating Landscape: Self-Supervised Learning of Decoupled Motion and Appearance for Single-Image Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/ab351fa2be0a268742905b04f9ce0f0369139a41\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1812.01874\",\"authors\":[{\"authorId\":\"5437547\",\"name\":\"Qiyang Hu\"},{\"authorId\":\"1822120361\",\"name\":\"Adrian Walchli\"},{\"authorId\":\"21529935\",\"name\":\"Tiziano Portenier\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"},{\"authorId\":\"144707901\",\"name\":\"P. Favaro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6bb74e29321772ea815f88769d31a902a2c3e996\",\"title\":\"Learning to Take Directions One Step at a Time\",\"url\":\"https://www.semanticscholar.org/paper/6bb74e29321772ea815f88769d31a902a2c3e996\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2010.08188\",\"authors\":[{\"authorId\":\"52061414\",\"name\":\"SungHyun Park\"},{\"authorId\":\"1438418837\",\"name\":\"Kangyeol Kim\"},{\"authorId\":\"48174707\",\"name\":\"J. Lee\"},{\"authorId\":\"1934247587\",\"name\":\"Jaegul Choo\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"49899493\",\"name\":\"Sookyung Kim\"},{\"authorId\":\"1387328701\",\"name\":\"Edward Choi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"58db26d7064d16bd45d2fda6b5ded997f47278e5\",\"title\":\"Vid-ODE: Continuous-Time Video Generation with Neural Ordinary Differential Equation\",\"url\":\"https://www.semanticscholar.org/paper/58db26d7064d16bd45d2fda6b5ded997f47278e5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491174614\",\"name\":\"Kohei Matsuzaki\"},{\"authorId\":\"2764854\",\"name\":\"Kazuyuki Tasaka\"}],\"doi\":\"10.1109/IROS40897.2019.8967842\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e9fcbaffe21d80067db719066ae7f964868f310\",\"title\":\"Representation Learning via Parallel Subset Reconstruction for 3D Point Cloud Generation\",\"url\":\"https://www.semanticscholar.org/paper/2e9fcbaffe21d80067db719066ae7f964868f310\",\"venue\":\"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2019},{\"arxivId\":\"2007.12130\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"2479187\",\"name\":\"Moitreya Chatterjee\"},{\"authorId\":\"145237406\",\"name\":\"N. Ahuja\"}],\"doi\":\"10.1007/978-3-030-58583-9_42\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"title\":\"Sound2Sight: Generating Visual Dynamics from Sound and Context\",\"url\":\"https://www.semanticscholar.org/paper/02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143672737\",\"name\":\"A. Mu\\u00f1oz\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"49965376\",\"name\":\"Max Argus\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1233d892716e3c8efab5a72bef14c8a4bdf668d4\",\"title\":\"Multi-Variate Temporal GAN for Large Scale Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/1233d892716e3c8efab5a72bef14c8a4bdf668d4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"1386326892\",\"name\":\"M. K\\u00f6rner\"}],\"doi\":\"10.3390/make2020006\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dce78cb893437f55e8a6f1b806e7415b0d729045\",\"title\":\"The Importance of Loss Functions for Increasing the Generalization Abilities of a Deep Learning-Based Next Frame Prediction Model for Traffic Scenes\",\"url\":\"https://www.semanticscholar.org/paper/dce78cb893437f55e8a6f1b806e7415b0d729045\",\"venue\":\"Machine Learning and Knowledge Extraction\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"c8344be8d1417ed53b8ffea3af7061ce91d68321\",\"title\":\"Supplementary Material for Disentangling Propagation and Generation for Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c8344be8d1417ed53b8ffea3af7061ce91d68321\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1910.12713\",\"authors\":[{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"29955511\",\"name\":\"A. Tao\"},{\"authorId\":\"47062069\",\"name\":\"Guilin Liu\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2301680\",\"name\":\"Bryan Catanzaro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bea92a69b48287caa3a25ff3dfe727bed8888348\",\"title\":\"Few-shot Video-to-Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/bea92a69b48287caa3a25ff3dfe727bed8888348\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2012.06628\",\"authors\":[{\"authorId\":\"121544228\",\"name\":\"Z. Li\"},{\"authorId\":\"1813796\",\"name\":\"Zhaopeng Cui\"},{\"authorId\":\"1821761\",\"name\":\"M. Oswald\"},{\"authorId\":\"88550057\",\"name\":\"Marc Pollefeys\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bbaa5a95aba6cccb7c9885db0329c413f12bbb22\",\"title\":\"Street-view Panoramic Video Synthesis from a Single Satellite Image\",\"url\":\"https://www.semanticscholar.org/paper/bbaa5a95aba6cccb7c9885db0329c413f12bbb22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.05214\",\"authors\":[{\"authorId\":\"152298373\",\"name\":\"Sergiu Oprea\"},{\"authorId\":\"1410236705\",\"name\":\"P. Martinez-Gonzalez\"},{\"authorId\":\"1397392435\",\"name\":\"Alberto Garcia-Garcia\"},{\"authorId\":\"1410269918\",\"name\":\"John Alejandro Castro-Vargas\"},{\"authorId\":\"1405686926\",\"name\":\"S. Orts-Escolano\"},{\"authorId\":\"1429069120\",\"name\":\"J. Garci\\u0301a-Rodri\\u0301guez\"},{\"authorId\":\"1689415\",\"name\":\"Antonis A. Argyros\"}],\"doi\":\"10.1109/TPAMI.2020.3045007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ded24959bbd7715073fa700d5898cdbb2b8353f7\",\"title\":\"A Review on Deep Learning Techniques for Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ded24959bbd7715073fa700d5898cdbb2b8353f7\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"2004.01823\",\"authors\":[{\"authorId\":\"143672737\",\"name\":\"A. Mu\\u00f1oz\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"49965376\",\"name\":\"Max Argus\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"31ce95cc12c4e58fd5d7051d9797589859d5dda1\",\"title\":\"Temporal Shift GAN for Large Scale Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/31ce95cc12c4e58fd5d7051d9797589859d5dda1\",\"venue\":\"\",\"year\":2020}],\"corpusId\":4622697,\"doi\":\"10.1109/CVPR.2018.00819\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"9d629ee73070e7c693ae6924aa52df129a127b33\",\"references\":[{\"arxivId\":\"1704.05831\",\"authors\":[{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"8299168\",\"name\":\"Y. Zou\"},{\"authorId\":\"2459821\",\"name\":\"Sungryull Sohn\"},{\"authorId\":\"10668384\",\"name\":\"Xunyu Lin\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f230cacc511b17b491bf3d90015bbbf85b9ef6af\",\"title\":\"Learning to Generate Long-term Future via Hierarchical Prediction\",\"url\":\"https://www.semanticscholar.org/paper/f230cacc511b17b491bf3d90015bbbf85b9ef6af\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1607.02586\",\"authors\":[{\"authorId\":\"3222730\",\"name\":\"Tianfan Xue\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"18555073\",\"name\":\"K. Bouman\"},{\"authorId\":\"36668046\",\"name\":\"B. Freeman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"397e9b56e46d3cc34af1525493e597facb104570\",\"title\":\"Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/397e9b56e46d3cc34af1525493e597facb104570\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143928130\",\"name\":\"J. Walker\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1109/CVPR.2014.416\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc0bb8f933e514dd9441e3082a34a9f129e35500\",\"title\":\"Patch to the Future: Unsupervised Visual Prediction\",\"url\":\"https://www.semanticscholar.org/paper/cc0bb8f933e514dd9441e3082a34a9f129e35500\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2017.319\",\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6d2892f82a89bfc81f9924adb8bd070fe007adf7\",\"title\":\"Generating the Future with Adversarial Transformers\",\"url\":\"https://www.semanticscholar.org/paper/6d2892f82a89bfc81f9924adb8bd070fe007adf7\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1506.05751\",\"authors\":[{\"authorId\":\"40081727\",\"name\":\"Emily L. Denton\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"47900aca2f0b50da3010ad59b394c870f0e6c02e\",\"title\":\"Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41210105\",\"name\":\"Zhou Wang\"},{\"authorId\":\"144492850\",\"name\":\"A. Bovik\"},{\"authorId\":\"120732976\",\"name\":\"H.R. Sheikh\"},{\"authorId\":\"122128303\",\"name\":\"E.P. Simoncelli\"}],\"doi\":\"10.1109/TIP.2003.819861\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eae2e0fa72e898c289365c0af16daf57a7a6cf40\",\"title\":\"Image quality assessment: from error visibility to structural similarity\",\"url\":\"https://www.semanticscholar.org/paper/eae2e0fa72e898c289365c0af16daf57a7a6cf40\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2004},{\"arxivId\":\"1504.08023\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fb3b63090f95af97723efe565893eb25ea9188c\",\"title\":\"Anticipating the future by watching unlabeled video\",\"url\":\"https://www.semanticscholar.org/paper/0fb3b63090f95af97723efe565893eb25ea9188c\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1502.04681\",\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"title\":\"Unsupervised Learning of Video Representations using LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1605.03557\",\"authors\":[{\"authorId\":\"1822702\",\"name\":\"Tinghui Zhou\"},{\"authorId\":\"2757335\",\"name\":\"Shubham Tulsiani\"},{\"authorId\":\"8397461\",\"name\":\"Weilun Sun\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1007/978-3-319-46493-0_18\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b635705558b9ffcc973966371415b7124830007\",\"title\":\"View Synthesis by Appearance Flow\",\"url\":\"https://www.semanticscholar.org/paper/5b635705558b9ffcc973966371415b7124830007\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1312.6114\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"1678311\",\"name\":\"M. Welling\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"title\":\"Auto-Encoding Variational Bayes\",\"url\":\"https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":\"1605.07157\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb\",\"title\":\"Unsupervised Learning for Physical Interaction through Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/f110cfdbe9ded7a384bcf5c0d56e536bd275a7eb\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121279007\",\"name\":\"R. S. Jones\"}],\"doi\":\"10.1111/j.1467-2995.2006.00288.x\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d20bfd7a53f4367c277ae3b921994a0c2fc3b4ec\",\"title\":\"D\\u00e9ja vu.\",\"url\":\"https://www.semanticscholar.org/paper/d20bfd7a53f4367c277ae3b921994a0c2fc3b4ec\",\"venue\":\"Veterinary anaesthesia and analgesia\",\"year\":2006},{\"arxivId\":\"1605.08104\",\"authors\":[{\"authorId\":\"2023002\",\"name\":\"William Lotter\"},{\"authorId\":\"1852992\",\"name\":\"G. Kreiman\"},{\"authorId\":\"145679323\",\"name\":\"D. Cox\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad367b44f3434b9ba6b46b41ab083210f6827a9f\",\"title\":\"Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning\",\"url\":\"https://www.semanticscholar.org/paper/ad367b44f3434b9ba6b46b41ab083210f6827a9f\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.441\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d721f4d64b8e722222c876f0a0f226ed49476347\",\"title\":\"Action Recognition with Improved Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/d721f4d64b8e722222c876f0a0f226ed49476347\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1609.02612\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.13016/M26GIH-TNYZ\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"title\":\"Generating Videos with Scene Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1710.00421\",\"authors\":[{\"authorId\":\"2664705\",\"name\":\"Y. Li\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"144752689\",\"name\":\"David Edwin Carlson\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a2bbf1f895a1850da3bb6d92b4ffbe61b68145d\",\"title\":\"Video Generation From Text\",\"url\":\"https://www.semanticscholar.org/paper/3a2bbf1f895a1850da3bb6d92b4ffbe61b68145d\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1601.06759\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"41f1d50c85d3180476c4c7b3eea121278b0d8474\",\"title\":\"Pixel Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/41f1d50c85d3180476c4c7b3eea121278b0d8474\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1606.05328\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"2311318\",\"name\":\"Lasse Espeholt\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0936352b78a52bc5d2b5e3f04233efc56664af51\",\"title\":\"Conditional Image Generation with PixelCNN Decoders\",\"url\":\"https://www.semanticscholar.org/paper/0936352b78a52bc5d2b5e3f04233efc56664af51\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Kautz\"},{\"authorId\":null,\"name\":\"N. Kalchbrenner\"},{\"authorId\":null,\"name\":\"L. Espeholt\"},{\"authorId\":null,\"name\":\"O. Vinyals\"},{\"authorId\":null,\"name\":\"A. Graves\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Conditional image generation with pixel - cnn decoders Generating videos with scene dynamics\",\"url\":\"\",\"venue\":\"In Advances In Neural Information Processing Systems\",\"year\":2016},{\"arxivId\":\"1412.6604\",\"authors\":[{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"143627859\",\"name\":\"Joan Bruna\"},{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2939803\",\"name\":\"Ronan Collobert\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"355f98e4827a1b6ad3f29d07ea2bcf9ad078295c\",\"title\":\"Video (language) modeling: a baseline for generative models of natural videos\",\"url\":\"https://www.semanticscholar.org/paper/355f98e4827a1b6ad3f29d07ea2bcf9ad078295c\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1707.04993\",\"authors\":[{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00165\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"title\":\"MoCoGAN: Decomposing Motion and Content for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1606.07873\",\"authors\":[{\"authorId\":\"143928130\",\"name\":\"J. Walker\"},{\"authorId\":\"2786693\",\"name\":\"C. Doersch\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1007/978-3-319-46478-7_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e96c4626e4b3d09727bcbfbdb2672dd9b886743\",\"title\":\"An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/5e96c4626e4b3d09727bcbfbdb2672dd9b886743\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1708.05980\",\"authors\":[{\"authorId\":\"8268761\",\"name\":\"T. Marwah\"},{\"authorId\":\"47351893\",\"name\":\"G. Mittal\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":\"10.1109/ICCV.2017.159\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1491b98d8e9ccca13bec883f94d935d2dff24053\",\"title\":\"Attentive Semantic Video Generation Using Captions\",\"url\":\"https://www.semanticscholar.org/paper/1491b98d8e9ccca13bec883f94d935d2dff24053\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1706.04124\",\"authors\":[{\"authorId\":\"19205596\",\"name\":\"Baoyang Chen\"},{\"authorId\":\"1788029\",\"name\":\"W. Wang\"},{\"authorId\":\"3258842\",\"name\":\"J. Wang\"},{\"authorId\":\"8082703\",\"name\":\"Xiongtao Chen\"}],\"doi\":\"10.1145/3126686.3126737\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"24aac045f1e1a4c13a58eab4c7618dccd4c0e671\",\"title\":\"Video Imagination from a Single Image with Transformation Generation\",\"url\":\"https://www.semanticscholar.org/paper/24aac045f1e1a4c13a58eab4c7618dccd4c0e671\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1505.00295\",\"authors\":[{\"authorId\":\"143928130\",\"name\":\"J. Walker\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1109/ICCV.2015.281\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"098fa9b4c3f7fb41c7a178d36f5dbb50a3ffa377\",\"title\":\"Dense Optical Flow Prediction from a Static Image\",\"url\":\"https://www.semanticscholar.org/paper/098fa9b4c3f7fb41c7a178d36f5dbb50a3ffa377\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1702.02463\",\"authors\":[{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"28919105\",\"name\":\"Raymond A. Yeh\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"36966089\",\"name\":\"Y. Liu\"},{\"authorId\":\"1696487\",\"name\":\"A. Agarwala\"}],\"doi\":\"10.1109/ICCV.2017.478\",\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8970e9caed1fca960ead644e6453a1a7321a7e6d\",\"title\":\"Video Frame Synthesis Using Deep Voxel Flow\",\"url\":\"https://www.semanticscholar.org/paper/8970e9caed1fca960ead644e6453a1a7321a7e6d\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37991449\",\"name\":\"Kris M. Kitani\"},{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1007/978-3-642-33765-9_15\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d8a5addbd17d2c7c8043d8877234675da19938a\",\"title\":\"Activity Forecasting\",\"url\":\"https://www.semanticscholar.org/paper/0d8a5addbd17d2c7c8043d8877234675da19938a\",\"venue\":\"ECCV\",\"year\":2012},{\"arxivId\":\"1705.00053\",\"authors\":[{\"authorId\":\"143928130\",\"name\":\"J. Walker\"},{\"authorId\":\"35789996\",\"name\":\"Kenneth Marino\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1109/ICCV.2017.361\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3df8cc0384814c3fb05c44e494ced947a7d43f36\",\"title\":\"The Pose Knows: Video Forecasting by Generating Pose Futures\",\"url\":\"https://www.semanticscholar.org/paper/3df8cc0384814c3fb05c44e494ced947a7d43f36\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1610.00527\",\"authors\":[{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1841008\",\"name\":\"Ivo Danihelka\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b01871c114b122340209562972ff515b86b16ccf\",\"title\":\"Video Pixel Networks\",\"url\":\"https://www.semanticscholar.org/paper/b01871c114b122340209562972ff515b86b16ccf\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143928529\",\"name\":\"Tian Lan\"},{\"authorId\":\"2708745\",\"name\":\"Tsung-Chuan Chen\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"}],\"doi\":\"10.1007/978-3-319-10578-9_45\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2edc655b678da8fac7db70c3389070eba8727c5\",\"title\":\"A Hierarchical Representation for Future Action Prediction\",\"url\":\"https://www.semanticscholar.org/paper/e2edc655b678da8fac7db70c3389070eba8727c5\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1511.05440\",\"authors\":[{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2341378\",\"name\":\"C. Couprie\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"title\":\"Deep multi-scale video prediction beyond mean square error\",\"url\":\"https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1507.08750\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4257bc131c36504a04382290cbc27ca8bb27813\",\"title\":\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144648787\",\"name\":\"M. Saito\"},{\"authorId\":\"8252749\",\"name\":\"E. Matsumoto\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0f4f16d5b5f9efe304369120651fa688a03d495\",\"title\":\"Temporal Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/f0f4f16d5b5f9efe304369120651fa688a03d495\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1710.11252\",\"authors\":[{\"authorId\":\"3365707\",\"name\":\"M. Babaeizadeh\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"143775101\",\"name\":\"R. Campbell\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59d86da5c5936e7a236678bf5eaaa7753c226fb1\",\"title\":\"Stochastic Variational Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/59d86da5c5936e7a236678bf5eaaa7753c226fb1\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47237027\",\"name\":\"Andreas Geiger\"},{\"authorId\":\"37108776\",\"name\":\"Philip Lenz\"},{\"authorId\":\"1760556\",\"name\":\"C. Stiller\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":\"10.1177/0278364913491297\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"79b949d9b35c3f51dd20fb5c746cc81fc87147eb\",\"title\":\"Vision meets robotics: The KITTI dataset\",\"url\":\"https://www.semanticscholar.org/paper/79b949d9b35c3f51dd20fb5c746cc81fc87147eb\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2013},{\"arxivId\":\"1611.07715\",\"authors\":[{\"authorId\":\"2578924\",\"name\":\"X. Zhu\"},{\"authorId\":\"3372084\",\"name\":\"Y. Xiong\"},{\"authorId\":\"3304536\",\"name\":\"Jifeng Dai\"},{\"authorId\":\"145347148\",\"name\":\"L. Yuan\"},{\"authorId\":\"1732264\",\"name\":\"Y. Wei\"}],\"doi\":\"10.1109/CVPR.2017.441\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c544788faa5b6031db5020bbdaeb25e68c24e19\",\"title\":\"Deep Feature Flow for Video Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5c544788faa5b6031db5020bbdaeb25e68c24e19\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}],\"title\":\"Controllable Video Generation with Sparse Trajectories\",\"topics\":[{\"topic\":\"Sparse matrix\",\"topicId\":\"126\",\"url\":\"https://www.semanticscholar.org/topic/126\"},{\"topic\":\"Pixel\",\"topicId\":\"4254\",\"url\":\"https://www.semanticscholar.org/topic/4254\"},{\"topic\":\"Computer vision\",\"topicId\":\"5332\",\"url\":\"https://www.semanticscholar.org/topic/5332\"},{\"topic\":\"Video clip\",\"topicId\":\"30493\",\"url\":\"https://www.semanticscholar.org/topic/30493\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Programming paradigm\",\"topicId\":\"29522\",\"url\":\"https://www.semanticscholar.org/topic/29522\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"}],\"url\":\"https://www.semanticscholar.org/paper/9d629ee73070e7c693ae6924aa52df129a127b33\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"