"{\"abstract\":\"As two of the five traditional human senses (sight, hearing, taste, smell, and touch), vision and sound are basic sources through which humans understand the world. Often correlated during natural events, these two modalities combine to jointly affect human perception. In this paper, we pose the task of generating sound given visual input. Such capabilities could help enable applications in virtual reality (generating sound for virtual scenes automatically) or provide additional accessibility to images or videos for people with visual impairments. As a first step in this direction, we apply learning-based methods to generate raw waveform samples given input video frames. We evaluate our models on a dataset of videos containing a variety of sounds (such as ambient sounds and sounds from people/animals). Our experiments show that the generated sounds are fairly realistic and have good temporal synchronization with the visual inputs.\",\"arxivId\":\"1712.01393\",\"authors\":[{\"authorId\":\"49455017\",\"name\":\"Yipin Zhou\",\"url\":\"https://www.semanticscholar.org/author/49455017\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\",\"url\":\"https://www.semanticscholar.org/author/8056043\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\",\"url\":\"https://www.semanticscholar.org/author/144823841\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\",\"url\":\"https://www.semanticscholar.org/author/145262461\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\",\"url\":\"https://www.semanticscholar.org/author/1685538\"}],\"citationVelocity\":18,\"citations\":[{\"arxivId\":\"1910.10997\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"50487517\",\"name\":\"Xudong Xu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/ICCV.2019.00037\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"82feef7ae3b3d5ea16ce8bfdf9a01d9aadb4b7be\",\"title\":\"Vision-Infused Deep Audio Inpainting\",\"url\":\"https://www.semanticscholar.org/paper/82feef7ae3b3d5ea16ce8bfdf9a01d9aadb4b7be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2008.00820\",\"authors\":[{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"29969244\",\"name\":\"Y. Zhang\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"4506893\",\"name\":\"Hongdong Xiao\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1109/TIP.2020.3009820\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"d0ed8a26aa689992bd0e870df9e428987ea2a4e2\",\"title\":\"Generating Visually Aligned Sound From Videos\",\"url\":\"https://www.semanticscholar.org/paper/d0ed8a26aa689992bd0e870df9e428987ea2a4e2\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1911.02001\",\"authors\":[{\"authorId\":\"49923155\",\"name\":\"Hsin-Ying Lee\"},{\"authorId\":\"47008546\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"153699069\",\"name\":\"Mingyu Liu\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"8646392\",\"name\":\"Yu-Ding Lu\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12b38ff98d8cb847d735f8fb2b331cdcc30cae91\",\"title\":\"Dancing to Music\",\"url\":\"https://www.semanticscholar.org/paper/12b38ff98d8cb847d735f8fb2b331cdcc30cae91\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2009.08083\",\"authors\":[{\"authorId\":\"50520638\",\"name\":\"Cheng-Che Lee\"},{\"authorId\":\"49661170\",\"name\":\"Wan-Yi Lin\"},{\"authorId\":\"1946996739\",\"name\":\"Yen-Ting Shih\"},{\"authorId\":\"3262164\",\"name\":\"Pei-Yi Kuo\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"}],\"doi\":\"10.1145/3394171.3413624\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"839da7325f0608195649299692121c25cca2f346\",\"title\":\"Crossing You in Style: Cross-modal Style Transfer from Music to Visual Arts\",\"url\":\"https://www.semanticscholar.org/paper/839da7325f0608195649299692121c25cca2f346\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1803.08842\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"145458657\",\"name\":\"Jing Shi\"},{\"authorId\":\"2868721\",\"name\":\"Bochen Li\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01216-8_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"title\":\"Audio-Visual Event Localization in Unconstrained Videos\",\"url\":\"https://www.semanticscholar.org/paper/a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2612784\",\"name\":\"Qiongyi Zhou\"},{\"authorId\":\"3383868\",\"name\":\"Changde Du\"},{\"authorId\":\"1390649600\",\"name\":\"D. Li\"},{\"authorId\":\"1678624678\",\"name\":\"Haibao Wang\"},{\"authorId\":\"40148023\",\"name\":\"J. Liu\"},{\"authorId\":\"46350338\",\"name\":\"H. He\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207466\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f946800fbd6942eab2fbf641d53d26fcc9068a4a\",\"title\":\"Simultaneous Neural Spike Encoding and Decoding Based on Cross-modal Dual Deep Generative Model\",\"url\":\"https://www.semanticscholar.org/paper/f946800fbd6942eab2fbf641d53d26fcc9068a4a\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100803446\",\"name\":\"Citra Fadillah\"},{\"authorId\":\"1423709446\",\"name\":\"RR Ratna Amalia Rahayu\"}],\"doi\":\"10.1109/ICSECC.2019.8907207\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a4ea24554e0682959875fbbb4a2c17a8bf72ab3\",\"title\":\"Sound Visualization Using Typography Composition Based GIF\",\"url\":\"https://www.semanticscholar.org/paper/6a4ea24554e0682959875fbbb4a2c17a8bf72ab3\",\"venue\":\"2019 International Conference on Sustainable Engineering and Creative Computing (ICSECC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153246625\",\"name\":\"Chang\\u2019an Chen\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2368282\",\"name\":\"Carl Schissler\"},{\"authorId\":\"80532894\",\"name\":\"S. V. A. Gar\\u00ed\"},{\"authorId\":\"1390024605\",\"name\":\"Ziad Al-Halah\"},{\"authorId\":\"2736958\",\"name\":\"Vamsi K. Ithapu\"},{\"authorId\":\"153812886\",\"name\":\"P. Robinson\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6090aa8ef7989101e3f18c44a84e3f5a80e905a5\",\"title\":\"Audio-Visual Embodied Navigation\",\"url\":\"https://www.semanticscholar.org/paper/6090aa8ef7989101e3f18c44a84e3f5a80e905a5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.10055\",\"authors\":[{\"authorId\":\"46708281\",\"name\":\"Y. Okamoto\"},{\"authorId\":\"144328402\",\"name\":\"Keisuke Imoto\"},{\"authorId\":\"2212530\",\"name\":\"Tatsuya Komatsu\"},{\"authorId\":\"2424104\",\"name\":\"Shinnosuke Takamichi\"},{\"authorId\":\"153219368\",\"name\":\"Takumi Yagyu\"},{\"authorId\":\"2742155\",\"name\":\"Ryosuke Yamanishi\"},{\"authorId\":\"144979325\",\"name\":\"Y. Yamashita\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0f47ae1f2852c6d06305ef2c4bae00e3d0b6255a\",\"title\":\"Overview of Tasks and Investigation of Subjective Evaluation Methods in Environmental Sound Synthesis and Conversion\",\"url\":\"https://www.semanticscholar.org/paper/0f47ae1f2852c6d06305ef2c4bae00e3d0b6255a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.05538\",\"authors\":[{\"authorId\":\"153318498\",\"name\":\"Shiguang Liu\"},{\"authorId\":\"1699159\",\"name\":\"D. Manocha\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"770a44ea4c18d2a62c24b1f2758743d04d357e45\",\"title\":\"Sound Synthesis, Propagation, and Rendering: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/770a44ea4c18d2a62c24b1f2758743d04d357e45\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.09805\",\"authors\":[{\"authorId\":\"1491401265\",\"name\":\"Shuang Ma\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"1381192475\",\"name\":\"Daniel McDuff\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7df5dd31f2ce600a4be589dff4d6a758be262324\",\"title\":\"Learning Audio-Visual Representations with Active Contrastive Coding\",\"url\":\"https://www.semanticscholar.org/paper/7df5dd31f2ce600a4be589dff4d6a758be262324\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.07673\",\"authors\":[{\"authorId\":\"8778936\",\"name\":\"D. Zeng\"}],\"doi\":\"10.1109/ICDMW.2019.00156\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a74dc7344852678905d76f6768a43d6941ae668\",\"title\":\"Learning Joint Embedding for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/7a74dc7344852678905d76f6768a43d6941ae668\",\"venue\":\"2019 International Conference on Data Mining Workshops (ICDMW)\",\"year\":2019},{\"arxivId\":\"1804.05490\",\"authors\":[{\"authorId\":\"144823644\",\"name\":\"Olivier Augereau\"},{\"authorId\":\"40411993\",\"name\":\"M. Iwata\"},{\"authorId\":\"3277321\",\"name\":\"K. Kise\"}],\"doi\":\"10.3390/jimaging4070087\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"31e6e9f24a60936a8ef901a74664dc6c514f604d\",\"title\":\"A survey of comics research in computer science\",\"url\":\"https://www.semanticscholar.org/paper/31e6e9f24a60936a8ef901a74664dc6c514f604d\",\"venue\":\"J. Imaging\",\"year\":2018},{\"arxivId\":\"2006.07931\",\"authors\":[{\"authorId\":\"134192555\",\"name\":\"J. F. Montesinos\"},{\"authorId\":\"9936815\",\"name\":\"Olga Slizovskaia\"},{\"authorId\":\"1916387\",\"name\":\"G. Haro\"}],\"doi\":\"10.1109/MMSP48831.2020.9287124\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b36193e42615bf1ec6202f22be6bfc5ed86d444\",\"title\":\"Solos: A Dataset for Audio-Visual Music Analysis\",\"url\":\"https://www.semanticscholar.org/paper/5b36193e42615bf1ec6202f22be6bfc5ed86d444\",\"venue\":\"2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)\",\"year\":2020},{\"arxivId\":\"2012.03478\",\"authors\":[{\"authorId\":\"1491240396\",\"name\":\"Kun Su\"},{\"authorId\":\"10598535\",\"name\":\"X. Liu\"},{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"866ba646300961747ddc29ce57153858e2be588a\",\"title\":\"Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements\",\"url\":\"https://www.semanticscholar.org/paper/866ba646300961747ddc29ce57153858e2be588a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394839535\",\"name\":\"Angie Boggust\"},{\"authorId\":\"3104038\",\"name\":\"Kartik Audhkhasi\"},{\"authorId\":\"5113463\",\"name\":\"D. Joshi\"},{\"authorId\":\"30507748\",\"name\":\"David Harwath\"},{\"authorId\":\"70913918\",\"name\":\"S. Thomas\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"1891570\",\"name\":\"Dan Gutfreund\"},{\"authorId\":\"48380309\",\"name\":\"Y. Zhang\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"1774515\",\"name\":\"M. Picheny\"},{\"authorId\":\"36217631\",\"name\":\"J. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"67e733b987eb508fdaa60746bf0f5ed4ad192dbe\",\"title\":\"Grounding Spoken Words in Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/67e733b987eb508fdaa60746bf0f5ed4ad192dbe\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":\"1911.12618\",\"authors\":[{\"authorId\":\"39622233\",\"name\":\"J. Ram\\u00edrez\"},{\"authorId\":\"145254282\",\"name\":\"M. Flores\"}],\"doi\":\"10.1007/s10844-019-00582-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cdbfe79a93e6f9e1ce7ae0d0a9d272c496c3a964\",\"title\":\"Machine learning for music genre: multifaceted review and experimentation with audioset\",\"url\":\"https://www.semanticscholar.org/paper/cdbfe79a93e6f9e1ce7ae0d0a9d272c496c3a964\",\"venue\":\"Journal of Intelligent Information Systems\",\"year\":2019},{\"arxivId\":\"1812.04204\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2019.00041\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b809837560cf11937ee857338eb1a7ccd2abc7b2\",\"title\":\"2.5D Visual Sound\",\"url\":\"https://www.semanticscholar.org/paper/b809837560cf11937ee857338eb1a7ccd2abc7b2\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1904.09115\",\"authors\":[{\"authorId\":\"48080348\",\"name\":\"D. Hu\"},{\"authorId\":\"49370704\",\"name\":\"Dong Wang\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"144962210\",\"name\":\"F. Nie\"},{\"authorId\":\"39669401\",\"name\":\"Q. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00816\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6ccd4369781ccc31bd6e08a837d4ab0d9ddda4c\",\"title\":\"Listen to the Image\",\"url\":\"https://www.semanticscholar.org/paper/e6ccd4369781ccc31bd6e08a837d4ab0d9ddda4c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1804.01665\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-030-01219-9_3\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5c9ca16cb2337fd5948c7af28c29c156981250e8\",\"title\":\"Learning to Separate Object Sounds by Watching Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/5c9ca16cb2337fd5948c7af28c29c156981250e8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144157676\",\"name\":\"Sahil Shah\"},{\"authorId\":\"25017988\",\"name\":\"A. Bansal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7e3f1a37a9e8e0202026b8c2080621ca8732c179\",\"title\":\"AML Final Report Sight & Sound\",\"url\":\"https://www.semanticscholar.org/paper/7e3f1a37a9e8e0202026b8c2080621ca8732c179\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2002.10981\",\"authors\":[{\"authorId\":\"48359318\",\"name\":\"Sanchita Ghose\"},{\"authorId\":\"2845029\",\"name\":\"John J. Prevost\"}],\"doi\":\"10.1109/TMM.2020.3005033\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1337db4d3283e77e959a683ef5cb15949f1d5400\",\"title\":\"AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent Videos with Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1337db4d3283e77e959a683ef5cb15949f1d5400\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.01616\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"153246625\",\"name\":\"Chang\\u2019an Chen\"},{\"authorId\":\"9187007\",\"name\":\"Z. Al-Halah\"},{\"authorId\":\"2368282\",\"name\":\"Carl Schissler\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-030-58545-7_38\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33006edd74d92453903df5a53fa25539047c1850\",\"title\":\"VisualEchoes: Spatial Image Representation Learning through Echolocation\",\"url\":\"https://www.semanticscholar.org/paper/33006edd74d92453903df5a53fa25539047c1850\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2006.14348\",\"authors\":[{\"authorId\":\"1491240396\",\"name\":\"Kun Su\"},{\"authorId\":\"10598535\",\"name\":\"X. Liu\"},{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6127cceb5847551cc09814a0d00cf63ba21b546\",\"title\":\"Audeo: Audio Generation for a Silent Performance Video\",\"url\":\"https://www.semanticscholar.org/paper/c6127cceb5847551cc09814a0d00cf63ba21b546\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1912.11474\",\"authors\":[{\"authorId\":\"153246625\",\"name\":\"Chang\\u2019an Chen\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2368282\",\"name\":\"Carl Schissler\"},{\"authorId\":\"80532894\",\"name\":\"S. V. A. Gar\\u00ed\"},{\"authorId\":\"1390024605\",\"name\":\"Ziad Al-Halah\"},{\"authorId\":\"2736958\",\"name\":\"Vamsi K. Ithapu\"},{\"authorId\":\"153812886\",\"name\":\"P. Robinson\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-030-58539-6_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c47ef056ac57e83405f9ee63c32c6a185011d187\",\"title\":\"SoundSpaces: Audio-Visual Navigation in 3D Environments\",\"url\":\"https://www.semanticscholar.org/paper/c47ef056ac57e83405f9ee63c32c6a185011d187\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1908.03737\",\"authors\":[{\"authorId\":\"8778936\",\"name\":\"D. Zeng\"},{\"authorId\":\"145562329\",\"name\":\"Yi Yu\"},{\"authorId\":\"1683426\",\"name\":\"Keizo Oyama\"}],\"doi\":\"10.1145/3387164\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"41e6abf0d3607345949004521c301eaf52c26533\",\"title\":\"Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/41e6abf0d3607345949004521c301eaf52c26533\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yujia Wang\"},{\"authorId\":\"14484252\",\"name\":\"W. Liang\"},{\"authorId\":\"1993490392\",\"name\":\"Wanwan Li\"},{\"authorId\":\"40580714\",\"name\":\"Dingzeyu Li\"},{\"authorId\":\"37397820\",\"name\":\"Lap-Fai Yu\"}],\"doi\":\"10.1145/3394171.3413894\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6363638e0bcbd2bc7fdf64467fe5cb52754f51c2\",\"title\":\"Scene-Aware Background Music Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/6363638e0bcbd2bc7fdf64467fe5cb52754f51c2\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"2173143\",\"name\":\"C. Zhang\"},{\"authorId\":\"144823841\",\"name\":\"Chen Fang\"},{\"authorId\":\"48707717\",\"name\":\"Z. Wang\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1007/978-3-030-11024-6_43\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d4e7a7588b5d1474a4cd884e81730342e18186e9\",\"title\":\"Visually Indicated Sound Generation by Perceptually Optimized Classification\",\"url\":\"https://www.semanticscholar.org/paper/d4e7a7588b5d1474a4cd884e81730342e18186e9\",\"venue\":\"ECCV Workshops\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144039832\",\"name\":\"P. Barros\"},{\"authorId\":\"2236890\",\"name\":\"Manfred Eppe\"},{\"authorId\":\"2988592\",\"name\":\"G. Parisi\"},{\"authorId\":\"144227938\",\"name\":\"X. Liu\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.3389/frobt.2019.00137\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"afbc0881addf58dd82afb2580d0fecd42f65acd6\",\"title\":\"Expectation Learning for Stimulus Prediction Across Modalities Improves Unisensory Classification\",\"url\":\"https://www.semanticscholar.org/paper/afbc0881addf58dd82afb2580d0fecd42f65acd6\",\"venue\":\"Front. Robot. AI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98727476\",\"name\":\"Tsan-Hwei Huang\"},{\"authorId\":\"2024357948\",\"name\":\"Hunter Hsieh\"},{\"authorId\":\"2025282857\",\"name\":\"Jiaqi Qin\"},{\"authorId\":\"2024727250\",\"name\":\"Hsien-Fung Liu\"},{\"authorId\":\"2377003\",\"name\":\"M. Eirinaki\"}],\"doi\":\"10.1109/TransAI49837.2020.00008\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"30855a62d640c70429a81eec1b22bcc2a63c9c30\",\"title\":\"Play it again IMuCo! Music Composition to Match your Mood\",\"url\":\"https://www.semanticscholar.org/paper/30855a62d640c70429a81eec1b22bcc2a63c9c30\",\"venue\":\"2020 Second International Conference on Transdisciplinary AI (TransAI)\",\"year\":2020},{\"arxivId\":\"2004.02541\",\"authors\":[{\"authorId\":\"9686806\",\"name\":\"Daniel Michelsanti\"},{\"authorId\":\"9936815\",\"name\":\"Olga Slizovskaia\"},{\"authorId\":\"1916387\",\"name\":\"G. Haro\"},{\"authorId\":\"145217215\",\"name\":\"E. G\\u00f3mez\"},{\"authorId\":\"71668001\",\"name\":\"Zheng-Hua Tan\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":\"10.21437/interspeech.2020-1026\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a0d2dc8123277cf3c894a10121272207dc39413\",\"title\":\"Vocoder-Based Speech Synthesis from Silent Videos\",\"url\":\"https://www.semanticscholar.org/paper/1a0d2dc8123277cf3c894a10121272207dc39413\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40069761\",\"name\":\"Auston Sterling\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"9e404eb374035b8a79b45ef4fcb6f728f4372ef2\",\"title\":\"AUDIO-MATERIAL MODELING AND RECONSTRUCTION FOR MULTIMODAL INTERACTION\",\"url\":\"https://www.semanticscholar.org/paper/9e404eb374035b8a79b45ef4fcb6f728f4372ef2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1904.04540\",\"authors\":[{\"authorId\":\"1787190\",\"name\":\"Hirokazu Kameoka\"},{\"authorId\":\"2007195\",\"name\":\"Kou Tanaka\"},{\"authorId\":\"96646484\",\"name\":\"Aaron Valero Puche\"},{\"authorId\":\"2991962\",\"name\":\"Yasunori Ohishi\"},{\"authorId\":\"50509323\",\"name\":\"Takuhiro Kaneko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a354cd50fa34379b79c951c165d9c7238780fe8\",\"title\":\"Crossmodal Voice Conversion\",\"url\":\"https://www.semanticscholar.org/paper/5a354cd50fa34379b79c951c165d9c7238780fe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46885959\",\"name\":\"R. Wang\"},{\"authorId\":\"32885778\",\"name\":\"Huaibo Huang\"},{\"authorId\":\"1391221164\",\"name\":\"Xufeng Zhang\"},{\"authorId\":\"35043641\",\"name\":\"Jixin Ma\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"}],\"doi\":\"10.1109/ICMEW.2019.00-70\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2a8ca7fbb86dc0b8f4d9c4c44e7bffb186c585a\",\"title\":\"A Novel Distance Learning for Elastic Cross-Modal Audio-Visual Matching\",\"url\":\"https://www.semanticscholar.org/paper/a2a8ca7fbb86dc0b8f4d9c4c44e7bffb186c585a\",\"venue\":\"2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)\",\"year\":2019},{\"arxivId\":\"1912.04487\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":\"10.1109/cvpr42600.2020.01047\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"title\":\"Listen to Look: Action Recognition by Previewing Audio\",\"url\":\"https://www.semanticscholar.org/paper/c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1904.07750\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/ICCV.2019.00398\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a88fa82b2032b8234a2005c26de8fbb096aa27a\",\"title\":\"Co-Separating Sounds of Visual Objects\",\"url\":\"https://www.semanticscholar.org/paper/7a88fa82b2032b8234a2005c26de8fbb096aa27a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29724362\",\"name\":\"Longlong Jing\"},{\"authorId\":\"19263938\",\"name\":\"Elahe Vahdani\"},{\"authorId\":\"3419368\",\"name\":\"Jiaxing Tan\"},{\"authorId\":\"8193125\",\"name\":\"Y. Tian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e304622f51f30f281cd8cc09a992b7519f135d2d\",\"title\":\"Cross-modal Center Loss\",\"url\":\"https://www.semanticscholar.org/paper/e304622f51f30f281cd8cc09a992b7519f135d2d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.09199\",\"authors\":[{\"authorId\":\"41020711\",\"name\":\"Andrew Rouditchenko\"},{\"authorId\":\"1394839535\",\"name\":\"Angie Boggust\"},{\"authorId\":\"30507748\",\"name\":\"David Harwath\"},{\"authorId\":\"47669634\",\"name\":\"Dhiraj Joshi\"},{\"authorId\":\"70913918\",\"name\":\"S. Thomas\"},{\"authorId\":\"3104038\",\"name\":\"Kartik Audhkhasi\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"144707379\",\"name\":\"Brian Kingsbury\"},{\"authorId\":\"1774515\",\"name\":\"M. Picheny\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"},{\"authorId\":\"152450847\",\"name\":\"J. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1141ac4a1443ae7b44266a84a7f042f38759ac47\",\"title\":\"AVLnet: Learning Audio-Visual Language Representations from Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/1141ac4a1443ae7b44266a84a7f042f38759ac47\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1804.03160\",\"authors\":[{\"authorId\":\"144077750\",\"name\":\"Hang Zhao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"41020711\",\"name\":\"Andrew Rouditchenko\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-030-01246-5_35\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fe018f22600d07cbd0452a070e03708886470015\",\"title\":\"The Sound of Pixels\",\"url\":\"https://www.semanticscholar.org/paper/fe018f22600d07cbd0452a070e03708886470015\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48359318\",\"name\":\"Sanchita Ghose\"},{\"authorId\":\"39409158\",\"name\":\"John J. Prevost\"}],\"doi\":\"10.1109/SoSE50414.2020.9130483\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"06525364255afaa0a159edd4204e6a561174579f\",\"title\":\"Enabling an IoT System of Systems through Auto Sound Synthesis in Silent Video with DNN\",\"url\":\"https://www.semanticscholar.org/paper/06525364255afaa0a159edd4204e6a561174579f\",\"venue\":\"2020 IEEE 15th International Conference of System of Systems Engineering (SoSE)\",\"year\":2020},{\"arxivId\":\"1908.11602\",\"authors\":[{\"authorId\":\"50487517\",\"name\":\"Xudong Xu\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/ICCV.2019.00097\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b7a5f074e06cf8d71e26fb5d25b86b4164a5bb54\",\"title\":\"Recursive Visual Sound Separation Using Minus-Plus Net\",\"url\":\"https://www.semanticscholar.org/paper/b7a5f074e06cf8d71e26fb5d25b86b4164a5bb54\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1908.07094\",\"authors\":[{\"authorId\":\"145798572\",\"name\":\"S. Ma\"},{\"authorId\":\"1801452\",\"name\":\"D. McDuff\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":\"10.1109/ICCV.2019.00769\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1e1feac96004866052787115ea08a4dcdd888b9\",\"title\":\"Unpaired Image-to-Speech Synthesis With Multimodal Information Bottleneck\",\"url\":\"https://www.semanticscholar.org/paper/e1e1feac96004866052787115ea08a4dcdd888b9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2007.13729\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144220896\",\"name\":\"Xiaoyu Chen\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f978b3829acad6ac8b3372d2fa8d38a45b96d3d\",\"title\":\"Noisy Agents: Self-supervised Exploration by Predicting Auditory Events\",\"url\":\"https://www.semanticscholar.org/paper/5f978b3829acad6ac8b3372d2fa8d38a45b96d3d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.07984\",\"authors\":[{\"authorId\":\"8301799\",\"name\":\"Lingyu Zhu\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b2cf1d0538f3ad0d5fae1af4d97571b9291eabc\",\"title\":\"Separating Sounds from a Single Image\",\"url\":\"https://www.semanticscholar.org/paper/7b2cf1d0538f3ad0d5fae1af4d97571b9291eabc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2202838\",\"name\":\"David B. Lindell\"},{\"authorId\":\"1731170\",\"name\":\"G. Wetzstein\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":\"10.1109/CVPR.2019.00694\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c5b1bbc49e4be0efd98e0b513e0b41659a32413\",\"title\":\"Acoustic Non-Line-Of-Sight Imaging\",\"url\":\"https://www.semanticscholar.org/paper/7c5b1bbc49e4be0efd98e0b513e0b41659a32413\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1911.09649\",\"authors\":[{\"authorId\":\"40895287\",\"name\":\"Arda Senocak\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"3053231\",\"name\":\"J. Kim\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"98758720\",\"name\":\"I. Kweon\"}],\"doi\":\"10.1109/tpami.2019.2952095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"edd75cbdad797febb765e2bcaa8653b77138e3a5\",\"title\":\"Learning to Localize Sound Sources in Visual Scenes: Analysis and Applications\",\"url\":\"https://www.semanticscholar.org/paper/edd75cbdad797febb765e2bcaa8653b77138e3a5\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2019},{\"arxivId\":\"1909.12780\",\"authors\":[{\"authorId\":\"41016678\",\"name\":\"Givi Meishvili\"},{\"authorId\":\"5641221\",\"name\":\"S. Jenni\"},{\"authorId\":\"144707901\",\"name\":\"P. Favaro\"}],\"doi\":\"10.1109/cvpr42600.2020.00144\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b073e91f84ab59e482c2f1e22918f46ef606a531\",\"title\":\"Learning to Have an Ear for Face Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/b073e91f84ab59e482c2f1e22918f46ef606a531\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2007.10984\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-030-58621-8_44\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"43971a0a2593f660427e016032b983b52f8dd8eb\",\"title\":\"Foley Music: Learning to Generate Music from Videos\",\"url\":\"https://www.semanticscholar.org/paper/43971a0a2593f660427e016032b983b52f8dd8eb\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1904.05979\",\"authors\":[{\"authorId\":\"144077750\",\"name\":\"Hang Zhao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"2650832\",\"name\":\"W. Ma\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/ICCV.2019.00182\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c880de441a41c351955ad0bf8f712eeee500ac67\",\"title\":\"The Sound of Motions\",\"url\":\"https://www.semanticscholar.org/paper/c880de441a41c351955ad0bf8f712eeee500ac67\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2001.04758\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"100602595\",\"name\":\"Mandi Luo\"},{\"authorId\":\"77886607\",\"name\":\"Rui Wang\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"title\":\"Deep Audio-Visual Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.07473\",\"authors\":[{\"authorId\":\"2564871\",\"name\":\"Yan-Bo Lin\"},{\"authorId\":\"3312576\",\"name\":\"Yu-Jhe Li\"},{\"authorId\":\"2733735\",\"name\":\"Y. Wang\"}],\"doi\":\"10.1109/ICASSP.2019.8683226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6dad33f2e317cc23abeba8596a4c8a4a13117d54\",\"title\":\"Dual-modality Seq2Seq Network for Audio-visual Event Localization\",\"url\":\"https://www.semanticscholar.org/paper/6dad33f2e317cc23abeba8596a4c8a4a13117d54\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"2007.09902\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"48670507\",\"name\":\"Xudong Xu\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"}],\"doi\":\"10.1007/978-3-030-58610-2_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"73aa926dad010a3f1bb89faa31241f97a89cc461\",\"title\":\"Sep-Stereo: Visually Guided Stereophonic Audio Generation by Associating Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/73aa926dad010a3f1bb89faa31241f97a89cc461\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.09476\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"47940821\",\"name\":\"Hang Zhao\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR42600.2020.01049\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b643a7186c08db9f13d7204f6e5e739f97902e71\",\"title\":\"Music Gesture for Visual Sound Separation\",\"url\":\"https://www.semanticscholar.org/paper/b643a7186c08db9f13d7204f6e5e739f97902e71\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}],\"corpusId\":32701102,\"doi\":\"10.1109/CVPR.2018.00374\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":8,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"f2d126e02401ec9f3c131eac423620529996df2f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1716857\",\"name\":\"J. Yamagishi\"},{\"authorId\":\"20418388\",\"name\":\"T. Nose\"},{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"1749989\",\"name\":\"Z. Ling\"},{\"authorId\":\"1726559\",\"name\":\"T. Toda\"},{\"authorId\":\"1723069\",\"name\":\"K. Tokuda\"},{\"authorId\":\"144783569\",\"name\":\"Simon King\"},{\"authorId\":\"145086187\",\"name\":\"S. Renals\"}],\"doi\":\"10.1109/TASL.2009.2016394\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a66edfcd2e3ccb72e8f257197eb3b3c73430e26b\",\"title\":\"Robust Speaker-Adaptive HMM-Based Text-to-Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/a66edfcd2e3ccb72e8f257197eb3b3c73430e26b\",\"venue\":\"IEEE Transactions on Audio, Speech, and Language Processing\",\"year\":2009},{\"arxivId\":\"1804.03641\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1007/978-3-030-01231-1_39\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"title\":\"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\",\"url\":\"https://www.semanticscholar.org/paper/171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1609.08675\",\"authors\":[{\"authorId\":\"1389570466\",\"name\":\"Sami Abu-El-Haija\"},{\"authorId\":\"144317839\",\"name\":\"Nisarg Kothari\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"1820908\",\"name\":\"A. Natsev\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"2758088\",\"name\":\"B. Varadarajan\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716\",\"title\":\"YouTube-8M: A Large-Scale Video Classification Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1705.08168\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/ICCV.2017.73\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b5f696f73c1264ccb8e97d3b738a2342ecd6bee\",\"title\":\"Look, Listen and Learn\",\"url\":\"https://www.semanticscholar.org/paper/9b5f696f73c1264ccb8e97d3b738a2342ecd6bee\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"47680287\",\"name\":\"W. Dong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2009.5206848\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b47265245e8db53a553049dcb27ed3e495fd625\",\"title\":\"ImageNet: A large-scale hierarchical image database\",\"url\":\"https://www.semanticscholar.org/paper/1b47265245e8db53a553049dcb27ed3e495fd625\",\"venue\":\"CVPR 2009\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Y. Zhou\"},{\"authorId\":null,\"name\":\"Z. Wang\"},{\"authorId\":null,\"name\":\"C. Fang\"},{\"authorId\":null,\"name\":\"T. Bui\"},{\"authorId\":null,\"name\":\"T. L. Berg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Visual to sound: Generating natural sound for videos in the wild\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":\"1712.06651\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01246-5_27\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"dfc504536e8434eb008680343abb77010965169e\",\"title\":\"Objects that Sound\",\"url\":\"https://www.semanticscholar.org/paper/dfc504536e8434eb008680343abb77010965169e\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"}],\"doi\":\"10.1109/ICASSP.2013.6639215\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20969e35837d93b6e4eb52d75c8713abc6069f4a\",\"title\":\"Statistical parametric speech synthesis using deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/20969e35837d93b6e4eb52d75c8713abc6069f4a\",\"venue\":\"2013 IEEE International Conference on Acoustics, Speech and Signal Processing\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3116662\",\"name\":\"J. Gemmeke\"},{\"authorId\":\"1745455\",\"name\":\"D. Ellis\"},{\"authorId\":\"36794621\",\"name\":\"Dylan Freedman\"},{\"authorId\":\"35996413\",\"name\":\"A. Jansen\"},{\"authorId\":\"39965499\",\"name\":\"W. Lawrence\"},{\"authorId\":\"1974225\",\"name\":\"R. C. Moore\"},{\"authorId\":\"2114994\",\"name\":\"M. Plakal\"},{\"authorId\":\"39687627\",\"name\":\"M. Ritter\"}],\"doi\":\"10.1109/ICASSP.2017.7952261\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ba2218b708ca64ab556e39d5997202e012717d5\",\"title\":\"Audio Set: An ontology and human-labeled dataset for audio events\",\"url\":\"https://www.semanticscholar.org/paper/5ba2218b708ca64ab556e39d5997202e012717d5\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1612.07837\",\"authors\":[{\"authorId\":\"34719201\",\"name\":\"Soroush Mehri\"},{\"authorId\":\"145411463\",\"name\":\"K. Kumar\"},{\"authorId\":\"2708454\",\"name\":\"Ishaan Gulrajani\"},{\"authorId\":\"39458024\",\"name\":\"Rithesh Kumar\"},{\"authorId\":\"145342039\",\"name\":\"Shubham Jain\"},{\"authorId\":\"143778281\",\"name\":\"J. Sotelo\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e221e2c2ca8bd74a7b818406c8a2a342760e7d65\",\"title\":\"SampleRNN: An Unconditional End-to-End Neural Audio Generation Model\",\"url\":\"https://www.semanticscholar.org/paper/e221e2c2ca8bd74a7b818406c8a2a342760e7d65\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Kumar S. Mehri\"},{\"authorId\":null,\"name\":\"I. Gulrajani\"},{\"authorId\":null,\"name\":\"R. Kumar\"},{\"authorId\":null,\"name\":\"S. Jain\"},{\"authorId\":null,\"name\":\"J. Sotelo\"},{\"authorId\":null,\"name\":\"A. C. Courville\"},{\"authorId\":null,\"name\":\"Y. Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\": A method for stochastic optimization\",\"url\":\"\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34876459\",\"name\":\"A. Hunt\"},{\"authorId\":\"1690706\",\"name\":\"A. Black\"}],\"doi\":\"10.1109/ICASSP.1996.541110\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1dd0140d51e870a713340ae30734c8438b03d1a3\",\"title\":\"Unit selection in a concatenative speech synthesis system using a large speech database\",\"url\":\"https://www.semanticscholar.org/paper/1dd0140d51e870a713340ae30734c8438b03d1a3\",\"venue\":\"1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings\",\"year\":1996},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143778281\",\"name\":\"J. Sotelo\"},{\"authorId\":\"34719201\",\"name\":\"Soroush Mehri\"},{\"authorId\":\"145411463\",\"name\":\"K. Kumar\"},{\"authorId\":\"144660120\",\"name\":\"J. F. Santos\"},{\"authorId\":\"2182706\",\"name\":\"Kyle Kastner\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9203d6c076bffe87336f2ea91f5851436c02dbe6\",\"title\":\"Char2Wav: End-to-End Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/9203d6c076bffe87336f2ea91f5851436c02dbe6\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3232265\",\"name\":\"Deqing Sun\"},{\"authorId\":\"49863424\",\"name\":\"S. Roth\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1109/CVPR.2010.5539939\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a87428c6b2205240485ee6bb9cfb00fd9ed359c\",\"title\":\"Secrets of optical flow estimation and their principles\",\"url\":\"https://www.semanticscholar.org/paper/0a87428c6b2205240485ee6bb9cfb00fd9ed359c\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1608.07017\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-319-46448-0_48\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93a87dfa72f22fba14ef243a62c7d0a6906dfed7\",\"title\":\"Ambient Sound Provides Supervision for Visual Learning\",\"url\":\"https://www.semanticscholar.org/paper/93a87dfa72f22fba14ef243a62c7d0a6906dfed7\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1895488\",\"name\":\"David F. Harwath\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145898106\",\"name\":\"James R. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"87697847062e7eefec99a8c3aec888e3eef5e5b9\",\"title\":\"Unsupervised Learning of Spoken Language with Visual Context\",\"url\":\"https://www.semanticscholar.org/paper/87697847062e7eefec99a8c3aec888e3eef5e5b9\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1610.09001\",\"authors\":[{\"authorId\":\"3152281\",\"name\":\"Y. Aytar\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"title\":\"SoundNet: Learning Sound Representations from Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1609.03499\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"df0402517a7338ae28bc54acaac400de6b456a46\",\"title\":\"WaveNet: A Generative Model for Raw Audio\",\"url\":\"https://www.semanticscholar.org/paper/df0402517a7338ae28bc54acaac400de6b456a46\",\"venue\":\"SSW\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33082636\",\"name\":\"Takayoshi Yoshimura\"},{\"authorId\":\"1723069\",\"name\":\"K. Tokuda\"},{\"authorId\":\"2882083\",\"name\":\"T. Masuko\"},{\"authorId\":\"48648898\",\"name\":\"T. Kobayashi\"},{\"authorId\":\"1788590\",\"name\":\"T. Kitamura\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"505f9e5e4ea3711707ad0fe862401ef49215cb12\",\"title\":\"Simultaneous modeling of spectrum, pitch and duration in HMM-based speech synthesis\",\"url\":\"https://www.semanticscholar.org/paper/505f9e5e4ea3711707ad0fe862401ef49215cb12\",\"venue\":\"EUROSPEECH\",\"year\":1999},{\"arxivId\":\"1512.08512\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145358192\",\"name\":\"E. Adelson\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1109/CVPR.2016.264\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ac640c2d0f33fb3ab49f37b26982948fc31e3191\",\"title\":\"Visually Indicated Sounds\",\"url\":\"https://www.semanticscholar.org/paper/ac640c2d0f33fb3ab49f37b26982948fc31e3191\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"1723069\",\"name\":\"K. Tokuda\"},{\"authorId\":\"1690706\",\"name\":\"A. Black\"}],\"doi\":\"10.1016/j.specom.2009.04.004\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"317d2b6e97b878e77f8aad964575bcaadddb83cf\",\"title\":\"Statistical Parametric Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/317d2b6e97b878e77f8aad964575bcaadddb83cf\",\"venue\":\"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07\",\"year\":2007},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2717546\",\"name\":\"Zhoutong Zhang\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"12493779\",\"name\":\"Q. Li\"},{\"authorId\":\"25699206\",\"name\":\"Zhengjia Huang\"},{\"authorId\":\"50690309\",\"name\":\"James Traer\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1109/ICCV.2017.141\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0472a1e41e9ef5b8c619443328d360f88ca34362\",\"title\":\"Generative Modeling of Audible Shapes for Object Perception\",\"url\":\"https://www.semanticscholar.org/paper/0472a1e41e9ef5b8c619443328d360f88ca34362\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1804.01665\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-030-01219-9_3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c9ca16cb2337fd5948c7af28c29c156981250e8\",\"title\":\"Learning to Separate Object Sounds by Watching Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/5c9ca16cb2337fd5948c7af28c29c156981250e8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1704.08292\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"4037274\",\"name\":\"Sudhanshu Srivastava\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1145/3126686.3126723\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"27e8965cc9c166e9afee46e611039f0ce8263e51\",\"title\":\"Deep Cross-Modal Audio-Visual Generation\",\"url\":\"https://www.semanticscholar.org/paper/27e8965cc9c166e9afee46e611039f0ce8263e51\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1803.03849\",\"authors\":[{\"authorId\":\"40895287\",\"name\":\"Arda Senocak\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"3053231\",\"name\":\"J. Kim\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"}],\"doi\":\"10.1109/CVPR.2018.00458\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b91d738cd1f5d550c5b27f328e55308a0a73b2d2\",\"title\":\"Learning to Localize Sound Source in Visual Scenes\",\"url\":\"https://www.semanticscholar.org/paper/b91d738cd1f5d550c5b27f328e55308a0a73b2d2\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}],\"title\":\"Visual to Sound: Generating Natural Sound for Videos in the Wild\",\"topics\":[{\"topic\":\"Virtual reality\",\"topicId\":\"4462\",\"url\":\"https://www.semanticscholar.org/topic/4462\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Accessibility\",\"topicId\":\"3596\",\"url\":\"https://www.semanticscholar.org/topic/3596\"},{\"topic\":\"Pixel\",\"topicId\":\"4254\",\"url\":\"https://www.semanticscholar.org/topic/4254\"},{\"topic\":\"Synchronization (computer science)\",\"topicId\":\"8935\",\"url\":\"https://www.semanticscholar.org/topic/8935\"},{\"topic\":\"Waveform\",\"topicId\":\"1613\",\"url\":\"https://www.semanticscholar.org/topic/1613\"}],\"url\":\"https://www.semanticscholar.org/paper/f2d126e02401ec9f3c131eac423620529996df2f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"