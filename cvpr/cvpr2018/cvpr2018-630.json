"{\"abstract\":\"Despite the recent success of end-to-end learned representations, hand-crafted optical flow features are still widely used in video analysis tasks. To fill this gap, we propose TVNet, a novel end-to-end trainable neural network, to learn optical-flow-like features from data. TVNet subsumes a specific optical flow solver, the TV-L1 method, and is initialized by unfolding its optimization iterations as neural layers. TVNet can therefore be used directly without any extra learning. Moreover, it can be naturally concatenated with other task-specific networks to formulate an end-to-end architecture, thus making our method more efficient than current multi-stage approaches by avoiding the need to pre-compute and store features on disk. Finally, the parameters of the TVNet can be further fine-tuned by end-to-end training. This enables TVNet to learn richer and task-specific patterns beyond exact optical flow. Extensive experiments on two action recognition benchmarks verify the effectiveness of the proposed approach. Our TVNet achieves better accuracies than all compared methods, while being competitive with the fastest counterpart in terms of features extraction time.\",\"arxivId\":\"1804.00413\",\"authors\":[{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\",\"url\":\"https://www.semanticscholar.org/author/2548303\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\",\"url\":\"https://www.semanticscholar.org/author/2978255\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\",\"url\":\"https://www.semanticscholar.org/author/144158271\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\",\"url\":\"https://www.semanticscholar.org/author/2490652\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\",\"url\":\"https://www.semanticscholar.org/author/40206014\"},{\"authorId\":\"1768190\",\"name\":\"J. Huang\",\"url\":\"https://www.semanticscholar.org/author/1768190\"}],\"citationVelocity\":31,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"90724813\",\"name\":\"Y. Cheng\"},{\"authorId\":\"49461618\",\"name\":\"Guangya Li\"},{\"authorId\":\"145837602\",\"name\":\"N. Wong\"},{\"authorId\":\"50688512\",\"name\":\"Haibao Chen\"},{\"authorId\":\"50875432\",\"name\":\"Hao Yu\"}],\"doi\":\"10.1109/iccad45719.2019.8942052\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b66c09389da0b52d2af07807dd3719baf40816\",\"title\":\"DEEPEYE: A Deeply Tensor-Compressed Neural Network Hardware Accelerator: Invited Paper\",\"url\":\"https://www.semanticscholar.org/paper/c2b66c09389da0b52d2af07807dd3719baf40816\",\"venue\":\"2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2896701\",\"name\":\"G. An\"},{\"authorId\":\"4464686\",\"name\":\"ZhenXing Zheng\"},{\"authorId\":\"144953181\",\"name\":\"D. Wu\"},{\"authorId\":\"153168978\",\"name\":\"Wen Zhou\"}],\"doi\":\"10.1016/j.jvcir.2019.102650\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a22e41d8ac3c56072d5fe1136567d1b82d9bb46e\",\"title\":\"Deep spectral feature pyramid in the frequency domain for long-term action recognition\",\"url\":\"https://www.semanticscholar.org/paper/a22e41d8ac3c56072d5fe1136567d1b82d9bb46e\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2019},{\"arxivId\":\"1805.06374\",\"authors\":[{\"authorId\":\"8133623\",\"name\":\"Wanjia Liu\"},{\"authorId\":\"31385532\",\"name\":\"Huaijin Chen\"},{\"authorId\":\"46186660\",\"name\":\"R. Goel\"},{\"authorId\":\"35633657\",\"name\":\"Yuzhong Huang\"},{\"authorId\":\"145280967\",\"name\":\"A. Veeraraghavan\"},{\"authorId\":\"46463998\",\"name\":\"Ankit B. Patel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a0c37f07710184597befaa7e6cf2f0893ff440e9\",\"title\":\"Fast Retinomorphic Event Stream for Video Recognition and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a0c37f07710184597befaa7e6cf2f0893ff440e9\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143611428\",\"name\":\"Z. Tu\"},{\"authorId\":\"144561919\",\"name\":\"W. Xie\"},{\"authorId\":\"2581829\",\"name\":\"Dejun Zhang\"},{\"authorId\":\"1754666\",\"name\":\"R. Poppe\"},{\"authorId\":\"1739867\",\"name\":\"R. Veltkamp\"},{\"authorId\":\"2913552\",\"name\":\"Baoxin Li\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1016/J.IMAGE.2018.12.002\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2e66cb1f4140f93efd2e76a03bcda9fe7cd64c4\",\"title\":\"A survey of variational and CNN-based optical flow techniques\",\"url\":\"https://www.semanticscholar.org/paper/a2e66cb1f4140f93efd2e76a03bcda9fe7cd64c4\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"19188615\",\"name\":\"Ricardo Manh\\u00e3es Savii\"},{\"authorId\":\"34539976\",\"name\":\"S. Santos\"},{\"authorId\":\"23962586\",\"name\":\"J. Almeida\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"574aba95ba9718ab86a9c05f3728f50d1ce6ec41\",\"title\":\"GIBIS at MediaEval 2018: Predicting Media Memorability Task\",\"url\":\"https://www.semanticscholar.org/paper/574aba95ba9718ab86a9c05f3728f50d1ce6ec41\",\"venue\":\"MediaEval\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47120363\",\"name\":\"X. Wang\"},{\"authorId\":\"3316344\",\"name\":\"Junsan Zhang\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"},{\"authorId\":\"144019071\",\"name\":\"Philip S. Yu\"},{\"authorId\":\"47055140\",\"name\":\"J. Zhu\"},{\"authorId\":\"46382188\",\"name\":\"H. Li\"}],\"doi\":\"10.1145/3357384.3357935\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"51e4e7b322c53dac73048fa5bd5dd1c5145b2d82\",\"title\":\"Video-level Multi-model Fusion for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/51e4e7b322c53dac73048fa5bd5dd1c5145b2d82\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145491338\",\"name\":\"S. Palacio\"},{\"authorId\":\"1824088\",\"name\":\"V. Campos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ffd9e745e58f9dc876af7f68e9ef5b9d56d6461\",\"title\":\"VIDEO UNDERSTANDING THROUGH THE DISENTANGLEMENT OF APPEARANCE AND MOTION\",\"url\":\"https://www.semanticscholar.org/paper/7ffd9e745e58f9dc876af7f68e9ef5b9d56d6461\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"1390826784\",\"name\":\"Rui-Qi Wang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/TIP.2020.2987425\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"174168393288dbb61a6ec4c674a0298e489b9e4d\",\"title\":\"Confidence-Guided Self Refinement for Action Prediction in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/174168393288dbb61a6ec4c674a0298e489b9e4d\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1527097295\",\"name\":\"Yue Li\"},{\"authorId\":\"31399226\",\"name\":\"Wenrui Ding\"},{\"authorId\":\"2563751\",\"name\":\"Y. Zhu\"},{\"authorId\":\"46843692\",\"name\":\"Yuanjun Huang\"},{\"authorId\":\"9325297\",\"name\":\"Yalong Jiang\"},{\"authorId\":\"46242227\",\"name\":\"B. Zhang\"}],\"doi\":\"10.1109/ICIP40778.2020.9191306\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a3f7e4074a356474113cdb6cb4c7ca7cb3e097d5\",\"title\":\"Cam-Net: Compressed Attentive Multi-Granularity Network For Dynamic Scene Classification\",\"url\":\"https://www.semanticscholar.org/paper/a3f7e4074a356474113cdb6cb4c7ca7cb3e097d5\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":\"1911.11319\",\"authors\":[{\"authorId\":\"1384480816\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"120026268\",\"name\":\"Yao Zhou\"},{\"authorId\":null,\"name\":\"Yu Lu\"},{\"authorId\":\"1726357\",\"name\":\"Wayne Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b5829a65c4eb19ec8922a1575c6900d5f94046f\",\"title\":\"Learning Efficient Video Representation with Video Shuffle Networks\",\"url\":\"https://www.semanticscholar.org/paper/3b5829a65c4eb19ec8922a1575c6900d5f94046f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.09933\",\"authors\":[{\"authorId\":\"30557120\",\"name\":\"Heeseung Kwon\"},{\"authorId\":\"16142867\",\"name\":\"Manjin Kim\"},{\"authorId\":\"2483916\",\"name\":\"Suha Kwak\"},{\"authorId\":\"72643925\",\"name\":\"Minsu Cho\"}],\"doi\":\"10.1007/978-3-030-58517-4_21\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b5be8a78db1631159500e7cee249729820e355b2\",\"title\":\"MotionSqueeze: Neural Motion Feature Learning for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b5be8a78db1631159500e7cee249729820e355b2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wenming Cao\"},{\"authorId\":\"78064961\",\"name\":\"Y. Li\"},{\"authorId\":\"48427549\",\"name\":\"Z. He\"}],\"doi\":\"10.1109/ACCESS.2019.2944649\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"970ff5912facc672ceb846adce2c5d9e42a902b9\",\"title\":\"Weighted Optical Flow Prediction and Attention Model for Object Tracking\",\"url\":\"https://www.semanticscholar.org/paper/970ff5912facc672ceb846adce2c5d9e42a902b9\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2257769\",\"name\":\"Xintong Han\"},{\"authorId\":\"49015548\",\"name\":\"Weilin Huang\"},{\"authorId\":\"153004038\",\"name\":\"X. Hu\"},{\"authorId\":\"1915350\",\"name\":\"M. Scott\"}],\"doi\":\"10.1109/ICCV.2019.01057\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ce718a70af2b0ad234693ee4f25131afae62c19\",\"title\":\"ClothFlow: A Flow-Based Model for Clothed Person Generation\",\"url\":\"https://www.semanticscholar.org/paper/4ce718a70af2b0ad234693ee4f25131afae62c19\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145586191\",\"name\":\"Can Zhang\"},{\"authorId\":\"26981150\",\"name\":\"Yue-Xian Zou\"},{\"authorId\":\"115151196\",\"name\":\"G. Chen\"},{\"authorId\":\"48204311\",\"name\":\"L. Gan\"}],\"doi\":\"10.1145/3343031.3350876\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ab7f7501d889607d6a9aa3f5ea465a8b1c44f7ff\",\"title\":\"PAN: Persistent Appearance Network with an Efficient Motion Cue for Fast Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ab7f7501d889607d6a9aa3f5ea465a8b1c44f7ff\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"8277017\",\"name\":\"Q. Wu\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1109/TIP.2019.2922108\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff4f13d4973cfe74f8ac7ef8384548c22284011e\",\"title\":\"Breaking Winner-Takes-All: Iterative-Winners-Out Networks for Weakly Supervised Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/ff4f13d4973cfe74f8ac7ef8384548c22284011e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"2004.04730\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"}],\"doi\":\"10.1109/cvpr42600.2020.00028\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"908cca0abefc35acc38033603714fbb1bcadc49d\",\"title\":\"X3D: Expanding Architectures for Efficient Video Recognition\",\"url\":\"https://www.semanticscholar.org/paper/908cca0abefc35acc38033603714fbb1bcadc49d\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1906.03349\",\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"}],\"doi\":\"10.1109/cvpr42600.2020.00043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45a924794b2c6501b671cf4249c6ac4fa1671597\",\"title\":\"Video Modeling With Correlation Networks\",\"url\":\"https://www.semanticscholar.org/paper/45a924794b2c6501b671cf4249c6ac4fa1671597\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2011.12619\",\"authors\":[{\"authorId\":\"147084112\",\"name\":\"Jack Humphreys\"},{\"authorId\":\"48354826\",\"name\":\"Z. Chen\"},{\"authorId\":\"145047838\",\"name\":\"Dacheng Tao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e88bff50c417703239e0a832fddb267196ab5a99\",\"title\":\"Recent Progress in Appearance-based Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/e88bff50c417703239e0a832fddb267196ab5a99\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.03249\",\"authors\":[{\"authorId\":\"1720749879\",\"name\":\"Miao Liu\"},{\"authorId\":\"40897068\",\"name\":\"Xin Chen\"},{\"authorId\":\"23614019\",\"name\":\"Y. Zhang\"},{\"authorId\":\"47002659\",\"name\":\"Yanchao Li\"},{\"authorId\":\"50779871\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"76db87564c7e6a6f417fca41b9f659a879de5027\",\"title\":\"Attention Distillation for Learning Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/76db87564c7e6a6f417fca41b9f659a879de5027\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50313556\",\"name\":\"Y. Fang\"},{\"authorId\":\"3283437\",\"name\":\"Zhihao Ma\"},{\"authorId\":\"51450622\",\"name\":\"Hao Zheng\"},{\"authorId\":\"29790429\",\"name\":\"Wanting Ji\"}],\"doi\":\"10.1007/s00521-020-05146-5\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"7a5c1bdefb276e500bea25ecf894d5fc46b404aa\",\"title\":\"Trainable TV-L1 model as recurrent nets for low-level vision\",\"url\":\"https://www.semanticscholar.org/paper/7a5c1bdefb276e500bea25ecf894d5fc46b404aa\",\"venue\":\"Neural Comput. Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31586818\",\"name\":\"H. G. Chen\"},{\"authorId\":\"8133623\",\"name\":\"Wanjia Liu\"},{\"authorId\":\"46186660\",\"name\":\"R. Goel\"},{\"authorId\":\"2492444\",\"name\":\"Rhonald C. Lua\"},{\"authorId\":\"47732006\",\"name\":\"S. Mittal\"},{\"authorId\":\"35633657\",\"name\":\"Yuzhong Huang\"},{\"authorId\":\"145280967\",\"name\":\"A. Veeraraghavan\"},{\"authorId\":\"46463998\",\"name\":\"Ankit B. Patel\"}],\"doi\":\"10.1109/TCI.2019.2948755\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"7377982e58702e0f44beff2c00ab2d32825cf557\",\"title\":\"Fast Retinomorphic Event-Driven Representations for Video Gameplay and Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/7377982e58702e0f44beff2c00ab2d32825cf557\",\"venue\":\"IEEE Transactions on Computational Imaging\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145873652\",\"name\":\"Y. Kong\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"46956675\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TPAMI.2018.2882805\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"02c293bc06c580305c8b62a8ed90f37a75608493\",\"title\":\"Adversarial Action Prediction Networks\",\"url\":\"https://www.semanticscholar.org/paper/02c293bc06c580305c8b62a8ed90f37a75608493\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1909.03252\",\"authors\":[{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"145913039\",\"name\":\"Y. Rong\"},{\"authorId\":\"144259957\",\"name\":\"P. Zhao\"},{\"authorId\":\"1768190\",\"name\":\"J. Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1109/ICCV.2019.00719\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd8e725159159ca2169d302f6cb510e3b1cc1a4b\",\"title\":\"Graph Convolutional Networks for Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/fd8e725159159ca2169d302f6cb510e3b1cc1a4b\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"},{\"authorId\":\"145952554\",\"name\":\"Michael Chan\"},{\"authorId\":\"28130078\",\"name\":\"P. P. Liang\"},{\"authorId\":\"31726556\",\"name\":\"Edmund Tong\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/CVPR.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"title\":\"Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47933134\",\"name\":\"X. Huang\"},{\"authorId\":\"52317130\",\"name\":\"F. P\\u00e9rez\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"1765951\",\"name\":\"Maksims Volkovs\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e00484961fb2f30d2d48a5f9853fa3ebab140cac\",\"title\":\"Improving Transformer Optimization Through Better Initialization\",\"url\":\"https://www.semanticscholar.org/paper/e00484961fb2f30d2d48a5f9853fa3ebab140cac\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2195345\",\"name\":\"Zheng Shou\"}],\"doi\":\"10.7916/D8-SRKZ-T696\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ea5b4e02a448174cbc6872eaec27695b04e9b87\",\"title\":\"Deep Learning for Action Understanding in Video\",\"url\":\"https://www.semanticscholar.org/paper/3ea5b4e02a448174cbc6872eaec27695b04e9b87\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1912.04462\",\"authors\":[{\"authorId\":\"3264239\",\"name\":\"Shi-Yuan Huang\"},{\"authorId\":\"1821514\",\"name\":\"Xu-Dong Lin\"},{\"authorId\":\"35862299\",\"name\":\"S. Karaman\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c801bcaa007c6bf6ce4027bdb45f88f7a861db3\",\"title\":\"Flow-Distilled IP Two-Stream Networks for Compressed Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1c801bcaa007c6bf6ce4027bdb45f88f7a861db3\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50999137\",\"name\":\"M. R. Souza\"},{\"authorId\":\"8125221\",\"name\":\"H. Maia\"},{\"authorId\":\"3052490\",\"name\":\"M. Vieira\"},{\"authorId\":\"1398585275\",\"name\":\"H. Pedrini\"}],\"doi\":\"10.1016/j.neucom.2020.04.035\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"609651891f7605789894397f7d5c6f78b6dba2df\",\"title\":\"Survey on visual rhythms: A spatio-temporal representation for video sequences\",\"url\":\"https://www.semanticscholar.org/paper/609651891f7605789894397f7d5c6f78b6dba2df\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1909.09300\",\"authors\":[{\"authorId\":\"3928052\",\"name\":\"T. Li\"},{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"2931940\",\"name\":\"M. Zhao\"},{\"authorId\":\"51149370\",\"name\":\"Yingcheng Liu\"},{\"authorId\":\"1785714\",\"name\":\"D. Katabi\"}],\"doi\":\"10.1109/ICCV.2019.00096\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02ec4b311978f8a16ae4ecd232e2464c08e59d84\",\"title\":\"Making the Invisible Visible: Action Recognition Through Walls and Occlusions\",\"url\":\"https://www.semanticscholar.org/paper/02ec4b311978f8a16ae4ecd232e2464c08e59d84\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46673205\",\"name\":\"Zibin Guo\"},{\"authorId\":\"21542283\",\"name\":\"Mingkun Yang\"},{\"authorId\":\"1515536715\",\"name\":\"Ninghao Chen\"},{\"authorId\":\"2741851\",\"name\":\"Zhuoling Xiao\"},{\"authorId\":\"49225415\",\"name\":\"Bo Yan\"},{\"authorId\":\"2712253\",\"name\":\"Shuisheng Lin\"},{\"authorId\":\"48207276\",\"name\":\"L. Zhou\"}],\"doi\":\"10.1109/GLOBECOM38437.2019.9013757\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"34b28293b63161b145e3152690f5560e4bc2c365\",\"title\":\"LightVO: Lightweight Inertial-Assisted Monocular Visual Odometry with Dense Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/34b28293b63161b145e3152690f5560e4bc2c365\",\"venue\":\"2019 IEEE Global Communications Conference (GLOBECOM)\",\"year\":2019},{\"arxivId\":\"1908.09442\",\"authors\":[{\"authorId\":\"47057388\",\"name\":\"X. Li\"},{\"authorId\":\"6873935\",\"name\":\"T. Lin\"},{\"authorId\":\"153201747\",\"name\":\"Xiao Liu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1724520\",\"name\":\"W. Zuo\"},{\"authorId\":\"46651287\",\"name\":\"C. Li\"},{\"authorId\":\"46550771\",\"name\":\"X. Long\"},{\"authorId\":\"2192303\",\"name\":\"D. He\"},{\"authorId\":\"32379958\",\"name\":\"Fu Li\"},{\"authorId\":\"2671368\",\"name\":\"Shilei Wen\"}],\"doi\":\"10.1145/3394171.3413860\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"317e0392d2a830df88dd093df01ef4d2943e5c96\",\"title\":\"Deep Concept-wise Temporal Convolutional Networks for Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/317e0392d2a830df88dd093df01ef4d2943e5c96\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.07949\",\"authors\":[{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"2192303\",\"name\":\"D. He\"},{\"authorId\":\"46550737\",\"name\":\"Xiang Long\"},{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"2671368\",\"name\":\"Shilei Wen\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2336388131b3cb41eb44e927aeac10a1dabbedad\",\"title\":\"RSPNet: Relative Speed Perception for Unsupervised Video Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/2336388131b3cb41eb44e927aeac10a1dabbedad\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"90724813\",\"name\":\"Y. Cheng\"},{\"authorId\":\"71300641\",\"name\":\"G. Li\"},{\"authorId\":\"1873081\",\"name\":\"N. Wong\"},{\"authorId\":\"50688512\",\"name\":\"Haibao Chen\"},{\"authorId\":\"37629830\",\"name\":\"H. Yu\"}],\"doi\":\"10.1145/3381805\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac1961d8096e171b840c1cb8d5f7b299bcdf9d2f\",\"title\":\"DEEPEYE: A Deeply Tensor-Compressed Neural Network for Video Comprehension on Terminal Devices\",\"url\":\"https://www.semanticscholar.org/paper/ac1961d8096e171b840c1cb8d5f7b299bcdf9d2f\",\"venue\":\"ACM Trans. Embed. Comput. Syst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46331912\",\"name\":\"M. Liu\"},{\"authorId\":\"89507637\",\"name\":\"X. Chen\"},{\"authorId\":\"144781413\",\"name\":\"Y. Zhang\"},{\"authorId\":\"48513361\",\"name\":\"Y. Li\"},{\"authorId\":\"144177248\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"dc7de1c65a52db271016313980ae577d19aace24\",\"title\":\"Paying More Attention to Motion: Attention Distillation for Learning Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/dc7de1c65a52db271016313980ae577d19aace24\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47045743\",\"name\":\"Yili Qu\"},{\"authorId\":\"83156561\",\"name\":\"Yaobin Ke\"},{\"authorId\":null,\"name\":\"Wei Yu\"}],\"doi\":\"10.1109/ICSESS.2018.8663724\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"38ebb0049603c3b942d3ef8777240bd060f5c614\",\"title\":\"A Solution for Input Limit in CNN Due to Fully-Connected Layer\",\"url\":\"https://www.semanticscholar.org/paper/38ebb0049603c3b942d3ef8777240bd060f5c614\",\"venue\":\"2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31595791\",\"name\":\"J. Li\"},{\"authorId\":\"6820648\",\"name\":\"Xianglong Liu\"},{\"authorId\":\"150341144\",\"name\":\"Wenxuan Zhang\"},{\"authorId\":\"47473953\",\"name\":\"Mingyuan Zhang\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1109/TMM.2020.2965434\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ea173f972d39b8631e9ac8fe59a947c70ba31e3\",\"title\":\"Spatio-Temporal Attention Networks for Action Recognition and Detection\",\"url\":\"https://www.semanticscholar.org/paper/2ea173f972d39b8631e9ac8fe59a947c70ba31e3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1908.02486\",\"authors\":[{\"authorId\":\"51128181\",\"name\":\"Boyuan Jiang\"},{\"authorId\":\"47446949\",\"name\":\"M. Wang\"},{\"authorId\":\"35893447\",\"name\":\"W. Gan\"},{\"authorId\":\"145717890\",\"name\":\"W. Wu\"},{\"authorId\":\"1721677\",\"name\":\"J. Yan\"}],\"doi\":\"10.1109/ICCV.2019.00209\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b812b20192ffac37b03bde0261934a2a8c7fdf47\",\"title\":\"STM: SpatioTemporal and Motion Encoding for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/b812b20192ffac37b03bde0261934a2a8c7fdf47\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121339452\",\"name\":\"Y. Tian\"},{\"authorId\":\"1810382\",\"name\":\"Zhaohui Che\"},{\"authorId\":\"116279011\",\"name\":\"Wenbo Bao\"},{\"authorId\":\"1509899757\",\"name\":\"Guangtao Zhai\"},{\"authorId\":\"97709070\",\"name\":\"Z. Gao\"}],\"doi\":\"10.1007/978-3-030-58568-6_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"27568c821ab008facaa57dc4f99217a294e196c4\",\"title\":\"Self-supervised Motion Representation via Scattering Local Motion Cues\",\"url\":\"https://www.semanticscholar.org/paper/27568c821ab008facaa57dc4f99217a294e196c4\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2008.08072\",\"authors\":[{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"},{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"3462309\",\"name\":\"Juhana Kangaspunta\"},{\"authorId\":\"2148510\",\"name\":\"A. Angelova\"}],\"doi\":\"10.1007/978-3-030-58565-5_39\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a156594c076a8f0e073e5656ae8e3311212d2422\",\"title\":\"AssembleNet++: Assembling Modality Representations via Attention Connections\",\"url\":\"https://www.semanticscholar.org/paper/a156594c076a8f0e073e5656ae8e3311212d2422\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.06149\",\"authors\":[{\"authorId\":\"2283619\",\"name\":\"Peisen Zhao\"},{\"authorId\":\"3041937\",\"name\":\"Lingxi Xie\"},{\"authorId\":\"49890039\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/tmm.2020.3025665\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c1fc76cc67ad82df1f7695b4d9e0b7536a70732\",\"title\":\"Universal-to-Specific Framework for Complex Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6c1fc76cc67ad82df1f7695b4d9e0b7536a70732\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34413657\",\"name\":\"G. Shen\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"1768190\",\"name\":\"J. Huang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1145/3343031.3350981\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"42733f865df60e735014097f7136cb13bfda0351\",\"title\":\"Facial Image-to-Video Translation by a Hidden Affine Transformation\",\"url\":\"https://www.semanticscholar.org/paper/42733f865df60e735014097f7136cb13bfda0351\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4464686\",\"name\":\"ZhenXing Zheng\"},{\"authorId\":\"2896701\",\"name\":\"G. An\"},{\"authorId\":\"144953181\",\"name\":\"D. Wu\"},{\"authorId\":\"144695333\",\"name\":\"Q. Ruan\"}],\"doi\":\"10.1016/J.NEUCOM.2019.05.058\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"67d3ec7b1070405575974e449cfdc495b43f7b21\",\"title\":\"Spatial-temporal pyramid based Convolutional Neural Network for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/67d3ec7b1070405575974e449cfdc495b43f7b21\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1844956391\",\"name\":\"Ling Zhou\"},{\"authorId\":\"47403609\",\"name\":\"Xiuyan Shao\"},{\"authorId\":\"151423308\",\"name\":\"Qirong Mao\"}],\"doi\":\"10.1016/j.imavis.2020.104043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f5decc82a4122f2258256cceeeb4b9211e3ad4e0\",\"title\":\"A survey of micro-expression recognition\",\"url\":\"https://www.semanticscholar.org/paper/f5decc82a4122f2258256cceeeb4b9211e3ad4e0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49741227\",\"name\":\"Haifeng Sang\"},{\"authorId\":\"152254334\",\"name\":\"Z. Zhao\"},{\"authorId\":\"3030181\",\"name\":\"Dakuo He\"}],\"doi\":\"10.1109/ACCESS.2019.2936628\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"267dce4f4c6102e24eeac34d4315de7e7d25b538\",\"title\":\"Two-Level Attention Model Based Video Action Recognition Network\",\"url\":\"https://www.semanticscholar.org/paper/267dce4f4c6102e24eeac34d4315de7e7d25b538\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27046089\",\"name\":\"D. Li\"},{\"authorId\":\"153189209\",\"name\":\"Tingting Jiang\"},{\"authorId\":\"144889891\",\"name\":\"M. Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a0c52da5de27eff0f5a0dad74953ea5357ee70f8\",\"title\":\"uality Assessment of Inthe-Wild Videos\",\"url\":\"https://www.semanticscholar.org/paper/a0c52da5de27eff0f5a0dad74953ea5357ee70f8\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50313556\",\"name\":\"Y. Fang\"},{\"authorId\":\"49951082\",\"name\":\"Haiyan Fan\"},{\"authorId\":\"41191188\",\"name\":\"Lin Sun\"},{\"authorId\":\"2985328\",\"name\":\"Yulan Guo\"},{\"authorId\":\"3283437\",\"name\":\"Zhihao Ma\"}],\"doi\":\"10.1109/ICASSP.2019.8683012\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"84ba880ee7997a7e7cc0887e8ccc781ea45c3de1\",\"title\":\"From TV-L1 to Gated Recurrent Nets\",\"url\":\"https://www.semanticscholar.org/paper/84ba880ee7997a7e7cc0887e8ccc781ea45c3de1\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35654996\",\"name\":\"B. Pan\"},{\"authorId\":\"2282025\",\"name\":\"Jiankai Sun\"},{\"authorId\":\"35992009\",\"name\":\"Wuwei Lin\"},{\"authorId\":\"48170350\",\"name\":\"Limin Wang\"},{\"authorId\":\"8131625\",\"name\":\"W. Lin\"}],\"doi\":\"10.1109/CVPRW.2019.00059\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3e9285552fc3c1402a09e3d29ee09c130cf2d419\",\"title\":\"Cross-Stream Selective Networks for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/3e9285552fc3c1402a09e3d29ee09c130cf2d419\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":\"1908.00375\",\"authors\":[{\"authorId\":\"27046089\",\"name\":\"D. Li\"},{\"authorId\":\"40570471\",\"name\":\"T. Jiang\"},{\"authorId\":\"144889895\",\"name\":\"Ming Jiang\"}],\"doi\":\"10.1145/3343031.3351028\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"98ed747e424b7945bb9563dad73afcad1ebb9b36\",\"title\":\"Quality Assessment of In-the-Wild Videos\",\"url\":\"https://www.semanticscholar.org/paper/98ed747e424b7945bb9563dad73afcad1ebb9b36\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27046089\",\"name\":\"Dingquan Li\"},{\"authorId\":\"40570471\",\"name\":\"Tingting Jiang\"},{\"authorId\":\"144889895\",\"name\":\"Ming Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2eebdd917d167aefcb7384ab8cd62599588662ed\",\"title\":\"Quality Assessment of Inthe-Wild Videos\",\"url\":\"https://www.semanticscholar.org/paper/2eebdd917d167aefcb7384ab8cd62599588662ed\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1707.07857\",\"authors\":[{\"authorId\":\"1739194311\",\"name\":\"Zixuan Chen\"},{\"authorId\":\"1791718\",\"name\":\"Chun-Chao Guo\"},{\"authorId\":\"66117656\",\"name\":\"J. Lai\"},{\"authorId\":\"46397019\",\"name\":\"X. Xie\"}],\"doi\":\"10.1109/TCSVT.2019.2908779\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"597f870b32649ea02c0dcd2be9f91c3950502086\",\"title\":\"Motion-Appearance Interactive Encoding for Object Segmentation in Unconstrained Videos\",\"url\":\"https://www.semanticscholar.org/paper/597f870b32649ea02c0dcd2be9f91c3950502086\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10029285\",\"name\":\"Nieves Crasto\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"47e1b171fab52368f14f41955cdc7ca7775ded58\",\"title\":\"RGB TVL 1 Flow RGB + TVL 1 FlowMARS MARS + RGB MERS MERS + RGB Accuracy vs Time on MiniKinetics\",\"url\":\"https://www.semanticscholar.org/paper/47e1b171fab52368f14f41955cdc7ca7775ded58\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.03462\",\"authors\":[{\"authorId\":\"145586191\",\"name\":\"Can Zhang\"},{\"authorId\":\"26981150\",\"name\":\"Yue-Xian Zou\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"48204311\",\"name\":\"L. Gan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60840dce8073545641198c297796902fa390c719\",\"title\":\"PAN: Towards Fast Action Recognition via Learning Persistence of Appearance\",\"url\":\"https://www.semanticscholar.org/paper/60840dce8073545641198c297796902fa390c719\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71663535\",\"name\":\"Shichao Jin\"},{\"authorId\":\"6824339\",\"name\":\"Yanjun Su\"},{\"authorId\":\"145369990\",\"name\":\"Shang Gao\"},{\"authorId\":\"7363734\",\"name\":\"Fangfang Wu\"},{\"authorId\":\"48939639\",\"name\":\"Qin Ma\"},{\"authorId\":\"119709554\",\"name\":\"K. Xu\"},{\"authorId\":\"48939639\",\"name\":\"Qin Ma\"},{\"authorId\":\"51129003\",\"name\":\"Tianyu Hu\"},{\"authorId\":\"153240364\",\"name\":\"J. Liu\"},{\"authorId\":\"17696878\",\"name\":\"Shuxin Pang\"},{\"authorId\":\"32546194\",\"name\":\"Hongcan Guan\"},{\"authorId\":\"47539691\",\"name\":\"Jing Zhang\"},{\"authorId\":\"153683053\",\"name\":\"Q. Guo\"}],\"doi\":\"10.1109/TGRS.2019.2953092\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"30347958768159ecca3b6e0f560e2d89b8779129\",\"title\":\"Separating the Structural Components of Maize for Field Phenotyping Using Terrestrial LiDAR Data and Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/30347958768159ecca3b6e0f560e2d89b8779129\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145530174\",\"name\":\"Li Du\"},{\"authorId\":\"51303461\",\"name\":\"Zixuan Wang\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"},{\"authorId\":\"1741668\",\"name\":\"Zhicheng Zhao\"},{\"authorId\":\"144310030\",\"name\":\"F. Su\"},{\"authorId\":\"3268756\",\"name\":\"B. Zhuang\"},{\"authorId\":\"47195274\",\"name\":\"N. Boulgouris\"}],\"doi\":\"10.1109/TVT.2019.2905598\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c06c698de5ad3beadcc326adcdcf477253c0a44a\",\"title\":\"Adaptive Visual Interaction Based Multi-Target Future State Prediction For Autonomous Driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/c06c698de5ad3beadcc326adcdcf477253c0a44a\",\"venue\":\"IEEE Transactions on Vehicular Technology\",\"year\":2019},{\"arxivId\":\"1810.01455\",\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":\"10.1109/CVPR.2019.01018\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7d8be0302b754c903d96fd056c20207ad90ab4e6\",\"title\":\"Representation Flow for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/7d8be0302b754c903d96fd056c20207ad90ab4e6\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46276482\",\"name\":\"J. Li\"},{\"authorId\":\"145708711\",\"name\":\"Ping Wei\"},{\"authorId\":\"1978363545\",\"name\":\"Yongchi Zhang\"},{\"authorId\":\"153873673\",\"name\":\"N. Zheng\"}],\"doi\":\"10.1145/3394171.3413641\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05444a5a49f717dc199957b66e9c472219171f88\",\"title\":\"A Slow-I-Fast-P Architecture for Compressed Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/05444a5a49f717dc199957b66e9c472219171f88\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1908.00497\",\"authors\":[{\"authorId\":\"145294961\",\"name\":\"L. Chi\"},{\"authorId\":\"9443552\",\"name\":\"G. Tian\"},{\"authorId\":\"145353089\",\"name\":\"Y. Mu\"},{\"authorId\":\"144876834\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/ICCVW.2019.00552\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5769318fd67d1104e561b7382b305b5ca810d6d2\",\"title\":\"Two-Stream Video Classification with Cross-Modality Attention\",\"url\":\"https://www.semanticscholar.org/paper/5769318fd67d1104e561b7382b305b5ca810d6d2\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145309464\",\"name\":\"Min Jiang\"},{\"authorId\":\"51010977\",\"name\":\"N. Pan\"},{\"authorId\":\"1644912978\",\"name\":\"Jun Kong\"}],\"doi\":\"10.1016/j.jvcir.2020.102846\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1378199d4242d3d8699bf79d13cfcbbdefea9120\",\"title\":\"Spatial-temporal saliency action mask attention network for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/1378199d4242d3d8699bf79d13cfcbbdefea9120\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"1912.00381\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":\"10.1109/cvpr42600.2020.00118\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"57cee868188127305f966a178ca22025b397d911\",\"title\":\"Gate-Shift Networks for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/57cee868188127305f966a178ca22025b397d911\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10029285\",\"name\":\"Nieves Crasto\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/CVPR.2019.00807\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e93bd99350bf03acf870f83bb9c7ab1a7d17147\",\"title\":\"MARS: Motion-Augmented RGB Stream for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6e93bd99350bf03acf870f83bb9c7ab1a7d17147\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1909.13130\",\"authors\":[{\"authorId\":\"153918891\",\"name\":\"Chenxu Luo\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/ICCV.2019.00561\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8b8fe4727c8094b17e61886e69a602f8d0403091\",\"title\":\"Grouped Spatial-Temporal Aggregation for Efficient Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/8b8fe4727c8094b17e61886e69a602f8d0403091\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1901.03460\",\"authors\":[{\"authorId\":\"73423138\",\"name\":\"Zheng Shou\"},{\"authorId\":\"3305169\",\"name\":\"Zhicheng Yan\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48030192\",\"name\":\"Xudong Lin\"},{\"authorId\":\"70351911\",\"name\":\"Shih-Fu Chang\"}],\"doi\":\"10.1109/CVPR.2019.00136\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d878aac73038c3bc175ccc2b93acc04675f33bbd\",\"title\":\"DMC-Net: Generating Discriminative Motion Cues for Fast Compressed Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d878aac73038c3bc175ccc2b93acc04675f33bbd\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2010.08263\",\"authors\":[{\"authorId\":\"49602505\",\"name\":\"Xing Yan\"},{\"authorId\":\"47527753\",\"name\":\"Weizhong Zhang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"143604502\",\"name\":\"Q. Wu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a0a10d9c94e9c849d9004bdb077fb3274c82c6e\",\"title\":\"Parsimonious Quantile Regression of Financial Asset Tail Dynamics via Sequential Learning\",\"url\":\"https://www.semanticscholar.org/paper/2a0a10d9c94e9c849d9004bdb077fb3274c82c6e\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"2007.13098\",\"authors\":[{\"authorId\":\"151230204\",\"name\":\"Hongtao Yang\"},{\"authorId\":\"49104770\",\"name\":\"T. Zhang\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"33913193\",\"name\":\"Xuming He\"}],\"doi\":\"10.1145/3422852.3423480\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c2d5620d12d3c9a6620200185fd0206ff51e2a6b\",\"title\":\"Towards Purely Unsupervised Disentanglement of Appearance and Shape for Person Images Generation\",\"url\":\"https://www.semanticscholar.org/paper/c2d5620d12d3c9a6620200185fd0206ff51e2a6b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1809.05571\",\"authors\":[{\"authorId\":\"3232265\",\"name\":\"Deqing Sun\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/TPAMI.2019.2894353\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bc294b251085b76f166c2ec16c3ee77f02800ec0\",\"title\":\"Models Matter, So Does Training: An Empirical Study of CNNs for Optical Flow Estimation\",\"url\":\"https://www.semanticscholar.org/paper/bc294b251085b76f166c2ec16c3ee77f02800ec0\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1907.03083\",\"authors\":[{\"authorId\":\"34469457\",\"name\":\"R. Liu\"},{\"authorId\":\"50709517\",\"name\":\"L. Ma\"},{\"authorId\":\"145802386\",\"name\":\"Xiaoming Yuan\"},{\"authorId\":\"29785837\",\"name\":\"Shangzhi Zeng\"},{\"authorId\":\"90652528\",\"name\":\"J. Zhang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"72dda744db8cbd17dc8691078c44ddad9fa7a243\",\"title\":\"Bilevel Integrative Optimization for Ill-posed Inverse Problems\",\"url\":\"https://www.semanticscholar.org/paper/72dda744db8cbd17dc8691078c44ddad9fa7a243\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39893135\",\"name\":\"M. Yu\"},{\"authorId\":\"1712773\",\"name\":\"Weizhe Zhang\"},{\"authorId\":\"48411615\",\"name\":\"Qingxiang Zeng\"},{\"authorId\":\"47074418\",\"name\":\"C. Wang\"},{\"authorId\":\"38158055\",\"name\":\"J. Li\"}],\"doi\":\"10.1109/ICAIIC.2019.8669069\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad0dba3713a1210f0b89c66c25e93b22e5e04d9e\",\"title\":\"Human-Object Contour for Action Recognition with Attentional Multi-modal Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/ad0dba3713a1210f0b89c66c25e93b22e5e04d9e\",\"venue\":\"2019 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)\",\"year\":2019},{\"arxivId\":\"2012.06567\",\"authors\":[{\"authorId\":\"1805946041\",\"name\":\"Yi Zhu\"},{\"authorId\":\"21657733\",\"name\":\"X. Li\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"22539483\",\"name\":\"Chongruo Wu\"},{\"authorId\":\"152781163\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2397422\",\"name\":\"Joseph Tighe\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"49140510\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"title\":\"A Comprehensive Study of Deep Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48186677\",\"name\":\"Yuxiang Sun\"},{\"authorId\":\"48613185\",\"name\":\"Weixun Zuo\"},{\"authorId\":\"100520046\",\"name\":\"M. Liu\"}],\"doi\":\"10.1109/LRA.2020.2975414\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ed93fa4f32eb4f3bb27768303756b36b2fc4c19\",\"title\":\"See the Future: A Semantic Segmentation Network Predicting Ego-Vehicle Trajectory With a Single Monocular Camera\",\"url\":\"https://www.semanticscholar.org/paper/4ed93fa4f32eb4f3bb27768303756b36b2fc4c19\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":\"2003.12039\",\"authors\":[{\"authorId\":\"8048414\",\"name\":\"Zachary Teed\"},{\"authorId\":\"145820819\",\"name\":\"Jun Deng\"}],\"doi\":\"10.1007/978-3-030-58536-5_24\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9186aaea568f616c466f0648a6bc913461c3cc7\",\"title\":\"RAFT: Recurrent All-Pairs Field Transforms for Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/c9186aaea568f616c466f0648a6bc913461c3cc7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1908.04353\",\"authors\":[{\"authorId\":\"40478348\",\"name\":\"Dong Cao\"},{\"authorId\":\"49287317\",\"name\":\"Lisha Xu\"},{\"authorId\":null,\"name\":\"HaiBo Chen\"}],\"doi\":\"10.1007/978-3-030-41299-9_3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2099f70cabf2a8efaa3c84ee4d3c59bcc7d1518f\",\"title\":\"Action Recognition in Untrimmed Videos with Composite Self-Attention Two-Stream Framework\",\"url\":\"https://www.semanticscholar.org/paper/2099f70cabf2a8efaa3c84ee4d3c59bcc7d1518f\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8702449\",\"name\":\"Wanneng Wang\"},{\"authorId\":\"47009814\",\"name\":\"Yanan Ma\"},{\"authorId\":\"144947764\",\"name\":\"Ke Gao\"},{\"authorId\":\"152813130\",\"name\":\"J. Cao\"}],\"doi\":\"10.1145/3343031.3350884\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0be689463698d92433b4fd343379a32e763c9f2c\",\"title\":\"Cost-free Transfer Learning Mechanism: Deep Digging Relationships of Action Categories\",\"url\":\"https://www.semanticscholar.org/paper/0be689463698d92433b4fd343379a32e763c9f2c\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1416650467\",\"name\":\"Vida Adeli\"},{\"authorId\":\"1707752\",\"name\":\"E. F. Ersi\"},{\"authorId\":\"1734678\",\"name\":\"A. Harati\"}],\"doi\":\"10.1016/j.imavis.2019.08.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"da6e00eeee9c068e081e24836b230024eaa2eeae\",\"title\":\"A component-based video content representation for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/da6e00eeee9c068e081e24836b230024eaa2eeae\",\"venue\":\"Image Vis. Comput.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35653798\",\"name\":\"Yuankai Qi\"},{\"authorId\":\"1761159\",\"name\":\"S. Zhang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"145235479\",\"name\":\"L. Su\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018835\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed2ec0f143adddf35fdf118a4a02174738637062\",\"title\":\"Learning Attribute-Specific Representations for Visual Tracking\",\"url\":\"https://www.semanticscholar.org/paper/ed2ec0f143adddf35fdf118a4a02174738637062\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2006.12263\",\"authors\":[{\"authorId\":\"73535631\",\"name\":\"Lingtong Kong\"},{\"authorId\":\"145651836\",\"name\":\"Jie Yang\"}],\"doi\":\"10.1109/ICIP40778.2020.9191101\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ad052787d23f3324ac782658ef4c36a3d39e259\",\"title\":\"Fdflownet: Fast Optical Flow Estimation Using A Deep Lightweight Network\",\"url\":\"https://www.semanticscholar.org/paper/7ad052787d23f3324ac782658ef4c36a3d39e259\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398447065\",\"name\":\"Sadjad Asghari-Esfeden\"},{\"authorId\":\"1687866\",\"name\":\"M. Sznaier\"},{\"authorId\":\"143867334\",\"name\":\"O. Camps\"}],\"doi\":\"10.1109/WACV45572.2020.9093500\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51bcb7de98bedf65215910fcfd08555d5eed682a\",\"title\":\"Dynamic Motion Representation for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/51bcb7de98bedf65215910fcfd08555d5eed682a\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491240780\",\"name\":\"Zhensheng Shi\"},{\"authorId\":\"1491233401\",\"name\":\"Liangjie Cao\"},{\"authorId\":\"153747406\",\"name\":\"Cheng Guan\"},{\"authorId\":\"2336297\",\"name\":\"Haiyong Zheng\"},{\"authorId\":\"29344734\",\"name\":\"Zhaorui Gu\"},{\"authorId\":\"144803009\",\"name\":\"Z. Yu\"},{\"authorId\":\"49721778\",\"name\":\"B. Zheng\"}],\"doi\":\"10.1109/ACCESS.2020.2968024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8744061c96cbcf492d5e504d3f6cfc7d0059643\",\"title\":\"Learning Attention-Enhanced Spatiotemporal Representation for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/c8744061c96cbcf492d5e504d3f6cfc7d0059643\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143623051\",\"name\":\"K. Ahmad\"},{\"authorId\":\"3058987\",\"name\":\"N. Conci\"}],\"doi\":\"10.1145/3306240\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e231537cb0680136b9f4c2dd3124f83ce0792780\",\"title\":\"How Deep Features Have Improved Event Recognition in Multimedia\",\"url\":\"https://www.semanticscholar.org/paper/e231537cb0680136b9f4c2dd3124f83ce0792780\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"2011.02543\",\"authors\":[{\"authorId\":\"30621486\",\"name\":\"Stepan Alekseevich Komkov\"},{\"authorId\":\"2007675511\",\"name\":\"Maksim Dzabraev\"},{\"authorId\":\"1380315305\",\"name\":\"Aleksandr Petiushko\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e0be00a9dac71413ebfca60ea6b40a5d73c5877\",\"title\":\"Mutual Modality Learning for Video Action Classification\",\"url\":\"https://www.semanticscholar.org/paper/0e0be00a9dac71413ebfca60ea6b40a5d73c5877\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.07853\",\"authors\":[{\"authorId\":\"31162518\",\"name\":\"Xingyu Liu\"},{\"authorId\":\"1926578\",\"name\":\"Joon-Young Lee\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"}],\"doi\":\"10.1109/CVPR.2019.00440\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66c21a179824037becb94105cafea75c746ad89a\",\"title\":\"Learning Video Representations From Correspondence Proposals\",\"url\":\"https://www.semanticscholar.org/paper/66c21a179824037becb94105cafea75c746ad89a\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2005.02591\",\"authors\":[{\"authorId\":\"9734988\",\"name\":\"Yuecong Xu\"},{\"authorId\":\"2562263\",\"name\":\"Jianfei Yang\"},{\"authorId\":\"120974533\",\"name\":\"Kezhi Mao\"},{\"authorId\":\"2037059\",\"name\":\"Jian-Xiong Yin\"},{\"authorId\":\"144308998\",\"name\":\"S. See\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fa2d451e839f4108aaa5fdeaa5a5722f3b232d52\",\"title\":\"Exploiting Inter-Frame Regional Correlation for Efficient Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fa2d451e839f4108aaa5fdeaa5a5722f3b232d52\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"23911916\",\"name\":\"Mehrtash Harandi\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"2641547\",\"name\":\"H. Liu\"},{\"authorId\":\"143775741\",\"name\":\"W. Liu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1109/TIP.2018.2877936\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5b8894cf0606b991a913b84a2a3e8b43e4c32de\",\"title\":\"Toward Efficient Action Recognition: Principal Backpropagation for Training Two-Stream Networks\",\"url\":\"https://www.semanticscholar.org/paper/f5b8894cf0606b991a913b84a2a3e8b43e4c32de\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"2011.05005\",\"authors\":[{\"authorId\":null,\"name\":\"Yikai Wang\"},{\"authorId\":\"51026885\",\"name\":\"W. Huang\"},{\"authorId\":\"2323566\",\"name\":\"Fu-Chun Sun\"},{\"authorId\":\"1754673\",\"name\":\"Tingyang Xu\"},{\"authorId\":\"95984192\",\"name\":\"Yu Rong\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51110c76a6e470a86f409e8d9c9b4629788b5fba\",\"title\":\"Deep Multimodal Fusion by Channel Exchanging\",\"url\":\"https://www.semanticscholar.org/paper/51110c76a6e470a86f409e8d9c9b4629788b5fba\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2012.10283\",\"authors\":[{\"authorId\":\"3330863\",\"name\":\"F. Hu\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"123746715\",\"name\":\"K. McGuinness\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eeb3393f1b01fb523b6aa990c5778e824ccc439d\",\"title\":\"Temporal Bilinear Encoding Network of Audio-Visual Features at Low Sampling Rates\",\"url\":\"https://www.semanticscholar.org/paper/eeb3393f1b01fb523b6aa990c5778e824ccc439d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390626541\",\"name\":\"Ziming Liu\"},{\"authorId\":\"2041335607\",\"name\":\"Jinyang Li\"},{\"authorId\":\"47296615\",\"name\":\"Guangyu Gao\"},{\"authorId\":\"2041264362\",\"name\":\"Alex K. Qin\"}],\"doi\":\"10.1109/ACCESS.2020.3043386\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"16df18da2ab27ef1affeefe62ab842ce0f9d8520\",\"title\":\"Temporal Memory Network Towards Real-Time Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/16df18da2ab27ef1affeefe62ab842ce0f9d8520\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1659125162\",\"name\":\"Danfeng Zhuang\"},{\"authorId\":\"145309461\",\"name\":\"Min Jiang\"},{\"authorId\":\"1644912978\",\"name\":\"Jun Kong\"},{\"authorId\":\"1878899344\",\"name\":\"Tianshan Liu\"}],\"doi\":\"10.1007/s13042-020-01204-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88f3b42b45e5f5ee60a22eb30cd41e7acc8662c9\",\"title\":\"Spatiotemporal attention enhanced features fusion network for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/88f3b42b45e5f5ee60a22eb30cd41e7acc8662c9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27046089\",\"name\":\"D. Li\"},{\"authorId\":\"153189209\",\"name\":\"Tingting Jiang\"},{\"authorId\":\"1410304992\",\"name\":\"Ming Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c70210d08008523cddc585c40e86c3132b930d49\",\"title\":\"uality Assessment of In-the-Wild Videos\",\"url\":\"https://www.semanticscholar.org/paper/c70210d08008523cddc585c40e86c3132b930d49\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46276307\",\"name\":\"Jianwei Li\"},{\"authorId\":\"143836554\",\"name\":\"Hainan Cui\"},{\"authorId\":\"1878916102\",\"name\":\"Tianxiao Guo\"},{\"authorId\":\"1877155774\",\"name\":\"Qingrui Hu\"},{\"authorId\":\"8034759\",\"name\":\"Y. Shen\"}],\"doi\":\"10.1109/ICMEW46912.2020.9106049\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"95920910eb5ca01a28b0d3fbb90cfb3e025a4382\",\"title\":\"Efficient Fitness Action Analysis Based on Spatio-Temporal Feature Encoding\",\"url\":\"https://www.semanticscholar.org/paper/95920910eb5ca01a28b0d3fbb90cfb3e025a4382\",\"venue\":\"2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)\",\"year\":2020},{\"arxivId\":\"2007.10321\",\"authors\":[{\"authorId\":\"50031265\",\"name\":\"Xitong Yang\"},{\"authorId\":\"40058797\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"2391885\",\"name\":\"Sifei Liu\"},{\"authorId\":\"3232265\",\"name\":\"Deqing Sun\"},{\"authorId\":\"152296574\",\"name\":\"L. Davis\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1cedd24562c1e95415b246a1a0e9912d8de6f0f7\",\"title\":\"Hierarchical Contrastive Motion Learning for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1cedd24562c1e95415b246a1a0e9912d8de6f0f7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.05979\",\"authors\":[{\"authorId\":\"144077750\",\"name\":\"Hang Zhao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"2650832\",\"name\":\"W. Ma\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/ICCV.2019.00182\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c880de441a41c351955ad0bf8f712eeee500ac67\",\"title\":\"The Sound of Motions\",\"url\":\"https://www.semanticscholar.org/paper/c880de441a41c351955ad0bf8f712eeee500ac67\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1812.08249\",\"authors\":[{\"authorId\":\"143935879\",\"name\":\"Jonathan C. Stroud\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"}],\"doi\":\"10.1109/WACV45572.2020.9093274\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2766379db5d9907766dab034a52867d7394a265e\",\"title\":\"D3D: Distilled 3D Networks for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2766379db5d9907766dab034a52867d7394a265e\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"DA COMPUTA\\u00c7\\u00c3O\"},{\"authorId\":\"3052490\",\"name\":\"M. Vieira\"},{\"authorId\":\"3387755\",\"name\":\"S. Villela\"},{\"authorId\":null,\"name\":\"Hemerson Aparecido da Costa\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"838871719b323c2b457c769cdf66d68f06df9523\",\"title\":\"Data Augmentation of Visual Rhythms using Symmetric Extension for Deep Learning Video Based Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/838871719b323c2b457c769cdf66d68f06df9523\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98294026\",\"name\":\"S. Liu\"},{\"authorId\":\"145453305\",\"name\":\"Xin Ma\"},{\"authorId\":\"2610029\",\"name\":\"H. Wu\"},{\"authorId\":\"32083314\",\"name\":\"Yi-bin Li\"}],\"doi\":\"10.1109/ACCESS.2020.2979549\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b2a5d27304a17f1b613e8816613e3deb3992ab3\",\"title\":\"An End to End Framework With Adaptive Spatio-Temporal Attention Module for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/0b2a5d27304a17f1b613e8816613e3deb3992ab3\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2012.08976\",\"authors\":[{\"authorId\":\"13763932\",\"name\":\"Dongxu Wei\"},{\"authorId\":\"144838755\",\"name\":\"Xiaowei Xu\"},{\"authorId\":\"1888007\",\"name\":\"Haibin Shen\"},{\"authorId\":\"47942157\",\"name\":\"K. Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f00163263f974b9d38b5bd399b1ed15d085cecd\",\"title\":\"C2F-FWN: Coarse-to-Fine Flow Warping Network for Spatial-Temporal Consistent Motion Transfer\",\"url\":\"https://www.semanticscholar.org/paper/6f00163263f974b9d38b5bd399b1ed15d085cecd\",\"venue\":\"\",\"year\":2020}],\"corpusId\":4553112,\"doi\":\"10.1109/CVPR.2018.00630\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":15,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"259dcd1afe22e569132ccc41697ac368504c4dd1\",\"references\":[{\"arxivId\":\"1608.00859\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-46484-8_2\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"title\":\"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1511.06432\",\"authors\":[{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"title\":\"Delving Deeper into Convolutional Networks for Learning Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1411.6031\",\"authors\":[{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/CVPR.2015.7298676\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"295a1c000c5e3fd5959bb250e8376a06efa405b1\",\"title\":\"Finding action tubes\",\"url\":\"https://www.semanticscholar.org/paper/295a1c000c5e3fd5959bb250e8376a06efa405b1\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1603.03590\",\"authors\":[{\"authorId\":\"2515835\",\"name\":\"Till Kroeger\"},{\"authorId\":\"1732855\",\"name\":\"R. Timofte\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-46493-0_29\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9ceb5b92cad5cb135aaab43d4f25a6e34afe6e9f\",\"title\":\"Fast Optical Flow Using Dense Inverse Search\",\"url\":\"https://www.semanticscholar.org/paper/9ceb5b92cad5cb135aaab43d4f25a6e34afe6e9f\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1604.06573\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2016.213\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d9aced120e530484609164c836da64548693484\",\"title\":\"Convolutional Two-Stream Network Fusion for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1506.02025\",\"authors\":[{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe87ea16d5eb1c7509da9a0314bbf4c7b0676506\",\"title\":\"Spatial Transformer Networks\",\"url\":\"https://www.semanticscholar.org/paper/fe87ea16d5eb1c7509da9a0314bbf4c7b0676506\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"3205375\",\"name\":\"T. Lindeberg\"}],\"doi\":\"10.1109/ICCV.2003.1238378\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f90d79809325d2b78e35a79ecb372407f81b3993\",\"title\":\"Space-time interest points\",\"url\":\"https://www.semanticscholar.org/paper/f90d79809325d2b78e35a79ecb372407f81b3993\",\"venue\":\"Proceedings Ninth IEEE International Conference on Computer Vision\",\"year\":2003},{\"arxivId\":\"1612.01925\",\"authors\":[{\"authorId\":\"48105320\",\"name\":\"Eddy Ilg\"},{\"authorId\":\"153200643\",\"name\":\"N. Mayer\"},{\"authorId\":\"2872102\",\"name\":\"Tonmoy Saikia\"},{\"authorId\":\"3316866\",\"name\":\"Margret Keuper\"},{\"authorId\":\"2841331\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/CVPR.2017.179\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edd846e76cacfba5be37da99c006e3ccc9b861b0\",\"title\":\"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/edd846e76cacfba5be37da99c006e3ccc9b861b0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"2304222\",\"name\":\"E. Gavves\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.1109/CVPR.2016.331\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5aae6f1aedb3e78a05bc430a1d8b86cac33c5184\",\"title\":\"Dynamic Image Networks for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5aae6f1aedb3e78a05bc430a1d8b86cac33c5184\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1601.07532\",\"authors\":[{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1007/978-3-319-54193-8_26\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3788cb8e73e1c0a62b762731da7769288a9098c9\",\"title\":\"Learning to Extract Motion from Videos in Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/3788cb8e73e1c0a62b762731da7769288a9098c9\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":\"1708.03805\",\"authors\":[{\"authorId\":\"38812373\",\"name\":\"Yunlong Bian\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"48033101\",\"name\":\"Xiao Liu\"},{\"authorId\":\"50984378\",\"name\":\"F. Li\"},{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"48515099\",\"name\":\"Y. Li\"},{\"authorId\":\"2002488\",\"name\":\"Heng Qi\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"35247507\",\"name\":\"Shilei Wen\"},{\"authorId\":\"1695082\",\"name\":\"Y. Lin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e591e31b9b6326571951a5193dde1c0f87beba78\",\"title\":\"Revisiting the Effectiveness of Off-the-shelf Temporal Modeling Approaches for Large-scale Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/e591e31b9b6326571951a5193dde1c0f87beba78\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1611.02155\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1709096\",\"name\":\"R. Wildes\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5f4d2f6ad967f7c9369c04e75cda08f12cb8afbd\",\"title\":\"Spatiotemporal Residual Networks for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5f4d2f6ad967f7c9369c04e75cda08f12cb8afbd\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1711.10305\",\"authors\":[{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2017.590\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024d037d46ae933c7e12fd16af61953c7161773a\",\"title\":\"Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks\",\"url\":\"https://www.semanticscholar.org/paper/024d037d46ae933c7e12fd16af61953c7161773a\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"2055900\",\"name\":\"Lixin Duan\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1007/978-3-319-46487-9_52\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5498b3cc5ac12ac5209ec3c8c751e87b59e9c31\",\"title\":\"Webly-Supervised Video Recognition by Mutually Voting for Relevant Web Images and Web Video Frames\",\"url\":\"https://www.semanticscholar.org/paper/c5498b3cc5ac12ac5209ec3c8c751e87b59e9c31\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2121584\",\"name\":\"Wangjiang Zhu\"},{\"authorId\":\"145815850\",\"name\":\"Jie Hu\"},{\"authorId\":\"143847421\",\"name\":\"Gang Sun\"},{\"authorId\":\"47300766\",\"name\":\"X. Cao\"},{\"authorId\":\"143970608\",\"name\":\"Y. Qiao\"}],\"doi\":\"10.1109/CVPR.2016.219\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c822785c29ceaf67a0de9c699716c94fefbd37d\",\"title\":\"A Key Volume Mining Deep Framework for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/4c822785c29ceaf67a0de9c699716c94fefbd37d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1504.06852\",\"authors\":[{\"authorId\":\"1382344214\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"152702479\",\"name\":\"P. Fischer\"},{\"authorId\":\"48105320\",\"name\":\"Eddy Ilg\"},{\"authorId\":\"2880264\",\"name\":\"Philip H\\u00e4usser\"},{\"authorId\":\"3322806\",\"name\":\"Caner Hazirbas\"},{\"authorId\":\"2943639\",\"name\":\"V. Golkov\"},{\"authorId\":\"1715782\",\"name\":\"P. V. D. Smagt\"},{\"authorId\":\"153685345\",\"name\":\"D. Cremers\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/ICCV.2015.316\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c2fb5b39428818d7ec8cc78e152e19c21b7db568\",\"title\":\"FlowNet: Learning Optical Flow with Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/c2fb5b39428818d7ec8cc78e152e19c21b7db568\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1605.04988\",\"authors\":[{\"authorId\":\"35696058\",\"name\":\"Samitha Herath\"},{\"authorId\":\"1686714\",\"name\":\"M. Harandi\"},{\"authorId\":\"29905643\",\"name\":\"F. Porikli\"}],\"doi\":\"10.1016/j.imavis.2017.01.010\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9979b794d0bd06a1959a6b169f2cf32ba8ba376b\",\"title\":\"Going deeper into action recognition: A survey\",\"url\":\"https://www.semanticscholar.org/paper/9979b794d0bd06a1959a6b169f2cf32ba8ba376b\",\"venue\":\"Image Vis. Comput.\",\"year\":2017},{\"arxivId\":\"1705.07750\",\"authors\":[{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.502\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"title\":\"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\"url\":\"https://www.semanticscholar.org/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1507.02159\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f05473c587e2a3b587f51eb808695a1c10bc153\",\"title\":\"Towards Good Practices for Very Deep Two-Stream ConvNets\",\"url\":\"https://www.semanticscholar.org/paper/1f05473c587e2a3b587f51eb808695a1c10bc153\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1704.00389\",\"authors\":[{\"authorId\":\"46759150\",\"name\":\"Yi Zhu\"},{\"authorId\":\"2362534\",\"name\":\"Zhenzhong Lan\"},{\"authorId\":\"145211099\",\"name\":\"S. Newsam\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/978-3-030-20893-6_23\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"47195627755d88121af2513646ac41eec8645fb7\",\"title\":\"Hidden Two-Stream Convolutional Networks for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/47195627755d88121af2513646ac41eec8645fb7\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"48246959\",\"name\":\"Naiyan Wang\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"1739816\",\"name\":\"D. Yeung\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/CVPR.2015.7298872\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3de050d1707524512eeab99780df3cbdee09670c\",\"title\":\"DevNet: A Deep Event Network for multimedia event detection and evidence recounting\",\"url\":\"https://www.semanticscholar.org/paper/3de050d1707524512eeab99780df3cbdee09670c\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1506.01497\",\"authors\":[{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"144716314\",\"name\":\"J. Sun\"}],\"doi\":\"10.1109/TPAMI.2016.2577031\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"title\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123446103\",\"name\":\"Hilde Kuehne\"},{\"authorId\":\"119268487\",\"name\":\"Hueihan Jhuang\"},{\"authorId\":\"1930964\",\"name\":\"E. Garrote\"},{\"authorId\":\"145031878\",\"name\":\"T. Poggio\"},{\"authorId\":\"1981539\",\"name\":\"Thomas Serre\"}],\"doi\":\"10.1109/ICCV.2011.6126543\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8b3b8848a311c501e704c45c6d50430ab7068956\",\"title\":\"HMDB: A large video database for human motion recognition\",\"url\":\"https://www.semanticscholar.org/paper/8b3b8848a311c501e704c45c6d50430ab7068956\",\"venue\":\"2011 International Conference on Computer Vision\",\"year\":2011},{\"arxivId\":\"1604.07669\",\"authors\":[{\"authorId\":\"3047890\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"}],\"doi\":\"10.1109/CVPR.2016.297\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d4cf68fad61cd7dc9061670ba3864a7fdf87d4c\",\"title\":\"Real-Time Action Recognition with Enhanced Motion Vector CNNs\",\"url\":\"https://www.semanticscholar.org/paper/3d4cf68fad61cd7dc9061670ba3864a7fdf87d4c\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.5244/C.22.99\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719\",\"title\":\"A Spatio-Temporal Descriptor Based on 3D-Gradients\",\"url\":\"https://www.semanticscholar.org/paper/56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719\",\"venue\":\"BMVC\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38108961\",\"name\":\"Mingyu Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7084bced25ec0b230c5aa0159077dc38d6915ca\",\"title\":\"MoSIFT: Recognizing Human Actions in Surveillance Videos\",\"url\":\"https://www.semanticscholar.org/paper/f7084bced25ec0b230c5aa0159077dc38d6915ca\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"2976163\",\"name\":\"Kuiyuan Yang\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2016.106\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26201e1b76f276187e31c7db84eab0bfcda01102\",\"title\":\"You Lead, We Exceed: Labor-Free Video Concept Learning by Jointly Exploiting Web Videos and Images\",\"url\":\"https://www.semanticscholar.org/paper/26201e1b76f276187e31c7db84eab0bfcda01102\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1505.04868\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"}],\"doi\":\"10.1109/CVPR.2015.7299059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"df658828fb4146877f1031ec9d07052f7eb31186\",\"title\":\"Action recognition with trajectory-pooled deep-convolutional descriptors\",\"url\":\"https://www.semanticscholar.org/paper/df658828fb4146877f1031ec9d07052f7eb31186\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"143836295\",\"name\":\"W. Xu\"},{\"authorId\":\"41216159\",\"name\":\"Ming Yang\"},{\"authorId\":\"144782042\",\"name\":\"Kai Yu\"}],\"doi\":\"10.1109/TPAMI.2012.59\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"title\":\"3D Convolutional Neural Networks for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":\"1711.09550\",\"authors\":[{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"48033101\",\"name\":\"Xiao Liu\"},{\"authorId\":\"35247507\",\"name\":\"Shilei Wen\"}],\"doi\":\"10.1109/CVPR.2018.00817\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5406fd98aa22bc2a0c1a8bc2a58ca3eb7a91155d\",\"title\":\"Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/5406fd98aa22bc2a0c1a8bc2a58ca3eb7a91155d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1713941\",\"name\":\"Christopher Zach\"},{\"authorId\":\"1730097\",\"name\":\"T. Pock\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"}],\"doi\":\"10.1007/978-3-540-74936-3_22\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c\",\"title\":\"A Duality Based Approach for Realtime TV-L1 Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/0f6bbe9afab5fd61f36de5461e9e6a30ca462c7c\",\"venue\":\"DAGM-Symposium\",\"year\":2007},{\"arxivId\":\"1612.03052\",\"authors\":[{\"authorId\":\"2340579\",\"name\":\"J. Ng\"},{\"authorId\":\"119675494\",\"name\":\"Jonghyun Choi\"},{\"authorId\":\"144660077\",\"name\":\"Jan Neumann\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1109/WACV.2018.00179\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"a6db73f10084ce6a4186363ea9d7475a9a658a11\",\"title\":\"ActionFlowNet: Learning Motion Representation for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a6db73f10084ce6a4186363ea9d7475a9a658a11\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1503.08909\",\"authors\":[{\"authorId\":\"2340579\",\"name\":\"J. Ng\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"}],\"doi\":\"10.1109/CVPR.2015.7299101\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5418b2a482720e013d487a385c26fae0f017c6a6\",\"title\":\"Beyond short snippets: Deep networks for video classification\",\"url\":\"https://www.semanticscholar.org/paper/5418b2a482720e013d487a385c26fae0f017c6a6\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1766837\",\"name\":\"Xiaojiang Peng\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1007/978-3-319-46493-0_45\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ddbd24a73ba3d74028596f393bb07a6b87a469c0\",\"title\":\"Multi-region Two-Stream R-CNN for Action Detection\",\"url\":\"https://www.semanticscholar.org/paper/ddbd24a73ba3d74028596f393bb07a6b87a469c0\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.441\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d721f4d64b8e722222c876f0a0f226ed49476347\",\"title\":\"Action Recognition with Improved Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/d721f4d64b8e722222c876f0a0f226ed49476347\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1611.00850\",\"authors\":[{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1109/CVPR.2017.291\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a93d81c7f033f1e2b54d3288b60d214f55ccc010\",\"title\":\"Optical Flow Estimation Using a Spatial Pyramid Network\",\"url\":\"https://www.semanticscholar.org/paper/a93d81c7f033f1e2b54d3288b60d214f55ccc010\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"3428663\",\"name\":\"J\\u00e9r\\u00f4me Revaud\"},{\"authorId\":\"113130084\",\"name\":\"Zaid Harchaoui\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.175\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"56493a503eecb659bbef2fe4dd1fb915d251cac0\",\"title\":\"DeepFlow: Large Displacement Optical Flow with Deep Matching\",\"url\":\"https://www.semanticscholar.org/paper/56493a503eecb659bbef2fe4dd1fb915d251cac0\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145347688\",\"name\":\"S. Baker\"},{\"authorId\":\"1709053\",\"name\":\"D. Scharstein\"},{\"authorId\":\"69395700\",\"name\":\"J. Lewis\"},{\"authorId\":\"145920814\",\"name\":\"S. Roth\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"},{\"authorId\":\"1717841\",\"name\":\"R. Szeliski\"}],\"doi\":\"10.1007/s11263-010-0390-2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"804836b8ad86ef8042e3dcbd45442a52f031ee03\",\"title\":\"A Database and Evaluation Methodology for Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/804836b8ad86ef8042e3dcbd45442a52f031ee03\",\"venue\":\"2007 IEEE 11th International Conference on Computer Vision\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1404610182\",\"name\":\"Orit Kliper-Gross\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"145128144\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/TPAMI.2011.209\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1fbde67e87890e5d45864e66edb86136fbdbe20e\",\"title\":\"The Action Similarity Labeling Challenge\",\"url\":\"https://www.semanticscholar.org/paper/1fbde67e87890e5d45864e66edb86136fbdbe20e\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2012},{\"arxivId\":\"1603.04467\",\"authors\":[{\"authorId\":\"145832079\",\"name\":\"M. Abadi\"},{\"authorId\":\"145984138\",\"name\":\"A. Agarwal\"},{\"authorId\":\"144758007\",\"name\":\"P. Barham\"},{\"authorId\":\"2445241\",\"name\":\"E. Brevdo\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"48738717\",\"name\":\"Craig Citro\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"36347083\",\"name\":\"Andy Davis\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"145139947\",\"name\":\"M. Devin\"},{\"authorId\":\"1780892\",\"name\":\"Sanjay Ghemawat\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"3384453\",\"name\":\"A. Harp\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"2090818\",\"name\":\"M. Isard\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1944541\",\"name\":\"R. J\\u00f3zefowicz\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1942300\",\"name\":\"M. Kudlur\"},{\"authorId\":\"3369421\",\"name\":\"Josh Levenberg\"},{\"authorId\":\"143767989\",\"name\":\"Dan Man\\u00e9\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"144375552\",\"name\":\"Sherry Moore\"},{\"authorId\":\"20154699\",\"name\":\"D. Murray\"},{\"authorId\":\"153301219\",\"name\":\"Chris Olah\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"32163737\",\"name\":\"B. Steiner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"35210462\",\"name\":\"Kunal Talwar\"},{\"authorId\":\"2080690\",\"name\":\"P. Tucker\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"38062095\",\"name\":\"V. Vasudevan\"},{\"authorId\":\"1765169\",\"name\":\"F. Vi\\u00e9gas\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"47941411\",\"name\":\"Pete Warden\"},{\"authorId\":\"145233583\",\"name\":\"M. Wattenberg\"},{\"authorId\":\"35078078\",\"name\":\"Martin Wicke\"},{\"authorId\":\"47112093\",\"name\":\"Y. Yu\"},{\"authorId\":\"2777763\",\"name\":\"X. Zheng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"title\":\"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\",\"url\":\"https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"venue\":\"ArXiv\",\"year\":2016}],\"title\":\"End-to-End Learning of Motion Representation for Video Understanding\",\"topics\":[{\"topic\":\"Optical flow\",\"topicId\":\"26430\",\"url\":\"https://www.semanticscholar.org/topic/26430\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Feature learning\",\"topicId\":\"20551\",\"url\":\"https://www.semanticscholar.org/topic/20551\"},{\"topic\":\"End-to-end encryption\",\"topicId\":\"854929\",\"url\":\"https://www.semanticscholar.org/topic/854929\"},{\"topic\":\"Video content analysis\",\"topicId\":\"98626\",\"url\":\"https://www.semanticscholar.org/topic/98626\"},{\"topic\":\"Concatenation\",\"topicId\":\"2262\",\"url\":\"https://www.semanticscholar.org/topic/2262\"},{\"topic\":\"Performance\",\"topicId\":\"3097\",\"url\":\"https://www.semanticscholar.org/topic/3097\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Iteration\",\"topicId\":\"11823\",\"url\":\"https://www.semanticscholar.org/topic/11823\"},{\"topic\":\"Fastest\",\"topicId\":\"172009\",\"url\":\"https://www.semanticscholar.org/topic/172009\"},{\"topic\":\"Solver\",\"topicId\":\"6215\",\"url\":\"https://www.semanticscholar.org/topic/6215\"},{\"topic\":\"Unfolding (DSP implementation)\",\"topicId\":\"10125\",\"url\":\"https://www.semanticscholar.org/topic/10125\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"}],\"url\":\"https://www.semanticscholar.org/paper/259dcd1afe22e569132ccc41697ac368504c4dd1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"