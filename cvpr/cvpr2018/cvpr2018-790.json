"{\"abstract\":\"We present a method that gets as input an audio of violin or piano playing, and outputs a video of skeleton predictions which are further used to animate an avatar. The key idea is to create an animation of an avatar that moves their hands similarly to how a pianist or violinist would do, just from audio. Notably, it's not clear if body movement can be predicted from music at all and our aim in this work is to explore this possibility. In this paper, we present the first result that shows that natural body dynamics can be predicted. We built an LSTM network that is trained on violin and piano recital videos uploaded to the Internet. The predicted points are applied onto a rigged avatar to create the animation.\",\"arxivId\":\"1712.09382\",\"authors\":[{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\",\"url\":\"https://www.semanticscholar.org/author/2003419\"},{\"authorId\":\"32273391\",\"name\":\"L. Dery\",\"url\":\"https://www.semanticscholar.org/author/32273391\"},{\"authorId\":\"1411184751\",\"name\":\"Hayden Schoen\",\"url\":\"https://www.semanticscholar.org/author/1411184751\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\",\"url\":\"https://www.semanticscholar.org/author/1397689071\"}],\"citationVelocity\":17,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"46228775\",\"name\":\"Yoonjae Cho\"},{\"authorId\":\"33085949\",\"name\":\"Dohyeong Kim\"},{\"authorId\":\"116406543\",\"name\":\"Edwin M. Truman\"},{\"authorId\":\"153427058\",\"name\":\"Jean-Charles Bazin\"}],\"doi\":\"10.1109/ICCVW.2019.00458\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"337825eef0731eea6bc67544f6d30cf3f383c78f\",\"title\":\"FaceSyncNet: A Deep Learning-Based Approach for Non-Linear Synchronization of Facial Performance Videos\",\"url\":\"https://www.semanticscholar.org/paper/337825eef0731eea6bc67544f6d30cf3f383c78f\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2813082\",\"name\":\"J. Lee\"},{\"authorId\":\"21149653\",\"name\":\"Bardia Doosti\"},{\"authorId\":\"2005401\",\"name\":\"Yupeng Gu\"},{\"authorId\":\"83444422\",\"name\":\"David Cartledge\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"145087434\",\"name\":\"C. Raphael\"}],\"doi\":\"10.1109/WACV.2019.00165\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82155ad2e32ebbb7581b6e3359b89daf95030f71\",\"title\":\"Observing Pianist Accuracy and Form with Computer Vision\",\"url\":\"https://www.semanticscholar.org/paper/82155ad2e32ebbb7581b6e3359b89daf95030f71\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":\"2001.04463\",\"authors\":[{\"authorId\":\"151500851\",\"name\":\"K. Deng\"},{\"authorId\":\"3294630\",\"name\":\"Aayush Bansal\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1dcee6a820b9d23c9cca11b1de96dd14f97e9f2a\",\"title\":\"Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/1dcee6a820b9d23c9cca11b1de96dd14f97e9f2a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1810.12541\",\"authors\":[{\"authorId\":\"145215929\",\"name\":\"Youngwoo Yoon\"},{\"authorId\":\"38108552\",\"name\":\"Woo-Ri Ko\"},{\"authorId\":\"145416765\",\"name\":\"M. Jang\"},{\"authorId\":\"8152662\",\"name\":\"Jaeyeon Lee\"},{\"authorId\":\"1684726\",\"name\":\"Jaehong Kim\"},{\"authorId\":\"1717371\",\"name\":\"Geehyuk Lee\"}],\"doi\":\"10.1109/ICRA.2019.8793720\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1612a6053c7f017dd0897d26971a1dd9cd1bebc1\",\"title\":\"Robots Learn Social Skills: End-to-End Learning of Co-Speech Gesture Generation for Humanoid Robots\",\"url\":\"https://www.semanticscholar.org/paper/1612a6053c7f017dd0897d26971a1dd9cd1bebc1\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3440249\",\"name\":\"Ruozi Huang\"},{\"authorId\":\"46353980\",\"name\":\"Huang Hu\"},{\"authorId\":\"145717875\",\"name\":\"Wei Wu\"},{\"authorId\":\"2505139\",\"name\":\"K. Sawada\"},{\"authorId\":\"144315664\",\"name\":\"Mi Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b210d4594f19342ff625cf50d8d4bccfdc0d2ae5\",\"title\":\"Dance Revolution: Long Sequence Dance Generation with Music via Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/b210d4594f19342ff625cf50d8d4bccfdc0d2ae5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.04758\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"100602595\",\"name\":\"Mandi Luo\"},{\"authorId\":\"77886607\",\"name\":\"Rui Wang\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"title\":\"Deep Audio-Visual Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1612.08727\",\"authors\":[{\"authorId\":\"2868721\",\"name\":\"Bochen Li\"},{\"authorId\":\"8016212\",\"name\":\"Xinzhao Liu\"},{\"authorId\":\"27361710\",\"name\":\"K. Dinesh\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"145621177\",\"name\":\"Gaurav Sharma\"}],\"doi\":\"10.1109/TMM.2018.2856090\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3af5e203368fa2c7959d035493571d181a8682af\",\"title\":\"Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications\",\"url\":\"https://www.semanticscholar.org/paper/3af5e203368fa2c7959d035493571d181a8682af\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"2007.09198\",\"authors\":[{\"authorId\":\"46840906\",\"name\":\"Miao Liao\"},{\"authorId\":\"144268595\",\"name\":\"Sibo Zhang\"},{\"authorId\":\"1585288737\",\"name\":\"Peng Wang\"},{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"38958903\",\"name\":\"Ruigang Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a0b4f6f589a4e37d5d652968110b43816574175a\",\"title\":\"Personalized Speech2Video with 3D Skeleton Regularization and Expressive Body Poses\",\"url\":\"https://www.semanticscholar.org/paper/a0b4f6f589a4e37d5d652968110b43816574175a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.09476\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"47940821\",\"name\":\"Hang Zhao\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR42600.2020.01049\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b643a7186c08db9f13d7204f6e5e739f97902e71\",\"title\":\"Music Gesture for Visual Sound Separation\",\"url\":\"https://www.semanticscholar.org/paper/b643a7186c08db9f13d7204f6e5e739f97902e71\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151368062\",\"name\":\"Yukitaka Tsuchiya\"},{\"authorId\":\"21583404\",\"name\":\"Takahiro Itazuri\"},{\"authorId\":\"41015901\",\"name\":\"R. Natsume\"},{\"authorId\":\"48333526\",\"name\":\"S. Yamamoto\"},{\"authorId\":\"1707631\",\"name\":\"T. Kato\"},{\"authorId\":\"1731030\",\"name\":\"S. Morishima\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c1fb788324287f7e7d84606f32f1878196764b36\",\"title\":\"Generating Video from Single Image and Sound\",\"url\":\"https://www.semanticscholar.org/paper/c1fb788324287f7e7d84606f32f1878196764b36\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":\"2010.06194\",\"authors\":[{\"authorId\":\"1789820\",\"name\":\"N. Wake\"},{\"authorId\":\"9000389\",\"name\":\"M. Sato\"},{\"authorId\":\"1882605\",\"name\":\"Kazuhiro Sasabuchi\"},{\"authorId\":\"1500391189\",\"name\":\"Minako Nakamura\"},{\"authorId\":\"66117417\",\"name\":\"K. Ikeuchi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"65497441b0d644e6b09c4cad768fd0b7781301e6\",\"title\":\"Labeling the Phrase Set of the Conversation Agent, Rinna\",\"url\":\"https://www.semanticscholar.org/paper/65497441b0d644e6b09c4cad768fd0b7781301e6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.07816\",\"authors\":[{\"authorId\":\"51038715\",\"name\":\"Y. Lin\"},{\"authorId\":\"8665310\",\"name\":\"H. Kao\"},{\"authorId\":\"1944621645\",\"name\":\"Yih-Chih Tseng\"},{\"authorId\":\"144090142\",\"name\":\"M. Tsai\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"}],\"doi\":\"10.1145/3394171.3413921\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f1e36672c791f79d674acefe9d7dfeb827697db5\",\"title\":\"A Human-Computer Duet System for Music Performance\",\"url\":\"https://www.semanticscholar.org/paper/f1e36672c791f79d674acefe9d7dfeb827697db5\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10755743\",\"name\":\"Takayuki Nakatsuka\"},{\"authorId\":\"2844427\",\"name\":\"M. Hamanaka\"},{\"authorId\":\"1490867805\",\"name\":\"Shigeo Morishima\"}],\"doi\":\"10.5220/0008876600270035\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c1de74772253f4b9ebb4ea86b1602fa1445786f\",\"title\":\"Audio-guided Video Interpolation via Human Pose Features\",\"url\":\"https://www.semanticscholar.org/paper/6c1de74772253f4b9ebb4ea86b1602fa1445786f\",\"venue\":\"VISIGRAPP\",\"year\":2020},{\"arxivId\":\"1901.07677\",\"authors\":[{\"authorId\":\"41018093\",\"name\":\"Dario Pavllo\"},{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"},{\"authorId\":\"2529182\",\"name\":\"David Grangier\"}],\"doi\":\"10.1007/s11263-019-01245-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2549dba602502ced05f25bea41b1e66b5f0bfd40\",\"title\":\"Modeling Human Motion with Quaternion-Based Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/2549dba602502ced05f25bea41b1e66b5f0bfd40\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3050542\",\"name\":\"L. Yu\"},{\"authorId\":\"50812076\",\"name\":\"J. Yu\"},{\"authorId\":\"40177644\",\"name\":\"Q. Ling\"}],\"doi\":\"10.1109/TMM.2018.2887027\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca95e1b28687e821fe41a6f13779511edfa86e47\",\"title\":\"BLTRCNN-Based 3-D Articulatory Movement Prediction: Learning Articulatory Synchronicity From Both Text and Audio Inputs\",\"url\":\"https://www.semanticscholar.org/paper/ca95e1b28687e821fe41a6f13779511edfa86e47\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042286691\",\"name\":\"Alysha Bogaers\"},{\"authorId\":\"1730934\",\"name\":\"Zerrin Yumak\"},{\"authorId\":\"50416821\",\"name\":\"A. Volk\"}],\"doi\":\"10.1145/3395035.3425244\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"df1a9639f3fa6af34258ee8ec079905f50c47e61\",\"title\":\"Music-Driven Animation Generation of Expressive Musical Gestures\",\"url\":\"https://www.semanticscholar.org/paper/df1a9639f3fa6af34258ee8ec079905f50c47e61\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.15240\",\"authors\":[{\"authorId\":\"1794679\",\"name\":\"C. Guo\"},{\"authorId\":\"2407738\",\"name\":\"Xinxin Zuo\"},{\"authorId\":\"1768118967\",\"name\":\"Sen Wang\"},{\"authorId\":\"9399556\",\"name\":\"Shihao Zou\"},{\"authorId\":\"1841581716\",\"name\":\"Qingyao Sun\"},{\"authorId\":\"1841709579\",\"name\":\"Annan Deng\"},{\"authorId\":\"1473876432\",\"name\":\"M. Gong\"},{\"authorId\":\"145193182\",\"name\":\"L. Cheng\"}],\"doi\":\"10.1145/3394171.3413635\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b7b4996858b91bc4db5dd5ce13215b5422591529\",\"title\":\"Action2Motion: Conditioned Generation of 3D Human Motions\",\"url\":\"https://www.semanticscholar.org/paper/b7b4996858b91bc4db5dd5ce13215b5422591529\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1909.12780\",\"authors\":[{\"authorId\":\"41016678\",\"name\":\"Givi Meishvili\"},{\"authorId\":\"5641221\",\"name\":\"S. Jenni\"},{\"authorId\":\"144707901\",\"name\":\"P. Favaro\"}],\"doi\":\"10.1109/cvpr42600.2020.00144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b073e91f84ab59e482c2f1e22918f46ef606a531\",\"title\":\"Learning to Have an Ear for Face Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/b073e91f84ab59e482c2f1e22918f46ef606a531\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2868721\",\"name\":\"Bochen Li\"},{\"authorId\":\"3328330\",\"name\":\"A. Maezawa\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f556491f5250839db4d7a1d71799e44fe02d8ef\",\"title\":\"Skeleton Plays Piano: Online Generation of Pianist Body Movements from MIDI Performance\",\"url\":\"https://www.semanticscholar.org/paper/5f556491f5250839db4d7a1d71799e44fe02d8ef\",\"venue\":\"ISMIR\",\"year\":2018},{\"arxivId\":\"1904.05876\",\"authors\":[{\"authorId\":\"38211837\",\"name\":\"Idan Schwartz\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"1918412\",\"name\":\"T. Hazan\"}],\"doi\":\"10.1109/CVPR.2019.01283\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf072e469d82e71f0515f32b686fb084f4f31714\",\"title\":\"A Simple Baseline for Audio-Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/cf072e469d82e71f0515f32b686fb084f4f31714\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202217\",\"name\":\"I. Habibie\"},{\"authorId\":\"2470018\",\"name\":\"WeiPeng Xu\"},{\"authorId\":\"39503308\",\"name\":\"Dushyant Mehta\"},{\"authorId\":\"46458089\",\"name\":\"Lingjie Liu\"},{\"authorId\":\"145361968\",\"name\":\"H. Seidel\"},{\"authorId\":\"1403428213\",\"name\":\"Gerard Pons-Moll\"},{\"authorId\":\"1854465\",\"name\":\"M. Elgharib\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1afeacdae33129e84406ee2ed635fd81ae6efa7\",\"title\":\"Learning Speech-driven 3D Conversational Gestures from Video\",\"url\":\"https://www.semanticscholar.org/paper/b1afeacdae33129e84406ee2ed635fd81ae6efa7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.12130\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"2479187\",\"name\":\"Moitreya Chatterjee\"},{\"authorId\":\"145237406\",\"name\":\"N. Ahuja\"}],\"doi\":\"10.1007/978-3-030-58583-9_42\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"title\":\"Sound2Sight: Generating Visual Dynamics from Sound and Context\",\"url\":\"https://www.semanticscholar.org/paper/02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102356614\",\"name\":\"X. Zhang\"},{\"authorId\":\"66653936\",\"name\":\"X. Wu\"},{\"authorId\":\"103972572\",\"name\":\"Xinliang Zhai\"},{\"authorId\":\"2163652\",\"name\":\"Xianye Ben\"},{\"authorId\":\"40480346\",\"name\":\"C. Tu\"}],\"doi\":\"10.1109/CVPR42600.2020.01235\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ac827181a5d1d4e27eb662c2547fd2d6eb87c3b\",\"title\":\"DAVD-Net: Deep Audio-Aided Video Decompression of Talking Heads\",\"url\":\"https://www.semanticscholar.org/paper/3ac827181a5d1d4e27eb662c2547fd2d6eb87c3b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1905.09773\",\"authors\":[{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"2112779\",\"name\":\"Tali Dekel\"},{\"authorId\":\"9340074\",\"name\":\"Changil Kim\"},{\"authorId\":\"2138834\",\"name\":\"Inbar Mosseri\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"144544291\",\"name\":\"Michael Rubinstein\"},{\"authorId\":\"1752521\",\"name\":\"W. Matusik\"}],\"doi\":\"10.1109/CVPR.2019.00772\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4b54f2e0ebbe450e2c527cf5a00d4816980a913\",\"title\":\"Speech2Face: Learning the Face Behind a Voice\",\"url\":\"https://www.semanticscholar.org/paper/e4b54f2e0ebbe450e2c527cf5a00d4816980a913\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1805.06485\",\"authors\":[{\"authorId\":\"41018093\",\"name\":\"Dario Pavllo\"},{\"authorId\":\"2529182\",\"name\":\"David Grangier\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"643ab168dfa1df777b9276281a54af32bf9b25d2\",\"title\":\"QuaterNet: A Quaternion-based Recurrent Model for Human Motion\",\"url\":\"https://www.semanticscholar.org/paper/643ab168dfa1df777b9276281a54af32bf9b25d2\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47785129\",\"name\":\"Lingyun Yu\"},{\"authorId\":\"119883542\",\"name\":\"J. Yu\"},{\"authorId\":\"40177644\",\"name\":\"Q. Ling\"}],\"doi\":\"10.1109/ICDM.2019.00089\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c93fb94cbc6e5cc67851c9436127f56ab7e8725\",\"title\":\"Mining Audio, Text and Visual Information for Talking Face Generation\",\"url\":\"https://www.semanticscholar.org/paper/6c93fb94cbc6e5cc67851c9436127f56ab7e8725\",\"venue\":\"2019 IEEE International Conference on Data Mining (ICDM)\",\"year\":2019},{\"arxivId\":\"2007.09198\",\"authors\":[{\"authorId\":\"46840906\",\"name\":\"Miao Liao\"},{\"authorId\":\"144268595\",\"name\":\"Sibo Zhang\"},{\"authorId\":\"1585288737\",\"name\":\"Peng Wang\"},{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"2407738\",\"name\":\"Xinxin Zuo\"},{\"authorId\":\"38958903\",\"name\":\"Ruigang Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab58561f2646091e8f7dac5020fabfed9e74eab7\",\"title\":\"Speech2Video Synthesis with 3D Skeleton Regularization and Expressive Body Poses\",\"url\":\"https://www.semanticscholar.org/paper/ab58561f2646091e8f7dac5020fabfed9e74eab7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1906.04160\",\"authors\":[{\"authorId\":\"2361255\",\"name\":\"Shiry Ginosar\"},{\"authorId\":\"48319922\",\"name\":\"A. Bar\"},{\"authorId\":\"51169014\",\"name\":\"Gefen Kohavi\"},{\"authorId\":\"1715365\",\"name\":\"C. Chan\"},{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/CVPR.2019.00361\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e609684188c842092ae7bcae81429471e422df4d\",\"title\":\"Learning Individual Styles of Conversational Gesture\",\"url\":\"https://www.semanticscholar.org/paper/e609684188c842092ae7bcae81429471e422df4d\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2006.06911\",\"authors\":[{\"authorId\":\"1492114642\",\"name\":\"Jingyuan Li\"},{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf31f1b70e9714a5eb64a40be84daaf9fcf6f0ad\",\"title\":\"Iterate & Cluster: Iterative Semi-Supervised Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bf31f1b70e9714a5eb64a40be84daaf9fcf6f0ad\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48211981\",\"name\":\"Jun-Wei Liu\"},{\"authorId\":\"121504262\",\"name\":\"Hung-Yi Lin\"},{\"authorId\":\"46843792\",\"name\":\"Yu-fen Huang\"},{\"authorId\":\"8665310\",\"name\":\"H. Kao\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054463\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6b055efd15bca7604ec35806f07092bfa16632cb\",\"title\":\"Body Movement Generation for Expressive Violin Performance Applying Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6b055efd15bca7604ec35806f07092bfa16632cb\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2006.07931\",\"authors\":[{\"authorId\":\"134192555\",\"name\":\"J. F. Montesinos\"},{\"authorId\":\"9936815\",\"name\":\"Olga Slizovskaia\"},{\"authorId\":\"1916387\",\"name\":\"G. Haro\"}],\"doi\":\"10.1109/MMSP48831.2020.9287124\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b36193e42615bf1ec6202f22be6bfc5ed86d444\",\"title\":\"Solos: A Dataset for Audio-Visual Music Analysis\",\"url\":\"https://www.semanticscholar.org/paper/5b36193e42615bf1ec6202f22be6bfc5ed86d444\",\"venue\":\"2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP)\",\"year\":2020},{\"arxivId\":\"2007.10984\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"145592817\",\"name\":\"D. Huang\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-030-58621-8_44\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43971a0a2593f660427e016032b983b52f8dd8eb\",\"title\":\"Foley Music: Learning to Generate Music from Videos\",\"url\":\"https://www.semanticscholar.org/paper/43971a0a2593f660427e016032b983b52f8dd8eb\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.12553\",\"authors\":[{\"authorId\":\"118242121\",\"name\":\"Chaitanya Ahuja\"},{\"authorId\":\"52605659\",\"name\":\"Dong Won Lee\"},{\"authorId\":\"1718158\",\"name\":\"Yukiko I. Nakano\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1007/978-3-030-58523-5_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e78e32481218e9391a88e6d0e30c0062ae71bec\",\"title\":\"Style Transfer for Co-Speech Gesture Animation: A Multi-Speaker Conditional-Mixture Approach\",\"url\":\"https://www.semanticscholar.org/paper/6e78e32481218e9391a88e6d0e30c0062ae71bec\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2009.08015\",\"authors\":[{\"authorId\":\"8665310\",\"name\":\"H. Kao\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"}],\"doi\":\"10.1145/3394171.3413848\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"daa00c142fdf25aa454d8ca52e4d20f5d0605964\",\"title\":\"Temporally Guided Music-to-Body-Movement Generation\",\"url\":\"https://www.semanticscholar.org/paper/daa00c142fdf25aa454d8ca52e4d20f5d0605964\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.02631\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"50024256\",\"name\":\"Y. Li\"},{\"authorId\":\"2007680419\",\"name\":\"Feixia Zhu\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"597e8a6f560fcacc74e7637312c3d799f3dae241\",\"title\":\"Lets Play Music: Audio-driven Performance Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/597e8a6f560fcacc74e7637312c3d799f3dae241\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.08027\",\"authors\":[{\"authorId\":\"101033083\",\"name\":\"X. Guo\"},{\"authorId\":\"9073063\",\"name\":\"Jiugang Li\"},{\"authorId\":\"151469503\",\"name\":\"Yifan Zhao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"13e5869153ba4996c3264c2eaa41b2a87f5869d7\",\"title\":\"DanceIt: Music-inspired Dancing Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/13e5869153ba4996c3264c2eaa41b2a87f5869d7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.03478\",\"authors\":[{\"authorId\":\"1491240396\",\"name\":\"Kun Su\"},{\"authorId\":\"10598535\",\"name\":\"X. Liu\"},{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"866ba646300961747ddc29ce57153858e2be588a\",\"title\":\"Multi-Instrumentalist Net: Unsupervised Generation of Music from Body Movements\",\"url\":\"https://www.semanticscholar.org/paper/866ba646300961747ddc29ce57153858e2be588a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.11660\",\"authors\":[{\"authorId\":\"153697517\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7cf674af18a46a06cb75cc67c16ec4ee7ef27c51\",\"title\":\"R-FORCE: Robust Learning for Random Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/7cf674af18a46a06cb75cc67c16ec4ee7ef27c51\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1903.01013\",\"authors\":[{\"authorId\":\"21149653\",\"name\":\"Bardia Doosti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f75d6907a088ac5b46143b54836be2084b2598cf\",\"title\":\"Hand Pose Estimation: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/f75d6907a088ac5b46143b54836be2084b2598cf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.08462\",\"authors\":[{\"authorId\":\"88741603\",\"name\":\"A. Nivaggioli\"},{\"authorId\":\"2953019\",\"name\":\"D. Rohmer\"}],\"doi\":\"10.1145/3359566.3360067\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03ea37a0d14d28a3b5887ab4450cebeef56d0fa1\",\"title\":\"Animation Synthesis Triggered by Vocal Mimics\",\"url\":\"https://www.semanticscholar.org/paper/03ea37a0d14d28a3b5887ab4450cebeef56d0fa1\",\"venue\":\"MIG\",\"year\":2019},{\"arxivId\":\"1903.05448\",\"authors\":[{\"authorId\":\"82939564\",\"name\":\"D. Borer\"},{\"authorId\":\"152780452\",\"name\":\"Dominik Lutz\"},{\"authorId\":\"144449892\",\"name\":\"M. Guay\"}],\"doi\":\"10.33965/cgv2019_201906l033\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b17f23cf17854f720a0b8731e4629232347db0d6\",\"title\":\"Animating an Autonomous 3D Talking Avatar\",\"url\":\"https://www.semanticscholar.org/paper/b17f23cf17854f720a0b8731e4629232347db0d6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2006.14348\",\"authors\":[{\"authorId\":\"1491240396\",\"name\":\"Kun Su\"},{\"authorId\":\"10598535\",\"name\":\"X. Liu\"},{\"authorId\":\"2003419\",\"name\":\"Eli Shlizerman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6127cceb5847551cc09814a0d00cf63ba21b546\",\"title\":\"Audeo: Audio Generation for a Silent Performance Video\",\"url\":\"https://www.semanticscholar.org/paper/c6127cceb5847551cc09814a0d00cf63ba21b546\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1911.04069\",\"authors\":[{\"authorId\":\"7289061\",\"name\":\"Hyemin Ahn\"},{\"authorId\":\"2744489\",\"name\":\"Jaehun Kim\"},{\"authorId\":\"46175669\",\"name\":\"Kihyun Kim\"},{\"authorId\":\"34184385\",\"name\":\"Songhwai Oh\"}],\"doi\":\"10.1109/LRA.2020.2977333\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"57b41397f92f760fe3a9b61c81a88315064e594c\",\"title\":\"Generative Autoregressive Networks for 3D Dancing Move Synthesis From Music\",\"url\":\"https://www.semanticscholar.org/paper/57b41397f92f760fe3a9b61c81a88315064e594c\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82939564\",\"name\":\"D. Borer\"},{\"authorId\":\"144651665\",\"name\":\"D. Lutz\"},{\"authorId\":\"144449892\",\"name\":\"M. Guay\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"25d775d8ccc036ece79a448493b0363102ea506f\",\"title\":\"Animating an Autonomous 3 D Talking Avatar\",\"url\":\"https://www.semanticscholar.org/paper/25d775d8ccc036ece79a448493b0363102ea506f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1854465\",\"name\":\"M. Elgharib\"},{\"authorId\":\"70659697\",\"name\":\"M. Mendiratta\"},{\"authorId\":\"34105638\",\"name\":\"Justus Thies\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"},{\"authorId\":\"145361968\",\"name\":\"H. Seidel\"},{\"authorId\":\"9102722\",\"name\":\"A. Tewari\"},{\"authorId\":\"2019625111\",\"name\":\"Vladislav Golyanik\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":\"10.1145/3414685.3417808\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0024d00985ed1e64b274cec37dbfc81f03a23d1b\",\"title\":\"Egocentric videoconferencing\",\"url\":\"https://www.semanticscholar.org/paper/0024d00985ed1e64b274cec37dbfc81f03a23d1b\",\"venue\":\"ACM Trans. Graph.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2153397\",\"name\":\"Runpeng Cui\"},{\"authorId\":\"1576511129\",\"name\":\"Zhong Cao\"},{\"authorId\":\"2782958\",\"name\":\"Weishen Pan\"},{\"authorId\":\"14966740\",\"name\":\"Changshui Zhang\"},{\"authorId\":\"1724003\",\"name\":\"J. Wang\"}],\"doi\":\"10.1109/TMM.2019.2960700\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd47c05f454a3c3980a6c623201db45acb08a70c\",\"title\":\"Deep Gesture Video Generation With Learning on Regions of Interest\",\"url\":\"https://www.semanticscholar.org/paper/dd47c05f454a3c3980a6c623201db45acb08a70c\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1807.10550\",\"authors\":[{\"authorId\":\"8792285\",\"name\":\"Olivia Wiles\"},{\"authorId\":\"32445716\",\"name\":\"A. Koepke\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01261-8_41\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9ea992f009492888c482d5f4006281eaa8b758e7\",\"title\":\"X2Face: A network for controlling face generation by using images, audio, and pose codes\",\"url\":\"https://www.semanticscholar.org/paper/9ea992f009492888c482d5f4006281eaa8b758e7\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1911.02001\",\"authors\":[{\"authorId\":\"49923155\",\"name\":\"Hsin-Ying Lee\"},{\"authorId\":\"47008546\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"153699069\",\"name\":\"Mingyu Liu\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"8646392\",\"name\":\"Yu-Ding Lu\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"12b38ff98d8cb847d735f8fb2b331cdcc30cae91\",\"title\":\"Dancing to Music\",\"url\":\"https://www.semanticscholar.org/paper/12b38ff98d8cb847d735f8fb2b331cdcc30cae91\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118242121\",\"name\":\"Chaitanya Ahuja\"},{\"authorId\":\"152496368\",\"name\":\"Dong Won Lee\"},{\"authorId\":\"1500659510\",\"name\":\"Ryo Ishii\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.170\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d5924c8cdef6270a955ba82c2b07a8282d869744\",\"title\":\"No Gestures Left Behind: Learning Relationships between Spoken Language and Freeform Gestures\",\"url\":\"https://www.semanticscholar.org/paper/d5924c8cdef6270a955ba82c2b07a8282d869744\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.13341\",\"authors\":[{\"authorId\":\"2012907957\",\"name\":\"Yuchi Zhang\"},{\"authorId\":\"2041009115\",\"name\":\"Willis Peng\"},{\"authorId\":\"12698154\",\"name\":\"B. Wandt\"},{\"authorId\":\"2933543\",\"name\":\"H. Rhodin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a8c415a40c1371199fe5487a4d3f4df420236e0b\",\"title\":\"AudioViewer: Learning to Visualize Sound\",\"url\":\"https://www.semanticscholar.org/paper/a8c415a40c1371199fe5487a4d3f4df420236e0b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.13729\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144220896\",\"name\":\"Xiaoyu Chen\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f978b3829acad6ac8b3372d2fa8d38a45b96d3d\",\"title\":\"Noisy Agents: Self-supervised Exploration by Predicting Auditory Events\",\"url\":\"https://www.semanticscholar.org/paper/5f978b3829acad6ac8b3372d2fa8d38a45b96d3d\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":4696924,\"doi\":\"10.1109/CVPR.2018.00790\",\"fieldsOfStudy\":[\"Computer Science\",\"Engineering\"],\"influentialCitationCount\":7,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"c6d60aaad68fa78c914ee34c26bceab033a88622\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"67151772\",\"name\":\"\\u90d1\\u65b9\"},{\"authorId\":\"70042389\",\"name\":\"\\u5f20\\u56fd\\u4eae\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6b3366181aaa4618cd350e59ec06fb782bbcb7c\",\"title\":\"Comparison of Different Implementations of MFCC\",\"url\":\"https://www.semanticscholar.org/paper/b6b3366181aaa4618cd350e59ec06fb782bbcb7c\",\"venue\":\"\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"145982988\",\"name\":\"Xiao Chu\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CVPR.2014.299\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"05ffc37ed1289c9dbd01f1cd96d5a5ae908b12cb\",\"title\":\"Multi-source Deep Learning for Human Pose Estimation\",\"url\":\"https://www.semanticscholar.org/paper/05ffc37ed1289c9dbd01f1cd96d5a5ae908b12cb\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2803571\",\"name\":\"R. Loughran\"},{\"authorId\":\"145772610\",\"name\":\"J. Walker\"},{\"authorId\":\"117838237\",\"name\":\"M. O'Neill\"},{\"authorId\":\"1411559607\",\"name\":\"M. O'Farrell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dee28b8e50354bc22dfbd07b95a2d11d8c2089f5\",\"title\":\"The Use of Mel-frequency Cepstral Coefficients in Musical Instrument Identification\",\"url\":\"https://www.semanticscholar.org/paper/dee28b8e50354bc22dfbd07b95a2d11d8c2089f5\",\"venue\":\"ICMC\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2788144\",\"name\":\"S. Wang\"},{\"authorId\":\"2444581\",\"name\":\"D. Demirdjian\"}],\"doi\":\"10.1145/1088463.1088476\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5fd97138ff5d9975154e685fb5be4b45c673d746\",\"title\":\"Inferring body pose using speech content\",\"url\":\"https://www.semanticscholar.org/paper/5fd97138ff5d9975154e685fb5be4b45c673d746\",\"venue\":\"ICMI '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1734756\",\"name\":\"Keith Grochow\"},{\"authorId\":\"47361651\",\"name\":\"S. Martin\"},{\"authorId\":\"1747779\",\"name\":\"Aaron Hertzmann\"},{\"authorId\":\"1986848\",\"name\":\"Z. Popovic\"}],\"doi\":\"10.1145/1186562.1015755\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69ab9e293ae3b54f7b004ac89d789716e0ea5aa4\",\"title\":\"Style-based inverse kinematics\",\"url\":\"https://www.semanticscholar.org/paper/69ab9e293ae3b54f7b004ac89d789716e0ea5aa4\",\"venue\":\"SIGGRAPH '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1016/j.cviu.2006.10.019\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f983444b19195846608ee4fad91ef77cc14f6f41\",\"title\":\"Multimodal human-computer interaction: A survey\",\"url\":\"https://www.semanticscholar.org/paper/f983444b19195846608ee4fad91ef77cc14f6f41\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1977041\",\"name\":\"P. Wagner\"},{\"authorId\":\"3018492\",\"name\":\"Zofia Malisz\"},{\"authorId\":\"5864138\",\"name\":\"Stefan Kopp\"}],\"doi\":\"10.1016/j.specom.2013.09.008\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8073de0cf614f37c229265b57313b6131a7172fe\",\"title\":\"Gesture and speech in interaction: An overview\",\"url\":\"https://www.semanticscholar.org/paper/8073de0cf614f37c229265b57313b6131a7172fe\",\"venue\":\"Speech Commun.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1630277553\",\"name\":\"M. Sankar\"}],\"doi\":\"10.1515/9783111548050-024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"517a45c35f9ad52ac75a27c5a5f190d41cbc5e65\",\"title\":\"M\",\"url\":\"https://www.semanticscholar.org/paper/517a45c35f9ad52ac75a27c5a5f190d41cbc5e65\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49935357\",\"name\":\"Abe Davis\"},{\"authorId\":\"144544291\",\"name\":\"Michael Rubinstein\"},{\"authorId\":\"34004812\",\"name\":\"N. Wadhwa\"},{\"authorId\":\"1781063\",\"name\":\"Gautham J. Mysore\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1145/2601097.2601119\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3c0cab96b85b469da9c1084aad85702136eca1d\",\"title\":\"The visual microphone\",\"url\":\"https://www.semanticscholar.org/paper/e3c0cab96b85b469da9c1084aad85702136eca1d\",\"venue\":\"ACM Trans. Graph.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"E. Insafutdinov\"},{\"authorId\":null,\"name\":\"L. Pishchulin\"},{\"authorId\":null,\"name\":\"B. Andres\"},{\"authorId\":null,\"name\":\"M. Andriluka\"},{\"authorId\":null,\"name\":\"B. Schiele\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deepercut: A deeper\",\"url\":\"\",\"venue\":\"stronger, and faster multiperson pose estimation model. In European Conference on Computer Vision, pages 34\\u201350. Springer\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. M. Seitz S. Suwajanakorn\"},{\"authorId\":null,\"name\":\"I. Kemelmacher-Shlizerman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Synthesizing obama : learning lip sync from au\",\"url\":\"\",\"venue\":\"ACM Transactions on Graphics\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2188620\",\"name\":\"Yaniv Taigman\"},{\"authorId\":\"41216159\",\"name\":\"Ming Yang\"},{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":\"10.1109/CVPR.2014.220\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f2efadf66817f1b38f58b3f50c7c8f34c69d89a\",\"title\":\"DeepFace: Closing the Gap to Human-Level Performance in Face Verification\",\"url\":\"https://www.semanticscholar.org/paper/9f2efadf66817f1b38f58b3f50c7c8f34c69d89a\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152823345\",\"name\":\"M. Sol\\u00e8r\"},{\"authorId\":\"31798873\",\"name\":\"J. Bazin\"},{\"authorId\":\"47520207\",\"name\":\"O. Wang\"},{\"authorId\":\"153243248\",\"name\":\"A. Krause\"},{\"authorId\":\"1388791172\",\"name\":\"Alexander Sorkine-Hornung\"}],\"doi\":\"10.1007/978-3-319-48881-3_59\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"333aa36e80f1a7fa29cf069d81d4d2e12679bc67\",\"title\":\"Suggesting Sounds for Images from Video Collections\",\"url\":\"https://www.semanticscholar.org/paper/333aa36e80f1a7fa29cf069d81d4d2e12679bc67\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31669599\",\"name\":\"L. S. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"421c86ff724a2fc4b95839995514c3345e51230f\",\"title\":\"Joint processing of audio-visual information for the recognition of emotional expressions in human-c\",\"url\":\"https://www.semanticscholar.org/paper/421c86ff724a2fc4b95839995514c3345e51230f\",\"venue\":\"\",\"year\":2000},{\"arxivId\":\"1607.08128\",\"authors\":[{\"authorId\":\"2988774\",\"name\":\"Federica Bogo\"},{\"authorId\":\"20615377\",\"name\":\"A. Kanazawa\"},{\"authorId\":\"3266545\",\"name\":\"Christoph Lassner\"},{\"authorId\":\"2871555\",\"name\":\"P. Gehler\"},{\"authorId\":\"143881914\",\"name\":\"J. Romero\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1007/978-3-319-46454-1_34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4233b07033a1ef8af188383f30602a5fd0aa2181\",\"title\":\"Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image\",\"url\":\"https://www.semanticscholar.org/paper/4233b07033a1ef8af188383f30602a5fd0aa2181\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1705.02445\",\"authors\":[{\"authorId\":\"144442429\",\"name\":\"J. Martinez\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"},{\"authorId\":\"143881914\",\"name\":\"J. Romero\"}],\"doi\":\"10.1109/CVPR.2017.497\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f065002afcb90240a41f05f138269c5675b9805\",\"title\":\"On Human Motion Prediction Using Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/7f065002afcb90240a41f05f138269c5675b9805\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Schwarz\"},{\"authorId\":null,\"name\":\"C. C. Marais\"},{\"authorId\":null,\"name\":\"T. Leyvand\"},{\"authorId\":null,\"name\":\"S. E. Hudson\"},{\"authorId\":null,\"name\":\"J. Mankoff\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Combining body pose\",\"url\":\"\",\"venue\":\"gaze, and gesture to determine intention to interact in vision-based interfaces. In Proceedings of the 32nd annual ACM conference on Human factors in computing systems, pages 3443\\u20133452. ACM\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145466134\",\"name\":\"J. Schwarz\"},{\"authorId\":\"2030314\",\"name\":\"C. Marais\"},{\"authorId\":\"3316156\",\"name\":\"Tommer Leyvand\"},{\"authorId\":\"1749296\",\"name\":\"S. Hudson\"},{\"authorId\":\"3055754\",\"name\":\"Jennifer Mankoff\"}],\"doi\":\"10.1145/2556288.2556989\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"132931961972ad2df98bba3c32ef249eb9429be6\",\"title\":\"Combining body pose, gaze, and gesture to determine intention to interact in vision-based interfaces\",\"url\":\"https://www.semanticscholar.org/paper/132931961972ad2df98bba3c32ef249eb9429be6\",\"venue\":\"CHI\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202217\",\"name\":\"I. Habibie\"},{\"authorId\":\"145273745\",\"name\":\"Daniel Holden\"},{\"authorId\":\"144735987\",\"name\":\"Jonathan Schwarz\"},{\"authorId\":\"51298335\",\"name\":\"Joe Yearsley\"},{\"authorId\":\"2254293\",\"name\":\"T. Komura\"}],\"doi\":\"10.5244/C.31.119\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8af36ff388edcf817b823990222557998993ae4\",\"title\":\"A Recurrent Variational Autoencoder for Human Motion Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/f8af36ff388edcf817b823990222557998993ae4\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"2986395\",\"name\":\"Panna Felsen\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/ICCV.2015.494\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ec7433aeb4777e7d5c903920ae945e5429d3bc4\",\"title\":\"Recurrent Network Models for Human Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/1ec7433aeb4777e7d5c903920ae945e5429d3bc4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2976930\",\"name\":\"Tero Karras\"},{\"authorId\":\"1761103\",\"name\":\"Timo Aila\"},{\"authorId\":\"36436218\",\"name\":\"S. Laine\"},{\"authorId\":\"3468872\",\"name\":\"Antti Herva\"},{\"authorId\":\"49244945\",\"name\":\"J. Lehtinen\"}],\"doi\":\"10.1145/3072959.3073658\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"title\":\"Audio-driven facial animation by joint end-to-end learning of pose and emotion\",\"url\":\"https://www.semanticscholar.org/paper/95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144549270\",\"name\":\"M. Brand\"}],\"doi\":\"10.1145/311535.311537\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2062c4359e57b22789cc38d0a97cc12acb930f43\",\"title\":\"Voice puppetry\",\"url\":\"https://www.semanticscholar.org/paper/2062c4359e57b22789cc38d0a97cc12acb930f43\",\"venue\":\"SIGGRAPH '99\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3205238\",\"name\":\"Eldar Insafutdinov\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"16576043\",\"name\":\"B. Andres\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"538542cbc07530d2c1142052792a582268ecddc7\",\"title\":\"Dense-CNN: Fully Convolutional Neural Networks for Human Body Pose Estimation\",\"url\":\"https://www.semanticscholar.org/paper/538542cbc07530d2c1142052792a582268ecddc7\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Isola A. Owens\"},{\"authorId\":null,\"name\":\"J. McDermott\"},{\"authorId\":null,\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Vnect : Real - time 3 d human pose estimation with a single rgb camera Multi - source deep learning for human pose estimation\",\"url\":\"\",\"venue\":\"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3956276\",\"name\":\"A. Dittmann\"},{\"authorId\":\"2335983\",\"name\":\"L. Llewellyn\"}],\"doi\":\"10.1037/H0027035\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3cf2ac2ea9a008f5d7a08f1fedb6dd9bfab66e79\",\"title\":\"Body movement and speech rhythm in social conversation.\",\"url\":\"https://www.semanticscholar.org/paper/3cf2ac2ea9a008f5d7a08f1fedb6dd9bfab66e79\",\"venue\":\"Journal of personality and social psychology\",\"year\":1969},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113248309\",\"name\":\"Colin Grubb\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"27efabd3992fdfdda98923e7f30e6968d2adcaa9\",\"title\":\"Multimodal Emotion Recognition\",\"url\":\"https://www.semanticscholar.org/paper/27efabd3992fdfdda98923e7f30e6968d2adcaa9\",\"venue\":\"\",\"year\":2013},{\"arxivId\":\"1408.5093\",\"authors\":[{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"3049736\",\"name\":\"S. Karayev\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1145/2647868.2654889\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bdb186ec4726e00a8051119636d4df3b94043b5\",\"title\":\"Caffe: Convolutional Architecture for Fast Feature Embedding\",\"url\":\"https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3956276\",\"name\":\"A. Dittmann\"}],\"doi\":\"10.1016/B978-0-08-015867-9.50011-3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc5241305aad2c303de12234c6b076165fc55a0c\",\"title\":\"Chapter 7 \\u2013 The Body Movement-Speech Rhythm Relationship as a Cue to Speech Encoding\",\"url\":\"https://www.semanticscholar.org/paper/bc5241305aad2c303de12234c6b076165fc55a0c\",\"venue\":\"\",\"year\":1972},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7819140\",\"name\":\"D. S. Boomer\"},{\"authorId\":\"3956276\",\"name\":\"A. Dittmann\"}],\"doi\":\"10.1097/00005053-196410000-00003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b4b6897dcc5b06350f1c6595c19163caccc6b441\",\"title\":\"SPEECH RATE, FILLED PAUSE, AND BODY MOVEMENT IN INTERVIEWS\",\"url\":\"https://www.semanticscholar.org/paper/b4b6897dcc5b06350f1c6595c19163caccc6b441\",\"venue\":\"The Journal of nervous and mental disease\",\"year\":1964},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1007/11573425_1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bc2d21ab6b79935fccf3aad37deee5176683101\",\"title\":\"Multimodal Human Computer Interaction: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7bc2d21ab6b79935fccf3aad37deee5176683101\",\"venue\":\"ICCV-HCI\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2086862\",\"name\":\"E. Haga\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"81834b26af63c4ead477884db8459344beb52ca6\",\"title\":\"Correspondences between music and body movement\",\"url\":\"https://www.semanticscholar.org/paper/81834b26af63c4ead477884db8459344beb52ca6\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145636412\",\"name\":\"Z. Fang\"},{\"authorId\":\"9422028\",\"name\":\"Zhang Guo-liang\"},{\"authorId\":\"65953874\",\"name\":\"Song Zhanjiang\"}],\"doi\":\"10.1007/BF02943243\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dacafefc695d5abb1f6ddad0690e939bec2e18ec\",\"title\":\"Comparison of different implementations of MFCC\",\"url\":\"https://www.semanticscholar.org/paper/dacafefc695d5abb1f6ddad0690e939bec2e18ec\",\"venue\":\"\",\"year\":2001},{\"arxivId\":\"1512.08512\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145358192\",\"name\":\"E. Adelson\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1109/CVPR.2016.264\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac640c2d0f33fb3ab49f37b26982948fc31e3191\",\"title\":\"Visually Indicated Sounds\",\"url\":\"https://www.semanticscholar.org/paper/ac640c2d0f33fb3ab49f37b26982948fc31e3191\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1704.07809\",\"authors\":[{\"authorId\":\"145386542\",\"name\":\"Tomas Simon\"},{\"authorId\":\"7996087\",\"name\":\"Hanbyul Joo\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"},{\"authorId\":\"1774867\",\"name\":\"Yaser Sheikh\"}],\"doi\":\"10.1109/CVPR.2017.494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a85a0c49581665015f4ce760c45d4adb289d58d\",\"title\":\"Hand Keypoint Detection in Single Images Using Multiview Bootstrapping\",\"url\":\"https://www.semanticscholar.org/paper/7a85a0c49581665015f4ce760c45d4adb289d58d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Kaiser\"},{\"authorId\":null,\"name\":\"F. Eyben\"},{\"authorId\":null,\"name\":\"B. Schuller\"},{\"authorId\":null,\"name\":\"G. Rigoll\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volutional pose machines\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2928799\",\"name\":\"Zicheng Liao\"},{\"authorId\":\"1841911\",\"name\":\"Y. Yu\"},{\"authorId\":\"1849733\",\"name\":\"Bingchen Gong\"},{\"authorId\":\"26953623\",\"name\":\"Lechao Cheng\"}],\"doi\":\"10.1145/2766966\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"137a8466203582e303366a3503dd2d3ad04cbfde\",\"title\":\"Audeosynth: Music-driven Video Montage\",\"url\":\"https://www.semanticscholar.org/paper/137a8466203582e303366a3503dd2d3ad04cbfde\",\"venue\":\"ACM Trans. Graph.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688533\",\"name\":\"C. Liu\"},{\"authorId\":\"1747779\",\"name\":\"Aaron Hertzmann\"},{\"authorId\":\"1986848\",\"name\":\"Z. Popovic\"}],\"doi\":\"10.1145/1186822.1073314\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a0c7d16a27f673e807b79483637d89b7b539cf5\",\"title\":\"Learning physics-based motion style with nonlinear inverse optimization\",\"url\":\"https://www.semanticscholar.org/paper/6a0c7d16a27f673e807b79483637d89b7b539cf5\",\"venue\":\"ACM Trans. Graph.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":\"1707.05363\",\"authors\":[{\"authorId\":\"2825680\",\"name\":\"Zimo Li\"},{\"authorId\":\"46432859\",\"name\":\"Y. Zhou\"},{\"authorId\":\"2190637\",\"name\":\"Shuangjiu Xiao\"},{\"authorId\":\"48128884\",\"name\":\"C. He\"},{\"authorId\":\"46178892\",\"name\":\"H. Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8457209a8544542bdc70fcb2f4ba45d586ddd9f6\",\"title\":\"Auto-Conditioned LSTM Network for Extended Complex Human Motion Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/8457209a8544542bdc70fcb2f4ba45d586ddd9f6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143636342\",\"name\":\"Sarah L. Taylor\"},{\"authorId\":\"2066626\",\"name\":\"T. Kim\"},{\"authorId\":\"1740159\",\"name\":\"Yisong Yue\"},{\"authorId\":\"30303590\",\"name\":\"M. Mahler\"},{\"authorId\":\"1988242\",\"name\":\"James Krahe\"},{\"authorId\":\"36969558\",\"name\":\"Anastasio Garcia Rodriguez\"},{\"authorId\":\"1788773\",\"name\":\"J. Hodgins\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.1145/3072959.3073699\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"title\":\"A deep learning approach for generalized speech animation\",\"url\":\"https://www.semanticscholar.org/paper/cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1602.00134\",\"authors\":[{\"authorId\":\"2797981\",\"name\":\"Shih-En Wei\"},{\"authorId\":\"20569810\",\"name\":\"V. Ramakrishna\"},{\"authorId\":\"1733113\",\"name\":\"T. Kanade\"},{\"authorId\":\"1774867\",\"name\":\"Yaser Sheikh\"}],\"doi\":\"10.1109/CVPR.2016.511\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"864e7db59f2ccfec1ee9f6eba79566ac7b0634df\",\"title\":\"Convolutional Pose Machines\",\"url\":\"https://www.semanticscholar.org/paper/864e7db59f2ccfec1ee9f6eba79566ac7b0634df\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1705.00053\",\"authors\":[{\"authorId\":\"143928130\",\"name\":\"J. Walker\"},{\"authorId\":\"35789996\",\"name\":\"Kenneth Marino\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1109/ICCV.2017.361\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3df8cc0384814c3fb05c44e494ced947a7d43f36\",\"title\":\"The Pose Knows: Video Forecasting by Generating Pose Futures\",\"url\":\"https://www.semanticscholar.org/paper/3df8cc0384814c3fb05c44e494ced947a7d43f36\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"}],\"doi\":\"10.1109/TPAMI.2018.2844175\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1a0912bb76777469295bb2c059faee907e7f3258\",\"title\":\"Mask R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/1a0912bb76777469295bb2c059faee907e7f3258\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1611.05358\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.367\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"title\":\"Lip Reading Sentences in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1705.01583\",\"authors\":[{\"authorId\":\"39503308\",\"name\":\"Dushyant Mehta\"},{\"authorId\":\"39612999\",\"name\":\"Srinath Sridhar\"},{\"authorId\":\"7651044\",\"name\":\"Oleksandr Sotnychenko\"},{\"authorId\":\"2933543\",\"name\":\"H. Rhodin\"},{\"authorId\":\"32776367\",\"name\":\"Mohammad Shafiei\"},{\"authorId\":\"145156858\",\"name\":\"H. Seidel\"},{\"authorId\":\"9765909\",\"name\":\"Weipeng Xu\"},{\"authorId\":\"1863006\",\"name\":\"D. Casas\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":\"10.1145/3072959.3073596\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2031b062f4c41f43a32835430b1d55a422baa564\",\"title\":\"VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera\",\"url\":\"https://www.semanticscholar.org/paper/2031b062f4c41f43a32835430b1d55a422baa564\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8422060\",\"name\":\"Yutong Ban\"},{\"authorId\":\"1780746\",\"name\":\"Laurent Girin\"},{\"authorId\":\"1382657362\",\"name\":\"Xavier Alameda-Pineda\"},{\"authorId\":\"1794229\",\"name\":\"R. Horaud\"}],\"doi\":\"10.1109/ICCVW.2017.60\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"226f07b76d6327690e5903a7d73dcb1d13dd6b46\",\"title\":\"Exploiting the Complementarity of Audio and Visual Data in Multi-speaker Tracking\",\"url\":\"https://www.semanticscholar.org/paper/226f07b76d6327690e5903a7d73dcb1d13dd6b46\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":\"1812.08008\",\"authors\":[{\"authorId\":\"47060433\",\"name\":\"Zhe Cao\"},{\"authorId\":\"2915997\",\"name\":\"T. \\u0160imon\"},{\"authorId\":\"2797981\",\"name\":\"Shih-En Wei\"},{\"authorId\":\"1774867\",\"name\":\"Yaser Sheikh\"}],\"doi\":\"10.1109/CVPR.2017.143\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e8db1519245426f3a78752a3d8360484f4626b1\",\"title\":\"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields\",\"url\":\"https://www.semanticscholar.org/paper/9e8db1519245426f3a78752a3d8360484f4626b1\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Covell C. Bregler\"},{\"authorId\":null,\"name\":\"M. Slaney\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"M . Brand . Voice puppetry\",\"url\":\"\",\"venue\":\"Proceedings of the 26 th annual conference on Computer graphics and interactive techniques , pages 21 \\u2013 28\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145273745\",\"name\":\"Daniel Holden\"},{\"authorId\":\"144152654\",\"name\":\"J. Saito\"},{\"authorId\":\"2254293\",\"name\":\"T. Komura\"}],\"doi\":\"10.1145/2897824.2925975\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f67d67ea1b416c5dd5f103f684abce9c20ef9ddb\",\"title\":\"A deep learning framework for character motion synthesis and editing\",\"url\":\"https://www.semanticscholar.org/paper/f67d67ea1b416c5dd5f103f684abce9c20ef9ddb\",\"venue\":\"ACM Trans. Graph.\",\"year\":2016},{\"arxivId\":\"1704.03432\",\"authors\":[{\"authorId\":\"2820136\",\"name\":\"Yu-Wei Chao\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"31844147\",\"name\":\"Brian L. Price\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"}],\"doi\":\"10.1109/CVPR.2017.388\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e5e25db9957411ed2ba1e60d2d34d67b4408a20\",\"title\":\"Forecasting Human Dynamics from Static Images\",\"url\":\"https://www.semanticscholar.org/paper/1e5e25db9957411ed2ba1e60d2d34d67b4408a20\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A T Dittmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The body movement-speech rhythm relationship as a cue to speech encoding. Studies in dyadic communication\",\"url\":\"\",\"venue\":\"\",\"year\":1972},{\"arxivId\":\"1605.02914\",\"authors\":[{\"authorId\":\"1882784\",\"name\":\"Vasileios Belagiannis\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/FG.2017.64\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2dd46b83a1cf5c7c811a462728d9797c270c2cb4\",\"title\":\"Recurrent Human Pose Estimation\",\"url\":\"https://www.semanticscholar.org/paper/2dd46b83a1cf5c7c811a462728d9797c270c2cb4\",\"venue\":\"2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37016781\",\"name\":\"Supasorn Suwajanakorn\"},{\"authorId\":\"1396612598\",\"name\":\"Steven M. Seitz\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\"}],\"doi\":\"10.1145/3072959.3073640\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"title\":\"Synthesizing Obama\",\"url\":\"https://www.semanticscholar.org/paper/7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1958681\",\"name\":\"M. Thompson\"},{\"authorId\":\"27469242\",\"name\":\"G. Luck\"}],\"doi\":\"10.1177/1029864911423457\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8f9734957d3b77cabe2253c62a4b1c68ffbf3cd0\",\"title\":\"Exploring relationships between pianists\\u2019 body movements, their expressive intentions, and structural elements of the music\",\"url\":\"https://www.semanticscholar.org/paper/8f9734957d3b77cabe2253c62a4b1c68ffbf3cd0\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1605.03170\",\"authors\":[{\"authorId\":\"3205238\",\"name\":\"Eldar Insafutdinov\"},{\"authorId\":\"2299109\",\"name\":\"L. Pishchulin\"},{\"authorId\":\"16576043\",\"name\":\"B. Andres\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-46466-4_3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2f4cae1acba37426372718fc30745055c8c2140\",\"title\":\"DeeperCut: A Deeper, Stronger, and Faster Multi-person Pose Estimation Model\",\"url\":\"https://www.semanticscholar.org/paper/a2f4cae1acba37426372718fc30745055c8c2140\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2428034\",\"name\":\"C. Bregler\"},{\"authorId\":\"1800748\",\"name\":\"M. Covell\"},{\"authorId\":\"145290352\",\"name\":\"M. Slaney\"}],\"doi\":\"10.1145/258734.258880\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a78995510cf33edf0ee4265abe23ffdc55986cb\",\"title\":\"Video Rewrite: driving visual speech with audio\",\"url\":\"https://www.semanticscholar.org/paper/3a78995510cf33edf0ee4265abe23ffdc55986cb\",\"venue\":\"SIGGRAPH '97\",\"year\":1997},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ee98071da172de6e1d6d1fcd560bad9e0a87d5e\",\"title\":\"Rec. ITU-R BT.1359-1 1 RECOMMENDATION ITU-R BT.1359-1 RELATIVE TIMING OF SOUND AND VISION FOR BROADCASTING\",\"url\":\"https://www.semanticscholar.org/paper/0ee98071da172de6e1d6d1fcd560bad9e0a87d5e\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"L.S.-H. Chen\"},{\"authorId\":null,\"name\":\"T. S. Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Joint processing of audiovisual information for the recognition of emotional expressions in human-computer interaction\",\"url\":\"\",\"venue\":\"University of Illinois at Urbana-Champaign\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103575\",\"name\":\"M. W\\u00f6llmer\"},{\"authorId\":\"40365624\",\"name\":\"M. Kaiser\"},{\"authorId\":\"1751126\",\"name\":\"F. Eyben\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"},{\"authorId\":\"145512909\",\"name\":\"G. Rigoll\"}],\"doi\":\"10.1016/j.imavis.2012.03.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"752c6738a1ef1c18943636bc3c98a4dbd6455b33\",\"title\":\"LSTM-Modeling of continuous emotions in an audiovisual affect recognition framework\",\"url\":\"https://www.semanticscholar.org/paper/752c6738a1ef1c18943636bc3c98a4dbd6455b33\",\"venue\":\"Image Vis. Comput.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. S. Boomer\"},{\"authorId\":null,\"name\":\"A. T. Dittmann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Speech rate\",\"url\":\"\",\"venue\":\"filled pause, and body movement in interviews. The Journal of nervous and mental disease, 139(4):324\\u2013327\",\"year\":1964},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. R. Thompson\"},{\"authorId\":null,\"name\":\"G. Luck\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Exploring relationships between pianists body movements\",\"url\":\"\",\"venue\":\"their expressive intentions, and structural elements of the music. Musicae Scientiae, 16(1):19\\u201340\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"N Sebe\"},{\"authorId\":null,\"name\":\"I Cohen\"},{\"authorId\":null,\"name\":\"T S Huang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Multimodal emotion recognition. Handbook of Pattern Recognition and Computer Vision\",\"url\":\"\",\"venue\":\"\",\"year\":2005}],\"title\":\"Audio to Body Dynamics\",\"topics\":[{\"topic\":\"Avatar (computing)\",\"topicId\":\"92945\",\"url\":\"https://www.semanticscholar.org/topic/92945\"},{\"topic\":\"Internet\",\"topicId\":\"7952\",\"url\":\"https://www.semanticscholar.org/topic/7952\"},{\"topic\":\"Long short-term memory\",\"topicId\":\"117199\",\"url\":\"https://www.semanticscholar.org/topic/117199\"}],\"url\":\"https://www.semanticscholar.org/paper/c6d60aaad68fa78c914ee34c26bceab033a88622\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"