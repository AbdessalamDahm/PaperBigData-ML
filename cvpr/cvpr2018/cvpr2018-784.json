"{\"abstract\":\"Video captioning which automatically translates video clips into natural language sentences is a very important task in computer vision. By virtue of recent deep learning technologies, video captioning has made great progress. However, learning an effective mapping from the visual sequence space to the language space is still a challenging problem due to the long-term multimodal dependency modelling and semantic misalignment. Inspired by the facts that memory modelling poses potential advantages to long-term sequential problems [35] and working memory is the key factor of visual attention [33], we propose a Multimodal Memory Model (M3) to describe videos, which builds a visual and textual shared memory to model the long-term visual-textual dependency and further guide visual attention on described visual targets to solve visual-textual alignments. Specifically, similar to [10], the proposed M3 attaches an external memory to store and retrieve both visual and textual contents by interacting with video and sentence with multiple read and write operations. To evaluate the proposed model, we perform experiments on two public datasets: MSVD and MSR-VTT. The experimental results demonstrate that our method outperforms most of the state-of-the-art methods in terms of BLEU and METEOR.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"46583706\",\"name\":\"J. Wang\",\"url\":\"https://www.semanticscholar.org/author/46583706\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\",\"url\":\"https://www.semanticscholar.org/author/145200778\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\",\"url\":\"https://www.semanticscholar.org/author/49867037\"},{\"authorId\":\"40476140\",\"name\":\"Liang Wang\",\"url\":\"https://www.semanticscholar.org/author/40476140\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\",\"url\":\"https://www.semanticscholar.org/author/143874948\"}],\"citationVelocity\":17,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8194130\",\"name\":\"Qinyu Li\"}],\"doi\":\"10.1145/3303083\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"91aa0eb38446643cd622b060a76043b0ca2d7991\",\"title\":\"Rich Visual and Language Representation with Complementary Semantics for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/91aa0eb38446643cd622b060a76043b0ca2d7991\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38397846\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"50857536\",\"name\":\"Z. Wang\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"},{\"authorId\":\"11675891\",\"name\":\"D. Feng\"},{\"authorId\":\"145808910\",\"name\":\"Tieniu Tan\"}],\"doi\":\"10.1145/3343031.3350992\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5a402ff486216270c38ee72f793e49e6f4b861e1\",\"title\":\"Stacked Memory Network for Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/5a402ff486216270c38ee72f793e49e6f4b861e1\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"153489843\",\"name\":\"T. Chen\"},{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"145909567\",\"name\":\"Wanli Yu\"}],\"doi\":\"10.1109/ACCESS.2020.3010872\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"title\":\"Attention-Based Convolutional LSTM for Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144669461\",\"name\":\"Kuncheng Fang\"},{\"authorId\":\"144913277\",\"name\":\"Lian Zhou\"},{\"authorId\":\"145020731\",\"name\":\"Cheng Jin\"},{\"authorId\":\"7550713\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"35632219\",\"name\":\"Kangnian Weng\"},{\"authorId\":\"1689115\",\"name\":\"Tao Zhang\"},{\"authorId\":\"145631869\",\"name\":\"W. Fan\"}],\"doi\":\"10.1609/AAAI.V33I01.33018271\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"title\":\"Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention\",\"url\":\"https://www.semanticscholar.org/paper/506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2002.11886\",\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5dff9ae50c0aadbd99ca59ff70425f63213243e\",\"title\":\"Hierarchical Memory Decoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a5dff9ae50c0aadbd99ca59ff70425f63213243e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1902.10322\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1109/CVPR.2019.01277\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"20888a7aebaf77a306c0886f165bd0d468db806d\",\"title\":\"Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20888a7aebaf77a306c0886f165bd0d468db806d\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.04375\",\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/CVPR.2019.00852\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"title\":\"Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143780023\",\"name\":\"Zhong Ji\"},{\"authorId\":\"11179553\",\"name\":\"Zhigang Lin\"},{\"authorId\":\"1500530481\",\"name\":\"Haoran Wang\"},{\"authorId\":\"101084939\",\"name\":\"Y. He\"}],\"doi\":\"10.1109/ACCESS.2020.2975594\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"284cd0512ecd5d7cec335b0038444085398ebaf5\",\"title\":\"Multi-Modal Memory Enhancement Attention Network for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/284cd0512ecd5d7cec335b0038444085398ebaf5\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"1693997\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1145/3240508.3240538\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72f9116a04e584081635500e9f0789fa26e4d15f\",\"title\":\"Hierarchical Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72f9116a04e584081635500e9f0789fa26e4d15f\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1908.10072\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"67074535\",\"name\":\"W. Zhang\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"46641690\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/ICCV.2019.00273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"title\":\"Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.11451\",\"authors\":[{\"authorId\":\"3310120\",\"name\":\"Ali Diba\"},{\"authorId\":\"143794218\",\"name\":\"M. Fayyaz\"},{\"authorId\":\"145878079\",\"name\":\"Vivek Sharma\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"1576263143\",\"name\":\"Jurgen Gall\"},{\"authorId\":\"49157259\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-030-58558-7_35\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c2fe9e87e31e6fdb2b7bae5f9cc3c2d2612d13a\",\"title\":\"Large Scale Holistic Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/1c2fe9e87e31e6fdb2b7bae5f9cc3c2d2612d13a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3239379\",\"name\":\"J. Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1007/978-3-030-05710-7_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"title\":\"Hierarchical Vision-Language Alignment for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938051940\",\"name\":\"Dylan Flaute\"},{\"authorId\":\"2405109\",\"name\":\"B. Narayanan\"}],\"doi\":\"10.1117/12.2568016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"title\":\"Video captioning using weakly supervised convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"1974929\",\"name\":\"Jiewu Xia\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1007/s11042-020-09674-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"title\":\"Double-channel language feature mining based model for video description\",\"url\":\"https://www.semanticscholar.org/paper/0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"venue\":\"Multim. Tools Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46276803\",\"name\":\"J. Li\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1016/j.jvcir.2020.102875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"title\":\"Translating video into language by enhancing visual and language representations\",\"url\":\"https://www.semanticscholar.org/paper/32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"1907.12905\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":\"10.1145/3343031.3351060\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"title\":\"Watch It Twice: Video Captioning with a Refocused Video Encoder\",\"url\":\"https://www.semanticscholar.org/paper/5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144368930\",\"name\":\"Yan Huang\"},{\"authorId\":\"144143336\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00587\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54f5c5e1f80000e1da9c6b182e0e76f6e61c0ad5\",\"title\":\"ACMM: Aligned Cross-Modal Memory for Few-Shot Image and Sentence Matching\",\"url\":\"https://www.semanticscholar.org/paper/54f5c5e1f80000e1da9c6b182e0e76f6e61c0ad5\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46842113\",\"name\":\"S. Muralidharan\"},{\"authorId\":null,\"name\":\"smuralid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"04d46aa5f75b7bee55f173cc76610643d755e154\",\"title\":\"Memory Augmented Recurrent Neural Networks for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04d46aa5f75b7bee55f173cc76610643d755e154\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.07335\",\"authors\":[{\"authorId\":\"16286752\",\"name\":\"Md. Mushfiqur Rahman\"},{\"authorId\":\"1945520590\",\"name\":\"Thasin Abedin\"},{\"authorId\":\"1945917684\",\"name\":\"Khondokar S. S. Prottoy\"},{\"authorId\":\"1945917586\",\"name\":\"Ayana Moshruba\"},{\"authorId\":\"32826273\",\"name\":\"F. H. Siddiqui\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2a0c5a1d5ae87ae734da2ef9c5eac6b2146536b\",\"title\":\"Semantically Sensible Video Captioning (SSVC)\",\"url\":\"https://www.semanticscholar.org/paper/e2a0c5a1d5ae87ae734da2ef9c5eac6b2146536b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.09345\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"1747500\",\"name\":\"A. Mian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"title\":\"Empirical Autopsy of Deep Video Captioning Frameworks\",\"url\":\"https://www.semanticscholar.org/paper/62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TMM.2019.2896515\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"title\":\"Generating Video Descriptions With Latent Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"1905.01077\",\"authors\":[{\"authorId\":\"50763020\",\"name\":\"Jingwen Chen\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1609/aaai.v33i01.33018167\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d40892479541c2d173c836534e6fb2acb597de49\",\"title\":\"Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d40892479541c2d173c836534e6fb2acb597de49\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"},{\"authorId\":\"145952554\",\"name\":\"Michael Chan\"},{\"authorId\":\"28130078\",\"name\":\"P. P. Liang\"},{\"authorId\":\"31726556\",\"name\":\"Edmund Tong\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/CVPR.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"title\":\"Social-IQ: A Question Answering Benchmark for Artificial Social Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e17aac3bbe0aabda715cafad392f31a1e046c17c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/TIP.2020.2988435\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"title\":\"Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36251013\",\"name\":\"Wei Li\"},{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1016/j.patrec.2017.10.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ba881ec9ed2b435468ba6bbdc1821bde7778417\",\"title\":\"Multimodal architecture for video captioning with memory networks and an attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/0ba881ec9ed2b435468ba6bbdc1821bde7778417\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92104112\",\"name\":\"Tangming Chen\"},{\"authorId\":\"9398015\",\"name\":\"Qike Zhao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1007/978-3-030-33982-1_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cf546bfb933bddbd9e7017525d9621405e549b9\",\"title\":\"Boundary Detector Encoder and Decoder with Soft Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1cf546bfb933bddbd9e7017525d9621405e549b9\",\"venue\":\"APWeb/WAIM Workshops\",\"year\":2019},{\"arxivId\":\"1905.02963\",\"authors\":[{\"authorId\":\"145114776\",\"name\":\"L. Sun\"},{\"authorId\":\"143721383\",\"name\":\"Bing Li\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":\"10.1109/ICME.2019.00226\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"title\":\"Multimodal Semantic Attention Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390818869\",\"name\":\"Jinlei Xu\"},{\"authorId\":\"144546140\",\"name\":\"T. Xu\"},{\"authorId\":\"123432231\",\"name\":\"Xin Tian\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144911521\",\"name\":\"Y. Ji\"}],\"doi\":\"10.1109/IJCNN.2019.8851897\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf7b38dd24c20223e006066be4202d1da700af37\",\"title\":\"Context Gating with Short Temporal Information for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf7b38dd24c20223e006066be4202d1da700af37\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49868702\",\"name\":\"Ran Wei\"},{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1016/j.jvcir.2020.102751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"title\":\"Exploiting the local temporal information for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":\"10.1145/3347449.3357484\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"989730c00381805543baa470a2d6490cc5354a13\",\"title\":\"L-STAP: Learned Spatio-Temporal Adaptive Pooling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/989730c00381805543baa470a2d6490cc5354a13\",\"venue\":\"AI4TV@MM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387548078\",\"name\":\"K. Lin\"},{\"authorId\":\"1738276592\",\"name\":\"Zhuoxin Gan\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.98\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3263b941d0a77bbd2040612ec774ef063ef64c48\",\"title\":\"Semi-Supervised Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3263b941d0a77bbd2040612ec774ef063ef64c48\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2002.11566\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":null,\"name\":\"Bing Li\"},{\"authorId\":\"39397292\",\"name\":\"Peijin Wang\"},{\"authorId\":\"48594951\",\"name\":\"Weiming Hu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1109/cvpr42600.2020.01329\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1dd557a8839733a5ee06d19989a265e61f603c1\",\"title\":\"Object Relational Graph With Teacher-Recommended Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1dd557a8839733a5ee06d19989a265e61f603c1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2011.07307\",\"authors\":[{\"authorId\":\"47357643\",\"name\":\"Shen Gao\"},{\"authorId\":\"46772896\",\"name\":\"Xiuying Chen\"},{\"authorId\":\"2780667\",\"name\":\"Z. Ren\"},{\"authorId\":\"144060461\",\"name\":\"Dongyan Zhao\"},{\"authorId\":\"1399646334\",\"name\":\"Rui Yan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d667e9bee49863ac84c570b61274f9668a3a0c2\",\"title\":\"Meaningful Answer Generation of E-Commerce Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/0d667e9bee49863ac84c570b61274f9668a3a0c2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1609/AAAI.V34I07.6731\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"4df184d6a74f1ffd84b644735c9afb5060552770\",\"title\":\"Joint Commonsense and Relation Reasoning for Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4df184d6a74f1ffd84b644735c9afb5060552770\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50846763\",\"name\":\"W. Zhang\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"1993631009\",\"name\":\"Haizhou Shi\"},{\"authorId\":\"1490931889\",\"name\":\"Haochen Shi\"},{\"authorId\":\"145974114\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"49336516\",\"name\":\"W. Wang\"}],\"doi\":\"10.1145/3394171.3413746\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fa8f78e665c037da28a7e35a98c4d2521e3f12e\",\"title\":\"Relational Graph Learning for Grounded Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/9fa8f78e665c037da28a7e35a98c4d2521e3f12e\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97514733\",\"name\":\"Jin-woo Choi\"},{\"authorId\":\"2515597\",\"name\":\"Gaurav Sharma\"},{\"authorId\":\"1790643\",\"name\":\"S. Schulter\"},{\"authorId\":\"50535349\",\"name\":\"J. Huang\"}],\"doi\":\"10.1007/978-3-030-58610-2_40\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"19d574a2238ad11142de1d6f2713315880b2d218\",\"title\":\"Shuffle and Attend: Video Domain Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/19d574a2238ad11142de1d6f2713315880b2d218\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1813915\",\"name\":\"S. Liu\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3240508.3240667\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"849642b4701ac11c035326069f707f23a51a6f1a\",\"title\":\"SibNet: Sibling Convolutional Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/849642b4701ac11c035326069f707f23a51a6f1a\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"1790251284\",\"name\":\"Lin Li\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"9594118\",\"name\":\"C. Gu\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/s11063-020-10352-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"title\":\"Adaptively Converting Auxiliary Attributes and Textual Embedding for Video Captioning Based on BiLSTM\",\"url\":\"https://www.semanticscholar.org/paper/3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"venue\":\"Neural Process. Lett.\",\"year\":2020},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020}],\"corpusId\":53871875,\"doi\":\"10.1109/CVPR.2018.00784\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":3,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"b910a6f687a4e56062dc326786cee297bd60e8c1\",\"references\":[{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1685771\",\"name\":\"P. Blunsom\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"944a1cfd79dbfb6fef460360a0765ba790f4027a\",\"title\":\"Recurrent Continuous Translation Models\",\"url\":\"https://www.semanticscholar.org/paper/944a1cfd79dbfb6fef460360a0765ba790f4027a\",\"venue\":\"EMNLP\",\"year\":2013},{\"arxivId\":\"1611.09312\",\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"153925540\",\"name\":\"C. Grana\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2017.339\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"title\":\"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1611.06492\",\"authors\":[{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"34762956\",\"name\":\"Abhinav Agarwalla\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"144240262\",\"name\":\"P. Mitra\"}],\"doi\":\"10.1109/CVPRW.2017.273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"title\":\"Recurrent Memory Addressing for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":\"36880488\",\"name\":\"Cheng Chen\"},{\"authorId\":\"1717863\",\"name\":\"Yizhou Wang\"},{\"authorId\":\"40570471\",\"name\":\"T. Jiang\"},{\"authorId\":\"143816774\",\"name\":\"Fang Fang\"},{\"authorId\":\"144779803\",\"name\":\"Y. Yao\"}],\"doi\":\"10.1109/CVPR.2011.5995423\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c28041fa976c32d6ef7ca77d85b0274cc87f3360\",\"title\":\"Simulating human saccadic scanpaths on natural images\",\"url\":\"https://www.semanticscholar.org/paper/c28041fa976c32d6ef7ca77d85b0274cc87f3360\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":\"1410.5401\",\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"89504302\",\"name\":\"G. Wayne\"},{\"authorId\":\"1841008\",\"name\":\"Ivo Danihelka\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c3823aacea60bc1f2cabb9283144690a3d015db5\",\"title\":\"Neural Turing Machines\",\"url\":\"https://www.semanticscholar.org/paper/c3823aacea60bc1f2cabb9283144690a3d015db5\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b9f8101c61b415f946625b69f69fc9e3d0d6fc4\",\"title\":\"Generating Natural-Language Video Descriptions Using Text-Mined Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/3b9f8101c61b415f946625b69f69fc9e3d0d6fc4\",\"venue\":\"AAAI\",\"year\":2013},{\"arxivId\":\"1503.01007\",\"authors\":[{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d38e8631bba0720becdaf7b89f79d9f9dca45d82\",\"title\":\"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets\",\"url\":\"https://www.semanticscholar.org/paper/d38e8631bba0720becdaf7b89f79d9f9dca45d82\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1410.3916\",\"authors\":[{\"authorId\":\"145183709\",\"name\":\"J. Weston\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"71ae756c75ac89e2d731c9c79649562b5768ff39\",\"title\":\"Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/71ae756c75ac89e2d731c9c79649562b5768ff39\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.06432\",\"authors\":[{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"title\":\"Delving Deeper into Convolutional Networks for Learning Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":\"1409.2329\",\"authors\":[{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97\",\"title\":\"Recurrent Neural Network Regularization\",\"url\":\"https://www.semanticscholar.org/paper/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1212.5701\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8729441d734782c3ed532a7d2d9611b438c0a09a\",\"title\":\"ADADELTA: An Adaptive Learning Rate Method\",\"url\":\"https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":\"1609.06647\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"48818137\",\"name\":\"Samy Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/TPAMI.2016.2587640\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e\",\"title\":\"Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning Challenge\",\"url\":\"https://www.semanticscholar.org/paper/62f74d3aaf9e86633e4d88b04a6d04ca93e8b81e\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1506.02516\",\"authors\":[{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"2910877\",\"name\":\"K. Hermann\"},{\"authorId\":\"2573615\",\"name\":\"Mustafa Suleyman\"},{\"authorId\":\"1685771\",\"name\":\"P. Blunsom\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e837b79de602c69395498c1fbbe39bbb4e6f75ad\",\"title\":\"Learning to Transduce with Unbounded Memory\",\"url\":\"https://www.semanticscholar.org/paper/e837b79de602c69395498c1fbbe39bbb4e6f75ad\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"title\":\"Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"venue\":\"COLING\",\"year\":2014},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1610.06258\",\"authors\":[{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"2273228\",\"name\":\"Catalin Ionescu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c91ae35dbcb6d479580ecd235eabf98374acdb55\",\"title\":\"Using Fast Weights to Attend to the Recent Past\",\"url\":\"https://www.semanticscholar.org/paper/c91ae35dbcb6d479580ecd235eabf98374acdb55\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1311.2524\",\"authors\":[{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"153652146\",\"name\":\"J. Malik\"}],\"doi\":\"10.1109/CVPR.2014.81\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f4df08d9072fc2ac181b7fced6a245315ce05c8\",\"title\":\"Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/2f4df08d9072fc2ac181b7fced6a245315ce05c8\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1506.02075\",\"authors\":[{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6e565308c8081e807709cb4a917443b737e6cdb4\",\"title\":\"Large-scale Simple Question Answering with Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/6e565308c8081e807709cb4a917443b737e6cdb4\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1603.01417\",\"authors\":[{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"3375440\",\"name\":\"Stephen Merity\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f96898d15a1bf1fa8925b1280d0e07a7a8e72194\",\"title\":\"Dynamic Memory Networks for Visual and Textual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f96898d15a1bf1fa8925b1280d0e07a7a8e72194\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":\"1409.4842\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"46641766\",\"name\":\"W. Liu\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"48840704\",\"name\":\"Scott Reed\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"}],\"doi\":\"10.1109/CVPR.2015.7298594\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"title\":\"Going deeper with convolutions\",\"url\":\"https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1611.02261\",\"authors\":[{\"authorId\":\"2915023\",\"name\":\"Rasool Fakoor\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1738740\",\"name\":\"S. B. Kang\"},{\"authorId\":\"143967473\",\"name\":\"Pushmeet Kohli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"title\":\"Memory-augmented Attention Modelling for Videos\",\"url\":\"https://www.semanticscholar.org/paper/249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"B. Jimmy\"},{\"authorId\":null,\"name\":\"G. E. Hinton\"},{\"authorId\":null,\"name\":\"V. Mnih\"},{\"authorId\":null,\"name\":\"J. Leibo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and C\",\"url\":\"\",\"venue\":\"lonescu. Using fast weights to attend to the recent past. In NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1512.00567\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"103590098\",\"name\":\"Jon Shlens\"},{\"authorId\":\"3282833\",\"name\":\"Z. Wojna\"}],\"doi\":\"10.1109/CVPR.2016.308\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"23ffaa0fe06eae05817f527a47ac3291077f9e58\",\"title\":\"Rethinking the Inception Architecture for Computer Vision\",\"url\":\"https://www.semanticscholar.org/paper/23ffaa0fe06eae05817f527a47ac3291077f9e58\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016}],\"title\":\"M3: Multimodal Memory Modelling for Video Captioning\",\"topics\":[{\"topic\":\"Multimodal interaction\",\"topicId\":\"42592\",\"url\":\"https://www.semanticscholar.org/topic/42592\"},{\"topic\":\"Computer vision\",\"topicId\":\"5332\",\"url\":\"https://www.semanticscholar.org/topic/5332\"},{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"},{\"topic\":\"Meteor\",\"topicId\":\"131649\",\"url\":\"https://www.semanticscholar.org/topic/131649\"},{\"topic\":\"Shared memory\",\"topicId\":\"8934\",\"url\":\"https://www.semanticscholar.org/topic/8934\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"BLEU\",\"topicId\":\"250421\",\"url\":\"https://www.semanticscholar.org/topic/250421\"},{\"topic\":\"Video clip\",\"topicId\":\"30493\",\"url\":\"https://www.semanticscholar.org/topic/30493\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Microsoft Research\",\"topicId\":\"73897\",\"url\":\"https://www.semanticscholar.org/topic/73897\"},{\"topic\":\"Memory model (programming)\",\"topicId\":\"118507\",\"url\":\"https://www.semanticscholar.org/topic/118507\"}],\"url\":\"https://www.semanticscholar.org/paper/b910a6f687a4e56062dc326786cee297bd60e8c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"