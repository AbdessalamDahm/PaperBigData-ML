"{\"abstract\":\"Taking a photo outside, can we predict the immediate future, e.g., how would the cloud move in the sky? We address this problem by presenting a generative adversarial network (GAN) based two-stage approach to generating realistic time-lapse videos of high resolution. Given the first frame, our model learns to generate long-term future frames. The first stage generates videos of realistic contents for each frame. The second stage refines the generated video from the first stage by enforcing it to be closer to real videos with regard to motion dynamics. To further encourage vivid motion in the final generated video, Gram matrix is employed to model the motion more precisely. We build a large scale time-lapse dataset, and test our approach on this new dataset. Using our model, we are able to generate realistic videos of up to 128 \\u00c3\\u2014 128 resolution for 32 frames. Quantitative and qualitative experiment results demonstrate the superiority of our model over the state-of-the-art models.\",\"arxivId\":\"1709.07592\",\"authors\":[{\"authorId\":\"39272336\",\"name\":\"W. Xiong\",\"url\":\"https://www.semanticscholar.org/author/39272336\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\",\"url\":\"https://www.semanticscholar.org/author/145909988\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\",\"url\":\"https://www.semanticscholar.org/author/145698310\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\",\"url\":\"https://www.semanticscholar.org/author/46641573\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\",\"url\":\"https://www.semanticscholar.org/author/33642939\"}],\"citationVelocity\":21,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"47845209\",\"name\":\"Zihao Yan\"},{\"authorId\":\"2154334\",\"name\":\"Ruizhen Hu\"},{\"authorId\":\"66060467\",\"name\":\"Xingguang Yan\"},{\"authorId\":\"49329812\",\"name\":\"L. Chen\"},{\"authorId\":\"3276873\",\"name\":\"O. V. Kaick\"},{\"authorId\":\"114464478\",\"name\":\"Hao Zhang\"},{\"authorId\":\"40586368\",\"name\":\"Hui Huang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4b9e893a1825f9106b7232ddbdfa34d32a32802a\",\"title\":\"RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud\",\"url\":\"https://www.semanticscholar.org/paper/4b9e893a1825f9106b7232ddbdfa34d32a32802a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152831141\",\"name\":\"Pauline Luc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ae02e601eae125ce137324c678ab68e9ab272ea0\",\"title\":\"Self-supervised learning of predictive segmentation models from video\",\"url\":\"https://www.semanticscholar.org/paper/ae02e601eae125ce137324c678ab68e9ab272ea0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2001.06590\",\"authors\":[{\"authorId\":\"47767812\",\"name\":\"Lirong Wu\"},{\"authorId\":\"47942157\",\"name\":\"K. Huang\"},{\"authorId\":\"1888007\",\"name\":\"Haibin Shen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1109/tcsvt.2020.3027741\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8d1468af0c6e3717803bdeb485f38eabc959021e\",\"title\":\"A Foreground-background Parallel Compression with Residual Encoding for Surveillance Video\",\"url\":\"https://www.semanticscholar.org/paper/8d1468af0c6e3717803bdeb485f38eabc959021e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1638059605\",\"name\":\"Yuki Endo\"},{\"authorId\":\"2504432\",\"name\":\"Y. Kanamori\"},{\"authorId\":\"36375845\",\"name\":\"Shigeru Kuriyama\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ab351fa2be0a268742905b04f9ce0f0369139a41\",\"title\":\"Animating Landscape: Self-Supervised Learning of Decoupled Motion and Appearance for Single-Image Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/ab351fa2be0a268742905b04f9ce0f0369139a41\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1811.10699\",\"authors\":[{\"authorId\":\"50415746\",\"name\":\"Shruti Vyas\"},{\"authorId\":\"2116440\",\"name\":\"Y. Rawat\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fc941fbbc906bf7834a21eea520e2d2277aa57f5\",\"title\":\"Time-Aware and View-Aware Video Rendering for Unsupervised Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/fc941fbbc906bf7834a21eea520e2d2277aa57f5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1807.09951\",\"authors\":[{\"authorId\":\"48096253\",\"name\":\"Long Zhao\"},{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"6812347\",\"name\":\"Yu Tian\"},{\"authorId\":\"143980996\",\"name\":\"Mubbasir Kapadia\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1007/978-3-030-01267-0_24\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ec5cdb68fa5a1dc011c9df01e45a0a1c4d59d110\",\"title\":\"Learning to Forecast and Refine Residual Motion for Image-to-Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/ec5cdb68fa5a1dc011c9df01e45a0a1c4d59d110\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"2388085\",\"name\":\"M. K\\u00f6rner\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0fc9b202107bafa4b755c913c904d8ab046b8113\",\"title\":\"FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing Autoencoder GANs\",\"url\":\"https://www.semanticscholar.org/paper/0fc9b202107bafa4b755c913c904d8ab046b8113\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2008.09646\",\"authors\":[{\"authorId\":\"1581479411\",\"name\":\"Abhinav Sagar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b367a582b8921997bbbea433bf99c4671568b44c\",\"title\":\"HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN\",\"url\":\"https://www.semanticscholar.org/paper/b367a582b8921997bbbea433bf99c4671568b44c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.09655\",\"authors\":[{\"authorId\":\"145879692\",\"name\":\"E. Logacheva\"},{\"authorId\":\"1956107\",\"name\":\"R. Suvorov\"},{\"authorId\":\"50168812\",\"name\":\"Oleg Khomenko\"},{\"authorId\":\"51995877\",\"name\":\"A. Mashikhin\"},{\"authorId\":\"1740145\",\"name\":\"V. Lempitsky\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7594d6e232599e7f792e416a420a4102435ce631\",\"title\":\"DeepLandscape: Adversarial Modeling of Landscape Video\",\"url\":\"https://www.semanticscholar.org/paper/7594d6e232599e7f792e416a420a4102435ce631\",\"venue\":\"ECCV 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35136213\",\"name\":\"S. Adiga\"},{\"authorId\":\"38524969\",\"name\":\"M. Attia\"},{\"authorId\":\"3339244\",\"name\":\"Wei-Ting Chang\"},{\"authorId\":\"2938360\",\"name\":\"R. Tandon\"}],\"doi\":\"10.1109/GlobalSIP.2018.8646478\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f06b52db384eddb91a28d5b95f44970848cd2613\",\"title\":\"ON THE TRADEOFF BETWEEN MODE COLLAPSE AND SAMPLE QUALITY IN GENERATIVE ADVERSARIAL NETWORKS\",\"url\":\"https://www.semanticscholar.org/paper/f06b52db384eddb91a28d5b95f44970848cd2613\",\"venue\":\"2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1396380539\",\"name\":\"Liang Gonog\"},{\"authorId\":\"143959713\",\"name\":\"Yimin Zhou\"}],\"doi\":\"10.1109/ICIEA.2019.8833686\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c124d5c6ce8e1d423d05dff3bb9d48d1b1094310\",\"title\":\"A Review: Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/c124d5c6ce8e1d423d05dff3bb9d48d1b1094310\",\"venue\":\"2019 14th IEEE Conference on Industrial Electronics and Applications (ICIEA)\",\"year\":2019},{\"arxivId\":\"2007.07978\",\"authors\":[{\"authorId\":\"120708124\",\"name\":\"A. H. Nielsen\"},{\"authorId\":\"3074923\",\"name\":\"Alexandros Iosifidis\"},{\"authorId\":\"2550309\",\"name\":\"H. Karstoft\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"079a9ebd58e0a7095cf4cbaae5adde07b11c170b\",\"title\":\"CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds\",\"url\":\"https://www.semanticscholar.org/paper/079a9ebd58e0a7095cf4cbaae5adde07b11c170b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82342433\",\"name\":\"G. Jin\"},{\"authorId\":\"47599111\",\"name\":\"Q. Wang\"},{\"authorId\":\"143850511\",\"name\":\"X. Zhao\"},{\"authorId\":\"3038982\",\"name\":\"Yanghe Feng\"},{\"authorId\":\"145433266\",\"name\":\"Q. Cheng\"},{\"authorId\":\"2078113\",\"name\":\"Jincai Huang\"}],\"doi\":\"10.1109/BigData47090.2019.9006388\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4fad3475af250a85bd34792030711dc207b37794\",\"title\":\"Crime-GAN: A Context-based Sequence Generative Network for Crime Forecasting with Adversarial Loss\",\"url\":\"https://www.semanticscholar.org/paper/4fad3475af250a85bd34792030711dc207b37794\",\"venue\":\"2019 IEEE International Conference on Big Data (Big Data)\",\"year\":2019},{\"arxivId\":\"1904.09412\",\"authors\":[{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018263\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c970f99e844f774236511a40bf43b8950dde339f\",\"title\":\"Cubic LSTMs for Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c970f99e844f774236511a40bf43b8950dde339f\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48915513\",\"name\":\"P. Johnston\"},{\"authorId\":\"1807106\",\"name\":\"Eyad Elyan\"}],\"doi\":\"10.1016/J.DIIN.2019.03.006\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd5080bc7bc45a40ed55b3f3c809dbba44698a5d\",\"title\":\"A review of digital video tampering: From simple editing to full synthesis\",\"url\":\"https://www.semanticscholar.org/paper/cd5080bc7bc45a40ed55b3f3c809dbba44698a5d\",\"venue\":\"Digit. Investig.\",\"year\":2019},{\"arxivId\":\"1804.00113\",\"authors\":[{\"authorId\":\"143905981\",\"name\":\"Baoyuan Wu\"},{\"authorId\":\"33349597\",\"name\":\"Weidong Chen\"},{\"authorId\":\"145551072\",\"name\":\"Peng Sun\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"1794837\",\"name\":\"Siwei Lyu\"}],\"doi\":\"10.1109/CVPR.2018.00831\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ba7fb86657a2ee18577af26ee407723e97353d8\",\"title\":\"Tagging Like Humans: Diverse and Distinct Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/4ba7fb86657a2ee18577af26ee407723e97353d8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1810.01325\",\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"119567230\",\"name\":\"Marco Korner\"}],\"doi\":\"10.5194/isprs-archives-xlii-2-w16-3-2019\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947\",\"title\":\"FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing GANs\",\"url\":\"https://www.semanticscholar.org/paper/b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"1860612\",\"name\":\"X. Yang\"},{\"authorId\":\"33768582\",\"name\":\"Yongqiang Tang\"},{\"authorId\":\"40538957\",\"name\":\"Wensheng Zhang\"}],\"doi\":\"10.1109/LGRS.2019.2922326\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1ae5b2ef9e7043f3c7015de386d85d42718dc3c3\",\"title\":\"Learning to Generate Radar Image Sequences Using Two-Stage Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/1ae5b2ef9e7043f3c7015de386d85d42718dc3c3\",\"venue\":\"IEEE Geoscience and Remote Sensing Letters\",\"year\":2020},{\"arxivId\":\"1910.06699\",\"authors\":[{\"authorId\":\"37868227\",\"name\":\"C. D. Souza\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"3407519\",\"name\":\"Y. Cabon\"},{\"authorId\":\"26734366\",\"name\":\"N. Murray\"},{\"authorId\":\"3940956\",\"name\":\"A. Pe\\u00f1a\"}],\"doi\":\"10.1007/s11263-019-01222-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"099bdf9cf1c59c7f5ed201759157fb505f51a762\",\"title\":\"Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models\",\"url\":\"https://www.semanticscholar.org/paper/099bdf9cf1c59c7f5ed201759157fb505f51a762\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3397429\",\"name\":\"K. Zhang\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\"},{\"authorId\":\"144746940\",\"name\":\"B. Stenger\"},{\"authorId\":\"144850642\",\"name\":\"W. Ren\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"40124570\",\"name\":\"Hongdong Li\"}],\"doi\":\"10.1145/3394171.3413929\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3bd0816668fc28af82eb52db2f5d92cbc886642\",\"title\":\"Every Moment Matters: Detail-Aware Networks to Bring a Blurry Image Alive\",\"url\":\"https://www.semanticscholar.org/paper/e3bd0816668fc28af82eb52db2f5d92cbc886642\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2008.04776\",\"authors\":[{\"authorId\":\"73329364\",\"name\":\"Jiangning Zhang\"},{\"authorId\":\"145194966\",\"name\":\"Chao Xu\"},{\"authorId\":\"1391190989\",\"name\":\"L. Liu\"},{\"authorId\":\"47446949\",\"name\":\"M. Wang\"},{\"authorId\":\"1490938675\",\"name\":\"Xia Wu\"},{\"authorId\":\"93006732\",\"name\":\"Y. Liu\"},{\"authorId\":\"122376816\",\"name\":\"Yunliang Jiang\"}],\"doi\":\"10.1007/978-3-030-58558-7_18\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c1847cbb4064ca05593f7b408783e9f183953488\",\"title\":\"DTVNet: Dynamic Time-lapse Video Generation via Single Still Image\",\"url\":\"https://www.semanticscholar.org/paper/c1847cbb4064ca05593f7b408783e9f183953488\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2518211\",\"name\":\"Chaoyue Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b2ae2633d5addd3dac975345e72b66dc6300020f\",\"title\":\"Generative modelling and adversarial learning\",\"url\":\"https://www.semanticscholar.org/paper/b2ae2633d5addd3dac975345e72b66dc6300020f\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22770939\",\"name\":\"Tsai-Ho Sun\"},{\"authorId\":\"12548782\",\"name\":\"Chien-Hsun Lai\"},{\"authorId\":\"1992587\",\"name\":\"S. Wong\"},{\"authorId\":null,\"name\":\"Yu-Shuen Wang\"}],\"doi\":\"10.1145/3343031.3351041\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8136365526da5b5aa2717ea11676b63be985f93\",\"title\":\"Adversarial Colorization of Icons Based on Contour and Color Conditions\",\"url\":\"https://www.semanticscholar.org/paper/c8136365526da5b5aa2717ea11676b63be985f93\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48954595\",\"name\":\"Akanksha Sharma\"},{\"authorId\":\"2150759\",\"name\":\"Neeru Jindal\"},{\"authorId\":\"1828828101\",\"name\":\"P. S. Rana\"}],\"doi\":\"10.1007/s11042-020-09308-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"488f1390252f68695957a74ac16f0958c9155f24\",\"title\":\"Potential of generative adversarial net algorithms in image and video processing applications\\u2013 a survey\",\"url\":\"https://www.semanticscholar.org/paper/488f1390252f68695957a74ac16f0958c9155f24\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819450790\",\"name\":\"Qi Chen\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"49251914\",\"name\":\"Jun-kai Chen\"},{\"authorId\":\"8277017\",\"name\":\"Q. Wu\"},{\"authorId\":\"133678193\",\"name\":\"A. van den Hengel\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1109/TIP.2020.3003227\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a2418fd9c453492f1ca833d5595571249a3ee55\",\"title\":\"Scripted Video Generation With a Bottom-Up Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/5a2418fd9c453492f1ca833d5595571249a3ee55\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1617827791\",\"name\":\"Byeongkeun Kang\"},{\"authorId\":\"2906509\",\"name\":\"Subarna Tripathi\"},{\"authorId\":\"143803197\",\"name\":\"T. Nguyen\"}],\"doi\":\"10.1109/ACCESS.2020.3027800\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"25401477355cfa9a976ecc45896b4e490c985f71\",\"title\":\"Generating Images in Compressed Domain Using Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/25401477355cfa9a976ecc45896b4e490c985f71\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1393157266\",\"name\":\"Ken\\u2019ich Morooka\"},{\"authorId\":\"40460824\",\"name\":\"X. Zhang\"},{\"authorId\":\"1915539\",\"name\":\"Shoko Miyauchi\"},{\"authorId\":\"1801213\",\"name\":\"R. Kurazume\"},{\"authorId\":\"3979815\",\"name\":\"E. Ohno\"}],\"doi\":\"10.1007/978-3-030-39770-8_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e11d273299945ba943e0c1094628142a148bd40\",\"title\":\"GAN-Based Method for Synthesizing Multi-focus Cell Images\",\"url\":\"https://www.semanticscholar.org/paper/7e11d273299945ba943e0c1094628142a148bd40\",\"venue\":\"PSIVT Workshops\",\"year\":2019},{\"arxivId\":\"1809.01372\",\"authors\":[{\"authorId\":\"7192666\",\"name\":\"H. Huang\"},{\"authorId\":\"2063358\",\"name\":\"Senzhe Xu\"},{\"authorId\":\"15996814\",\"name\":\"Junxiong Cai\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"144110127\",\"name\":\"S. Hu\"}],\"doi\":\"10.1109/TIP.2019.2925550\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"14d1e3c6405ed22d30d88d577275233cca89d3a0\",\"title\":\"Temporally Coherent Video Harmonization Using Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/14d1e3c6405ed22d30d88d577275233cca89d3a0\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2011.02250\",\"authors\":[{\"authorId\":\"51235324\",\"name\":\"Nuha Aldausari\"},{\"authorId\":\"145313633\",\"name\":\"A. Sowmya\"},{\"authorId\":\"47600265\",\"name\":\"N. Marcus\"},{\"authorId\":\"4911295\",\"name\":\"G. Mohammadi\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b\",\"title\":\"Video Generative Adversarial Networks: A Review\",\"url\":\"https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48546530\",\"name\":\"D. Wiesner\"},{\"authorId\":\"5295863\",\"name\":\"Tereza Ne\\u010dasov\\u00e1\"},{\"authorId\":\"1721826\",\"name\":\"D. Svoboda\"}],\"doi\":\"10.1007/978-3-030-30645-8_61\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0388e2aa2759cf46be5099d04854b5d91b41aca3\",\"title\":\"On Generative Modeling of Cell Shape Using 3D GANs\",\"url\":\"https://www.semanticscholar.org/paper/0388e2aa2759cf46be5099d04854b5d91b41aca3\",\"venue\":\"ICIAP\",\"year\":2019},{\"arxivId\":\"1807.08536\",\"authors\":[{\"authorId\":\"3700393\",\"name\":\"Minjun Li\"},{\"authorId\":\"7192666\",\"name\":\"H. Huang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-01240-3_12\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b3c0f109b92bbb75ab2c63bb0cfc7f47cc3d44c3\",\"title\":\"Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/b3c0f109b92bbb75ab2c63bb0cfc7f47cc3d44c3\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40617564\",\"name\":\"W. Li\"},{\"authorId\":\"51305314\",\"name\":\"Zehuan Yuan\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"},{\"authorId\":\"1697065\",\"name\":\"C. Wang\"}],\"doi\":\"10.1109/icme46284.2020.9102778\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d3f072ef328c2a9fd0649ce8f27fab8839ff4bdd\",\"title\":\"Moflowgan: Video Generation With Flow Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d3f072ef328c2a9fd0649ce8f27fab8839ff4bdd\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103664864\",\"name\":\"Chia-chi Cheng\"},{\"authorId\":\"40846050\",\"name\":\"Hungyu Chen\"},{\"authorId\":\"37811787\",\"name\":\"Wei-Chen Chiu\"}],\"doi\":\"10.1109/cvpr42600.2020.00568\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4daed2840a1fd1cada30cdee782c1d0c1f72f577\",\"title\":\"Time Flies: Animating a Still Image With Time-Lapse Video As Reference\",\"url\":\"https://www.semanticscholar.org/paper/4daed2840a1fd1cada30cdee782c1d0c1f72f577\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1901.07838\",\"authors\":[{\"authorId\":\"1801046\",\"name\":\"B. Kang\"},{\"authorId\":\"2906509\",\"name\":\"Subarna Tripathi\"},{\"authorId\":\"143803197\",\"name\":\"T. Nguyen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71978167951e0764bdb313465720ca0b7c510b53\",\"title\":\"Toward Joint Image Generation and Compression using Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/71978167951e0764bdb313465720ca0b7c510b53\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681659\",\"name\":\"K. Yanai\"},{\"authorId\":null,\"name\":\"UEC Toyko\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d549112f0cad6fbfb40bee9109164eeb37fa393\",\"title\":\"SSA-GAN: Cloud Video Generation from a Single Image with Spatial Self-Attention Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/7d549112f0cad6fbfb40bee9109164eeb37fa393\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2003.04035\",\"authors\":[{\"authorId\":\"152831141\",\"name\":\"Pauline Luc\"},{\"authorId\":\"31993415\",\"name\":\"A. Clark\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"40550616\",\"name\":\"D. Casas\"},{\"authorId\":\"2895238\",\"name\":\"Yotam Doron\"},{\"authorId\":\"51042571\",\"name\":\"Albin Cassirer\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e93e16a1a9f060841956ae2eca735f5cce457c8c\",\"title\":\"Transformation-based Adversarial Video Prediction on Large-Scale Data\",\"url\":\"https://www.semanticscholar.org/paper/e93e16a1a9f060841956ae2eca735f5cce457c8c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.10662\",\"authors\":[{\"authorId\":\"69544685\",\"name\":\"Jie Wu\"},{\"authorId\":\"1765674\",\"name\":\"Tianshui Chen\"},{\"authorId\":\"1721715\",\"name\":\"Hefeng Wu\"},{\"authorId\":\"10665619\",\"name\":\"Z. Yang\"},{\"authorId\":\"1773818\",\"name\":\"Guangchun Luo\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/tmm.2020.3011317\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0edc8eb2d2b53482faf34906dacf996bc4127f6\",\"title\":\"Fine-Grained Image Captioning with Global-Local Discriminative Objective\",\"url\":\"https://www.semanticscholar.org/paper/f0edc8eb2d2b53482faf34906dacf996bc4127f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1901.01649\",\"authors\":[{\"authorId\":\"67083303\",\"name\":\"Guohao Ying\"},{\"authorId\":\"50817744\",\"name\":\"Yingtian Zou\"},{\"authorId\":\"144363232\",\"name\":\"L. Wan\"},{\"authorId\":\"1736595\",\"name\":\"Yiming Hu\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"}],\"doi\":\"10.1007/978-3-030-20876-9_18\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62f4f3654f9e4c3b94aa70dd4a158c961288fbc4\",\"title\":\"Better Guider Predicts Future Better: Difference Guided Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/62f4f3654f9e4c3b94aa70dd4a158c961288fbc4\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1450748978\",\"name\":\"Rudy Herlambang\"}],\"doi\":\"10.4108/eai.27-4-2019.2286906\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40e3668fe99d86a6cf56ae0b43b44fc311c57ef8\",\"title\":\"Aesthetical Value of Architecture of the Great Mosque of Kasunanan Surakarta toward Timelapse Application\",\"url\":\"https://www.semanticscholar.org/paper/40e3668fe99d86a6cf56ae0b43b44fc311c57ef8\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1811.10673\",\"authors\":[{\"authorId\":\"49899501\",\"name\":\"Sungsoo Kim\"},{\"authorId\":\"2199399\",\"name\":\"J. Park\"},{\"authorId\":\"46920670\",\"name\":\"C. G. Bampis\"},{\"authorId\":\"6226178\",\"name\":\"Jaeseong Lee\"},{\"authorId\":\"8306347\",\"name\":\"M. Markey\"},{\"authorId\":\"1718469\",\"name\":\"A. Dimakis\"},{\"authorId\":\"1747569\",\"name\":\"A. Bovik\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054165\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c351a76401c3f1fef420ce634c3e9a87131fa826\",\"title\":\"Adversarial Video Compression Guided by Soft Edge Detection\",\"url\":\"https://www.semanticscholar.org/paper/c351a76401c3f1fef420ce634c3e9a87131fa826\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35687142\",\"name\":\"Daichi Horita\"},{\"authorId\":\"1681659\",\"name\":\"K. Yanai\"}],\"doi\":\"10.1007/978-3-030-41404-7_44\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4d682b993762623c1fea5c91e19429df04a4c87d\",\"title\":\"SSA-GAN: End-to-End Time-Lapse Video Generation with Spatial Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d682b993762623c1fea5c91e19429df04a4c87d\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":\"1904.00680\",\"authors\":[{\"authorId\":\"7532506\",\"name\":\"Seonghyeon Nam\"},{\"authorId\":\"1797422\",\"name\":\"Chongyang Ma\"},{\"authorId\":\"1752091\",\"name\":\"M. Chai\"},{\"authorId\":\"48858384\",\"name\":\"William Brendel\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"1754380\",\"name\":\"S. Kim\"}],\"doi\":\"10.1109/CVPR.2019.00150\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"54efdd4c03e47a11db7ef710b8dc87c70d34e529\",\"title\":\"End-To-End Time-Lapse Video Synthesis From a Single Outdoor Image\",\"url\":\"https://www.semanticscholar.org/paper/54efdd4c03e47a11db7ef710b8dc87c70d34e529\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1801.01726\",\"authors\":[{\"authorId\":\"3446352\",\"name\":\"P. Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"3229527\",\"name\":\"Daoyuan Jia\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7a66d713776e78ae60617eee2715443a8565a23\",\"title\":\"Semantic-aware Grad-GAN for Virtual-to-Real Urban Scene Adaption\",\"url\":\"https://www.semanticscholar.org/paper/a7a66d713776e78ae60617eee2715443a8565a23\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47995432\",\"name\":\"H. Xu\"},{\"authorId\":\"8555475\",\"name\":\"Jiayi Ma\"},{\"authorId\":\"49469106\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1109/TIP.2020.2999855\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62e99100119b361e878a9cc67d87b57fe2f463c4\",\"title\":\"MEF-GAN: Multi-Exposure Image Fusion via Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/62e99100119b361e878a9cc67d87b57fe2f463c4\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2011.15128\",\"authors\":[{\"authorId\":\"118300344\",\"name\":\"Aleksander Holynski\"},{\"authorId\":\"1396759259\",\"name\":\"Brian Curless\"},{\"authorId\":\"1679223\",\"name\":\"S. Seitz\"},{\"authorId\":\"1717841\",\"name\":\"R. Szeliski\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a9eb5c59763b5da7a9a0e1c5ef543b2f02630feb\",\"title\":\"Animating Pictures with Eulerian Motion Fields\",\"url\":\"https://www.semanticscholar.org/paper/a9eb5c59763b5da7a9a0e1c5ef543b2f02630feb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.13736\",\"authors\":[{\"authorId\":\"3129798\",\"name\":\"Pourya Shamsolmoali\"},{\"authorId\":\"2345767\",\"name\":\"Masoumeh Zareapoor\"},{\"authorId\":\"1697195\",\"name\":\"E. Granger\"},{\"authorId\":\"46544755\",\"name\":\"Huiyu Zhou\"},{\"authorId\":\"1775398\",\"name\":\"R. Wang\"},{\"authorId\":\"145214646\",\"name\":\"M. E. Celebi\"},{\"authorId\":\"145889118\",\"name\":\"Jie Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d27aa2eb68e4761adc1e8045a39c29e3d8f8cb0b\",\"title\":\"Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies\",\"url\":\"https://www.semanticscholar.org/paper/d27aa2eb68e4761adc1e8045a39c29e3d8f8cb0b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152831141\",\"name\":\"Pauline Luc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e98a7f4e73f49248c912b141c43031a241d4ac33\",\"title\":\"Self-supervised learning of predictive segmentation models from video. (Apprentissage autosupervis\\u00e9 de mod\\u00e8les pr\\u00e9dictifs de segmentation \\u00e0 partir de vid\\u00e9os)\",\"url\":\"https://www.semanticscholar.org/paper/e98a7f4e73f49248c912b141c43031a241d4ac33\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2006.14865\",\"authors\":[{\"authorId\":\"5102388\",\"name\":\"Zihao Yan\"},{\"authorId\":\"2154334\",\"name\":\"Ruizhen Hu\"},{\"authorId\":\"66060467\",\"name\":\"Xingguang Yan\"},{\"authorId\":\"49329812\",\"name\":\"L. Chen\"},{\"authorId\":\"3276873\",\"name\":\"O. V. Kaick\"},{\"authorId\":\"3410520\",\"name\":\"Hongxing Zhang\"},{\"authorId\":\"40586368\",\"name\":\"Hui Huang\"}],\"doi\":\"10.1145/3355089.3356573\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a57df2413afa2448f50cad4d4357ab8d5006c2ba\",\"title\":\"RPM-Net\",\"url\":\"https://www.semanticscholar.org/paper/a57df2413afa2448f50cad4d4357ab8d5006c2ba\",\"venue\":\"ACM Trans. Graph.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46758964\",\"name\":\"Jie Yan\"},{\"authorId\":\"1783055\",\"name\":\"Guihe Qin\"},{\"authorId\":\"10431066\",\"name\":\"R. Zhao\"},{\"authorId\":\"46992126\",\"name\":\"Yan-hua Liang\"},{\"authorId\":\"150270946\",\"name\":\"Qianyi Xu\"}],\"doi\":\"10.1109/ACCESS.2019.2961383\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c19ef8d8dfefb8ca898fc3b9b58cb3b499ed3836\",\"title\":\"Mixpred: Video Prediction Beyond Optical Flow\",\"url\":\"https://www.semanticscholar.org/paper/c19ef8d8dfefb8ca898fc3b9b58cb3b499ed3836\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48096253\",\"name\":\"Long Zhao\"},{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"144978989\",\"name\":\"Y. Tian\"},{\"authorId\":\"1379758006\",\"name\":\"Mubbasir Kapadia\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1007/s11263-020-01328-9\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f0ca9eb353e33e37e725a8f89bd07ca1fc09a285\",\"title\":\"Towards Image-to-Video Translation: A Structure-Aware Approach via Multi-stage Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/f0ca9eb353e33e37e725a8f89bd07ca1fc09a285\",\"venue\":\"International Journal of Computer Vision\",\"year\":2020},{\"arxivId\":\"1704.04886\",\"authors\":[{\"authorId\":\"144932914\",\"name\":\"B. Zhao\"},{\"authorId\":\"153028349\",\"name\":\"Xiao Wu\"},{\"authorId\":\"3493929\",\"name\":\"Zhi-Qi Cheng\"},{\"authorId\":\"46935239\",\"name\":\"H. Liu\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"}],\"doi\":\"10.1145/3240508.3240536\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e659f58d5dd55e2b9ab20c21046c38465f386ffe\",\"title\":\"Multi-View Image Generation from a Single-View\",\"url\":\"https://www.semanticscholar.org/paper/e659f58d5dd55e2b9ab20c21046c38465f386ffe\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1812.01874\",\"authors\":[{\"authorId\":\"5437547\",\"name\":\"Qiyang Hu\"},{\"authorId\":\"1822120361\",\"name\":\"Adrian Walchli\"},{\"authorId\":\"21529935\",\"name\":\"Tiziano Portenier\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"},{\"authorId\":\"144707901\",\"name\":\"P. Favaro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bb74e29321772ea815f88769d31a902a2c3e996\",\"title\":\"Learning to Take Directions One Step at a Time\",\"url\":\"https://www.semanticscholar.org/paper/6bb74e29321772ea815f88769d31a902a2c3e996\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24386782\",\"name\":\"Fengling Mao\"},{\"authorId\":\"24386782\",\"name\":\"Fengling Mao\"},{\"authorId\":\"145341877\",\"name\":\"Bingpeng Ma\"},{\"authorId\":\"145375324\",\"name\":\"H. Chang\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"144481158\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1007/S11432-020-2900-X\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a7e8ac77b2702c67f056facf99099241803a89d4\",\"title\":\"Learning Efficient Text-to-Image Synthesis via Interstage Cross-Sample Similarity Distillation\",\"url\":\"https://www.semanticscholar.org/paper/a7e8ac77b2702c67f056facf99099241803a89d4\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"2002.00509\",\"authors\":[{\"authorId\":\"1404109463\",\"name\":\"Eduardo C. Garrido-Merch\\u00e1n\"},{\"authorId\":\"145228714\",\"name\":\"M. Molina\"}],\"doi\":\"10.1007/978-3-030-61705-9_29\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e4a6502d1c8b4d64bcacea1321a307b7003c71c\",\"title\":\"A Machine Consciousness architecture based on Deep Learning and Gaussian Processes\",\"url\":\"https://www.semanticscholar.org/paper/7e4a6502d1c8b4d64bcacea1321a307b7003c71c\",\"venue\":\"HAIS\",\"year\":2020},{\"arxivId\":\"1908.05519\",\"authors\":[{\"authorId\":\"2210804\",\"name\":\"Nathanael Perraudin\"},{\"authorId\":\"48294059\",\"name\":\"Ankit Srivastava\"},{\"authorId\":\"40401747\",\"name\":\"A. Lucchi\"},{\"authorId\":\"49287712\",\"name\":\"T. Kacprzak\"},{\"authorId\":\"47483298\",\"name\":\"T. Hofmann\"},{\"authorId\":\"123213185\",\"name\":\"A. R\\u00e9fr\\u00e9gier\"}],\"doi\":\"10.1186/s40668-019-0032-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a9ea825bc7114a8c2eb3505c5b5ab6f59fbd0d62\",\"title\":\"Cosmological N-body simulations: a challenge for scalable generative models\",\"url\":\"https://www.semanticscholar.org/paper/a9ea825bc7114a8c2eb3505c5b5ab6f59fbd0d62\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9115468\",\"name\":\"Guanzhao Li\"},{\"authorId\":\"73329404\",\"name\":\"J. Zhang\"},{\"authorId\":\"4194168\",\"name\":\"Danni Chen\"},{\"authorId\":\"12907722\",\"name\":\"Z. Liu\"},{\"authorId\":\"14598885\",\"name\":\"Junting He\"}],\"doi\":\"10.1117/1.JEI.28.3.033029\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"27c6155fe17144cb28e727ca33ce57479e919db7\",\"title\":\"Chinese flower-bird character generation based on pencil drawings or brush drawings\",\"url\":\"https://www.semanticscholar.org/paper/27c6155fe17144cb28e727ca33ce57479e919db7\",\"venue\":\"J. Electronic Imaging\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47727108\",\"name\":\"J. Torres\"},{\"authorId\":\"145576205\",\"name\":\"Bryan Gardiner\"},{\"authorId\":\"1413278834\",\"name\":\"Ilias Dahi\"},{\"authorId\":\"1978811\",\"name\":\"S. Moffett\"},{\"authorId\":\"40489164\",\"name\":\"Marco Herbst\"},{\"authorId\":\"51980227\",\"name\":\"J. Condell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b57bf68f2b3a0dd8bc52ae5af54b95074693751e\",\"title\":\"An Efficient Approach to Automatic Generation of Time-lapse Video Sequence\",\"url\":\"https://www.semanticscholar.org/paper/b57bf68f2b3a0dd8bc52ae5af54b95074693751e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1910.05253\",\"authors\":[{\"authorId\":\"22770939\",\"name\":\"Tsai-Ho Sun\"},{\"authorId\":\"1685457535\",\"name\":\"Chien-Hsun Lai\"},{\"authorId\":\"1992587\",\"name\":\"S. Wong\"},{\"authorId\":null,\"name\":\"Yu-Shuen Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc6ae3b30cf755a944995e8d96ffe088c829e2c6\",\"title\":\"Adversarial Colorization Of Icons Based On Structure And Color Conditions\",\"url\":\"https://www.semanticscholar.org/paper/cc6ae3b30cf755a944995e8d96ffe088c829e2c6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2006.04731\",\"authors\":[{\"authorId\":\"14405369\",\"name\":\"N. Geneva\"},{\"authorId\":\"1680274\",\"name\":\"N. Zabaras\"}],\"doi\":\"10.3934/fods.2020019\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74a32b1b303ef4e517a141c5a0dd0b95d36c1a54\",\"title\":\"Multi-fidelity Generative Deep Learning Turbulent Flows\",\"url\":\"https://www.semanticscholar.org/paper/74a32b1b303ef4e517a141c5a0dd0b95d36c1a54\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bea684bb4f9b96c09169ef1a45fc3f4ebf24369c\",\"title\":\"From Video Classification to Video Prediction: Deep Learning Approaches to Video Modelling\",\"url\":\"https://www.semanticscholar.org/paper/bea684bb4f9b96c09169ef1a45fc3f4ebf24369c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"153771536\",\"name\":\"J. Yan\"},{\"authorId\":\"6765511\",\"name\":\"Z. Wang\"}],\"doi\":\"10.1109/ICIP.2019.8803041\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26262acf8083a9317021b5d086636853341cae84\",\"title\":\"Enhancement of Weakly Illuminated Images by Deep Fusion Networks\",\"url\":\"https://www.semanticscholar.org/paper/26262acf8083a9317021b5d086636853341cae84\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":\"1910.07192\",\"authors\":[{\"authorId\":\"2420042\",\"name\":\"Y. Endo\"},{\"authorId\":\"2504432\",\"name\":\"Y. Kanamori\"},{\"authorId\":\"36375845\",\"name\":\"Shigeru Kuriyama\"}],\"doi\":\"10.1145/3355089.3356523\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf234eb0d6f4eae787be83c8d9756e8c34e2e80a\",\"title\":\"Animating landscape\",\"url\":\"https://www.semanticscholar.org/paper/bf234eb0d6f4eae787be83c8d9756e8c34e2e80a\",\"venue\":\"ACM Trans. Graph.\",\"year\":2019},{\"arxivId\":\"1909.10833\",\"authors\":[{\"authorId\":\"144027562\",\"name\":\"P. K\\u00f6nig\"},{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"2388085\",\"name\":\"M. K\\u00f6rner\"}],\"doi\":\"10.1109/ITSC.2019.8917046\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5bacdc2068aa755016faf585f2d2f90b9046d4eb\",\"title\":\"Enhancing Traffic Scene Predictions with Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/5bacdc2068aa755016faf585f2d2f90b9046d4eb\",\"venue\":\"2019 IEEE Intelligent Transportation Systems Conference (ITSC)\",\"year\":2019},{\"arxivId\":\"1811.00445\",\"authors\":[{\"authorId\":\"35660603\",\"name\":\"Wenbin Li\"},{\"authorId\":\"39272336\",\"name\":\"W. Xiong\"},{\"authorId\":\"145585312\",\"name\":\"Haofu Liao\"},{\"authorId\":\"144162046\",\"name\":\"Jing Huo\"},{\"authorId\":\"145644823\",\"name\":\"Y. Gao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1016/j.neunet.2020.08.011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8fc45e0d7056de2fbb34107e038842e5d5735205\",\"title\":\"CariGAN: Caricature Generation through Weakly Paired Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/8fc45e0d7056de2fbb34107e038842e5d5735205\",\"venue\":\"Neural Networks\",\"year\":2020},{\"arxivId\":\"1804.00213\",\"authors\":[{\"authorId\":\"144850642\",\"name\":\"W. Ren\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1718428\",\"name\":\"Jiawei Zhang\"},{\"authorId\":\"9416881\",\"name\":\"J. Pan\"},{\"authorId\":\"1719250\",\"name\":\"Xiaochun Cao\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"}],\"doi\":\"10.1109/CVPR.2018.00343\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dbd21420cd1299fdab9a04c6b4b3b5b4d4a38a6d\",\"title\":\"Gated Fusion Network for Single Image Dehazing\",\"url\":\"https://www.semanticscholar.org/paper/dbd21420cd1299fdab9a04c6b4b3b5b4d4a38a6d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}],\"corpusId\":1504491,\"doi\":\"10.1109/CVPR.2018.00251\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":12,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"87a818723a2ada66a1193baf17b0383d9766781b\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2017.319\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d2892f82a89bfc81f9924adb8bd070fe007adf7\",\"title\":\"Generating the Future with Adversarial Transformers\",\"url\":\"https://www.semanticscholar.org/paper/6d2892f82a89bfc81f9924adb8bd070fe007adf7\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12114845\",\"name\":\"P. Ruvolo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"133fc77b6481751dd5ffd6957a88eecd3ce136d6\",\"title\":\"Dude, Where's My Robot?: A Localization Challenge for Undergraduate Robotics\",\"url\":\"https://www.semanticscholar.org/paper/133fc77b6481751dd5ffd6957a88eecd3ce136d6\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1607.02586\",\"authors\":[{\"authorId\":\"3222730\",\"name\":\"Tianfan Xue\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"18555073\",\"name\":\"K. Bouman\"},{\"authorId\":\"36668046\",\"name\":\"B. Freeman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"397e9b56e46d3cc34af1525493e597facb104570\",\"title\":\"Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/397e9b56e46d3cc34af1525493e597facb104570\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1707.04993\",\"authors\":[{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00165\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"title\":\"MoCoGAN: Decomposing Motion and Content for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1611.07004\",\"authors\":[{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"1822702\",\"name\":\"Tinghui Zhou\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2017.632\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"8acbe90d5b852dadea7810345451a99608ee54c7\",\"title\":\"Image-to-Image Translation with Conditional Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2612900\",\"name\":\"Amardeep Sathyanarayana\"},{\"authorId\":\"3184189\",\"name\":\"Seyed Omid Sadjadi\"},{\"authorId\":\"144270764\",\"name\":\"J. Hansen\"}],\"doi\":\"10.1109/ITSC.2012.6338717\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f3a711920c0fe87f2a7920ad3a1ceca7e7b433a\",\"title\":\"Leveraging sensor information from portable devices towards automatic driving maneuver recognition\",\"url\":\"https://www.semanticscholar.org/paper/4f3a711920c0fe87f2a7920ad3a1ceca7e7b433a\",\"venue\":\"2012 15th International IEEE Conference on Intelligent Transportation Systems\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1891828\",\"name\":\"Leon A. Gatys\"},{\"authorId\":\"1746183\",\"name\":\"Alexander S. Ecker\"},{\"authorId\":\"1731199\",\"name\":\"M. Bethge\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6\",\"title\":\"Texture Synthesis Using Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0d0eeb46fc5ec778a62bb94aa2ef261b08e6f8c6\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1505.00853\",\"authors\":[{\"authorId\":\"48310008\",\"name\":\"B. Xu\"},{\"authorId\":\"48246959\",\"name\":\"Naiyan Wang\"},{\"authorId\":\"1913774\",\"name\":\"T. Chen\"},{\"authorId\":null,\"name\":\"Mu Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"adf3b591281688b7e71b254ab931b2aa39b4b59f\",\"title\":\"Empirical Evaluation of Rectified Activations in Convolutional Network\",\"url\":\"https://www.semanticscholar.org/paper/adf3b591281688b7e71b254ab931b2aa39b4b59f\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"1682058\",\"name\":\"H. Zhang\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.1007/978-3-030-01261-8_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a1653e88be986aee2f37792c3fb05f0ee7fbef94\",\"title\":\"Generative Semantic Manipulation with Mask-Contrasting GAN\",\"url\":\"https://www.semanticscholar.org/paper/a1653e88be986aee2f37792c3fb05f0ee7fbef94\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1611.04021\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1da2431a799f68888b7e035fe49fe47a4735b71b\",\"title\":\"Leveraging Video Descriptions to Learn Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1da2431a799f68888b7e035fe49fe47a4735b71b\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1609.02612\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.13016/M26GIH-TNYZ\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"title\":\"Generating Videos with Scene Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1706.06982\",\"authors\":[{\"authorId\":\"19251410\",\"name\":\"Matthew Tesfaldet\"},{\"authorId\":\"2575536\",\"name\":\"M. Brubaker\"},{\"authorId\":\"3150825\",\"name\":\"K. Derpanis\"}],\"doi\":\"10.1109/CVPR.2018.00701\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"380f159cf407d2aa52232b8f7f29ae2405c35c65\",\"title\":\"Two-Stream Convolutional Networks for Dynamic Texture Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/380f159cf407d2aa52232b8f7f29ae2405c35c65\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1605.05396\",\"authors\":[{\"authorId\":\"144828948\",\"name\":\"S. Reed\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"3084614\",\"name\":\"Xinchen Yan\"},{\"authorId\":\"2876316\",\"name\":\"L. Logeswaran\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c7f040a150abf21dbcefe1f22e0f98fa184f41a\",\"title\":\"Generative Adversarial Text to Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/6c7f040a150abf21dbcefe1f22e0f98fa184f41a\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1603.08155\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"3304525\",\"name\":\"Alexandre Alahi\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-46475-6_43\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fa3720371e78d04973ce9752781bc337480b68f\",\"title\":\"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/9fa3720371e78d04973ce9752781bc337480b68f\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1612.03242\",\"authors\":[{\"authorId\":\"120811666\",\"name\":\"Han Zhang\"},{\"authorId\":\"145017761\",\"name\":\"Tao Xu\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1109/ICCV.2017.629\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921\",\"title\":\"StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1609.04802\",\"authors\":[{\"authorId\":\"1779917\",\"name\":\"C. Ledig\"},{\"authorId\":\"2073063\",\"name\":\"L. Theis\"},{\"authorId\":\"3108066\",\"name\":\"Ferenc Husz\\u00e1r\"},{\"authorId\":\"79382929\",\"name\":\"J. Caballero\"},{\"authorId\":\"83015038\",\"name\":\"Andrew Aitken\"},{\"authorId\":\"41203992\",\"name\":\"Alykhan Tejani\"},{\"authorId\":\"1853456\",\"name\":\"J. Totz\"},{\"authorId\":\"34627233\",\"name\":\"Zehan Wang\"},{\"authorId\":\"152554375\",\"name\":\"W. Shi\"}],\"doi\":\"10.1109/CVPR.2017.19\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3\",\"title\":\"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1706.05274\",\"authors\":[{\"authorId\":\"46275941\",\"name\":\"J. Li\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"49020088\",\"name\":\"Yunchao Wei\"},{\"authorId\":\"39001620\",\"name\":\"T. Xu\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":\"10.1109/CVPR.2017.211\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"508eb5a6156b8fa1b4547b611e85969438116fa2\",\"title\":\"Perceptual Generative Adversarial Networks for Small Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/508eb5a6156b8fa1b4547b611e85969438116fa2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"150426657\",\"name\":\"\\u62d3\\u6d77 \\u6749\\u5c71\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8a5f27e7805f8de84ea008d59452ff864271696\",\"title\":\"\\u201cUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\\u201d\\u306e\\u5b66\\u7fd2\\u5831\\u544a\",\"url\":\"https://www.semanticscholar.org/paper/e8a5f27e7805f8de84ea008d59452ff864271696\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1706.08033\",\"authors\":[{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"10668384\",\"name\":\"Xunyu Lin\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8375ff50b8a6f1a10dd809129a18df96888ac8b\",\"title\":\"Decomposing Motion and Content for Natural Video Sequence Prediction\",\"url\":\"https://www.semanticscholar.org/paper/b8375ff50b8a6f1a10dd809129a18df96888ac8b\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1511.05440\",\"authors\":[{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2341378\",\"name\":\"C. Couprie\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"title\":\"Deep multi-scale video prediction beyond mean square error\",\"url\":\"https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1411.1784\",\"authors\":[{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"353ecf7b66b3e9ff5e9f41145a147e899a2eea5c\",\"title\":\"Conditional Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/353ecf7b66b3e9ff5e9f41145a147e899a2eea5c\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1708.00315\",\"authors\":[{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"1682058\",\"name\":\"H. Zhang\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cb4418b5bddaaceb92caea9e72c8cc528ce4e3cc\",\"title\":\"Generative Semantic Manipulation with Contrasting GAN\",\"url\":\"https://www.semanticscholar.org/paper/cb4418b5bddaaceb92caea9e72c8cc528ce4e3cc\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1704.05831\",\"authors\":[{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"8299168\",\"name\":\"Y. Zou\"},{\"authorId\":\"2459821\",\"name\":\"Sungryull Sohn\"},{\"authorId\":\"10668384\",\"name\":\"Xunyu Lin\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f230cacc511b17b491bf3d90015bbbf85b9ef6af\",\"title\":\"Learning to Generate Long-term Future via Hierarchical Prediction\",\"url\":\"https://www.semanticscholar.org/paper/f230cacc511b17b491bf3d90015bbbf85b9ef6af\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/CVPR.2016.339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d9671ec394ec374021702642713aa634b8556312\",\"title\":\"Harnessing Object and Scene Semantics for Large-Scale Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/d9671ec394ec374021702642713aa634b8556312\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2877311\",\"name\":\"Mart\\u00edn Arjovsky\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"52184096\",\"name\":\"L. Bottou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"acd87843a451d18b4dc6474ddce1ae946429eaf1\",\"title\":\"Wasserstein Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/acd87843a451d18b4dc6474ddce1ae946429eaf1\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1312.6114\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"1678311\",\"name\":\"M. Welling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"title\":\"Auto-Encoding Variational Bayes\",\"url\":\"https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145557639\",\"name\":\"V. Nair\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a538b05ebb01a40323997629e171c91aa28b8e2f\",\"title\":\"Rectified Linear Units Improve Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":\"1412.6622\",\"authors\":[{\"authorId\":\"40555034\",\"name\":\"E. Hoffer\"},{\"authorId\":\"2048494\",\"name\":\"Nir Ailon\"}],\"doi\":\"10.1007/978-3-319-24261-3_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ac1a7d6630cd1c7b70c4c2c7bc92c40df1162ca\",\"title\":\"Deep Metric Learning Using Triplet Network\",\"url\":\"https://www.semanticscholar.org/paper/3ac1a7d6630cd1c7b70c4c2c7bc92c40df1162ca\",\"venue\":\"SIMBAD\",\"year\":2015},{\"arxivId\":\"1705.08051\",\"authors\":[{\"authorId\":\"49462765\",\"name\":\"Shuai Xiao\"},{\"authorId\":\"1682124\",\"name\":\"Mehrdad Farajtabar\"},{\"authorId\":\"145315788\",\"name\":\"Xiaojing Ye\"},{\"authorId\":\"3063894\",\"name\":\"Junchi Yan\"},{\"authorId\":\"15469693\",\"name\":\"Xiaokang Yang\"},{\"authorId\":\"1779453\",\"name\":\"L. Song\"},{\"authorId\":\"145203884\",\"name\":\"H. Zha\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"293fd0e760a3e4845292b998000ba1b143c0a731\",\"title\":\"Wasserstein Learning of Deep Generative Point Process Models\",\"url\":\"https://www.semanticscholar.org/paper/293fd0e760a3e4845292b998000ba1b143c0a731\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1505.04597\",\"authors\":[{\"authorId\":\"1737326\",\"name\":\"O. Ronneberger\"},{\"authorId\":\"152702479\",\"name\":\"P. Fischer\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1007/978-3-319-24574-4_28\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6364fdaa0a0eccd823a779fcdd489173f938e91a\",\"title\":\"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/6364fdaa0a0eccd823a779fcdd489173f938e91a\",\"venue\":\"MICCAI\",\"year\":2015},{\"arxivId\":\"1603.05027\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1007/978-3-319-46493-0_38\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"77f0a39b8e02686fd85b01971f8feb7f60971f80\",\"title\":\"Identity Mappings in Deep Residual Networks\",\"url\":\"https://www.semanticscholar.org/paper/77f0a39b8e02686fd85b01971f8feb7f60971f80\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1804.00113\",\"authors\":[{\"authorId\":\"143905981\",\"name\":\"Baoyuan Wu\"},{\"authorId\":\"33349597\",\"name\":\"Weidong Chen\"},{\"authorId\":\"145551072\",\"name\":\"Peng Sun\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"1794837\",\"name\":\"Siwei Lyu\"}],\"doi\":\"10.1109/CVPR.2018.00831\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ba7fb86657a2ee18577af26ee407723e97353d8\",\"title\":\"Tagging Like Humans: Diverse and Distinct Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/4ba7fb86657a2ee18577af26ee407723e97353d8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1511.06434\",\"authors\":[{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"2096458\",\"name\":\"Luke Metz\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8388f1be26329fa45e5807e968a641ce170ea078\",\"title\":\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1506.05751\",\"authors\":[{\"authorId\":\"40081727\",\"name\":\"Emily L. Denton\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"47900aca2f0b50da3010ad59b394c870f0e6c02e\",\"title\":\"Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1702.07006\",\"authors\":[{\"authorId\":\"50756254\",\"name\":\"Christina M. Funke\"},{\"authorId\":\"1891828\",\"name\":\"Leon A. Gatys\"},{\"authorId\":\"1746183\",\"name\":\"Alexander S. Ecker\"},{\"authorId\":\"1731199\",\"name\":\"M. Bethge\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9842c755296559d1b5a9e4ed6c9b068220f0989c\",\"title\":\"Synthesising Dynamic Textures using Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/9842c755296559d1b5a9e4ed6c9b068220f0989c\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1608.07724\",\"authors\":[{\"authorId\":\"49455017\",\"name\":\"Yipin Zhou\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1007/978-3-319-46484-8_16\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5bf3b2a5591f2a2f0c648dc5a1f4ff3bd6b9102a\",\"title\":\"Learning Temporal Transformations from Time-Lapse Videos\",\"url\":\"https://www.semanticscholar.org/paper/5bf3b2a5591f2a2f0c648dc5a1f4ff3bd6b9102a\",\"venue\":\"ECCV\",\"year\":2016}],\"title\":\"Learning to Generate Time-Lapse Videos Using Multi-stage Dynamic Generative Adversarial Networks\",\"topics\":[{\"topic\":\"Gramian matrix\",\"topicId\":\"5437\",\"url\":\"https://www.semanticscholar.org/topic/5437\"},{\"topic\":\"Generative adversarial networks\",\"topicId\":\"258908\",\"url\":\"https://www.semanticscholar.org/topic/258908\"},{\"topic\":\"Motion capture\",\"topicId\":\"76471\",\"url\":\"https://www.semanticscholar.org/topic/76471\"},{\"topic\":\"Data science\",\"topicId\":\"89193\",\"url\":\"https://www.semanticscholar.org/topic/89193\"},{\"topic\":\"The New York Times\",\"topicId\":\"251489\",\"url\":\"https://www.semanticscholar.org/topic/251489\"},{\"topic\":\"mdadm\",\"topicId\":\"2180483\",\"url\":\"https://www.semanticscholar.org/topic/2180483\"},{\"topic\":\"Image resolution\",\"topicId\":\"881\",\"url\":\"https://www.semanticscholar.org/topic/881\"}],\"url\":\"https://www.semanticscholar.org/paper/87a818723a2ada66a1193baf17b0383d9766781b\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"