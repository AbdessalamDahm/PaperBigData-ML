"{\"abstract\":\"We present a novel method to incorporate the recent advent in static saliency models to predict the saliency in videos. Our model augments the static saliency models with the Attentional Push effect of the photographer and the scene actors in a shared attention setting. We demonstrate that not only it is imperative to use static Attentional Push cues, noticeable performance improvement is achievable by learning the time-varying nature of Attentional Push. We propose a multi-stream Convolutional Long Short-Term Memory network (ConvLSTM) structure which augments state-of-the-art in static saliency models with dynamic Attentional Push. Our network contains four pathways, a saliency pathway and three Attentional Push pathways. The multi-pathway structure is followed by an augmenting convnet that learns to combine the complementary and time-varying outputs of the ConvLSTMs by minimizing the relative entropy between the augmented saliency and viewers fixation patterns on videos. We evaluate our model by comparing the performance of several augmented static saliency models with state-of-the-art in spatiotemporal saliency on three largest dynamic eye tracking datasets, HOLLYWOOD2, UCF-Sport and DIEM. Experimental results illustrates that solid performance gain is achievable using the proposed methodology.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"38111179\",\"name\":\"Siavash Gorji\",\"url\":\"https://www.semanticscholar.org/author/38111179\"},{\"authorId\":\"47125588\",\"name\":\"James J. Clark\",\"url\":\"https://www.semanticscholar.org/author/47125588\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1810.05680\",\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"}],\"doi\":\"10.1007/978-1-4614-7320-6_100656-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"554fecc6cc5d0ca43cdde2eee30a9f819fe49ae6\",\"title\":\"Bottom-up Attention, Models of\",\"url\":\"https://www.semanticscholar.org/paper/554fecc6cc5d0ca43cdde2eee30a9f819fe49ae6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3d173893dd3f22fd73acc16b94f0ea98469c9855\",\"title\":\"Saliency Prediction in the Deep Learning Era: Successes, Limitations, and Future Challenges\",\"url\":\"https://www.semanticscholar.org/paper/3d173893dd3f22fd73acc16b94f0ea98469c9855\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1808.09559\",\"authors\":[{\"authorId\":\"51244663\",\"name\":\"Panagiotis Linardos\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"51245064\",\"name\":\"M. Cherto\"},{\"authorId\":\"1737981\",\"name\":\"C. Gurrin\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf664deef0dbd74d4a7abf744560fe03bf90e863\",\"title\":\"Temporal Saliency Adaptation in Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/cf664deef0dbd74d4a7abf744560fe03bf90e863\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1810.03716\",\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4df34e0194faa27078832cb5078a2af6c9d0ea9b\",\"title\":\"Saliency Prediction in the Deep Learning Era: An Empirical Investigation\",\"url\":\"https://www.semanticscholar.org/paper/4df34e0194faa27078832cb5078a2af6c9d0ea9b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1812.00722\",\"authors\":[{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/CVPRW.2019.00109\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e330af2d061ade245622b9445d5be9717f66b60f\",\"title\":\"SUSiNet: See, Understand and Summarize It\",\"url\":\"https://www.semanticscholar.org/paper/e330af2d061ade245622b9445d5be9717f66b60f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":\"1811.07480\",\"authors\":[{\"authorId\":\"50251712\",\"name\":\"Ziqi Zhou\"},{\"authorId\":\"40514580\",\"name\":\"Z. Wang\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"},{\"authorId\":\"47673404\",\"name\":\"S. Wang\"},{\"authorId\":\"2736861\",\"name\":\"M. Sun\"}],\"doi\":\"10.1016/j.patcog.2020.107275\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b7de478275fdc5e154f6d904b910e8d291033edd\",\"title\":\"Global and Local Sensitivity Guided Key Salient Object Re-augmentation for Video Saliency Detection\",\"url\":\"https://www.semanticscholar.org/paper/b7de478275fdc5e154f6d904b910e8d291033edd\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409985550\",\"name\":\"Linardos Panagiotis\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"51245064\",\"name\":\"M. Cherto\"},{\"authorId\":\"1737981\",\"name\":\"C. Gurrin\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b02efcd0b2c26c9e68b3807d7e958af763b56ab\",\"title\":\"The impact of temporal regularisation in egocentric saliency prediction\",\"url\":\"https://www.semanticscholar.org/paper/1b02efcd0b2c26c9e68b3807d7e958af763b56ab\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23999143\",\"name\":\"Deng-Ping Fan\"},{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"37535930\",\"name\":\"Ming-Ming Cheng\"},{\"authorId\":\"145953515\",\"name\":\"J. Shen\"}],\"doi\":\"10.1109/CVPR.2019.00875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21648dd098a3c0ae871844b254775fec14df6904\",\"title\":\"Shifting More Attention to Video Salient Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/21648dd098a3c0ae871844b254775fec14df6904\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2001.03063\",\"authors\":[{\"authorId\":\"3200442\",\"name\":\"A. Tsiami\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/cvpr42600.2020.00482\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e296a1f529d9caa125e8c6a56cac61423e04b41b\",\"title\":\"STAViS: Spatio-Temporal AudioVisual Saliency Network\",\"url\":\"https://www.semanticscholar.org/paper/e296a1f529d9caa125e8c6a56cac61423e04b41b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3327256\",\"name\":\"M. Favorskaya\"},{\"authorId\":\"122575918\",\"name\":\"L. C. Jain\"}],\"doi\":\"10.31799/1684-8853-2019-3-10-36\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6fe7379f40fd2354e5bbe5f8be79069feb0c6ea2\",\"title\":\"Saliency detection in deep learning era: trends of development\",\"url\":\"https://www.semanticscholar.org/paper/6fe7379f40fd2354e5bbe5f8be79069feb0c6ea2\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2007931\",\"name\":\"Kao Zhang\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"},{\"authorId\":\"1792817\",\"name\":\"S. Liu\"}],\"doi\":\"10.1109/TIP.2020.3036749\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51b23ce1f48d769a31a4a4f4982fbfe5d4de41f4\",\"title\":\"A Spatial-Temporal Recurrent Neural Network for Video Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/51b23ce1f48d769a31a4a4f4982fbfe5d4de41f4\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51244663\",\"name\":\"Panagiotis Linardos\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fd369da5f97e9d3ca6863030ec10a689754bcbb6\",\"title\":\"LINARDOS ET AL: TEMPORAL RECURRENCES FOR VIDEO SALIENCY PREDICTION 1 Temporal Recurrences for Video Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/fd369da5f97e9d3ca6863030ec10a689754bcbb6\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1806489\",\"name\":\"Woo-Sup Han\"},{\"authorId\":\"32589409\",\"name\":\"Il Song Han\"}],\"doi\":\"10.1007/978-3-030-46643-5_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"10952f1ae952fa5bc5078163b32d5102423a46c0\",\"title\":\"Multimodal Brain Image Segmentation and Analysis with Neuromorphic Attention-Based Learning\",\"url\":\"https://www.semanticscholar.org/paper/10952f1ae952fa5bc5078163b32d5102423a46c0\",\"venue\":\"BrainLes@MICCAI\",\"year\":2019},{\"arxivId\":\"2003.05477\",\"authors\":[{\"authorId\":\"1430747783\",\"name\":\"Richard Droste\"},{\"authorId\":\"2840852\",\"name\":\"Jianbo Jiao\"},{\"authorId\":\"144870105\",\"name\":\"J. Noble\"}],\"doi\":\"10.1007/978-3-030-58558-7_25\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b01cb5c42ba693c84967bfd2ec94e420586164cc\",\"title\":\"Unified Image and Video Saliency Modeling\",\"url\":\"https://www.semanticscholar.org/paper/b01cb5c42ba693c84967bfd2ec94e420586164cc\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xinyi Wu\"},{\"authorId\":\"153006222\",\"name\":\"Zhenyao Wu\"},{\"authorId\":\"24392163\",\"name\":\"J. Zhang\"},{\"authorId\":\"47668707\",\"name\":\"Li-li Ju\"},{\"authorId\":\"40696794\",\"name\":\"Song Wang\"}],\"doi\":\"10.1609/AAAI.V34I07.6927\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3d51ce14a3be17ffc37bb94f9664429d0dad4564\",\"title\":\"SalSAC: A Video Saliency Prediction Model with Shuffled Attentions and Correlation-Based ConvLSTM\",\"url\":\"https://www.semanticscholar.org/paper/3d51ce14a3be17ffc37bb94f9664429d0dad4564\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"145953515\",\"name\":\"J. Shen\"},{\"authorId\":\"39380386\",\"name\":\"Jianwen Xie\"},{\"authorId\":\"37535930\",\"name\":\"Ming-Ming Cheng\"},{\"authorId\":\"1805398\",\"name\":\"Haibin Ling\"},{\"authorId\":\"51004202\",\"name\":\"A. Borji\"}],\"doi\":\"10.1109/TPAMI.2019.2924417\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60782ad3f40d13fae19fbe26b6f7c0ac5011f83d\",\"title\":\"Revisiting Video Saliency Prediction in the Deep Learning Era\",\"url\":\"https://www.semanticscholar.org/paper/60782ad3f40d13fae19fbe26b6f7c0ac5011f83d\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10604706\",\"name\":\"Ali Selman Aydin\"},{\"authorId\":\"113653487\",\"name\":\"Shirin Feiz\"},{\"authorId\":\"145070406\",\"name\":\"V. Ashok\"},{\"authorId\":\"1519965383\",\"name\":\"I. Ramakrishnan\"}],\"doi\":\"10.1145/3377325.3377494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b581597fe0576ebc88a0aade41cbf1366a33ecb8\",\"title\":\"Towards making videos accessible for low vision screen magnifier users\",\"url\":\"https://www.semanticscholar.org/paper/b581597fe0576ebc88a0aade41cbf1366a33ecb8\",\"venue\":\"IUI\",\"year\":2020},{\"arxivId\":\"1907.01869\",\"authors\":[{\"authorId\":\"51244663\",\"name\":\"Panagiotis Linardos\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"144011211\",\"name\":\"J. J. Nieto\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"3100480\",\"name\":\"Xavier Giro-i-Nieto\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d826cd1c9ad907ae1c57a14740eb84a3075f2725\",\"title\":\"Simple vs complex temporal recurrences for video saliency prediction\",\"url\":\"https://www.semanticscholar.org/paper/d826cd1c9ad907ae1c57a14740eb84a3075f2725\",\"venue\":\"BMVC\",\"year\":2019}],\"corpusId\":52044382,\"doi\":\"10.1109/CVPR.2018.00783\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"9a2fae46c67189fb2aea33f12091772e635361f1\",\"references\":[{\"arxivId\":\"1612.08242\",\"authors\":[{\"authorId\":\"40497777\",\"name\":\"Joseph Redmon\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2017.690\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7d39d69b23424446f0400ef603b2e3e22d0309d6\",\"title\":\"YOLO9000: Better, Faster, Stronger\",\"url\":\"https://www.semanticscholar.org/paper/7d39d69b23424446f0400ef603b2e3e22d0309d6\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1941997\",\"name\":\"Chenlei Guo\"},{\"authorId\":\"29906646\",\"name\":\"L. Zhang\"}],\"doi\":\"10.1109/TIP.2009.2030969\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"01825573781674bcf85d0f5d2ec456842f75ad3c\",\"title\":\"A Novel Multiresolution Spatiotemporal Saliency Detection Model and Its Applications in Image and Video Compression\",\"url\":\"https://www.semanticscholar.org/paper/01825573781674bcf85d0f5d2ec456842f75ad3c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46837630\",\"name\":\"Po-He Tseng\"},{\"authorId\":\"34414200\",\"name\":\"R. Carmi\"},{\"authorId\":\"145777626\",\"name\":\"I. G. Cameron\"},{\"authorId\":\"144440243\",\"name\":\"D. Munoz\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":\"10.1167/9.7.4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fe311d83782515767e5584c4fa0239daee14075c\",\"title\":\"Quantifying center bias of observers in free viewing of dynamic natural scenes.\",\"url\":\"https://www.semanticscholar.org/paper/fe311d83782515767e5584c4fa0239daee14075c\",\"venue\":\"Journal of vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48493294\",\"name\":\"V. Mahadevan\"},{\"authorId\":\"1699559\",\"name\":\"N. Vasconcelos\"}],\"doi\":\"10.1109/TPAMI.2009.112\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62eb421cdac9de9255578f14fba55146d958be44\",\"title\":\"Spatiotemporal Saliency in Dynamic Scenes\",\"url\":\"https://www.semanticscholar.org/paper/62eb421cdac9de9255578f14fba55146d958be44\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2286630\",\"name\":\"E. Vig\"},{\"authorId\":\"1944405\",\"name\":\"M. Dorr\"},{\"authorId\":\"2042941\",\"name\":\"D. Cox\"}],\"doi\":\"10.1109/CVPR.2014.358\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8693c32dff29e851760fa0b6af464050ffc383d6\",\"title\":\"Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images\",\"url\":\"https://www.semanticscholar.org/paper/8693c32dff29e851760fa0b6af464050ffc383d6\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7667827\",\"name\":\"L. Duan\"},{\"authorId\":\"2026288\",\"name\":\"Tao Xi\"},{\"authorId\":\"48058046\",\"name\":\"S. Cui\"},{\"authorId\":\"144097734\",\"name\":\"H. Qi\"},{\"authorId\":\"1747569\",\"name\":\"A. Bovik\"}],\"doi\":\"10.1016/j.image.2015.08.005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6018f3402ecc2aa657909a9eb4034b0fed328ff7\",\"title\":\"A spatiotemporal weighted dissimilarity-based method for video saliency detection\",\"url\":\"https://www.semanticscholar.org/paper/6018f3402ecc2aa657909a9eb4034b0fed328ff7\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701293\",\"name\":\"J. Zhang\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"}],\"doi\":\"10.1109/TPAMI.2015.2473844\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"48f6af7b895465440ea13e6e85ae38d7209d5a5c\",\"title\":\"Exploiting Surroundedness for Saliency Detection: A Boolean Map Approach\",\"url\":\"https://www.semanticscholar.org/paper/48f6af7b895465440ea13e6e85ae38d7209d5a5c\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2296560\",\"name\":\"Yasin Kavak\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"}],\"doi\":\"10.1016/j.image.2016.11.003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf54a133c89f730adc5ea12c3ac646971120781c\",\"title\":\"A comparative study for feature integration strategies in dynamic saliency estimation\",\"url\":\"https://www.semanticscholar.org/paper/cf54a133c89f730adc5ea12c3ac646971120781c\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2017},{\"arxivId\":\"1312.7570\",\"authors\":[{\"authorId\":\"37710702\",\"name\":\"Stefan Mathe\"},{\"authorId\":\"1781120\",\"name\":\"C. Sminchisescu\"}],\"doi\":\"10.1109/TPAMI.2014.2366154\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e254d0e670ce8217e74aaf1421c72e63e62929e3\",\"title\":\"Actions in the Eye: Dynamic Gaze Datasets and Learnt Saliency Models for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/e254d0e670ce8217e74aaf1421c72e63e62929e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681157\",\"name\":\"M. Mancas\"},{\"authorId\":\"1843875\",\"name\":\"V. Ferrera\"},{\"authorId\":\"2989773\",\"name\":\"Nicolas Riche\"},{\"authorId\":\"145144649\",\"name\":\"J. Taylor\"}],\"doi\":\"10.1007/978-1-4939-3435-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d2f7e840c6479b14c745069193135e7b18177d6\",\"title\":\"From Human Attention to Computational Attention\",\"url\":\"https://www.semanticscholar.org/paper/3d2f7e840c6479b14c745069193135e7b18177d6\",\"venue\":\"Springer Series in Cognitive and Neural Systems\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"}],\"doi\":\"10.1007/978-3-319-46454-1_49\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a6d6d0de32939a5d15d2abfb04d131884d2cadc4\",\"title\":\"Where Should Saliency Models Look Next?\",\"url\":\"https://www.semanticscholar.org/paper/a6d6d0de32939a5d15d2abfb04d131884d2cadc4\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143612308\",\"name\":\"D. Parks\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":\"10.1016/j.visres.2014.10.027\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"de133b03624e91db8861ca94f075998dc26259ee\",\"title\":\"Augmented saliency model using automatic 3D head pose detection and learned gaze following in natural scenes\",\"url\":\"https://www.semanticscholar.org/paper/de133b03624e91db8861ca94f075998dc26259ee\",\"venue\":\"Vision Research\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2306103\",\"name\":\"Parag K. Mital\"},{\"authorId\":\"145165599\",\"name\":\"T. Smith\"},{\"authorId\":\"3252072\",\"name\":\"R. L. Hill\"},{\"authorId\":\"1889476\",\"name\":\"J. Henderson\"}],\"doi\":\"10.1007/s12559-010-9074-z\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"27266e3be4d9b9572e8e11f4ef34110a2a00f515\",\"title\":\"Clustering of Gaze During Dynamic Scene Viewing is Predicted by Motion\",\"url\":\"https://www.semanticscholar.org/paper/27266e3be4d9b9572e8e11f4ef34110a2a00f515\",\"venue\":\"Cognitive Computation\",\"year\":2010},{\"arxivId\":\"1603.08199\",\"authors\":[{\"authorId\":\"1809420\",\"name\":\"Loris Bazzani\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f3158ce4e1c9e908e7d06533d711d84205c973b9\",\"title\":\"Recurrent Mixture Density Network for Spatiotemporal Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/f3158ce4e1c9e908e7d06533d711d84205c973b9\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2502152\",\"name\":\"A. Treisman\"},{\"authorId\":\"46475250\",\"name\":\"G. Gelade\"}],\"doi\":\"10.1016/0010-0285(80)90005-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"76361a44e145732a39dbc68d9418871038c83be2\",\"title\":\"A feature-integration theory of attention\",\"url\":\"https://www.semanticscholar.org/paper/76361a44e145732a39dbc68d9418871038c83be2\",\"venue\":\"Cognitive Psychology\",\"year\":1980},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145624227\",\"name\":\"C. Koch\"},{\"authorId\":\"1743045\",\"name\":\"S. Ullman\"}],\"doi\":\"10.1007/978-94-009-3833-5_5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0ce672e6ef09e9ab4c43f1eb75443dc7a5cb38c6\",\"title\":\"Shifts in selective visual attention: towards the underlying neural circuitry.\",\"url\":\"https://www.semanticscholar.org/paper/0ce672e6ef09e9ab4c43f1eb75443dc7a5cb38c6\",\"venue\":\"Human neurobiology\",\"year\":1985},{\"arxivId\":\"1411.1045\",\"authors\":[{\"authorId\":\"2997408\",\"name\":\"Matthias K\\u00fcmmerer\"},{\"authorId\":\"2073063\",\"name\":\"L. Theis\"},{\"authorId\":\"1731199\",\"name\":\"M. Bethge\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"652e652bca63f60c2c5f5840d4a34bb743f699b9\",\"title\":\"Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet\",\"url\":\"https://www.semanticscholar.org/paper/652e652bca63f60c2c5f5840d4a34bb743f699b9\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"2037692\",\"name\":\"Dicky N. Sihite\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":\"10.1016/j.visres.2013.07.016\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad7019eba4380a35c347f7ed7acd040800e14305\",\"title\":\"What stands out in a scene? A study of human explicit saliency judgment\",\"url\":\"https://www.semanticscholar.org/paper/ad7019eba4380a35c347f7ed7acd040800e14305\",\"venue\":\"Vision Research\",\"year\":2013},{\"arxivId\":\"1609.01064\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/ICPR.2016.7900174\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"57e143c96a41ba81d9d59120a1cd0dc04905d3f1\",\"title\":\"A deep multi-level network for saliency prediction\",\"url\":\"https://www.semanticscholar.org/paper/57e143c96a41ba81d9d59120a1cd0dc04905d3f1\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152627906\",\"name\":\"T. Judd\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"daef3fdc4190927c063ae94c12437cf82a6d1c20\",\"title\":\"A Benchmark of Computational Models of Saliency to Predict Human Fixations\",\"url\":\"https://www.semanticscholar.org/paper/daef3fdc4190927c063ae94c12437cf82a6d1c20\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2675203\",\"name\":\"Seyed Hossein Khatoonabadi\"},{\"authorId\":\"1699559\",\"name\":\"N. Vasconcelos\"},{\"authorId\":\"1730101\",\"name\":\"I. Bajic\"},{\"authorId\":\"37207452\",\"name\":\"Yufeng Shan\"}],\"doi\":\"10.1109/CVPR.2015.7299189\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"12c1df9b8c6e55bd6597b335dd9d9bac9fe5ee26\",\"title\":\"How many bits does it take for a stimulus to be salient?\",\"url\":\"https://www.semanticscholar.org/paper/12c1df9b8c6e55bd6597b335dd9d9bac9fe5ee26\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34646933\",\"name\":\"T. V. Nguyen\"},{\"authorId\":\"1775497\",\"name\":\"Mengdi Xu\"},{\"authorId\":\"36264491\",\"name\":\"Guangyu Gao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":\"10.1145/2502081.2502128\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"04f74bf0d88ca566cd75804e3e48fc36c22c47f4\",\"title\":\"Static saliency vs. dynamic saliency: a comparative study\",\"url\":\"https://www.semanticscholar.org/paper/04f74bf0d88ca566cd75804e3e48fc36c22c47f4\",\"venue\":\"MM '13\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2060684\",\"name\":\"Monica S. Castelhano\"},{\"authorId\":\"2437483\",\"name\":\"Mareike Wieth\"},{\"authorId\":\"1889476\",\"name\":\"J. Henderson\"}],\"doi\":\"10.1007/978-3-540-77343-6_16\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e126541f9fef48df3d23f8d5f1d4be98557bdb6\",\"title\":\"I See What You See: Eye Movements in Real-World Scenes Are Affected by Perceived Direction of Gaze\",\"url\":\"https://www.semanticscholar.org/paper/2e126541f9fef48df3d23f8d5f1d4be98557bdb6\",\"venue\":\"WAPCV\",\"year\":2007},{\"arxivId\":\"1611.09571\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/TIP.2018.2851672\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b7e336a7d3cd82ae8cd5e85d3cb62f0b6091a0e5\",\"title\":\"Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model\",\"url\":\"https://www.semanticscholar.org/paper/b7e336a7d3cd82ae8cd5e85d3cb62f0b6091a0e5\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1504.06755\",\"authors\":[{\"authorId\":\"2366042\",\"name\":\"Pingmei Xu\"},{\"authorId\":\"1865091\",\"name\":\"Krista A. Ehinger\"},{\"authorId\":\"2507239\",\"name\":\"Y. Zhang\"},{\"authorId\":\"37737599\",\"name\":\"Adam Finkelstein\"},{\"authorId\":\"1697413\",\"name\":\"S. Kulkarni\"},{\"authorId\":\"40599257\",\"name\":\"J. Xiao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3433627f803953280b66ae1576d083fc9a68385a\",\"title\":\"TurkerGaze: Crowdsourcing Saliency with Webcam based Eye Tracking\",\"url\":\"https://www.semanticscholar.org/paper/3433627f803953280b66ae1576d083fc9a68385a\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1782447\",\"name\":\"Yury Gitman\"},{\"authorId\":\"34966076\",\"name\":\"M. Erofeev\"},{\"authorId\":\"1776883\",\"name\":\"D. Vatolin\"},{\"authorId\":\"2575557\",\"name\":\"Bolshakov Andrey\"},{\"authorId\":\"152750350\",\"name\":\"A. Fedorov\"}],\"doi\":\"10.1109/ICIP.2014.7025220\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"68b03a697e25285d4e6456252b418bdf8646e532\",\"title\":\"Semiautomatic visual-attention modeling and its application to video compression\",\"url\":\"https://www.semanticscholar.org/paper/68b03a697e25285d4e6456252b418bdf8646e532\",\"venue\":\"2014 IEEE International Conference on Image Processing (ICIP)\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2989773\",\"name\":\"Nicolas Riche\"},{\"authorId\":\"1681157\",\"name\":\"M. Mancas\"},{\"authorId\":\"2896417\",\"name\":\"Matthieu Duvinage\"},{\"authorId\":\"3028207\",\"name\":\"Makiese Mibulumukini\"},{\"authorId\":\"50276543\",\"name\":\"B. Gosselin\"},{\"authorId\":\"49164810\",\"name\":\"T. Dutoit\"}],\"doi\":\"10.1016/j.image.2013.03.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83ec029e1a7b41669f8254b24c070050d9d01d75\",\"title\":\"RARE2012: A multi-scale rarity-based saliency detection with its comparative statistical analysis\",\"url\":\"https://www.semanticscholar.org/paper/83ec029e1a7b41669f8254b24c070050d9d01d75\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3084719\",\"name\":\"Xiaodi Hou\"},{\"authorId\":\"5381034\",\"name\":\"L. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4e7ec75337766f2b649bb9f99c6887ee0f1674a6\",\"title\":\"Dynamic visual attention: searching for coding length increments\",\"url\":\"https://www.semanticscholar.org/paper/4e7ec75337766f2b649bb9f99c6887ee0f1674a6\",\"venue\":\"NIPS\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47382812\",\"name\":\"P. Ricciardelli\"},{\"authorId\":\"2250559\",\"name\":\"E. Bricolo\"},{\"authorId\":\"2829523\",\"name\":\"S. Aglioti\"},{\"authorId\":\"1811494\",\"name\":\"L. Chelazzi\"}],\"doi\":\"10.1097/00001756-200212030-00018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e63461dda5fd074d2d2361960dd64e435c54b96\",\"title\":\"My eyes want to look where your eyes are looking: Exploring the tendency to imitate another individual's gaze\",\"url\":\"https://www.semanticscholar.org/paper/4e63461dda5fd074d2d2361960dd64e435c54b96\",\"venue\":\"Neuroreport\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144645142\",\"name\":\"M. Jiang\"},{\"authorId\":\"1961257\",\"name\":\"Shengsheng Huang\"},{\"authorId\":\"2104164\",\"name\":\"Juanyong Duan\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"}],\"doi\":\"10.1109/CVPR.2015.7298710\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c71db5d3546e22227662ee0f0ce586495ef18899\",\"title\":\"SALICON: Saliency in Context\",\"url\":\"https://www.semanticscholar.org/paper/c71db5d3546e22227662ee0f0ce586495ef18899\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4639656\",\"name\":\"Y. Fang\"},{\"authorId\":\"41210105\",\"name\":\"Zhou Wang\"},{\"authorId\":\"144968898\",\"name\":\"W. Lin\"}],\"doi\":\"10.1109/ICME.2013.6607572\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d24fda5e12ff14eaf553723547a2400bf0c20be\",\"title\":\"Video saliency incorporating spatiotemporal cues and uncertainty weighting\",\"url\":\"https://www.semanticscholar.org/paper/2d24fda5e12ff14eaf553723547a2400bf0c20be\",\"venue\":\"ICME\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34040188\",\"name\":\"G. Kuhn\"},{\"authorId\":\"3124688\",\"name\":\"A. Kingstone\"}],\"doi\":\"10.3758/APP.71.2.314\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e360ee5819f913f701412eed39e52517c1b1ec5\",\"title\":\"Look away! Eyes and arrows engage oculomotor responses automatically\",\"url\":\"https://www.semanticscholar.org/paper/1e360ee5819f913f701412eed39e52517c1b1ec5\",\"venue\":\"Attention, perception & psychophysics\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742936\",\"name\":\"S. Ramanathan\"},{\"authorId\":\"3180583\",\"name\":\"Victoria Yanulevskaya\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1145/2072298.2072305\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4e7e405ef50372a1f09ac0de67b1f8a819c7e14\",\"title\":\"Can computers learn from humans to see better?: inferring scene semantics from viewers' eye movements\",\"url\":\"https://www.semanticscholar.org/paper/a4e7e405ef50372a1f09ac0de67b1f8a819c7e14\",\"venue\":\"MM '11\",\"year\":2011},{\"arxivId\":\"1603.00845\",\"authors\":[{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"2470219\",\"name\":\"E. Sayrol\"},{\"authorId\":\"3100480\",\"name\":\"Xavier Giro-i-Nieto\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"}],\"doi\":\"10.1109/CVPR.2016.71\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9528e2e8c20517ab916f803c0371abb4f0ed488b\",\"title\":\"Shallow and Deep Convolutional Networks for Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/9528e2e8c20517ab916f803c0371abb4f0ed488b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48416279\",\"name\":\"J. Shen\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":\"10.1016/j.visres.2012.06.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e680dbfd3a8f0f4dde6babfd27a4c882e202802\",\"title\":\"Top-down influences on visual attention during listening are modulated by observer sex\",\"url\":\"https://www.semanticscholar.org/paper/8e680dbfd3a8f0f4dde6babfd27a4c882e202802\",\"venue\":\"Vision Research\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4639656\",\"name\":\"Y. Fang\"},{\"authorId\":\"144968898\",\"name\":\"W. Lin\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"},{\"authorId\":\"34224146\",\"name\":\"C. Tsai\"},{\"authorId\":\"1685088\",\"name\":\"Chia-Wen Lin\"}],\"doi\":\"10.1109/TCSVT.2013.2273613\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4bda701ba50feb0344bea852f4db908b2cc9352\",\"title\":\"A Video Saliency Detection Model in Compressed Domain\",\"url\":\"https://www.semanticscholar.org/paper/b4bda701ba50feb0344bea852f4db908b2cc9352\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38111179\",\"name\":\"Siavash Gorji\"},{\"authorId\":\"47125588\",\"name\":\"James J. Clark\"}],\"doi\":\"10.1109/CVPR.2017.370\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9285f4a6a06e975bde3ae3267fccd971d4fff98a\",\"title\":\"Attentional Push: A Deep Convolutional Network for Augmenting Image Salience with Shared Attention Modeling in Social Scenes\",\"url\":\"https://www.semanticscholar.org/paper/9285f4a6a06e975bde3ae3267fccd971d4fff98a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1604.03605\",\"authors\":[{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"152627906\",\"name\":\"T. Judd\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"}],\"doi\":\"10.1109/TPAMI.2018.2815601\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4\",\"title\":\"What Do Different Evaluation Metrics Tell Us About Saliency Models?\",\"url\":\"https://www.semanticscholar.org/paper/bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1603.03669\",\"authors\":[{\"authorId\":\"2190047\",\"name\":\"G. Leifman\"},{\"authorId\":\"40635259\",\"name\":\"D. Rudoy\"},{\"authorId\":\"2161276\",\"name\":\"Tristan Swedish\"},{\"authorId\":\"1398734187\",\"name\":\"E. Bayro-Corrochano\"},{\"authorId\":\"145711633\",\"name\":\"R. Raskar\"}],\"doi\":\"10.1109/ICCV.2017.188\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca1ff286ad8eba2aa6bc442f0f8321572fd0b090\",\"title\":\"Learning Gaze Transitions from Depth to Improve Video Saliency Estimation\",\"url\":\"https://www.semanticscholar.org/paper/ca1ff286ad8eba2aa6bc442f0f8321572fd0b090\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/ICCV.2017.160\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"241b86d3c71d14b8cc6044a425b047a0724cfdc9\",\"title\":\"Following Gaze in Video\",\"url\":\"https://www.semanticscholar.org/paper/241b86d3c71d14b8cc6044a425b047a0724cfdc9\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145131953\",\"name\":\"L. Wang\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"},{\"authorId\":\"1790976\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/ICCV.2015.357\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0\",\"title\":\"Visual Tracking with Fully Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144494743\",\"name\":\"Zhe Wu\"},{\"authorId\":\"145235479\",\"name\":\"L. Su\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"},{\"authorId\":\"143847266\",\"name\":\"Bo Wu\"},{\"authorId\":\"33122401\",\"name\":\"J. Li\"},{\"authorId\":\"46439250\",\"name\":\"Guorong Li\"}],\"doi\":\"10.1109/ICME.2016.7552929\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3252c6c520ae06bfa82084822f857519e262103b\",\"title\":\"Video saliency prediction with optimized optical flow and gravity center bias\",\"url\":\"https://www.semanticscholar.org/paper/3252c6c520ae06bfa82084822f857519e262103b\",\"venue\":\"2016 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7294d3ac0001e4b36c67aeb5c31d1db8ba1da23a\",\"title\":\"Where are they looking?\",\"url\":\"https://www.semanticscholar.org/paper/7294d3ac0001e4b36c67aeb5c31d1db8ba1da23a\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1703007\",\"name\":\"S. Lee\"},{\"authorId\":\"97243882\",\"name\":\"Y. Kim\"},{\"authorId\":\"1806479\",\"name\":\"S. Choi\"}],\"doi\":\"10.1109/6046.890059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7fd551cce8b043308a50460ccd12d19472a43b1f\",\"title\":\"Fast Scene Change Detection using Direct Feature Extraction from MPEG Compressed Videos\",\"url\":\"https://www.semanticscholar.org/paper/7fd551cce8b043308a50460ccd12d19472a43b1f\",\"venue\":\"IEEE Trans. Multim.\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7326223\",\"name\":\"L. Itti\"},{\"authorId\":\"145624227\",\"name\":\"C. Koch\"},{\"authorId\":\"3271571\",\"name\":\"E. Niebur\"}],\"doi\":\"10.1109/34.730558\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4816f0b6f0d05da3901441bfa5cc7be044b4da8b\",\"title\":\"A Model of Saliency-Based Visual Attention for Rapid Scene Analysis\",\"url\":\"https://www.semanticscholar.org/paper/4816f0b6f0d05da3901441bfa5cc7be044b4da8b\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143905571\",\"name\":\"Nian Liu\"},{\"authorId\":\"7181955\",\"name\":\"J. Han\"},{\"authorId\":\"39901030\",\"name\":\"Dingwen Zhang\"},{\"authorId\":\"2330182\",\"name\":\"S. Wen\"},{\"authorId\":\"145251057\",\"name\":\"T. Liu\"}],\"doi\":\"10.1109/CVPR.2015.7298633\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c6551c3c5e4c9779b2bbc6c8edcdeab545bc824d\",\"title\":\"Predicting eye fixations using convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/c6551c3c5e4c9779b2bbc6c8edcdeab545bc824d\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143775672\",\"name\":\"Tilke Judd\"},{\"authorId\":\"1865091\",\"name\":\"Krista A. Ehinger\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/ICCV.2009.5459462\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c7614898de1cfee1a3a1c564f52c1e67879c29b3\",\"title\":\"Learning to predict where humans look\",\"url\":\"https://www.semanticscholar.org/paper/c7614898de1cfee1a3a1c564f52c1e67879c29b3\",\"venue\":\"2009 IEEE 12th International Conference on Computer Vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680975\",\"name\":\"R. Rosenholtz\"},{\"authorId\":\"39602599\",\"name\":\"Amal Dorai\"},{\"authorId\":\"39246870\",\"name\":\"R. Freeman\"}],\"doi\":\"10.1145/1870076.1870080\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5032d00a307adf967a4265a208fd01048210fb1\",\"title\":\"Do predictions of visual perception aid design?\",\"url\":\"https://www.semanticscholar.org/paper/c5032d00a307adf967a4265a208fd01048210fb1\",\"venue\":\"TAP\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3119801\",\"name\":\"Xavier Glorot\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b71ac1e9fb49420d13e084ac67254a0bbd40f83f\",\"title\":\"Understanding the difficulty of training deep feedforward neural networks\",\"url\":\"https://www.semanticscholar.org/paper/b71ac1e9fb49420d13e084ac67254a0bbd40f83f\",\"venue\":\"AISTATS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40635259\",\"name\":\"D. Rudoy\"},{\"authorId\":\"1976171\",\"name\":\"D. Goldman\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1398327241\",\"name\":\"L. Zelnik-Manor\"}],\"doi\":\"10.1109/CVPR.2013.152\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3d7924e07914a147d7f6b868050d4edd093b952d\",\"title\":\"Learning Video Saliency from Human Gaze Using Candidate Selection\",\"url\":\"https://www.semanticscholar.org/paper/3d7924e07914a147d7f6b868050d4edd093b952d\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"143612308\",\"name\":\"D. Parks\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":\"10.1167/14.13.3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"46212252f24346c23d695df100301b33571e473b\",\"title\":\"Complementary effects of gaze direction and early saliency in guiding fixations during free viewing.\",\"url\":\"https://www.semanticscholar.org/paper/46212252f24346c23d695df100301b33571e473b\",\"venue\":\"Journal of vision\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1951233\",\"name\":\"Patrik Polatsek\"},{\"authorId\":\"2678977\",\"name\":\"W. Benesova\"},{\"authorId\":\"2065653\",\"name\":\"L. Paletta\"},{\"authorId\":\"2386156\",\"name\":\"R. Perko\"}],\"doi\":\"10.1109/LSP.2016.2523339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e632641321aff694788104c06d3e6736e0b8c39b\",\"title\":\"Novelty-based Spatiotemporal Saliency Detection for Prediction of Gaze in Egocentric Video\",\"url\":\"https://www.semanticscholar.org/paper/e632641321aff694788104c06d3e6736e0b8c39b\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3155984\",\"name\":\"Ben Benfold\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"}],\"doi\":\"10.5244/C.23.14\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e7c43db2ae9ac0c146f4b2f4f380445146f7d2da\",\"title\":\"Guiding Visual Surveillance by Tracking Human Attention\",\"url\":\"https://www.semanticscholar.org/paper/e7c43db2ae9ac0c146f4b2f4f380445146f7d2da\",\"venue\":\"BMVC\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3393537\",\"name\":\"Souad Chaabouni\"},{\"authorId\":\"1381345946\",\"name\":\"J. Benois-Pineau\"},{\"authorId\":\"3410172\",\"name\":\"C. Amar\"}],\"doi\":\"10.1109/ICIP.2016.7532629\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f5c5b5be74b6ce35f51722b40cdf94ef4cc7bab\",\"title\":\"Transfer learning with deep networks for saliency prediction in natural video\",\"url\":\"https://www.semanticscholar.org/paper/9f5c5b5be74b6ce35f51722b40cdf94ef4cc7bab\",\"venue\":\"2016 IEEE International Conference on Image Processing (ICIP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1941997\",\"name\":\"Chenlei Guo\"},{\"authorId\":\"144818403\",\"name\":\"Qi Ma\"},{\"authorId\":\"29906646\",\"name\":\"L. Zhang\"}],\"doi\":\"10.1109/CVPR.2008.4587715\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16f4cfd4cbf504a151827b5a94df5deafd4930ac\",\"title\":\"Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform\",\"url\":\"https://www.semanticscholar.org/paper/16f4cfd4cbf504a151827b5a94df5deafd4930ac\",\"venue\":\"2008 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2008},{\"arxivId\":\"1510.02927\",\"authors\":[{\"authorId\":\"1784761\",\"name\":\"S. Kruthiventi\"},{\"authorId\":\"50430041\",\"name\":\"Kumar Ayush\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1109/TIP.2017.2710620\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1295cbaf3b03de2eb8c79530289f5939d7819e5c\",\"title\":\"DeepFix: A Fully Convolutional Neural Network for Predicting Human Eye Fixations\",\"url\":\"https://www.semanticscholar.org/paper/1295cbaf3b03de2eb8c79530289f5939d7819e5c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145873572\",\"name\":\"J. F. Ferreira\"},{\"authorId\":\"143820964\",\"name\":\"J. Dias\"}],\"doi\":\"10.1109/TAMD.2014.2303072\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d2c442ef8ee1a5328680d9e41cba16b0dfbdac3\",\"title\":\"Attentional Mechanisms for Socially Interactive Robots\\u2013A Survey\",\"url\":\"https://www.semanticscholar.org/paper/9d2c442ef8ee1a5328680d9e41cba16b0dfbdac3\",\"venue\":\"IEEE Transactions on Autonomous Mental Development\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7326223\",\"name\":\"L. Itti\"},{\"authorId\":\"144902513\",\"name\":\"P. Baldi\"}],\"doi\":\"10.1109/CVPR.2005.40\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71860dd7a275f7794f2054829062849a3ea4a73f\",\"title\":\"A principled approach to detecting surprising events in video\",\"url\":\"https://www.semanticscholar.org/paper/71860dd7a275f7794f2054829062849a3ea4a73f\",\"venue\":\"2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)\",\"year\":2005},{\"arxivId\":\"1607.04730\",\"authors\":[{\"authorId\":\"34847250\",\"name\":\"\\u00c7agdas Bak\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dac2e8365df576e6d3f16b01d48e0d5e4156b81f\",\"title\":\"Two-Stream Convolutional Networks for Dynamic Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/dac2e8365df576e6d3f16b01d48e0d5e4156b81f\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145165599\",\"name\":\"T. Smith\"}],\"doi\":\"10.3167/PROJ.2012.060102\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d0e67593c8866929ba88c7ec901931329a15ece\",\"title\":\"The attentional theory of cinematic continuity\",\"url\":\"https://www.semanticscholar.org/paper/7d0e67593c8866929ba88c7ec901931329a15ece\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12525266\",\"name\":\"Xinyi Cui\"},{\"authorId\":\"50383828\",\"name\":\"Qingshan Liu\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1145/1631272.1631370\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3257d74e4ad050a1a518a53b0d23751f771f483\",\"title\":\"Temporal spectral residual: fast motion saliency detection\",\"url\":\"https://www.semanticscholar.org/paper/b3257d74e4ad050a1a518a53b0d23751f771f483\",\"venue\":\"MM '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743773\",\"name\":\"M. Xu\"},{\"authorId\":\"47420847\",\"name\":\"Lai Jiang\"},{\"authorId\":\"6485172\",\"name\":\"Xiaoyan Sun\"},{\"authorId\":\"3238659\",\"name\":\"Zhaoting Ye\"},{\"authorId\":\"1754571\",\"name\":\"Z. Wang\"}],\"doi\":\"10.1109/TIP.2016.2628583\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d3ed9103b9f9eee87ee0b1d3430b44c32bc140bc\",\"title\":\"Learning to Detect Video Saliency With HEVC Features\",\"url\":\"https://www.semanticscholar.org/paper/d3ed9103b9f9eee87ee0b1d3430b44c32bc140bc\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017}],\"title\":\"Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push\",\"topics\":[{\"topic\":\"Long short-term memory\",\"topicId\":\"117199\",\"url\":\"https://www.semanticscholar.org/topic/117199\"},{\"topic\":\"Gene regulatory network\",\"topicId\":\"22947\",\"url\":\"https://www.semanticscholar.org/topic/22947\"},{\"topic\":\"Convolutional neural network\",\"topicId\":\"29860\",\"url\":\"https://www.semanticscholar.org/topic/29860\"},{\"topic\":\"Eye tracking\",\"topicId\":\"7621\",\"url\":\"https://www.semanticscholar.org/topic/7621\"},{\"topic\":\"Kullback\\u2013Leibler divergence\",\"topicId\":\"21299\",\"url\":\"https://www.semanticscholar.org/topic/21299\"},{\"topic\":\"Image viewer\",\"topicId\":\"272923\",\"url\":\"https://www.semanticscholar.org/topic/272923\"},{\"topic\":\"Imperative programming\",\"topicId\":\"1485\",\"url\":\"https://www.semanticscholar.org/topic/1485\"}],\"url\":\"https://www.semanticscholar.org/paper/9a2fae46c67189fb2aea33f12091772e635361f1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}\n"