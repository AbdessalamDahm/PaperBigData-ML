"{\"abstract\":\"Cross-modal matching has been a highlighted research topic in both vision and language areas. Learning appropriate mining strategy to sample and weight informative pairs is crucial for the cross-modal matching performance. However, most existing metric learning methods are developed for unimodal matching, which is unsuitable for cross-modal matching on multimodal data with heterogeneous features. To address this problem, we propose a simple and interpretable universal weighting framework for cross-modal matching, which provides a tool to analyze the interpretability of various loss functions. Furthermore, we introduce a new polynomial loss under the universal weighting framework, which defines a weight function for the positive and negative informative pairs respectively. Experimental results on two image-text matching benchmarks and two video-text matching benchmarks validate the efficacy of the proposed method.\",\"arxivId\":\"2010.03403\",\"authors\":[{\"authorId\":\"1490652152\",\"name\":\"Jiwei Wei\",\"url\":\"https://www.semanticscholar.org/author/1490652152\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\",\"url\":\"https://www.semanticscholar.org/author/47158869\"},{\"authorId\":\"1524912498\",\"name\":\"Yang Yang\",\"url\":\"https://www.semanticscholar.org/author/1524912498\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\",\"url\":\"https://www.semanticscholar.org/author/50006507\"},{\"authorId\":null,\"name\":\"Zheng Wang\",\"url\":null},{\"authorId\":\"1724393\",\"name\":\"H. Shen\",\"url\":\"https://www.semanticscholar.org/author/1724393\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993657140\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3394171.3414025\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2bf257acf56fb214af71b123f1b734263242cc36\",\"title\":\"One-shot Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/2bf257acf56fb214af71b123f1b734263242cc36\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.09634\",\"authors\":[{\"authorId\":\"1624475253\",\"name\":\"Yujie Zhong\"},{\"authorId\":\"11246861\",\"name\":\"Linhai Xie\"},{\"authorId\":\"1421686725\",\"name\":\"Sen Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"},{\"authorId\":\"2666898\",\"name\":\"Yishu Miao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f416cdb1cd51a73fdb7f8f4194308798ae57033c\",\"title\":\"Watch and Learn: Mapping Language and Noisy Real-world Videos with Self-supervision\",\"url\":\"https://www.semanticscholar.org/paper/f416cdb1cd51a73fdb7f8f4194308798ae57033c\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":219615871,\"doi\":\"10.1109/CVPR42600.2020.01302\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"references\":[{\"arxivId\":\"1904.03451\",\"authors\":[{\"authorId\":\"2166891\",\"name\":\"S. Dey\"},{\"authorId\":\"40420775\",\"name\":\"Pau Riba\"},{\"authorId\":\"39083167\",\"name\":\"A. Dutta\"},{\"authorId\":\"143826881\",\"name\":\"J. Llad\\u00f3s\"},{\"authorId\":\"1705408\",\"name\":\"Yi-Zhe Song\"}],\"doi\":\"10.1109/CVPR.2019.00228\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c2b23d8bd04542b43d341515e39bfc169f9788f\",\"title\":\"Doodle to Search: Practical Zero-Shot Sketch-Based Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/5c2b23d8bd04542b43d341515e39bfc169f9788f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"2081160\",\"name\":\"S. Ji\"},{\"authorId\":\"1725522\",\"name\":\"X. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7214c15fef26ce470a285ab808a6be92d4193e7\",\"title\":\"Dual Dense Encoding for Zero-Example Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/a7214c15fef26ce470a285ab808a6be92d4193e7\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47908786\",\"name\":\"Y. Liu\"},{\"authorId\":\"1687503\",\"name\":\"Yanming Guo\"},{\"authorId\":\"143866184\",\"name\":\"E. Bakker\"},{\"authorId\":\"1731570\",\"name\":\"M. Lew\"}],\"doi\":\"10.1109/ICCV.2017.442\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"90208e8cbda1f39f3d06e41d97898408b13192a7\",\"title\":\"Learning a Recurrent Residual Fusion Network for Multimodal Matching\",\"url\":\"https://www.semanticscholar.org/paper/90208e8cbda1f39f3d06e41d97898408b13192a7\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1611.00471\",\"authors\":[{\"authorId\":\"34758272\",\"name\":\"H. Nam\"},{\"authorId\":\"2577039\",\"name\":\"Jung-Woo Ha\"},{\"authorId\":\"2947441\",\"name\":\"J. Kim\"}],\"doi\":\"10.1109/CVPR.2017.232\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f651593fa6c83d717fc961482696a53b6fca5ab5\",\"title\":\"Dual Attention Networks for Multimodal Reasoning and Matching\",\"url\":\"https://www.semanticscholar.org/paper/f651593fa6c83d717fc961482696a53b6fca5ab5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"1678783\",\"name\":\"D. Zhao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1007/s11263-016-0893-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ca7b4bbc20d81b15a8c592d0d5c7ae4851085c99\",\"title\":\"Recognizing an Action Using Its Name: A Knowledge-Based Approach\",\"url\":\"https://www.semanticscholar.org/paper/ca7b4bbc20d81b15a8c592d0d5c7ae4851085c99\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"50177639\",\"name\":\"Xiaoping Zhou\"},{\"authorId\":\"50812077\",\"name\":\"J. Yu\"},{\"authorId\":\"46173234\",\"name\":\"Y. Yang\"},{\"authorId\":\"49480212\",\"name\":\"L. Liu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TIP.2019.2899987\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d402bfeeee71b68a24d651f88b88d10cb8b7aaae\",\"title\":\"Scalable Zero-Shot Learning via Binary Visual-Semantic Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/d402bfeeee71b68a24d651f88b88d10cb8b7aaae\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2279670\",\"name\":\"Andrea Frome\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4aa4069693bee00d1b0759ca3df35e59284e9845\",\"title\":\"DeViSE: A Deep Visual-Semantic Embedding Model\",\"url\":\"https://www.semanticscholar.org/paper/4aa4069693bee00d1b0759ca3df35e59284e9845\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":\"1712.02036\",\"authors\":[{\"authorId\":\"144368930\",\"name\":\"Yan Huang\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"144143336\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1109/CVPR.2018.00645\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f322eef6a4c965910e03f6997b1bc2acd413e273\",\"title\":\"Learning Semantic Concepts and Order for Image and Sentence Matching\",\"url\":\"https://www.semanticscholar.org/paper/f322eef6a4c965910e03f6997b1bc2acd413e273\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1709.01362\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1109/TMM.2018.2832602\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"031af500679ae544d0fc614f938de45a07c87c82\",\"title\":\"Predicting Visual Features From Text for Image and Video Caption Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/031af500679ae544d0fc614f938de45a07c87c82\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":\"1412.6622\",\"authors\":[{\"authorId\":\"40555034\",\"name\":\"E. Hoffer\"},{\"authorId\":\"2048494\",\"name\":\"Nir Ailon\"}],\"doi\":\"10.1007/978-3-319-24261-3_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ac1a7d6630cd1c7b70c4c2c7bc92c40df1162ca\",\"title\":\"Deep Metric Learning Using Triplet Network\",\"url\":\"https://www.semanticscholar.org/paper/3ac1a7d6630cd1c7b70c4c2c7bc92c40df1162ca\",\"venue\":\"SIMBAD\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"143663452\",\"name\":\"Huimin Lu\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"27718163\",\"name\":\"Y. Yang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"40286455\",\"name\":\"Xuelong Li\"}],\"doi\":\"10.1109/TCYB.2019.2928180\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"07daca3acd1025b68b0bef27e451ffb9ea7f607b\",\"title\":\"Ternary Adversarial Networks With Self-Supervision for Zero-Shot Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/07daca3acd1025b68b0bef27e451ffb9ea7f607b\",\"venue\":\"IEEE Transactions on Cybernetics\",\"year\":2020},{\"arxivId\":\"1909.02701\",\"authors\":[{\"authorId\":\"49243413\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"2410227\",\"name\":\"Yulun Zhang\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"47003439\",\"name\":\"Yuanyuan Li\"},{\"authorId\":\"46956675\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/ICCV.2019.00475\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"26500af6bb97bf2dab1cc14dfb3b8b08fef67940\",\"title\":\"Visual Semantic Reasoning for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/26500af6bb97bf2dab1cc14dfb3b8b08fef67940\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490652152\",\"name\":\"Jiwei Wei\"},{\"authorId\":\"46285717\",\"name\":\"Y. Yang\"},{\"authorId\":\"46275512\",\"name\":\"J. Li\"},{\"authorId\":\"152366931\",\"name\":\"Lei Zhu\"},{\"authorId\":\"46911598\",\"name\":\"Lin Zuo\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3338533.3366552\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9e914760b93db2e079eb1f7c905aa7037aaa88a7\",\"title\":\"Residual Graph Convolutional Networks for Zero-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/9e914760b93db2e079eb1f7c905aa7037aaa88a7\",\"venue\":\"MMAsia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"145297531\",\"name\":\"A. Lai\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1162/tacl_a_00166\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"44040913380206991b1991daf1192942e038fe31\",\"title\":\"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\"url\":\"https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2867056\",\"name\":\"M. Hu\"},{\"authorId\":\"46173234\",\"name\":\"Y. Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":null,\"name\":\"Ning Xie\"},{\"authorId\":\"2248826\",\"name\":\"R. Hong\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TIP.2018.2890144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ea3b7a84f9a29f06510fffbc446009ddca9762f8\",\"title\":\"Collective Reconstructive Embeddings for Cross-Modal Hashing\",\"url\":\"https://www.semanticscholar.org/paper/ea3b7a84f9a29f06510fffbc446009ddca9762f8\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"1908.04011\",\"authors\":[{\"authorId\":\"134814700\",\"name\":\"T. Wang\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1145/3343031.3350875\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4e1be8dd4d303e8e8ffe4343b2200d7952d16c4\",\"title\":\"Matching Images and Text with Multi-modal Tensor Fusion and Re-ranking\",\"url\":\"https://www.semanticscholar.org/paper/b4e1be8dd4d303e8e8ffe4343b2200d7952d16c4\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2978170\",\"name\":\"Fartash Faghri\"},{\"authorId\":\"1793739\",\"name\":\"David J. Fleet\"},{\"authorId\":\"51131802\",\"name\":\"J. Kiros\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8\",\"title\":\"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives\",\"url\":\"https://www.semanticscholar.org/paper/f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yale Song\"},{\"authorId\":null,\"name\":\"Mohammad Soleymani\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Polysemous visualsemantic embedding for cross-modal retrieval\",\"url\":\"\",\"venue\":\"In CVPR,\",\"year\":2019},{\"arxivId\":\"1810.07212\",\"authors\":[{\"authorId\":\"3047890\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"}],\"doi\":\"10.1007/978-3-030-01261-8_23\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea133d0067740902bc26a082c842d9e7ba48ecf6\",\"title\":\"Cross-Modal and Hierarchical Modeling of Video and Text\",\"url\":\"https://www.semanticscholar.org/paper/ea133d0067740902bc26a082c842d9e7ba48ecf6\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"3428237\",\"name\":\"Juncheng Billy Li\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"2968713\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1145/3206025.3206064\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9dbca9da6a72ba3739813288b677888a6cf76272\",\"title\":\"Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/9dbca9da6a72ba3739813288b677888a6cf76272\",\"venue\":\"ICMR\",\"year\":2018},{\"arxivId\":\"1803.08024\",\"authors\":[{\"authorId\":\"1863953\",\"name\":\"Kuang-Huei Lee\"},{\"authorId\":\"1683647\",\"name\":\"X. Chen\"},{\"authorId\":\"144988571\",\"name\":\"Gang Hua\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"}],\"doi\":\"10.1007/978-3-030-01225-0_13\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"45dd2a3cd7c27f2e9509b023d702408f5ac11c9d\",\"title\":\"Stacked Cross Attention for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/45dd2a3cd7c27f2e9509b023d702408f5ac11c9d\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30296838\",\"name\":\"Bokun Wang\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3123266.3123326\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"61060bea27a3410260988540b627ccc5ba131822\",\"title\":\"Adversarial Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/61060bea27a3410260988540b627ccc5ba131822\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1708.02478\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TNNLS.2018.2851077\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d78c47093fbf3d85225fd502674aba4a29b3987\",\"title\":\"From Deterministic to Generative: Multimodal Stochastic RNNs for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7d78c47093fbf3d85225fd502674aba4a29b3987\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2019},{\"arxivId\":\"1711.06420\",\"authors\":[{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"39298199\",\"name\":\"Li Niu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"}],\"doi\":\"10.1109/CVPR.2018.00750\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"724b253a55e86ad230ba05c7eb78f249e09258d9\",\"title\":\"Look, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/724b253a55e86ad230ba05c7eb78f249e09258d9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1707.07998\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"31790073\",\"name\":\"C. Buehler\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1109/CVPR.2018.00636\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"title\":\"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"40893802\",\"name\":\"Luchen Liu\"},{\"authorId\":\"1524912498\",\"name\":\"Yang Yang\"},{\"authorId\":\"153030342\",\"name\":\"Xing M. Xu\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"120313754\",\"name\":\"Richang Hong\"}],\"doi\":\"10.1109/tkde.2020.2970050\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b81a7acfbb55bfae6ddf4a2ff0c0af06fec0e2d4\",\"title\":\"Exploiting Subspace Relation in Semantic Labels for Cross-modal Hashing\",\"url\":\"https://www.semanticscholar.org/paper/b81a7acfbb55bfae6ddf4a2ff0c0af06fec0e2d4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1605.00743\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"40381920\",\"name\":\"Tianbao Yang\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/CVPR.2016.17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1e4f2c791f81a5fe95e693644e002fb652adb39\",\"title\":\"Learning Attributes Equals Multi-Source Domain Generalization\",\"url\":\"https://www.semanticscholar.org/paper/e1e4f2c791f81a5fe95e693644e002fb652adb39\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46356361\",\"name\":\"H. Cui\"},{\"authorId\":\"152366931\",\"name\":\"Lei Zhu\"},{\"authorId\":\"31115284\",\"name\":\"Jingjing Li\"},{\"authorId\":\"46173234\",\"name\":\"Y. Yang\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"}],\"doi\":\"10.1109/TIP.2019.2940693\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4fdbb1d23a00d8af05e9431273c082a443f795c\",\"title\":\"Scalable Deep Hashing for Large-Scale Social Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d4fdbb1d23a00d8af05e9431273c082a443f795c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1906.04402\",\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"152714397\",\"name\":\"M. Soleymani\"}],\"doi\":\"10.1109/CVPR.2019.00208\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a39d5919531a56de0e36f6b76142041b5d508213\",\"title\":\"Polysemous Visual-Semantic Embedding for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/a39d5919531a56de0e36f6b76142041b5d508213\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"48099761\",\"name\":\"K. Paliwal\"}],\"doi\":\"10.1109/78.650093\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e23c34414e66118ecd9b08cf0cd4d016f59b0b85\",\"title\":\"Bidirectional recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/e23c34414e66118ecd9b08cf0cd4d016f59b0b85\",\"venue\":\"IEEE Trans. Signal Process.\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"144780837\",\"name\":\"Li He\"},{\"authorId\":\"143663452\",\"name\":\"Huimin Lu\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"}],\"doi\":\"10.1007/s11280-018-0541-x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da80d7363fcd6e415448d17d505cc3e2c9c09854\",\"title\":\"Deep adversarial metric learning for cross-modal retrieval\",\"url\":\"https://www.semanticscholar.org/paper/da80d7363fcd6e415448d17d505cc3e2c9c09854\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":\"1903.00252\",\"authors\":[{\"authorId\":\"40478933\",\"name\":\"Ji Liu\"},{\"authorId\":\"47058801\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018754\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"99617c6bf1b1412f76f05d7a46f7058cf53bd3b8\",\"title\":\"Optimal Projection Guided Transfer Hashing for Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/99617c6bf1b1412f76f05d7a46f7058cf53bd3b8\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47158537\",\"name\":\"Xing Xu\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"79327198\",\"name\":\"Y. Yang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TIP.2017.2676345\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e397b9cad491b76a2240fb7731c40cde4c2a2f7c\",\"title\":\"Learning Discriminative Binary Codes for Large-scale Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/e397b9cad491b76a2240fb7731c40cde4c2a2f7c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1754854\",\"name\":\"Venice Erin Liong\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"1689805\",\"name\":\"Y. Tan\"},{\"authorId\":\"49640256\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/TMM.2016.2646180\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"fe7ae13bf5fc80cf0837bacbe44905bd8749f03f\",\"title\":\"Deep Coupled Metric Learning for Cross-Modal Matching\",\"url\":\"https://www.semanticscholar.org/paper/fe7ae13bf5fc80cf0837bacbe44905bd8749f03f\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":\"1907.13487\",\"authors\":[{\"authorId\":\"40457423\",\"name\":\"Y. Liu\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"title\":\"Use What You Have: Video retrieval using representations from collaborative experts\",\"url\":\"https://www.semanticscholar.org/paper/b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/ICCV.2017.83\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"title\":\"Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"39491387\",\"name\":\"J. Zhou\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"}],\"doi\":\"10.1109/TIP.2018.2855422\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd3d94fac6a282414406716040b10c1746634ecd\",\"title\":\"Video Captioning by Adversarial LSTM\",\"url\":\"https://www.semanticscholar.org/paper/fd3d94fac6a282414406716040b10c1746634ecd\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018}],\"title\":\"Universal Weighting Metric Learning for Cross-Modal Matching\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"