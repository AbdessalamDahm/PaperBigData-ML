"{\"abstract\":\"Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip-reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose a novel approach with key design choices to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is four times more intelligible than previous works in this space.\",\"arxivId\":\"2005.08209\",\"authors\":[{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\",\"url\":\"https://www.semanticscholar.org/author/1380234931\"},{\"authorId\":\"41052499\",\"name\":\"R. Mukhopadhyay\",\"url\":\"https://www.semanticscholar.org/author/41052499\"},{\"authorId\":\"1744135\",\"name\":\"Vinay Namboodiri\",\"url\":\"https://www.semanticscholar.org/author/1744135\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\",\"url\":\"https://www.semanticscholar.org/author/1694502\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2011.07340\",\"authors\":[{\"authorId\":\"3083497\",\"name\":\"R. Yadav\"},{\"authorId\":\"50847752\",\"name\":\"Ashish Sardana\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1893306\",\"name\":\"Rajesh M. Hegde\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"76e48bb2092fdedab57dc6a4e8ccb06f930a392d\",\"title\":\"Speech Prediction in Silent Videos using Variational Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/76e48bb2092fdedab57dc6a4e8ccb06f930a392d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a593bf44e46bd4cb426c658ed87f4fce43f13aa\",\"title\":\"Lip-syncing Videos In The Wild\",\"url\":\"https://www.semanticscholar.org/paper/8a593bf44e46bd4cb426c658ed87f4fce43f13aa\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.04359\",\"authors\":[{\"authorId\":\"2007779424\",\"name\":\"Shrishti Saha Shetu\"},{\"authorId\":\"2536241\",\"name\":\"Soumitro Chakrabarty\"},{\"authorId\":\"120248637\",\"name\":\"Emanuel A. P. Habets\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f8aa59c6297fa179d12f901dd2c54f7d51709a7\",\"title\":\"An Empirical Study of Visual Features for DNN based Audio-Visual Speech Enhancement in Multi-talker Environments\",\"url\":\"https://www.semanticscholar.org/paper/0f8aa59c6297fa179d12f901dd2c54f7d51709a7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.10852\",\"authors\":[{\"authorId\":\"40121503\",\"name\":\"Sindhu B. Hegde\"},{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\"},{\"authorId\":\"41052499\",\"name\":\"R. Mukhopadhyay\"},{\"authorId\":\"1744135\",\"name\":\"Vinay Namboodiri\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2ca56c7bf6fae7096d35806412a1820a2a1712e\",\"title\":\"Visual Speech Enhancement Without A Real Visual Stream\",\"url\":\"https://www.semanticscholar.org/paper/c2ca56c7bf6fae7096d35806412a1820a2a1712e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2008.09586\",\"authors\":[{\"authorId\":\"9686806\",\"name\":\"Daniel Michelsanti\"},{\"authorId\":\"71668001\",\"name\":\"Zheng-Hua Tan\"},{\"authorId\":\"47180604\",\"name\":\"Shixiong Zhang\"},{\"authorId\":\"121983569\",\"name\":\"Yanchen Xu\"},{\"authorId\":\"143872259\",\"name\":\"M. Yu\"},{\"authorId\":null,\"name\":\"Dong Yu\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e1859067487893f6580e934e9ee3408a2fa8b7e1\",\"title\":\"An Overview of Deep-Learning-Based Audio-Visual Speech Enhancement and Separation\",\"url\":\"https://www.semanticscholar.org/paper/e1859067487893f6580e934e9ee3408a2fa8b7e1\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":218674585,\"doi\":\"10.1109/cvpr42600.2020.01381\",\"fieldsOfStudy\":[\"Computer Science\",\"Engineering\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"ba684a9966995e5a8c6efef46aeb57bd387ff51f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. Daniel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Griffin and Jae S . Lim . Signal estimation from modified short - time fourier transform\",\"url\":\"\",\"venue\":\"Advances in neural information processing systems\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1865384\",\"name\":\"D. Lewkowicz\"},{\"authorId\":\"1422508119\",\"name\":\"Amy M Hansen-Tift\"}],\"doi\":\"10.1073/pnas.1114783109\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a859c6c56184505e83dd0ee704bc9214f923694\",\"title\":\"Infants deploy selective attention to the mouth of a talking face when learning speech\",\"url\":\"https://www.semanticscholar.org/paper/3a859c6c56184505e83dd0ee704bc9214f923694\",\"venue\":\"Proceedings of the National Academy of Sciences\",\"year\":2012},{\"arxivId\":\"1708.01204\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICCVW.2017.61\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2b598c73e9335277106fcb8acdad6cda227c6cdf\",\"title\":\"Improved Speech Reconstruction from Silent Video\",\"url\":\"https://www.semanticscholar.org/paper/2b598c73e9335277106fcb8acdad6cda227c6cdf\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":\"1601.08188\",\"authors\":[{\"authorId\":\"143910530\",\"name\":\"Michael Wand\"},{\"authorId\":\"2865775\",\"name\":\"J. Koutn\\u00edk\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/ICASSP.2016.7472852\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"675419a4faaf71ab8178f79523308d3dd3392913\",\"title\":\"Lipreading with long short-term memory\",\"url\":\"https://www.semanticscholar.org/paper/675419a4faaf71ab8178f79523308d3dd3392913\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143967982\",\"name\":\"M. Cooke\"},{\"authorId\":\"32406400\",\"name\":\"J. Barker\"},{\"authorId\":\"75117630\",\"name\":\"S. Cunningham\"},{\"authorId\":\"48914950\",\"name\":\"X. Shao\"}],\"doi\":\"10.1121/1.2229005\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"title\":\"An audio-visual corpus for speech perception and automatic speech recognition.\",\"url\":\"https://www.semanticscholar.org/paper/5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":2006},{\"arxivId\":\"1806.06053\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2018-1943\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"04187519dc8c468f2b5b17442413ada7830068e5\",\"title\":\"Deep Lip Reading: a comparison of models and an online application\",\"url\":\"https://www.semanticscholar.org/paper/04187519dc8c468f2b5b17442413ada7830068e5\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shuiwang Ji\"},{\"authorId\":null,\"name\":\"Wei Xu\"},{\"authorId\":null,\"name\":\"Ming Yang\"},{\"authorId\":null,\"name\":\"Kai Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"tional neural networks for human action recognition\",\"url\":\"\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2012},{\"arxivId\":\"1705.08947\",\"authors\":[{\"authorId\":\"9544702\",\"name\":\"Andrew Gibiansky\"},{\"authorId\":\"2676352\",\"name\":\"Sercan \\u00d6. Arik\"},{\"authorId\":\"2040049\",\"name\":\"G. Diamos\"},{\"authorId\":\"152617830\",\"name\":\"J. Miller\"},{\"authorId\":\"12240560\",\"name\":\"Kainan Peng\"},{\"authorId\":\"34337724\",\"name\":\"W. Ping\"},{\"authorId\":\"34042420\",\"name\":\"Jonathan Raiman\"},{\"authorId\":\"2389316\",\"name\":\"Yanqi Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a5bcfda3b753f8266b9ba27d34fc86b6d374a1b\",\"title\":\"Deep Voice 2: Multi-Speaker Neural Text-to-Speech\",\"url\":\"https://www.semanticscholar.org/paper/5a5bcfda3b753f8266b9ba27d34fc86b6d374a1b\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1611.05358\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.367\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"title\":\"Lip Reading Sentences in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1708.05237\",\"authors\":[{\"authorId\":\"145599121\",\"name\":\"Shifeng Zhang\"},{\"authorId\":\"8362374\",\"name\":\"Xiangyu Zhu\"},{\"authorId\":\"145754448\",\"name\":\"Z. Lei\"},{\"authorId\":\"1704812\",\"name\":\"Hailin Shi\"},{\"authorId\":\"40509061\",\"name\":\"Xiaobo Wang\"},{\"authorId\":\"34679741\",\"name\":\"S. Li\"}],\"doi\":\"10.1109/ICCV.2017.30\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dcf71245addaf66a868221041aabe23c0a074312\",\"title\":\"S^3FD: Single Shot Scale-Invariant Face Detector\",\"url\":\"https://www.semanticscholar.org/paper/dcf71245addaf66a868221041aabe23c0a074312\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1712.05884\",\"authors\":[{\"authorId\":\"143724108\",\"name\":\"Jonathan Shen\"},{\"authorId\":\"34320634\",\"name\":\"R. Pang\"},{\"authorId\":\"39571582\",\"name\":\"Ron J. Weiss\"},{\"authorId\":\"46503039\",\"name\":\"M. Schuster\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"1867996\",\"name\":\"Z. Yang\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"36562659\",\"name\":\"Yu Zhang\"},{\"authorId\":null,\"name\":\"Yuxuan Wang\"},{\"authorId\":\"1380248814\",\"name\":\"R. Skerry-Ryan\"},{\"authorId\":\"2278009\",\"name\":\"R. A. Saurous\"},{\"authorId\":\"2259013\",\"name\":\"Yannis Agiomyrgiannakis\"},{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/ICASSP.2018.8461368\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1a2599e467e855f845dcbf9282f8bdbd97b85708\",\"title\":\"Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions\",\"url\":\"https://www.semanticscholar.org/paper/1a2599e467e855f845dcbf9282f8bdbd97b85708\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54184-6_6\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"title\":\"Lip Reading in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":\"1609.03499\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df0402517a7338ae28bc54acaac400de6b456a46\",\"title\":\"WaveNet: A Generative Model for Raw Audio\",\"url\":\"https://www.semanticscholar.org/paper/df0402517a7338ae28bc54acaac400de6b456a46\",\"venue\":\"SSW\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34337724\",\"name\":\"W. Ping\"},{\"authorId\":\"12240560\",\"name\":\"Kainan Peng\"},{\"authorId\":\"9544702\",\"name\":\"Andrew Gibiansky\"},{\"authorId\":\"2676352\",\"name\":\"Sercan \\u00d6. Arik\"},{\"authorId\":\"33850592\",\"name\":\"A. Kannan\"},{\"authorId\":\"46617804\",\"name\":\"Sharan Narang\"},{\"authorId\":\"34042420\",\"name\":\"Jonathan Raiman\"},{\"authorId\":\"144909221\",\"name\":\"J. Miller\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e142e52af456af42c6dfa55a15f11706b0c95531\",\"title\":\"Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/e142e52af456af42c6dfa55a15f11706b0c95531\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2478352\",\"name\":\"C. Kello\"},{\"authorId\":\"2546518\",\"name\":\"D. Plaut\"}],\"doi\":\"10.1121/1.1715112\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"84a2b78b323ec75cb513c24751cf918b45bea66e\",\"title\":\"A neural network model of the articulatory-acoustic forward mapping trained on recordings of articulatory parameters.\",\"url\":\"https://www.semanticscholar.org/paper/84a2b78b323ec75cb513c24751cf918b45bea66e\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":2004},{\"arxivId\":\"1804.04121\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2018-1400\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3\",\"title\":\"The Conversation: Deep Audio-Visual Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eoin Gillen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Griffin and Jae S . Lim . Signal estimation from modified short - time fourier transform\",\"url\":\"\",\"venue\":\"In ICASSP\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"143836295\",\"name\":\"W. Xu\"},{\"authorId\":\"41216159\",\"name\":\"Ming Yang\"},{\"authorId\":\"144782042\",\"name\":\"Kai Yu\"}],\"doi\":\"10.1109/TPAMI.2012.59\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"title\":\"3D Convolutional Neural Networks for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3902663\",\"name\":\"L. Iezzoni\"},{\"authorId\":\"152585944\",\"name\":\"B. O'day\"},{\"authorId\":\"2267100\",\"name\":\"Mary B Killeen\"},{\"authorId\":\"123468253\",\"name\":\"H. Harker\"}],\"doi\":\"10.7326/0003-4819-140-5-200403020-00011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b1b66c52566e1cd01c6ba5cb97b506303afe362\",\"title\":\"Communicating about Health Care: Observations from Persons Who Are Deaf or Hard of Hearing\",\"url\":\"https://www.semanticscholar.org/paper/2b1b66c52566e1cd01c6ba5cb97b506303afe362\",\"venue\":\"Annals of Internal Medicine\",\"year\":2004},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1904.02882\",\"authors\":[{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"35196056\",\"name\":\"V. Dang\"},{\"authorId\":\"49344058\",\"name\":\"R. Clark\"},{\"authorId\":null,\"name\":\"Yu Zhang\"},{\"authorId\":\"39571582\",\"name\":\"Ron J. Weiss\"},{\"authorId\":\"144025280\",\"name\":\"Y. Jia\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"}],\"doi\":\"10.21437/interspeech.2019-2441\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2789b6c84ba1422746246685001accba5563e7c1\",\"title\":\"LibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech\",\"url\":\"https://www.semanticscholar.org/paper/2789b6c84ba1422746246685001accba5563e7c1\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1907.01367\",\"authors\":[{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"34604467\",\"name\":\"Rohit Jain\"},{\"authorId\":\"66287688\",\"name\":\"Khwaja Mohd. Salik\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"},{\"authorId\":\"144221742\",\"name\":\"Yifang Yin\"},{\"authorId\":\"144809527\",\"name\":\"Roger Zimmermann\"}],\"doi\":\"10.1609/aaai.v33i01.33012588\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"174812d636fdb17c1cbe24b8d9888d340a5dbf56\",\"title\":\"Lipper: Synthesizing Thy Speech using Multi-View Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/174812d636fdb17c1cbe24b8d9888d340a5dbf56\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1806.04558\",\"authors\":[{\"authorId\":\"1691944\",\"name\":\"Ye Jia\"},{\"authorId\":\"46867976\",\"name\":\"Y. Zhang\"},{\"authorId\":\"39571582\",\"name\":\"Ron J. Weiss\"},{\"authorId\":\"19127909\",\"name\":\"Q. Wang\"},{\"authorId\":\"143724108\",\"name\":\"Jonathan Shen\"},{\"authorId\":\"153311281\",\"name\":\"F. Ren\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"14902530\",\"name\":\"P. Nguyen\"},{\"authorId\":\"34320634\",\"name\":\"R. Pang\"},{\"authorId\":\"1387449884\",\"name\":\"I. Lopez-Moreno\"},{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8fc09dfcff78ac9057ff0834a83d23eb38ca198a\",\"title\":\"Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/8fc09dfcff78ac9057ff0834a83d23eb38ca198a\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48190580\",\"name\":\"D. Griffin\"},{\"authorId\":\"49719219\",\"name\":\"J. Lim\"}],\"doi\":\"10.1109/ICASSP.1983.1172092\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14bc876fae55faf5669beb01667a4f3bd324a4f1\",\"title\":\"Signal estimation from modified short-time Fourier transform\",\"url\":\"https://www.semanticscholar.org/paper/14bc876fae55faf5669beb01667a4f3bd324a4f1\",\"venue\":\"ICASSP\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14884451\",\"name\":\"C. Lieu\"},{\"authorId\":\"3733652\",\"name\":\"G. Sadler\"},{\"authorId\":\"11142187\",\"name\":\"J. Fullerton\"},{\"authorId\":\"15045819\",\"name\":\"Paulette Deyo Stohlmann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d42a770ed3af52dd73adddef514a0d184915a581\",\"title\":\"Communication strategies for nurses interacting with patients who are deaf.\",\"url\":\"https://www.semanticscholar.org/paper/d42a770ed3af52dd73adddef514a0d184915a581\",\"venue\":\"Dermatology nursing\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2683590\",\"name\":\"C. Taal\"},{\"authorId\":\"1814710\",\"name\":\"R. Hendriks\"},{\"authorId\":\"3131859\",\"name\":\"R. Heusdens\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":\"10.1109/ICASSP.2010.5495701\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"584ab846ba7585ae5cc58aa75a3a6bfe2eb7c096\",\"title\":\"A short-time objective intelligibility measure for time-frequency weighted noisy speech\",\"url\":\"https://www.semanticscholar.org/paper/584ab846ba7585ae5cc58aa75a3a6bfe2eb7c096\",\"venue\":\"2010 IEEE International Conference on Acoustics, Speech and Signal Processing\",\"year\":2010},{\"arxivId\":\"1809.00496\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a46174aa635759070984ed7062c9402695bce830\",\"title\":\"LRS3-TED: a large-scale dataset for visual speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/a46174aa635759070984ed7062c9402695bce830\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"},{\"authorId\":\"2683590\",\"name\":\"C. Taal\"}],\"doi\":\"10.1109/TASLP.2016.2585878\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6284fbe48b5219ecd0a02d2e15fa83eeee56d675\",\"title\":\"An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers\",\"url\":\"https://www.semanticscholar.org/paper/6284fbe48b5219ecd0a02d2e15fa83eeee56d675\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144686633\",\"name\":\"N. Harte\"},{\"authorId\":\"23710772\",\"name\":\"E. Gillen\"}],\"doi\":\"10.1109/TMM.2015.2407694\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d6829332d0596659272451920d9ff778b0b400af\",\"title\":\"TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech\",\"url\":\"https://www.semanticscholar.org/paper/d6829332d0596659272451920d9ff778b0b400af\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2015},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47171715\",\"name\":\"D. Ebert\"},{\"authorId\":\"2905224\",\"name\":\"P. Heckerling\"}],\"doi\":\"10.1001/JAMA.1995.03520270061032\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d0ac2fc8361c5ca0718b7eb74d539e514b0afc7b\",\"title\":\"Communication with deaf patients. Knowledge, beliefs, and practices of physicians.\",\"url\":\"https://www.semanticscholar.org/paper/d0ac2fc8361c5ca0718b7eb74d539e514b0afc7b\",\"venue\":\"JAMA\",\"year\":1995},{\"arxivId\":\"1710.09798\",\"authors\":[{\"authorId\":\"145559125\",\"name\":\"H. Akbari\"},{\"authorId\":\"49583376\",\"name\":\"Himani Arora\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1686269\",\"name\":\"Nima Mesgarani\"}],\"doi\":\"10.1109/ICASSP.2018.8461856\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"edf0d924b740e436ca4641ccdc8ec3983132fa18\",\"title\":\"Lip2Audspec: Speech Reconstruction from Silent Lip Movements Video\",\"url\":\"https://www.semanticscholar.org/paper/edf0d924b740e436ca4641ccdc8ec3983132fa18\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":\"1910.10997\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"50487517\",\"name\":\"Xudong Xu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/ICCV.2019.00037\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82feef7ae3b3d5ea16ce8bfdf9a01d9aadb4b7be\",\"title\":\"Vision-Infused Deep Audio Inpainting\",\"url\":\"https://www.semanticscholar.org/paper/82feef7ae3b3d5ea16ce8bfdf9a01d9aadb4b7be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35007805\",\"name\":\"Thomas Le Cornu\"},{\"authorId\":\"1772594\",\"name\":\"B. Milner\"}],\"doi\":\"10.1109/TASLP.2017.2716178\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a11dca9e8538c4887f525e263e64496aa68457c1\",\"title\":\"Generating Intelligible Audio Speech From Visual Speech\",\"url\":\"https://www.semanticscholar.org/paper/a11dca9e8538c4887f525e263e64496aa68457c1\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3404049\",\"name\":\"Leyuan Qu\"},{\"authorId\":\"1798067\",\"name\":\"Cornelius Weber\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.21437/interspeech.2019-1393\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfba89013c4ca7d90d86ac91fe98586756256440\",\"title\":\"LipSound: Neural Mel-Spectrogram Reconstruction for Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/dfba89013c4ca7d90d86ac91fe98586756256440\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1906.06301\",\"authors\":[{\"authorId\":\"2160245\",\"name\":\"Konstantinos Vougioukas\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.21437/interspeech.2019-1445\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"e17330208d553f184ab91e1fc63fa18a087d9d8a\",\"title\":\"Video-Driven Speech Reconstruction using Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/e17330208d553f184ab91e1fc63fa18a087d9d8a\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1701.00495\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICASSP.2017.7953127\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5c87c275ddde2e0b75264fe9dad7b130db410601\",\"title\":\"Vid2speech: Speech reconstruction from silent video\",\"url\":\"https://www.semanticscholar.org/paper/5c87c275ddde2e0b75264fe9dad7b130db410601\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leyuan Qu\"},{\"authorId\":null,\"name\":\"Cornelius Weber\"},{\"authorId\":null,\"name\":\"Stefan Wermter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lip - sound : Neural mel - spectrogram reconstruction for lip read\",\"url\":\"\",\"venue\":\"American Annals of the Deaf\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1401441056\",\"name\":\"Helen Margellos-Anast\"},{\"authorId\":\"1456142634\",\"name\":\"Helen Teri Toby Linda Raymond Lisa Dorothea Barbara E St Hedding\"},{\"authorId\":\"102773278\",\"name\":\"H. S. Perlman\"},{\"authorId\":\"37203410\",\"name\":\"Helen E. Miller\"},{\"authorId\":\"48542000\",\"name\":\"H. Rodgers\"},{\"authorId\":\"1456160572\",\"name\":\"Helen Teri Toby Linda Raymond Lisa Dorothea Barbara E St Kivland\"},{\"authorId\":\"1456160567\",\"name\":\"Helen Teri Toby Linda Raymond Lisa Dorothea Barbara E St DeGutis\"},{\"authorId\":\"1456142095\",\"name\":\"Helen Teri Toby Linda Raymond Lisa Dorothea Barbara E St Giloth\"},{\"authorId\":\"1456141667\",\"name\":\"Helen Teri Toby Linda Raymond Lisa Dorothea Barbara E St Whitman\"}],\"doi\":\"10.1353/aad.2005.0039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e752e24c5daeb67a5a21286ec446c5f4d9aef408\",\"title\":\"Developing a Standardized Comprehensive Health Survey for Use With Deaf Adults\",\"url\":\"https://www.semanticscholar.org/paper/e752e24c5daeb67a5a21286ec446c5f4d9aef408\",\"venue\":\"American annals of the deaf\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34857174\",\"name\":\"A. Rix\"},{\"authorId\":\"1836184\",\"name\":\"J. Beerends\"},{\"authorId\":\"3124646\",\"name\":\"M. Hollier\"},{\"authorId\":\"8228168\",\"name\":\"A. P. Hekstra\"}],\"doi\":\"10.1109/ICASSP.2001.941023\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd5e786fd6ced91db79105ca289f49816fe17c80\",\"title\":\"Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs\",\"url\":\"https://www.semanticscholar.org/paper/dd5e786fd6ced91db79105ca289f49816fe17c80\",\"venue\":\"2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221)\",\"year\":2001},{\"arxivId\":\"1803.04988\",\"authors\":[{\"authorId\":\"143885335\",\"name\":\"Kai Xu\"},{\"authorId\":\"49620929\",\"name\":\"Dawei Li\"},{\"authorId\":\"48289425\",\"name\":\"N. Cassimatis\"},{\"authorId\":\"1709719\",\"name\":\"Xiaolong Wang\"}],\"doi\":\"10.1109/FG.2018.00088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfa043929dbf123438cbbc1648b0d2252010f34a\",\"title\":\"LCANet: End-to-End Lipreading with Cascaded Attention-CTC\",\"url\":\"https://www.semanticscholar.org/paper/dfa043929dbf123438cbbc1648b0d2252010f34a\",\"venue\":\"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)\",\"year\":2018}],\"title\":\"Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis\",\"topics\":[{\"topic\":\"Speech synthesis\",\"topicId\":\"12604\",\"url\":\"https://www.semanticscholar.org/topic/12604\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Humans\",\"topicId\":\"732\",\"url\":\"https://www.semanticscholar.org/topic/732\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Vocabulary\",\"topicId\":\"14901\",\"url\":\"https://www.semanticscholar.org/topic/14901\"}],\"url\":\"https://www.semanticscholar.org/paper/ba684a9966995e5a8c6efef46aeb57bd387ff51f\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"