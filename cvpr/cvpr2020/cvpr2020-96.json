"{\"abstract\":\"Deep learning models have shown impressive performance across a spectrum of computer vision applications including medical diagnosis and autonomous driving. One of the major concerns that these models face is their susceptibility to adversarial attacks. Realizing the importance of this issue, more researchers are working towards developing robust models that are less affected by adversarial attacks. Adversarial training method shows promising results in this direction. In adversarial training regime, models are trained with mini-batches augmented with adversarial samples. Fast and simple methods (e.g., single-step gradient ascent) are used for generating adversarial samples, in order to reduce computational complexity. It is shown that models trained using single-step adversarial training method (adversarial samples are generated using non-iterative method) are pseudo robust. Further, this pseudo robustness of models is attributed to the gradient masking effect. However, existing works fail to explain when and why gradient masking effect occurs during single-step adversarial training. In this work, (i) we show that models trained using single-step adversarial training method learn to prevent the generation of single-step adversaries, and this is due to over-fitting of the model during the initial stages of training, and (ii) to mitigate this effect, we propose a single-step adversarial training method with dropout scheduling. Unlike models trained using existing single-step adversarial training methods, models trained using the proposed single-step adversarial training method are robust against both single-step and multi-step adversarial attacks, and the performance is on par with models trained using computationally expensive multi-step adversarial training methods, in white-box and black-box settings.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"1865741005\",\"name\":\"Vivek B.S.\",\"url\":\"https://www.semanticscholar.org/author/1865741005\"},{\"authorId\":\"134685329\",\"name\":\"R. Venkatesh Babu\",\"url\":\"https://www.semanticscholar.org/author/134685329\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":221087333,\"doi\":\"10.1109/CVPR42600.2020.00103\",\"fieldsOfStudy\":null,\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"43d3b8ac3d6dab64c55fc02cb016d56a33a7d5b0\",\"references\":[{\"arxivId\":\"1706.06083\",\"authors\":[{\"authorId\":\"143826246\",\"name\":\"A. Madry\"},{\"authorId\":\"17775913\",\"name\":\"Aleksandar Makelov\"},{\"authorId\":\"33404869\",\"name\":\"L. Schmidt\"},{\"authorId\":\"2754804\",\"name\":\"D. Tsipras\"},{\"authorId\":\"2869958\",\"name\":\"Adrian Vladu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7aa38b85fa8cba64d6a4010543f6695dbf5f1386\",\"title\":\"Towards Deep Learning Models Resistant to Adversarial Attacks\",\"url\":\"https://www.semanticscholar.org/paper/7aa38b85fa8cba64d6a4010543f6695dbf5f1386\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1711.01991\",\"authors\":[{\"authorId\":\"3011497\",\"name\":\"Cihang Xie\"},{\"authorId\":null,\"name\":\"Jianyu Wang\"},{\"authorId\":\"2852303\",\"name\":\"Zhishuai Zhang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a089c56eec68df722b2a5a52727143aacdc2532\",\"title\":\"Mitigating adversarial effects through randomization\",\"url\":\"https://www.semanticscholar.org/paper/9a089c56eec68df722b2a5a52727143aacdc2532\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":\"1805.06605\",\"authors\":[{\"authorId\":\"3383048\",\"name\":\"Pouya Samangouei\"},{\"authorId\":\"2747758\",\"name\":\"Maya Kabkab\"},{\"authorId\":\"9215658\",\"name\":\"R. Chellappa\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f7bb1636ced9036b3d0edafc7d82ad43164d41a3\",\"title\":\"Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/f7bb1636ced9036b3d0edafc7d82ad43164d41a3\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1787610\",\"name\":\"J. Tygar\"}],\"doi\":\"10.1109/MIC.2011.112\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e24b8a9531573d284647239affc6c855505b0de4\",\"title\":\"Adversarial Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/e24b8a9531573d284647239affc6c855505b0de4\",\"venue\":\"IEEE Internet Comput.\",\"year\":2011},{\"arxivId\":\"1611.01236\",\"authors\":[{\"authorId\":\"145714153\",\"name\":\"A. Kurakin\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e2a85a6766b982ff7c8980e57ca6342d22493827\",\"title\":\"Adversarial Machine Learning at Scale\",\"url\":\"https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1608.04644\",\"authors\":[{\"authorId\":\"2483738\",\"name\":\"Nicholas Carlini\"},{\"authorId\":\"145394689\",\"name\":\"D. Wagner\"}],\"doi\":\"10.1109/SP.2017.49\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"df40ce107a71b770c9d0354b78fdd8989da80d2f\",\"title\":\"Towards Evaluating the Robustness of Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/df40ce107a71b770c9d0354b78fdd8989da80d2f\",\"venue\":\"2017 IEEE Symposium on Security and Privacy (SP)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47619311\",\"name\":\"J. Buckman\"},{\"authorId\":\"39788470\",\"name\":\"Aurko Roy\"},{\"authorId\":\"2402716\",\"name\":\"Colin Raffel\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8b9127bee0f7d109da2672ba06d0f39a5a60335a\",\"title\":\"Thermometer Encoding: One Hot Way To Resist Adversarial Examples\",\"url\":\"https://www.semanticscholar.org/paper/8b9127bee0f7d109da2672ba06d0f39a5a60335a\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1801.02613\",\"authors\":[{\"authorId\":\"9576855\",\"name\":\"Xingjun Ma\"},{\"authorId\":\"143771567\",\"name\":\"Bo Li\"},{\"authorId\":null,\"name\":\"Yisen Wang\"},{\"authorId\":\"144757691\",\"name\":\"S. Erfani\"},{\"authorId\":\"2825361\",\"name\":\"S. Wijewickrema\"},{\"authorId\":\"4480560\",\"name\":\"M. E. Houle\"},{\"authorId\":\"1710013\",\"name\":\"Grant Schoenebeck\"},{\"authorId\":\"143711382\",\"name\":\"D. Song\"},{\"authorId\":\"145148600\",\"name\":\"J. Bailey\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a18ada04d93981178234d9c8907fb99ea92fddcb\",\"title\":\"Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality\",\"url\":\"https://www.semanticscholar.org/paper/a18ada04d93981178234d9c8907fb99ea92fddcb\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1902.06705\",\"authors\":[{\"authorId\":\"2483738\",\"name\":\"Nicholas Carlini\"},{\"authorId\":\"38939786\",\"name\":\"Anish Athalye\"},{\"authorId\":\"1967156\",\"name\":\"Nicolas Papernot\"},{\"authorId\":\"40634590\",\"name\":\"W. Brendel\"},{\"authorId\":\"19237612\",\"name\":\"Jonas Rauber\"},{\"authorId\":\"2754804\",\"name\":\"D. Tsipras\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"143826246\",\"name\":\"A. Madry\"},{\"authorId\":\"145714153\",\"name\":\"A. Kurakin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5\",\"title\":\"On Evaluating Adversarial Robustness\",\"url\":\"https://www.semanticscholar.org/paper/be94fe9f2414639cd3f6cef0fdeafd4a10d1b2e5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1801.09344\",\"authors\":[{\"authorId\":\"2655157\",\"name\":\"Aditi Raghunathan\"},{\"authorId\":\"5164568\",\"name\":\"J. Steinhardt\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"966e3c7a65ec75a6359b55c0cecaf3896d318432\",\"title\":\"Certified Defenses against Adversarial Examples\",\"url\":\"https://www.semanticscholar.org/paper/966e3c7a65ec75a6359b55c0cecaf3896d318432\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1611.02770\",\"authors\":[{\"authorId\":\"1887192\",\"name\":\"Y. Liu\"},{\"authorId\":\"2727656\",\"name\":\"X. Chen\"},{\"authorId\":null,\"name\":\"Chang Liu\"},{\"authorId\":\"143711382\",\"name\":\"D. Song\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"99e5a8c10cf92749d4a7c2949691c3a6046e499a\",\"title\":\"Delving into Transferable Adversarial Examples and Black-box Attacks\",\"url\":\"https://www.semanticscholar.org/paper/99e5a8c10cf92749d4a7c2949691c3a6046e499a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1312.6199\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"143627859\",\"name\":\"Joan Bruna\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad\",\"title\":\"Intriguing properties of neural networks\",\"url\":\"https://www.semanticscholar.org/paper/d891dc72cbd40ffaeefdc79f2e7afe1e530a23ad\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":\"1412.6572\",\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bee044c8e8903fb67523c1f8c105ab4718600cdb\",\"title\":\"Explaining and Harnessing Adversarial Examples\",\"url\":\"https://www.semanticscholar.org/paper/bee044c8e8903fb67523c1f8c105ab4718600cdb\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1707.05572\",\"authors\":[{\"authorId\":\"2217000\",\"name\":\"Konda Reddy Mopuri\"},{\"authorId\":\"39957046\",\"name\":\"U. Garg\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.5244/C.31.30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21f360cce06ffdb3e7a8a56f7499cc79ff86405a\",\"title\":\"Fast Feature Fool: A data independent approach to universal adversarial perturbations\",\"url\":\"https://www.semanticscholar.org/paper/21f360cce06ffdb3e7a8a56f7499cc79ff86405a\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"93311460\",\"name\":\"S. VivekB.\"},{\"authorId\":\"30553248\",\"name\":\"Arya Baburaj\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1109/CVPRW.2019.00014\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ed3ab6bde2f17488c7b85ffb6d3875916ba4629\",\"title\":\"Regularizer to Mitigate Gradient Masking Effect During Single-Step Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/7ed3ab6bde2f17488c7b85ffb6d3875916ba4629\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1684175\",\"name\":\"B. Biggio\"},{\"authorId\":\"1716261\",\"name\":\"G. Fumera\"},{\"authorId\":\"1710171\",\"name\":\"F. Roli\"}],\"doi\":\"10.1142/S0218001414600027\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c20a8ff87d734cf8e9b95e2bd1a5e6467bac2d0f\",\"title\":\"Pattern Recognition Systems under Attack: Design Issues and Research Challenges\",\"url\":\"https://www.semanticscholar.org/paper/c20a8ff87d734cf8e9b95e2bd1a5e6467bac2d0f\",\"venue\":\"Int. J. Pattern Recognit. Artif. Intell.\",\"year\":2014},{\"arxivId\":\"1803.01442\",\"authors\":[{\"authorId\":\"16404879\",\"name\":\"Guneet S. Dhillon\"},{\"authorId\":\"3371922\",\"name\":\"Kamyar Azizzadenesheli\"},{\"authorId\":\"32219137\",\"name\":\"Zachary Chase Lipton\"},{\"authorId\":\"38267634\",\"name\":\"J. Bernstein\"},{\"authorId\":\"3125761\",\"name\":\"Jean Kossaifi\"},{\"authorId\":\"19268451\",\"name\":\"A. Khanna\"},{\"authorId\":\"2047844\",\"name\":\"Anima Anandkumar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f201c77e7ccdf1f37115e16accac3486a65c03d\",\"title\":\"Stochastic Activation Pruning for Robust Adversarial Defense\",\"url\":\"https://www.semanticscholar.org/paper/2f201c77e7ccdf1f37115e16accac3486a65c03d\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1711.00851\",\"authors\":[{\"authorId\":\"145116464\",\"name\":\"J. Z. Kolter\"},{\"authorId\":\"51026953\",\"name\":\"E. Wong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b23012689e0f17912fb38d4984775e567cff8d6\",\"title\":\"Provable defenses against adversarial examples via the convex outer adversarial polytope\",\"url\":\"https://www.semanticscholar.org/paper/4b23012689e0f17912fb38d4984775e567cff8d6\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alex Krizhevsky\"},{\"authorId\":null,\"name\":\"Vinod Nair\"},{\"authorId\":null,\"name\":\"Geoffrey Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cifar-10 (canadian institute for advanced research\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1808.01753\",\"authors\":[{\"authorId\":\"93311460\",\"name\":\"S. VivekB.\"},{\"authorId\":\"2217000\",\"name\":\"Konda Reddy Mopuri\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1007/978-3-030-01267-0_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"53444de423d3abdd7cbc9ad3bfe39c68589f20b1\",\"title\":\"Gray-box Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/53444de423d3abdd7cbc9ad3bfe39c68589f20b1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1708.06131\",\"authors\":[{\"authorId\":\"1684175\",\"name\":\"B. Biggio\"},{\"authorId\":\"2338858\",\"name\":\"I. Corona\"},{\"authorId\":\"3248803\",\"name\":\"Davide Maiorca\"},{\"authorId\":\"39743720\",\"name\":\"B. Nelson\"},{\"authorId\":\"2118348\",\"name\":\"Nedim Srndic\"},{\"authorId\":\"1754215\",\"name\":\"P. Laskov\"},{\"authorId\":\"1779484\",\"name\":\"G. Giacinto\"},{\"authorId\":\"1710171\",\"name\":\"F. Roli\"}],\"doi\":\"10.1007/978-3-642-40994-3_25\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"033c08ca48aaed2d5ab0a17d668d410538678ed8\",\"title\":\"Evasion Attacks against Machine Learning at Test Time\",\"url\":\"https://www.semanticscholar.org/paper/033c08ca48aaed2d5ab0a17d668d410538678ed8\",\"venue\":\"ECML/PKDD\",\"year\":2013},{\"arxivId\":\"1710.10766\",\"authors\":[{\"authorId\":\"144404428\",\"name\":\"Yang Song\"},{\"authorId\":\"3307885\",\"name\":\"Taesup Kim\"},{\"authorId\":\"2388416\",\"name\":\"Sebastian Nowozin\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"},{\"authorId\":\"1684887\",\"name\":\"Nate Kushman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e83291498a3bc6b0efe8f9571e9c9ca1811707bd\",\"title\":\"PixelDefend: Leveraging Generative Models to Understand and Defend against Adversarial Examples\",\"url\":\"https://www.semanticscholar.org/paper/e83291498a3bc6b0efe8f9571e9c9ca1811707bd\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1702.04267\",\"authors\":[{\"authorId\":\"2708564\",\"name\":\"J. H. Metzen\"},{\"authorId\":\"3081854\",\"name\":\"Tim Genewein\"},{\"authorId\":\"47092548\",\"name\":\"Volker Fischer\"},{\"authorId\":\"3452473\",\"name\":\"B. Bischoff\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"531e3a7b7768f199fdd401b266504db245ca039a\",\"title\":\"On Detecting Adversarial Perturbations\",\"url\":\"https://www.semanticscholar.org/paper/531e3a7b7768f199fdd401b266504db245ca039a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1708.07747\",\"authors\":[{\"authorId\":\"145642373\",\"name\":\"H. Xiao\"},{\"authorId\":\"4565995\",\"name\":\"K. Rasul\"},{\"authorId\":\"2742129\",\"name\":\"Roland Vollgraf\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"f9c602cc436a9ea2f9e7db48c77d924e09ce3c32\",\"title\":\"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1801.08092\",\"authors\":[{\"authorId\":\"2217000\",\"name\":\"Konda Reddy Mopuri\"},{\"authorId\":\"32077665\",\"name\":\"Aditya Ganeshan\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1109/TPAMI.2018.2861800\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"60f997a400188233f2448d00a77c25784c4c8dc3\",\"title\":\"Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations\",\"url\":\"https://www.semanticscholar.org/paper/60f997a400188233f2448d00a77c25784c4c8dc3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1909.04385\",\"authors\":[{\"authorId\":\"32077665\",\"name\":\"Aditya Ganeshan\"},{\"authorId\":\"93311460\",\"name\":\"S. VivekB.\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1109/ICCV.2019.00816\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"adff1f774661690c8504c569eb401c5030244555\",\"title\":\"FDA: Feature Disruptive Attack\",\"url\":\"https://www.semanticscholar.org/paper/adff1f774661690c8504c569eb401c5030244555\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1705.07204\",\"authors\":[{\"authorId\":\"2444919\",\"name\":\"Florian Tram\\u00e8r\"},{\"authorId\":\"145714153\",\"name\":\"A. Kurakin\"},{\"authorId\":\"1967156\",\"name\":\"Nicolas Papernot\"},{\"authorId\":\"1752788\",\"name\":\"D. Boneh\"},{\"authorId\":\"144061974\",\"name\":\"P. McDaniel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"136dee73f203df2f4831994bf4f0c0a4ad2e764e\",\"title\":\"Ensemble Adversarial Training: Attacks and Defenses\",\"url\":\"https://www.semanticscholar.org/paper/136dee73f203df2f4831994bf4f0c0a4ad2e764e\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1901.08573\",\"authors\":[{\"authorId\":\"40975176\",\"name\":\"Hongyang Zhang\"},{\"authorId\":\"29001000\",\"name\":\"Yaodong Yu\"},{\"authorId\":\"2784735\",\"name\":\"J. Jiao\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"},{\"authorId\":\"1701847\",\"name\":\"L. Ghaoui\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6c405d4b5dc41a86be05acd59c06ed19daf01d14\",\"title\":\"Theoretically Principled Trade-off between Robustness and Accuracy\",\"url\":\"https://www.semanticscholar.org/paper/6c405d4b5dc41a86be05acd59c06ed19daf01d14\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1967156\",\"name\":\"Nicolas Papernot\"},{\"authorId\":\"144061974\",\"name\":\"P. McDaniel\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1680133\",\"name\":\"S. Jha\"},{\"authorId\":\"144643812\",\"name\":\"Z. Y. Celik\"},{\"authorId\":\"144231976\",\"name\":\"A. Swami\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"94187ef33e34af2cdb42502083c6f9b4c3f5ba6b\",\"title\":\"Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples\",\"url\":\"https://www.semanticscholar.org/paper/94187ef33e34af2cdb42502083c6f9b4c3f5ba6b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1511.04599\",\"authors\":[{\"authorId\":\"1403182206\",\"name\":\"Seyed-Mohsen Moosavi-Dezfooli\"},{\"authorId\":\"33054064\",\"name\":\"Alhussein Fawzi\"},{\"authorId\":\"48036489\",\"name\":\"P. Frossard\"}],\"doi\":\"10.1109/CVPR.2016.282\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35\",\"title\":\"DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1802.00420\",\"authors\":[{\"authorId\":\"38939786\",\"name\":\"Anish Athalye\"},{\"authorId\":\"2483738\",\"name\":\"Nicholas Carlini\"},{\"authorId\":\"145394689\",\"name\":\"D. Wagner\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"651adaa058f821a890f2c5d1053d69eb481a8352\",\"title\":\"Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples\",\"url\":\"https://www.semanticscholar.org/paper/651adaa058f821a890f2c5d1053d69eb481a8352\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nicolas Papernot\"},{\"authorId\":null,\"name\":\"Patrick D. McDaniel\"},{\"authorId\":null,\"name\":\"Xi Wu\"},{\"authorId\":null,\"name\":\"Somesh Jha\"},{\"authorId\":null,\"name\":\"Ananthram Swami\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Distillation as a Defense to Adver- 955 sarial Perturbations against Deep Neural Networks\",\"url\":\"\",\"venue\":\"arXiv preprint arXiv:1511.04508,\",\"year\":2015},{\"arxivId\":\"1807.10272\",\"authors\":[{\"authorId\":\"39468283\",\"name\":\"L. Engstrom\"},{\"authorId\":\"34562927\",\"name\":\"Andrew Ilyas\"},{\"authorId\":\"38939786\",\"name\":\"A. Athalye\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6effa092456e30e7e54954fd28b755e0a75b52b8\",\"title\":\"Evaluating and Understanding the Robustness of Adversarial Logit Pairing\",\"url\":\"https://www.semanticscholar.org/paper/6effa092456e30e7e54954fd28b755e0a75b52b8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1511.04508\",\"authors\":[{\"authorId\":\"1967156\",\"name\":\"Nicolas Papernot\"},{\"authorId\":\"144061974\",\"name\":\"P. McDaniel\"},{\"authorId\":\"37785191\",\"name\":\"Xi Wu\"},{\"authorId\":\"1680133\",\"name\":\"S. Jha\"},{\"authorId\":\"144231976\",\"name\":\"A. Swami\"}],\"doi\":\"10.1109/SP.2016.41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6adf016e7531c91100d3cf4a74f5d4c87b26b528\",\"title\":\"Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6adf016e7531c91100d3cf4a74f5d4c87b26b528\",\"venue\":\"2016 IEEE Symposium on Security and Privacy (SP)\",\"year\":2016},{\"arxivId\":\"1607.02533\",\"authors\":[{\"authorId\":\"145714153\",\"name\":\"A. Kurakin\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"}],\"doi\":\"10.1201/9781351251389-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b\",\"title\":\"Adversarial examples in the physical world\",\"url\":\"https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1511.07528\",\"authors\":[{\"authorId\":\"1967156\",\"name\":\"Nicolas Papernot\"},{\"authorId\":\"144061974\",\"name\":\"P. McDaniel\"},{\"authorId\":\"1680133\",\"name\":\"S. Jha\"},{\"authorId\":\"2623167\",\"name\":\"Matt Fredrikson\"},{\"authorId\":\"144643812\",\"name\":\"Z. Y. Celik\"},{\"authorId\":\"144231976\",\"name\":\"A. Swami\"}],\"doi\":\"10.1109/EuroSP.2016.36\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"819167ace2f0caae7745d2f25a803979be5fbfae\",\"title\":\"The Limitations of Deep Learning in Adversarial Settings\",\"url\":\"https://www.semanticscholar.org/paper/819167ace2f0caae7745d2f25a803979be5fbfae\",\"venue\":\"2016 IEEE European Symposium on Security and Privacy (EuroS&P)\",\"year\":2016},{\"arxivId\":\"1605.07146\",\"authors\":[{\"authorId\":\"2134433\",\"name\":\"Sergey Zagoruyko\"},{\"authorId\":\"2505902\",\"name\":\"Nikos Komodakis\"}],\"doi\":\"10.5244/C.30.87\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c4e9156ca07705531e45960b7a919dc473abb51\",\"title\":\"Wide Residual Networks\",\"url\":\"https://www.semanticscholar.org/paper/1c4e9156ca07705531e45960b7a919dc473abb51\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":\"1711.00117\",\"authors\":[{\"authorId\":\"144993411\",\"name\":\"Chuan Guo\"},{\"authorId\":\"2139712\",\"name\":\"Mayank Rana\"},{\"authorId\":\"5723508\",\"name\":\"M. Ciss\\u00e9\"},{\"authorId\":\"1803520\",\"name\":\"L. V. D. Maaten\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e225dd59ef4954db21479cdcbee497624b2d6d0f\",\"title\":\"Countering Adversarial Images using Input Transformations\",\"url\":\"https://www.semanticscholar.org/paper/e225dd59ef4954db21479cdcbee497624b2d6d0f\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"145115014\",\"name\":\"Corinna Cortes\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2\",\"title\":\"The mnist database of handwritten digits\",\"url\":\"https://www.semanticscholar.org/paper/dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3431029\",\"name\":\"Y. Dong\"},{\"authorId\":\"37906910\",\"name\":\"Fangzhou Liao\"},{\"authorId\":\"19201674\",\"name\":\"Tianyu Pang\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"145296845\",\"name\":\"J. Zhu\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"}],\"doi\":\"10.1109/CVPR.2018.00957\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8e37a3b227b68953f8067215828dc8b8714cb21b\",\"title\":\"Boosting Adversarial Attacks with Momentum\",\"url\":\"https://www.semanticscholar.org/paper/8e37a3b227b68953f8067215828dc8b8714cb21b\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018}],\"title\":\"Single-Step Adversarial Training With Dropout Scheduling\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/43d3b8ac3d6dab64c55fc02cb016d56a33a7d5b0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"