"{\"abstract\":\"In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze the mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce an ENtangled Transformer block (ENT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint videotext representation to be aware of fine-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT significantly outperform the state-of-the-arts, demonstrating its superiority in video-text representation learning.\",\"arxivId\":\"2011.07231\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\",\"url\":\"https://www.semanticscholar.org/author/2948393\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\",\"url\":\"https://www.semanticscholar.org/author/91893932\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.02824\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144721617\",\"name\":\"Y. Asano\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"145414740\",\"name\":\"J. Henriques\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"title\":\"Support-set bottlenecks for video-text representation learning\",\"url\":\"https://www.semanticscholar.org/paper/78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.00451\",\"authors\":[{\"authorId\":\"153276988\",\"name\":\"Antoine Yang\"},{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"title\":\"Just Ask: Learning to Answer Questions from Millions of Narrated Videos\",\"url\":\"https://www.semanticscholar.org/paper/b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.06353\",\"authors\":[{\"authorId\":\"35347136\",\"name\":\"Huaishao Luo\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"66782928\",\"name\":\"Tianrui Li\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4243555758433880a67b15b50f752b1e2a8c4609\",\"title\":\"UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation\",\"url\":\"https://www.semanticscholar.org/paper/4243555758433880a67b15b50f752b1e2a8c4609\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.04124\",\"authors\":[{\"authorId\":\"40176903\",\"name\":\"Sangho Lee\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"},{\"authorId\":\"6555176\",\"name\":\"Thomas Breuel\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3655a6344ffb57838fd7f7acce651f36d3a9d526\",\"title\":\"Parameter Efficient Multimodal Transformers for Video Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/3655a6344ffb57838fd7f7acce651f36d3a9d526\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.08673\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c8f6878dac0c5c99c192ded715d2e864eed7302\",\"title\":\"A Closer Look at the Robustness of Vision-and-Language Pre-trained Models\",\"url\":\"https://www.semanticscholar.org/paper/3c8f6878dac0c5c99c192ded715d2e864eed7302\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.08395\",\"authors\":[{\"authorId\":\"4852522\",\"name\":\"Tariq Habib Afridi\"},{\"authorId\":\"47686775\",\"name\":\"Aftab Alam\"},{\"authorId\":\"1645748271\",\"name\":\"M. N. Khan\"},{\"authorId\":\"1649689679\",\"name\":\"Jawad Khan\"},{\"authorId\":\"2806926\",\"name\":\"Young-Koo Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0903f184dfccc813d8165d0f69c87f79520014e\",\"title\":\"A Multimodal Memes Classification: A Survey and Open Research Issues\",\"url\":\"https://www.semanticscholar.org/paper/b0903f184dfccc813d8165d0f69c87f79520014e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.05381\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"46200183\",\"name\":\"Chaoxi Xu\"},{\"authorId\":\"145789906\",\"name\":\"G. Yang\"},{\"authorId\":\"48632140\",\"name\":\"Xun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"title\":\"Hybrid Space Learning for Language-based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/be749fb4719cfc56d689ab11a27a9a6f8fd76570\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.06567\",\"authors\":[{\"authorId\":\"1805946041\",\"name\":\"Yi Zhu\"},{\"authorId\":\"21657733\",\"name\":\"X. Li\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"22539483\",\"name\":\"Chongruo Wu\"},{\"authorId\":\"152781163\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2397422\",\"name\":\"Joseph Tighe\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"49140510\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"title\":\"A Comprehensive Study of Deep Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.07517\",\"authors\":[{\"authorId\":\"49606678\",\"name\":\"Jianan Wang\"},{\"authorId\":\"49730189\",\"name\":\"Boyang Li\"},{\"authorId\":\"46515715\",\"name\":\"Xiangyu Fan\"},{\"authorId\":\"1845592115\",\"name\":\"J. Lin\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f5cbd60f07a19afc3566376e404a490865e5def\",\"title\":\"Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions\",\"url\":\"https://www.semanticscholar.org/paper/5f5cbd60f07a19afc3566376e404a490865e5def\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.00597\",\"authors\":[{\"authorId\":\"2007582232\",\"name\":\"Simon Ging\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"1835025\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"80089ad641bae28b0e57771afef181b60011069e\",\"title\":\"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/80089ad641bae28b0e57771afef181b60011069e\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2011.11760\",\"authors\":[{\"authorId\":\"24040986\",\"name\":\"Gabriel Huang\"},{\"authorId\":\"1560385163\",\"name\":\"Bo Pang\"},{\"authorId\":\"2062703\",\"name\":\"Z. Zhu\"},{\"authorId\":\"66193113\",\"name\":\"C. Rivera\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"title\":\"Multimodal Pretraining for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2005.00200\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1664725279\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1520007550\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"title\":\"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2003.06845\",\"authors\":[{\"authorId\":\"1410309633\",\"name\":\"Fan Ma\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"2815926\",\"name\":\"Shengxin Zha\"},{\"authorId\":\"38896301\",\"name\":\"G. Kundu\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"},{\"authorId\":\"73423138\",\"name\":\"Zheng Shou\"}],\"doi\":\"10.1007/978-3-030-58548-8_25\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84d710727a9a5775ab4691a969f52bc3062325e2\",\"title\":\"SF-Net: Single-Frame Supervision for Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/84d710727a9a5775ab4691a969f52bc3062325e2\",\"venue\":\"ECCV\",\"year\":2020}],\"corpusId\":219617394,\"doi\":\"10.1109/cvpr42600.2020.00877\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"8cda672bd5487ec2c67d5c217dc84ed8fb786640\",\"references\":[{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1703.09788\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e10a5e0baf2aa87d804795af071808a9377cc80a\",\"title\":\"Towards Automatic Learning of Procedures From Web Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/e10a5e0baf2aa87d804795af071808a9377cc80a\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1704.04689\",\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"119745921\",\"name\":\"Dong Zhang\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":\"10.1109/ICCV.2017.157\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ad2339c48ad4ffdd6100310dcbb1fb78e72fac98\",\"title\":\"Video Fill In the Blank Using LR/RL LSTMs with Spatial-Temporal Attentions\",\"url\":\"https://www.semanticscholar.org/paper/ad2339c48ad4ffdd6100310dcbb1fb78e72fac98\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1506.01497\",\"authors\":[{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"144716314\",\"name\":\"J. Sun\"}],\"doi\":\"10.1109/TPAMI.2016.2577031\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"title\":\"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/424561d8585ff8ebce7d5d07de8dbf7aae5e7270\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2015},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1506.09215\",\"authors\":[{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"143688116\",\"name\":\"Nishant Agrawal\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1388317459\",\"name\":\"S. Lacoste-Julien\"}],\"doi\":\"10.1109/CVPR.2016.495\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e17ba2b5d0769e7f2602d859ea77a153846cf27d\",\"title\":\"Unsupervised Learning from Narrated Instruction Videos\",\"url\":\"https://www.semanticscholar.org/paper/e17ba2b5d0769e7f2602d859ea77a153846cf27d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1908.06066\",\"authors\":[{\"authorId\":\"150112700\",\"name\":\"Gen Li\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143795948\",\"name\":\"Yuejian Fang\"},{\"authorId\":\"71790825\",\"name\":\"Daxin Jiang\"},{\"authorId\":\"143849622\",\"name\":\"M. Zhou\"}],\"doi\":\"10.1609/AAAI.V34I07.6795\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"title\":\"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/ICCV.2017.20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"62e6b749ed5825739aa906021c5e613803d5cbe2\",\"title\":\"Temporal Tessellation: A Unified Approach for Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/62e6b749ed5825739aa906021c5e613803d5cbe2\",\"venue\":\"ICCV\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145969200\",\"name\":\"B. Klein\"},{\"authorId\":\"3004979\",\"name\":\"G. Lev\"},{\"authorId\":\"2251827\",\"name\":\"Gil Sadeh\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":\"10.1109/CVPR.2015.7299073\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51239b320c73f3f2219286bf62f24d6763379328\",\"title\":\"Associating neural word embeddings with deep image representations using Fisher Vectors\",\"url\":\"https://www.semanticscholar.org/paper/51239b320c73f3f2219286bf62f24d6763379328\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/s11263-017-1033-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"828ac57f755db989e2886042a85278ae4823297c\",\"title\":\"Uncovering the Temporal Context for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/828ac57f755db989e2886042a85278ae4823297c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1903.08225\",\"authors\":[{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"1939006\",\"name\":\"Ramazan Gokberk Cinbis\"},{\"authorId\":\"1786435\",\"name\":\"David F. Fouhey\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/CVPR.2019.00365\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3605e41ce77dfd259eaebed906804bb60f634f75\",\"title\":\"Cross-Task Weakly Supervised Learning From Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/3605e41ce77dfd259eaebed906804bb60f634f75\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1908.02265\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"title\":\"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1705.06950\",\"authors\":[{\"authorId\":\"21028601\",\"name\":\"W. Kay\"},{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"11809518\",\"name\":\"Brian Zhang\"},{\"authorId\":\"38961760\",\"name\":\"Chloe Hillier\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"143740871\",\"name\":\"F. Viola\"},{\"authorId\":\"143897708\",\"name\":\"T. Green\"},{\"authorId\":\"2830305\",\"name\":\"T. Back\"},{\"authorId\":\"1820908\",\"name\":\"A. Natsev\"},{\"authorId\":\"2573615\",\"name\":\"Mustafa Suleyman\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6\",\"title\":\"The Kinetics Human Action Video Dataset\",\"url\":\"https://www.semanticscholar.org/paper/86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1809.01696\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.18653/v1/D18-1167\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7e1313061b0d56364bd2c41f017deb954bb05db\",\"title\":\"TVQA: Localized, Compositional Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e7e1313061b0d56364bd2c41f017deb954bb05db\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1807.00230\",\"authors\":[{\"authorId\":\"3443095\",\"name\":\"Bruno Korbar\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e057a9b195f0ea2d1c5a1e88ff9606f9b67ef8b\",\"title\":\"Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization\",\"url\":\"https://www.semanticscholar.org/paper/2e057a9b195f0ea2d1c5a1e88ff9606f9b67ef8b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1602.07332\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"40591424\",\"name\":\"J. Kravitz\"},{\"authorId\":\"6000646\",\"name\":\"Stephanie Chen\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"1388344504\",\"name\":\"David A. Shamma\"},{\"authorId\":\"1379506718\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-016-0981-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"title\":\"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\",\"url\":\"https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"1712.04851\",\"authors\":[{\"authorId\":\"1817030\",\"name\":\"Saining Xie\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"144035504\",\"name\":\"Zhuowen Tu\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1007/978-3-030-01267-0_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"815aa52cfc02961d82415f080384594639a21984\",\"title\":\"Rethinking Spatiotemporal Feature Learning: Speed-Accuracy Trade-offs in Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/815aa52cfc02961d82415f080384594639a21984\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1007/978-3-319-46448-0_32\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d4e3616d0b27957c4107ae877dc0dd4504b69ab\",\"title\":\"Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification\",\"url\":\"https://www.semanticscholar.org/paper/6d4e3616d0b27957c4107ae877dc0dd4504b69ab\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1610.04062\",\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"119745921\",\"name\":\"Dong Zhang\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9fb31d0375552500bd494af20ab0c3109c9be3d2\",\"title\":\"Video Fill in the Blank with Merging LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/9fb31d0375552500bd494af20ab0c3109c9be3d2\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1712.06651\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01246-5_27\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfc504536e8434eb008680343abb77010965169e\",\"title\":\"Objects that Sound\",\"url\":\"https://www.semanticscholar.org/paper/dfc504536e8434eb008680343abb77010965169e\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1906.03327\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2019.00272\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9311779489e597315488749ee6c386bfa3f3512e\",\"title\":\"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\"url\":\"https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1901.08634\",\"authors\":[{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"143707112\",\"name\":\"M. Collins\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b496b11fb2091678cc2d299cc778046d9a64b0a4\",\"title\":\"A BERT Baseline for the Natural Questions\",\"url\":\"https://www.semanticscholar.org/paper/b496b11fb2091678cc2d299cc778046d9a64b0a4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"153731335\",\"name\":\"Zhe Gan\"},{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"33c10383189c118465f8b40e8dba9213f57fa570\",\"title\":\"UNITER: Learning UNiversal Image-TExt Representations\",\"url\":\"https://www.semanticscholar.org/paper/33c10383189c118465f8b40e8dba9213f57fa570\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"title\":\"Video Captioning and Retrieval Models with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1808.02559\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"153188107\",\"name\":\"Jongseok Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1007/978-3-030-01234-2_29\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8befcd91c24038e5c26df0238d26e2311b21719a\",\"title\":\"A Joint Sequence Fusion Model for Video Question Answering and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8befcd91c24038e5c26df0238d26e2311b21719a\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1906.08237\",\"authors\":[{\"authorId\":\"47087291\",\"name\":\"Z. Yang\"},{\"authorId\":\"3422912\",\"name\":\"Zihang Dai\"},{\"authorId\":\"35729970\",\"name\":\"Yiming Yang\"},{\"authorId\":\"143712374\",\"name\":\"J. Carbonell\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0c6abdbdecf04ffac65c440da77fb9d66bb474c\",\"title\":\"XLNet: Generalized Autoregressive Pretraining for Language Understanding\",\"url\":\"https://www.semanticscholar.org/paper/e0c6abdbdecf04ffac65c440da77fb9d66bb474c\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1903.02874\",\"authors\":[{\"authorId\":\"35299091\",\"name\":\"Yansong Tang\"},{\"authorId\":\"50792340\",\"name\":\"Dajun Ding\"},{\"authorId\":\"39358728\",\"name\":\"Yongming Rao\"},{\"authorId\":\"145473095\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2118333\",\"name\":\"Danyang Zhang\"},{\"authorId\":\"48096213\",\"name\":\"L. Zhao\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"49640256\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/CVPR.2019.00130\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"title\":\"COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1609.08124\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2068f66a10254d457cdb5fab74b0128b24bfdb65\",\"title\":\"Learning Language-Visual Embedding for Movie Understanding with Natural-Language\",\"url\":\"https://www.semanticscholar.org/paper/2068f66a10254d457cdb5fab74b0128b24bfdb65\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1611.07810\",\"authors\":[{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":\"10.1109/CVPR.2017.778\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"title\":\"A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1506.06724\",\"authors\":[{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/ICCV.2015.11\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e6824e137847be0599bb0032e37042ed2ef5045\",\"title\":\"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books\",\"url\":\"https://www.semanticscholar.org/paper/0e6824e137847be0599bb0032e37042ed2ef5045\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1804.00819\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"34872128\",\"name\":\"Yingbo Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.1109/CVPR.2018.00911\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"35ed258aede3df17ee20a6635364cb5fd2461049\",\"title\":\"End-to-End Dense Video Captioning with Masked Transformer\",\"url\":\"https://www.semanticscholar.org/paper/35ed258aede3df17ee20a6635364cb5fd2461049\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1609.08144\",\"authors\":[{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"3153147\",\"name\":\"Wolfgang Macherey\"},{\"authorId\":\"2048712\",\"name\":\"M. Krikun\"},{\"authorId\":\"145144022\",\"name\":\"Yuan Cao\"},{\"authorId\":\"145312182\",\"name\":\"Q. Gao\"},{\"authorId\":\"113439369\",\"name\":\"Klaus Macherey\"},{\"authorId\":\"2367620\",\"name\":\"Jeff Klingner\"},{\"authorId\":\"145825976\",\"name\":\"Apurva Shah\"},{\"authorId\":\"145657834\",\"name\":\"M. Johnson\"},{\"authorId\":\"2600217\",\"name\":\"X. Liu\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"2776283\",\"name\":\"S. Gouws\"},{\"authorId\":\"2739610\",\"name\":\"Y. Kato\"},{\"authorId\":\"1765329\",\"name\":\"Taku Kudo\"},{\"authorId\":\"1754386\",\"name\":\"H. Kazawa\"},{\"authorId\":\"144077726\",\"name\":\"K. Stevens\"},{\"authorId\":\"35066890\",\"name\":\"G. Kurian\"},{\"authorId\":\"35173708\",\"name\":\"Nishant Patil\"},{\"authorId\":\"47824598\",\"name\":\"W. Wang\"},{\"authorId\":\"39660914\",\"name\":\"C. Young\"},{\"authorId\":\"27070552\",\"name\":\"J. Smith\"},{\"authorId\":\"2909504\",\"name\":\"Jason Riesa\"},{\"authorId\":\"29951847\",\"name\":\"Alex Rudnick\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"48342565\",\"name\":\"Macduff Hughes\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\",\"title\":\"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1804.02748\",\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"12387007\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc50c9392fd23b6c88915177c6ae904a498aacea\",\"title\":\"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset\",\"url\":\"https://www.semanticscholar.org/paper/fc50c9392fd23b6c88915177c6ae904a498aacea\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1711.11248\",\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"4439383\",\"name\":\"Jamie Ray\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/CVPR.2018.00675\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"89c3050522a0bb9820c32dc7444e003ef0d3e2e4\",\"title\":\"A Closer Look at Spatiotemporal Convolutions for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/89c3050522a0bb9820c32dc7444e003ef0d3e2e4\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1611.09053\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1109/CVPR.2017.147\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"533d14e539ae5cdca0ece392487a2b19106d468a\",\"title\":\"Bidirectional Multirate Reconstruction for Temporal Modeling in Videos\",\"url\":\"https://www.semanticscholar.org/paper/533d14e539ae5cdca0ece392487a2b19106d468a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1805.06875\",\"authors\":[{\"authorId\":\"32774629\",\"name\":\"A. Richard\"},{\"authorId\":\"123446103\",\"name\":\"Hilde Kuehne\"},{\"authorId\":\"3434584\",\"name\":\"Ahsan Iqbal\"},{\"authorId\":\"145689714\",\"name\":\"Juergen Gall\"}],\"doi\":\"10.1109/CVPR.2018.00771\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"80184c6a88fc97a09393b7336bc2ddb12e9b1030\",\"title\":\"NeuralNetwork-Viterbi: A Framework for Weakly Supervised Video Learning\",\"url\":\"https://www.semanticscholar.org/paper/80184c6a88fc97a09393b7336bc2ddb12e9b1030\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1904.01766\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"49588480\",\"name\":\"A. Myers\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2019.00756\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c41a11c0e9b8b92b4faaf97749841170b760760a\",\"title\":\"VideoBERT: A Joint Model for Video and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1807.05520\",\"authors\":[{\"authorId\":\"50220789\",\"name\":\"M. Caron\"},{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"3271933\",\"name\":\"M. Douze\"}],\"doi\":\"10.1007/978-3-030-01264-9_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d033b30f38642e4b6dd146bb8b464bfb58aad96\",\"title\":\"Deep Clustering for Unsupervised Learning of Visual Features\",\"url\":\"https://www.semanticscholar.org/paper/1d033b30f38642e4b6dd146bb8b464bfb58aad96\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jin-Hwa Kim\"},{\"authorId\":null,\"name\":\"Kyoung-Woon On\"},{\"authorId\":null,\"name\":\"Woosang Lim\"},{\"authorId\":null,\"name\":\"Jeonghee Kim\"},{\"authorId\":null,\"name\":\"Jung-Woo Ha\"},{\"authorId\":null,\"name\":\"Byoung-Tak Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hadamard Product for Low-rank Bilinear Pooling\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1810.04805\",\"authors\":[{\"authorId\":\"39172707\",\"name\":\"J. Devlin\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"3259253\",\"name\":\"Kristina Toutanova\"}],\"doi\":\"10.18653/v1/N19-1423\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\"title\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"url\":\"https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":\"1803.10699\",\"authors\":[{\"authorId\":\"144529493\",\"name\":\"Li Ding\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1109/CVPR.2018.00681\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c6ce420976f958e7582a2f452c3a541faa82074\",\"title\":\"Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment\",\"url\":\"https://www.semanticscholar.org/paper/6c6ce420976f958e7582a2f452c3a541faa82074\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1908.08530\",\"authors\":[{\"authorId\":\"145499378\",\"name\":\"Weijie Su\"},{\"authorId\":\"2578924\",\"name\":\"X. Zhu\"},{\"authorId\":\"47746274\",\"name\":\"Y. Cao\"},{\"authorId\":\"48218753\",\"name\":\"B. Li\"},{\"authorId\":\"152309485\",\"name\":\"Lewei Lu\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"3304536\",\"name\":\"Jifeng Dai\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2527626c11a84f15709e943fbfa2356e19930e3b\",\"title\":\"VL-BERT: Pre-training of Generic Visual-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/2527626c11a84f15709e943fbfa2356e19930e3b\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1804.02516\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3448af861bf5d44ce7ab6b25002504815212252e\",\"title\":\"Learning a Text-Video Embedding from Incomplete and Heterogeneous Data\",\"url\":\"https://www.semanticscholar.org/paper/3448af861bf5d44ce7ab6b25002504815212252e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1007/978-3-030-01234-2_46\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"396520344fb4894849a826b3de56fc86d4d79100\",\"title\":\"Compound Memory Networks for Few-Shot Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/396520344fb4894849a826b3de56fc86d4d79100\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1908.07490\",\"authors\":[{\"authorId\":\"3218666\",\"name\":\"Hao Hao Tan\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D19-1514\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"79c93274429d6355959f1e4374c2147bb81ea649\",\"title\":\"LXMERT: Learning Cross-Modality Encoder Representations from Transformers\",\"url\":\"https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1512.02902\",\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2016.501\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"title\":\"MovieQA: Understanding Stories in Movies through Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1704.04497\",\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.149\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"title\":\"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}],\"title\":\"ActBERT: Learning Global-Local Video-Text Representations\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/8cda672bd5487ec2c67d5c217dc84ed8fb786640\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"