"{\"abstract\":\"Modern smartphone cameras can match traditional DSLR cameras in many areas thanks to the introduction of camera arrays and multi-frame processing. Among all types of DSLR effects, the narrow depth of field (DoF) or so called bokeh probably arouses most interest. Today's smartphones try to overcome the physical lens and sensor limitations by introducing computational methods that utilize a depth map to synthesize the narrow DoF effect from all-in-focus images. However, a high quality depth map remains to be the key differentiator between computational bokeh and DSLR optical bokeh. Empowered by a novel wavelet synthesis network architecture, we have greatly narrowed the gap between DSLR and smartphone camera in terms of the bokeh more than ever before. We describe three key Modern smartphone cameras can match traditional digital single lens reflex (DSLR) cameras in many areas thanks to the introduction of camera arrays and multi-frame processing. Among all types of DSLR effects, the narrow depth of field (DoF) or so called bokeh probably arouses most interest. Today's smartphones try to overcome the physical lens and sensor limitations by introducing computational methods that utilize a depth map to synthesize the narrow DoF effect from all-in-focus images. However, a high quality depth map remains to be the key differentiator between computational bokeh and DSLR optical bokeh. Empowered by a novel wavelet synthesis network architecture, we have narrowed the gap between DSLR and smartphone camera in terms of bokeh more than ever before. We describe three key enablers of our bokeh solution: a synthetic graphics engine to generate training data with precisely prescribed characteristics that match the real smartphone captures, a novel wavelet synthesis neural network (WSN) architecture to produce unprecedented high definition disparity map promptly on smartphones, and a new evaluation metric to quantify the quality of the disparity map for real images from the bokeh rendering perspective. Experimental results show that the disparity map produced from our neural network achieves much better accuracy than the other state-of-the-art CNN based algorithms. Combining the high resolution disparity map with our rendering algorithm, we demonstrate visually superior bokeh pictures compared with existing top rated flagship smartphones listed on the DXOMARK mobiles.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"37165624\",\"name\":\"Chenchi Luo\",\"url\":\"https://www.semanticscholar.org/author/37165624\"},{\"authorId\":\"7771410\",\"name\":\"Yingmao Li\",\"url\":\"https://www.semanticscholar.org/author/7771410\"},{\"authorId\":\"3408849\",\"name\":\"Kaimo Lin\",\"url\":\"https://www.semanticscholar.org/author/3408849\"},{\"authorId\":\"46878993\",\"name\":\"G. Chen\",\"url\":\"https://www.semanticscholar.org/author/46878993\"},{\"authorId\":\"70412850\",\"name\":\"Seok-Jun Lee\",\"url\":\"https://www.semanticscholar.org/author/70412850\"},{\"authorId\":\"47819328\",\"name\":\"Jihwan Choi\",\"url\":\"https://www.semanticscholar.org/author/47819328\"},{\"authorId\":\"50987486\",\"name\":\"Youngjun Yoo\",\"url\":\"https://www.semanticscholar.org/author/50987486\"},{\"authorId\":\"3350987\",\"name\":\"M. Polley\",\"url\":\"https://www.semanticscholar.org/author/3350987\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2011.13317\",\"authors\":[{\"authorId\":\"26988468\",\"name\":\"Diogo C. Luvizon\"},{\"authorId\":\"2029235759\",\"name\":\"Gustavo Sutter P. Carvalho\"},{\"authorId\":\"153218066\",\"name\":\"A. A. D. Santos\"},{\"authorId\":\"1416353646\",\"name\":\"Jhonatas S. Concei\\u00e7\\u00e3o\"},{\"authorId\":\"1500520107\",\"name\":\"Jose L. Flores-Campana\"},{\"authorId\":\"1637003263\",\"name\":\"Luis G. L. Decker\"},{\"authorId\":\"50999137\",\"name\":\"M. R. Souza\"},{\"authorId\":\"1743455\",\"name\":\"H. Pedrini\"},{\"authorId\":\"2029237447\",\"name\":\"Antonio Joia\"},{\"authorId\":\"3131975\",\"name\":\"O. A. B. Penatti\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"61831a5e534e9598415bbff92c47bdf13d1c0759\",\"title\":\"Adaptive Multiplane Image Generation from a Single Internet Picture\",\"url\":\"https://www.semanticscholar.org/paper/61831a5e534e9598415bbff92c47bdf13d1c0759\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":219635496,\"doi\":\"10.1109/CVPR42600.2020.00248\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"8bd5d2b3c16fa42d0d1f9a9b3295e9de7b658d2f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2509265\",\"name\":\"Zhengfa Liang\"},{\"authorId\":\"9196685\",\"name\":\"Yiliu Feng\"},{\"authorId\":\"2985328\",\"name\":\"Yulan Guo\"},{\"authorId\":\"2263219\",\"name\":\"H. Liu\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"2570205\",\"name\":\"Linbo Qiao\"},{\"authorId\":null,\"name\":\"Li Zhou\"},{\"authorId\":\"50561652\",\"name\":\"J. Zhang\"}],\"doi\":\"10.1109/CVPR.2018.00297\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33389b1da844f13aba317e8b16c814888c26c827\",\"title\":\"Learning for Disparity Estimation Through Feature Constancy\",\"url\":\"https://www.semanticscholar.org/paper/33389b1da844f13aba317e8b16c814888c26c827\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1612.01925\",\"authors\":[{\"authorId\":\"48105320\",\"name\":\"Eddy Ilg\"},{\"authorId\":\"153200643\",\"name\":\"N. Mayer\"},{\"authorId\":\"2872102\",\"name\":\"Tonmoy Saikia\"},{\"authorId\":\"3316866\",\"name\":\"Margret Keuper\"},{\"authorId\":\"2841331\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/CVPR.2017.179\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"edd846e76cacfba5be37da99c006e3ccc9b861b0\",\"title\":\"FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/edd846e76cacfba5be37da99c006e3ccc9b861b0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1406.2283\",\"authors\":[{\"authorId\":\"2060028\",\"name\":\"D. Eigen\"},{\"authorId\":\"1940183\",\"name\":\"Christian Puhrsch\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"06feba1ffd596b41884cea6e8ef0da89b6dd2233\",\"title\":\"Depth Map Prediction from a Single Image using a Multi-Scale Deep Network\",\"url\":\"https://www.semanticscholar.org/paper/06feba1ffd596b41884cea6e8ef0da89b6dd2233\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2522642\",\"name\":\"Haiting Lin\"},{\"authorId\":\"2118531\",\"name\":\"Can Chen\"},{\"authorId\":\"1738740\",\"name\":\"S. B. Kang\"},{\"authorId\":\"2152356\",\"name\":\"J. Yu\"}],\"doi\":\"10.1109/ICCV.2015.394\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c71d08db88a9a7c108c56fef2d5320b51d2e417f\",\"title\":\"Depth Recovery from Light Field Using Focal Stack Symmetry\",\"url\":\"https://www.semanticscholar.org/paper/c71d08db88a9a7c108c56fef2d5320b51d2e417f\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1803.08669\",\"authors\":[{\"authorId\":\"2936466\",\"name\":\"Jia-Ren Chang\"},{\"authorId\":\"4769561\",\"name\":\"Y. Chen\"}],\"doi\":\"10.1109/CVPR.2018.00567\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"316b1b9d96149e7bb3d9d6afc0295881c6123cc8\",\"title\":\"Pyramid Stereo Matching Network\",\"url\":\"https://www.semanticscholar.org/paper/316b1b9d96149e7bb3d9d6afc0295881c6123cc8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1510.05970\",\"authors\":[{\"authorId\":\"3105120\",\"name\":\"J. Zbontar\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad3e7515c61ccdc2c36887e7d4929c78904881db\",\"title\":\"Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches\",\"url\":\"https://www.semanticscholar.org/paper/ad3e7515c61ccdc2c36887e7d4929c78904881db\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":\"1704.04861\",\"authors\":[{\"authorId\":\"144727050\",\"name\":\"A. Howard\"},{\"authorId\":\"2717876\",\"name\":\"Menglong Zhu\"},{\"authorId\":null,\"name\":\"Bo Chen\"},{\"authorId\":\"2741985\",\"name\":\"D. Kalenichenko\"},{\"authorId\":\"47825047\",\"name\":\"W. Wang\"},{\"authorId\":\"47447630\",\"name\":\"Tobias Weyand\"},{\"authorId\":\"2612392\",\"name\":\"M. Andreetto\"},{\"authorId\":\"2595180\",\"name\":\"H. Adam\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3647d6d0f151dc05626449ee09cc7bce55be497e\",\"title\":\"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\",\"url\":\"https://www.semanticscholar.org/paper/3647d6d0f151dc05626449ee09cc7bce55be497e\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1606.06650\",\"authors\":[{\"authorId\":\"3422575\",\"name\":\"\\u00d6zg\\u00fcn \\u00c7i\\u00e7ek\"},{\"authorId\":\"145080076\",\"name\":\"A. Abdulkadir\"},{\"authorId\":\"2720674\",\"name\":\"S. Lienkamp\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"1737326\",\"name\":\"O. Ronneberger\"}],\"doi\":\"10.1007/978-3-319-46723-8_49\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7fc464470b441c691d10e7331b14a525bc79b8bb\",\"title\":\"3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation\",\"url\":\"https://www.semanticscholar.org/paper/7fc464470b441c691d10e7331b14a525bc79b8bb\",\"venue\":\"MICCAI\",\"year\":2016},{\"arxivId\":\"1905.10107\",\"authors\":[{\"authorId\":\"2509750\",\"name\":\"M. Poggi\"},{\"authorId\":\"122207814\",\"name\":\"Davide Pallotti\"},{\"authorId\":\"121670758\",\"name\":\"Fabio Tosi\"},{\"authorId\":\"10261545\",\"name\":\"S. Mattoccia\"}],\"doi\":\"10.1109/CVPR.2019.00107\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"77353207f4322a827b9af446855f2df71e6065e4\",\"title\":\"Guided Stereo Matching\",\"url\":\"https://www.semanticscholar.org/paper/77353207f4322a827b9af446855f2df71e6065e4\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1805.07036\",\"authors\":[{\"authorId\":\"33385667\",\"name\":\"Tak-Wai Hui\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":\"10.1109/CVPR.2018.00936\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"051b3763c2ad4e4271db712b0e9a4cfe298d05db\",\"title\":\"LiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow Estimation\",\"url\":\"https://www.semanticscholar.org/paper/051b3763c2ad4e4271db712b0e9a4cfe298d05db\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1505.04597\",\"authors\":[{\"authorId\":\"1737326\",\"name\":\"O. Ronneberger\"},{\"authorId\":\"152702479\",\"name\":\"P. Fischer\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1007/978-3-319-24574-4_28\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6364fdaa0a0eccd823a779fcdd489173f938e91a\",\"title\":\"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/6364fdaa0a0eccd823a779fcdd489173f938e91a\",\"venue\":\"MICCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390449566\",\"name\":\"W. Hauser\"},{\"authorId\":\"1397073036\",\"name\":\"B. Neveu\"},{\"authorId\":\"1381757914\",\"name\":\"Jean-Benoit Jourdain\"},{\"authorId\":\"2798863\",\"name\":\"Cl\\u00e9ment Viard\"},{\"authorId\":\"1739682\",\"name\":\"F. Guichard\"}],\"doi\":\"10.2352/ISSN.2470-1173.2018.12.IQSP-340\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10941e95e01221cc7f110e99dac021fdd0fa60f2\",\"title\":\"Image quality benchmark of computational bokeh\",\"url\":\"https://www.semanticscholar.org/paper/10941e95e01221cc7f110e99dac021fdd0fa60f2\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2177291\",\"name\":\"Beyang Liu\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"}],\"doi\":\"10.1109/CVPR.2010.5539823\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"893ebdaeb66b2d0c5b5be100e2d19e54a3c0824e\",\"title\":\"Single image depth estimation from predicted semantic labels\",\"url\":\"https://www.semanticscholar.org/paper/893ebdaeb66b2d0c5b5be100e2d19e54a3c0824e\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":\"1603.04992\",\"authors\":[{\"authorId\":\"145513196\",\"name\":\"R. Garg\"},{\"authorId\":\"144828045\",\"name\":\"B. V. Kumar\"},{\"authorId\":\"145575177\",\"name\":\"G. Carneiro\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"}],\"doi\":\"10.1007/978-3-319-46484-8_45\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d3beb32fa0efdd10280bad003ef37e5f62f6cbd\",\"title\":\"Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue\",\"url\":\"https://www.semanticscholar.org/paper/4d3beb32fa0efdd10280bad003ef37e5f62f6cbd\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47237027\",\"name\":\"Andreas Geiger\"},{\"authorId\":\"37108776\",\"name\":\"Philip Lenz\"},{\"authorId\":\"1760556\",\"name\":\"C. Stiller\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":\"10.1177/0278364913491297\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79b949d9b35c3f51dd20fb5c746cc81fc87147eb\",\"title\":\"Vision meets robotics: The KITTI dataset\",\"url\":\"https://www.semanticscholar.org/paper/79b949d9b35c3f51dd20fb5c746cc81fc87147eb\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2013},{\"arxivId\":\"1609.01326\",\"authors\":[{\"authorId\":\"3256056\",\"name\":\"Weichao Qiu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1007/978-3-319-49409-8_75\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"debbe1c4f45854004868a163dbcaf318edaffe0e\",\"title\":\"UnrealCV: Connecting Computer Vision to Unreal Engine\",\"url\":\"https://www.semanticscholar.org/paper/debbe1c4f45854004868a163dbcaf318edaffe0e\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2336460\",\"name\":\"Dongwei Liu\"},{\"authorId\":\"1801297\",\"name\":\"R. Nicolescu\"},{\"authorId\":\"1729664\",\"name\":\"R. Klette\"}],\"doi\":\"10.1007/978-3-319-23192-1_17\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd4ad54cbf709ddc7bc2855d6528d19b34a270f8\",\"title\":\"Bokeh Effects Based on Stereo Vision\",\"url\":\"https://www.semanticscholar.org/paper/bd4ad54cbf709ddc7bc2855d6528d19b34a270f8\",\"venue\":\"CAIP\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1849598\",\"name\":\"D. Butler\"},{\"authorId\":\"49820715\",\"name\":\"J. Wulff\"},{\"authorId\":\"2715753\",\"name\":\"G. Stanley\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ff54b7096a47a415864155dc9094869bef98621\",\"title\":\"MPI-SINTEL OPTICAL FLOW BENCHMARK : SUPPLEMENTAL MATERIAL\",\"url\":\"https://www.semanticscholar.org/paper/4ff54b7096a47a415864155dc9094869bef98621\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1606.00915\",\"authors\":[{\"authorId\":\"34192119\",\"name\":\"Liang-Chieh Chen\"},{\"authorId\":\"2776496\",\"name\":\"G. Papandreou\"},{\"authorId\":\"2010660\",\"name\":\"I. Kokkinos\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/TPAMI.2017.2699184\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cab372bc3824780cce20d9dd1c22d4df39ed081a\",\"title\":\"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\",\"url\":\"https://www.semanticscholar.org/paper/cab372bc3824780cce20d9dd1c22d4df39ed081a\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39022297\",\"name\":\"A. Jansson\"},{\"authorId\":\"3268101\",\"name\":\"Eric J. Humphrey\"},{\"authorId\":\"1681005\",\"name\":\"N. Montecchio\"},{\"authorId\":\"13709609\",\"name\":\"Rachel M. Bittner\"},{\"authorId\":\"47311278\",\"name\":\"A. Kumar\"},{\"authorId\":\"3205667\",\"name\":\"Tillman Weyde\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"83ea11b45cba0fc7ee5d60f608edae9c1443861d\",\"title\":\"Singing Voice Separation with Deep U-Net Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/83ea11b45cba0fc7ee5d60f608edae9c1443861d\",\"venue\":\"ISMIR\",\"year\":2017},{\"arxivId\":\"1801.04381\",\"authors\":[{\"authorId\":\"144882893\",\"name\":\"Mark Sandler\"},{\"authorId\":\"144727050\",\"name\":\"A. Howard\"},{\"authorId\":\"2717876\",\"name\":\"Menglong Zhu\"},{\"authorId\":\"3422677\",\"name\":\"A. Zhmoginov\"},{\"authorId\":\"34192119\",\"name\":\"Liang-Chieh Chen\"}],\"doi\":\"10.1109/CVPR.2018.00474\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4\",\"title\":\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\",\"url\":\"https://www.semanticscholar.org/paper/dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1806.04171\",\"authors\":[{\"authorId\":\"34004812\",\"name\":\"N. Wadhwa\"},{\"authorId\":\"145203564\",\"name\":\"R. Garg\"},{\"authorId\":\"48226420\",\"name\":\"D. Jacobs\"},{\"authorId\":\"2782517\",\"name\":\"Bryan E. Feldman\"},{\"authorId\":\"8804320\",\"name\":\"Nori Kanazawa\"},{\"authorId\":\"1384104871\",\"name\":\"R. Carroll\"},{\"authorId\":\"1405595195\",\"name\":\"Yair Movshovitz-Attias\"},{\"authorId\":\"50329510\",\"name\":\"J. Barron\"},{\"authorId\":\"1782328\",\"name\":\"Y. Pritch\"},{\"authorId\":\"1801789\",\"name\":\"M. Levoy\"}],\"doi\":\"10.1145/3197517.3201329\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4597bd719691521a35faca0601ad4caef0abd6ac\",\"title\":\"Synthetic depth-of-field with a single-camera mobile phone\",\"url\":\"https://www.semanticscholar.org/paper/4597bd719691521a35faca0601ad4caef0abd6ac\",\"venue\":\"ACM Trans. Graph.\",\"year\":2018},{\"arxivId\":\"1612.01105\",\"authors\":[{\"authorId\":\"3459894\",\"name\":\"Hengshuang Zhao\"},{\"authorId\":\"1788070\",\"name\":\"J. Shi\"},{\"authorId\":\"50844674\",\"name\":\"Xiaojuan Qi\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"},{\"authorId\":\"1729056\",\"name\":\"J. Jia\"}],\"doi\":\"10.1109/CVPR.2017.660\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1031a69923b80ad01cf3fbb703d10757a80e699b\",\"title\":\"Pyramid Scene Parsing Network\",\"url\":\"https://www.semanticscholar.org/paper/1031a69923b80ad01cf3fbb703d10757a80e699b\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1504.06852\",\"authors\":[{\"authorId\":\"1382344214\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"152702479\",\"name\":\"P. Fischer\"},{\"authorId\":\"48105320\",\"name\":\"Eddy Ilg\"},{\"authorId\":\"2880264\",\"name\":\"Philip H\\u00e4usser\"},{\"authorId\":\"3322806\",\"name\":\"Caner Hazirbas\"},{\"authorId\":\"2943639\",\"name\":\"V. Golkov\"},{\"authorId\":\"1715782\",\"name\":\"P. V. D. Smagt\"},{\"authorId\":\"153685345\",\"name\":\"D. Cremers\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/ICCV.2015.316\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c2fb5b39428818d7ec8cc78e152e19c21b7db568\",\"title\":\"FlowNet: Learning Optical Flow with Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/c2fb5b39428818d7ec8cc78e152e19c21b7db568\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1711.10684\",\"authors\":[{\"authorId\":\"7969319\",\"name\":\"Zhengxin Zhang\"},{\"authorId\":\"150270468\",\"name\":\"Qingjie Liu\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"}],\"doi\":\"10.1109/LGRS.2018.2802944\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f06ff5f719eb9cd939dde8fc9b199b17adcbc75f\",\"title\":\"Road Extraction by Deep Residual U-Net\",\"url\":\"https://www.semanticscholar.org/paper/f06ff5f719eb9cd939dde8fc9b199b17adcbc75f\",\"venue\":\"IEEE Geoscience and Remote Sensing Letters\",\"year\":2018}],\"title\":\"Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre Bokeh Effect on Smartphones\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/8bd5d2b3c16fa42d0d1f9a9b3295e9de7b658d2f\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"