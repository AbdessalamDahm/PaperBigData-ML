"{\"abstract\":\"We introduce STAViS, a spatio-temporal audiovisual saliency network that combines spatio-temporal visual and auditory information in order to efficiently address the problem of saliency estimation in videos. Our approach employs a single network that combines visual saliency and auditory features and learns to appropriately localize sound sources and to fuse the two saliencies in order to obtain a final saliency map. The network has been designed, trained end-to-end, and evaluated on six different databases that contain audiovisual eye-tracking data of a large variety of videos. We compare our method against 8 different state-of-the-art visual saliency models. Evaluation results across databases indicate that our STAViS model outperforms our visual only variant as well as the other state-of-the-art models in the majority of cases. Also, the consistently good performance it achieves for all databases indicates that it is appropriate for estimating saliency \\\"in-the-wild\\\". The code is available at https://github.com/atsiami/STAViS.\",\"arxivId\":\"2001.03063\",\"authors\":[{\"authorId\":\"3200442\",\"name\":\"A. Tsiami\",\"url\":\"https://www.semanticscholar.org/author/3200442\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\",\"url\":\"https://www.semanticscholar.org/author/2539459\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\",\"url\":\"https://www.semanticscholar.org/author/1750686\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.06170\",\"authors\":[{\"authorId\":\"151444035\",\"name\":\"Samyak Jain\"},{\"authorId\":\"34935738\",\"name\":\"Pradeep Yarlagadda\"},{\"authorId\":\"48236457\",\"name\":\"R. Subramanian\"},{\"authorId\":\"145091336\",\"name\":\"V. Gandhi\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"40a7f20cf6a05a481146bd7f18ba747d17700ebd\",\"title\":\"AViNet: Diving Deep into Audio-Visual Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/40a7f20cf6a05a481146bd7f18ba747d17700ebd\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.04968\",\"authors\":[{\"authorId\":\"1390651399\",\"name\":\"Yao Jiang\"},{\"authorId\":\"46995707\",\"name\":\"T. Zhou\"},{\"authorId\":\"94805190\",\"name\":\"Ge-Peng Ji\"},{\"authorId\":\"1847070\",\"name\":\"Keren Fu\"},{\"authorId\":\"101581202\",\"name\":\"Qi-jun Zhao\"},{\"authorId\":\"23999143\",\"name\":\"Deng-Ping Fan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"610157d3f619e3f39df817d345b29c7adfff164d\",\"title\":\"Light Field Salient Object Detection: A Review and Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/610157d3f619e3f39df817d345b29c7adfff164d\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":210116668,\"doi\":\"10.1109/cvpr42600.2020.00482\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"e296a1f529d9caa125e8c6a56cac61423e04b41b\",\"references\":[{\"arxivId\":\"1411.1045\",\"authors\":[{\"authorId\":\"2997408\",\"name\":\"Matthias K\\u00fcmmerer\"},{\"authorId\":\"2073063\",\"name\":\"L. Theis\"},{\"authorId\":\"1731199\",\"name\":\"M. Bethge\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"652e652bca63f60c2c5f5840d4a34bb743f699b9\",\"title\":\"Deep Gaze I: Boosting Saliency Prediction with Feature Maps Trained on ImageNet\",\"url\":\"https://www.semanticscholar.org/paper/652e652bca63f60c2c5f5840d4a34bb743f699b9\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3133294\",\"name\":\"Y. Chen\"},{\"authorId\":\"34646933\",\"name\":\"T. V. Nguyen\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"},{\"authorId\":\"143625300\",\"name\":\"J. Yuan\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"},{\"authorId\":\"47446553\",\"name\":\"M. Wang\"}],\"doi\":\"10.1109/TCSVT.2014.2329380\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5bab2fe034daf2f71a58fb659454e0614dcffc7d\",\"title\":\"Audio Matters in Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/5bab2fe034daf2f71a58fb659454e0614dcffc7d\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"78856769\",\"name\":\"R\\u00e9mi Ratajczak\"},{\"authorId\":\"31617713\",\"name\":\"D. Pellerin\"},{\"authorId\":\"1705915\",\"name\":\"C. Garbay\"},{\"authorId\":\"2940700\",\"name\":\"Quentin Labourey\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70379efb48b4e184a39cff0796c24166b0765850\",\"title\":\"A Fast Audiovisual Attention Model for Human Detection and Localization on a Companion Robot\",\"url\":\"https://www.semanticscholar.org/paper/70379efb48b4e184a39cff0796c24166b0765850\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"87220863\",\"name\":\"E. van der Burg\"},{\"authorId\":\"145453702\",\"name\":\"C. Olivers\"},{\"authorId\":\"2446877\",\"name\":\"A. Bronkhorst\"},{\"authorId\":\"145639856\",\"name\":\"J. Theeuwes\"}],\"doi\":\"10.1037/0096-1523.34.5.1053\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ddb4798846e0d5442781aeb1b121d449a0b905d\",\"title\":\"Pip and pop: nonspatial auditory signals improve spatial visual search.\",\"url\":\"https://www.semanticscholar.org/paper/7ddb4798846e0d5442781aeb1b121d449a0b905d\",\"venue\":\"Journal of experimental psychology. Human perception and performance\",\"year\":2008},{\"arxivId\":\"1701.01081\",\"authors\":[{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"1399086207\",\"name\":\"C. Canton-Ferrer\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"147166602\",\"name\":\"J. Torres\"},{\"authorId\":\"2470219\",\"name\":\"E. Sayrol\"},{\"authorId\":\"3100480\",\"name\":\"Xavier Giro-i-Nieto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"258fad95e709b6d0572ae6cc99efbbb14d32bdf2\",\"title\":\"SalGAN: Visual Saliency Prediction with Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/258fad95e709b6d0572ae6cc99efbbb14d32bdf2\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"79505031\",\"name\":\"G. Kov\\u00e1cs\"},{\"authorId\":\"1794638\",\"name\":\"Y. Kunii\"},{\"authorId\":\"1769766\",\"name\":\"Takao Maeda\"},{\"authorId\":\"144921219\",\"name\":\"H. Hashimoto\"}],\"doi\":\"10.1080/01691864.2019.1602564\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"74aa248b031956dad974d7d27eeaefb868aafccf\",\"title\":\"Saliency and spatial information-based landmark selection for mobile robot navigation in natural environments\",\"url\":\"https://www.semanticscholar.org/paper/74aa248b031956dad974d7d27eeaefb868aafccf\",\"venue\":\"Adv. Robotics\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alex Graves\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks\",\"url\":\"\",\"venue\":\"Proc. IEEE Int. Conf. Acous., Speech, and Signal Processing (ICASSP)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144129667\",\"name\":\"M. A. Meredith\"},{\"authorId\":\"3220152\",\"name\":\"B. Stein\"}],\"doi\":\"10.1152/JN.1986.56.3.640\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59f1adbe119370e0c3de97adfc1c8110e6f57c10\",\"title\":\"Visual, auditory, and somatosensory convergence on cells in superior colliculus results in multisensory integration.\",\"url\":\"https://www.semanticscholar.org/paper/59f1adbe119370e0c3de97adfc1c8110e6f57c10\",\"venue\":\"Journal of neurophysiology\",\"year\":1986},{\"arxivId\":\"1705.02544\",\"authors\":[{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"145953515\",\"name\":\"J. Shen\"}],\"doi\":\"10.1109/TIP.2017.2787612\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"65c1a8cde4030217bec7cb96b8bf94c5321e8d8e\",\"title\":\"Deep Visual Attention Prediction\",\"url\":\"https://www.semanticscholar.org/paper/65c1a8cde4030217bec7cb96b8bf94c5321e8d8e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1801.07424\",\"authors\":[{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"145953515\",\"name\":\"J. Shen\"},{\"authorId\":\"143929120\",\"name\":\"F. Guo\"},{\"authorId\":\"37535930\",\"name\":\"Ming-Ming Cheng\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":\"10.1109/CVPR.2018.00514\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fdf8c9c4c30c6005c2f0e92ce9db3de5ab8b5d29\",\"title\":\"Revisiting Video Saliency: A Large-Scale Benchmark and a New Model\",\"url\":\"https://www.semanticscholar.org/paper/fdf8c9c4c30c6005c2f0e92ce9db3de5ab8b5d29\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Georgios Evangelopoulos\"},{\"authorId\":null,\"name\":\"Athanasia Zlatintsi\"},{\"authorId\":null,\"name\":\"Alexandros Potamianos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Petros Maragos, Konstantinos Rapantzikos, Georgios Skoumas, and Yannis Avrithis. Multimodal saliency and fusion for movie summarization based on aural, visual, textual attention\",\"url\":\"\",\"venue\":\"IEEE Trans. Multimedia\",\"year\":null},{\"arxivId\":\"1803.08842\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"145458657\",\"name\":\"Jing Shi\"},{\"authorId\":\"2868721\",\"name\":\"Bochen Li\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01216-8_16\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"title\":\"Audio-Visual Event Localization in Unconstrained Videos\",\"url\":\"https://www.semanticscholar.org/paper/a5e58ef7c11515847967019fbe01fa033d9bdd88\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2624078\",\"name\":\"A. Coutrot\"},{\"authorId\":\"3252109\",\"name\":\"N. Guyader\"}],\"doi\":\"10.1007/978-1-4939-3435-5_16\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5487c3f9b226a92a30bf214aa37d071a4dd9437c\",\"title\":\"Multimodal Saliency Models for Videos\",\"url\":\"https://www.semanticscholar.org/paper/5487c3f9b226a92a30bf214aa37d071a4dd9437c\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2286630\",\"name\":\"E. Vig\"},{\"authorId\":\"1944405\",\"name\":\"M. Dorr\"},{\"authorId\":\"2042941\",\"name\":\"D. Cox\"}],\"doi\":\"10.1109/CVPR.2014.358\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8693c32dff29e851760fa0b6af464050ffc383d6\",\"title\":\"Large-Scale Optimization of Hierarchical Features for Saliency Prediction in Natural Images\",\"url\":\"https://www.semanticscholar.org/paper/8693c32dff29e851760fa0b6af464050ffc383d6\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1804.03641\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1007/978-3-030-01231-1_39\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"title\":\"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\",\"url\":\"https://www.semanticscholar.org/paper/171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4439383\",\"name\":\"Jamie Ray\"},{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"35259685\",\"name\":\"Y. Wang\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1007/978-3-030-01264-9_39\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71167cf519940a7373adc221401c396198763ab0\",\"title\":\"Scenes-Objects-Actions: A Multi-task, Multi-label Video Dataset\",\"url\":\"https://www.semanticscholar.org/paper/71167cf519940a7373adc221401c396198763ab0\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergi Caelles\"},{\"authorId\":null,\"name\":\"Kevis-Kokitsi Maninis\"},{\"authorId\":null,\"name\":\"Jordi Pont-Tuset\"},{\"authorId\":null,\"name\":\"Laura Leal-Taix\\u00e9\"},{\"authorId\":null,\"name\":\"Daniel Cremers\"},{\"authorId\":null,\"name\":\"Luc Van Gool\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Oneshot video object segmentation\",\"url\":\"\",\"venue\":\"In Proc. IEEE Conf. on Computer Vision and Pattern Recognition\",\"year\":2017},{\"arxivId\":\"1904.05979\",\"authors\":[{\"authorId\":\"144077750\",\"name\":\"Hang Zhao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"2650832\",\"name\":\"W. Ma\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/ICCV.2019.00182\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c880de441a41c351955ad0bf8f712eeee500ac67\",\"title\":\"The Sound of Motions\",\"url\":\"https://www.semanticscholar.org/paper/c880de441a41c351955ad0bf8f712eeee500ac67\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1711.09577\",\"authors\":[{\"authorId\":\"2199251\",\"name\":\"K. Hara\"},{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"},{\"authorId\":\"1732705\",\"name\":\"Y. Satoh\"}],\"doi\":\"10.1109/CVPR.2018.00685\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d716435f0cb0cac56237f74b1ced940aabce6a2b\",\"title\":\"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\",\"url\":\"https://www.semanticscholar.org/paper/d716435f0cb0cac56237f74b1ced940aabce6a2b\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144247007\",\"name\":\"X. Huang\"},{\"authorId\":\"3329744\",\"name\":\"Chengyao Shen\"},{\"authorId\":\"2343486\",\"name\":\"X. Boix\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"}],\"doi\":\"10.1109/ICCV.2015.38\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1281e443d2cf1c1dd71ed3b7b0376d408d0958af\",\"title\":\"SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/1281e443d2cf1c1dd71ed3b7b0376d408d0958af\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1604.03605\",\"authors\":[{\"authorId\":\"3326347\",\"name\":\"Z. Bylinskii\"},{\"authorId\":\"152627906\",\"name\":\"T. Judd\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"}],\"doi\":\"10.1109/TPAMI.2018.2815601\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4\",\"title\":\"What Do Different Evaluation Metrics Tell Us About Saliency Models?\",\"url\":\"https://www.semanticscholar.org/paper/bfdf3431ff5182b585b3b0c11fa8cbfc13a6bde4\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"2037692\",\"name\":\"Dicky N. Sihite\"},{\"authorId\":\"7326223\",\"name\":\"L. Itti\"}],\"doi\":\"10.1109/TIP.2012.2210727\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c0b60d02b2d59123f6b336fe2e287bdb02a2a776\",\"title\":\"Quantitative Analysis of Human-Model Agreement in Visual Saliency Modeling: A Comparative Study\",\"url\":\"https://www.semanticscholar.org/paper/c0b60d02b2d59123f6b336fe2e287bdb02a2a776\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47839505\",\"name\":\"J. Ruesch\"},{\"authorId\":\"153695440\",\"name\":\"Manuel Lopes\"},{\"authorId\":\"145036494\",\"name\":\"A. Bernardino\"},{\"authorId\":\"2481597\",\"name\":\"J. H\\u00f6rnstein\"},{\"authorId\":\"1398909021\",\"name\":\"J. Santos-Victor\"},{\"authorId\":\"38749462\",\"name\":\"R. Pfeifer\"}],\"doi\":\"10.1109/ROBOT.2008.4543329\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a30f69ebeda5936419f95c721c272b941d3794f8\",\"title\":\"Multimodal saliency-based bottom-up attention a framework for the humanoid robot iCub\",\"url\":\"https://www.semanticscholar.org/paper/a30f69ebeda5936419f95c721c272b941d3794f8\",\"venue\":\"2008 IEEE International Conference on Robotics and Automation\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2033590\",\"name\":\"N. Sidaty\"},{\"authorId\":\"1755023\",\"name\":\"Mohamed-Chaker Larabi\"},{\"authorId\":\"2715507\",\"name\":\"A. Saadane\"}],\"doi\":\"10.1016/j.neucom.2016.08.130\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3406594d884560833ad65d06aa0a83e0f44eb09\",\"title\":\"Toward an audiovisual attention model for multimodal video content\",\"url\":\"https://www.semanticscholar.org/paper/c3406594d884560833ad65d06aa0a83e0f44eb09\",\"venue\":\"Neurocomputing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144129667\",\"name\":\"M. A. Meredith\"},{\"authorId\":\"3220152\",\"name\":\"B. Stein\"}],\"doi\":\"10.1126/SCIENCE.6867718\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a315f5374673931529dc54cf286cda3b64827d4\",\"title\":\"Interactions among converging sensory inputs in the superior colliculus.\",\"url\":\"https://www.semanticscholar.org/paper/5a315f5374673931529dc54cf286cda3b64827d4\",\"venue\":\"Science\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2246414\",\"name\":\"Xiongkuo Min\"},{\"authorId\":\"144826390\",\"name\":\"G. Zhai\"},{\"authorId\":\"144839077\",\"name\":\"K. Gu\"},{\"authorId\":\"1795291\",\"name\":\"X. Yang\"}],\"doi\":\"10.1145/2996463\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"915e68b9de2f982e96c29a56a5610f2bc1bbb34a\",\"title\":\"Fixation prediction through multimodal analysis\",\"url\":\"https://www.semanticscholar.org/paper/915e68b9de2f982e96c29a56a5610f2bc1bbb34a\",\"venue\":\"2015 Visual Communications and Image Processing (VCIP)\",\"year\":2015},{\"arxivId\":\"1504.06375\",\"authors\":[{\"authorId\":\"1817030\",\"name\":\"Saining Xie\"},{\"authorId\":\"144035504\",\"name\":\"Zhuowen Tu\"}],\"doi\":\"10.1007/s11263-017-1004-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4100414ad0763bdc91bd15bb6e0424a44d7a35fe\",\"title\":\"Holistically-Nested Edge Detection\",\"url\":\"https://www.semanticscholar.org/paper/4100414ad0763bdc91bd15bb6e0424a44d7a35fe\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2205256\",\"name\":\"Sudarshan Ramenahalli\"},{\"authorId\":\"3306327\",\"name\":\"D. Mendat\"},{\"authorId\":\"1405798279\",\"name\":\"S. Dura-Bernal\"},{\"authorId\":\"2889774\",\"name\":\"E. Culurciello\"},{\"authorId\":\"3271571\",\"name\":\"E. Niebur\"},{\"authorId\":\"2730857\",\"name\":\"A. Andreou\"}],\"doi\":\"10.1109/CISS.2013.6552285\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef483498459b9fbb64b24b807ce3743535d919d1\",\"title\":\"Audio-visual saliency map: Overview, basic models and hardware implementation\",\"url\":\"https://www.semanticscholar.org/paper/ef483498459b9fbb64b24b807ce3743535d919d1\",\"venue\":\"2013 47th Annual Conference on Information Sciences and Systems (CISS)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2704722\",\"name\":\"A. Vatakis\"},{\"authorId\":\"144248893\",\"name\":\"C. Spence\"}],\"doi\":\"10.3758/BF03193776\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d96132c35edbe14cd29d7c56ffb95714ca29399\",\"title\":\"Crossmodal binding: Evaluating the \\u201cunity assumption\\u201d using audiovisual speech stimuli\",\"url\":\"https://www.semanticscholar.org/paper/9d96132c35edbe14cd29d7c56ffb95714ca29399\",\"venue\":\"Perception & psychophysics\",\"year\":2007},{\"arxivId\":\"1712.06651\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01246-5_27\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfc504536e8434eb008680343abb77010965169e\",\"title\":\"Objects that Sound\",\"url\":\"https://www.semanticscholar.org/paper/dfc504536e8434eb008680343abb77010965169e\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1804.01793\",\"authors\":[{\"authorId\":\"35129473\",\"name\":\"Saumya Jetley\"},{\"authorId\":\"26734366\",\"name\":\"N. Murray\"},{\"authorId\":\"2286630\",\"name\":\"E. Vig\"}],\"doi\":\"10.1109/CVPR.2016.620\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4d42c041bf30021550e581775c1e04f253edf54\",\"title\":\"End-to-End Saliency Mapping via Probability Distribution Prediction\",\"url\":\"https://www.semanticscholar.org/paper/a4d42c041bf30021550e581775c1e04f253edf54\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2602925\",\"name\":\"Boris Schauerte\"},{\"authorId\":\"34855624\",\"name\":\"B. K\\u00fchn\"},{\"authorId\":\"1787004\",\"name\":\"K. Kroschel\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1109/IROS.2011.6095124\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c15d3ca34d885b05185b1e814876ea99bd8e833d\",\"title\":\"Multimodal saliency-based attention for object-based scene analysis\",\"url\":\"https://www.semanticscholar.org/paper/c15d3ca34d885b05185b1e814876ea99bd8e833d\",\"venue\":\"2011 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"145953515\",\"name\":\"J. Shen\"},{\"authorId\":\"39380386\",\"name\":\"Jianwen Xie\"},{\"authorId\":\"37535930\",\"name\":\"Ming-Ming Cheng\"},{\"authorId\":\"1805398\",\"name\":\"Haibin Ling\"},{\"authorId\":\"51004202\",\"name\":\"A. Borji\"}],\"doi\":\"10.1109/TPAMI.2019.2924417\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60782ad3f40d13fae19fbe26b6f7c0ac5011f83d\",\"title\":\"Revisiting Video Saliency Prediction in the Deep Learning Era\",\"url\":\"https://www.semanticscholar.org/paper/60782ad3f40d13fae19fbe26b6f7c0ac5011f83d\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2021},{\"arxivId\":\"1909.02869\",\"authors\":[{\"authorId\":\"12746771\",\"name\":\"P. Primus\"},{\"authorId\":\"1406798986\",\"name\":\"Hamid Eghbal-zadeh\"},{\"authorId\":\"94699847\",\"name\":\"David Eitelsebner\"},{\"authorId\":\"28921847\",\"name\":\"Khaled Koutini\"},{\"authorId\":\"3202583\",\"name\":\"A. Arzt\"},{\"authorId\":\"145964711\",\"name\":\"G. Widmer\"}],\"doi\":\"10.33682/v9qj-8954\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"48633f36bc53b7c537dcd099ad649d1921d3a986\",\"title\":\"Exploiting Parallel Audio Recordings to Enforce Device Invariance in CNN-based Acoustic Scene Classification\",\"url\":\"https://www.semanticscholar.org/paper/48633f36bc53b7c537dcd099ad649d1921d3a986\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1705.07750\",\"authors\":[{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.502\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"title\":\"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\"url\":\"https://www.semanticscholar.org/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1712.01769\",\"authors\":[{\"authorId\":\"145039780\",\"name\":\"Chung-Cheng Chiu\"},{\"authorId\":\"1784851\",\"name\":\"T. Sainath\"},{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"},{\"authorId\":\"2557391\",\"name\":\"Rohit Prabhavalkar\"},{\"authorId\":\"40133958\",\"name\":\"Patrick Nguyen\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"31801501\",\"name\":\"A. Kannan\"},{\"authorId\":\"39571582\",\"name\":\"Ron J. Weiss\"},{\"authorId\":\"2251957\",\"name\":\"K. Rao\"},{\"authorId\":\"1398413062\",\"name\":\"Katya Gonina\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"143771569\",\"name\":\"Bo Li\"},{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"1771090\",\"name\":\"M. Bacchiani\"}],\"doi\":\"10.1109/ICASSP.2018.8462105\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6b61535f1544835cca3851ceb34222ebc5b4377\",\"title\":\"State-of-the-Art Speech Recognition with Sequence-to-Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/c6b61535f1544835cca3851ceb34222ebc5b4377\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3200442\",\"name\":\"A. Tsiami\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"2243473\",\"name\":\"Athanasios Katsamanis\"},{\"authorId\":\"46788705\",\"name\":\"A. Vatakis\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1016/J.IMAGE.2019.05.001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5cfa95cd409a47fbacce89c6a1df6d0410b810e5\",\"title\":\"A behaviorally inspired fusion approach for computational audiovisual saliency modeling\",\"url\":\"https://www.semanticscholar.org/paper/5cfa95cd409a47fbacce89c6a1df6d0410b810e5\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"},{\"authorId\":\"145551629\",\"name\":\"H. Grabner\"},{\"authorId\":\"1848930\",\"name\":\"Hayko Riemenschneider\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-10584-0_33\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"799bf307438ec2171e6f0bd5b8040f678d5b28da\",\"title\":\"Creating Summaries from User Videos\",\"url\":\"https://www.semanticscholar.org/paper/799bf307438ec2171e6f0bd5b8040f678d5b28da\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1016/j.image.2015.08.004\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"933fe12164f8e4c6c1075dc557b1b3fdcb4f5168\",\"title\":\"A perceptually based spatio-temporal computational framework for visual saliency estimation\",\"url\":\"https://www.semanticscholar.org/paper/933fe12164f8e4c6c1075dc557b1b3fdcb4f5168\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38111179\",\"name\":\"Siavash Gorji\"},{\"authorId\":\"47125588\",\"name\":\"James J. Clark\"}],\"doi\":\"10.1109/CVPR.2018.00783\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9a2fae46c67189fb2aea33f12091772e635361f1\",\"title\":\"Going from Image to Video Saliency: Augmenting Image Salience with Dynamic Attentional Push\",\"url\":\"https://www.semanticscholar.org/paper/9a2fae46c67189fb2aea33f12091772e635361f1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143786831\",\"name\":\"X. Yuan\"},{\"authorId\":\"5391729\",\"name\":\"J. Yue\"},{\"authorId\":\"40600391\",\"name\":\"Yanan Zhang\"}],\"doi\":\"10.1109/ROBIO.2018.8664750\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d32dbe4e0ea9d6442a38cdc638f649f52907a1c\",\"title\":\"RGB-D Saliency Detection: Dataset and Algorithm for Robot Vision\",\"url\":\"https://www.semanticscholar.org/paper/2d32dbe4e0ea9d6442a38cdc638f649f52907a1c\",\"venue\":\"2018 IEEE International Conference on Robotics and Biomimetics (ROBIO)\",\"year\":2018},{\"arxivId\":\"1705.08168\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/ICCV.2017.73\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9b5f696f73c1264ccb8e97d3b738a2342ecd6bee\",\"title\":\"Look, Listen and Learn\",\"url\":\"https://www.semanticscholar.org/paper/9b5f696f73c1264ccb8e97d3b738a2342ecd6bee\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710606\",\"name\":\"Georgios Evangelopoulos\"},{\"authorId\":\"2641229\",\"name\":\"A. Zlatintsi\"},{\"authorId\":\"1791187\",\"name\":\"A. Potamianos\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"},{\"authorId\":\"2057872\",\"name\":\"K. Rapantzikos\"},{\"authorId\":\"2880178\",\"name\":\"Georgios Skoumas\"},{\"authorId\":\"1744904\",\"name\":\"Yannis Avrithis\"}],\"doi\":\"10.1109/TMM.2013.2267205\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"463df0d0d0731e62315b5c9322e2f8e8757014f7\",\"title\":\"Multimodal Saliency and Fusion for Movie Summarization Based on Aural, Visual, and Textual Attention\",\"url\":\"https://www.semanticscholar.org/paper/463df0d0d0731e62315b5c9322e2f8e8757014f7\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Harry McGurk\"},{\"authorId\":null,\"name\":\"John MacDonald\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hearing lips and seeing\",\"url\":\"\",\"venue\":\"voices. Nature,\",\"year\":1976},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47420847\",\"name\":\"Lai Jiang\"},{\"authorId\":\"1743773\",\"name\":\"M. Xu\"},{\"authorId\":\"92349749\",\"name\":\"T. Liu\"},{\"authorId\":\"27647516\",\"name\":\"Minglang Qiao\"},{\"authorId\":\"1754571\",\"name\":\"Z. Wang\"}],\"doi\":\"10.1007/978-3-030-01264-9_37\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"184ffa4a4c36051de56e07d785e5b53928d8c472\",\"title\":\"DeepVS: A Deep Learning Based Video Saliency Prediction Approach\",\"url\":\"https://www.semanticscholar.org/paper/184ffa4a4c36051de56e07d785e5b53928d8c472\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2624078\",\"name\":\"A. Coutrot\"},{\"authorId\":\"3252109\",\"name\":\"N. Guyader\"}],\"doi\":\"10.1167/14.8.5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a9f2486ded05be499a7400ae548f0b90f2d8e34\",\"title\":\"How saliency, faces, and sound influence gaze in dynamic social scenes.\",\"url\":\"https://www.semanticscholar.org/paper/0a9f2486ded05be499a7400ae548f0b90f2d8e34\",\"venue\":\"Journal of vision\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2054241\",\"name\":\"H. McGurk\"},{\"authorId\":\"143965550\",\"name\":\"J. Macdonald\"}],\"doi\":\"10.1038/264746A0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eef41ae597a20ea377461d522fd5100da6a7a9b7\",\"title\":\"Hearing lips and seeing voices\",\"url\":\"https://www.semanticscholar.org/paper/eef41ae597a20ea377461d522fd5100da6a7a9b7\",\"venue\":\"Nature\",\"year\":1976},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30469750\",\"name\":\"Qiuxia Lai\"},{\"authorId\":\"2693875\",\"name\":\"Wenguan Wang\"},{\"authorId\":\"145947590\",\"name\":\"Hanqiu Sun\"},{\"authorId\":\"11901550\",\"name\":\"J. Shen\"}],\"doi\":\"10.1109/TIP.2019.2936112\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be34dd48c374b0313db1f0b70347464cbe0b8f22\",\"title\":\"Video Saliency Prediction Using Spatiotemporal Residual Attentive Networks\",\"url\":\"https://www.semanticscholar.org/paper/be34dd48c374b0313db1f0b70347464cbe0b8f22\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1905.10693\",\"authors\":[{\"authorId\":\"2319672\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"51004202\",\"name\":\"A. Borji\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"},{\"authorId\":\"1776374\",\"name\":\"Juho Kannala\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"827b68bdb1e38bb75e0db020ce764d25997b4c60\",\"title\":\"DAVE: A Deep Audio-Visual Embedding for Dynamic Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/827b68bdb1e38bb75e0db020ce764d25997b4c60\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1610.09001\",\"authors\":[{\"authorId\":\"3152281\",\"name\":\"Y. Aytar\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"title\":\"SoundNet: Learning Sound Representations from Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2146425\",\"name\":\"Guanghan Song\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b9ce52d700beb90012b8449365284a2b6e472ef8\",\"title\":\"Effect of sound in videos on gaze : contribution to audio-visual saliency modelling\",\"url\":\"https://www.semanticscholar.org/paper/b9ce52d700beb90012b8449365284a2b6e472ef8\",\"venue\":\"\",\"year\":2013},{\"arxivId\":\"1907.01869\",\"authors\":[{\"authorId\":\"51244663\",\"name\":\"Panagiotis Linardos\"},{\"authorId\":\"2890278\",\"name\":\"Eva Mohedano\"},{\"authorId\":\"144011211\",\"name\":\"J. J. Nieto\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"3100480\",\"name\":\"Xavier Giro-i-Nieto\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d826cd1c9ad907ae1c57a14740eb84a3075f2725\",\"title\":\"Simple vs complex temporal recurrences for video saliency prediction\",\"url\":\"https://www.semanticscholar.org/paper/d826cd1c9ad907ae1c57a14740eb84a3075f2725\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5021307\",\"name\":\"Vincent Sitzmann\"},{\"authorId\":\"143772343\",\"name\":\"Ana Serrano\"},{\"authorId\":\"48453720\",\"name\":\"A. Pavel\"},{\"authorId\":\"1820412\",\"name\":\"M. Agrawala\"},{\"authorId\":\"143876232\",\"name\":\"D. Gutierrez\"},{\"authorId\":\"1731170\",\"name\":\"G. Wetzstein\"}],\"doi\":\"10.1109/TVCG.2018.2793599\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3d382d1aad38e7e1a9249a7bd323f9a5cf92ca8\",\"title\":\"Saliency in VR: How Do People Explore Virtual Environments?\",\"url\":\"https://www.semanticscholar.org/paper/c3d382d1aad38e7e1a9249a7bd323f9a5cf92ca8\",\"venue\":\"IEEE Transactions on Visualization and Computer Graphics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2624078\",\"name\":\"A. Coutrot\"},{\"authorId\":\"3252109\",\"name\":\"N. Guyader\"}],\"doi\":\"10.1109/ICIP.2014.7025219\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"160ade45f8818a7e92e2ff8891afa3e86470e14d\",\"title\":\"An audiovisual attention model for natural conversation scenes\",\"url\":\"https://www.semanticscholar.org/paper/160ade45f8818a7e92e2ff8891afa3e86470e14d\",\"venue\":\"2014 IEEE International Conference on Image Processing (ICIP)\",\"year\":2014},{\"arxivId\":\"1608.00859\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-46484-8_2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"title\":\"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1303.5778\",\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1109/ICASSP.2013.6638947\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4177ec52d1b80ed57f2e72b0f9a42365f1a8598d\",\"title\":\"Speech recognition with deep recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/4177ec52d1b80ed57f2e72b0f9a42365f1a8598d\",\"venue\":\"2013 IEEE International Conference on Acoustics, Speech and Signal Processing\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sudarshan Ramenahalli\"},{\"authorId\":null,\"name\":\"Salvador Daniel R Mendat\"},{\"authorId\":null,\"name\":\"Eugenio Dura-Bernal\"},{\"authorId\":null,\"name\":\"Culurciello\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Ernst Niebur, and Andreas Andreou. Audio-visual saliency map: Overview, basic models and hardware implementation\",\"url\":\"\",\"venue\":\"Proc. Information Sciences and Systems (CISS)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144988039\",\"name\":\"Tung Dang\"},{\"authorId\":\"2682660\",\"name\":\"C. Papachristos\"},{\"authorId\":\"144332366\",\"name\":\"K. Alexis\"}],\"doi\":\"10.1109/ICRA.2018.8460992\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cfdf93f363f5366dd2feede0bc207a9d0fdd75e\",\"title\":\"Visual Saliency-Aware Receding Horizon Autonomous Exploration with Application to Aerial Robotics\",\"url\":\"https://www.semanticscholar.org/paper/8cfdf93f363f5366dd2feede0bc207a9d0fdd75e\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"3411472\",\"name\":\"G. Panagiotaropoulou\"},{\"authorId\":\"3200442\",\"name\":\"A. Tsiami\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/CVPRW.2018.00269\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"113eda25ece35fd7ecb51cb104182a973ea2313e\",\"title\":\"Audio-Visual Temporal Saliency Modeling Validated by fMRI Data\",\"url\":\"https://www.semanticscholar.org/paper/113eda25ece35fd7ecb51cb104182a973ea2313e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":\"1603.00845\",\"authors\":[{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"2470219\",\"name\":\"E. Sayrol\"},{\"authorId\":\"3100480\",\"name\":\"Xavier Giro-i-Nieto\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"}],\"doi\":\"10.1109/CVPR.2016.71\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9528e2e8c20517ab916f803c0371abb4f0ed488b\",\"title\":\"Shallow and Deep Convolutional Networks for Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/9528e2e8c20517ab916f803c0371abb4f0ed488b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2306103\",\"name\":\"Parag K. Mital\"},{\"authorId\":\"145165599\",\"name\":\"T. Smith\"},{\"authorId\":\"3252072\",\"name\":\"R. L. Hill\"},{\"authorId\":\"1889476\",\"name\":\"J. Henderson\"}],\"doi\":\"10.1007/s12559-010-9074-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"27266e3be4d9b9572e8e11f4ef34110a2a00f515\",\"title\":\"Clustering of Gaze During Dynamic Scene Viewing is Predicted by Motion\",\"url\":\"https://www.semanticscholar.org/paper/27266e3be4d9b9572e8e11f4ef34110a2a00f515\",\"venue\":\"Cognitive Computation\",\"year\":2010},{\"arxivId\":\"1908.05786\",\"authors\":[{\"authorId\":\"41018180\",\"name\":\"Kyle Min\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/ICCV.2019.00248\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b93aaf71e4c18d304ab9b3e77598cb0423d0952\",\"title\":\"TASED-Net: Temporally-Aggregating Spatial Encoder-Decoder Network for Video Saliency Detection\",\"url\":\"https://www.semanticscholar.org/paper/5b93aaf71e4c18d304ab9b3e77598cb0423d0952\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1703.06870\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"}],\"doi\":\"10.1109/ICCV.2017.322\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ea99a5535388196d0d44be5b4d7dd02029a43bb2\",\"title\":\"Mask R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/ea99a5535388196d0d44be5b4d7dd02029a43bb2\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1804.03160\",\"authors\":[{\"authorId\":\"144077750\",\"name\":\"Hang Zhao\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"41020711\",\"name\":\"Andrew Rouditchenko\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-030-01246-5_35\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fe018f22600d07cbd0452a070e03708886470015\",\"title\":\"The Sound of Pixels\",\"url\":\"https://www.semanticscholar.org/paper/fe018f22600d07cbd0452a070e03708886470015\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1603.03669\",\"authors\":[{\"authorId\":\"2190047\",\"name\":\"G. Leifman\"},{\"authorId\":\"40635259\",\"name\":\"D. Rudoy\"},{\"authorId\":\"2161276\",\"name\":\"Tristan Swedish\"},{\"authorId\":\"1398734187\",\"name\":\"E. Bayro-Corrochano\"},{\"authorId\":\"145711633\",\"name\":\"R. Raskar\"}],\"doi\":\"10.1109/ICCV.2017.188\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca1ff286ad8eba2aa6bc442f0f8321572fd0b090\",\"title\":\"Learning Gaze Transitions from Depth to Improve Video Saliency Estimation\",\"url\":\"https://www.semanticscholar.org/paper/ca1ff286ad8eba2aa6bc442f0f8321572fd0b090\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1812.00722\",\"authors\":[{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/CVPRW.2019.00109\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e330af2d061ade245622b9445d5be9717f66b60f\",\"title\":\"SUSiNet: See, Understand and Summarize It\",\"url\":\"https://www.semanticscholar.org/paper/e330af2d061ade245622b9445d5be9717f66b60f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":\"1611.09571\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"2275344\",\"name\":\"G. Serra\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/TIP.2018.2851672\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b7e336a7d3cd82ae8cd5e85d3cb62f0b6091a0e5\",\"title\":\"Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model\",\"url\":\"https://www.semanticscholar.org/paper/b7e336a7d3cd82ae8cd5e85d3cb62f0b6091a0e5\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50814507\",\"name\":\"Cagdas Bak\"},{\"authorId\":\"3044594\",\"name\":\"Aysun Kocak\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"}],\"doi\":\"10.1109/TMM.2017.2777665\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ebd1083ad9e10ed2feded8319c5d472bf9f420b\",\"title\":\"Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction\",\"url\":\"https://www.semanticscholar.org/paper/1ebd1083ad9e10ed2feded8319c5d472bf9f420b\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":\"1803.03849\",\"authors\":[{\"authorId\":\"40895287\",\"name\":\"Arda Senocak\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"3053231\",\"name\":\"J. Kim\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"}],\"doi\":\"10.1109/CVPR.2018.00458\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b91d738cd1f5d550c5b27f328e55308a0a73b2d2\",\"title\":\"Learning to Localize Sound Source in Visual Scenes\",\"url\":\"https://www.semanticscholar.org/paper/b91d738cd1f5d550c5b27f328e55308a0a73b2d2\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1807.00230\",\"authors\":[{\"authorId\":\"3443095\",\"name\":\"Bruno Korbar\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2e057a9b195f0ea2d1c5a1e88ff9606f9b67ef8b\",\"title\":\"Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization\",\"url\":\"https://www.semanticscholar.org/paper/2e057a9b195f0ea2d1c5a1e88ff9606f9b67ef8b\",\"venue\":\"NeurIPS\",\"year\":2018}],\"title\":\"STAViS: Spatio-Temporal AudioVisual Saliency Network\",\"topics\":[{\"topic\":\"Database\",\"topicId\":\"1307\",\"url\":\"https://www.semanticscholar.org/topic/1307\"},{\"topic\":\"Eye tracking\",\"topicId\":\"7621\",\"url\":\"https://www.semanticscholar.org/topic/7621\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Norm (social)\",\"topicId\":\"76329\",\"url\":\"https://www.semanticscholar.org/topic/76329\"}],\"url\":\"https://www.semanticscholar.org/paper/e296a1f529d9caa125e8c6a56cac61423e04b41b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"