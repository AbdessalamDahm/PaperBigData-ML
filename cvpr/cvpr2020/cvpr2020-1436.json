"{\"abstract\":\"Vision is often used as a complementary modality for audio speech recognition (ASR), especially in the noisy environment where performance of solo audio modality significantly deteriorates. After combining visual modality, ASR is upgraded to the multi-modality speech recognition (MSR). In this paper, we propose a two-stage speech recognition model. In the first stage, the target voice is separated from background noises with help from the corresponding visual information of lip movements, making the model \\u2018listen' clearly. At the second stage, the audio modality combines visual modality again to better understand the speech by a MSR sub-network, further improving the recognition rate. There are some other key contributions: we introduce a pseudo-3D residual convolution (P3D)-based visual front-end to extract more discriminative features; we upgrade the temporal convolution block from 1D ResNet with the temporal convolutional network (TCN), which is more suitable for the temporal tasks; the MSR sub-network is built on the top of Element-wise-Attention Gated Recurrent Unit (EleAtt-GRU), which is more effective than Transformer in long sequences. We conducted extensive experiments on the LRS3-TED and the LRW datasets. Our two-stage model (audio enhanced multi-modality speech recognition, AE-MSR) consistently achieves the state-of-the-art performance by a significant margin, which demonstrates the necessity and effectiveness of AE-MSR.\",\"arxivId\":\"2005.05592\",\"authors\":[{\"authorId\":\"145764891\",\"name\":\"Bo Xu\",\"url\":\"https://www.semanticscholar.org/author/145764891\"},{\"authorId\":\"102517285\",\"name\":\"Cheng Lu\",\"url\":\"https://www.semanticscholar.org/author/102517285\"},{\"authorId\":\"49813886\",\"name\":\"Yandong Guo\",\"url\":\"https://www.semanticscholar.org/author/49813886\"},{\"authorId\":\"2044516\",\"name\":\"J. Wang\",\"url\":\"https://www.semanticscholar.org/author/2044516\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.14360\",\"authors\":[{\"authorId\":\"1652999406\",\"name\":\"Hang Chen\"},{\"authorId\":\"1515709515\",\"name\":\"Jun Du\"},{\"authorId\":\"1943030\",\"name\":\"Y. Hu\"},{\"authorId\":\"153634883\",\"name\":\"Li-Rong Dai\"},{\"authorId\":\"9391905\",\"name\":\"Chin-Hui Lee\"},{\"authorId\":\"2207938\",\"name\":\"B. Yin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"368b02bb9b7ed2420ce3ade6961d4ba84e03af67\",\"title\":\"Lip-reading with Hierarchical Pyramidal Convolution and Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/368b02bb9b7ed2420ce3ade6961d4ba84e03af67\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.14233\",\"authors\":[{\"authorId\":\"1384480816\",\"name\":\"Pingchuan Ma\"},{\"authorId\":null,\"name\":\"Yujiang Wang\"},{\"authorId\":\"145218422\",\"name\":\"J. Shen\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5070c4871691eb53267786c512e84f3c27b354c5\",\"title\":\"Lip-reading with Densely Connected Temporal Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/5070c4871691eb53267786c512e84f3c27b354c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.03708\",\"authors\":[{\"authorId\":null,\"name\":\"Yujiang Wang\"},{\"authorId\":\"2490055\",\"name\":\"M. Dong\"},{\"authorId\":\"144081946\",\"name\":\"Jie Shen\"},{\"authorId\":\"49416123\",\"name\":\"Yi-Ming Lin\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1cc0962a3c7fbdd8291c8c2bbdfc3ea7068fcc10\",\"title\":\"Dilated Convolutions with Lateral Inhibitions for Semantic Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/1cc0962a3c7fbdd8291c8c2bbdfc3ea7068fcc10\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":218595849,\"doi\":\"10.1109/CVPR42600.2020.01444\",\"fieldsOfStudy\":[\"Computer Science\",\"Engineering\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"1a75b57ecbc140553b4dc2ce2337ed78786da338\",\"references\":[{\"arxivId\":\"1703.04105\",\"authors\":[{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"}],\"doi\":\"10.21437/INTERSPEECH.2017-85\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4afdb836301c0233bb8cf0d8a33212ac0c113381\",\"title\":\"Combining Residual Networks with LSTMs for Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/4afdb836301c0233bb8cf0d8a33212ac0c113381\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54184-6_6\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"title\":\"Lip Reading in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/74f1c93dd3a8c3f9fa59fadef9a744234b2977eb\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144333684\",\"name\":\"William Chan\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"}],\"doi\":\"10.1109/ICASSP.2016.7472621\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3056add22b20e3361c38c0472d294a79d4031cb4\",\"title\":\"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/3056add22b20e3361c38c0472d294a79d4031cb4\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016},{\"arxivId\":\"1708.01204\",\"authors\":[{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICCVW.2017.61\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b598c73e9335277106fcb8acdad6cda227c6cdf\",\"title\":\"Improved Speech Reconstruction from Silent Video\",\"url\":\"https://www.semanticscholar.org/paper/2b598c73e9335277106fcb8acdad6cda227c6cdf\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dorothy G Clegg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The listening eye: a simple introduction to the art of lip-reading\",\"url\":\"\",\"venue\":\"Methuen & Company,\",\"year\":1953},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37800738\",\"name\":\"K. Kumar\"},{\"authorId\":\"40894914\",\"name\":\"T. Chen\"},{\"authorId\":\"1697819\",\"name\":\"R. Stern\"}],\"doi\":\"10.1109/ICASSP.2007.366941\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b44ce1bd48d615f77c6eae09aba017fa2facef4c\",\"title\":\"Profile View Lip Reading\",\"url\":\"https://www.semanticscholar.org/paper/b44ce1bd48d615f77c6eae09aba017fa2facef4c\",\"venue\":\"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07\",\"year\":2007},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"2077454\",\"name\":\"A. Ephrat\"},{\"authorId\":\"3203099\",\"name\":\"Tavi Halperin\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICASSP.2018.8462527\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e246fd8721f5718981d735c328d74e50afc0e9d0\",\"title\":\"Seeing Through Noise: Visually Driven Speaker Separation And Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/e246fd8721f5718981d735c328d74e50afc0e9d0\",\"venue\":\"ICASSP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40510875\",\"name\":\"M. F. Woodward\"},{\"authorId\":\"67341808\",\"name\":\"C. G. Barber\"}],\"doi\":\"10.1044/JSHR.0303.212\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc757d0816adcce7cd649e772779f38eecf7fa27\",\"title\":\"Phoneme perception in lipreading.\",\"url\":\"https://www.semanticscholar.org/paper/cc757d0816adcce7cd649e772779f38eecf7fa27\",\"venue\":\"Journal of speech and hearing research\",\"year\":1960},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2920582\",\"name\":\"E. Petajan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ffe618d38e8ee1735aecf2f3c57a4cdef8118157\",\"title\":\"Automatic lipreading to enhance speech recognition (speech reading)\",\"url\":\"https://www.semanticscholar.org/paper/ffe618d38e8ee1735aecf2f3c57a4cdef8118157\",\"venue\":\"\",\"year\":1984},{\"arxivId\":\"1809.00496\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a46174aa635759070984ed7062c9402695bce830\",\"title\":\"LRS3-TED: a large-scale dataset for visual speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/a46174aa635759070984ed7062c9402695bce830\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrzej Czyzewski\"},{\"authorId\":null,\"name\":\"Bozena Kostek\"},{\"authorId\":null,\"name\":\"Piotr Bratoszewski\"},{\"authorId\":null,\"name\":\"Jozef Kotus\"},{\"authorId\":null,\"name\":\"Marcin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Szykulski. An audio-visual corpus for multimodal automatic speech recognition\",\"url\":\"\",\"venue\":\"Journal of Intelligent Information Systems,\",\"year\":2017},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1907.04975\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/interspeech.2019-3114\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7473594725d668a4cb174a86dbcba7c2148a598e\",\"title\":\"My lips are concealed: Audio-visual speech enhancement through obstructions\",\"url\":\"https://www.semanticscholar.org/paper/7473594725d668a4cb174a86dbcba7c2148a598e\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"143913738\",\"name\":\"S. Fern\\u00e1ndez\"},{\"authorId\":\"145842938\",\"name\":\"F. Gomez\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1145/1143844.1143891\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"96494e722f58705fa20302fe6179d483f52705b4\",\"title\":\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/96494e722f58705fa20302fe6179d483f52705b4\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":\"1409.4842\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"46641766\",\"name\":\"W. Liu\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"48840704\",\"name\":\"Scott Reed\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"}],\"doi\":\"10.1109/CVPR.2015.7298594\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"title\":\"Going deeper with convolutions\",\"url\":\"https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8726117\",\"name\":\"Jen-Cheng Hou\"},{\"authorId\":\"2426246\",\"name\":\"S. Wang\"},{\"authorId\":\"145274549\",\"name\":\"Ying-Hui Lai\"},{\"authorId\":\"145403933\",\"name\":\"Y. Tsao\"},{\"authorId\":\"144600099\",\"name\":\"Hsiu-Wen Chang\"},{\"authorId\":\"1710199\",\"name\":\"H. Wang\"}],\"doi\":\"10.1109/TETCI.2017.2784878\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ddf1461979a5e39321b931cfe5b470999b5e4aab\",\"title\":\"Audio-Visual Speech Enhancement Using Multimodal Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/ddf1461979a5e39321b931cfe5b470999b5e4aab\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2018},{\"arxivId\":\"1802.06424\",\"authors\":[{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"1799540\",\"name\":\"Themos Stafylakis\"},{\"authorId\":\"144933397\",\"name\":\"Pingchuan Ma\"},{\"authorId\":\"7943876\",\"name\":\"Feipeng Cai\"},{\"authorId\":\"2610880\",\"name\":\"Georgios Tzimiropoulos\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1109/ICASSP.2018.8461326\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43cf4abfe1bc6e38b845740f7ee00715ce9d5a39\",\"title\":\"End-to-End Audiovisual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/43cf4abfe1bc6e38b845740f7ee00715ce9d5a39\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"104117062\",\"name\":\"Ziheng Zhou\"},{\"authorId\":\"83433495\",\"name\":\"G. Zhao\"},{\"authorId\":\"46761465\",\"name\":\"Xiaopeng Hong\"},{\"authorId\":\"145962204\",\"name\":\"M. Pietik\\u00e4inen\"}],\"doi\":\"10.1016/j.imavis.2014.06.004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c842fbd4c032dd4d931eb6ff1eaa2a13450b7af\",\"title\":\"A review of recent advances in visual speech decoding\",\"url\":\"https://www.semanticscholar.org/paper/4c842fbd4c032dd4d931eb6ff1eaa2a13450b7af\",\"venue\":\"Image Vis. Comput.\",\"year\":2014},{\"arxivId\":\"1711.10305\",\"authors\":[{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/ICCV.2017.590\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"024d037d46ae933c7e12fd16af61953c7161773a\",\"title\":\"Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks\",\"url\":\"https://www.semanticscholar.org/paper/024d037d46ae933c7e12fd16af61953c7161773a\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1713014\",\"name\":\"A. Czyzewski\"},{\"authorId\":\"1796260\",\"name\":\"B. Kostek\"},{\"authorId\":\"2972926\",\"name\":\"P. Bratoszewski\"},{\"authorId\":\"144030731\",\"name\":\"J. Kotus\"},{\"authorId\":\"1678848\",\"name\":\"Marcin S. Szczuka\"}],\"doi\":\"10.1007/s10844-016-0438-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd54941cc46656005d31b1b24e3a002a7acd5b3f\",\"title\":\"An audio-visual corpus for multimodal automatic speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/cd54941cc46656005d31b1b24e3a002a7acd5b3f\",\"venue\":\"Journal of Intelligent Information Systems\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144598072\",\"name\":\"D. King\"}],\"doi\":\"10.1145/1577069.1755843\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"title\":\"Dlib-ml: A Machine Learning Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/2ea6a93199c9227fa0c1c7de13725f918c9be3a4\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1712.01769\",\"authors\":[{\"authorId\":\"145039780\",\"name\":\"Chung-Cheng Chiu\"},{\"authorId\":\"1784851\",\"name\":\"T. Sainath\"},{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"},{\"authorId\":\"2557391\",\"name\":\"Rohit Prabhavalkar\"},{\"authorId\":\"40133958\",\"name\":\"Patrick Nguyen\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"31801501\",\"name\":\"A. Kannan\"},{\"authorId\":\"39571582\",\"name\":\"Ron J. Weiss\"},{\"authorId\":\"2251957\",\"name\":\"K. Rao\"},{\"authorId\":\"1398413062\",\"name\":\"Katya Gonina\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"},{\"authorId\":\"143771569\",\"name\":\"Bo Li\"},{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"1771090\",\"name\":\"M. Bacchiani\"}],\"doi\":\"10.1109/ICASSP.2018.8462105\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6b61535f1544835cca3851ceb34222ebc5b4377\",\"title\":\"State-of-the-Art Speech Recognition with Sequence-to-Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/c6b61535f1544835cca3851ceb34222ebc5b4377\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1602.07868\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d2c6941a9b4608ba52b328369a3352db2092ae0\",\"title\":\"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/3d2c6941a9b4608ba52b328369a3352db2092ae0\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1909.01939\",\"authors\":[{\"authorId\":\"48754312\",\"name\":\"Pengfei Zhang\"},{\"authorId\":\"3280033\",\"name\":\"J. Xue\"},{\"authorId\":\"40093162\",\"name\":\"Cuiling Lan\"},{\"authorId\":\"144864745\",\"name\":\"Wenjun Zeng\"},{\"authorId\":\"2687716\",\"name\":\"Zhanning Gao\"},{\"authorId\":\"145608731\",\"name\":\"N. Zheng\"}],\"doi\":\"10.1109/TIP.2019.2937724\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1d0b8606483d6d9c1cdb5b8953bf61773bf633db\",\"title\":\"EleAtt-RNN: Adding Attentiveness to Neurons in Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/1d0b8606483d6d9c1cdb5b8953bf61773bf633db\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1711.08789\",\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"153677544\",\"name\":\"Asaph Shamir\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f04d8a78cd33ac944353c55d3d40230ef504f449\",\"title\":\"Visual Speech Enhancement using Noise-Invariant Training\",\"url\":\"https://www.semanticscholar.org/paper/f04d8a78cd33ac944353c55d3d40230ef504f449\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2054241\",\"name\":\"H. McGurk\"},{\"authorId\":\"143965550\",\"name\":\"J. Macdonald\"}],\"doi\":\"10.1038/264746A0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eef41ae597a20ea377461d522fd5100da6a7a9b7\",\"title\":\"Hearing lips and seeing voices\",\"url\":\"https://www.semanticscholar.org/paper/eef41ae597a20ea377461d522fd5100da6a7a9b7\",\"venue\":\"Nature\",\"year\":1976},{\"arxivId\":\"1803.01271\",\"authors\":[{\"authorId\":\"35836381\",\"name\":\"Shaojie Bai\"},{\"authorId\":\"145116464\",\"name\":\"J. Z. Kolter\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"921196c32213a229245a9705ee4768bc941e7a26\",\"title\":\"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\",\"url\":\"https://www.semanticscholar.org/paper/921196c32213a229245a9705ee4768bc941e7a26\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-319-54427-4_19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"87defac1045bfa9af0162cd248d193e9be6eb25b\",\"title\":\"Out of Time: Automated Lip Sync in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/87defac1045bfa9af0162cd248d193e9be6eb25b\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":\"1810.06990\",\"authors\":[{\"authorId\":\"145071104\",\"name\":\"Shuang Yang\"},{\"authorId\":\"2631335\",\"name\":\"Y. Zhang\"},{\"authorId\":\"80007827\",\"name\":\"Dalu Feng\"},{\"authorId\":\"1943661\",\"name\":\"Mingmin Yang\"},{\"authorId\":null,\"name\":\"Chenhao Wang\"},{\"authorId\":\"31227090\",\"name\":\"Jingyun Xiao\"},{\"authorId\":\"2566176\",\"name\":\"K. Long\"},{\"authorId\":\"145455919\",\"name\":\"S. Shan\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1109/FG.2019.8756582\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"60fa866c24c78b3f022651370cc194f3865560d0\",\"title\":\"LRW-1000: A Naturally-Distributed Large-Scale Benchmark for Lip Reading in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/60fa866c24c78b3f022651370cc194f3865560d0\",\"venue\":\"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)\",\"year\":2019},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1809.02108\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/TPAMI.2018.2889052\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fcecc4ef2c32dbedda61648febb39a0f905c367e\",\"title\":\"Deep Audio-Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fcecc4ef2c32dbedda61648febb39a0f905c367e\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3033647\",\"name\":\"R. D. Easton\"},{\"authorId\":\"49765246\",\"name\":\"M. Basala\"}],\"doi\":\"10.3758/BF03204211\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e50d8b9aa31cd6f971151b3e1ddb332da3191f35\",\"title\":\"Perceptual dominance during lipreading\",\"url\":\"https://www.semanticscholar.org/paper/e50d8b9aa31cd6f971151b3e1ddb332da3191f35\",\"venue\":\"Perception & psychophysics\",\"year\":1982},{\"arxivId\":\"1603.04467\",\"authors\":[{\"authorId\":\"145832079\",\"name\":\"M. Abadi\"},{\"authorId\":\"145984138\",\"name\":\"A. Agarwal\"},{\"authorId\":\"144758007\",\"name\":\"P. Barham\"},{\"authorId\":\"2445241\",\"name\":\"E. Brevdo\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"48738717\",\"name\":\"Craig Citro\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"36347083\",\"name\":\"Andy Davis\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"145139947\",\"name\":\"M. Devin\"},{\"authorId\":\"1780892\",\"name\":\"Sanjay Ghemawat\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"3384453\",\"name\":\"A. Harp\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"2090818\",\"name\":\"M. Isard\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1944541\",\"name\":\"R. J\\u00f3zefowicz\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1942300\",\"name\":\"M. Kudlur\"},{\"authorId\":\"3369421\",\"name\":\"Josh Levenberg\"},{\"authorId\":\"143767989\",\"name\":\"Dan Man\\u00e9\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"144375552\",\"name\":\"Sherry Moore\"},{\"authorId\":\"20154699\",\"name\":\"D. Murray\"},{\"authorId\":\"153301219\",\"name\":\"Chris Olah\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"32163737\",\"name\":\"B. Steiner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"35210462\",\"name\":\"Kunal Talwar\"},{\"authorId\":\"2080690\",\"name\":\"P. Tucker\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"38062095\",\"name\":\"V. Vasudevan\"},{\"authorId\":\"1765169\",\"name\":\"F. Vi\\u00e9gas\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"47941411\",\"name\":\"Pete Warden\"},{\"authorId\":\"145233583\",\"name\":\"M. Wattenberg\"},{\"authorId\":\"35078078\",\"name\":\"Martin Wicke\"},{\"authorId\":\"47112093\",\"name\":\"Y. Yu\"},{\"authorId\":\"2777763\",\"name\":\"X. Zheng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"title\":\"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\",\"url\":\"https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Harry McGurk\"},{\"authorId\":null,\"name\":\"John MacDonald\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hearing lips and seeing\",\"url\":\"\",\"venue\":\"voices. Nature,\",\"year\":1976},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145557639\",\"name\":\"V. Nair\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a538b05ebb01a40323997629e171c91aa28b8e2f\",\"title\":\"Rectified Linear Units Improve Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":\"1611.05358\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"33666044\",\"name\":\"A. Senior\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.367\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"title\":\"Lip Reading Sentences in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/bed6d0097df1e9ac82f789f6da268cdb3dd65bc3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27485318\",\"name\":\"Aviv Gabbay\"},{\"authorId\":\"153677544\",\"name\":\"Asaph Shamir\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.21437/Interspeech.2018-1955\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f997d69d78af086dec4462e4319c6d241f42c0c1\",\"title\":\"Visual Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/f997d69d78af086dec4462e4319c6d241f42c0c1\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"1610.02357\",\"authors\":[{\"authorId\":\"47924294\",\"name\":\"F. Chollet\"}],\"doi\":\"10.1109/CVPR.2017.195\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5b6ec746d309b165f9f9def873a2375b6fb40f3d\",\"title\":\"Xception: Deep Learning with Depthwise Separable Convolutions\",\"url\":\"https://www.semanticscholar.org/paper/5b6ec746d309b165f9f9def873a2375b6fb40f3d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":\"1806.06053\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2018-1943\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"04187519dc8c468f2b5b17442413ada7830068e5\",\"title\":\"Deep Lip Reading: a comparison of models and an online application\",\"url\":\"https://www.semanticscholar.org/paper/04187519dc8c468f2b5b17442413ada7830068e5\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"1804.04121\",\"authors\":[{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2018-1400\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3\",\"title\":\"The Conversation: Deep Audio-Visual Speech Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/e3cac1f3fa0ca9ba41f1cb0fbbd28a0f320903e3\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"1611.01599\",\"authors\":[{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"291c0e453503a704c0fd932a067ca054cc7edad6\",\"title\":\"LipNet: End-to-End Sentence-level Lipreading\",\"url\":\"https://www.semanticscholar.org/paper/291c0e453503a704c0fd932a067ca054cc7edad6\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143967982\",\"name\":\"M. Cooke\"},{\"authorId\":\"32406400\",\"name\":\"J. Barker\"},{\"authorId\":\"75117630\",\"name\":\"S. Cunningham\"},{\"authorId\":\"48914950\",\"name\":\"X. Shao\"}],\"doi\":\"10.1121/1.2229005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"title\":\"An audio-visual corpus for speech perception and automatic speech recognition.\",\"url\":\"https://www.semanticscholar.org/paper/5129350ec0bd8f1fe78a9b864865709f8d8de058\",\"venue\":\"The Journal of the Acoustical Society of America\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f\",\"title\":\"Towards End-To-End Speech Recognition with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48359098\",\"name\":\"C. Fisher\"}],\"doi\":\"10.1044/JSHR.1104.796\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0774057249f18840d000397e3510408e8659b0a8\",\"title\":\"Confusions among visually perceived consonants.\",\"url\":\"https://www.semanticscholar.org/paper/0774057249f18840d000397e3510408e8659b0a8\",\"venue\":\"Journal of speech and hearing research\",\"year\":1968}],\"title\":\"Discriminative Multi-Modality Speech Recognition\",\"topics\":[{\"topic\":\"Speech recognition\",\"topicId\":\"2869\",\"url\":\"https://www.semanticscholar.org/topic/2869\"},{\"topic\":\"Modality (human\\u2013computer interaction)\",\"topicId\":\"462\",\"url\":\"https://www.semanticscholar.org/topic/462\"},{\"topic\":\"Convolution\",\"topicId\":\"571\",\"url\":\"https://www.semanticscholar.org/topic/571\"},{\"topic\":\"Subnetwork\",\"topicId\":\"36979\",\"url\":\"https://www.semanticscholar.org/topic/36979\"},{\"topic\":\"Region of interest\",\"topicId\":\"32373\",\"url\":\"https://www.semanticscholar.org/topic/32373\"},{\"topic\":\"Activation function\",\"topicId\":\"84273\",\"url\":\"https://www.semanticscholar.org/topic/84273\"},{\"topic\":\"Dlib\",\"topicId\":\"1361938\",\"url\":\"https://www.semanticscholar.org/topic/1361938\"},{\"topic\":\"Sigmoid function\",\"topicId\":\"106754\",\"url\":\"https://www.semanticscholar.org/topic/106754\"},{\"topic\":\"2.5D\",\"topicId\":\"33594\",\"url\":\"https://www.semanticscholar.org/topic/33594\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Transformer\",\"topicId\":\"6977\",\"url\":\"https://www.semanticscholar.org/topic/6977\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"},{\"topic\":\"IBM Personal Computer XT\",\"topicId\":\"270844\",\"url\":\"https://www.semanticscholar.org/topic/270844\"},{\"topic\":\"Pixel\",\"topicId\":\"4254\",\"url\":\"https://www.semanticscholar.org/topic/4254\"},{\"topic\":\"Structure of observed learning outcome\",\"topicId\":\"376290\",\"url\":\"https://www.semanticscholar.org/topic/376290\"}],\"url\":\"https://www.semanticscholar.org/paper/1a75b57ecbc140553b4dc2ce2337ed78786da338\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020}\n"