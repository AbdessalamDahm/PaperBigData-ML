"{\"abstract\":\"The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\",\"url\":\"https://www.semanticscholar.org/author/2363529\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\",\"url\":\"https://www.semanticscholar.org/author/2026123\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\",\"url\":\"https://www.semanticscholar.org/author/38972663\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\",\"url\":\"https://www.semanticscholar.org/author/3587688\"}],\"citationVelocity\":23,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1007/978-3-319-16865-4_40\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c50caaa3983b4377f8bbf572e1c13fb49b3d601\",\"title\":\"Leveraging High Level Visual Information for Matching Images and Captions\",\"url\":\"https://www.semanticscholar.org/paper/5c50caaa3983b4377f8bbf572e1c13fb49b3d601\",\"venue\":\"ACCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48773674\",\"name\":\"Lydia Weiland\"},{\"authorId\":\"145798497\",\"name\":\"Laura Dietz\"},{\"authorId\":\"1801255\",\"name\":\"Simone Paolo Ponzetto\"}],\"doi\":\"10.18653/v1/W15-2808\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40ae9a83086cb198296ba0f47646f8cf8e676a7a\",\"title\":\"Image with a Message: Towards Detecting Non-Literal Image Usages by Visual Linking\",\"url\":\"https://www.semanticscholar.org/paper/40ae9a83086cb198296ba0f47646f8cf8e676a7a\",\"venue\":\"VL@EMNLP\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3268248\",\"name\":\"H. Zhang\"},{\"authorId\":\"9370254\",\"name\":\"P. Lai\"},{\"authorId\":\"21409233\",\"name\":\"S. Paul\"},{\"authorId\":\"9745898\",\"name\":\"S. Kothawade\"},{\"authorId\":\"37586303\",\"name\":\"S. Nikolaidis\"},{\"authorId\":\"3268248\",\"name\":\"H. Zhang\"},{\"authorId\":\"9370254\",\"name\":\"P. Lai\"},{\"authorId\":\"21409233\",\"name\":\"S. Paul\"},{\"authorId\":\"9745898\",\"name\":\"S. Kothawade\"},{\"authorId\":\"37586303\",\"name\":\"S. Nikolaidis\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d4f333f127a8fd12cbb8fb13ab7c390f9a50a278\",\"title\":\"Learning Collaborative Action Plans from YouTube Videos\",\"url\":\"https://www.semanticscholar.org/paper/d4f333f127a8fd12cbb8fb13ab7c390f9a50a278\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"104510214\",\"name\":\"K. Li\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TETCI.2019.2892755\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"title\":\"Visual to Text: Survey of Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.24963/ijcai.2019/877\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"title\":\"Deep Learning for Video Captioning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1807.00858\",\"authors\":[{\"authorId\":\"3112203\",\"name\":\"Yongqiang Huang\"},{\"authorId\":\"144825829\",\"name\":\"Y. Sun\"}],\"doi\":\"10.1177/0278364919849091\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e19e455327176d02f9c82e43bd9638d97a9908d\",\"title\":\"A dataset of daily interactive manipulation\",\"url\":\"https://www.semanticscholar.org/paper/2e19e455327176d02f9c82e43bd9638d97a9908d\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/3122865\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"title\":\"Frontiers of Multimedia Research\",\"url\":\"https://www.semanticscholar.org/paper/8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"47001807\",\"name\":\"Y. Li\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2b593ec292395182f5f6556d47bf3d38ec7e191\",\"title\":\"Robot Learning Manipulation Action Plans by \\\"Watching\\\" Unconstrained Videos from the World Wide Web\",\"url\":\"https://www.semanticscholar.org/paper/e2b593ec292395182f5f6556d47bf3d38ec7e191\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":\"1712.00796\",\"authors\":[{\"authorId\":\"7311172\",\"name\":\"Giorgos Bouritsas\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"2641229\",\"name\":\"A. Zlatintsi\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/CVPR.2018.00516\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e1379e2f0509af074808c1e464ef78fb6abd5ba\",\"title\":\"Multimodal Visual Concept Learning with Weakly Supervised Techniques\",\"url\":\"https://www.semanticscholar.org/paper/9e1379e2f0509af074808c1e464ef78fb6abd5ba\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1792872\",\"name\":\"Reza Bahmanyar\"},{\"authorId\":\"1403914698\",\"name\":\"D. Espinoza-Molina\"},{\"authorId\":\"1777167\",\"name\":\"M. Datcu\"}],\"doi\":\"10.1109/LGRS.2018.2794511\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"09baf165d71a263e1273a64d061c8f420be3734c\",\"title\":\"Multisensor Earth Observation Image Classification Based on a Multimodal Latent Dirichlet Allocation Model\",\"url\":\"https://www.semanticscholar.org/paper/09baf165d71a263e1273a64d061c8f420be3734c\",\"venue\":\"IEEE Geoscience and Remote Sensing Letters\",\"year\":2018},{\"arxivId\":\"1806.11538\",\"authors\":[{\"authorId\":\"2180892\",\"name\":\"Yikang Li\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"9349527\",\"name\":\"Yawen Cui\"},{\"authorId\":\"46865320\",\"name\":\"Jianping Shi\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-01246-5_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"acfe5b5c99be70fa3120d410e7be55b9fe299f40\",\"title\":\"Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/acfe5b5c99be70fa3120d410e7be55b9fe299f40\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48773674\",\"name\":\"Lydia Weiland\"},{\"authorId\":\"2680937\",\"name\":\"Ioana Hulpus\"},{\"authorId\":\"1801255\",\"name\":\"Simone Paolo Ponzetto\"},{\"authorId\":\"145798497\",\"name\":\"Laura Dietz\"}],\"doi\":\"10.1007/978-3-319-51814-5_34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ffa8e426856b312e38bd1de223ed9fad0be77630\",\"title\":\"Using Object Detection, NLP, and Knowledge Bases to Understand the Message of Images\",\"url\":\"https://www.semanticscholar.org/paper/ffa8e426856b312e38bd1de223ed9fad0be77630\",\"venue\":\"MMM\",\"year\":2017},{\"arxivId\":\"1906.02549\",\"authors\":[{\"authorId\":\"26907392\",\"name\":\"Zhenfang Chen\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"145909988\",\"name\":\"Wenhan Luo\"},{\"authorId\":\"1698116\",\"name\":\"K. Wong\"}],\"doi\":\"10.18653/v1/P19-1183\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7e158a7619c6d0d583a8bbdf00658b7d4bbe0374\",\"title\":\"Weakly-Supervised Spatio-Temporally Grounding Natural Sentence in Video\",\"url\":\"https://www.semanticscholar.org/paper/7e158a7619c6d0d583a8bbdf00658b7d4bbe0374\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"},{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"}],\"doi\":\"10.21437/Interspeech.2016-380\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"title\":\"Generating Natural Video Descriptions via Multimodal Processing\",\"url\":\"https://www.semanticscholar.org/paper/2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"venue\":\"INTERSPEECH\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46844237\",\"name\":\"Yongqiang Huang\"},{\"authorId\":\"34882711\",\"name\":\"M. Bianchi\"},{\"authorId\":\"2646612\",\"name\":\"M. Liarokapis\"},{\"authorId\":\"91252313\",\"name\":\"Y. Sun\"}],\"doi\":\"10.1089/BIG.2016.0042\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9e8b055e5d18e61d928839bd02d95d39e7ed3361\",\"title\":\"Recent Data Sets on Object Manipulation: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/9e8b055e5d18e61d928839bd02d95d39e7ed3361\",\"venue\":\"Big Data\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26548883\",\"name\":\"RaviKiran Krishnan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"07f8c5559a5aacfc37d082b1922f38d9f88cff67\",\"title\":\"Generalized Conditional Matching Algorithm for Ordered and Unordered Sets\",\"url\":\"https://www.semanticscholar.org/paper/07f8c5559a5aacfc37d082b1922f38d9f88cff67\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1502.06648\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-015-0851-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0138ef03aa5cf7bf7d0ce70b3f98876af022ffcd\",\"title\":\"Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data\",\"url\":\"https://www.semanticscholar.org/paper/0138ef03aa5cf7bf7d0ce70b3f98876af022ffcd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"40508553\",\"name\":\"Y. Yu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/2733373.2806218\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3293f8db118f58f05efb5f56e7b694005e7cb526\",\"title\":\"Searching Persuasively: Joint Event Detection and Evidence Recounting with Limited Supervision\",\"url\":\"https://www.semanticscholar.org/paper/3293f8db118f58f05efb5f56e7b694005e7cb526\",\"venue\":\"ACM Multimedia\",\"year\":2015},{\"arxivId\":\"1506.02059\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f1d775083122bb3efa4ea9d235a34276644bfee7\",\"title\":\"Sentence Directed Video Object Codetection\",\"url\":\"https://www.semanticscholar.org/paper/f1d775083122bb3efa4ea9d235a34276644bfee7\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7853493\",\"name\":\"Atiqah Izzati Masrani\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a686af9cfe287e9a5bad1359d4d2498debd6f7cc\",\"title\":\"Corpus Generation and Analysis: Incorporating Audio Data Towards Curbing Missing Information\",\"url\":\"https://www.semanticscholar.org/paper/a686af9cfe287e9a5bad1359d4d2498debd6f7cc\",\"venue\":\"KDWeb\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/WACV.2019.00026\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"183bf77d4f9b4eb227ba1d5a26eff5b6ab3d889d\",\"title\":\"Going Deeper With Semantics: Video Activity Interpretation Using Semantic Contextualization\",\"url\":\"https://www.semanticscholar.org/paper/183bf77d4f9b4eb227ba1d5a26eff5b6ab3d889d\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":\"1808.02280\",\"authors\":[{\"authorId\":\"2964003\",\"name\":\"C. Lee\"},{\"authorId\":\"1947838\",\"name\":\"Shang-Ming Wang\"},{\"authorId\":\"39908989\",\"name\":\"H. Chang\"},{\"authorId\":\"144300094\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/SLT.2018.8639505\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de91454ff9fc82c68359360b661c25f52dd2c0cb\",\"title\":\"ODSQA: Open-Domain Spoken Question Answering Dataset\",\"url\":\"https://www.semanticscholar.org/paper/de91454ff9fc82c68359360b661c25f52dd2c0cb\",\"venue\":\"2018 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-54407-6_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"title\":\"Video Captioning via Sentence Augmentation and Spatio-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"144332826\",\"name\":\"Chen Kong\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":\"10.5244/C.29.93\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"63de6bb482d07f9e81ecbe9acfb62433b4ce6fe6\",\"title\":\"Generating Multi-sentence Natural Language Descriptions of Indoor Scenes\",\"url\":\"https://www.semanticscholar.org/paper/63de6bb482d07f9e81ecbe9acfb62433b4ce6fe6\",\"venue\":\"BMVC\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1828838\",\"name\":\"Sravanthi Bondugula\"}],\"doi\":\"10.13016/M2V504\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fabbc7f921d77b5aa9157310df29ad81367fe92d\",\"title\":\"Efficient Image and Video Representations for Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/fabbc7f921d77b5aa9157310df29ad81367fe92d\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1802.09745\",\"authors\":[{\"authorId\":null,\"name\":\"Xin Li\"},{\"authorId\":\"144811861\",\"name\":\"M. Chuah\"}],\"doi\":\"10.1109/WACV.2018.00046\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"162148b5dcabd9354a722ad75e5390596f3ae4c3\",\"title\":\"ReHAR: Robust and Efficient Human Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/162148b5dcabd9354a722ad75e5390596f3ae4c3\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145668114\",\"name\":\"Y. Xie\"},{\"authorId\":\"1748992\",\"name\":\"K. Eguchi\"}],\"doi\":\"10.1587/TRANSINF.E97.D.714\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a859343c4047115a653646f5ee8816c3aa232f1\",\"title\":\"Multimedia Topic Models Considering Burstiness of Local Features\",\"url\":\"https://www.semanticscholar.org/paper/7a859343c4047115a653646f5ee8816c3aa232f1\",\"venue\":\"IEICE Trans. Inf. Syst.\",\"year\":2014},{\"arxivId\":\"1412.4729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.3115/v1/N15-1173\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cef41606f1e1324b683441e694f0e1c96387abf\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/8cef41606f1e1324b683441e694f0e1c96387abf\",\"venue\":\"HLT-NAACL\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"40085065\",\"name\":\"Percy Liang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-10590-1_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"title\":\"Linking People in Videos with \\\"Their\\\" Names Using Coreference Resolution\",\"url\":\"https://www.semanticscholar.org/paper/a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"title\":\"Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"venue\":\"COLING\",\"year\":2014},{\"arxivId\":\"1410.5861\",\"authors\":[{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"48390827\",\"name\":\"G. Chen\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41aa8c1c90d74f2653ef4b3a2e02ac473af61e47\",\"title\":\"Compositional Structure Learning for Action Understanding\",\"url\":\"https://www.semanticscholar.org/paper/41aa8c1c90d74f2653ef4b3a2e02ac473af61e47\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98050388\",\"name\":\"Lintang\"},{\"authorId\":\"38495716\",\"name\":\"Lin Liu\"},{\"authorId\":\"145484940\",\"name\":\"Jianhou Gan\"}],\"doi\":\"10.1109/IHMSC.2017.179\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d217bd2e4ebdedef1899d7569ff43f31dd407e8a\",\"title\":\"Using Multi-Modal Topic Modeling in National Culture Resources: Methods and Applications\",\"url\":\"https://www.semanticscholar.org/paper/d217bd2e4ebdedef1899d7569ff43f31dd407e8a\",\"venue\":\"2017 9th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2dda7f46bbb8684a3a497dd40aabd4d759a62386\",\"title\":\"YouCookII Dataset\",\"url\":\"https://www.semanticscholar.org/paper/2dda7f46bbb8684a3a497dd40aabd4d759a62386\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2311199\",\"name\":\"Hui Li Tan\"},{\"authorId\":\"9453727\",\"name\":\"Keng-Teck Ma\"},{\"authorId\":\"7296648\",\"name\":\"Hongyuan Zhu\"},{\"authorId\":\"144416799\",\"name\":\"M. Rice\"},{\"authorId\":\"6516914\",\"name\":\"J. Lim\"},{\"authorId\":\"1694051\",\"name\":\"Cheston Tan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"962bc89565d6d7fc4dd3c294775649c490a3a7cd\",\"title\":\"A Survey of Procedural Video Datasets\",\"url\":\"https://www.semanticscholar.org/paper/962bc89565d6d7fc4dd3c294775649c490a3a7cd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1703.09788\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e10a5e0baf2aa87d804795af071808a9377cc80a\",\"title\":\"Towards Automatic Learning of Procedures From Web Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/e10a5e0baf2aa87d804795af071808a9377cc80a\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47313839\",\"name\":\"V. Krovi\"},{\"authorId\":\"51187951\",\"name\":\"Jason Corso\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d3c99ec84197320443127d2f2f2a8f42c878b310\",\"title\":\"Final Report : GBS : Guidance By Semantics-Using High-Level Visual Inference to Improve Vision-based Mobile Robot Localization Report\",\"url\":\"https://www.semanticscholar.org/paper/d3c99ec84197320443127d2f2f2a8f42c878b310\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/CVPR.2014.329\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d11189106d6383297b2a439b8c5e1b6ea19ecf5\",\"title\":\"DISCOVER: Discovering Important Segments for Classification of Video Events and Recounting\",\"url\":\"https://www.semanticscholar.org/paper/0d11189106d6383297b2a439b8c5e1b6ea19ecf5\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1007/978-3-319-10590-1_50\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"88f7a3d6f0521803ca59fde45601e94c3a34a403\",\"title\":\"Semantic Aware Video Transcription Using Random Forest Classifiers\",\"url\":\"https://www.semanticscholar.org/paper/88f7a3d6f0521803ca59fde45601e94c3a34a403\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1906.04236\",\"authors\":[{\"authorId\":\"41079205\",\"name\":\"Oana Ignat\"},{\"authorId\":\"8003011\",\"name\":\"Laura Burdick\"},{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"145557251\",\"name\":\"R. Mihalcea\"}],\"doi\":\"10.18653/v1/P19-1643\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0fe96f493334987589f8ccd55f2d1209df258449\",\"title\":\"Identifying Visible Actions in Lifestyle Vlogs\",\"url\":\"https://www.semanticscholar.org/paper/0fe96f493334987589f8ccd55f2d1209df258449\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1704.03114\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"46867115\",\"name\":\"Yuqi Zhang\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/CVPR.2017.352\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5fcd93997b7dde90594dc1caa27ba9d560bbe63d\",\"title\":\"Detecting Visual Relationships with Deep Relational Networks\",\"url\":\"https://www.semanticscholar.org/paper/5fcd93997b7dde90594dc1caa27ba9d560bbe63d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1704.01502\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"47008023\",\"name\":\"Z. Su\"},{\"authorId\":\"3700393\",\"name\":\"Minjun Li\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/CVPR.2017.548\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"title\":\"Weakly Supervised Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314993\",\"name\":\"Wei-ying Wang\"},{\"authorId\":\"1994473516\",\"name\":\"Jieting Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3394171.3413890\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b116c8cd44a34f440f260a890e0600b61d92c262\",\"title\":\"VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation\",\"url\":\"https://www.semanticscholar.org/paper/b116c8cd44a34f440f260a890e0600b61d92c262\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"}],\"doi\":\"10.25781/KAUST-VR909\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b5c549fd9172f2d1be80cc77b0de9e90d3416463\",\"title\":\"Efficient Localization of Human Actions and Moments in Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5c549fd9172f2d1be80cc77b0de9e90d3416463\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1511.05914\",\"authors\":[{\"authorId\":\"2089428\",\"name\":\"D. P. Barrett\"},{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":\"10.1007/s00138-016-0768-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6a348493e0a3bd6b35cf02de9e71da675062841d\",\"title\":\"Collecting and annotating the large continuous action dataset\",\"url\":\"https://www.semanticscholar.org/paper/6a348493e0a3bd6b35cf02de9e71da675062841d\",\"venue\":\"Machine Vision and Applications\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1145/2964284.2984066\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"61d409b92860480a9188d23ba59b822ddc6331f9\",\"title\":\"Multimodal Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61d409b92860480a9188d23ba59b822ddc6331f9\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46503814\",\"name\":\"Lukas Diem\"},{\"authorId\":\"1736319\",\"name\":\"M. Zaharieva\"}],\"doi\":\"10.1109/CBMI.2015.7153602\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fa60a0aa335793b0cf56529d5f02cc4076292576\",\"title\":\"Interpretable video representation\",\"url\":\"https://www.semanticscholar.org/paper/fa60a0aa335793b0cf56529d5f02cc4076292576\",\"venue\":\"2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)\",\"year\":2015},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21243543\",\"name\":\"Shuang Wu\"},{\"authorId\":\"1828838\",\"name\":\"Sravanthi Bondugula\"},{\"authorId\":\"1689313\",\"name\":\"Florian Luisier\"},{\"authorId\":\"2433508\",\"name\":\"Xiaodan Zhuang\"},{\"authorId\":\"49824581\",\"name\":\"P. Natarajan\"}],\"doi\":\"10.1109/CVPR.2014.341\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"16a4e39dfa15e9ae5906a521464e008a24e628b4\",\"title\":\"Zero-Shot Event Detection Using Multi-modal Fusion of Weakly Supervised Concepts\",\"url\":\"https://www.semanticscholar.org/paper/16a4e39dfa15e9ae5906a521464e008a24e628b4\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"26559284\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"title\":\"Captioning Near-Future Activity Sequences\",\"url\":\"https://www.semanticscholar.org/paper/dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2909813\",\"name\":\"C. C. Tan\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1007/s13735-015-0090-3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34c1ae7fdd00b4e46959443728e1a586bd78c74a\",\"title\":\"On the use of commonsense ontology for multimedia event recounting\",\"url\":\"https://www.semanticscholar.org/paper/34c1ae7fdd00b4e46959443728e1a586bd78c74a\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3114252\",\"name\":\"O. Sener\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"48473045\",\"name\":\"A. Saxena\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9ae19caad6a1cac362b979992e8fac77221ebf85\",\"title\":\"Unsupervised Activity Learning and Parsing Learned Action 1 : Selected Visual Atoms : Selected Language Atoms\",\"url\":\"https://www.semanticscholar.org/paper/9ae19caad6a1cac362b979992e8fac77221ebf85\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82564091\",\"name\":\"Ey\\u00fcp \\u00d6zer\"},{\"authorId\":\"1658966195\",\"name\":\"\\u0130lteber Nur Karap\\u0131nar\"},{\"authorId\":\"1658997114\",\"name\":\"Sena Ba\\u015fbu\\u011f\"},{\"authorId\":\"1657299979\",\"name\":\"S\\u00fcmeyye Turan\"},{\"authorId\":\"52219742\",\"name\":\"An\\u0131l Utku\"},{\"authorId\":\"153780569\",\"name\":\"M. Akcayol\"}],\"doi\":\"10.14569/ijacsa.2020.0110365\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f682a618e680e6c488f620a65cf5cf657fc6986b\",\"title\":\"Deep Learning based, a New Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f682a618e680e6c488f620a65cf5cf657fc6986b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/TPAMI.2014.2366143\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8403bf4e3060487cbc8acceb1fb256a4f1cfc76\",\"title\":\"Adopting Abstract Images for Semantic Scene Understanding\",\"url\":\"https://www.semanticscholar.org/paper/f8403bf4e3060487cbc8acceb1fb256a4f1cfc76\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2016},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1503.00064\",\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"144332826\",\"name\":\"Chen Kong\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c80c4ba8226ec556e1775e647f91bb8c126b5e57\",\"title\":\"Generating Multi-Sentence Lingual Descriptions of Indoor Scenes\",\"url\":\"https://www.semanticscholar.org/paper/c80c4ba8226ec556e1775e647f91bb8c126b5e57\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144942149\",\"name\":\"N. A. Harbi\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.18653/v1/W17-3512\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b68422873ff69dd26614a5114eb576e4b816d05d\",\"title\":\"Natural Language Descriptions for Human Activities in Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/b68422873ff69dd26614a5114eb576e4b816d05d\",\"venue\":\"INLG\",\"year\":2017},{\"arxivId\":\"1703.02521\",\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"35198686\",\"name\":\"Joseph J. Lim\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2017.116\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35f1bcff4552632419742bbb6e1927ef5e998eb4\",\"title\":\"Unsupervised Visual-Linguistic Reference Resolution in Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/35f1bcff4552632419742bbb6e1927ef5e998eb4\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1943885\",\"name\":\"B. F. Yuksel\"},{\"authorId\":\"1710568\",\"name\":\"Pooyan Fazli\"},{\"authorId\":\"1666622832\",\"name\":\"Umang Mathur\"},{\"authorId\":\"80018107\",\"name\":\"Vaishali Bisht\"},{\"authorId\":\"49899969\",\"name\":\"S. Kim\"},{\"authorId\":\"153231420\",\"name\":\"J. J. Lee\"},{\"authorId\":\"1666224668\",\"name\":\"Seung Jung Jin\"},{\"authorId\":\"40027011\",\"name\":\"Yue-Ting Siu\"},{\"authorId\":\"144851350\",\"name\":\"Joshua A. Miele\"},{\"authorId\":\"40100223\",\"name\":\"I. Yoon\"}],\"doi\":\"10.1145/3357236.3395433\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12066c4db32a715ad7709889a142d294229936cd\",\"title\":\"Human-in-the-Loop Machine Learning to Increase Video Accessibility for Visually Impaired and Blind Users\",\"url\":\"https://www.semanticscholar.org/paper/12066c4db32a715ad7709889a142d294229936cd\",\"venue\":\"Conference on Designing Interactive Systems\",\"year\":2020},{\"arxivId\":\"1809.04938\",\"authors\":[{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"145500855\",\"name\":\"Lei Cui\"},{\"authorId\":\"10780897\",\"name\":\"Damai Dai\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.1609/AAAI.V33I01.33016810\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"09e39f4334417f92bddc20072f43efade9bd5b60\",\"title\":\"LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/09e39f4334417f92bddc20072f43efade9bd5b60\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2934386\",\"name\":\"Tongxin Hu\"},{\"authorId\":\"3157379\",\"name\":\"Wentong Liao\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"1779035\",\"name\":\"B. Rosenhahn\"}],\"doi\":\"10.1007/978-3-030-33676-9_23\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1591d983571bb37c909f36b09d4051e83147450f\",\"title\":\"Exploiting Attention for Visual Relationship Detection\",\"url\":\"https://www.semanticscholar.org/paper/1591d983571bb37c909f36b09d4051e83147450f\",\"venue\":\"GCPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145668114\",\"name\":\"Yang Xie\"},{\"authorId\":\"1748992\",\"name\":\"Koji Eguchi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"77933349f884acfa51e16d98c62ae6031099d396\",\"title\":\"Models Considering Burstiness of Local Features\",\"url\":\"https://www.semanticscholar.org/paper/77933349f884acfa51e16d98c62ae6031099d396\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145102294\",\"name\":\"Moreira de Souza\"},{\"authorId\":\"123900281\",\"name\":\"Fillipe Dias\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8a75bcb25a8ba707924b0403f9d65d25d1e1e1bb\",\"title\":\"Semantic Description of Activities in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8a75bcb25a8ba707924b0403f9d65d25d1e1e1bb\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145380510\",\"name\":\"L. Rossetto\"},{\"authorId\":\"2155883\",\"name\":\"Ivan Giangreco\"},{\"authorId\":\"145717652\",\"name\":\"H. Schuldt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d3b63c8b53638a4dfee1c6f433080977c062d11\",\"title\":\"OSVC - Open Short Video Collection 1.0\",\"url\":\"https://www.semanticscholar.org/paper/9d3b63c8b53638a4dfee1c6f433080977c062d11\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.22028/D291-26562\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ab1719f573a6c121d7d7da5053fe5f12de0182e7\",\"title\":\"Combining visual recognition and computational linguistics : linguistic knowledge for visual recognition and natural language descriptions of visual content\",\"url\":\"https://www.semanticscholar.org/paper/ab1719f573a6c121d7d7da5053fe5f12de0182e7\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"144942149\",\"name\":\"N. A. Harbi\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.1016/j.ins.2014.12.034\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"997e9a556278ec007cb763441b384fd7165b5d60\",\"title\":\"A framework for creating natural language descriptions of video streams\",\"url\":\"https://www.semanticscholar.org/paper/997e9a556278ec007cb763441b384fd7165b5d60\",\"venue\":\"Inf. Sci.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2015.7298878\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5838af587938e74b5758414c384dcf16dd6e1d1e\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/5838af587938e74b5758414c384dcf16dd6e1d1e\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/ICCV.2017.83\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"title\":\"Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786361\",\"name\":\"Zhenxing Niu\"},{\"authorId\":\"144988571\",\"name\":\"Gang Hua\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"},{\"authorId\":\"10699750\",\"name\":\"X. Gao\"}],\"doi\":\"10.1016/j.cviu.2015.01.010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c62cde148b4c9c0ab61a460801c9cfcc68aecd4\",\"title\":\"Visual Topic Network: Building better image representations for images in social media\",\"url\":\"https://www.semanticscholar.org/paper/1c62cde148b4c9c0ab61a460801c9cfcc68aecd4\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3493606\",\"name\":\"M. Bruni\"},{\"authorId\":\"1789269\",\"name\":\"Tiberio Uricchio\"},{\"authorId\":\"2831602\",\"name\":\"Lorenzo Seidenari\"},{\"authorId\":\"8196487\",\"name\":\"A. D. Bimbo\"}],\"doi\":\"10.1145/2964284.2967301\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e5eedbdf779277b96caf4309aa631ac3e38ae6d\",\"title\":\"Do Textual Descriptions Help Action Recognition?\",\"url\":\"https://www.semanticscholar.org/paper/5e5eedbdf779277b96caf4309aa631ac3e38ae6d\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Weiying Wang\"},{\"authorId\":null,\"name\":\"Yongcheng Wang\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.18653/v1/D19-1517\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"34e7db6ffcdfeffc29eab0622384ed412d9b4558\",\"title\":\"YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/34e7db6ffcdfeffc29eab0622384ed412d9b4558\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2014.101\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6226f4040fce4e636c75b9fd3abd42c4f32639dd\",\"title\":\"Actionness Ranking with Lattice Conditional Ordinal Random Fields\",\"url\":\"https://www.semanticscholar.org/paper/6226f4040fce4e636c75b9fd3abd42c4f32639dd\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1909.05424\",\"authors\":[{\"authorId\":\"20132361\",\"name\":\"Changhan Wang\"},{\"authorId\":\"153408223\",\"name\":\"A. Jain\"},{\"authorId\":\"7167137\",\"name\":\"Danlu Chen\"},{\"authorId\":\"3016273\",\"name\":\"Jiatao Gu\"}],\"doi\":\"10.18653/v1/D19-3043\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"37c3f85d0a055de946235729d39787b9ed0e4ca3\",\"title\":\"VizSeq: A Visual Analysis Toolkit for Text Generation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/37c3f85d0a055de946235729d39787b9ed0e4ca3\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47897687\",\"name\":\"H. Tan\"},{\"authorId\":\"47297550\",\"name\":\"Hongyuan Zhu\"},{\"authorId\":\"6516914\",\"name\":\"J. Lim\"},{\"authorId\":\"1694051\",\"name\":\"Cheston Tan\"}],\"doi\":\"10.1016/J.CVIU.2020.103107\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d79912ee3469c1c93f2c726f9a9b4baf7682ec5f\",\"title\":\"A comprehensive survey of procedural video datasets\",\"url\":\"https://www.semanticscholar.org/paper/d79912ee3469c1c93f2c726f9a9b4baf7682ec5f\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9430995\",\"name\":\"Vahid Chahkandi\"},{\"authorId\":\"1840177\",\"name\":\"Mohammad Javad Fadaeieslam\"},{\"authorId\":\"1914793\",\"name\":\"F. Yaghmaee\"}],\"doi\":\"10.1007/s13735-018-0158-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d4b61c59188b830b9bbeb346d86f950585204af1\",\"title\":\"Improvement of image description using bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/d4b61c59188b830b9bbeb346d86f950585204af1\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2018},{\"arxivId\":\"1812.02501\",\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"144031869\",\"name\":\"A. Yao\"}],\"doi\":\"10.1109/ICCV.2019.00095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15a36f9639f608c4567302de65355543fdcee910\",\"title\":\"Zero-Shot Anticipation for Instructional Activities\",\"url\":\"https://www.semanticscholar.org/paper/15a36f9639f608c4567302de65355543fdcee910\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1705.07818\",\"authors\":[{\"authorId\":\"144529493\",\"name\":\"Li Ding\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c1219c1334c292fb4e5fd0c434dacd0e1e4d5e28\",\"title\":\"TricorNet: A Hybrid Temporal Convolutional and Recurrent Network for Video Action Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/c1219c1334c292fb4e5fd0c434dacd0e1e4d5e28\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff8eb9448c3d764e631f54e2936251f5ef5defcb\",\"title\":\"Generative Grammar Semantic Trees Parse Graphs Training Descriptions New Image Scene Graph Semantic Trees Generated Description Vision Models Training Images\",\"url\":\"https://www.semanticscholar.org/paper/ff8eb9448c3d764e631f54e2936251f5ef5defcb\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"35084211\",\"name\":\"M. Lewis\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D18-1117\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"title\":\"A Dataset for Telling the Stories of Social Media Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1908.00943\",\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"49745735\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"199c2a410cf4430841907e27d5b7026efd95a6ec\",\"title\":\"Prediction and Description of Near-Future Activities in Video.\",\"url\":\"https://www.semanticscholar.org/paper/199c2a410cf4430841907e27d5b7026efd95a6ec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50565167\",\"name\":\"K. Barnard\"}],\"doi\":\"10.2200/S00705ED1V01Y201602COV007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4e69a89a9e322ca78ed3f1140e74aee5ab844f1\",\"title\":\"Computational Methods for Integrating Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c4e69a89a9e322ca78ed3f1140e74aee5ab844f1\",\"venue\":\"Computational Methods for Integrating Vision and Language\",\"year\":2016},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2896212\",\"name\":\"Fangyi Zhang\"},{\"authorId\":\"1803115\",\"name\":\"B. Upcroft\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3540db1f9ec340e45b6096651e195fb7bf7e7e1\",\"title\":\"Vision-Based Autonomous Robotic Manipulation Learning from Observation and Exploration \\u2217 Stage 2 Proposal\",\"url\":\"https://www.semanticscholar.org/paper/e3540db1f9ec340e45b6096651e195fb7bf7e7e1\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d90bcce87174d2e85004036ae3e977c1ef222695\",\"title\":\"Scale-Adaptive Video Understanding.\",\"url\":\"https://www.semanticscholar.org/paper/d90bcce87174d2e85004036ae3e977c1ef222695\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1804.00819\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"34872128\",\"name\":\"Yingbo Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.1109/CVPR.2018.00911\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"35ed258aede3df17ee20a6635364cb5fd2461049\",\"title\":\"End-to-End Dense Video Captioning with Masked Transformer\",\"url\":\"https://www.semanticscholar.org/paper/35ed258aede3df17ee20a6635364cb5fd2461049\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1930287\",\"name\":\"L. Zhang\"},{\"authorId\":\"1772337\",\"name\":\"R. Radke\"}],\"doi\":\"10.1145/3382507.3418886\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c43909efc347aab615a8688e0329d5b3d2cc1b62\",\"title\":\"Temporal Attention and Consistency Measuring for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/c43909efc347aab615a8688e0329d5b3d2cc1b62\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"1931707\",\"name\":\"M. B\\u00e4uml\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1109/CVPR.2015.7298792\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"45ca387a4080b6aee610783ed03d19bd1891503f\",\"title\":\"Book2Movie: Aligning video scenes with book chapters\",\"url\":\"https://www.semanticscholar.org/paper/45ca387a4080b6aee610783ed03d19bd1891503f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748081\",\"name\":\"R. Srihari\"},{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f9cde411f21877372cc411c8417c8ca4181cd0fc\",\"title\":\"Natural language summarization of text and videos using topic models\",\"url\":\"https://www.semanticscholar.org/paper/f9cde411f21877372cc411c8417c8ca4181cd0fc\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1511.02492\",\"authors\":[{\"authorId\":\"3000952\",\"name\":\"A. Habibian\"},{\"authorId\":\"1722052\",\"name\":\"Thomas Mensink\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1109/TPAMI.2016.2627563\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de7bdc64e75f008efd9b25fdc5250f528757a698\",\"title\":\"Video2vec Embeddings Recognize Events When Examples Are Scarce\",\"url\":\"https://www.semanticscholar.org/paper/de7bdc64e75f008efd9b25fdc5250f528757a698\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1506.08438\",\"authors\":[{\"authorId\":\"3114252\",\"name\":\"O. Sener\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"1681995\",\"name\":\"A. Saxena\"}],\"doi\":\"10.1109/ICCV.2015.509\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"71d114b58379b99cb96f01fbb9d1667d970ed53c\",\"title\":\"Unsupervised Semantic Parsing of Video Collections\",\"url\":\"https://www.semanticscholar.org/paper/71d114b58379b99cb96f01fbb9d1667d970ed53c\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1109/CVPR.2015.7298966\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"title\":\"Deep correlation for matching images and text\",\"url\":\"https://www.semanticscholar.org/paper/efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1609.06782\",\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3122865.3122867\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"title\":\"Deep Learning for Video Classification and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"venue\":\"Frontiers of Multimedia Research\",\"year\":2018},{\"arxivId\":\"1906.00283\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"122175026\",\"name\":\"P\\u00e9ter Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"title\":\"Learning to Generate Grounded Image Captions without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/93c7166af45c86caec363e0b84dfd85ed3b51acc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51469630\",\"name\":\"Wenyuan Xue\"},{\"authorId\":\"39870808\",\"name\":\"Q. Li\"},{\"authorId\":\"1490934465\",\"name\":\"Dacheng Tao\"}],\"doi\":\"10.1109/ICDAR.2019.00125\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e03ada27dc07ee80dd199308aa243a9ccffeff5d\",\"title\":\"ReS2TIM: Reconstruct Syntactic Structures from Table Images\",\"url\":\"https://www.semanticscholar.org/paper/e03ada27dc07ee80dd199308aa243a9ccffeff5d\",\"venue\":\"2019 International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2019},{\"arxivId\":\"1611.06492\",\"authors\":[{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"34762956\",\"name\":\"Abhinav Agarwalla\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"144240262\",\"name\":\"P. Mitra\"}],\"doi\":\"10.1109/CVPRW.2017.273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"title\":\"Recurrent Memory Addressing for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1511.06674\",\"authors\":[{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"1749627\",\"name\":\"Marius Leordeanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8120a64a73b89294990b3c1e4567b503869b8979\",\"title\":\"Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/8120a64a73b89294990b3c1e4567b503869b8979\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3114252\",\"name\":\"O. Sener\"}],\"doi\":\"10.7298/X47942ND\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cffebdf88e406c27b892857d1520cb2d7ccda573\",\"title\":\"Learning from large-scale visual data for robots\",\"url\":\"https://www.semanticscholar.org/paper/cffebdf88e406c27b892857d1520cb2d7ccda573\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"title\":\"Beyond Labels and Captions: Contextualizing Grounded Semantics for Explainable Visual Interpretation\",\"url\":\"https://www.semanticscholar.org/paper/267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":\"10.1007/s11263-017-1018-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"44855e53801d09763c1fb5f90ab73e5c3758a728\",\"title\":\"Sentence Directed Video Object Codiscovery\",\"url\":\"https://www.semanticscholar.org/paper/44855e53801d09763c1fb5f90ab73e5c3758a728\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50754085\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"49479171\",\"name\":\"Chen Fang\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"2047181\",\"name\":\"Seokhwan Kim\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"144015161\",\"name\":\"Y. Fu\"}],\"doi\":\"10.1109/cvpr42600.2020.01254\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1a780c6219996c8481c117056efcf071cbfbd15\",\"title\":\"Screencast Tutorial Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b1a780c6219996c8481c117056efcf071cbfbd15\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1906.07901\",\"authors\":[{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"3448602\",\"name\":\"Jind\\u0159ich Libovick\\u00fd\"},{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":\"10.18653/v1/P19-1659\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"37cc2c54cc60ee1301baca2d95bf003c76dd07d5\",\"title\":\"Multimodal Abstractive Summarization for How2 Videos\",\"url\":\"https://www.semanticscholar.org/paper/37cc2c54cc60ee1301baca2d95bf003c76dd07d5\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"},{\"authorId\":\"143868576\",\"name\":\"A. Srivastava\"},{\"authorId\":\"2422388\",\"name\":\"Jingyong Su\"}],\"doi\":\"10.1109/ICPR.2014.28\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9f644534e51d15802b9895764d8e7e1f95c8c25\",\"title\":\"Pattern Theory-Based Interpretation of Activities\",\"url\":\"https://www.semanticscholar.org/paper/c9f644534e51d15802b9895764d8e7e1f95c8c25\",\"venue\":\"2014 22nd International Conference on Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\"},{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"},{\"authorId\":\"145323459\",\"name\":\"Catherine Hanson\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/ICSC.2013.56\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5925c2bc81f6de08764ca6ea3ed4963a89eaa300\",\"title\":\"Are Actor and Action Semantics Retained in Video Supervoxel Segmentation?\",\"url\":\"https://www.semanticscholar.org/paper/5925c2bc81f6de08764ca6ea3ed4963a89eaa300\",\"venue\":\"2013 IEEE Seventh International Conference on Semantic Computing\",\"year\":2013},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1707.06029\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"4945045\",\"name\":\"Yeonhwa Kim\"},{\"authorId\":\"143912065\",\"name\":\"Kyung Yoo\"},{\"authorId\":\"2135453\",\"name\":\"S. Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.648\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"title\":\"Supervising Neural Attention Models for Video Captioning by Human Gaze Data\",\"url\":\"https://www.semanticscholar.org/paper/1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1803.10404\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"48458657\",\"name\":\"Zhiheng Li\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01234-2_32\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"title\":\"Lip Movements Generation at a Glance\",\"url\":\"https://www.semanticscholar.org/paper/d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1903.02930\",\"authors\":[{\"authorId\":\"49513989\",\"name\":\"Antonios Anastasopoulos\"},{\"authorId\":\"9567965\",\"name\":\"Shankar Kumar\"},{\"authorId\":\"39977619\",\"name\":\"H. Liao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4791dc0c4519988536e4846cbb24ae6382b8fdfd\",\"title\":\"Neural Language Modeling with Visual Features\",\"url\":\"https://www.semanticscholar.org/paper/4791dc0c4519988536e4846cbb24ae6382b8fdfd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"},{\"authorId\":\"143868576\",\"name\":\"A. Srivastava\"},{\"authorId\":\"2422388\",\"name\":\"Jingyong Su\"}],\"doi\":\"10.1016/j.patrec.2016.01.028\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5bc70851961057e5a3be22910da27bdd1b15a45d\",\"title\":\"Pattern theory for representation and inference of semantic structures in videos\",\"url\":\"https://www.semanticscholar.org/paper/5bc70851961057e5a3be22910da27bdd1b15a45d\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"title\":\"Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework\",\"url\":\"https://www.semanticscholar.org/paper/1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"47001807\",\"name\":\"Y. Li\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":\"10.1109/CVPR.2015.7298637\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"055d3f243c221bce6be5d04bd79a403f45396f3a\",\"title\":\"Grasp type revisited: A modern perspective on a classical feature for vision\",\"url\":\"https://www.semanticscholar.org/paper/055d3f243c221bce6be5d04bd79a403f45396f3a\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2511655\",\"name\":\"Michael C. Burl\"},{\"authorId\":\"143939175\",\"name\":\"Russell L. Knight\"},{\"authorId\":\"144728727\",\"name\":\"Anthony C. Barrett\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc45ac8939300973c4ab4dcc42837fea012a964d\",\"title\":\"Visual Intelligence: Toward Machine Understanding of Video Content\",\"url\":\"https://www.semanticscholar.org/paper/cc45ac8939300973c4ab4dcc42837fea012a964d\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49596234\",\"name\":\"S. Kumar\"},{\"authorId\":\"3072685\",\"name\":\"Vikas Dhiman\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd7e880495bd95c738c2fd2c782499212b227e37\",\"title\":\"Learning Compositional Sparse Models of Bimodal Percepts\",\"url\":\"https://www.semanticscholar.org/paper/dd7e880495bd95c738c2fd2c782499212b227e37\",\"venue\":\"AAAI\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"},{\"authorId\":\"29072828\",\"name\":\"Seungwhan Moon\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/CVPR.2015.7298927\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8460ce5b162e2bc12433036060b64e0b2c457c0f\",\"title\":\"Joint photo stream and blog post summarization and exploration\",\"url\":\"https://www.semanticscholar.org/paper/8460ce5b162e2bc12433036060b64e0b2c457c0f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"2007.00394\",\"authors\":[{\"authorId\":\"1410307807\",\"name\":\"Yizhak Ben-Shabat\"},{\"authorId\":\"1490933487\",\"name\":\"Xin Yu\"},{\"authorId\":\"9120887\",\"name\":\"F. Saleh\"},{\"authorId\":\"145185576\",\"name\":\"Dylan Campbell\"},{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"71200893\",\"name\":\"H. Li\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"947b868aa1c38940df280ebeb8077d4e729fb988\",\"title\":\"The IKEA ASM Dataset: Understanding People Assembling Furniture through Actions, Objects and Pose\",\"url\":\"https://www.semanticscholar.org/paper/947b868aa1c38940df280ebeb8077d4e729fb988\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"L. Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3095774\",\"name\":\"Dan Oneata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e46f6a5fcf304a9e240258eb15e6755226ddff2\",\"title\":\"Robust and efficient models for action recognition and localization\",\"url\":\"https://www.semanticscholar.org/paper/4e46f6a5fcf304a9e240258eb15e6755226ddff2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1090/QAM/1530\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e0b6b6921e93ee2dbc279c4a63c630156e5d1e9\",\"title\":\"Generating open world descriptions of video using common sense knowledge in a pattern theory framework\",\"url\":\"https://www.semanticscholar.org/paper/3e0b6b6921e93ee2dbc279c4a63c630156e5d1e9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a57c4e0db828710c6b6449598e6a3b471beaaf0\",\"title\":\"ProcNets: Learning to Segment Procedures in Untrimmed and Unconstrained Videos\",\"url\":\"https://www.semanticscholar.org/paper/2a57c4e0db828710c6b6449598e6a3b471beaaf0\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":\"10.1145/3009906\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6e60536c847ac25dba4c1c071e0355e5537fe061\",\"title\":\"Computer Vision and Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/6e60536c847ac25dba4c1c071e0355e5537fe061\",\"venue\":\"ACM Comput. Surv.\",\"year\":2017},{\"arxivId\":\"1512.02902\",\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2016.501\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"title\":\"MovieQA: Understanding Stories in Movies through Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e03b932fa6c87b8a698c86b4b4963bf6e4b45933\",\"title\":\"Matrix ? Why does Cypher betray Morpheus ? How does the movie end ?\",\"url\":\"https://www.semanticscholar.org/paper/e03b932fa6c87b8a698c86b4b4963bf6e4b45933\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71596621\",\"name\":\"J. Malmaud\"},{\"authorId\":\"3003604\",\"name\":\"Earl J. Wagner\"},{\"authorId\":\"145375772\",\"name\":\"N. Chang\"},{\"authorId\":\"145079099\",\"name\":\"K. Murphy\"}],\"doi\":\"10.3115/v1/W14-2407\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f7a9b392edca880cdc8ed3bac6708a7d4fa348d\",\"title\":\"Cooking with Semantics\",\"url\":\"https://www.semanticscholar.org/paper/3f7a9b392edca880cdc8ed3bac6708a7d4fa348d\",\"venue\":\"ACL 2014\",\"year\":2014},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1403.6173\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"113090874\",\"name\":\"W. Qiu\"},{\"authorId\":\"33985877\",\"name\":\"Annemarie Friedrich\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-11752-2_15\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"889e723cd6d581e120ee6776b231fdf69707ab50\",\"title\":\"Coherent Multi-sentence Video Description with Variable Level of Detail\",\"url\":\"https://www.semanticscholar.org/paper/889e723cd6d581e120ee6776b231fdf69707ab50\",\"venue\":\"GCPR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2017976\",\"name\":\"Kehuang Li\"},{\"authorId\":\"144786385\",\"name\":\"Z. Huang\"},{\"authorId\":\"40542976\",\"name\":\"You-Chi Cheng\"},{\"authorId\":\"145785561\",\"name\":\"C. Lee\"}],\"doi\":\"10.1109/ICASSP.2014.6854454\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bbb7354ad8d120662999a18cb67aac27c3e5fa2a\",\"title\":\"A maximal figure-of-merit learning approach to maximizing mean average precision with deep neural network based classifiers\",\"url\":\"https://www.semanticscholar.org/paper/bbb7354ad8d120662999a18cb67aac27c3e5fa2a\",\"venue\":\"2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2014},{\"arxivId\":\"1607.00442\",\"authors\":[{\"authorId\":\"3112203\",\"name\":\"Yongqiang Huang\"},{\"authorId\":\"71732787\",\"name\":\"Yu Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"052880031be0a760a5b606b2ad3d22f237e8af70\",\"title\":\"Datasets on object manipulation and interaction: a survey\",\"url\":\"https://www.semanticscholar.org/paper/052880031be0a760a5b606b2ad3d22f237e8af70\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"},{\"authorId\":\"143868576\",\"name\":\"A. Srivastava\"},{\"authorId\":\"2422388\",\"name\":\"Jingyong Su\"}],\"doi\":\"10.1007/s11263-016-0913-6\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"17724a4f885b5e56670bcecd00fc6add218e5dd8\",\"title\":\"Spatially Coherent Interpretations of Videos Using Pattern Theory\",\"url\":\"https://www.semanticscholar.org/paper/17724a4f885b5e56670bcecd00fc6add218e5dd8\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"1708.03725\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc7a3573a464bca2cdca71f6f32e798464b85ee6\",\"title\":\"Exploiting Semantic Contextualization for Interpretation of Human Activity in Videos\",\"url\":\"https://www.semanticscholar.org/paper/bc7a3573a464bca2cdca71f6f32e798464b85ee6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3918123\",\"name\":\"M. Ravinder\"},{\"authorId\":\"35171821\",\"name\":\"T. Venugopal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c53a512b4d7dee0d8d0f3e5bf2c6ace7a00cbbae\",\"title\":\"Content-Based Video Indexing and Retrieval using Key frames Texture, Edge and Motion Features\",\"url\":\"https://www.semanticscholar.org/paper/c53a512b4d7dee0d8d0f3e5bf2c6ace7a00cbbae\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2011.11071\",\"authors\":[{\"authorId\":\"2028596941\",\"name\":\"Andreea-Maria Oncescu\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1614034792\",\"name\":\"Yang Liu\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"title\":\"QuerYD: A video dataset with high-quality textual and audio narrations\",\"url\":\"https://www.semanticscholar.org/paper/6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1605.03324\",\"authors\":[{\"authorId\":\"3114252\",\"name\":\"O. Sener\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"2484982\",\"name\":\"Chenxia Wu\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"1681995\",\"name\":\"A. Saxena\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fcbf808bdf140442cddf0710defb2766c2d25c30\",\"title\":\"Unsupervised Semantic Action Discovery from Video Collections\",\"url\":\"https://www.semanticscholar.org/paper/fcbf808bdf140442cddf0710defb2766c2d25c30\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2089428\",\"name\":\"D. P. Barrett\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eec326ea0a1a1044b011c2b454fe8b7ce1240a30\",\"title\":\"Learning in vision and robotics\",\"url\":\"https://www.semanticscholar.org/paper/eec326ea0a1a1044b011c2b454fe8b7ce1240a30\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1770771\",\"name\":\"N. Alharbi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bebb4393952052fec2c3b8cfb1adbf167b57737b\",\"title\":\"Describing human activities in video streams\",\"url\":\"https://www.semanticscholar.org/paper/bebb4393952052fec2c3b8cfb1adbf167b57737b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"}],\"doi\":\"10.1145/2911996.2912043\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"title\":\"Video Description Generation using Audio and Visual Cues\",\"url\":\"https://www.semanticscholar.org/paper/54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"venue\":\"ICMR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48084214\",\"name\":\"Suren Kumar\"},{\"authorId\":\"3072685\",\"name\":\"Vikas Dhiman\"},{\"authorId\":\"31717541\",\"name\":\"Parker A. Koch\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/TPAMI.2017.2693987\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0b42c82c6ab0508bd3bc8c9ee80057c546cfcaa\",\"title\":\"Learning Compositional Sparse Bimodal Models\",\"url\":\"https://www.semanticscholar.org/paper/c0b42c82c6ab0508bd3bc8c9ee80057c546cfcaa\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"},{\"authorId\":\"143868576\",\"name\":\"A. Srivastava\"},{\"authorId\":\"2422388\",\"name\":\"Jingyong Su\"}],\"doi\":\"10.1109/CVPR.2015.7298727\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dff0b891b83692ac62d50b25c08b9b4bbaeec327\",\"title\":\"Temporally coherent interpretations for long videos using pattern theory\",\"url\":\"https://www.semanticscholar.org/paper/dff0b891b83692ac62d50b25c08b9b4bbaeec327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3000952\",\"name\":\"AmirHossein Habibian\"},{\"authorId\":\"1722052\",\"name\":\"Thomas Mensink\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9e24181d33f8f06f79c0e9e109269f03a13a0cc5\",\"title\":\"UvA-DARE ( Digital Academic Repository ) Video 2 vec Embeddings Recognize Events when Examples are Scarce\",\"url\":\"https://www.semanticscholar.org/paper/9e24181d33f8f06f79c0e9e109269f03a13a0cc5\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1707.01340\",\"authors\":[{\"authorId\":\"145380510\",\"name\":\"Luca Rossetto\"},{\"authorId\":\"145717652\",\"name\":\"Heiko Schuldt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7c8452ac9791563d9a739bd079b05e518b20aea\",\"title\":\"Web Video in Numbers - An Analysis of Web-Video Metadata\",\"url\":\"https://www.semanticscholar.org/paper/b7c8452ac9791563d9a739bd079b05e518b20aea\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394475099\",\"name\":\"Beg\\u00fcm \\u00c7itamak\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"}],\"doi\":\"10.1109/SIU.2019.8806555\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"title\":\"MSVD-Turkish: A Large-Scale Dataset for Video Captioning in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"venue\":\"2019 27th Signal Processing and Communications Applications Conference (SIU)\",\"year\":2019},{\"arxivId\":\"1608.07068\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-46475-6_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"title\":\"Title Generation for User Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49120765\",\"name\":\"R. Singh\"},{\"authorId\":\"1387052806\",\"name\":\"Ankur Sonawane\"},{\"authorId\":\"33188415\",\"name\":\"R. Srivastava\"}],\"doi\":\"10.1007/s00530-019-00635-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"45a7a721cfe3f179c5bf86ae08340b1f38fdde48\",\"title\":\"Recent evolution of modern datasets for human activity recognition: a deep survey\",\"url\":\"https://www.semanticscholar.org/paper/45a7a721cfe3f179c5bf86ae08340b1f38fdde48\",\"venue\":\"Multimedia Systems\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2716932\",\"name\":\"Muhammad Attamimi\"},{\"authorId\":\"47597848\",\"name\":\"Yuji Ando\"},{\"authorId\":\"1693821\",\"name\":\"T. Nakamura\"},{\"authorId\":\"47734618\",\"name\":\"T. Nagai\"},{\"authorId\":\"2619938\",\"name\":\"D. Mochihashi\"},{\"authorId\":\"3236658\",\"name\":\"I. Kobayashi\"},{\"authorId\":\"7142317\",\"name\":\"Hideki Asoh\"}],\"doi\":\"10.1080/01691864.2016.1172507\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"1eeb31e4615b1665b87d5b0ac25739b597924c88\",\"title\":\"Learning word meanings and grammar for verbalization of daily life activities using multilayered multimodal latent Dirichlet allocation and Bayesian hidden Markov models\",\"url\":\"https://www.semanticscholar.org/paper/1eeb31e4615b1665b87d5b0ac25739b597924c88\",\"venue\":\"Adv. Robotics\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"40449507\",\"name\":\"Yang Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"115596871\",\"name\":\"Heng Shen\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TCSVT.2019.2912988\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8313ebb0ea219cfc58a18d326dfd3556467981c2\",\"title\":\"A Survey of Human Action Analysis in HRI Applications\",\"url\":\"https://www.semanticscholar.org/paper/8313ebb0ea219cfc58a18d326dfd3556467981c2\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/CRV.2017.51\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6718f2feea2d16b894b738551c38871c8afee11b\",\"title\":\"Towards a Knowledge-Based Approach for Generating Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/6718f2feea2d16b894b738551c38871c8afee11b\",\"venue\":\"2017 14th Conference on Computer and Robot Vision (CRV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"}],\"doi\":\"10.1007/978-3-030-00764-5_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2036b394a5dff537df48d6db62a0d61491c92046\",\"title\":\"iMakeup: Makeup Instructional Video Dataset for Fine-Grained Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2036b394a5dff537df48d6db62a0d61491c92046\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"title\":\"Story Understanding through Semantic Analysis and Automatic Alignment of Text and Video\",\"url\":\"https://www.semanticscholar.org/paper/39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2006.14262\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"title\":\"SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/ICCV.2015.377\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8336e17542aacd02e644bc029389800bf248d470\",\"title\":\"Action Detection by Implicit Intentional Motion Clustering\",\"url\":\"https://www.semanticscholar.org/paper/8336e17542aacd02e644bc029389800bf248d470\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1607.08584\",\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1007/978-3-319-46493-0_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18994755100686833b92c34af8ec1b35934e23d6\",\"title\":\"Connectionist Temporal Modeling for Weakly Supervised Action Labeling\",\"url\":\"https://www.semanticscholar.org/paper/18994755100686833b92c34af8ec1b35934e23d6\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.09392\",\"authors\":[{\"authorId\":\"35299091\",\"name\":\"Yansong Tang\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":null,\"name\":\"Jie Zhou\"}],\"doi\":\"10.1109/TPAMI.2020.2980824\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c974b4547ce5df86f07c428abb6206ad5e52fc0d\",\"title\":\"Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/c974b4547ce5df86f07c428abb6206ad5e52fc0d\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1903.02874\",\"authors\":[{\"authorId\":\"35299091\",\"name\":\"Yansong Tang\"},{\"authorId\":\"50792340\",\"name\":\"Dajun Ding\"},{\"authorId\":\"39358728\",\"name\":\"Yongming Rao\"},{\"authorId\":\"145473095\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2118333\",\"name\":\"Danyang Zhang\"},{\"authorId\":\"48096213\",\"name\":\"L. Zhao\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"49640256\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/CVPR.2019.00130\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"title\":\"COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"47659605\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/TPAMI.2017.2700381\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"17d4fee6b21c9277375d6cf0c9087828595009b6\",\"title\":\"Retrieval of Sentence Sequences for an Image Stream via Coherence Recurrent Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/17d4fee6b21c9277375d6cf0c9087828595009b6\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"1708.09666\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"}],\"doi\":\"10.1145/3078971.3079000\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"title\":\"Generating Video Descriptions with Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"}],\"doi\":\"10.3929/ethz-b-000204633\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ef504d7e5c94b9f9b8bfd3d1c44c6aa0d0515f2\",\"title\":\"Interest-Based Video Summarization via Subset Selection\",\"url\":\"https://www.semanticscholar.org/paper/8ef504d7e5c94b9f9b8bfd3d1c44c6aa0d0515f2\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1704.08723\",\"authors\":[{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb08f679f2cb29c7aa972d66fe9e9996c8dfae00\",\"title\":\"Action Understanding with Multiple Classes of Actors\",\"url\":\"https://www.semanticscholar.org/paper/cb08f679f2cb29c7aa972d66fe9e9996c8dfae00\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"2006.04058\",\"authors\":[{\"authorId\":\"48775710\",\"name\":\"Alok Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"098317f445d4753188658ebd3a72c272d10132cd\",\"title\":\"NITS-VC System for VATEX Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/098317f445d4753188658ebd3a72c272d10132cd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"124561dd9b4ae3b48847547afc2f366a8439aa76\",\"title\":\"Learning to Describe Video with Weak Supervision by Exploiting Negative Sentential Information\",\"url\":\"https://www.semanticscholar.org/paper/124561dd9b4ae3b48847547afc2f366a8439aa76\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"145809603\",\"name\":\"N. Siddharth\"},{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":\"10.1613/jair.4556\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"083d1055f81dd7c9b41233a92b9768a857d1db58\",\"title\":\"A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video\",\"url\":\"https://www.semanticscholar.org/paper/083d1055f81dd7c9b41233a92b9768a857d1db58\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2015},{\"arxivId\":\"1904.03493\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"46255707\",\"name\":\"Lei Li\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"title\":\"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research\",\"url\":\"https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1904.08709\",\"authors\":[{\"authorId\":\"48773674\",\"name\":\"Lydia Weiland\"},{\"authorId\":\"2680937\",\"name\":\"Ioana Hulpus\"},{\"authorId\":\"1801255\",\"name\":\"Simone Paolo Ponzetto\"},{\"authorId\":\"1750165\",\"name\":\"W. Effelsberg\"},{\"authorId\":\"145798497\",\"name\":\"Laura Dietz\"}],\"doi\":\"10.1016/j.datak.2018.07.006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d3293346fa3ae58fa1a93cad668da16827eb7620\",\"title\":\"Knowledge-rich image gist understanding beyond literal meaning\",\"url\":\"https://www.semanticscholar.org/paper/d3293346fa3ae58fa1a93cad668da16827eb7620\",\"venue\":\"Data Knowl. Eng.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0735e81975ebf6d9b054c34a05dd43225cf929e\",\"title\":\"Exploring semantic concepts for complex event analysis in unconstrained video clips\",\"url\":\"https://www.semanticscholar.org/paper/c0735e81975ebf6d9b054c34a05dd43225cf929e\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2006.13608\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"50144812\",\"name\":\"Z. Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394486.3403325\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d271e93c7566b231e560c48b4cc4942077d762f9\",\"title\":\"Comprehensive Information Integration Modeling Framework for Video Titling\",\"url\":\"https://www.semanticscholar.org/paper/d271e93c7566b231e560c48b4cc4942077d762f9\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1705.00581\",\"authors\":[{\"authorId\":\"2326243\",\"name\":\"Arun Balajee Vasudevan\"},{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"},{\"authorId\":\"145800409\",\"name\":\"Anna Volokitin\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1145/3123266.3123297\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"48031e454325487b5fa7972280d1a2400bdef1d4\",\"title\":\"Query-adaptive Video Summarization via Quality-aware Relevance Estimation\",\"url\":\"https://www.semanticscholar.org/paper/48031e454325487b5fa7972280d1a2400bdef1d4\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015}],\"corpusId\":12284555,\"doi\":\"10.1109/CVPR.2013.340\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":18,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"},{\"authorId\":\"144547315\",\"name\":\"E. Hovy\"}],\"doi\":\"10.3115/1073445.1073465\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c63bb976dc0d3a897f3b0920170a4c573ef904c6\",\"title\":\"Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics\",\"url\":\"https://www.semanticscholar.org/paper/c63bb976dc0d3a897f3b0920170a4c573ef904c6\",\"venue\":\"HLT-NAACL\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"36794849\",\"name\":\"L. Zhang\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.1109/ICCVW.2011.6130306\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3fbb3e7dc882f060bb2adf8aa0cf9f5bd6968dd7\",\"title\":\"Towards coherent natural language description of video streams\",\"url\":\"https://www.semanticscholar.org/paper/3fbb3e7dc882f060bb2adf8aa0cf9f5bd6968dd7\",\"venue\":\"2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2737253\",\"name\":\"M. Guillaumin\"},{\"authorId\":\"1722052\",\"name\":\"Thomas Mensink\"},{\"authorId\":\"1721683\",\"name\":\"J. Verbeek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2009.5459266\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9465208bf0524d3a90b99ab88a0086af09121233\",\"title\":\"TagProp: Discriminative metric learning in nearest neighbor models for image auto-annotation\",\"url\":\"https://www.semanticscholar.org/paper/9465208bf0524d3a90b99ab88a0086af09121233\",\"venue\":\"2009 IEEE 12th International Conference on Computer Vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2011.5995347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0302bb2d5476540cfb21467473f5eca843caf90b\",\"title\":\"Unbiased look at dataset bias\",\"url\":\"https://www.semanticscholar.org/paper/0302bb2d5476540cfb21467473f5eca843caf90b\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948848\",\"name\":\"S. Sadanand\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2012.6247806\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d90cb88d89408daf4a0fe5ac341a6b9db747a556\",\"title\":\"Action bank: A high-level representation of activity in video\",\"url\":\"https://www.semanticscholar.org/paper/d90cb88d89408daf4a0fe5ac341a6b9db747a556\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1796335\",\"name\":\"D. Blei\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":\"10.1016/B978-0-12-411519-4.00006-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f198043a866e9187925a8d8db9a55e3bfdd47f2c\",\"title\":\"Latent Dirichlet Allocation\",\"url\":\"https://www.semanticscholar.org/paper/f198043a866e9187925a8d8db9a55e3bfdd47f2c\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1697755\",\"name\":\"Timothy M. Hospedales\"},{\"authorId\":\"144784813\",\"name\":\"S. Gong\"},{\"authorId\":\"145406421\",\"name\":\"T. Xiang\"}],\"doi\":\"10.1109/ICCV.2009.5459342\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c189f03f4a8071888ba20813b077480e21e38e63\",\"title\":\"A Markov Clustering Topic Model for mining behaviour in video\",\"url\":\"https://www.semanticscholar.org/paper/c189f03f4a8071888ba20813b077480e21e38e63\",\"venue\":\"2009 IEEE 12th International Conference on Computer Vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31474374\",\"name\":\"P. Bilinski\"},{\"authorId\":\"144103389\",\"name\":\"F. Br\\u00e9mond\"}],\"doi\":\"10.1007/978-3-642-23968-7_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fefc85e0f99af72e88d7dfb8f8ce0040788a6e3\",\"title\":\"Evaluation of Local Descriptors for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/9fefc85e0f99af72e88d7dfb8f8ce0040788a6e3\",\"venue\":\"ICVS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152645191\",\"name\":\"M. Wainwright\"},{\"authorId\":\"152295980\",\"name\":\"M. Jordan\"}],\"doi\":\"10.1561/2200000001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d98d0d1900b13b87aa4ffd6b69c046beb63f0434\",\"title\":\"Graphical Models, Exponential Families, and Variational Inference\",\"url\":\"https://www.semanticscholar.org/paper/d98d0d1900b13b87aa4ffd6b69c046beb63f0434\",\"venue\":\"Found. Trends Mach. Learn.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":\"10.5860/choice.34-5144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e16a25faf7428e1fc5ed0a10b8196c0499c7fd0d\",\"title\":\"Graphical Models\",\"url\":\"https://www.semanticscholar.org/paper/e16a25faf7428e1fc5ed0a10b8196c0499c7fd0d\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Over\"},{\"authorId\":null,\"name\":\"G. Awad\"},{\"authorId\":null,\"name\":\"M. Michel\"},{\"authorId\":null,\"name\":\"J. Fiscus\"},{\"authorId\":null,\"name\":\"G. Sanders\"},{\"authorId\":null,\"name\":\"B. Shaw\"},{\"authorId\":null,\"name\":\"W. Kraaij\"},{\"authorId\":null,\"name\":\"A. F. Smeaton\"},{\"authorId\":null,\"name\":\"G. Qu\\u00e9enot\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Trecvid 2012 \\u2013 an overview of the goals\",\"url\":\"\",\"venue\":\"tasks, data, evaluation mechanisms and metrics. In TRECVID 2012\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143645506\",\"name\":\"P. Over\"},{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"145879936\",\"name\":\"R. Rose\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7845610526df9179638767b189d8d7dba75d9de5\",\"title\":\"TRECVID 2008 - Goals, Tasks, Data, Evaluation Mechanisms and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/7845610526df9179638767b189d8d7dba75d9de5\",\"venue\":\"TRECVID\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1756979\",\"name\":\"K. V. D. Sande\"},{\"authorId\":\"1695527\",\"name\":\"T. Gevers\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1109/TPAMI.2009.154\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1aa5a8ad5b7031ba39e1dc0537484694364a1312\",\"title\":\"Evaluating Color Descriptors for Object and Scene Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1aa5a8ad5b7031ba39e1dc0537484694364a1312\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/ICCV.2007.4408965\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df46c8c5e613c62a976a2013e0de21b92ab26450\",\"title\":\"Spatially Coherent Latent Topic Model for Concurrent Segmentation and Classification of Objects and Scenes\",\"url\":\"https://www.semanticscholar.org/paper/df46c8c5e613c62a976a2013e0de21b92ab26450\",\"venue\":\"2007 IEEE 11th International Conference on Computer Vision\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153531943\",\"name\":\"J\\u00f6rn Wanke\"},{\"authorId\":\"1782440\",\"name\":\"A. Ulges\"},{\"authorId\":\"1787591\",\"name\":\"Christoph H. Lampert\"},{\"authorId\":\"1733858\",\"name\":\"T. Breuel\"}],\"doi\":\"10.1145/1743384.1743433\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed1b06c937a638f6f0b59278acfe6e8dce5643c1\",\"title\":\"Topic models for semantics-preserving video compression\",\"url\":\"https://www.semanticscholar.org/paper/ed1b06c937a638f6f0b59278acfe6e8dce5643c1\",\"venue\":\"MIR '10\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Mimno\"},{\"authorId\":null,\"name\":\"A. McCallum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Rethinking LDA : Why priors matter Simultaneous image classification and annotation\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143712289\",\"name\":\"D. J. Patterson\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":\"10.1007/s11263-012-0564-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a061e7eab865fc8d2ef00e029b7070719ad2e9a\",\"title\":\"Efficiently Scaling up Crowdsourced Video Annotation\",\"url\":\"https://www.semanticscholar.org/paper/7a061e7eab865fc8d2ef00e029b7070719ad2e9a\",\"venue\":\"International Journal of Computer Vision\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-642-33718-5_11\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8623fe8b087cedcaac276e313f8fed6f0dfccc33\",\"title\":\"Script Data for Attribute-Based Recognition of Composite Activities\",\"url\":\"https://www.semanticscholar.org/paper/8623fe8b087cedcaac276e313f8fed6f0dfccc33\",\"venue\":\"ECCV\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3056091\",\"name\":\"M. Everingham\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"},{\"authorId\":\"145715698\",\"name\":\"C. K. Williams\"},{\"authorId\":\"33652486\",\"name\":\"J. Winn\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/s11263-009-0275-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"82635fb63640ae95f90ee9bdc07832eb461ca881\",\"title\":\"The Pascal Visual Object Classes (VOC) Challenge\",\"url\":\"https://www.semanticscholar.org/paper/82635fb63640ae95f90ee9bdc07832eb461ca881\",\"venue\":\"International Journal of Computer Vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1831395\",\"name\":\"H. Wallach\"},{\"authorId\":\"1705700\",\"name\":\"David Mimno\"},{\"authorId\":\"143753639\",\"name\":\"A. McCallum\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5554c9d5fa92af69992d72ed1fdfbe953b03fb4\",\"title\":\"Rethinking LDA: Why Priors Matter\",\"url\":\"https://www.semanticscholar.org/paper/e5554c9d5fa92af69992d72ed1fdfbe953b03fb4\",\"venue\":\"NIPS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2001731825\",\"name\":\"van de SandeKoen\"},{\"authorId\":\"1643863492\",\"name\":\"GeversTheo\"},{\"authorId\":\"1644332786\",\"name\":\"SnoekCees\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f748f0f1e86eb000c84325b8a5c3d1753bbb6e05\",\"title\":\"Evaluating Color Descriptors for Object and Scene Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f748f0f1e86eb000c84325b8a5c3d1753bbb6e05\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b9f8101c61b415f946625b69f69fc9e3d0d6fc4\",\"title\":\"Generating Natural-Language Video Descriptions Using Text-Mined Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/3b9f8101c61b415f946625b69f69fc9e3d0d6fc4\",\"venue\":\"AAAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143645506\",\"name\":\"P. Over\"},{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"2974372\",\"name\":\"Brian Antonishek\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"},{\"authorId\":\"1693391\",\"name\":\"G. Qu\\u00e9not\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"760f81db6dabbbd0744595e9fc3d55138e5cb88e\",\"title\":\"TRECVID 2015 - An Overview of the Goals, Tasks, Data, Evaluation Mechanisms and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/760f81db6dabbbd0744595e9fc3d55138e5cb88e\",\"venue\":\"TRECVID\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/CVPR.2011.5995466\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"169b847e69c35cfd475eb4dcc561a24de11762ca\",\"title\":\"Baby talk: Understanding and generating simple image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/169b847e69c35cfd475eb4dcc561a24de11762ca\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1717629\",\"name\":\"Yansong Feng\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60be767a255fd13f73ed4e64d9901b30cf6081e8\",\"title\":\"Topic Models for Image Annotation and Text Illustration\",\"url\":\"https://www.semanticscholar.org/paper/60be767a255fd13f73ed4e64d9901b30cf6081e8\",\"venue\":\"HLT-NAACL\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf60322f83714523e2d7c1d39983151fe9db7146\",\"title\":\"Collecting Image Annotations Using Amazon's Mechanical Turk\",\"url\":\"https://www.semanticscholar.org/paper/bf60322f83714523e2d7c1d39983151fe9db7146\",\"venue\":\"Mturk@HLT-NAACL\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2159982\",\"name\":\"A. Makadia\"},{\"authorId\":\"144658464\",\"name\":\"V. Pavlovic\"},{\"authorId\":\"2794322\",\"name\":\"S. Kumar\"}],\"doi\":\"10.1007/978-3-540-88690-7_24\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e9a6bc1bcaf78a8667221c63847de4dcbd4bfcb3\",\"title\":\"A New Baseline for Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/e9a6bc1bcaf78a8667221c63847de4dcbd4bfcb3\",\"venue\":\"ECCV\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1888731\",\"name\":\"M. Hejrati\"},{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/978-3-642-15561-1_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"title\":\"Every Picture Tells a Story: Generating Sentences from Images\",\"url\":\"https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.5244/C.22.99\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719\",\"title\":\"A Spatio-Temporal Descriptor Based on 3D-Gradients\",\"url\":\"https://www.semanticscholar.org/paper/56e95f8efb7dbbc0b1820eaf365edc6f3b3f6719\",\"venue\":\"BMVC\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1756655\",\"name\":\"C. L. Teo\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"76a1dca3a9c2b0229c1b12c95752dcf40dc95a11\",\"title\":\"Corpus-Guided Sentence Generation of Natural Images\",\"url\":\"https://www.semanticscholar.org/paper/76a1dca3a9c2b0229c1b12c95752dcf40dc95a11\",\"venue\":\"EMNLP\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"},{\"authorId\":\"1748081\",\"name\":\"R. Srihari\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1145/2433396.2433456\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbec6617cbe3d6567e4c527eb0baa9340d3e0d4a\",\"title\":\"Translating related words to videos and back through latent topics\",\"url\":\"https://www.semanticscholar.org/paper/cbec6617cbe3d6567e4c527eb0baa9340d3e0d4a\",\"venue\":\"WSDM '13\",\"year\":2013},{\"arxivId\":\"1204.2742\",\"authors\":[{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"},{\"authorId\":\"48540451\",\"name\":\"Alexander Bridge\"},{\"authorId\":\"3190146\",\"name\":\"Zachary Burchill\"},{\"authorId\":\"49081881\",\"name\":\"D. Coroian\"},{\"authorId\":\"1779136\",\"name\":\"S. Dickinson\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"38414598\",\"name\":\"A. Michaux\"},{\"authorId\":\"2587937\",\"name\":\"Sam Mussman\"},{\"authorId\":\"38052303\",\"name\":\"S. Narayanaswamy\"},{\"authorId\":\"2968009\",\"name\":\"D. Salvi\"},{\"authorId\":\"50269497\",\"name\":\"Lara Schmidt\"},{\"authorId\":\"2060623\",\"name\":\"Jiangnan Shangguan\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"},{\"authorId\":\"32655613\",\"name\":\"J. Waggoner\"},{\"authorId\":\"30102584\",\"name\":\"S. Wang\"},{\"authorId\":\"2223764\",\"name\":\"Jinlian Wei\"},{\"authorId\":\"1813304\",\"name\":\"Yifan Yin\"},{\"authorId\":\"48806246\",\"name\":\"Zhiqi Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"793c1c908672ea71aef9e1b41a46272aa27598f7\",\"title\":\"Video In Sentences Out\",\"url\":\"https://www.semanticscholar.org/paper/793c1c908672ea71aef9e1b41a46272aa27598f7\",\"venue\":\"UAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1792458\",\"name\":\"A. Belz\"},{\"authorId\":\"144568312\",\"name\":\"Ehud Reiter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a70e48c119742cb69b1cdbd62e58a8a8d0d28a8e\",\"title\":\"Comparing Automatic and Human Evaluation of NLG Systems\",\"url\":\"https://www.semanticscholar.org/paper/a70e48c119742cb69b1cdbd62e58a8a8d0d28a8e\",\"venue\":\"EACL\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1685089\",\"name\":\"Pedro F. Felzenszwalb\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":\"10.1109/TPAMI.2009.167\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e79272fe3d65197100eae8be9fec6469107969ae\",\"title\":\"Object Detection with Discriminatively Trained Part Based Models\",\"url\":\"https://www.semanticscholar.org/paper/e79272fe3d65197100eae8be9fec6469107969ae\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2718561\",\"name\":\"D. Putthividhya\"},{\"authorId\":\"1786990\",\"name\":\"H. Attias\"},{\"authorId\":\"1758601\",\"name\":\"S. Nagarajan\"}],\"doi\":\"10.1109/CVPR.2010.5540000\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6399d6c675cd496a030bc541879b422811bd6c8\",\"title\":\"Topic regression multi-modal Latent Dirichlet Allocation for image annotation\",\"url\":\"https://www.semanticscholar.org/paper/e6399d6c675cd496a030bc541879b422811bd6c8\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2011.5995711\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ec97294c1e5974c6b827f8fda67f2e96cf1d8339\",\"title\":\"Recognition using visual phrases\",\"url\":\"https://www.semanticscholar.org/paper/ec97294c1e5974c6b827f8fda67f2e96cf1d8339\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47074522\",\"name\":\"C. Wang\"},{\"authorId\":\"1796335\",\"name\":\"D. Blei\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/cvprw.2009.5206800\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7fdf31d5ebdd293b3027e6555e256a936ff5515a\",\"title\":\"Simultaneous image classification and annotation\",\"url\":\"https://www.semanticscholar.org/paper/7fdf31d5ebdd293b3027e6555e256a936ff5515a\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1796335\",\"name\":\"D. Blei\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":\"10.1145/860435.860460\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"473f4b7f8ae2b03dda2593f54b316ff7d55db26b\",\"title\":\"Modeling annotated data\",\"url\":\"https://www.semanticscholar.org/paper/473f4b7f8ae2b03dda2593f54b316ff7d55db26b\",\"venue\":\"SIGIR '03\",\"year\":2003},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Qualitative results from MER12 and our \\\" YouCook \\\" dataset. Only the top 5 sentences from our system are shown\",\"url\":\"\",\"venue\":\"Figure\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A Barbu\"},{\"authorId\":null,\"name\":\"A Bridge\"},{\"authorId\":null,\"name\":\"Z Burchill\"},{\"authorId\":null,\"name\":\"D Coroian\"},{\"authorId\":null,\"name\":\"S J Dickinson\"},{\"authorId\":null,\"name\":\"S Fidler\"},{\"authorId\":null,\"name\":\"A Michaux\"},{\"authorId\":null,\"name\":\"S Mussman\"},{\"authorId\":null,\"name\":\"S Narayanaswamy\"},{\"authorId\":null,\"name\":\"D Salvi\"},{\"authorId\":null,\"name\":\"L Schmidt\"},{\"authorId\":null,\"name\":\"J Shangguan\"},{\"authorId\":null,\"name\":\"J M Siskind\"},{\"authorId\":null,\"name\":\"J W Waggoner\"},{\"authorId\":null,\"name\":\"S Wang\"},{\"authorId\":null,\"name\":\"J Wei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yin, and Z. Zhang. Video in sentences out. In UAI\",\"url\":\"\",\"venue\":\"Yin, and Z. Zhang. Video in sentences out. In UAI\",\"year\":2012}],\"title\":\"A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching\",\"topics\":[{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Top-down and bottom-up design\",\"topicId\":\"96387\",\"url\":\"https://www.semanticscholar.org/topic/96387\"},{\"topic\":\"Sensor\",\"topicId\":\"1117\",\"url\":\"https://www.semanticscholar.org/topic/1117\"},{\"topic\":\"Sparse\",\"topicId\":\"16838\",\"url\":\"https://www.semanticscholar.org/topic/16838\"},{\"topic\":\"Computer vision\",\"topicId\":\"5332\",\"url\":\"https://www.semanticscholar.org/topic/5332\"},{\"topic\":\"Topic model\",\"topicId\":\"12431\",\"url\":\"https://www.semanticscholar.org/topic/12431\"},{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"Scalability\",\"topicId\":\"1360\",\"url\":\"https://www.semanticscholar.org/topic/1360\"},{\"topic\":\"Bottom-up parsing\",\"topicId\":\"682974\",\"url\":\"https://www.semanticscholar.org/topic/682974\"},{\"topic\":\"Language model\",\"topicId\":\"26812\",\"url\":\"https://www.semanticscholar.org/topic/26812\"},{\"topic\":\"Hybrid system\",\"topicId\":\"58579\",\"url\":\"https://www.semanticscholar.org/topic/58579\"},{\"topic\":\"High-level programming language\",\"topicId\":\"212045\",\"url\":\"https://www.semanticscholar.org/topic/212045\"},{\"topic\":\"Bottom-up proteomics\",\"topicId\":\"3514303\",\"url\":\"https://www.semanticscholar.org/topic/3514303\"},{\"topic\":\"Multimodal interaction\",\"topicId\":\"42592\",\"url\":\"https://www.semanticscholar.org/topic/42592\"},{\"topic\":\"Software propagation\",\"topicId\":\"2021211\",\"url\":\"https://www.semanticscholar.org/topic/2021211\"}],\"url\":\"https://www.semanticscholar.org/paper/a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013}\n"