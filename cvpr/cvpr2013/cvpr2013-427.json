"{\"abstract\":\"This paper presents a complete system for expressive visual text-to-speech (VTTS), which is capable of producing expressive output, in the form of a 'talking head', given an input text and a set of continuous expression weights. The face is modeled using an active appearance model (AAM), and several extensions are proposed which make it more applicable to the task of VTTS. The model allows for normalization with respect to both pose and blink state which significantly reduces artifacts in the resulting synthesized sequences. We demonstrate quantitative improvements in terms of reconstruction error over a million frames, as well as in large-scale user studies, comparing the output of different systems.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"145803757\",\"name\":\"R. Anderson\",\"url\":\"https://www.semanticscholar.org/author/145803757\"},{\"authorId\":\"144746940\",\"name\":\"B. Stenger\",\"url\":\"https://www.semanticscholar.org/author/144746940\"},{\"authorId\":\"2196928\",\"name\":\"V. Wan\",\"url\":\"https://www.semanticscholar.org/author/2196928\"},{\"authorId\":\"1745672\",\"name\":\"R. Cipolla\",\"url\":\"https://www.semanticscholar.org/author/1745672\"}],\"citationVelocity\":10,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1746060\",\"name\":\"Masahide Kawai\"},{\"authorId\":\"25584601\",\"name\":\"Tomoyori Iwao\"},{\"authorId\":\"1690063\",\"name\":\"Akinobu Maejima\"},{\"authorId\":\"1731030\",\"name\":\"S. Morishima\"}],\"doi\":\"10.1007/978-3-319-14249-4_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"42362ce1124bbe70bcd207d2e461b830427c50d5\",\"title\":\"Automatic Photorealistic 3D Inner Mouth Restoration from Frontal Images\",\"url\":\"https://www.semanticscholar.org/paper/42362ce1124bbe70bcd207d2e461b830427c50d5\",\"venue\":\"ISVC\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720236\",\"name\":\"J. Yu\"}],\"doi\":\"10.1007/978-3-319-51811-4_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be739d62ff233aa27d8b120d40226a90c07e8a80\",\"title\":\"A Real-Time 3D Visual Singing Synthesis: From Appearance to Internal Articulators\",\"url\":\"https://www.semanticscholar.org/paper/be739d62ff233aa27d8b120d40226a90c07e8a80\",\"venue\":\"MMM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Toru Ishikawa\"},{\"authorId\":null,\"name\":\"Takashi Nose\"},{\"authorId\":null,\"name\":\"Kazuki Sato\"},{\"authorId\":null,\"name\":\"Akinoti Ito\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"c2537db0d22eb86e74862415a5004fc0cb65d264\",\"title\":\"DNN-Based Talking Movie Generation Using Minimum Generation Error Criterion and Adversarial Learning with Face Direction Consideration\",\"url\":\"https://www.semanticscholar.org/paper/c2537db0d22eb86e74862415a5004fc0cb65d264\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143953446\",\"name\":\"J. Latorre\"},{\"authorId\":\"2196928\",\"name\":\"V. Wan\"},{\"authorId\":\"2874272\",\"name\":\"K. Yanagisawa\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"ee24840ab91461ff45a1e9c9fe122713120b66c2\",\"title\":\"Voice expression conversion with factorised HMM-TTS models\",\"url\":\"https://www.semanticscholar.org/paper/ee24840ab91461ff45a1e9c9fe122713120b66c2\",\"venue\":\"INTERSPEECH\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50178247\",\"name\":\"Najmeh Sadoughi\"},{\"authorId\":\"2106794\",\"name\":\"C. Busso\"}],\"doi\":\"10.1007/978-3-319-67401-8_49\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1550b4394d99c6f1ed5558fb63c0ba3f0b2c2ed\",\"title\":\"Joint Learning of Speech-Driven Facial Motion with Bidirectional Long-Short Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/a1550b4394d99c6f1ed5558fb63c0ba3f0b2c2ed\",\"venue\":\"IVA\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46349574\",\"name\":\"Lei Duan\"},{\"authorId\":\"1429809763\",\"name\":\"Md Mizanur Rahoman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d7a33cd88c75dfa96f100a29e608e4863988d8b\",\"title\":\"A Study on Crowdsourcing for Multi- Label Affect Annotation\",\"url\":\"https://www.semanticscholar.org/paper/8d7a33cd88c75dfa96f100a29e608e4863988d8b\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50650989\",\"name\":\"Kazuki Sato\"},{\"authorId\":\"20418388\",\"name\":\"Takashi Nose\"},{\"authorId\":\"2420860\",\"name\":\"Akinori Ito\"}],\"doi\":\"10.4236/JCC.2017.510006\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc2902afc09be502053bdbe4dc1c8cc627071d3f\",\"title\":\"HMM-Based Photo-Realistic Talking Face Synthesis Using Facial Expression Parameter Mapping with Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/fc2902afc09be502053bdbe4dc1c8cc627071d3f\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1746060\",\"name\":\"Masahide Kawai\"},{\"authorId\":\"25584601\",\"name\":\"Tomoyori Iwao\"},{\"authorId\":\"1690063\",\"name\":\"Akinobu Maejima\"},{\"authorId\":\"1731030\",\"name\":\"S. Morishima\"}],\"doi\":\"10.2197/ipsjjip.23.693\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6eb89ed2dd485b7cf1a3481276cbe8dedb787b37\",\"title\":\"Automatic Generation of Photorealistic 3D Inner Mouth Animation only from Frontal Images\",\"url\":\"https://www.semanticscholar.org/paper/6eb89ed2dd485b7cf1a3481276cbe8dedb787b37\",\"venue\":\"J. Inf. Process.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2196928\",\"name\":\"V. Wan\"},{\"authorId\":\"145803757\",\"name\":\"R. Anderson\"},{\"authorId\":\"144885767\",\"name\":\"A. Blokland\"},{\"authorId\":\"2784449\",\"name\":\"Norbert Braunschweiler\"},{\"authorId\":\"1734070\",\"name\":\"L. Chen\"},{\"authorId\":\"1894766\",\"name\":\"B. Kolluru\"},{\"authorId\":\"143953446\",\"name\":\"J. Latorre\"},{\"authorId\":\"145981206\",\"name\":\"R. Maia\"},{\"authorId\":\"144746940\",\"name\":\"B. Stenger\"},{\"authorId\":\"2874272\",\"name\":\"K. Yanagisawa\"},{\"authorId\":\"3101750\",\"name\":\"Y. Stylianou\"},{\"authorId\":\"1742578\",\"name\":\"M. Akamine\"},{\"authorId\":\"1740397\",\"name\":\"M. Gales\"},{\"authorId\":\"1745672\",\"name\":\"R. Cipolla\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c91397cc3cce338005de8d0d5e47ecb624a98bb\",\"title\":\"Photo-realistic expressive text to talking head synthesis\",\"url\":\"https://www.semanticscholar.org/paper/1c91397cc3cce338005de8d0d5e47ecb624a98bb\",\"venue\":\"INTERSPEECH\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699058\",\"name\":\"M. Zollh\\u00f6fer\"},{\"authorId\":\"34105638\",\"name\":\"Justus Thies\"},{\"authorId\":\"39912094\",\"name\":\"P. Garrido\"},{\"authorId\":\"143929823\",\"name\":\"D. Bradley\"},{\"authorId\":\"2486770\",\"name\":\"T. Beeler\"},{\"authorId\":\"144565371\",\"name\":\"P. P\\u00e9rez\"},{\"authorId\":\"144140066\",\"name\":\"M. Stamminger\"},{\"authorId\":\"2209612\",\"name\":\"M. Nie\\u00dfner\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":\"10.1111/cgf.13382\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fda48e021f7c1445406fa7db32a443726e0ef8d3\",\"title\":\"State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications\",\"url\":\"https://www.semanticscholar.org/paper/fda48e021f7c1445406fa7db32a443726e0ef8d3\",\"venue\":\"Comput. Graph. Forum\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51226870\",\"name\":\"Valentin Rothoft\"},{\"authorId\":\"51224097\",\"name\":\"Jiaxin Si\"},{\"authorId\":\"144440580\",\"name\":\"F. Jiang\"},{\"authorId\":\"2613907\",\"name\":\"R. Shen\"}],\"doi\":\"10.1109/ICCSEC.2017.8446759\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a26fd9df58bb76d6c7a3254820143b3da5bd584b\",\"title\":\"Monitor Pupils' Attention by Image Super-Resolution and Anomaly Detection\",\"url\":\"https://www.semanticscholar.org/paper/a26fd9df58bb76d6c7a3254820143b3da5bd584b\",\"venue\":\"2017 International Conference on Computer Systems, Electronics and Control (ICCSEC)\",\"year\":2017},{\"arxivId\":\"1705.09406\",\"authors\":[{\"authorId\":\"11138090\",\"name\":\"Tadas Baltru\\u0161aitis\"},{\"authorId\":\"118242121\",\"name\":\"Chaitanya Ahuja\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/TPAMI.2018.2798607\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91\",\"title\":\"Multimodal Machine Learning: A Survey and Taxonomy\",\"url\":\"https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31123128\",\"name\":\"Y. Zhao\"},{\"authorId\":\"2848440\",\"name\":\"Meshia C\\u00e9dric Oveneke\"},{\"authorId\":\"48219791\",\"name\":\"Dongmei Jiang\"},{\"authorId\":\"48077408\",\"name\":\"H. Sahli\"}],\"doi\":\"10.1007/s11042-018-6952-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6880855cfe631cbd2cbe2df98ff99ef9ddf77c0e\",\"title\":\"A video prediction approach for animating single face image\",\"url\":\"https://www.semanticscholar.org/paper/6880855cfe631cbd2cbe2df98ff99ef9ddf77c0e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7652095\",\"name\":\"Y. B. Kim\"},{\"authorId\":\"40267433\",\"name\":\"S. Kang\"},{\"authorId\":\"48602177\",\"name\":\"S. B. Lee\"},{\"authorId\":\"5702793\",\"name\":\"Jang Young Jung\"},{\"authorId\":\"3000093\",\"name\":\"Hyeong Ryeol Kam\"},{\"authorId\":\"40033160\",\"name\":\"J. Lee\"},{\"authorId\":\"30990451\",\"name\":\"Y. S. Kim\"},{\"authorId\":\"3103240\",\"name\":\"Joonsoo Lee\"},{\"authorId\":\"50320696\",\"name\":\"C. Kim\"}],\"doi\":\"10.7717/peerj.1502\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c0ee89dc2dad76147780f96294de9e421348c1f4\",\"title\":\"Efficiently detecting outlying behavior in video-game players\",\"url\":\"https://www.semanticscholar.org/paper/c0ee89dc2dad76147780f96294de9e421348c1f4\",\"venue\":\"PeerJ\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39912094\",\"name\":\"P. Garrido\"},{\"authorId\":\"1797649\",\"name\":\"Levi Valgaerts\"},{\"authorId\":\"1401658974\",\"name\":\"Hamid Sarmadi\"},{\"authorId\":\"41178028\",\"name\":\"I. Steiner\"},{\"authorId\":\"13640633\",\"name\":\"K. Varanasi\"},{\"authorId\":\"144565371\",\"name\":\"P. P\\u00e9rez\"},{\"authorId\":\"1680185\",\"name\":\"C. Theobalt\"}],\"doi\":\"10.1111/cgf.12552\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8128fb68dc94c086ba213a28ac132e53eb8da02a\",\"title\":\"VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track\",\"url\":\"https://www.semanticscholar.org/paper/8128fb68dc94c086ba213a28ac132e53eb8da02a\",\"venue\":\"Comput. Graph. Forum\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3521193\",\"name\":\"Y. Zhao\"},{\"authorId\":\"48219791\",\"name\":\"Dongmei Jiang\"},{\"authorId\":\"48077408\",\"name\":\"H. Sahli\"}],\"doi\":\"10.1109/ACII.2015.7344664\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd031dbf634103ff3c58ce87aa74ec6921b2e21d\",\"title\":\"3D emotional facial animation synthesis with factored conditional Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/dd031dbf634103ff3c58ce87aa74ec6921b2e21d\",\"venue\":\"2015 International Conference on Affective Computing and Intelligent Interaction (ACII)\",\"year\":2015},{\"arxivId\":\"1905.03079\",\"authors\":[{\"authorId\":\"116097067\",\"name\":\"Daniel Cudeiro\"},{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"114339785\",\"name\":\"Cassidy Laidlaw\"},{\"authorId\":\"1952002\",\"name\":\"A. Ranjan\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1109/CVPR.2019.01034\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"91a1370d26ee903296bb4990a84c23f2fa8e8c83\",\"title\":\"Capture, Learning, and Synthesis of 3D Speaking Styles\",\"url\":\"https://www.semanticscholar.org/paper/91a1370d26ee903296bb4990a84c23f2fa8e8c83\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6936088\",\"name\":\"Zichun Guo\"},{\"authorId\":\"1471730209\",\"name\":\"Xueguang Jin\"},{\"authorId\":\"144915622\",\"name\":\"R. Hao\"}],\"doi\":\"10.1109/icis46139.2019.8940177\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88c88cc65c6adac240ce82a9d8260f893e901c57\",\"title\":\"Avatar Social System Improve Perceptions of Disabled People\\u2019s Social Ability\",\"url\":\"https://www.semanticscholar.org/paper/88c88cc65c6adac240ce82a9d8260f893e901c57\",\"venue\":\"2019 IEEE/ACIS 18th International Conference on Computer and Information Science (ICIS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1755301\",\"name\":\"Lanfang Dong\"},{\"authorId\":\"1785017\",\"name\":\"Jianfu Wang\"},{\"authorId\":\"2423480\",\"name\":\"Kui Ni\"},{\"authorId\":\"47906461\",\"name\":\"Yatao Wang\"},{\"authorId\":\"144620591\",\"name\":\"X. Wu\"},{\"authorId\":\"2265513\",\"name\":\"Mingxue Xu\"}],\"doi\":\"10.1007/978-3-319-21963-9_57\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"66806536c0dfa2a42fab82e8cf171c138331f7bf\",\"title\":\"One Simple Virtual Avatar System Based on Single Image\",\"url\":\"https://www.semanticscholar.org/paper/66806536c0dfa2a42fab82e8cf171c138331f7bf\",\"venue\":\"ICIG\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"1792200\",\"name\":\"Stefanie Wuhrer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"047f8d5d5134dd12c67038623417f05ab9885056\",\"title\":\"Motion Synthesis In : Static Scan + Expression Out : Best Fitting Sequence + Angry Out : Animated Sequence Statistical Analysis Expression Recognition\",\"url\":\"https://www.semanticscholar.org/paper/047f8d5d5134dd12c67038623417f05ab9885056\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1919471\",\"name\":\"A. Thangthai\"},{\"authorId\":\"1772594\",\"name\":\"B. Milner\"},{\"authorId\":\"145324126\",\"name\":\"Sarah Taylor\"}],\"doi\":\"10.1016/j.csl.2018.11.003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1efe290b29cfc463b012ab8edc5b729de5ff3414\",\"title\":\"Synthesising visual speech using dynamic visemes and deep learning architectures\",\"url\":\"https://www.semanticscholar.org/paper/1efe290b29cfc463b012ab8edc5b729de5ff3414\",\"venue\":\"Comput. Speech Lang.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1812779\",\"name\":\"J. Vandeventer\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"528069963f0bd0861f380f53270c96c269a3ea1c\",\"title\":\"4D (3D Dynamic) statistical models of conversational expressions and the synthesis of highly-realistic 4D facial expression sequences\",\"url\":\"https://www.semanticscholar.org/paper/528069963f0bd0861f380f53270c96c269a3ea1c\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3228938\",\"name\":\"Dongyue Chen\"},{\"authorId\":\"2688900\",\"name\":\"Ziyi Luo\"},{\"authorId\":\"36098263\",\"name\":\"Tong Jia\"}],\"doi\":\"10.1109/ROBIO.2016.7866500\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8140b2487571420a2ea339bfd2484e90a40f888\",\"title\":\"A new face mesh model based on edge attractor and nonlinear global topological constraints\",\"url\":\"https://www.semanticscholar.org/paper/c8140b2487571420a2ea339bfd2484e90a40f888\",\"venue\":\"2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1780750\",\"name\":\"Timo Bolkart\"},{\"authorId\":\"1792200\",\"name\":\"Stefanie Wuhrer\"}],\"doi\":\"10.1016/j.cviu.2014.06.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03650399cbf53d916d10a507852c9e94a02ee13f\",\"title\":\"3D faces in motion: Fully automatic registration and statistical analysis\",\"url\":\"https://www.semanticscholar.org/paper/03650399cbf53d916d10a507852c9e94a02ee13f\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49526932\",\"name\":\"T. Ishikawa\"},{\"authorId\":\"20418388\",\"name\":\"T. Nose\"},{\"authorId\":\"50167588\",\"name\":\"A. Ito\"}],\"doi\":\"10.1007/978-3-030-03748-2_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9f1bd4bd99347f1ae0c4749a25a902d94ab42b9b\",\"title\":\"DNN-Based Talking Movie Generation with Face Direction Consideration\",\"url\":\"https://www.semanticscholar.org/paper/9f1bd4bd99347f1ae0c4749a25a902d94ab42b9b\",\"venue\":\"IIH-MSP 2018\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1396040632\",\"name\":\"Enas Altarawneh\"},{\"authorId\":\"144968076\",\"name\":\"M. Jenkin\"}],\"doi\":\"10.5220/0007947003600367\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0583c6bad506e7c1ef437c197e4cfb708f33c13d\",\"title\":\"Leveraging Cloud-based Tools to Talk with Robots\",\"url\":\"https://www.semanticscholar.org/paper/0583c6bad506e7c1ef437c197e4cfb708f33c13d\",\"venue\":\"ICINCO\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145803757\",\"name\":\"R. Anderson\"},{\"authorId\":\"144746940\",\"name\":\"B. Stenger\"},{\"authorId\":\"2196928\",\"name\":\"V. Wan\"},{\"authorId\":\"1745672\",\"name\":\"R. Cipolla\"}],\"doi\":\"10.1145/2503385.2503473\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e956dd1bcbac397190052689298ddf42c219d8b9\",\"title\":\"An expressive text-driven 3D talking head\",\"url\":\"https://www.semanticscholar.org/paper/e956dd1bcbac397190052689298ddf42c219d8b9\",\"venue\":\"SIGGRAPH '13\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32979389\",\"name\":\"T. Hirai\"},{\"authorId\":\"3314562\",\"name\":\"Yukara Ikemiya\"},{\"authorId\":\"5999120\",\"name\":\"K. Yoshii\"},{\"authorId\":\"1712260\",\"name\":\"Tomoyasu Nakano\"},{\"authorId\":\"144968710\",\"name\":\"M. Goto\"},{\"authorId\":\"1731030\",\"name\":\"S. Morishima\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e5140b45028d40e064af9c97a3c5118c34640833\",\"title\":\"Automatic singing voice to music video generation via mashup of singing video clips\",\"url\":\"https://www.semanticscholar.org/paper/e5140b45028d40e064af9c97a3c5118c34640833\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2922860\",\"name\":\"Wesley Mattheyses\"},{\"authorId\":\"1802474\",\"name\":\"W. Verhelst\"}],\"doi\":\"10.1016/j.specom.2014.11.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7001c0cf18a29453b43c39f913c6203362cc0cf9\",\"title\":\"Audiovisual speech synthesis: An overview of the state-of-the-art\",\"url\":\"https://www.semanticscholar.org/paper/7001c0cf18a29453b43c39f913c6203362cc0cf9\",\"venue\":\"Speech Commun.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143636342\",\"name\":\"Sarah L. Taylor\"},{\"authorId\":\"2066626\",\"name\":\"T. Kim\"},{\"authorId\":\"1740159\",\"name\":\"Yisong Yue\"},{\"authorId\":\"30303590\",\"name\":\"M. Mahler\"},{\"authorId\":\"1988242\",\"name\":\"James Krahe\"},{\"authorId\":\"36969558\",\"name\":\"Anastasio Garcia Rodriguez\"},{\"authorId\":\"1788773\",\"name\":\"J. Hodgins\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.1145/3072959.3073699\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"title\":\"A deep learning approach for generalized speech animation\",\"url\":\"https://www.semanticscholar.org/paper/cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17126804\",\"name\":\"F. Kuhnke\"},{\"authorId\":\"144835580\",\"name\":\"J. Ostermann\"}],\"doi\":\"10.1109/ICME.2017.8019546\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"26f5b8a79fac681ffb132c4863c51a55bc2b20e2\",\"title\":\"Visual speech synthesis from 3D mesh sequences driven by combined speech features\",\"url\":\"https://www.semanticscholar.org/paper/26f5b8a79fac681ffb132c4863c51a55bc2b20e2\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50178247\",\"name\":\"Najmeh Sadoughi\"},{\"authorId\":\"2106794\",\"name\":\"C. Busso\"}],\"doi\":\"10.1109/FG.2018.00066\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0afa7ecb368805ff201345cb2927f4a350953b2f\",\"title\":\"Expressive Speech-Driven Lip Movements with Multitask Learning\",\"url\":\"https://www.semanticscholar.org/paper/0afa7ecb368805ff201345cb2927f4a350953b2f\",\"venue\":\"2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46789246\",\"name\":\"J. Alexander\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97d0e85deb25589cdbd60dec09caf9c7efcdc84a\",\"title\":\"MASTER THESIS IN ARTIFICIAL INTELLIGENCE\",\"url\":\"https://www.semanticscholar.org/paper/97d0e85deb25589cdbd60dec09caf9c7efcdc84a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47840876\",\"name\":\"L. Xie\"},{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"50591589\",\"name\":\"Shan Yang\"}],\"doi\":\"10.1007/978-3-319-14418-4_1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5f9ba2e49df3ce89f06186d1404778956da1930\",\"title\":\"Visual Speech Animation\",\"url\":\"https://www.semanticscholar.org/paper/d5f9ba2e49df3ce89f06186d1404778956da1930\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50812076\",\"name\":\"J. Yu\"},{\"authorId\":\"3050542\",\"name\":\"L. Yu\"}],\"doi\":\"10.1109/ICIP.2018.8451618\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5dc51e2bcda3a104edd3fd45b4d2b7e447a7b774\",\"title\":\"Synthesizing Photo-Realistic 3D Talking Head: Learning Lip Synchronicity and Emotion from Audio and Video\",\"url\":\"https://www.semanticscholar.org/paper/5dc51e2bcda3a104edd3fd45b4d2b7e447a7b774\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145191247\",\"name\":\"N. Gopalan\"},{\"authorId\":\"66774746\",\"name\":\"Sivaiah Bellamkonda\"},{\"authorId\":\"66207184\",\"name\":\"Vinnakota Saran Chaitanya\"}],\"doi\":\"10.1109/ICIRCA.2018.8597226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd554183287fec879cc085eca6350f8769ab5d78\",\"title\":\"Facial Expression Recognition Using Geometric Landmark Points and Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/fd554183287fec879cc085eca6350f8769ab5d78\",\"venue\":\"2018 International Conference on Inventive Research in Computing Applications (ICIRCA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Viktor Igeland\"},{\"authorId\":null,\"name\":\"Examensarbete utf\\u00f6rt\"},{\"authorId\":null,\"name\":\"Handledare Gabriel Eilertsen\"},{\"authorId\":\"2049891\",\"name\":\"Examinator Jonas Unger\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8fa516d1893a95266bbf6afc99b5579df76b8ad\",\"title\":\"Generating Facial Animation With Emotions In A Neural Text-To-Speech Pipeline\",\"url\":\"https://www.semanticscholar.org/paper/c8fa516d1893a95266bbf6afc99b5579df76b8ad\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2343604\",\"name\":\"Shin-Jin Kang\"},{\"authorId\":\"122204266\",\"name\":\"Donggyun Kim\"},{\"authorId\":\"49170523\",\"name\":\"Y. Kim\"}],\"doi\":\"10.1177/1550147719864886\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"652f02bc2cb3186707da9db98d2e8095b3487f70\",\"title\":\"A visual-physiology multimodal system for detecting outlier behavior of participants in a reality TV show\",\"url\":\"https://www.semanticscholar.org/paper/652f02bc2cb3186707da9db98d2e8095b3487f70\",\"venue\":\"Int. J. Distributed Sens. Networks\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2976930\",\"name\":\"Tero Karras\"},{\"authorId\":\"1761103\",\"name\":\"Timo Aila\"},{\"authorId\":\"36436218\",\"name\":\"S. Laine\"},{\"authorId\":\"3468872\",\"name\":\"Antti Herva\"},{\"authorId\":\"49244945\",\"name\":\"J. Lehtinen\"}],\"doi\":\"10.1145/3072959.3073658\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"title\":\"Audio-driven facial animation by joint end-to-end learning of pose and emotion\",\"url\":\"https://www.semanticscholar.org/paper/95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97583812\",\"name\":\"J. Yu\"},{\"authorId\":\"50122374\",\"name\":\"Qiang Ling\"},{\"authorId\":\"2021652\",\"name\":\"Changwei Luo\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":\"10.1109/TASLP.2019.2935843\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7724039e6110e271865d830a6228107fa26145c4\",\"title\":\"Synthesizing 3D Trump: Predicting and Visualizing the Relationship Between Text, Speech, and Articulatory Movements\",\"url\":\"https://www.semanticscholar.org/paper/7724039e6110e271865d830a6228107fa26145c4\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17126804\",\"name\":\"F. Kuhnke\"},{\"authorId\":\"144835580\",\"name\":\"J. Ostermann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8f216dbd43dda14783677f44bb336c92211cd46\",\"title\":\"SYNTHESIS FROM 3 D MESH SEQUENCES DRIVEN BY COMBINED SPEECH FEATURES\",\"url\":\"https://www.semanticscholar.org/paper/c8f216dbd43dda14783677f44bb336c92211cd46\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1410.1037\",\"authors\":[{\"authorId\":\"144050302\",\"name\":\"Nannan Wang\"},{\"authorId\":\"10699750\",\"name\":\"X. Gao\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"},{\"authorId\":\"46402536\",\"name\":\"Haishun Yang\"},{\"authorId\":\"67180560\",\"name\":\"Xuelong Li\"}],\"doi\":\"10.1016/j.neucom.2017.05.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fcecaa3eed9574028bb3887a0eaa6a8b5a30bd9d\",\"title\":\"Facial feature point detection: A comprehensive survey\",\"url\":\"https://www.semanticscholar.org/paper/fcecaa3eed9574028bb3887a0eaa6a8b5a30bd9d\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37016781\",\"name\":\"Supasorn Suwajanakorn\"},{\"authorId\":\"1396612598\",\"name\":\"Steven M. Seitz\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\"}],\"doi\":\"10.1145/3072959.3073640\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"title\":\"Synthesizing Obama\",\"url\":\"https://www.semanticscholar.org/paper/7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11680100\",\"name\":\"D. Greenwood\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bfee2f0ec529402e6af905fc4b9efcfa10fc222\",\"title\":\"Predicting head pose from speech\",\"url\":\"https://www.semanticscholar.org/paper/3bfee2f0ec529402e6af905fc4b9efcfa10fc222\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145682405\",\"name\":\"J. Charles\"},{\"authorId\":\"34767834\",\"name\":\"D. Magee\"},{\"authorId\":\"1967104\",\"name\":\"David C. Hogg\"}],\"doi\":\"10.1007/978-3-319-49409-8_71\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e772f37f9027f6488eb62660238f6212e8fceac0\",\"title\":\"Virtual Immortality: Reanimating Characters from TV Shows\",\"url\":\"https://www.semanticscholar.org/paper/e772f37f9027f6488eb62660238f6212e8fceac0\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152782371\",\"name\":\"P. Edwards\"},{\"authorId\":\"2483922\",\"name\":\"C. Landreth\"},{\"authorId\":\"3018043\",\"name\":\"E. Fiume\"},{\"authorId\":\"34664064\",\"name\":\"Karan Singh\"}],\"doi\":\"10.1145/2897824.2925984\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"339e662f4a9489e7f5910bc2b047ba86b0f3b7a5\",\"title\":\"JALI: an animator-centric viseme model for expressive lip synchronization\",\"url\":\"https://www.semanticscholar.org/paper/339e662f4a9489e7f5910bc2b047ba86b0f3b7a5\",\"venue\":\"ACM Trans. Graph.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1858065\",\"name\":\"K. K. Htike\"}],\"doi\":\"10.1504/IJISTA.2017.10005128\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"47df89293bb2c31e9078b86df9b5bb1309e93438\",\"title\":\"A review on data-driven learning of a talking head model\",\"url\":\"https://www.semanticscholar.org/paper/47df89293bb2c31e9078b86df9b5bb1309e93438\",\"venue\":\"Int. J. Intell. Syst. Technol. Appl.\",\"year\":2017},{\"arxivId\":\"2005.13616\",\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"},{\"authorId\":\"115268571\",\"name\":\"Barry-John Theobald\"},{\"authorId\":\"145251024\",\"name\":\"P. Dixon\"},{\"authorId\":\"2497993\",\"name\":\"Reinhard Knothe\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"},{\"authorId\":\"123576773\",\"name\":\"Sachin Kajareker\"}],\"doi\":\"10.1145/3382507.3418840\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"title\":\"Modality Dropout for Improved Performance-driven Talking Faces\",\"url\":\"https://www.semanticscholar.org/paper/917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50650989\",\"name\":\"K. Sato\"},{\"authorId\":\"20418388\",\"name\":\"T. Nose\"},{\"authorId\":\"145233036\",\"name\":\"A. Ito\"},{\"authorId\":\"2939952\",\"name\":\"Y. Chiba\"},{\"authorId\":\"2420860\",\"name\":\"Akinori Ito\"},{\"authorId\":\"1732454\",\"name\":\"T. Shinozaki\"}],\"doi\":\"10.1007/978-3-319-63859-1_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96ebf9fac65bb69f7d27c57729072ee09efeaaa3\",\"title\":\"A Study on 2D Photo-Realistic Facial Animation Generation Using 3D Facial Feature Points and Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/96ebf9fac65bb69f7d27c57729072ee09efeaaa3\",\"venue\":\"IIH-MSP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720236\",\"name\":\"J. Yu\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":\"10.1109/ICME.2017.8019362\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"61197cd4d4d0d14d666c0d5abcb8ef2f212edb9d\",\"title\":\"From talking head to singing head: A significant enhancement for more natural human computer interaction\",\"url\":\"https://www.semanticscholar.org/paper/61197cd4d4d0d14d666c0d5abcb8ef2f212edb9d\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":\"1607.06871\",\"authors\":[{\"authorId\":\"1876581\",\"name\":\"C. N. Duong\"},{\"authorId\":\"1769788\",\"name\":\"K. Luu\"},{\"authorId\":\"2687827\",\"name\":\"Kha Gia Quach\"},{\"authorId\":\"144957197\",\"name\":\"T. Bui\"}],\"doi\":\"10.1007/s11263-018-1113-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c17d51bec44522ab113d2a522c1e4f40bdfc0538\",\"title\":\"Deep Appearance Models: A Deep Boltzmann Machine Approach for Face Modeling\",\"url\":\"https://www.semanticscholar.org/paper/c17d51bec44522ab113d2a522c1e4f40bdfc0538\",\"venue\":\"International Journal of Computer Vision\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9545305\",\"name\":\"T. Miyauchi\"},{\"authorId\":\"102876476\",\"name\":\"Hiroki Yoshimura\"},{\"authorId\":\"2817435\",\"name\":\"M. Nishiyama\"},{\"authorId\":\"76941685\",\"name\":\"Yoshio\"},{\"authorId\":\"101699038\",\"name\":\"Iwai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"996bb7abcab9f327d2a322529044b34e3ff6debe\",\"title\":\"Generating Image-based Avatars in Wait State by Measurement of Body Sway\",\"url\":\"https://www.semanticscholar.org/paper/996bb7abcab9f327d2a322529044b34e3ff6debe\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144206962\",\"name\":\"Lei Xie\"},{\"authorId\":\"30602591\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"144947353\",\"name\":\"S. Yang\"}],\"doi\":\"10.1007/978-3-319-30808-1_1-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0f9d8d1056e31615244f3a354ab277d625fff70\",\"title\":\"Visual Speech Animation\",\"url\":\"https://www.semanticscholar.org/paper/f0f9d8d1056e31615244f3a354ab277d625fff70\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119883542\",\"name\":\"J. Yu\"}],\"doi\":\"10.1109/VR.2019.8798288\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"818ce1e96569e19567ab3c9b5711504b4a9e3b51\",\"title\":\"A Real-Time Music VR System for 3D External and Internal Articulators\",\"url\":\"https://www.semanticscholar.org/paper/818ce1e96569e19567ab3c9b5711504b4a9e3b51\",\"venue\":\"2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40234034\",\"name\":\"L. Duan\"},{\"authorId\":\"51030602\",\"name\":\"S. Oyama\"},{\"authorId\":\"2384416\",\"name\":\"H. Sato\"},{\"authorId\":\"28711707\",\"name\":\"Masahito Kurihara\"}],\"doi\":\"10.1016/j.eswa.2014.03.048\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"48b56dcee2caecf21377a3ef5d62d1665fa2fce3\",\"title\":\"Separate or joint? Estimation of multiple labels from crowdsourced annotations\",\"url\":\"https://www.semanticscholar.org/paper/48b56dcee2caecf21377a3ef5d62d1665fa2fce3\",\"venue\":\"Expert Syst. Appl.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"51462829\",\"name\":\"Takaaki Yasui\"},{\"authorId\":\"102651823\",\"name\":\"L. Nguyen\"},{\"authorId\":\"65952507\",\"name\":\"Noboru Babaguchi\"}],\"doi\":\"10.3169/mta.8.60\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6578b4a7f82b28652efa758d16fa9250230c38a9\",\"title\":\"[Paper] Speech-driven Face Reenactment for a Video Sequence\",\"url\":\"https://www.semanticscholar.org/paper/6578b4a7f82b28652efa758d16fa9250230c38a9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1805.09488\",\"authors\":[{\"authorId\":\"1825026\",\"name\":\"Yang Zhou\"},{\"authorId\":\"145841883\",\"name\":\"Shan Xu\"},{\"authorId\":\"2483922\",\"name\":\"C. Landreth\"},{\"authorId\":\"2808670\",\"name\":\"E. Kalogerakis\"},{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"},{\"authorId\":\"144319838\",\"name\":\"K. Singh\"}],\"doi\":\"10.1145/3197517.3201292\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"846372d7181279e93860d54ae3b665ae0ef19d08\",\"title\":\"VisemeNet: Audio-Driven Animator-Centric Speech Animation\",\"url\":\"https://www.semanticscholar.org/paper/846372d7181279e93860d54ae3b665ae0ef19d08\",\"venue\":\"ACM Trans. Graph.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30192180\",\"name\":\"P. P. Filntisis\"},{\"authorId\":\"2243473\",\"name\":\"Athanasios Katsamanis\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/ICIP.2017.8296821\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ada2dd835896f978d0155ddc982c3d39fb5f30eb\",\"title\":\"Photorealistic adaptation and interpolation of facial expressions using HMMS and AAMS for audio-visual speech synthesis\",\"url\":\"https://www.semanticscholar.org/paper/ada2dd835896f978d0155ddc982c3d39fb5f30eb\",\"venue\":\"2017 IEEE International Conference on Image Processing (ICIP)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30192180\",\"name\":\"P. P. Filntisis\"},{\"authorId\":\"2243473\",\"name\":\"Athanasios Katsamanis\"},{\"authorId\":\"1934588\",\"name\":\"Pirros Tsiakoulis\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1016/j.specom.2017.08.011\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6533369bfc28111de64e04a467898176ef5a485e\",\"title\":\"Video-realistic expressive audio-visual speech synthesis for the Greek language\",\"url\":\"https://www.semanticscholar.org/paper/6533369bfc28111de64e04a467898176ef5a485e\",\"venue\":\"Speech Commun.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"18975632\",\"name\":\"Paula Dornhofer Paro Costa\"},{\"authorId\":\"1681041\",\"name\":\"Jos\\u00e9 Mario De Martino\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"41ab1b4ea8a48090c1fb53b7fc0777e3bfcca413\",\"title\":\"Expressive Talking Head for Interactive Conversational Systems\",\"url\":\"https://www.semanticscholar.org/paper/41ab1b4ea8a48090c1fb53b7fc0777e3bfcca413\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50650989\",\"name\":\"K. Sato\"},{\"authorId\":\"20418388\",\"name\":\"T. Nose\"},{\"authorId\":\"2420860\",\"name\":\"Akinori Ito\"}],\"doi\":\"10.1007/978-3-319-50212-0_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"761cd65da4273a5a7936ea46f234ab9a33ebd083\",\"title\":\"Synthesis of Photo-Realistic Facial Animation from Text Based on HMM and DNN with Animation Unit\",\"url\":\"https://www.semanticscholar.org/paper/761cd65da4273a5a7936ea46f234ab9a33ebd083\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1806.00154\",\"authors\":[{\"authorId\":\"50178247\",\"name\":\"Najmeh Sadoughi\"},{\"authorId\":\"2106794\",\"name\":\"C. Busso\"}],\"doi\":\"10.1109/TAFFC.2019.2916031\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b0e2250a2eb250682cfef8e369742cf10d0dfa2\",\"title\":\"Speech-Driven Expressive Talking Lips with Conditional Sequential Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8b0e2250a2eb250682cfef8e369742cf10d0dfa2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22323227\",\"name\":\"Singo Sawa\"},{\"authorId\":\"144125614\",\"name\":\"H. Kawashima\"},{\"authorId\":\"2779942\",\"name\":\"Kei Shimonishi\"},{\"authorId\":\"143658244\",\"name\":\"T. Matsuyama\"}],\"doi\":\"10.1145/2974804.2980499\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6850240ce38c7fc00db660ae8157496331b9771a\",\"title\":\"Modulating Dynamic Models for Lip Motion Generation\",\"url\":\"https://www.semanticscholar.org/paper/6850240ce38c7fc00db660ae8157496331b9771a\",\"venue\":\"HAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2817435\",\"name\":\"M. Nishiyama\"},{\"authorId\":\"9545305\",\"name\":\"T. Miyauchi\"},{\"authorId\":\"102876476\",\"name\":\"Hiroki Yoshimura\"},{\"authorId\":\"143805337\",\"name\":\"Y. Iwai\"}],\"doi\":\"10.1145/2974804.2974807\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f34efb645cbaeacfc23c94591da2ff2f641ce728\",\"title\":\"Synthesizing Realistic Image-based Avatars by Body Sway Analysis\",\"url\":\"https://www.semanticscholar.org/paper/f34efb645cbaeacfc23c94591da2ff2f641ce728\",\"venue\":\"HAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145324126\",\"name\":\"Sarah Taylor\"},{\"authorId\":\"2470657\",\"name\":\"A. Kato\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"},{\"authorId\":\"1772594\",\"name\":\"B. Milner\"}],\"doi\":\"10.21437/Interspeech.2016-483\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ba384ef80ab9295162833ed2658aac119fe9cc3\",\"title\":\"Audio-to-Visual Speech Conversion Using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/1ba384ef80ab9295162833ed2658aac119fe9cc3\",\"venue\":\"INTERSPEECH\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31869928\",\"name\":\"F. Shaw\"},{\"authorId\":\"115268571\",\"name\":\"Barry-John Theobald\"}],\"doi\":\"10.1109/MMUL.2016.63\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3292a71c2afae90994002a0b17f599982ef42c86\",\"title\":\"Expressive Modulation of Neutral Visual Speech\",\"url\":\"https://www.semanticscholar.org/paper/3292a71c2afae90994002a0b17f599982ef42c86\",\"venue\":\"IEEE MultiMedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2523015\",\"name\":\"A. Barbulescu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5aa00cedda7ad3598c5a104201af7671f2960db7\",\"title\":\"Generation of Audio-Visual Prosody for Expressive Virtual Actors. (G\\u00e9n\\u00e9ration de la Prosodie Audio-Visuelle pour les Acteurs Virtuels Expressifs )\",\"url\":\"https://www.semanticscholar.org/paper/5aa00cedda7ad3598c5a104201af7671f2960db7\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97583812\",\"name\":\"J. Yu\"},{\"authorId\":\"1390801159\",\"name\":\"Chang Wen Chen\"},{\"authorId\":\"152763940\",\"name\":\"Zengfu Wang\"}],\"doi\":\"10.1145/3343031.3350865\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2e1426e33714c40434bf9dc820aac663bf533537\",\"title\":\"3D Singing Head for Music VR: Learning External and Internal Articulatory Synchronicity from Lyric, Audio and Notes\",\"url\":\"https://www.semanticscholar.org/paper/2e1426e33714c40434bf9dc820aac663bf533537\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2008.01191\",\"authors\":[{\"authorId\":\"19283055\",\"name\":\"S. Rehman\"},{\"authorId\":\"48957858\",\"name\":\"M. Waqas\"},{\"authorId\":\"8150390\",\"name\":\"Shanshan Tu\"},{\"authorId\":\"1714415\",\"name\":\"A. Koubaa\"},{\"authorId\":\"3021929\",\"name\":\"O. Rehman\"},{\"authorId\":\"1684080504\",\"name\":\"Jawad Ahmad\"},{\"authorId\":\"145917475\",\"name\":\"M. Hanif\"},{\"authorId\":\"1621098616\",\"name\":\"Zhu Han\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cde7342715fd5807bffdaecc77e6592244d930e7\",\"title\":\"Deep Learning Techniques for Future Intelligent Cross-Media Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/cde7342715fd5807bffdaecc77e6592244d930e7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1708249\",\"name\":\"D. Haase\"},{\"authorId\":\"1679449\",\"name\":\"Erik Rodner\"},{\"authorId\":\"1728382\",\"name\":\"Joachim Denzler\"}],\"doi\":\"10.1109/CVPR.2014.185\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a949b8700ca6ba96ee40f75dfee1410c5bbdb3db\",\"title\":\"Instance-Weighted Transfer Learning of Active Appearance Models\",\"url\":\"https://www.semanticscholar.org/paper/a949b8700ca6ba96ee40f75dfee1410c5bbdb3db\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145921805\",\"name\":\"Jonathan Parker\"},{\"authorId\":\"145981206\",\"name\":\"R. Maia\"},{\"authorId\":\"3101750\",\"name\":\"Y. Stylianou\"},{\"authorId\":\"1745672\",\"name\":\"R. Cipolla\"}],\"doi\":\"10.1109/ICASSP.2017.7953092\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5fc7ca401fb2be7cdf824496302b17c6b78f176\",\"title\":\"Expressive visual text to speech and expression adaptation using deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/a5fc7ca401fb2be7cdf824496302b17c6b78f176\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1808.04572\",\"authors\":[{\"authorId\":\"38741604\",\"name\":\"Jun Shu\"},{\"authorId\":\"7814629\",\"name\":\"Zongben Xu\"},{\"authorId\":\"1803714\",\"name\":\"D. Meng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d510bca00b625f86606cb0096299b993090534a\",\"title\":\"Small Sample Learning in Big Data Era\",\"url\":\"https://www.semanticscholar.org/paper/4d510bca00b625f86606cb0096299b993090534a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1919471\",\"name\":\"A. Thangthai\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3d66e389e89e80353fb683bb0945c0d3faed0c19\",\"title\":\"HMM-based visual speech synthesis using dynamic visemes\",\"url\":\"https://www.semanticscholar.org/paper/3d66e389e89e80353fb683bb0945c0d3faed0c19\",\"venue\":\"AVSP\",\"year\":2015},{\"arxivId\":\"1905.06860\",\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"},{\"authorId\":\"9967657\",\"name\":\"Justin F. Binder\"},{\"authorId\":\"48095899\",\"name\":\"Gabriele Fanelli\"},{\"authorId\":\"113131955\",\"name\":\"P. Dixon\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"},{\"authorId\":\"2246174\",\"name\":\"T. Weise\"},{\"authorId\":\"123576773\",\"name\":\"Sachin Kajareker\"}],\"doi\":\"10.1145/3340555.3353745\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d599e1b5deb2c0a95438960ce0fe7b62723cb16\",\"title\":\"Speaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models\",\"url\":\"https://www.semanticscholar.org/paper/0d599e1b5deb2c0a95438960ce0fe7b62723cb16\",\"venue\":\"ICMI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13671896\",\"name\":\"S. A. Cassidy\"},{\"authorId\":\"144746940\",\"name\":\"B. Stenger\"},{\"authorId\":\"145406988\",\"name\":\"L. V. Dongen\"},{\"authorId\":\"2874272\",\"name\":\"K. Yanagisawa\"},{\"authorId\":\"152379613\",\"name\":\"R. Anderson\"},{\"authorId\":\"2196928\",\"name\":\"V. Wan\"},{\"authorId\":\"1411431204\",\"name\":\"S. Baron-Cohen\"},{\"authorId\":\"1745672\",\"name\":\"R. Cipolla\"}],\"doi\":\"10.1016/J.CVIU.2015.08.011\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d95b9159d55cc9351177a06955e9c7fabf2d6ea9\",\"title\":\"Expressive visual text-to-speech as an assistive technology for individuals with autism spectrum conditions\",\"url\":\"https://www.semanticscholar.org/paper/d95b9159d55cc9351177a06955e9c7fabf2d6ea9\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2016}],\"corpusId\":1075172,\"doi\":\"10.1109/CVPR.2013.434\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":6,\"is_open_access\":false,\"is_publisher_licensed\":true,\"paperId\":\"2d8de70f089b581d386aaa0ced5f9421a3fd08c0\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Gonzalez-Mora\"},{\"authorId\":null,\"name\":\"F De La Torre\"},{\"authorId\":null,\"name\":\"R Murthi\"},{\"authorId\":null,\"name\":\"N Guil\"},{\"authorId\":null,\"name\":\"E Zapata\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bilinear active appearance models. ICCV\",\"url\":\"\",\"venue\":\"Bilinear active appearance models. ICCV\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143636342\",\"name\":\"Sarah L. Taylor\"},{\"authorId\":\"30303590\",\"name\":\"M. Mahler\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.2312/SCA/SCA12/275-284\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"910bb564f42f298898c1831e6650b1f2efa07b42\",\"title\":\"Dynamic units of visual speech\",\"url\":\"https://www.semanticscholar.org/paper/910bb564f42f298898c1831e6650b1f2efa07b42\",\"venue\":\"SCA '12\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143867160\",\"name\":\"F. Torre\"},{\"authorId\":\"2105795\",\"name\":\"Michael J. Black\"}],\"doi\":\"10.1016/S1077-3142(03)00076-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8493a9e519d5c16493c64becbc497795438a353\",\"title\":\"Robust parameterized component analysis: theory and applications to 2D facial appearance models\",\"url\":\"https://www.semanticscholar.org/paper/b8493a9e519d5c16493c64becbc497795438a353\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1899617\",\"name\":\"Wan-Chun Ma\"},{\"authorId\":\"145776381\",\"name\":\"A. Jones\"},{\"authorId\":\"39058402\",\"name\":\"J. Chiang\"},{\"authorId\":\"1684297\",\"name\":\"T. Hawkins\"},{\"authorId\":\"40578921\",\"name\":\"S. Frederiksen\"},{\"authorId\":\"1808270\",\"name\":\"P. Peers\"},{\"authorId\":\"46761813\",\"name\":\"M. Vukovic\"},{\"authorId\":\"1744863\",\"name\":\"M. Ouhyoung\"},{\"authorId\":\"1778676\",\"name\":\"P. Debevec\"}],\"doi\":\"10.1145/1457515.1409074\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ca8ab21e3394b58506a76a4ee78175db7aa28902\",\"title\":\"Facial performance synthesis using deformation-driven polynomial displacement maps\",\"url\":\"https://www.semanticscholar.org/paper/ca8ab21e3394b58506a76a4ee78175db7aa28902\",\"venue\":\"SIGGRAPH Asia '08\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7205190\",\"name\":\"T. Cootes\"},{\"authorId\":\"50564384\",\"name\":\"G. Edwards\"},{\"authorId\":\"144482985\",\"name\":\"C. Taylor\"}],\"doi\":\"10.1109/34.927467\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c1cd4b045f47c567ea6ecb997f415a847b4982ab\",\"title\":\"Active Appearance Models\",\"url\":\"https://www.semanticscholar.org/paper/c1cd4b045f47c567ea6ecb997f415a847b4982ab\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145691100\",\"name\":\"K. Wampler\"},{\"authorId\":\"46672137\",\"name\":\"Daichi Sasaki\"},{\"authorId\":\"47059221\",\"name\":\"L. Zhang\"},{\"authorId\":\"1986848\",\"name\":\"Z. Popovic\"}],\"doi\":\"10.2312/SCA/SCA07/053-062\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eec9e6b3198c92059403b66cb2629c7305f4c983\",\"title\":\"Dynamic, expressive speech animation from a single mesh\",\"url\":\"https://www.semanticscholar.org/paper/eec9e6b3198c92059403b66cb2629c7305f4c983\",\"venue\":\"SCA '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Mahler\"},{\"authorId\":null,\"name\":\"B. Theobald\"},{\"authorId\":null,\"name\":\"I. Matthews\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"namic units of visual speech\",\"url\":\"\",\"venue\":\"Eurographics Symposium on Computer Animation\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1619078806\",\"name\":\"A. ADoefaa\"},{\"authorId\":\"1619192001\",\"name\":\"H. P. Doetsch\"},{\"authorId\":\"1619263939\",\"name\":\"Draweng Table\"},{\"authorId\":\"1619192001\",\"name\":\"H. P. Doetsch\"}],\"doi\":\"10.2307/j.ctvrnfqk1.10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4dc21a168070dc87266accd9ce2c06ee6115a285\",\"title\":\"? ? ? ? f ? ? ? ? ?\",\"url\":\"https://www.semanticscholar.org/paper/4dc21a168070dc87266accd9ce2c06ee6115a285\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1761178\",\"name\":\"I. Pandzic\"},{\"authorId\":\"144835580\",\"name\":\"J. Ostermann\"},{\"authorId\":\"2673368\",\"name\":\"D. R. Millen\"}],\"doi\":\"10.1007/s003710050182\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"567b6177b9063cb06e259ab5ce29a47568811c95\",\"title\":\"User evaluation: Synthetic talking faces for interactive services\",\"url\":\"https://www.semanticscholar.org/paper/567b6177b9063cb06e259ab5ce29a47568811c95\",\"venue\":\"The Visual Computer\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1399848407\",\"name\":\"J. Gonzalez-Mora\"},{\"authorId\":\"143867160\",\"name\":\"F. Torre\"},{\"authorId\":\"96675874\",\"name\":\"R. Murthi\"},{\"authorId\":\"1712683\",\"name\":\"Nicol\\u00e1s Guil Mata\"},{\"authorId\":\"1680890\",\"name\":\"E. Zapata\"}],\"doi\":\"10.1109/ICCV.2007.4409185\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"030c8b086dc2705c87dc1b8b1633fb6e0a138eed\",\"title\":\"Bilinear Active Appearance Models\",\"url\":\"https://www.semanticscholar.org/paper/030c8b086dc2705c87dc1b8b1633fb6e0a138eed\",\"venue\":\"2007 IEEE 11th International Conference on Computer Vision\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33578779\",\"name\":\"Irene Albrecht\"},{\"authorId\":\"144951065\",\"name\":\"M. Schr\\u00f6der\"},{\"authorId\":\"144213705\",\"name\":\"J\\u00f6rg Haber\"},{\"authorId\":\"145156858\",\"name\":\"H. Seidel\"}],\"doi\":\"10.1007/s10055-005-0153-5\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6a97ba1d89ac978a134f5149f073312710959e75\",\"title\":\"Mixed feelings: expression of non-basic emotions in a muscle-based talking head\",\"url\":\"https://www.semanticscholar.org/paper/6a97ba1d89ac978a134f5149f073312710959e75\",\"venue\":\"Virtual Reality\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Wampler\"},{\"authorId\":null,\"name\":\"D. Sasaki\"},{\"authorId\":null,\"name\":\"L. Zhang\"},{\"authorId\":null,\"name\":\"Z. Popovi\\u0107\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Dynamic\",\"url\":\"\",\"venue\":\"expressive speech animation from a single mesh. In SCA ACM/Eurographics, pages 53\\u201362\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143929823\",\"name\":\"D. Bradley\"},{\"authorId\":\"1752192\",\"name\":\"W. Heidrich\"},{\"authorId\":\"2775650\",\"name\":\"Tiberiu Popa\"},{\"authorId\":\"1778286\",\"name\":\"A. Sheffer\"}],\"doi\":\"10.1145/1833351.1778778\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"732248bd7cd9d1e0d84f3ef9bf379279226beab2\",\"title\":\"High resolution passive facial performance capture\",\"url\":\"https://www.semanticscholar.org/paper/732248bd7cd9d1e0d84f3ef9bf379279226beab2\",\"venue\":\"ACM Trans. Graph.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2353877\",\"name\":\"S. Deena\"},{\"authorId\":\"3108448\",\"name\":\"Shaobo Hou\"},{\"authorId\":\"1692470\",\"name\":\"Aphrodite Galata\"}],\"doi\":\"10.1145/1891903.1891942\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d2fd3dccd9dc154cec7ada22325b2bb9731980d\",\"title\":\"Visual speech synthesis by modelling coarticulation dynamics using a non-parametric switching state-space model\",\"url\":\"https://www.semanticscholar.org/paper/5d2fd3dccd9dc154cec7ada22325b2bb9731980d\",\"venue\":\"ICMI-MLMI '10\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48696060\",\"name\":\"Y. Cao\"},{\"authorId\":\"32362814\",\"name\":\"Wen C. Tien\"},{\"authorId\":\"1737527\",\"name\":\"P. Faloutsos\"},{\"authorId\":\"1817134\",\"name\":\"F. Pighin\"}],\"doi\":\"10.1145/1095878.1095881\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"029a24e233440512e6923868f83af92832bcbaf6\",\"title\":\"Expressive speech-driven facial animation\",\"url\":\"https://www.semanticscholar.org/paper/029a24e233440512e6923868f83af92832bcbaf6\",\"venue\":\"TOGS\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10699750\",\"name\":\"X. Gao\"},{\"authorId\":\"150352054\",\"name\":\"Y. Su\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/TSMCC.2009.2035631\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ab7e48c3532198cb8e666565aae7211c0be10cc\",\"title\":\"A Review of Active Appearance Models\",\"url\":\"https://www.semanticscholar.org/paper/4ab7e48c3532198cb8e666565aae7211c0be10cc\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1690706\",\"name\":\"A. Black\"},{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"1723069\",\"name\":\"K. Tokuda\"}],\"doi\":\"10.1109/ICASSP.2007.367298\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9eab375b7add5f02a0ae1293d0aa7880ea70233a\",\"title\":\"Statistical Parametric Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/9eab375b7add5f02a0ae1293d0aa7880ea70233a\",\"venue\":\"ICASSP\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"N. Braunschweiler\"},{\"authorId\":null,\"name\":\"M. Gales\"},{\"authorId\":null,\"name\":\"K. Knill\"},{\"authorId\":null,\"name\":\"S. Krstulovi\\u0107\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Text driven 3 D photo - realistic talking head DECface : A system for synthetic face aplications\",\"url\":\"\",\"venue\":\"Multimedia Tools and Applications\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"},{\"authorId\":\"144427091\",\"name\":\"J. Bangham\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"},{\"authorId\":\"8974886\",\"name\":\"G. Cawley\"}],\"doi\":\"10.1016/j.specom.2004.07.002\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5065a2850e483fb12f496c9b2f8ae2a44022435\",\"title\":\"Near-videorealistic synthetic talking faces: implementation and evaluation\",\"url\":\"https://www.semanticscholar.org/paper/a5065a2850e483fb12f496c9b2f8ae2a44022435\",\"venue\":\"Speech Commun.\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1932050\",\"name\":\"T. Ezzat\"},{\"authorId\":\"1685292\",\"name\":\"T. Poggio\"}],\"doi\":\"10.1109/CA.1998.681913\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf636d97a42837030fb37812b6ad56da624a0c5f\",\"title\":\"MikeTalk: a talking facial display based on morphing visemes\",\"url\":\"https://www.semanticscholar.org/paper/cf636d97a42837030fb37812b6ad56da624a0c5f\",\"venue\":\"Proceedings Computer Animation '98 (Cat. No.98EX169)\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103579533\",\"name\":\"Q. V. Soranus\"},{\"authorId\":\"46288330\",\"name\":\"E. Courtney\"}],\"doi\":\"10.1093/OSEO/INSTANCE.00076674\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1000dfbfef78c593c401c1f15974a409885f2b86\",\"title\":\"2 = 4 M\",\"url\":\"https://www.semanticscholar.org/paper/1000dfbfef78c593c401c1f15974a409885f2b86\",\"venue\":\"\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1620879412\",\"name\":\"Adam S. Opalski\"},{\"authorId\":\"1617806812\",\"name\":\"Artur Ruszczak\"},{\"authorId\":\"1620876654\",\"name\":\"Yurii Promovych\"},{\"authorId\":\"1620871083\",\"name\":\"Micha\\u0142 Horka\"},{\"authorId\":\"1620871057\",\"name\":\"Ladislav Derzsi\"},{\"authorId\":\"1620871097\",\"name\":\"Piotr\"},{\"authorId\":\"1620877859\",\"name\":\"Garstecki\"}],\"doi\":\"10.1515/9783111697888-007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ae2118598c88a81d0bafcdc2d9706ff17944c307\",\"title\":\"E\",\"url\":\"https://www.semanticscholar.org/paper/ae2118598c88a81d0bafcdc2d9706ff17944c307\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50564384\",\"name\":\"G. Edwards\"},{\"authorId\":\"1830709\",\"name\":\"A. Lanitis\"},{\"authorId\":\"144482985\",\"name\":\"C. Taylor\"},{\"authorId\":\"7205190\",\"name\":\"T. Cootes\"}],\"doi\":\"10.1016/S0262-8856(97)00069-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14e013ca6a01c63e4fa4b4339d00463d0d11a617\",\"title\":\"Statistical models of face images - improving specificity\",\"url\":\"https://www.semanticscholar.org/paper/14e013ca6a01c63e4fa4b4339d00463d0d11a617\",\"venue\":\"Image Vis. Comput.\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"143739485\",\"name\":\"Wei Han\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"},{\"authorId\":\"2316043\",\"name\":\"Q. Huo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28f628af0c0521f574831b16b57941d8d75a6db8\",\"title\":\"Text Driven 3D Photo-Realistic Talking Head\",\"url\":\"https://www.semanticscholar.org/paper/28f628af0c0521f574831b16b57941d8d75a6db8\",\"venue\":\"INTERSPEECH\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1691713\",\"name\":\"H. Zen\"},{\"authorId\":\"2784449\",\"name\":\"Norbert Braunschweiler\"},{\"authorId\":\"1782178\",\"name\":\"S. Buchholz\"},{\"authorId\":\"1740397\",\"name\":\"M. Gales\"},{\"authorId\":\"145962472\",\"name\":\"K. Knill\"},{\"authorId\":\"2095209\",\"name\":\"S. Krstulovic\"},{\"authorId\":\"143953446\",\"name\":\"J. Latorre\"}],\"doi\":\"10.1109/TASL.2012.2187195\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0db09381c6fe288f5512161801d4d18460e875f9\",\"title\":\"Statistical Parametric Speech Synthesis Based on Speaker and Language Factorization\",\"url\":\"https://www.semanticscholar.org/paper/0db09381c6fe288f5512161801d4d18460e875f9\",\"venue\":\"IEEE Transactions on Audio, Speech, and Language Processing\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144549270\",\"name\":\"M. Brand\"}],\"doi\":\"10.1145/311535.311537\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2062c4359e57b22789cc38d0a97cc12acb930f43\",\"title\":\"Voice puppetry\",\"url\":\"https://www.semanticscholar.org/paper/2062c4359e57b22789cc38d0a97cc12acb930f43\",\"venue\":\"SIGGRAPH '99\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143953446\",\"name\":\"J. Latorre\"},{\"authorId\":\"2196928\",\"name\":\"V. Wan\"},{\"authorId\":\"1740397\",\"name\":\"M. Gales\"},{\"authorId\":\"1734070\",\"name\":\"L. Chen\"},{\"authorId\":\"145078403\",\"name\":\"K. K. Chin\"},{\"authorId\":\"145962472\",\"name\":\"K. Knill\"},{\"authorId\":\"1742578\",\"name\":\"M. Akamine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97665f65d497d10a563774457ddd543c1dd6caab\",\"title\":\"Speech factorization for HMM-TTS based on cluster adaptive training\",\"url\":\"https://www.semanticscholar.org/paper/97665f65d497d10a563774457ddd543c1dd6caab\",\"venue\":\"INTERSPEECH\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2532990\",\"name\":\"J. Melench\\u00f3n\"},{\"authorId\":\"1762748\",\"name\":\"Elisa Mart\\u00ednez Marroqu\\u00edn\"},{\"authorId\":\"143867160\",\"name\":\"F. Torre\"},{\"authorId\":\"145197816\",\"name\":\"J. Montero\"}],\"doi\":\"10.1109/TASL.2008.2010213\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"016c19a2470bb29b2078ad8b4f77daafe64752df\",\"title\":\"Emphatic Visual Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/016c19a2470bb29b2078ad8b4f77daafe64752df\",\"venue\":\"IEEE Transactions on Audio, Speech, and Language Processing\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144263130\",\"name\":\"Y. Chang\"},{\"authorId\":\"1932050\",\"name\":\"T. Ezzat\"}],\"doi\":\"10.1145/1073368.1073388\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e01d25596e542a621475075d4a5200365a9fa0d\",\"title\":\"Transferable videorealistic speech animation\",\"url\":\"https://www.semanticscholar.org/paper/9e01d25596e542a621475075d4a5200365a9fa0d\",\"venue\":\"SCA '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39918065\",\"name\":\"K. Liu\"},{\"authorId\":\"144835580\",\"name\":\"J. Ostermann\"}],\"doi\":\"10.1109/ICME.2011.6011835\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2139c62658455713c378adf65241c6cab2051c11\",\"title\":\"Realistic facial expression synthesis for an image-based talking head\",\"url\":\"https://www.semanticscholar.org/paper/2139c62658455713c378adf65241c6cab2051c11\",\"venue\":\"2011 IEEE International Conference on Multimedia and Expo\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1792288\",\"name\":\"D. Cosker\"},{\"authorId\":\"2687639\",\"name\":\"S. Paddock\"},{\"authorId\":\"144353457\",\"name\":\"A. D. Marshall\"},{\"authorId\":\"1734823\",\"name\":\"Paul L. Rosin\"},{\"authorId\":\"2261736\",\"name\":\"S. Rushton\"}],\"doi\":\"10.1145/1012551.1012579\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ef310e5f254dee6a896b7c71abface1c1afff10\",\"title\":\"Towards perceptually realistic talking heads: models, methods and McGurk\",\"url\":\"https://www.semanticscholar.org/paper/4ef310e5f254dee6a896b7c71abface1c1afff10\",\"venue\":\"APGV '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2880906\",\"name\":\"V. Blanz\"},{\"authorId\":\"152979813\",\"name\":\"T. Vetter\"}],\"doi\":\"10.1145/311535.311556\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ae0ef252d1b42df430ffb90724132040abf20341\",\"title\":\"A morphable model for the synthesis of 3D faces\",\"url\":\"https://www.semanticscholar.org/paper/ae0ef252d1b42df430ffb90724132040abf20341\",\"venue\":\"SIGGRAPH '99\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2016927\",\"name\":\"J. Tena\"},{\"authorId\":\"143867160\",\"name\":\"F. Torre\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.1145/2010324.1964971\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf9adbab2c9c5c8602ce9299a452e99719d94027\",\"title\":\"Interactive region-based linear 3D face models\",\"url\":\"https://www.semanticscholar.org/paper/cf9adbab2c9c5c8602ce9299a452e99719d94027\",\"venue\":\"SIGGRAPH 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46398261\",\"name\":\"K. Waters\"},{\"authorId\":\"1736506\",\"name\":\"Thoms M. Levergood\"}],\"doi\":\"10.1007/BF01215883\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c57daf6f5ddad7858461b132f6000debde84ca5\",\"title\":\"DECface: A system for synthetic face applications\",\"url\":\"https://www.semanticscholar.org/paper/4c57daf6f5ddad7858461b132f6000debde84ca5\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40476154\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"7565008\",\"name\":\"Xiaojun Qian\"},{\"authorId\":\"143739485\",\"name\":\"Wei Han\"},{\"authorId\":\"1705574\",\"name\":\"F. Soong\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f4b28ec6b8360eef6f81094979b0cf2041fbbb4\",\"title\":\"Photo-real lips synthesis with trajectory-guided sample selection\",\"url\":\"https://www.semanticscholar.org/paper/7f4b28ec6b8360eef6f81094979b0cf2041fbbb4\",\"venue\":\"SSW\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748270\",\"name\":\"Eftychios Sifakis\"},{\"authorId\":\"1714992\",\"name\":\"A. Selle\"},{\"authorId\":\"1405782503\",\"name\":\"Avram Lev Robinson-Mosher\"},{\"authorId\":\"1688994\",\"name\":\"Ronald Fedkiw\"}],\"doi\":\"10.2312/SCA/SCA06/261-270\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"18d6d8dc2ba095ea0d547001d8747ceff194f6c7\",\"title\":\"Simulating speech with a physics-based facial muscle model\",\"url\":\"https://www.semanticscholar.org/paper/18d6d8dc2ba095ea0d547001d8747ceff194f6c7\",\"venue\":\"SCA '06\",\"year\":2006}],\"title\":\"Expressive Visual Text-to-Speech Using Active Appearance Models\",\"topics\":[{\"topic\":\"Active appearance model\",\"topicId\":\"279595\",\"url\":\"https://www.semanticscholar.org/topic/279595\"},{\"topic\":\"Speech synthesis\",\"topicId\":\"12604\",\"url\":\"https://www.semanticscholar.org/topic/12604\"},{\"topic\":\"Usability testing\",\"topicId\":\"34160\",\"url\":\"https://www.semanticscholar.org/topic/34160\"},{\"topic\":\"Pose (computer vision)\",\"topicId\":\"774341\",\"url\":\"https://www.semanticscholar.org/topic/774341\"}],\"url\":\"https://www.semanticscholar.org/paper/2d8de70f089b581d386aaa0ced5f9421a3fd08c0\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013}\n"