"{\"abstract\":\"Recently, there has been growing interest in multi-speaker speech recognition, where the utterances of multiple speakers are recognized from their mixture. Promising techniques have been proposed for this task, but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning. In this paper, we propose a new sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner. We further propose a new objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses. Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences, achieving 83.1 % relative improvement compared to a model trained without the proposed objective. Interestingly, the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules.\",\"arxivId\":\"1805.05826\",\"authors\":[{\"authorId\":\"49166310\",\"name\":\"Hiroshi Seki\",\"url\":\"https://www.semanticscholar.org/author/49166310\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\",\"url\":\"https://www.semanticscholar.org/author/145443186\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\",\"url\":\"https://www.semanticscholar.org/author/1746678\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\",\"url\":\"https://www.semanticscholar.org/author/9332945\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\",\"url\":\"https://www.semanticscholar.org/author/2387467\"}],\"citationVelocity\":13,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"50135338\",\"name\":\"Wen-Jie Li\"},{\"authorId\":\"48754077\",\"name\":\"P. Zhang\"},{\"authorId\":\"145244226\",\"name\":\"Y. Yan\"}],\"doi\":\"10.21437/interspeech.2019-1692\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1be6aea7962045aefb28dfe6d983d71539b2d2a3\",\"title\":\"Target Speaker Recovery and Recognition Network with Average x-Vector and Global Training\",\"url\":\"https://www.semanticscholar.org/paper/1be6aea7962045aefb28dfe6d983d71539b2d2a3\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47294714\",\"name\":\"Zongpu Zhang\"},{\"authorId\":\"145147423\",\"name\":\"Tao Song\"},{\"authorId\":\"46456198\",\"name\":\"L. Lin\"},{\"authorId\":\"47029808\",\"name\":\"Y. Hua\"},{\"authorId\":\"7791948\",\"name\":\"Xufeng He\"},{\"authorId\":\"2022078\",\"name\":\"Zhengui Xue\"},{\"authorId\":\"1840514\",\"name\":\"R. Ma\"},{\"authorId\":\"145676478\",\"name\":\"H. Guan\"}],\"doi\":\"10.1109/TBDATA.2018.2880978\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"92c2c73f360aceb44f5dbef07ee8fd14e5adbfab\",\"title\":\"Towards Ubiquitous Intelligent Computing: Heterogeneous Distributed Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/92c2c73f360aceb44f5dbef07ee8fd14e5adbfab\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2008.04546\",\"authors\":[{\"authorId\":\"1833359\",\"name\":\"N. Kanda\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"2334648\",\"name\":\"Y. Gaur\"},{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"134905390\",\"name\":\"Zhong Meng\"},{\"authorId\":\"49865106\",\"name\":\"Z. Chen\"},{\"authorId\":\"46372473\",\"name\":\"T. Yoshioka\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8edfff49c93e457ab125418fc46a9e5b25535982\",\"title\":\"Investigation of End-To-End Speaker-Attributed ASR for Continuous Multi-Talker Recordings.\",\"url\":\"https://www.semanticscholar.org/paper/8edfff49c93e457ab125418fc46a9e5b25535982\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1971632\",\"name\":\"Uday Kamath\"},{\"authorId\":\"46700290\",\"name\":\"John Liu\"},{\"authorId\":\"79308542\",\"name\":\"James Whitaker\"}],\"doi\":\"10.1007/978-3-030-14596-5_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0dbc8256e2205aefa63fe503e50d37dfc1134cd5\",\"title\":\"Transfer Learning: Scenarios, Self-Taught Learning, and Multitask Learning\",\"url\":\"https://www.semanticscholar.org/paper/0dbc8256e2205aefa63fe503e50d37dfc1134cd5\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1690812\",\"name\":\"M. Delcroix\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"3038673\",\"name\":\"Tsubasa Ochiai\"},{\"authorId\":\"1899649\",\"name\":\"K. Kinoshita\"},{\"authorId\":\"39678391\",\"name\":\"Shigeki Karita\"},{\"authorId\":\"2555553\",\"name\":\"A. Ogawa\"},{\"authorId\":\"144805536\",\"name\":\"T. Nakatani\"}],\"doi\":\"10.21437/interspeech.2019-1856\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de\",\"title\":\"End-to-End SpeakerBeam for Single Channel Target Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"2006.02786\",\"authors\":[{\"authorId\":\"107917583\",\"name\":\"Thilo von Neumann\"},{\"authorId\":\"3364548\",\"name\":\"Christoph B\\u00f6ddeker\"},{\"authorId\":\"3353106\",\"name\":\"Lukas Drude\"},{\"authorId\":\"1899649\",\"name\":\"K. Kinoshita\"},{\"authorId\":\"1690812\",\"name\":\"M. Delcroix\"},{\"authorId\":\"144805536\",\"name\":\"T. Nakatani\"},{\"authorId\":\"1390103475\",\"name\":\"R. Haeb-Umbach\"}],\"doi\":\"10.21437/Interspeech.2020-2519\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3c998db6137dfbc6feaa5ae89d302344f00b9833\",\"title\":\"Multi-talker ASR for an unknown number of sources: Joint training of source counting, separation and ASR\",\"url\":\"https://www.semanticscholar.org/paper/3c998db6137dfbc6feaa5ae89d302344f00b9833\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1909.08103\",\"authors\":[{\"authorId\":\"1833359\",\"name\":\"N. Kanda\"},{\"authorId\":\"7180730\",\"name\":\"Shota Horiguchi\"},{\"authorId\":\"144307323\",\"name\":\"Y. Fujita\"},{\"authorId\":\"3173997\",\"name\":\"Yawen Xue\"},{\"authorId\":\"2694044\",\"name\":\"K. Nagamatsu\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/ASRU46091.2019.9004009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"adc273bd25ab1e2a66543f23c7a801af0dd80e5b\",\"title\":\"Simultaneous Speech Recognition and Speaker Diarization for Monaural Dialogue Recordings with Target-Speaker Acoustic Models\",\"url\":\"https://www.semanticscholar.org/paper/adc273bd25ab1e2a66543f23c7a801af0dd80e5b\",\"venue\":\"2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2019},{\"arxivId\":\"2011.02921\",\"authors\":[{\"authorId\":\"1833359\",\"name\":\"N. Kanda\"},{\"authorId\":\"134905390\",\"name\":\"Zhong Meng\"},{\"authorId\":\"152309490\",\"name\":\"Liang Lu\"},{\"authorId\":\"2334648\",\"name\":\"Y. Gaur\"},{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"49865106\",\"name\":\"Z. Chen\"},{\"authorId\":\"46372473\",\"name\":\"T. Yoshioka\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c9369029144e1815276c4a08751d4f87806b394a\",\"title\":\"Minimum Bayes Risk Training for End-to-End Speaker-Attributed ASR\",\"url\":\"https://www.semanticscholar.org/paper/c9369029144e1815276c4a08751d4f87806b394a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40902044\",\"name\":\"Aswin Shanmugam Subramanian\"},{\"authorId\":\"145350701\",\"name\":\"Chao Weng\"},{\"authorId\":\"143872259\",\"name\":\"M. Yu\"},{\"authorId\":\"2213494\",\"name\":\"S. Zhang\"},{\"authorId\":\"121983569\",\"name\":\"Yanchen Xu\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"114483585\",\"name\":\"Dong Yu\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053692\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c985600f0aa223ddc76a2ea628f1fa23504dcbcd\",\"title\":\"Far-Field Location Guided Target Speech Extraction Using End-to-End Speech Recognition Objectives\",\"url\":\"https://www.semanticscholar.org/paper/c985600f0aa223ddc76a2ea628f1fa23504dcbcd\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"1910.13934\",\"authors\":[{\"authorId\":\"3353106\",\"name\":\"Lukas Drude\"},{\"authorId\":\"10709286\",\"name\":\"Jens Heitkaemper\"},{\"authorId\":\"3364548\",\"name\":\"Christoph B\\u00f6ddeker\"},{\"authorId\":\"1390103475\",\"name\":\"R. Haeb-Umbach\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"5f2d6bae5ae7e8cbedddf2a99d66df669b5fd389\",\"title\":\"SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition\",\"url\":\"https://www.semanticscholar.org/paper/5f2d6bae5ae7e8cbedddf2a99d66df669b5fd389\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.11613\",\"authors\":[{\"authorId\":\"48545344\",\"name\":\"Lijie Huang\"},{\"authorId\":\"32607116\",\"name\":\"Gaofeng Cheng\"},{\"authorId\":\"48754077\",\"name\":\"P. Zhang\"},{\"authorId\":\"143907244\",\"name\":\"Yi Yang\"},{\"authorId\":\"50433250\",\"name\":\"Shumin Xu\"},{\"authorId\":\"72278707\",\"name\":\"Jia-song Sun\"}],\"doi\":\"10.1109/APSIPAASC47483.2019.9023163\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8cccae3a7cd4608f4daea3062b0f768d30d9c9da\",\"title\":\"Utterance-level Permutation Invariant Training with Latency-controlled BLSTM for Single-channel Multi-talker Speech Separation\",\"url\":\"https://www.semanticscholar.org/paper/8cccae3a7cd4608f4daea3062b0f768d30d9c9da\",\"venue\":\"2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)\",\"year\":2019},{\"arxivId\":\"2003.12687\",\"authors\":[{\"authorId\":\"1833359\",\"name\":\"N. Kanda\"},{\"authorId\":\"2334648\",\"name\":\"Y. Gaur\"},{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"134905390\",\"name\":\"Zhong Meng\"},{\"authorId\":\"46372473\",\"name\":\"T. Yoshioka\"}],\"doi\":\"10.21437/interspeech.2020-0999\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"24a9c0dc4ff52ee5ffb922f5fad200fc187f6a1c\",\"title\":\"Serialized Output Training for End-to-End Overlapped Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/24a9c0dc4ff52ee5ffb922f5fad200fc187f6a1c\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1908.04737\",\"authors\":[{\"authorId\":\"51151648\",\"name\":\"Pavel Denisov\"},{\"authorId\":\"4160376\",\"name\":\"Ngoc Thang Vu\"}],\"doi\":\"10.21437/interspeech.2019-1130\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6c6ecb9bb11dc07490d2e579d6c7e7e70622d7e9\",\"title\":\"End-to-End Multi-Speaker Speech Recognition using Speaker Embeddings and Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/6c6ecb9bb11dc07490d2e579d6c7e7e70622d7e9\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2027644477\",\"name\":\"Ayesha Shafiq\"},{\"authorId\":\"3271668\",\"name\":\"Humera Tariq\"},{\"authorId\":null,\"name\":\"Fareed Alvi\"},{\"authorId\":\"46203601\",\"name\":\"U. Amjad\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9583f586fbbe87a40256f0cba0c5dc60c53980d\",\"title\":\"Voice Recognition System Design Aspects for Robotic Car Control\",\"url\":\"https://www.semanticscholar.org/paper/b9583f586fbbe87a40256f0cba0c5dc60c53980d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"}],\"doi\":\"10.21437/interspeech.2020-2015\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e4047639948c4676861585c302431e41998a96e\",\"title\":\"Learning Contextual Language Embeddings for Monaural Multi-Talker Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9e4047639948c4676861585c302431e41998a96e\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"40902044\",\"name\":\"Aswin Shanmugam Subramanian\"},{\"authorId\":\"47871517\",\"name\":\"Pengcheng Guo\"},{\"authorId\":\"1826565368\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"1471364901\",\"name\":\"Yuya Fujita\"},{\"authorId\":\"35976156\",\"name\":\"M. Omachi\"}],\"doi\":\"10.21437/interspeech.2020-2816\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bef4548a43fca8a7410734e4200157d50e257a29\",\"title\":\"End-to-End ASR with Adaptive Span Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/bef4548a43fca8a7410734e4200157d50e257a29\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1910.06522\",\"authors\":[{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/ASRU46091.2019.9003986\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"94f8ef5944ecbdd0350d406bf3a16a7f2dff7349\",\"title\":\"MIMO-Speech: End-to-End Multi-Channel Multi-Speaker Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/94f8ef5944ecbdd0350d406bf3a16a7f2dff7349\",\"venue\":\"2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/TASLP.2020.2988423\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"87eece8d39d1e25ba87550be8b01af32738cbf2c\",\"title\":\"Improving End-to-End Single-Channel Multi-Talker Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/87eece8d39d1e25ba87550be8b01af32738cbf2c\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":\"2007.06123\",\"authors\":[{\"authorId\":\"74202107\",\"name\":\"Omkar Ranadive\"},{\"authorId\":\"1810712920\",\"name\":\"Grant Gasser\"},{\"authorId\":\"1811413070\",\"name\":\"David Terpay\"},{\"authorId\":\"2855322\",\"name\":\"P. Seetharaman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d1cef6398b7e379a50a3f738f338f27616addb0\",\"title\":\"OtoWorld: Towards Learning to Separate by Learning to Move\",\"url\":\"https://www.semanticscholar.org/paper/0d1cef6398b7e379a50a3f738f338f27616addb0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"}],\"doi\":\"10.21437/interspeech.2019-3192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bdfafbfaedb757f0d661b21cb9168ed9f01eeab9\",\"title\":\"Knowledge Distillation for End-to-End Monaural Multi-Talker ASR System\",\"url\":\"https://www.semanticscholar.org/paper/bdfafbfaedb757f0d661b21cb9168ed9f01eeab9\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1904.08248\",\"authors\":[{\"authorId\":\"32246134\",\"name\":\"Luca Pasa\"},{\"authorId\":\"80442851\",\"name\":\"Giovanni Morrone\"},{\"authorId\":\"1749312\",\"name\":\"Leonardo Badino\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054697\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ae87e4469c19dd47bd440a21865c0ab162142499\",\"title\":\"An Analysis of Speech Enhancement and Recognition Losses in Limited Resources Multi-Talker Single Channel Audio-Visual ASR\",\"url\":\"https://www.semanticscholar.org/paper/ae87e4469c19dd47bd440a21865c0ab162142499\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"1912.08462\",\"authors\":[{\"authorId\":\"107917583\",\"name\":\"Thilo von Neumann\"},{\"authorId\":\"1899649\",\"name\":\"K. Kinoshita\"},{\"authorId\":\"3353106\",\"name\":\"Lukas Drude\"},{\"authorId\":\"3364548\",\"name\":\"Christoph B\\u00f6ddeker\"},{\"authorId\":\"1690812\",\"name\":\"M. Delcroix\"},{\"authorId\":\"144805536\",\"name\":\"T. Nakatani\"},{\"authorId\":\"1390103475\",\"name\":\"R. Haeb-Umbach\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053461\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f84d4dc1b7f09233db5c299268c6097441f51475\",\"title\":\"End-to-End Training of Time Domain Audio Separation and Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f84d4dc1b7f09233db5c299268c6097441f51475\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"1910.10279\",\"authors\":[{\"authorId\":\"34686558\",\"name\":\"M. Maciejewski\"},{\"authorId\":\"1816785\",\"name\":\"G. Wichern\"},{\"authorId\":\"2584132\",\"name\":\"E. McQuinn\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053327\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dfb410dcb4ed17358bbc331671c609b86e032b76\",\"title\":\"WHAMR!: Noisy and Reverberant Single-Channel Speech Separation\",\"url\":\"https://www.semanticscholar.org/paper/dfb410dcb4ed17358bbc331671c609b86e032b76\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145183069\",\"name\":\"H. Seki\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.21437/INTERSPEECH.2019-3038\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f9e3b7c6ca7d534694148bd0c7c37c1ef896a784\",\"title\":\"End-to-End Multilingual Multi-Speaker Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f9e3b7c6ca7d534694148bd0c7c37c1ef896a784\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3404556\",\"name\":\"Neil Zeghidour\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"035338c77af2cd927444eac8afe09a3ddc2ed205\",\"title\":\"Learning representations of speech from the raw waveform. (Apprentissage de repr\\u00e9sentations de la parole \\u00e0 partir du signal brut)\",\"url\":\"https://www.semanticscholar.org/paper/035338c77af2cd927444eac8afe09a3ddc2ed205\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.13006\",\"authors\":[{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"1380254932\",\"name\":\"F. Boyer\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"47871517\",\"name\":\"Pengcheng Guo\"},{\"authorId\":\"3326124\",\"name\":\"T. Hayashi\"},{\"authorId\":\"2040708369\",\"name\":\"Yosuke Higuchi\"},{\"authorId\":\"49661716\",\"name\":\"T. Hori\"},{\"authorId\":\"50526276\",\"name\":\"Wen-Chin Huang\"},{\"authorId\":\"49276525\",\"name\":\"Hirofumi Inaguma\"},{\"authorId\":\"2003740453\",\"name\":\"Naoyuki Kamo\"},{\"authorId\":\"39678391\",\"name\":\"Shigeki Karita\"},{\"authorId\":\"1391213316\",\"name\":\"Chenda Li\"},{\"authorId\":\"1739277022\",\"name\":\"Jing Shi\"},{\"authorId\":\"40902044\",\"name\":\"Aswin Shanmugam Subramanian\"},{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"659b476a10b0e676a031b1b17ebfe405c1904227\",\"title\":\"The 2020 ESPnet update: new features, broadened applications, performance improvements, and future plans\",\"url\":\"https://www.semanticscholar.org/paper/659b476a10b0e676a031b1b17ebfe405c1904227\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32246134\",\"name\":\"Luca Pasa\"},{\"authorId\":\"80442851\",\"name\":\"Giovanni Morrone\"},{\"authorId\":\"1749312\",\"name\":\"Leonardo Badino\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ae87e4469c19dd47bd440a21865c0ab162142499\",\"title\":\"Joined Audio-Visual Speech Enhancement and Recognition in the Cocktail Party: The Tug Of War Between Enhancement and Recognition Losses\",\"url\":\"https://www.semanticscholar.org/paper/ae87e4469c19dd47bd440a21865c0ab162142499\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.10876\",\"authors\":[{\"authorId\":\"1833359\",\"name\":\"N. Kanda\"},{\"authorId\":\"7180730\",\"name\":\"Shota Horiguchi\"},{\"authorId\":\"3128064\",\"name\":\"R. Takashima\"},{\"authorId\":\"144307323\",\"name\":\"Y. Fujita\"},{\"authorId\":\"2694044\",\"name\":\"K. Nagamatsu\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.21437/interspeech.2019-1126\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"388d41b99c9c0867301f345c65877a2796225ead\",\"title\":\"Auxiliary Interference Speaker Loss for Target-Speaker Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/388d41b99c9c0867301f345c65877a2796225ead\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1811.02062\",\"authors\":[{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"},{\"authorId\":\"1736727\",\"name\":\"Kai Yu\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/ICASSP.2019.8682822\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"649eb9fd9a18f9601270b7fcde8d6548bfc6ec75\",\"title\":\"End-to-end Monaural Multi-speaker ASR System without Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/649eb9fd9a18f9601270b7fcde8d6548bfc6ec75\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"2006.10930\",\"authors\":[{\"authorId\":\"1833359\",\"name\":\"N. Kanda\"},{\"authorId\":\"2334648\",\"name\":\"Y. Gaur\"},{\"authorId\":\"50142428\",\"name\":\"X. Wang\"},{\"authorId\":\"134905390\",\"name\":\"Zhong Meng\"},{\"authorId\":\"49865106\",\"name\":\"Z. Chen\"},{\"authorId\":\"1500653659\",\"name\":\"Tianyan Zhou\"},{\"authorId\":\"46372473\",\"name\":\"T. Yoshioka\"}],\"doi\":\"10.21437/interspeech.2020-1085\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a51e579678563da16ff17ad5fd028553a17f9d94\",\"title\":\"Joint Speaker Counting, Speech Recognition, and Speaker Identification for Overlapped Speech of Any Number of Speakers\",\"url\":\"https://www.semanticscholar.org/paper/a51e579678563da16ff17ad5fd028553a17f9d94\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500649019\",\"name\":\"Anshuman Tripathi\"},{\"authorId\":\"1500648246\",\"name\":\"Han Lu\"},{\"authorId\":\"2670103\",\"name\":\"H. Sak\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054328\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d7dcf644b4941a1996cf06e406e8f9b1054cd97\",\"title\":\"End-To-End Multi-Talker Overlapping Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2d7dcf644b4941a1996cf06e406e8f9b1054cd97\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2005.10479\",\"authors\":[{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"},{\"authorId\":\"40902044\",\"name\":\"Aswin Shanmugam Subramanian\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"}],\"doi\":\"10.21437/interspeech.2020-2432\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac7dfef83b093bdfdbdd86b23f15f86df7c625b9\",\"title\":\"End-to-End Far-Field Speech Recognition with Unified Dereverberation and Beamforming\",\"url\":\"https://www.semanticscholar.org/paper/ac7dfef83b093bdfdbdd86b23f15f86df7c625b9\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2002.03921\",\"authors\":[{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"1390725481\",\"name\":\"Wangyou Zhang\"},{\"authorId\":\"148407193\",\"name\":\"Yanmin Qian\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054029\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"201b79be15b6b01e62a82b29ac4d30d3e6a11799\",\"title\":\"End-To-End Multi-Speaker Speech Recognition With Transformer\",\"url\":\"https://www.semanticscholar.org/paper/201b79be15b6b01e62a82b29ac4d30d3e6a11799\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"1905.03500\",\"authors\":[{\"authorId\":\"47935908\",\"name\":\"T. Menne\"},{\"authorId\":\"4413385\",\"name\":\"I. Sklyar\"},{\"authorId\":\"144490010\",\"name\":\"R. Schl\\u00fcter\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.21437/Interspeech.2019-1728\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ad5d4ed726c1deb8b5aea3f3e508bc2ae1656998\",\"title\":\"Analysis of Deep Clustering as Preprocessing for Automatic Speech Recognition of Sparsely Overlapping Speech\",\"url\":\"https://www.semanticscholar.org/paper/ad5d4ed726c1deb8b5aea3f3e508bc2ae1656998\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"2011.11671\",\"authors\":[{\"authorId\":\"4413385\",\"name\":\"I. Sklyar\"},{\"authorId\":\"1396766299\",\"name\":\"A. Piunova\"},{\"authorId\":\"2027163265\",\"name\":\"Yulan Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7ea99e376425f4f2a08fed86c54b5b54efff7ad\",\"title\":\"Streaming Multi-speaker ASR with RNN-T\",\"url\":\"https://www.semanticscholar.org/paper/a7ea99e376425f4f2a08fed86c54b5b54efff7ad\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.01340\",\"authors\":[{\"authorId\":\"3353106\",\"name\":\"Lukas Drude\"},{\"authorId\":\"1411459644\",\"name\":\"Daniel Hasenklever\"},{\"authorId\":\"1390103475\",\"name\":\"R. Haeb-Umbach\"}],\"doi\":\"10.1109/ICASSP.2019.8683520\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"08262def29a23604efc0afd5c5251d8ce9fe9d49\",\"title\":\"Unsupervised Training of a Deep Clustering Model for Multichannel Blind Source Separation\",\"url\":\"https://www.semanticscholar.org/paper/08262def29a23604efc0afd5c5251d8ce9fe9d49\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48900508\",\"name\":\"R. Aihara\"},{\"authorId\":\"1816785\",\"name\":\"G. Wichern\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"}],\"doi\":\"10.1250/ast.41.465\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b96d382bc833098f6a095308af9ada799adcb81\",\"title\":\"Deep clustering-based single-channel speech separation and recent advances\",\"url\":\"https://www.semanticscholar.org/paper/1b96d382bc833098f6a095308af9ada799adcb81\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.05958\",\"authors\":[{\"authorId\":\"14352702\",\"name\":\"Jisi Zhang\"},{\"authorId\":\"146453319\",\"name\":\"Catalin Zorila\"},{\"authorId\":\"2538575\",\"name\":\"Rama Doddipatla\"},{\"authorId\":\"32406400\",\"name\":\"J. Barker\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053833\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"b7b0ac9edbc52d4a2637cb425c1376b9ffcdf275\",\"title\":\"On End-to-end Multi-channel Time Domain Speech Separation in Reverberant Environments\",\"url\":\"https://www.semanticscholar.org/paper/b7b0ac9edbc52d4a2637cb425c1376b9ffcdf275\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2006.14150\",\"authors\":[{\"authorId\":\"145749362\",\"name\":\"Jing Shi\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"47871517\",\"name\":\"Pengcheng Guo\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"144307323\",\"name\":\"Y. Fujita\"},{\"authorId\":\"46372563\",\"name\":\"Jiaming Xu\"},{\"authorId\":\"46798766\",\"name\":\"Bo Xu\"},{\"authorId\":\"144206960\",\"name\":\"Lei Xie\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1\",\"title\":\"Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals\",\"url\":\"https://www.semanticscholar.org/paper/e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":21673427,\"doi\":\"10.18653/v1/P18-1244\",\"fieldsOfStudy\":[\"Computer Science\",\"Engineering\",\"Mathematics\"],\"influentialCitationCount\":4,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"3400b8bf1ffde3ef3d35dfcea893e6506427aa21\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":null,\"name\":\"Jan Chorowski\"},{\"authorId\":null,\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":null,\"name\":\"Philemon Brakel\"},{\"authorId\":null,\"name\":\"Yoshua Bengio.\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Endto-end attention-based large vocabulary speech recognition\",\"url\":\"\",\"venue\":\"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"143913738\",\"name\":\"S. Fern\\u00e1ndez\"},{\"authorId\":\"145842938\",\"name\":\"F. Gomez\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1145/1143844.1143891\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"96494e722f58705fa20302fe6179d483f52705b4\",\"title\":\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/96494e722f58705fa20302fe6179d483f52705b4\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":\"1706.02737\",\"authors\":[{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"1682848\",\"name\":\"Y. Zhang\"},{\"authorId\":\"144333684\",\"name\":\"William Chan\"}],\"doi\":\"10.21437/INTERSPEECH.2017-1296\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186\",\"title\":\"Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM\",\"url\":\"https://www.semanticscholar.org/paper/c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":\"1707.06527\",\"authors\":[{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"}],\"doi\":\"10.1016/j.specom.2018.09.003\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"29a795013c995ca55ac8205e7c02a19f203f7727\",\"title\":\"Single-Channel Multi-talker Speech Recognition with Permutation Invariant Training\",\"url\":\"https://www.semanticscholar.org/paper/29a795013c995ca55ac8205e7c02a19f203f7727\",\"venue\":\"Speech Commun.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1792214\",\"name\":\"D. Povey\"},{\"authorId\":\"2268620\",\"name\":\"A. Ghoshal\"},{\"authorId\":\"2541218\",\"name\":\"Gilles Boulianne\"},{\"authorId\":\"1816892\",\"name\":\"L. Burget\"},{\"authorId\":\"3075141\",\"name\":\"O. Glembek\"},{\"authorId\":\"46356878\",\"name\":\"N. Goel\"},{\"authorId\":\"2592983\",\"name\":\"M. Hannemann\"},{\"authorId\":\"2745667\",\"name\":\"Petr Motl\\u00edcek\"},{\"authorId\":\"2480051\",\"name\":\"Yanmin Qian\"},{\"authorId\":\"35455336\",\"name\":\"P. Schwarz\"},{\"authorId\":\"3330139\",\"name\":\"J. Silovsk\\u00fd\"},{\"authorId\":\"1708033\",\"name\":\"G. Stemmer\"},{\"authorId\":\"2459598\",\"name\":\"K. Vesel\\u00fd\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a1a2cff2b70fb84a7ca7d97f8adcc5855851795\",\"title\":\"The Kaldi Speech Recognition Toolkit\",\"url\":\"https://www.semanticscholar.org/paper/3a1a2cff2b70fb84a7ca7d97f8adcc5855851795\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2912546\",\"name\":\"K. Maekawa\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3d2723e9d349ee260aea71cc27e26dfc0633e12f\",\"title\":\"CORPUS OF SPONTANEOUS JAPANESE : ITS DESIGN AND EVALUATION\",\"url\":\"https://www.semanticscholar.org/paper/3d2723e9d349ee260aea71cc27e26dfc0633e12f\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Garofalo\"},{\"authorId\":null,\"name\":\"David Graff\"},{\"authorId\":null,\"name\":\"Doug Paul\"},{\"authorId\":null,\"name\":\"David Pallett.\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CSR-I (wsj0) complete\",\"url\":\"\",\"venue\":\"Linguistic Data Consortium, Philadelphia, LDC93S6A.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Santiago Fern\\u00e1ndez\"},{\"authorId\":null,\"name\":\"Faustino Gomez\"},{\"authorId\":null,\"name\":\"J\\u00fcrgen Schmidhuber\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CSR - I ( wsj 0 ) complete\",\"url\":\"\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26433348\",\"name\":\"Morten Kolbaek\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"},{\"authorId\":\"1709835\",\"name\":\"Z. Tan\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":\"10.1109/TASLP.2017.2726762\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9451afc942f1f46a32079cedfd74bb4488e364ad\",\"title\":\"Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/9451afc942f1f46a32079cedfd74bb4488e364ad\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2017},{\"arxivId\":\"1506.07503\",\"authors\":[{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1862138\",\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b624504240fa52ab76167acfe3156150ca01cf3b\",\"title\":\"Attention-Based Models for Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/b624504240fa52ab76167acfe3156150ca01cf3b\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3117618\",\"name\":\"Seiya Tokui\"},{\"authorId\":\"1812144\",\"name\":\"Kenta Oono\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"67156902beca9bc90b728c8d5dd4ac9d8b27d3a3\",\"title\":\"Chainer : a Next-Generation Open Source Framework for Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/67156902beca9bc90b728c8d5dd4ac9d8b27d3a3\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Razvan Pascanu\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":null,\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kaldi recipe for Japanese spontaneous speech recognition and its evaluation\",\"url\":\"\",\"venue\":\"tumn Meeting of ASJ , 3 - Q7 .\",\"year\":2015},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40059964\",\"name\":\"Shane Settle\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.1109/ICASSP.2018.8461893\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2225950d1d3e02bc0d88a0c78325d00e0122b576\",\"title\":\"End-to-End Multi-Speaker Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2225950d1d3e02bc0d88a0c78325d00e0122b576\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":\"1607.02173\",\"authors\":[{\"authorId\":\"48427563\",\"name\":\"Yusuf Isik\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"145718850\",\"name\":\"Zhuo Chen\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.21437/Interspeech.2016-1176\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab94fae3d49cd7016a47020469dc257d8090f5bb\",\"title\":\"Single-Channel Multi-Speaker Separation Using Deep Clustering\",\"url\":\"https://www.semanticscholar.org/paper/ab94fae3d49cd7016a47020469dc257d8090f5bb\",\"venue\":\"INTERSPEECH\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Takafumi Moriya\"},{\"authorId\":null,\"name\":\"Takahiro Shinozaki\"},{\"authorId\":null,\"name\":\"Shinji Watanabe.\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kaldi recipe for Japanese spontaneous speech recognition and its evaluation\",\"url\":\"\",\"venue\":\"Autumn Meeting of ASJ, 3-Q-7.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143967982\",\"name\":\"M. Cooke\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"},{\"authorId\":\"2071376\",\"name\":\"Steven J. Rennie\"}],\"doi\":\"10.1016/j.csl.2009.02.006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7032a6e6540183bac5bb5321c808df37ddcd7722\",\"title\":\"Monaural speech separation and recognition challenge\",\"url\":\"https://www.semanticscholar.org/paper/7032a6e6540183bac5bb5321c808df37ddcd7722\",\"venue\":\"Comput. Speech Lang.\",\"year\":2010},{\"arxivId\":\"1212.5701\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8729441d734782c3ed532a7d2d9611b438c0a09a\",\"title\":\"ADADELTA: An Adaptive Learning Rate Method\",\"url\":\"https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.18653/v1/P17-1048\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4cd66273298128dfb5be290e891870085ecfc455\",\"title\":\"Joint CTC/attention decoding for end-to-end speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/4cd66273298128dfb5be290e891870085ecfc455\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":\"1609.06773\",\"authors\":[{\"authorId\":\"2678842\",\"name\":\"Suyoun Kim\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/ICASSP.2017.7953075\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9af2264799bdc3490e4650e2f5d126762caf420f\",\"title\":\"Joint CTC-attention based end-to-end speech recognition using multi-task learning\",\"url\":\"https://www.semanticscholar.org/paper/9af2264799bdc3490e4650e2f5d126762caf420f\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Linguistic Data Consortium.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CSR-II (wsj1) complete\",\"url\":\"\",\"venue\":\"Linguistic Data Consortium, Philadelphia, LDC94S13A.\",\"year\":1994},{\"arxivId\":\"1607.00325\",\"authors\":[{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"},{\"authorId\":\"3437492\",\"name\":\"Morten Kolb\\u00e6k\"},{\"authorId\":\"71668001\",\"name\":\"Zheng-Hua Tan\"},{\"authorId\":\"145416680\",\"name\":\"J. Jensen\"}],\"doi\":\"10.1109/ICASSP.2017.7952154\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d4f62ffbf7c51a5ad01b89c6889c649bf48baac8\",\"title\":\"Permutation invariant training of deep models for speaker-independent multi-talker speech separation\",\"url\":\"https://www.semanticscholar.org/paper/d4f62ffbf7c51a5ad01b89c6889c649bf48baac8\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1211.5063\",\"authors\":[{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"84069287da0a6b488b8c933f3cb5be759cb6237e\",\"title\":\"On the difficulty of training recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/84069287da0a6b488b8c933f3cb5be759cb6237e\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martin Cooke\"},{\"authorId\":null,\"name\":\"John R Hershey\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CSR - II ( wsj 1 ) complete\",\"url\":\"\",\"venue\":\"\",\"year\":1994},{\"arxivId\":\"1611.08930\",\"authors\":[{\"authorId\":\"145718850\",\"name\":\"Zhuo Chen\"},{\"authorId\":\"145714738\",\"name\":\"Yi Luo\"},{\"authorId\":\"1686269\",\"name\":\"Nima Mesgarani\"}],\"doi\":\"10.1109/ICASSP.2017.7952155\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5c46e8c6dbfac2fb298e592b74057b372917695c\",\"title\":\"Deep attractor network for single-microphone speaker separation\",\"url\":\"https://www.semanticscholar.org/paper/5c46e8c6dbfac2fb298e592b74057b372917695c\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1710.11351\",\"authors\":[{\"authorId\":\"2859858\",\"name\":\"Takuya Akiba\"},{\"authorId\":\"1958395\",\"name\":\"K. Fukuda\"},{\"authorId\":\"49549590\",\"name\":\"S. Suzuki\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cab01c156a38cc60e6ece6db211bef2d2629fd15\",\"title\":\"ChainerMN: Scalable Distributed Deep Learning Framework\",\"url\":\"https://www.semanticscholar.org/paper/cab01c156a38cc60e6ece6db211bef2d2629fd15\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1707.07048\",\"authors\":[{\"authorId\":\"2188489\",\"name\":\"Zhehuai Chen\"},{\"authorId\":\"1755472\",\"name\":\"J. Droppo\"},{\"authorId\":\"145364697\",\"name\":\"J. Li\"},{\"authorId\":\"8786439\",\"name\":\"Wayne Xiong\"}],\"doi\":\"10.1109/TASLP.2017.2765834\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5cad89b1b74e86d3da754699cfeda7a521715a2f\",\"title\":\"Progressive Joint Modeling in Unsupervised Single-Channel Overlapped Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5cad89b1b74e86d3da754699cfeda7a521715a2f\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.1109/ASRU.2017.8268948\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e1a20480e4168d58deec743035b7ff02720672d7\",\"title\":\"Multi-level language modeling and decoding for open vocabulary end-to-end speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/e1a20480e4168d58deec743035b7ff02720672d7\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":\"1508.04395\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"1862138\",\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":\"2616163\",\"name\":\"Philemon Brakel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.1109/ICASSP.2016.7472618\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"878ba5458e9e51f0b341fd9117fa0b43ef4096d3\",\"title\":\"End-to-end attention-based large vocabulary speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/878ba5458e9e51f0b341fd9117fa0b43ef4096d3\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016},{\"arxivId\":\"1508.04306\",\"authors\":[{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"},{\"authorId\":\"49865106\",\"name\":\"Z. Chen\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"}],\"doi\":\"10.1109/ICASSP.2016.7471631\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3332dc72fbe3907e45e8a500c6a1202ad5092c0f\",\"title\":\"Deep clustering: Discriminative embeddings for segmentation and separation\",\"url\":\"https://www.semanticscholar.org/paper/3332dc72fbe3907e45e8a500c6a1202ad5092c0f\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016}],\"title\":\"A Purely End-to-end System for Multi-speaker Speech Recognition\",\"topics\":[{\"topic\":\"Speech recognition\",\"topicId\":\"2869\",\"url\":\"https://www.semanticscholar.org/topic/2869\"},{\"topic\":\"Broadway (microprocessor)\",\"topicId\":\"44552\",\"url\":\"https://www.semanticscholar.org/topic/44552\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"End system\",\"topicId\":\"517951\",\"url\":\"https://www.semanticscholar.org/topic/517951\"},{\"topic\":\"Source separation\",\"topicId\":\"39125\",\"url\":\"https://www.semanticscholar.org/topic/39125\"},{\"topic\":\"Acknowledgment index\",\"topicId\":\"4975706\",\"url\":\"https://www.semanticscholar.org/topic/4975706\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Finite-state machine\",\"topicId\":\"4280\",\"url\":\"https://www.semanticscholar.org/topic/4280\"},{\"topic\":\"Digital Monster (virtual pet)\",\"topicId\":\"768511\",\"url\":\"https://www.semanticscholar.org/topic/768511\"},{\"topic\":\"Computational linguistics\",\"topicId\":\"26807\",\"url\":\"https://www.semanticscholar.org/topic/26807\"},{\"topic\":\"Hidden Markov model\",\"topicId\":\"11239\",\"url\":\"https://www.semanticscholar.org/topic/11239\"},{\"topic\":\"Loss function\",\"topicId\":\"3650\",\"url\":\"https://www.semanticscholar.org/topic/3650\"}],\"url\":\"https://www.semanticscholar.org/paper/3400b8bf1ffde3ef3d35dfcea893e6506427aa21\",\"venue\":\"ACL\",\"year\":2018}\n"