"{\"abstract\":\"We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\",\"url\":\"https://www.semanticscholar.org/author/48267618\"},{\"authorId\":\"145534769\",\"name\":\"N. Ding\",\"url\":\"https://www.semanticscholar.org/author/145534769\"},{\"authorId\":\"7685850\",\"name\":\"Sebastian Goodman\",\"url\":\"https://www.semanticscholar.org/author/7685850\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\",\"url\":\"https://www.semanticscholar.org/author/1737285\"}],\"citationVelocity\":60,\"citations\":[{\"arxivId\":\"1906.00378\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1609/AAAI.V33I01.33018207\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5822bc0001db30a5e8f5b7386e9f24e04003995\",\"title\":\"Unsupervised Bilingual Lexicon Induction from Mono-lingual Multimodal Data\",\"url\":\"https://www.semanticscholar.org/paper/c5822bc0001db30a5e8f5b7386e9f24e04003995\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2012.04638\",\"authors\":[{\"authorId\":\"21518096\",\"name\":\"Zhengyuan Yang\"},{\"authorId\":\"38534822\",\"name\":\"Y. Lu\"},{\"authorId\":\"46583994\",\"name\":\"Jianfeng Wang\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"1882479\",\"name\":\"D. Flor\\u00eancio\"},{\"authorId\":\"30602591\",\"name\":\"Lijuan Wang\"},{\"authorId\":\"1706673\",\"name\":\"C. Zhang\"},{\"authorId\":\"145637095\",\"name\":\"Lei Zhang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8deceb13cb3afcfbaab06a2c655f1935445635fe\",\"title\":\"TAP: Text-Aware Pre-training for Text-VQA and Text-Caption\",\"url\":\"https://www.semanticscholar.org/paper/8deceb13cb3afcfbaab06a2c655f1935445635fe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00908\",\"authors\":[{\"authorId\":\"2715920\",\"name\":\"Malihe Alikhani\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"122615952\",\"name\":\"Sheng-Jie Li\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"48060050\",\"name\":\"M. Stone\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7cc8205ddf8211d6ae2fce03de2953a4b978b66a\",\"title\":\"Clue: Cross-modal Coherence Modeling for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/7cc8205ddf8211d6ae2fce03de2953a4b978b66a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1980490557\",\"name\":\"Andrea Spreafico\"},{\"authorId\":\"1825424\",\"name\":\"G. Carenini\"}],\"doi\":\"10.1145/3399715.3399829\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"16ddaab1cb94e9307ef854727319299aeec04484\",\"title\":\"Neural Data-Driven Captioning of Time-Series Line Charts\",\"url\":\"https://www.semanticscholar.org/paper/16ddaab1cb94e9307ef854727319299aeec04484\",\"venue\":\"AVI\",\"year\":2020},{\"arxivId\":\"2004.00849\",\"authors\":[{\"authorId\":\"47272083\",\"name\":\"Zhicheng Huang\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"1453953482\",\"name\":\"Bei Liu\"},{\"authorId\":\"143890169\",\"name\":\"Dongmei Fu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c188874316557d501369e611a96cafc8058dffa\",\"title\":\"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers\",\"url\":\"https://www.semanticscholar.org/paper/5c188874316557d501369e611a96cafc8058dffa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123645165\",\"name\":\"Meng-Jiun Chiou\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"88b71ba7edccd052e1bc4e4c751a0da0b1aa5e24\",\"title\":\"RVL-BERT: Visual Relationship Detection with Visual-Linguistic Knowledge from Pre-trained Representations\",\"url\":\"https://www.semanticscholar.org/paper/88b71ba7edccd052e1bc4e4c751a0da0b1aa5e24\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.06884\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1753617513\",\"name\":\"Tan Wang\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1704030\",\"name\":\"J. Zhu\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413518\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"080ee4e93438f8b8cbdd894eef15af71f0c30097\",\"title\":\"DeVLBert: Learning Deconfounded Visio-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/080ee4e93438f8b8cbdd894eef15af71f0c30097\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1910.14671\",\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1743587c272c36dbda0adc50496bf7f34b8148f1\",\"title\":\"TAB-VCR: Tags and Attributes based Visual Commonsense Reasoning Baselines\",\"url\":\"https://www.semanticscholar.org/paper/1743587c272c36dbda0adc50496bf7f34b8148f1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.08530\",\"authors\":[{\"authorId\":\"145499378\",\"name\":\"Weijie Su\"},{\"authorId\":\"2578924\",\"name\":\"X. Zhu\"},{\"authorId\":\"47746274\",\"name\":\"Y. Cao\"},{\"authorId\":\"48218753\",\"name\":\"B. Li\"},{\"authorId\":\"152309485\",\"name\":\"Lewei Lu\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"3304536\",\"name\":\"Jifeng Dai\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2527626c11a84f15709e943fbfa2356e19930e3b\",\"title\":\"VL-BERT: Pre-training of Generic Visual-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/2527626c11a84f15709e943fbfa2356e19930e3b\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52109694\",\"name\":\"N. Oostdijk\"},{\"authorId\":\"2165139\",\"name\":\"H. Halteren\"},{\"authorId\":\"150980784\",\"name\":\"Erkan Basar\"},{\"authorId\":\"104810482\",\"name\":\"M. Larson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e136ef615748a3287601fe890c05b547c064e952\",\"title\":\"The Connection between the Text and Images of News Articles: New Insights for Multimedia Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e136ef615748a3287601fe890c05b547c064e952\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"145558284\",\"name\":\"Kai Lei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4eb0b7ced6c022eef4c6fb2ed3dcdbdccfc056dc\",\"title\":\"Aligning Visual Regions and Textual Concepts: Learning Fine-Grained Image Representations for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4eb0b7ced6c022eef4c6fb2ed3dcdbdccfc056dc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2003.12058\",\"authors\":[{\"authorId\":\"4055152\",\"name\":\"Sarah Pratt\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"20745881\",\"name\":\"Luca Weihs\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2684226\",\"name\":\"Aniruddha Kembhavi\"}],\"doi\":\"10.1007/978-3-030-58548-8_19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc261c0efb5f9ce82581932d1440630b861fb85f\",\"title\":\"Grounded Situation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/fc261c0efb5f9ce82581932d1440630b861fb85f\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"145303641\",\"name\":\"Yu Xing\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"145201124\",\"name\":\"Xiang Ren\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b7cb2e9d884427ef50b564d97d3fd403953afa6\",\"title\":\"COMMONGEN: Towards Generative Commonsense Reasoning via A Constrained Text Generation Challenge\",\"url\":\"https://www.semanticscholar.org/paper/8b7cb2e9d884427ef50b564d97d3fd403953afa6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2839437\",\"name\":\"T. Arnold\"},{\"authorId\":\"51033450\",\"name\":\"L. Tilton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f83f8097d24c8df4570cc04aceeb9015331b6839\",\"title\":\"Enriching Historic Photography with Structured Data using Image Region Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/f83f8097d24c8df4570cc04aceeb9015331b6839\",\"venue\":\"AI4HI\",\"year\":2020},{\"arxivId\":\"2006.08335\",\"authors\":[{\"authorId\":\"1750913684\",\"name\":\"Bofan Xue\"},{\"authorId\":\"1774825\",\"name\":\"D. Chan\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5ca80f5097b6ad4f4537d913c48abc470fb37342\",\"title\":\"A Dataset and Benchmarks for Multimedia Social Analysis\",\"url\":\"https://www.semanticscholar.org/paper/5ca80f5097b6ad4f4537d913c48abc470fb37342\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.02356\",\"authors\":[{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72be4e0750cf5591d527d7792aa861353526e311\",\"title\":\"Self-Supervised VQA: Answering Visual Questions using Images and Captions\",\"url\":\"https://www.semanticscholar.org/paper/72be4e0750cf5591d527d7792aa861353526e311\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143804033\",\"name\":\"K. Ueki\"},{\"authorId\":\"2495278\",\"name\":\"Takayuki Hori\"},{\"authorId\":\"1709528\",\"name\":\"T. Kobayashi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dbb2e25ed8ace254327c4653f80ab99bc1a8131b\",\"title\":\"Waseda_Meisei_SoftBank at TRECVID 2019: Ad-hoc Video Search\",\"url\":\"https://www.semanticscholar.org/paper/dbb2e25ed8ace254327c4653f80ab99bc1a8131b\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"2010.12831\",\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"30156979\",\"name\":\"Haoxuan You\"},{\"authorId\":\"2513111\",\"name\":\"Zhecan Wang\"},{\"authorId\":\"2778637\",\"name\":\"Alireza Zareian\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"101751639\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"16f82c1dae2f6e5149c1be95165d7081f08298b6\",\"title\":\"Weakly-supervised VisualBERT: Pre-training without Parallel Images and Captions\",\"url\":\"https://www.semanticscholar.org/paper/16f82c1dae2f6e5149c1be95165d7081f08298b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.00145\",\"authors\":[{\"authorId\":\"3381900\",\"name\":\"E. Dodds\"},{\"authorId\":\"31922487\",\"name\":\"J. Culpepper\"},{\"authorId\":\"80236158\",\"name\":\"Simao Herdade\"},{\"authorId\":\"29969244\",\"name\":\"Y. Zhang\"},{\"authorId\":\"145908678\",\"name\":\"K. Boakye\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e09df55b9aaf6e81b210815106d5ea075e3aaad0\",\"title\":\"Modality-Agnostic Attention Fusion for visual search with text feedback\",\"url\":\"https://www.semanticscholar.org/paper/e09df55b9aaf6e81b210815106d5ea075e3aaad0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3429472\",\"name\":\"Philipp Blandfort\"},{\"authorId\":\"2675822\",\"name\":\"Tushar Karayil\"},{\"authorId\":\"1473240224\",\"name\":\"J\\u00f6rn Hees\"},{\"authorId\":\"145279674\",\"name\":\"A. Dengel\"}],\"doi\":\"10.1007/s13735-019-00188-5\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"497e0ffd32cb4baa12195a686f778b2211b1f008\",\"title\":\"The Focus\\u2013Aspect\\u2013Value model for predicting subjective visual attributes\",\"url\":\"https://www.semanticscholar.org/paper/497e0ffd32cb4baa12195a686f778b2211b1f008\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2020},{\"arxivId\":\"2004.08070\",\"authors\":[{\"authorId\":\"51163002\",\"name\":\"Alasdair Tran\"},{\"authorId\":\"46953477\",\"name\":\"A. Mathews\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"}],\"doi\":\"10.1109/CVPR42600.2020.01305\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c9f19acb4a9d56085a6d2f8f1acef7514777345\",\"title\":\"Transform and Tell: Entity-Aware News Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c9f19acb4a9d56085a6d2f8f1acef7514777345\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2002.09536\",\"authors\":[{\"authorId\":\"38672865\",\"name\":\"M. Seshadri\"},{\"authorId\":\"145311801\",\"name\":\"M. Srikanth\"},{\"authorId\":\"1901449\",\"name\":\"M. Belov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ad4aaa61d7e56f42833aaae788544704c46dcb9e\",\"title\":\"Image to Language Understanding: Captioning approach\",\"url\":\"https://www.semanticscholar.org/paper/ad4aaa61d7e56f42833aaae788544704c46dcb9e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3252736\",\"name\":\"Kazuya Ueki\"},{\"authorId\":\"2495278\",\"name\":\"Takayuki Hori\"}],\"doi\":\"10.1145/3399637.3399657\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bc13ca2f12af0bf21090260c8a78492e7365eac\",\"title\":\"Comparison and Evaluation of Video Retrieval Approaches Using Query Sentences\",\"url\":\"https://www.semanticscholar.org/paper/6bc13ca2f12af0bf21090260c8a78492e7365eac\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1912.12394\",\"authors\":[{\"authorId\":\"3092435\",\"name\":\"Da Ju\"},{\"authorId\":\"35752280\",\"name\":\"Kurt Shuster\"},{\"authorId\":\"90841478\",\"name\":\"Y-Lan Boureau\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4aa5454addde1542e0d01cfc68e6f5129630964d\",\"title\":\"All-in-One Image-Grounded Conversational Agents\",\"url\":\"https://www.semanticscholar.org/paper/4aa5454addde1542e0d01cfc68e6f5129630964d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735191\",\"name\":\"F. Meziane\"},{\"authorId\":\"1748977\",\"name\":\"P. Cimiano\"},{\"authorId\":\"1743774\",\"name\":\"E. Bertino\"},{\"authorId\":\"2084334\",\"name\":\"E. M\\u00e9tais\"},{\"authorId\":\"1713525\",\"name\":\"H. Horacek\"}],\"doi\":\"10.1007/978-3-030-51310-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d5bb881e5fa582428150372557b9b0dd70e472c\",\"title\":\"Natural Language Processing and Information Systems: 25th International Conference on Applications of Natural Language to Information Systems, NLDB 2020, Saarbr\\u00fccken, Germany, June 24\\u201326, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/9d5bb881e5fa582428150372557b9b0dd70e472c\",\"venue\":\"NLDB\",\"year\":2020},{\"arxivId\":\"2001.01037\",\"authors\":[{\"authorId\":\"46969089\",\"name\":\"J. Sun\"},{\"authorId\":\"3633358\",\"name\":\"S. Lapuschkin\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"},{\"authorId\":\"49345823\",\"name\":\"Alexander Binder\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"82e836be97e706dca7029ce6a0553b4890726593\",\"title\":\"Understanding Image Captioning Models beyond Visualizing Attention\",\"url\":\"https://www.semanticscholar.org/paper/82e836be97e706dca7029ce6a0553b4890726593\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.02315\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"121944615\",\"name\":\"Stefan Lee\"}],\"doi\":\"10.1109/cvpr42600.2020.01045\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5f3fe42548216cd93816b1bf5c437cf47bc5fbf\",\"title\":\"12-in-1: Multi-Task Vision and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/b5f3fe42548216cd93816b1bf5c437cf47bc5fbf\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2004.08744\",\"authors\":[{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"43d77f0547f4a1bf4faf29e5e7548564b70e758a\",\"title\":\"Are we pretraining it right? Digging deeper into visio-linguistic pretraining\",\"url\":\"https://www.semanticscholar.org/paper/43d77f0547f4a1bf4faf29e5e7548564b70e758a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.01473\",\"authors\":[{\"authorId\":\"10007273\",\"name\":\"Qiaolin Xia\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"144934143\",\"name\":\"Dongdong Zhang\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"49575302\",\"name\":\"Zhifang Sui\"},{\"authorId\":\"144530394\",\"name\":\"Edward Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":null,\"name\":\"Xin Liu\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"title\":\"XGPT: Cross-modal Generative Pre-Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.10082\",\"authors\":[{\"authorId\":\"52150251\",\"name\":\"A. Goel\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"103192742\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4dcd3fa308b263cee8c7581e2d695dc15c2f51f\",\"title\":\"Injecting Prior Knowledge into Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/e4dcd3fa308b263cee8c7581e2d695dc15c2f51f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1810.10126\",\"authors\":[{\"authorId\":\"1678662\",\"name\":\"Yang Li\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"3422911\",\"name\":\"S. Si\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"31b3b0a526683048f69e703d5f098aea0e8a0ce0\",\"title\":\"Area Attention\",\"url\":\"https://www.semanticscholar.org/paper/31b3b0a526683048f69e703d5f098aea0e8a0ce0\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s42979-020-00238-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f732495721356944027051bb14100436a5dbdf5\",\"title\":\"AACR: Feature Fusion Effects of Algebraic Amalgamation Composed Representation on (De)Compositional Network for Caption Generation for Images\",\"url\":\"https://www.semanticscholar.org/paper/3f732495721356944027051bb14100436a5dbdf5\",\"venue\":\"SN Comput. Sci.\",\"year\":2020},{\"arxivId\":\"2012.13235\",\"authors\":[{\"authorId\":\"34384001\",\"name\":\"Vlad Sandulescu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"29b30ccea4cfe517a1b9facd5c95b2a1071399cf\",\"title\":\"Detecting Hateful Memes Using a Multimodal Deep Ensemble\",\"url\":\"https://www.semanticscholar.org/paper/29b30ccea4cfe517a1b9facd5c95b2a1071399cf\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.11390\",\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":\"10.1007/978-3-030-58586-0_14\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"468d5c15df63892ff06fb94c7b5cad0242685d02\",\"title\":\"Efficient Attention Mechanism for Visual Dialog that Can Handle All the Interactions Between Multiple Inputs\",\"url\":\"https://www.semanticscholar.org/paper/468d5c15df63892ff06fb94c7b5cad0242685d02\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.13198\",\"authors\":[{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"11882893\",\"name\":\"A. Yang\"},{\"authorId\":\"29343468\",\"name\":\"Yichang Zhang\"},{\"authorId\":\"1618186344\",\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b9779ddeb6a8a9de0f7e104d8742728aa14578d6\",\"title\":\"InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction\",\"url\":\"https://www.semanticscholar.org/paper/b9779ddeb6a8a9de0f7e104d8742728aa14578d6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.07758\",\"authors\":[{\"authorId\":\"96544413\",\"name\":\"M. Guevara\"},{\"authorId\":\"98892257\",\"name\":\"C. George\"},{\"authorId\":\"1390483392\",\"name\":\"Akshat Gupta\"},{\"authorId\":\"145849407\",\"name\":\"D. Byrne\"},{\"authorId\":\"6613865\",\"name\":\"Ramesh Krishnamurti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28e613b415aa51a983fcaa8d9ffd3311a4a3dd41\",\"title\":\"Multimodal Word Sense Disambiguation in Creative Practice\",\"url\":\"https://www.semanticscholar.org/paper/28e613b415aa51a983fcaa8d9ffd3311a4a3dd41\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.06066\",\"authors\":[{\"authorId\":\"150112700\",\"name\":\"Gen Li\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143795948\",\"name\":\"Yuejian Fang\"},{\"authorId\":\"71790825\",\"name\":\"Daxin Jiang\"},{\"authorId\":\"143849622\",\"name\":\"M. Zhou\"}],\"doi\":\"10.1609/AAAI.V34I07.6795\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"title\":\"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2006.02635\",\"authors\":[{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"143693093\",\"name\":\"L. Su\"},{\"authorId\":\"1380129958\",\"name\":\"Di Qi\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"144530394\",\"name\":\"Edward Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":\"1452981772\",\"name\":\"Lei Zhang\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1453953482\",\"name\":\"Bei Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144934143\",\"name\":\"Dongdong Zhang\"},{\"authorId\":null,\"name\":\"Xin Liu\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"dfb93b3072bc7d4fb3a53072fc04e28f057b1d4e\",\"title\":\"M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/dfb93b3072bc7d4fb3a53072fc04e28f057b1d4e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2548384\",\"name\":\"Hannaneh Hajishirzi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c\",\"title\":\"Probing Text Models for Common Ground with Visual Representations\",\"url\":\"https://www.semanticscholar.org/paper/467ac47b2e01ce6ae74e8d70561ca0f8f66c7b8c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1978797392\",\"name\":\"Saraswathi Sivamani\"},{\"authorId\":\"3323592\",\"name\":\"S. Chon\"},{\"authorId\":\"146475838\",\"name\":\"Do Yeon Choi\"},{\"authorId\":\"50001341\",\"name\":\"J. H. Park\"}],\"doi\":\"10.1109/ACCESS.2020.3024575\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a7a1d373dbc811f23b579ad85a3b7b635bff41b\",\"title\":\"Investigating and Suggesting the Evaluation Dataset for Image Classification Model\",\"url\":\"https://www.semanticscholar.org/paper/0a7a1d373dbc811f23b579ad85a3b7b635bff41b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2012.07000\",\"authors\":[{\"authorId\":\"145144396\",\"name\":\"Dandan Song\"},{\"authorId\":\"4274864\",\"name\":\"Siyi Ma\"},{\"authorId\":\"2037300711\",\"name\":\"Zhanchen Sun\"},{\"authorId\":\"3389167\",\"name\":\"S. Yang\"},{\"authorId\":\"31364087\",\"name\":\"Lejian Liao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7912b8bb86a6d32ed355651d05ff0cbf37e9504e\",\"title\":\"KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/7912b8bb86a6d32ed355651d05ff0cbf37e9504e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.00451\",\"authors\":[{\"authorId\":\"153276988\",\"name\":\"Antoine Yang\"},{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"title\":\"Just Ask: Learning to Answer Questions from Millions of Narrated Videos\",\"url\":\"https://www.semanticscholar.org/paper/b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.04631\",\"authors\":[{\"authorId\":\"35552695\",\"name\":\"Didac Sur\\u00eds\"},{\"authorId\":\"32486555\",\"name\":\"D. Epstein\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f1085830042fdc07961f4bf4a7e6ff60cd534fd9\",\"title\":\"Globetrotter: Unsupervised Multilingual Translation from Visual Alignment\",\"url\":\"https://www.semanticscholar.org/paper/f1085830042fdc07961f4bf4a7e6ff60cd534fd9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.08824\",\"authors\":[{\"authorId\":\"2799898\",\"name\":\"Andreas Veit\"},{\"authorId\":\"29950139\",\"name\":\"K. Wilber\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cb37f62511aa4e4398223655e02a8682b0876ca9\",\"title\":\"Improving Calibration in Deep Metric Learning With Cross-Example Softmax\",\"url\":\"https://www.semanticscholar.org/paper/cb37f62511aa4e4398223655e02a8682b0876ca9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2675822\",\"name\":\"Tushar Karayil\"},{\"authorId\":\"3429472\",\"name\":\"Philipp Blandfort\"},{\"authorId\":\"33884920\",\"name\":\"J. Hees\"},{\"authorId\":\"145279674\",\"name\":\"A. Dengel\"}],\"doi\":\"10.1145/3323873.3325026\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"404997e98d37eec41f6fc4b131c7dc8b0b437414\",\"title\":\"The Focus-Aspect-Value Model for Explainable Prediction of Subjective Visual Interpretation\",\"url\":\"https://www.semanticscholar.org/paper/404997e98d37eec41f6fc4b131c7dc8b0b437414\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97636424\",\"name\":\"G. Li\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"144303230\",\"name\":\"Ping Liu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/ICCV.2019.00902\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c4530882cfcef1d2b4aa2996f494dfac626b5d9\",\"title\":\"Entangled Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7c4530882cfcef1d2b4aa2996f494dfac626b5d9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2001.09545\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"66a2eb540af6f47db177599bc793ab0c6a6aa47e\",\"title\":\"aiTPR: Attribute Interaction-Tensor Product Representation for Image Caption\",\"url\":\"https://www.semanticscholar.org/paper/66a2eb540af6f47db177599bc793ab0c6a6aa47e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.05054\",\"authors\":[{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"50602231\",\"name\":\"Jeffrey Ling\"},{\"authorId\":\"123052390\",\"name\":\"Michael Collins\"},{\"authorId\":\"1781409\",\"name\":\"D. Reitter\"}],\"doi\":\"10.18653/v1/D19-1219\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b82153bf85d5d1edd3f170aace830e5328ca9ed0\",\"title\":\"Fusion of Detected Objects in Text for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b82153bf85d5d1edd3f170aace830e5328ca9ed0\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2001.07194\",\"authors\":[{\"authorId\":\"51333412\",\"name\":\"Yichao Zhou\"},{\"authorId\":\"35047069\",\"name\":\"Shaunak Mishra\"},{\"authorId\":null,\"name\":\"Manisha Verma\"},{\"authorId\":\"1886139\",\"name\":\"Narayan Bhamidipati\"},{\"authorId\":\"39685680\",\"name\":\"Wei Wang\"}],\"doi\":\"10.1145/3366423.3380001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d662f9e8cb0b6689fd5217b97acf2a8c6f41065\",\"title\":\"Recommending Themes for Ad Creative Design via Visual-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/2d662f9e8cb0b6689fd5217b97acf2a8c6f41065\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":\"2003.03305\",\"authors\":[{\"authorId\":\"40912088\",\"name\":\"M. Tanaka\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"22d733f5d5a995469dc916102f1806253645ae60\",\"title\":\"Captioning Images with Novel Objects via Online Vocabulary Expansion\",\"url\":\"https://www.semanticscholar.org/paper/22d733f5d5a995469dc916102f1806253645ae60\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.01180\",\"authors\":[{\"authorId\":\"48251796\",\"name\":\"Chenyun Wu\"},{\"authorId\":\"143903550\",\"name\":\"M. Timm\"},{\"authorId\":\"35208858\",\"name\":\"Subhransu Maji\"}],\"doi\":\"10.1007/978-3-030-58452-8_4\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"86f0fb3791761cdb5e9108721a536d752641d4bf\",\"title\":\"Describing Textures using Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/86f0fb3791761cdb5e9108721a536d752641d4bf\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1905.06139\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"title\":\"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations\",\"url\":\"https://www.semanticscholar.org/paper/cc663416cbbd577ea02e8b4ef0ea201f5a12d608\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2011.02655\",\"authors\":[{\"authorId\":\"47297245\",\"name\":\"H. Zhu\"},{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"29962444\",\"name\":\"Zhao-Heng Zheng\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1cb961c8bea36d5faf3c6011f15dc23832a3e8fc\",\"title\":\"Utilizing Every Image Object for Semi-supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/1cb961c8bea36d5faf3c6011f15dc23832a3e8fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.06436\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ad005019f7069a3aaff959fbaa657f2ef2143fc\",\"title\":\"MRRC: Multiple Role Representation Crossover Interpretation for Image Captioning With R-CNN Feature Distribution Composition (FDC)\",\"url\":\"https://www.semanticscholar.org/paper/4ad005019f7069a3aaff959fbaa657f2ef2143fc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.07788\",\"authors\":[{\"authorId\":\"2037383772\",\"name\":\"Niklas Muennighoff\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54e4f2ef7410de9f94683cc570cb82257d27c0ff\",\"title\":\"Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes\",\"url\":\"https://www.semanticscholar.org/paper/54e4f2ef7410de9f94683cc570cb82257d27c0ff\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.07467\",\"authors\":[{\"authorId\":\"48335426\",\"name\":\"Biao Zhang\"},{\"authorId\":\"2082372\",\"name\":\"Rico Sennrich\"}],\"doi\":\"10.5167/UZH-177483\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10eda4521c032adabaa8e70d6569e17370b29dcd\",\"title\":\"Root Mean Square Layer Normalization\",\"url\":\"https://www.semanticscholar.org/paper/10eda4521c032adabaa8e70d6569e17370b29dcd\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2006.06195\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"50580345\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1431754650\",\"name\":\"C. Zhu\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8dc49a2041a8a269fbf64911a4f2c8cef6738a5c\",\"title\":\"Large-Scale Adversarial Training for Vision-and-Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/8dc49a2041a8a269fbf64911a4f2c8cef6738a5c\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2006.08686\",\"authors\":[{\"authorId\":\"49967124\",\"name\":\"N. Trieu\"},{\"authorId\":\"7685850\",\"name\":\"Sebastian Goodman\"},{\"authorId\":\"145382418\",\"name\":\"P. Narayana\"},{\"authorId\":\"113477341\",\"name\":\"Kazoo Sone\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf315d3defed0ba7e12a1fc00ebb6444c86e117c\",\"title\":\"Multi-Image Summarization: Textual Summary from a Set of Cohesive Images\",\"url\":\"https://www.semanticscholar.org/paper/bf315d3defed0ba7e12a1fc00ebb6444c86e117c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32438821\",\"name\":\"T. Praveen\"},{\"authorId\":\"2840480\",\"name\":\"J. Jothi\"}],\"doi\":\"10.1007/978-981-15-5243-4_77\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"544e5f86bc98b56f638e69b3d019a6a7af19aa70\",\"title\":\"Enhancing Image Caption Quality with Pre-post Image Injections\",\"url\":\"https://www.semanticscholar.org/paper/544e5f86bc98b56f638e69b3d019a6a7af19aa70\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1d660fdc8e7b23b5fa877a735744d1323a196cdb\",\"title\":\"A Simple Baseline for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/1d660fdc8e7b23b5fa877a735744d1323a196cdb\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"8799308\",\"name\":\"U. Nadeem\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1007/978-3-030-49724-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"title\":\"Vision to Language: Methods, Metrics and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.09742\",\"authors\":[{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"49336560\",\"name\":\"Weining Wang\"},{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb3f19de52d330ded5b3eebb79b89876648f67cb\",\"title\":\"AutoCaption: Image Captioning with Neural Architecture Search\",\"url\":\"https://www.semanticscholar.org/paper/eb3f19de52d330ded5b3eebb79b89876648f67cb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2675822\",\"name\":\"Tushar Karayil\"},{\"authorId\":\"67063678\",\"name\":\"A. Irfan\"},{\"authorId\":\"48258541\",\"name\":\"Federico Raue\"},{\"authorId\":\"33884920\",\"name\":\"J. Hees\"},{\"authorId\":\"145279674\",\"name\":\"A. Dengel\"}],\"doi\":\"10.1007/978-3-030-30490-4_25\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"52941a210e543707b530aa6daf32c18561d8ecc3\",\"title\":\"Conditional GANs for Image Captioning with Sentiments\",\"url\":\"https://www.semanticscholar.org/paper/52941a210e543707b530aa6daf32c18561d8ecc3\",\"venue\":\"ICANN\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47002278\",\"name\":\"Yikuan Li\"},{\"authorId\":\"51464971\",\"name\":\"Hanyin Wang\"},{\"authorId\":\"1830568527\",\"name\":\"Yuan Luo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff554f6228cf1f939a0e9e44ada06ef9cd28be15\",\"title\":\"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports\",\"url\":\"https://www.semanticscholar.org/paper/ff554f6228cf1f939a0e9e44ada06ef9cd28be15\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.02339\",\"authors\":[{\"authorId\":\"2031911881\",\"name\":\"Edwin G. Ng\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0795ae19cae966a9f71d8da78ef09447ab9080a6\",\"title\":\"Understanding Guided Image Captioning Performance across Domains\",\"url\":\"https://www.semanticscholar.org/paper/0795ae19cae966a9f71d8da78ef09447ab9080a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28964453\",\"name\":\"Van-Quang Nguyen\"},{\"authorId\":\"9114621\",\"name\":\"Masanori Suganuma\"},{\"authorId\":\"1718872\",\"name\":\"Takayuki Okatani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"title\":\"Efficient Attention Mechanism for Handling All the Interactions between Many Inputs with Application to Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/fe05479dc4bf2698fcc4e29f09743f8eb1b0f9ec\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3429472\",\"name\":\"Philipp Blandfort\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d207010d9147485901057a9d380ad589a294005\",\"title\":\"Computational Approaches to Subjective Interpretation of Multimedia Messages\",\"url\":\"https://www.semanticscholar.org/paper/8d207010d9147485901057a9d380ad589a294005\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1906.00207\",\"authors\":[{\"authorId\":\"30134687\",\"name\":\"I. Comsa\"},{\"authorId\":\"2292927\",\"name\":\"Moritz Firsching\"},{\"authorId\":\"2141673\",\"name\":\"T. Fischbacher\"}],\"doi\":\"10.1007/JHEP08(2019)057\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c72cc4ac1bead4c6f2a9233d5e4f1a6763cdc791\",\"title\":\"SO(8) Supergravity and the Magic of Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/c72cc4ac1bead4c6f2a9233d5e4f1a6763cdc791\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.08617\",\"authors\":[{\"authorId\":\"84267967\",\"name\":\"C. Thomas\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-58523-5_19\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"432921b7a2c782cedc2a7d87b6194b906e31086d\",\"title\":\"Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/432921b7a2c782cedc2a7d87b6194b906e31086d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.03708\",\"authors\":[{\"authorId\":\"80389349\",\"name\":\"Zhuowan Li\"},{\"authorId\":\"2536742\",\"name\":\"Quan Hung Tran\"},{\"authorId\":\"2712573\",\"name\":\"Long Mai\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/CVPR42600.2020.00350\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"title\":\"Context-Aware Group Captioning via Self-Attention and Contrastive Features\",\"url\":\"https://www.semanticscholar.org/paper/477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2010.02591\",\"authors\":[{\"authorId\":\"50045528\",\"name\":\"Xuanli He\"},{\"authorId\":\"2536742\",\"name\":\"Quan Hung Tran\"},{\"authorId\":\"2561045\",\"name\":\"Gholamreza Haffari\"},{\"authorId\":\"50310099\",\"name\":\"W. Chang\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"35520711\",\"name\":\"Nhan Dam\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.87\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6b21fd64502205bdb5134f13157283c52b701cde\",\"title\":\"Scene Graph Modification Based on Natural Language Commands\",\"url\":\"https://www.semanticscholar.org/paper/6b21fd64502205bdb5134f13157283c52b701cde\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1812.06164\",\"authors\":[{\"authorId\":\"31571033\",\"name\":\"A. Salvador\"},{\"authorId\":\"3325894\",\"name\":\"M. Drozdzal\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"},{\"authorId\":\"144290131\",\"name\":\"A. Romero\"}],\"doi\":\"10.1109/CVPR.2019.01070\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ab155f8f532a238bc29e054b498a6945c157ace\",\"title\":\"Inverse Cooking: Recipe Generation From Food Images\",\"url\":\"https://www.semanticscholar.org/paper/7ab155f8f532a238bc29e054b498a6945c157ace\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s11042-019-08021-1\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"1ba03c16bf25d33efc0d977ab51392e1d9b5a0fb\",\"title\":\"Survey of deep learning and architectures for visual captioning\\u2014transitioning between media and natural languages\",\"url\":\"https://www.semanticscholar.org/paper/1ba03c16bf25d33efc0d977ab51392e1d9b5a0fb\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15181932\",\"name\":\"Rizal Setya Perdana\"},{\"authorId\":\"15167137\",\"name\":\"Y. Ishida\"}],\"doi\":\"10.1109/ELECSYM.2019.8901660\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"348a908617ff00c09c4d4456268da7bb60435441\",\"title\":\"Instance-based Deep Transfer Learning on Cross-domain Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/348a908617ff00c09c4d4456268da7bb60435441\",\"venue\":\"2019 International Electronics Symposium (IES)\",\"year\":2019},{\"arxivId\":\"2011.04305\",\"authors\":[{\"authorId\":\"1918424\",\"name\":\"Jiacheng Chen\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"1491232360\",\"name\":\"Hao Wu\"},{\"authorId\":\"1691963\",\"name\":\"Yuning Jiang\"},{\"authorId\":\"1906061249\",\"name\":\"Changhu Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a27f4af07d73baccda94b5d2a2535b7dfdb58019\",\"title\":\"Learning the Best Pooling Strategy for Visual Semantic Embedding\",\"url\":\"https://www.semanticscholar.org/paper/a27f4af07d73baccda94b5d2a2535b7dfdb58019\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.14308\",\"authors\":[{\"authorId\":\"145534763\",\"name\":\"N. Ding\"},{\"authorId\":\"8010189\",\"name\":\"Xinjie Fan\"},{\"authorId\":\"2362534\",\"name\":\"Zhenzhong Lan\"},{\"authorId\":\"50319359\",\"name\":\"D. Schuurmans\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4bf4ea600f32ceb4b2d005b1b039320cd698f37\",\"title\":\"Attention that does not Explain Away\",\"url\":\"https://www.semanticscholar.org/paper/a4bf4ea600f32ceb4b2d005b1b039320cd698f37\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.15020\",\"authors\":[{\"authorId\":\"27456119\",\"name\":\"Zarana Parekh\"},{\"authorId\":\"1759379\",\"name\":\"Jason Baldridge\"},{\"authorId\":\"46724030\",\"name\":\"Daniel Matthew Cer\"},{\"authorId\":\"40418206\",\"name\":\"Austin Waters\"},{\"authorId\":\"2781059\",\"name\":\"Yinfei Yang\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"3fc0cda3bfad305e9da00f3f564f5205e3833c75\",\"title\":\"Crisscrossed Captions: Extended Intramodal and Intermodal Semantic Similarity Judgments for MS-COCO\",\"url\":\"https://www.semanticscholar.org/paper/3fc0cda3bfad305e9da00f3f564f5205e3833c75\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.08876\",\"authors\":[{\"authorId\":\"3038511\",\"name\":\"Sanqiang Zhao\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/P19-1650\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"68490e9e0bca2f0b1ae2ca636effcd8fc63d2008\",\"title\":\"Informative Image Captioning with External Sources of Information\",\"url\":\"https://www.semanticscholar.org/paper/68490e9e0bca2f0b1ae2ca636effcd8fc63d2008\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"2012.12871\",\"authors\":[{\"authorId\":\"1751661088\",\"name\":\"Phillip Lippe\"},{\"authorId\":\"1661217828\",\"name\":\"Nithin Holla\"},{\"authorId\":\"1879340965\",\"name\":\"Shantanu Chandra\"},{\"authorId\":\"1723418777\",\"name\":\"Santhosh Rajamanickam\"},{\"authorId\":\"33537528\",\"name\":\"G. Antoniou\"},{\"authorId\":\"2362276\",\"name\":\"Ekaterina Shutova\"},{\"authorId\":\"2169553\",\"name\":\"H. Yannakoudakis\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"76e266d5220f963886cf30ae747d72fdf8c84378\",\"title\":\"A Multimodal Framework for the Detection of Hateful Memes\",\"url\":\"https://www.semanticscholar.org/paper/76e266d5220f963886cf30ae747d72fdf8c84378\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.10349\",\"authors\":[{\"authorId\":\"145617383\",\"name\":\"A. Sabir\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"},{\"authorId\":\"1778523\",\"name\":\"L. Padr\\u00f3\"}],\"doi\":\"10.1109/CVPRW50498.2020.00279\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d48fb3ca4bd24420de0c7b0910ee39d1fb86fb0b\",\"title\":\"Textual Visual Semantic Dataset for Text Spotting\",\"url\":\"https://www.semanticscholar.org/paper/d48fb3ca4bd24420de0c7b0910ee39d1fb86fb0b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2715920\",\"name\":\"Malihe Alikhani\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"51019115\",\"name\":\"S. Li\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"144884556\",\"name\":\"Matthew Stone\"}],\"doi\":\"10.18653/v1/2020.acl-main.583\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bb05ea53c4bfee60e045d26cc487d20141482949\",\"title\":\"Cross-modal Coherence Modeling for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/bb05ea53c4bfee60e045d26cc487d20141482949\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2011.10678\",\"authors\":[{\"authorId\":\"2778637\",\"name\":\"Alireza Zareian\"},{\"authorId\":\"2722334\",\"name\":\"Kevin Dela Rosa\"},{\"authorId\":\"1811433\",\"name\":\"D. Hu\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"497d765b1eccbc9e99a37592a1860744559695db\",\"title\":\"Open-Vocabulary Object Detection Using Captions\",\"url\":\"https://www.semanticscholar.org/paper/497d765b1eccbc9e99a37592a1860744559695db\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.14551\",\"authors\":[{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"25576460\",\"name\":\"Ruth C. Fong\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b796497f350d15bd47d06cb4794bad15a149a72\",\"title\":\"Quantifying Learnability and Describability of Visual Concepts Emerging in Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/1b796497f350d15bd47d06cb4794bad15a149a72\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2012.06946\",\"authors\":[{\"authorId\":\"46583603\",\"name\":\"J. Wang\"},{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41171e9024d0082c2a57f4887bac93131669b881\",\"title\":\"MiniVLM: A Smaller and Faster Vision-Language Model\",\"url\":\"https://www.semanticscholar.org/paper/41171e9024d0082c2a57f4887bac93131669b881\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1909.11740\",\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":\"10.1007/978-3-030-58577-8_7\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d8a305b9366608d54452ac30459ee57b4f5cf1c9\",\"title\":\"UNITER: UNiversal Image-TExt Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/d8a305b9366608d54452ac30459ee57b4f5cf1c9\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.13278\",\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1785083\",\"name\":\"Michael R. Lyu\"},{\"authorId\":\"145310663\",\"name\":\"Irwin King\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.269\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"06c7269c10125589d2599f684b751b1640f7a0cc\",\"title\":\"VD-BERT: A Unified Vision and Dialog Transformer with BERT\",\"url\":\"https://www.semanticscholar.org/paper/06c7269c10125589d2599f684b751b1640f7a0cc\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2006.16934\",\"authors\":[{\"authorId\":\"40471592\",\"name\":\"Fei Yu\"},{\"authorId\":\"11713158\",\"name\":\"Jiji Tang\"},{\"authorId\":\"2318321\",\"name\":\"Weichong Yin\"},{\"authorId\":\"144825828\",\"name\":\"Y. Sun\"},{\"authorId\":null,\"name\":\"Hao Tian\"},{\"authorId\":\"120155201\",\"name\":\"Hua Wu\"},{\"authorId\":\"144270729\",\"name\":\"Haifeng Wang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e34cf702b9c90889e268380572bec782280b59c3\",\"title\":\"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph\",\"url\":\"https://www.semanticscholar.org/paper/e34cf702b9c90889e268380572bec782280b59c3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41134094\",\"name\":\"V. Batra\"},{\"authorId\":\"28926612\",\"name\":\"Aparajita Haldar\"},{\"authorId\":\"49990905\",\"name\":\"Yulan He\"},{\"authorId\":\"1787789\",\"name\":\"H. Ferhatosmanoglu\"},{\"authorId\":\"1737941\",\"name\":\"George Vogiatzis\"},{\"authorId\":\"1720741\",\"name\":\"T. Guha\"}],\"doi\":\"10.1007/978-3-030-45439-5_4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5943ff4a89b72185d483952a1f50cf4e2accee0b\",\"title\":\"Variational Recurrent Sequence-to-Sequence Retrieval for Stepwise Illustration\",\"url\":\"https://www.semanticscholar.org/paper/5943ff4a89b72185d483952a1f50cf4e2accee0b\",\"venue\":\"ECIR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"},{\"authorId\":\"1888638\",\"name\":\"Akihiro Tamura\"},{\"authorId\":\"49584970\",\"name\":\"T. Ninomiya\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4174087859fb7e99e6f24cd7ae765e3b3011a4f9\",\"title\":\"A Visually-Grounded Parallel Corpus with Phrase-to-Region Linking\",\"url\":\"https://www.semanticscholar.org/paper/4174087859fb7e99e6f24cd7ae765e3b3011a4f9\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2991962\",\"name\":\"Y. Ohishi\"},{\"authorId\":\"34454585\",\"name\":\"A. Kimura\"},{\"authorId\":\"1858824\",\"name\":\"Takahito Kawanishi\"},{\"authorId\":\"1718803\",\"name\":\"Kunio Kashino\"},{\"authorId\":\"30507748\",\"name\":\"David Harwath\"},{\"authorId\":\"152450847\",\"name\":\"J. Glass\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053428\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f9acc521e5cf08a048b8473a5319882ef4d1b10\",\"title\":\"Trilingual Semantic Embeddings of Visually Grounded Speech with Self-Attention Mechanisms\",\"url\":\"https://www.semanticscholar.org/paper/8f9acc521e5cf08a048b8473a5319882ef4d1b10\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144081629\",\"name\":\"Mario G\\u00f3mez Mart\\u00ednez\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"faa8314259e9de1af8e841c265b0251531b32e04\",\"title\":\"Deep learning for image captioning: an encoder-decoder architecture with soft attention\",\"url\":\"https://www.semanticscholar.org/paper/faa8314259e9de1af8e841c265b0251531b32e04\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2002.06701\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"19b00a2bd1624a0931069a539a971e40b65bfb3a\",\"title\":\"Gaussian Smoothen Semantic Features (GSSF) - Exploring the Linguistic Aspects of Visual Captioning in Indian Languages (Bengali) Using MSCOCO Framework\",\"url\":\"https://www.semanticscholar.org/paper/19b00a2bd1624a0931069a539a971e40b65bfb3a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.10796\",\"authors\":[{\"authorId\":\"4868335\",\"name\":\"J. Park\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6966b0018daffa49eb2c38e68eb8964d56440233\",\"title\":\"Visual Commonsense Graphs: Reasoning about the Dynamic Context of a Still Image\",\"url\":\"https://www.semanticscholar.org/paper/6966b0018daffa49eb2c38e68eb8964d56440233\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.04295\",\"authors\":[{\"authorId\":\"98177814\",\"name\":\"Y. Li\"},{\"authorId\":\"1490931976\",\"name\":\"Gang Li\"},{\"authorId\":\"2265599\",\"name\":\"Luheng He\"},{\"authorId\":\"2923923\",\"name\":\"Jingjie Zheng\"},{\"authorId\":\"100566447\",\"name\":\"H. Li\"},{\"authorId\":\"46731502\",\"name\":\"Zhiwei Guan\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.443\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0b4a8b1c98bd13ce0a6281bb1af3761f7f887235\",\"title\":\"Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements\",\"url\":\"https://www.semanticscholar.org/paper/0b4a8b1c98bd13ce0a6281bb1af3761f7f887235\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1812.08658\",\"authors\":[{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"},{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"46395829\",\"name\":\"Yufei Wang\"},{\"authorId\":\"1606041624\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"145461380\",\"name\":\"Rishabh Jain\"},{\"authorId\":\"1607624548\",\"name\":\"Mark Johnson\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606363958\",\"name\":\"Devi Parikh\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"}],\"doi\":\"10.1109/ICCV.2019.00904\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"title\":\"nocaps: novel object captioning at scale\",\"url\":\"https://www.semanticscholar.org/paper/8b55402ffee2734bfc7d5d7595500916e1ef04e8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2012.11691\",\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"8350409\",\"name\":\"I. Padhi\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"},{\"authorId\":\"153598395\",\"name\":\"J. Ross\"},{\"authorId\":\"1999174380\",\"name\":\"Yair Schiff\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"290f6d003c30099f0983df981c8989e8877270d4\",\"title\":\"Alleviating Noisy Data in Image Captioning with Cooperative Distillation\",\"url\":\"https://www.semanticscholar.org/paper/290f6d003c30099f0983df981c8989e8877270d4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.00619\",\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"2548384\",\"name\":\"Hannaneh Hajishirzi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf45a40d129e02079ba482d3b1bc742a1f6ef36b\",\"title\":\"Probing Contextual Language Models for Common Ground with Visual Representations\",\"url\":\"https://www.semanticscholar.org/paper/cf45a40d129e02079ba482d3b1bc742a1f6ef36b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"2841633\",\"name\":\"Hyunmin Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.18653/v1/N19-1011\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4798919e74411d87f7745840e45b8bcf61128ff\",\"title\":\"AudioCaps: Generating Captions for Audios in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/c4798919e74411d87f7745840e45b8bcf61128ff\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2065332\",\"name\":\"H. Lee\"},{\"authorId\":\"152333274\",\"name\":\"Seunghyun Yoon\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"2007775508\",\"name\":\"Doo Soon Kim\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"1731707\",\"name\":\"K. Jung\"}],\"doi\":\"10.18653/v1/2020.eval4nlp-1.4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2be4e374800a0db69695eb4c558a6653dd258fcd\",\"title\":\"ViLBERTScore: Evaluating Image Caption Using Vision-and-Language BERT\",\"url\":\"https://www.semanticscholar.org/paper/2be4e374800a0db69695eb4c558a6653dd258fcd\",\"venue\":\"EVAL4NLP\",\"year\":2020},{\"arxivId\":\"2005.04917\",\"authors\":[{\"authorId\":\"6296042\",\"name\":\"Heikki Arponen\"},{\"authorId\":\"1823550\",\"name\":\"Tom E. Bishop\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"222c8d2e26b3fffcda8e2e239ab406cba29214d5\",\"title\":\"Learning to hash with semantic similarity metrics and empirical KL divergence\",\"url\":\"https://www.semanticscholar.org/paper/222c8d2e26b3fffcda8e2e239ab406cba29214d5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00246\",\"authors\":[{\"authorId\":\"2904055\",\"name\":\"Ashish V. Thapliyal\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/2020.acl-main.16\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b88c52150995965a66dc774603444ad570c3941d\",\"title\":\"Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage\",\"url\":\"https://www.semanticscholar.org/paper/b88c52150995965a66dc774603444ad570c3941d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1911.09753\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.1609/AAAI.V34I03.5655\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"title\":\"Reinforcing an Image Caption Generator Using Off-Line Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/b49c4285c6f744eaa5a653e7f7f0bc741ddbb8f5\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/TIP.2020.3042086\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"title\":\"Cross-Domain Image Captioning via Cross-Modal Retrieval and Model Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/bf2f223b2f4c425275f6b31f9e0111af32b41882\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":\"2012.15635\",\"authors\":[{\"authorId\":\"2042907541\",\"name\":\"Lorin Sweeney\"},{\"authorId\":null,\"name\":\"Graham Healy\"},{\"authorId\":null,\"name\":\"Alan F. Smeaton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5f5241d0e84562860e452c78bf613e6ba7a2c45\",\"title\":\"Leveraging Audio Gestalt to Predict Media Memorability\",\"url\":\"https://www.semanticscholar.org/paper/a5f5241d0e84562860e452c78bf613e6ba7a2c45\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2002.08911\",\"authors\":[{\"authorId\":\"51519704\",\"name\":\"Candace Ross\"},{\"authorId\":\"143912599\",\"name\":\"B. Katz\"},{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"59bb7f41e72bae283f8aa2222b346956ee197a7a\",\"title\":\"Measuring Social Biases in Grounded Vision and Language Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/59bb7f41e72bae283f8aa2222b346956ee197a7a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.05049\",\"authors\":[{\"authorId\":\"2008154246\",\"name\":\"Zongheng Tang\"},{\"authorId\":\"47303356\",\"name\":\"Yue Liao\"},{\"authorId\":\"101219260\",\"name\":\"Si Liu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"2103483\",\"name\":\"X. Jin\"},{\"authorId\":\"2292508\",\"name\":\"Hongxu Jiang\"},{\"authorId\":\"1410184682\",\"name\":\"Qian Yu\"},{\"authorId\":\"1510477221\",\"name\":\"Dong Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74c30c601d5de21af098389abf7be0f8261e6c13\",\"title\":\"Human-centric Spatio-Temporal Video Grounding With Visual Transformers\",\"url\":\"https://www.semanticscholar.org/paper/74c30c601d5de21af098389abf7be0f8261e6c13\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1562995755\",\"name\":\"Shahi Dost\"},{\"authorId\":\"48790273\",\"name\":\"L. Serafini\"},{\"authorId\":\"1766782\",\"name\":\"M. Rospocher\"},{\"authorId\":\"1795847\",\"name\":\"Lamberto Ballan\"},{\"authorId\":\"1749815\",\"name\":\"A. Sperduti\"}],\"doi\":\"10.1007/978-3-030-51310-8_24\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"215a291141722216a8682d58c38e4d3c17456324\",\"title\":\"Jointly Linking Visual and Textual Entity Mentions with Background Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/215a291141722216a8682d58c38e4d3c17456324\",\"venue\":\"NLDB\",\"year\":2020},{\"arxivId\":\"1908.09317\",\"authors\":[{\"authorId\":\"3422200\",\"name\":\"I. Laina\"},{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"145587209\",\"name\":\"N. Navab\"}],\"doi\":\"10.1109/ICCV.2019.00751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"title\":\"Towards Unsupervised Image Captioning With Shared Multimodal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/a4ff2a0b65b7dfdecee8d2e4bc0c5f7e1fee03be\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"35573122\",\"name\":\"Dingdong Yang\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":\"10.1007/978-3-030-28954-6_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"25f1d8619897093fd6eab70b2cb9c44fd030203c\",\"title\":\"Interpretable Text-to-Image Synthesis with Hierarchical Semantic Layout Generation\",\"url\":\"https://www.semanticscholar.org/paper/25f1d8619897093fd6eab70b2cb9c44fd030203c\",\"venue\":\"Explainable AI\",\"year\":2019},{\"arxivId\":\"1912.02379\",\"authors\":[{\"authorId\":\"46258988\",\"name\":\"Vishvak S. Murahari\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"}],\"doi\":\"10.1007/978-3-030-58523-5_20\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"604d7678235f5bb6039794e382d12058cecf8070\",\"title\":\"Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline\",\"url\":\"https://www.semanticscholar.org/paper/604d7678235f5bb6039794e382d12058cecf8070\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.11848\",\"authors\":[{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"2490189\",\"name\":\"Gregory Shakhnarovich\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ed8c8ee62b0afc403e44c29f6deedf6632885d5\",\"title\":\"Analysis of diversity-accuracy tradeoff in image captioning\",\"url\":\"https://www.semanticscholar.org/paper/9ed8c8ee62b0afc403e44c29f6deedf6632885d5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876154\",\"name\":\"Bin Huang\"},{\"authorId\":\"1993657480\",\"name\":\"Siao Tang\"},{\"authorId\":\"1993657377\",\"name\":\"Guangyao Shen\"},{\"authorId\":\"1993611266\",\"name\":\"Guohao Li\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3422852.3423484\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f642ddb69f712e2b517cb6c6d9a062506991d1ed\",\"title\":\"Commonsense Learning: An Indispensable Path towards Human-centric Multimedia\",\"url\":\"https://www.semanticscholar.org/paper/f642ddb69f712e2b517cb6c6d9a062506991d1ed\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.14973\",\"authors\":[{\"authorId\":\"2905057\",\"name\":\"A. Majumdar\"},{\"authorId\":\"3445289\",\"name\":\"Ayush Shrivastava\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1606382599\",\"name\":\"Peter Anderson\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/978-3-030-58539-6_16\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d1ac487f21829ef56c8ffdcd37ea414bce68c809\",\"title\":\"Improving Vision-and-Language Navigation with Image-Text Pairs from the Web\",\"url\":\"https://www.semanticscholar.org/paper/d1ac487f21829ef56c8ffdcd37ea414bce68c809\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1908.02265\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"title\":\"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1562995755\",\"name\":\"Shahi Dost\"},{\"authorId\":\"144077615\",\"name\":\"L. Serafini\"},{\"authorId\":\"1766782\",\"name\":\"M. Rospocher\"},{\"authorId\":\"1795847\",\"name\":\"Lamberto Ballan\"},{\"authorId\":\"1749815\",\"name\":\"A. Sperduti\"}],\"doi\":\"10.1145/3341105.3373958\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84afec744466b09959ce2ae44de642836ce704b4\",\"title\":\"VTKEL: a resource for visual-textual-knowledge entity linking\",\"url\":\"https://www.semanticscholar.org/paper/84afec744466b09959ce2ae44de642836ce704b4\",\"venue\":\"SAC\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73641462\",\"name\":\"Sethu Hareesh Kolluru\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"290ac36705430ca59512fd75371fa8bb3cce46d8\",\"title\":\"A neural architecture to learn image-text joint embedding\",\"url\":\"https://www.semanticscholar.org/paper/290ac36705430ca59512fd75371fa8bb3cce46d8\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2010.07954\",\"authors\":[{\"authorId\":\"31702389\",\"name\":\"Alexander Ku\"},{\"authorId\":\"1606382599\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1841199820\",\"name\":\"Roma Patel\"},{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"1759379\",\"name\":\"Jason Baldridge\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.356\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5a94aaa3ad624608e46a75de49452a03de568a07\",\"title\":\"Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding\",\"url\":\"https://www.semanticscholar.org/paper/5a94aaa3ad624608e46a75de49452a03de568a07\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2003.12462\",\"authors\":[{\"authorId\":\"144656873\",\"name\":\"O. Sidorov\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"}],\"doi\":\"10.1007/978-3-030-58536-5_44\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7442eaaf453e63195cdee037f8e23830b4004027\",\"title\":\"TextCaps: a Dataset for Image Captioning with Reading Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7442eaaf453e63195cdee037f8e23830b4004027\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2003.14080\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":\"10.1109/cvpr42600.2020.01098\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0\",\"title\":\"X-Linear Attention Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4adfa7b83342b77c830f2b0f6fc1b784c21e7ed0\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2009.04965\",\"authors\":[{\"authorId\":\"123645165\",\"name\":\"Meng-Jiun Chiou\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"180c8732d3f40984c07ad1f2203875e2bf9f9de2\",\"title\":\"Visual Relationship Detection with Visual-Linguistic Knowledge from Multimodal Representations.\",\"url\":\"https://www.semanticscholar.org/paper/180c8732d3f40984c07ad1f2203875e2bf9f9de2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.06666\",\"authors\":[{\"authorId\":\"1606411716\",\"name\":\"Karan Desai\"},{\"authorId\":\"31039758\",\"name\":\"J. Johnson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1932edb4e5b2aae30ec1e7344b16d6110f52ef4\",\"title\":\"VirTex: Learning Visual Representations from Textual Annotations\",\"url\":\"https://www.semanticscholar.org/paper/b1932edb4e5b2aae30ec1e7344b16d6110f52ef4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77506705\",\"name\":\"A. Savchenko\"},{\"authorId\":\"35835911\",\"name\":\"A. Alekseev\"},{\"authorId\":\"2399803\",\"name\":\"Sejeong Kwon\"},{\"authorId\":\"2617496\",\"name\":\"E. Tutubalina\"},{\"authorId\":\"34592111\",\"name\":\"E. Myasnikov\"},{\"authorId\":\"1742235\",\"name\":\"S. Nikolenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8c9e02afed8f9a678dda7b164692e1a9b759d42\",\"title\":\"Ad Lingua: Text Classification Improves Symbolism Prediction in Image Advertisements\",\"url\":\"https://www.semanticscholar.org/paper/d8c9e02afed8f9a678dda7b164692e1a9b759d42\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1909.04101\",\"authors\":[{\"authorId\":\"39191185\",\"name\":\"M. Forbes\"},{\"authorId\":\"1403585268\",\"name\":\"Christine Kaeser-Chen\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"}],\"doi\":\"10.18653/v1/D19-1065\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"035c71e4127a1d3b942b043352f1afac3f0ec451\",\"title\":\"Neural Naturalist: Generating Fine-Grained Image Comparisons\",\"url\":\"https://www.semanticscholar.org/paper/035c71e4127a1d3b942b043352f1afac3f0ec451\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9852531\",\"name\":\"J. Lin\"},{\"authorId\":\"10680632\",\"name\":\"Unnat Jain\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eefddfa610243968135726f9fddf4f69696863ed\",\"title\":\"TAB-VCR: Tags and Attributes based VCR Baselines\",\"url\":\"https://www.semanticscholar.org/paper/eefddfa610243968135726f9fddf4f69696863ed\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1909.08782\",\"authors\":[{\"authorId\":\"1387994137\",\"name\":\"Gabriel Ilharco\"},{\"authorId\":\"153533740\",\"name\":\"Yuan Zhang\"},{\"authorId\":\"1387994164\",\"name\":\"Jason Baldridge\"}],\"doi\":\"10.18653/v1/K19-1006\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c937b4499c259835e4e0b2f6b983ca435005fea3\",\"title\":\"Large-scale representation learning from visually grounded untranscribed speech\",\"url\":\"https://www.semanticscholar.org/paper/c937b4499c259835e4e0b2f6b983ca435005fea3\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.06671\",\"authors\":[{\"authorId\":\"152309095\",\"name\":\"Lily Li\"},{\"authorId\":\"32216164\",\"name\":\"O. Levi\"},{\"authorId\":\"10445523\",\"name\":\"Pedram Hosseini\"},{\"authorId\":\"3119101\",\"name\":\"David A. Broniatowski\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5ab7efdec4cc84f9b071902a54d0c6532a284c8\",\"title\":\"A Multi-Modal Method for Satire Detection using Textual and Visual Cues\",\"url\":\"https://www.semanticscholar.org/paper/b5ab7efdec4cc84f9b071902a54d0c6532a284c8\",\"venue\":\"NLP4IF\",\"year\":2020},{\"arxivId\":\"2010.00562\",\"authors\":[{\"authorId\":\"1403082631\",\"name\":\"J. G\\u00f3mez-P\\u00e9rez\"},{\"authorId\":\"144243879\",\"name\":\"R. Ortega\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.441\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"31988c840b07e00c588f7adf6e16c64e72811324\",\"title\":\"ISAAQ - Mastering Textbook Questions with Pre-trained Transformers and Bottom-Up and Top-Down Attention\",\"url\":\"https://www.semanticscholar.org/paper/31988c840b07e00c588f7adf6e16c64e72811324\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2009.05175\",\"authors\":[{\"authorId\":\"37619618\",\"name\":\"Khyathi Raghavi Chandu\"},{\"authorId\":\"153513927\",\"name\":\"P. Sharma\"},{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"87784755\",\"name\":\"A. Thapliyal\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d325b31c8ccffea52c25e8586bdb9d4dde26151\",\"title\":\"Weakly Supervised Content Selection for Improved Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9d325b31c8ccffea52c25e8586bdb9d4dde26151\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1659982088\",\"name\":\"Lin Sun\"},{\"authorId\":\"152924931\",\"name\":\"J. Wang\"},{\"authorId\":\"2029653429\",\"name\":\"Yindu Su\"},{\"authorId\":\"1660554077\",\"name\":\"Fangsheng Weng\"},{\"authorId\":\"46676270\",\"name\":\"Y. Sun\"},{\"authorId\":\"2629922\",\"name\":\"Zengwei Zheng\"},{\"authorId\":\"16098577\",\"name\":\"Yuanyi Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dacb7d17ad9d1a17421bbe9f38ca9fd9e0703653\",\"title\":\"RIVA: A Pre-trained Tweet Multimodal Model Based on Text-image Relation for Multimodal NER\",\"url\":\"https://www.semanticscholar.org/paper/dacb7d17ad9d1a17421bbe9f38ca9fd9e0703653\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1908.11310\",\"authors\":[{\"authorId\":\"19444389\",\"name\":\"Koustav Ghosal\"},{\"authorId\":\"36809068\",\"name\":\"Aakanksha Rana\"},{\"authorId\":\"118065745\",\"name\":\"A. Smolic\"}],\"doi\":\"10.1109/ICCVW.2019.00556\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5d204156022f65e34706d4211e05bcb578940939\",\"title\":\"Aesthetic Image Captioning From Weakly-Labelled Photographs\",\"url\":\"https://www.semanticscholar.org/paper/5d204156022f65e34706d4211e05bcb578940939\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1811.00119\",\"authors\":[{\"authorId\":\"50694821\",\"name\":\"Su Wang\"},{\"authorId\":null,\"name\":\"Rahul Gupta\"},{\"authorId\":\"145375772\",\"name\":\"N. Chang\"},{\"authorId\":\"1387994164\",\"name\":\"Jason Baldridge\"}],\"doi\":\"10.1609/aaai.v33i01.33017176\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2b4e48d6a0f7c612acc8748bc0dccacde2174c1d\",\"title\":\"A task in a suit and a tie: paraphrase generation with semantic augmentation\",\"url\":\"https://www.semanticscholar.org/paper/2b4e48d6a0f7c612acc8748bc0dccacde2174c1d\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52150251\",\"name\":\"A. Goel\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"103192742\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2518212\",\"name\":\"Hakan Bilen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d807fed82f61bc48aea76f191d2e19e2fc38f5d\",\"title\":\"Learning to Caption Images with Two-Stream Attention and Sentence Auto-Encoder\",\"url\":\"https://www.semanticscholar.org/paper/3d807fed82f61bc48aea76f191d2e19e2fc38f5d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.05963\",\"authors\":[{\"authorId\":\"80236158\",\"name\":\"Simao Herdade\"},{\"authorId\":\"40441990\",\"name\":\"Armin Kappeler\"},{\"authorId\":\"145908678\",\"name\":\"K. Boakye\"},{\"authorId\":\"145730823\",\"name\":\"J. Soares\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b499228aa74b59be32711c3926e44de208d6b636\",\"title\":\"Image Captioning: Transforming Objects into Words\",\"url\":\"https://www.semanticscholar.org/paper/b499228aa74b59be32711c3926e44de208d6b636\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2001.07966\",\"authors\":[{\"authorId\":\"1380129958\",\"name\":\"Di Qi\"},{\"authorId\":\"143693093\",\"name\":\"L. Su\"},{\"authorId\":\"50185694\",\"name\":\"Jia Song\"},{\"authorId\":\"3474078\",\"name\":\"E. Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":\"35378689\",\"name\":\"Arun Sacheti\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a9fd5511b42206a27748f373e0fdb7eb76a23055\",\"title\":\"ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data\",\"url\":\"https://www.semanticscholar.org/paper/a9fd5511b42206a27748f373e0fdb7eb76a23055\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.03775\",\"authors\":[{\"authorId\":\"23978705\",\"name\":\"J. Y. Koh\"},{\"authorId\":\"1759379\",\"name\":\"Jason Baldridge\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"2781059\",\"name\":\"Yinfei Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d807cd50e91e0f939e2d4b84ac21b8d4e482a623\",\"title\":\"Text-to-Image Generation Grounded by Fine-Grained User Attention\",\"url\":\"https://www.semanticscholar.org/paper/d807cd50e91e0f939e2d4b84ac21b8d4e482a623\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"292d6cbab27ba35c825d75130311a4b27f291af2\",\"title\":\"Visual Question Answering with Annotation-Efficient Zero Shot Learning under Linguistic Domain Shift\",\"url\":\"https://www.semanticscholar.org/paper/292d6cbab27ba35c825d75130311a4b27f291af2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.07526\",\"authors\":[{\"authorId\":\"3451494\",\"name\":\"Ana Marasovi\\u0107\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"39227408\",\"name\":\"Ronan Le Bras\"},{\"authorId\":\"1685669\",\"name\":\"N. A. Smith\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.253\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c9940a17504a3b83bd1e9d613b095ddb204d2ad0\",\"title\":\"Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs\",\"url\":\"https://www.semanticscholar.org/paper/c9940a17504a3b83bd1e9d613b095ddb204d2ad0\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.12975\",\"authors\":[{\"authorId\":\"1410423255\",\"name\":\"R\\u0131za Velio\\u011flu\"},{\"authorId\":\"38444971\",\"name\":\"Jewgeni Rose\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d952d45eb6e7c0c564974e2caf1d86db36750b20\",\"title\":\"Detecting Hate Speech in Memes Using Multimodal Deep Learning Approaches: Prize-winning solution to Hateful Memes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/d952d45eb6e7c0c564974e2caf1d86db36750b20\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2008208163\",\"name\":\"Fran\\u00e7ois Gard\\u00e8res\"},{\"authorId\":\"1824096\",\"name\":\"M. Ziaeefard\"},{\"authorId\":\"7816815\",\"name\":\"Baptiste Abeloos\"},{\"authorId\":\"1863173\",\"name\":\"F. L\\u00e9cu\\u00e9\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.44\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9958887e8dd5f84595818c50fb734b566996541a\",\"title\":\"ConceptBert: Concept-Aware Representation for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/9958887e8dd5f84595818c50fb734b566996541a\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1909.11059\",\"authors\":[{\"authorId\":\"48206987\",\"name\":\"L. Zhou\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1609/AAAI.V34I07.7005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"title\":\"Unified Vision-Language Pre-Training for Image Captioning and VQA\",\"url\":\"https://www.semanticscholar.org/paper/088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47127499\",\"name\":\"R. Rao\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"144478564\",\"name\":\"Elnaz Nouri\"},{\"authorId\":\"1780951\",\"name\":\"Debadeepta Dey\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":\"10.1109/CVPRW50498.2020.00486\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0b9c8fa914be1c59f3d5ad1a5e7e41164e4ca5b2\",\"title\":\"Quality and Relevance Metrics for Selection of Multimodal Pretraining Data\",\"url\":\"https://www.semanticscholar.org/paper/0b9c8fa914be1c59f3d5ad1a5e7e41164e4ca5b2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1490966079\",\"name\":\"A. Calabrese\"},{\"authorId\":\"143802044\",\"name\":\"Michele Bevilacqua\"},{\"authorId\":\"1733928\",\"name\":\"R. Navigli\"}],\"doi\":\"10.18653/v1/2020.acl-main.425\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"36d7e8c618bbc5e59ba3d13d7cce7e94d829eea5\",\"title\":\"Fatality Killed the Cat or: BabelPic, a Multimodal Dataset for Non-Concrete Concepts\",\"url\":\"https://www.semanticscholar.org/paper/36d7e8c618bbc5e59ba3d13d7cce7e94d829eea5\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"39882601\",\"name\":\"Yu Cheng\"},{\"authorId\":null,\"name\":\"Jingjing Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"54cf3aea22c9875eb6d02bcf41925da0d6376a23\",\"title\":\"Supplementary Material UNITER: UNiversal Image-TExt Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/54cf3aea22c9875eb6d02bcf41925da0d6376a23\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s13735-020-00198-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c85757cdd7de5f99ab1717a6355a09bb717013c\",\"title\":\"MRECN: mixed representation enhanced (de)compositional network for caption generation from visual features, modeling as pseudo tensor product representation\",\"url\":\"https://www.semanticscholar.org/paper/8c85757cdd7de5f99ab1717a6355a09bb717013c\",\"venue\":\"Int. J. Multim. Inf. Retr.\",\"year\":2020},{\"arxivId\":\"1911.10132\",\"authors\":[{\"authorId\":\"151430668\",\"name\":\"Chiranjib Sur\"}],\"doi\":\"10.1007/s11042-020-09865-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f\",\"title\":\"CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation - A Generalization of Bi Directional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/c8f8ea39c64cf08792cd49c6ad04e85e3b90c88f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.06775\",\"authors\":[{\"authorId\":\"1725438390\",\"name\":\"H. Tan\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.162\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dedcdc1fb3a6def9772dce674d89150923dd75b9\",\"title\":\"Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision\",\"url\":\"https://www.semanticscholar.org/paper/dedcdc1fb3a6def9772dce674d89150923dd75b9\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.11696\",\"authors\":[{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"8350409\",\"name\":\"I. Padhi\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"},{\"authorId\":\"153598395\",\"name\":\"J. Ross\"},{\"authorId\":\"1999174380\",\"name\":\"Yair Schiff\"},{\"authorId\":\"49832828\",\"name\":\"R. A. Young\"},{\"authorId\":\"2679155\",\"name\":\"Brian M. Belgodere\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"title\":\"Image Captioning as an Assistive Technology: Lessons Learned from VizWiz 2020 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/5d4b265d7f80b185c618fb230ea14dcf909beed7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.04790\",\"authors\":[{\"authorId\":\"1743722\",\"name\":\"Douwe Kiela\"},{\"authorId\":\"22593971\",\"name\":\"Hamed Firooz\"},{\"authorId\":\"152422011\",\"name\":\"Aravind Mohan\"},{\"authorId\":\"28554843\",\"name\":\"Vedanuj Goswami\"},{\"authorId\":\"50286460\",\"name\":\"Amanpreet Singh\"},{\"authorId\":\"1422035486\",\"name\":\"Pratik Ringshia\"},{\"authorId\":\"1389630028\",\"name\":\"Davide Testuggine\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"51b461040c381cb1489e55ea4b9686c709818b10\",\"title\":\"The Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes\",\"url\":\"https://www.semanticscholar.org/paper/51b461040c381cb1489e55ea4b9686c709818b10\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2012.04726\",\"authors\":[{\"authorId\":\"1380616323\",\"name\":\"Jeff Da\"},{\"authorId\":\"39191185\",\"name\":\"M. Forbes\"},{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"90390316\",\"name\":\"Anthony Zheng\"},{\"authorId\":\"2012510\",\"name\":\"Jena D. Hwang\"},{\"authorId\":\"2691021\",\"name\":\"Antoine Bosselut\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7c5d597e5f34a01809f1bf4fd6e0f3475f59fb4d\",\"title\":\"Edited Media Understanding: Reasoning About Implications of Manipulated Images\",\"url\":\"https://www.semanticscholar.org/paper/7c5d597e5f34a01809f1bf4fd6e0f3475f59fb4d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2715920\",\"name\":\"Malihe Alikhani\"},{\"authorId\":\"144884556\",\"name\":\"Matthew Stone\"}],\"doi\":\"10.18653/v1/W19-1806\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5834b21b80c8ec68d508684d25a613a604f06479\",\"title\":\"\\u201cCaption\\u201d as a Coherence Relation: Evidence and Implications\",\"url\":\"https://www.semanticscholar.org/paper/5834b21b80c8ec68d508684d25a613a604f06479\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2010.05864\",\"authors\":[{\"authorId\":\"2295601\",\"name\":\"Jingkang Yang\"},{\"authorId\":\"47483397\",\"name\":\"Weirong Chen\"},{\"authorId\":\"1739512\",\"name\":\"Litong Feng\"},{\"authorId\":\"1994100058\",\"name\":\"Xiaopeng Yan\"},{\"authorId\":\"1993661696\",\"name\":\"Huabin Zheng\"},{\"authorId\":\"1726357\",\"name\":\"Wayne Zhang\"}],\"doi\":\"10.1145/3394171.3413952\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d7bde9ed36e40737886c2e44bd05d16a8cd0dde\",\"title\":\"Webly Supervised Image Classification with Metadata: Automatic Noisy Label Correction via Visual-Semantic Graph\",\"url\":\"https://www.semanticscholar.org/paper/7d7bde9ed36e40737886c2e44bd05d16a8cd0dde\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2004.10796\",\"authors\":[{\"authorId\":\"4868335\",\"name\":\"J. Park\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1007/978-3-030-58558-7_30\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4aacd623a46adc6a03c925fe3ac007c271c9c6ab\",\"title\":\"VisualCOMET: Reasoning About the Dynamic Context of a Still Image\",\"url\":\"https://www.semanticscholar.org/paper/4aacd623a46adc6a03c925fe3ac007c271c9c6ab\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1906.00872\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"}],\"doi\":\"10.24963/ijcai.2019/685\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4844ca7cb47a187abf109e4416bd2f2116a4aff\",\"title\":\"From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots\",\"url\":\"https://www.semanticscholar.org/paper/f4844ca7cb47a187abf109e4416bd2f2116a4aff\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2011.15124\",\"authors\":[{\"authorId\":\"83574123\",\"name\":\"Emanuele Bugliarello\"},{\"authorId\":\"1750769\",\"name\":\"Ryan Cotterell\"},{\"authorId\":\"102837708\",\"name\":\"N. Okazaki\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"82a6f9cce3e8acd0ef9e3ca5c7592bd7b9c058fd\",\"title\":\"Multimodal Pretraining Unmasked: Unifying the Vision and Language BERTs\",\"url\":\"https://www.semanticscholar.org/paper/82a6f9cce3e8acd0ef9e3ca5c7592bd7b9c058fd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.00397\",\"authors\":[{\"authorId\":\"51115516\",\"name\":\"L. Yang\"},{\"authorId\":\"49435166\",\"name\":\"Fanqi Meng\"},{\"authorId\":\"31395194\",\"name\":\"Ming-Kuang Daniel Wu\"},{\"authorId\":\"1850625173\",\"name\":\"Vicent Ying\"},{\"authorId\":\"150345115\",\"name\":\"Xianchao Xu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c711f0b0f9e214d6ee462cffcac733221bf07026\",\"title\":\"SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space\",\"url\":\"https://www.semanticscholar.org/paper/c711f0b0f9e214d6ee462cffcac733221bf07026\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.13278\",\"authors\":[{\"authorId\":\"40482726\",\"name\":\"R. Devon Hjelm\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"04ea5fe569cd48c588b7fe420965016ed4ddd312\",\"title\":\"Representation Learning with Video Deep InfoMax\",\"url\":\"https://www.semanticscholar.org/paper/04ea5fe569cd48c588b7fe420965016ed4ddd312\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2262359\",\"name\":\"Yuxuan Wang\"},{\"authorId\":\"8471176\",\"name\":\"Yutai Hou\"},{\"authorId\":\"2256319\",\"name\":\"W. Che\"},{\"authorId\":\"152244126\",\"name\":\"T. Liu\"}],\"doi\":\"10.1007/s13042-020-01069-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"12aa5c0c9fa3077f41f153cdccfa2aacf8940be3\",\"title\":\"From static to dynamic word representations: a survey\",\"url\":\"https://www.semanticscholar.org/paper/12aa5c0c9fa3077f41f153cdccfa2aacf8940be3\",\"venue\":\"Int. J. Mach. Learn. Cybern.\",\"year\":2020},{\"arxivId\":\"1911.03705\",\"authors\":[{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"150341221\",\"name\":\"Wangchunshu Zhou\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.165\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"title\":\"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.05710\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"title\":\"Look Before you Speak: Visually Contextualized Utterances\",\"url\":\"https://www.semanticscholar.org/paper/b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.01655\",\"authors\":[{\"authorId\":\"2947115\",\"name\":\"Arjun Reddy Akula\"},{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"1403907739\",\"name\":\"Yaser Al-Onaizan\"},{\"authorId\":\"12554898\",\"name\":\"S. Zhu\"},{\"authorId\":\"145732771\",\"name\":\"Siva Reddy\"}],\"doi\":\"10.18653/v1/2020.acl-main.586\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1bc3c2b305d0b508caa2a39f4663c6e79402c9e1\",\"title\":\"Words aren't enough, their order matters: On the Robustness of Grounding Visual Referring Expressions\",\"url\":\"https://www.semanticscholar.org/paper/1bc3c2b305d0b508caa2a39f4663c6e79402c9e1\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2009.13682\",\"authors\":[{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"51188307\",\"name\":\"Kevin Lin\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"1720539\",\"name\":\"L. Zhang\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f147279c9d1edddda57f1f21f23b3b58998bad74\",\"title\":\"VIVO: Surpassing Human Performance in Novel Object Captioning with Visual Vocabulary Pre-Training\",\"url\":\"https://www.semanticscholar.org/paper/f147279c9d1edddda57f1f21f23b3b58998bad74\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.03557\",\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"144508458\",\"name\":\"Da Yin\"},{\"authorId\":\"1793529\",\"name\":\"C. Hsieh\"},{\"authorId\":\"2782886\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\"title\":\"VisualBERT: A Simple and Performant Baseline for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"153731335\",\"name\":\"Zhe Gan\"},{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"33c10383189c118465f8b40e8dba9213f57fa570\",\"title\":\"UNITER: Learning UNiversal Image-TExt Representations\",\"url\":\"https://www.semanticscholar.org/paper/33c10383189c118465f8b40e8dba9213f57fa570\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2006.13611\",\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":null,\"name\":\"Yang Wang\"},{\"authorId\":\"47382681\",\"name\":\"Peipei Song\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.24963/ijcai.2020/128\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9c3f13969af79ce26d584935e8e39a4423a9d63f\",\"title\":\"Recurrent Relational Memory Network for Unsupervised Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9c3f13969af79ce26d584935e8e39a4423a9d63f\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2033736279\",\"name\":\"Gustavo Hernandez Abrego\"},{\"authorId\":\"152146422\",\"name\":\"Bowen Liang\"},{\"authorId\":\"115438571\",\"name\":\"W. Wang\"},{\"authorId\":\"27456119\",\"name\":\"Zarana Parekh\"},{\"authorId\":\"2781059\",\"name\":\"Yinfei Yang\"},{\"authorId\":\"2305450\",\"name\":\"Yun-Hsuan Sung\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9309306e8f080d116813484edf04d91d9c3454b\",\"title\":\"Self-Supervised Learning for Pairwise Data Refinement\",\"url\":\"https://www.semanticscholar.org/paper/c9309306e8f080d116813484edf04d91d9c3454b\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2008.01576\",\"authors\":[{\"authorId\":\"46522599\",\"name\":\"Xihui Liu\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"1519062024\",\"name\":\"Jianming Zhang\"},{\"authorId\":\"7574699\",\"name\":\"Handong Zhao\"},{\"authorId\":\"2536742\",\"name\":\"Quan Hung Tran\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"119885708\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1007/978-3-030-58621-8_6\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"23828b8bd14830a2b738f01fb5f82803802c71d0\",\"title\":\"Open-Edit: Open-Domain Image Manipulation with Open-Vocabulary Instructions\",\"url\":\"https://www.semanticscholar.org/paper/23828b8bd14830a2b738f01fb5f82803802c71d0\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.02949\",\"authors\":[{\"authorId\":\"1490732820\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"39543696\",\"name\":\"Fei Sha\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.60\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"73068d13d6e53876c374ebd4c862ec01351c9f39\",\"title\":\"Learning to Represent Image and Text with Denotation Graph\",\"url\":\"https://www.semanticscholar.org/paper/73068d13d6e53876c374ebd4c862ec01351c9f39\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.12352\",\"authors\":[{\"authorId\":\"1845867134\",\"name\":\"Letitia Parcalabescu\"},{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"143876555\",\"name\":\"A. Frank\"},{\"authorId\":\"2338197\",\"name\":\"Iacer Calixto\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1133be974efcad7e6bbe656911f0a57d369bb9e4\",\"title\":\"Seeing past words: Testing the cross-modal capabilities of pretrained V&L models\",\"url\":\"https://www.semanticscholar.org/paper/1133be974efcad7e6bbe656911f0a57d369bb9e4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.13122\",\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5318abd4f12a5b25c2847e9c66713951341af504\",\"title\":\"SubICap: Towards Subword-informed Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5318abd4f12a5b25c2847e9c66713951341af504\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"11882893\",\"name\":\"A. Yang\"},{\"authorId\":\"29343468\",\"name\":\"Yichang Zhang\"},{\"authorId\":\"49723003\",\"name\":\"J. Liu\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee4918cc9b1dc28007454490fbe8366ec017b33d\",\"title\":\"InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/ee4918cc9b1dc28007454490fbe8366ec017b33d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.03098\",\"authors\":[{\"authorId\":\"1403171438\",\"name\":\"J. Pont-Tuset\"},{\"authorId\":\"1823362\",\"name\":\"J. Uijlings\"},{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"}],\"doi\":\"10.1007/978-3-030-58558-7_38\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"title\":\"Connecting Vision and Language with Localized Narratives\",\"url\":\"https://www.semanticscholar.org/paper/439369de9514e41e0f03fed552d8f6e5aebf51b2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66353591\",\"name\":\"Amaia Salvador Aguilera\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4cb1819c27e207a2afeccfe450b931cc317de1e1\",\"title\":\"Computer vision beyond the visible : image understanding through language\",\"url\":\"https://www.semanticscholar.org/paper/4cb1819c27e207a2afeccfe450b931cc317de1e1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1912.11872\",\"authors\":[{\"authorId\":\"153040576\",\"name\":\"T. Mei\"},{\"authorId\":\"101586660\",\"name\":\"W. Zhang\"},{\"authorId\":\"48577275\",\"name\":\"Ting Yao\"}],\"doi\":\"10.1017/ATSIP.2020.10\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"title\":\"Vision and Language: from Visual Perception to Content Creation\",\"url\":\"https://www.semanticscholar.org/paper/3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1909.02097\",\"authors\":[{\"authorId\":\"2059199\",\"name\":\"Soravit Changpinyo\"},{\"authorId\":\"144865339\",\"name\":\"Bo Pang\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/D19-1155\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b6e6822eabe2f64192a1965c23e38043866319c\",\"title\":\"Decoupled Box Proposal and Featurization with Ultrafine-Grained Semantic Labels Improve Image Captioning and Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3b6e6822eabe2f64192a1965c23e38043866319c\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1909.03396\",\"authors\":[{\"authorId\":\"2900341\",\"name\":\"Tomer Levinboim\"},{\"authorId\":\"2904055\",\"name\":\"Ashish V. Thapliyal\"},{\"authorId\":\"48267618\",\"name\":\"Piyush Sharma\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd153dbee4f4b86962ddacbc47010785f9cdec3c\",\"title\":\"Quality Estimation for Image Captions Based on Large-scale Human Evaluations\",\"url\":\"https://www.semanticscholar.org/paper/bd153dbee4f4b86962ddacbc47010785f9cdec3c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1380234931\",\"name\":\"K. Prajwal\"},{\"authorId\":\"1380374156\",\"name\":\"C V Jawahar\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":\"10.1145/3343031.3350939\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eeacc1da20db16e2a2bcf4a5902bcff556c7a0ed\",\"title\":\"Towards Increased Accessibility of Meme Images with the Help of Rich Face Emotion Captions\",\"url\":\"https://www.semanticscholar.org/paper/eeacc1da20db16e2a2bcf4a5902bcff556c7a0ed\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2003.03923\",\"authors\":[{\"authorId\":\"1410097225\",\"name\":\"X. Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e420efcd3fbace6f219c812a78d33cb64e25445\",\"title\":\"Deconfounded Image Captioning: A Causal Retrospect\",\"url\":\"https://www.semanticscholar.org/paper/1e420efcd3fbace6f219c812a78d33cb64e25445\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.06165\",\"authors\":[{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"145307652\",\"name\":\"Li Dong\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1007/978-3-030-58577-8_8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57\",\"title\":\"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.10151\",\"authors\":[{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"14487640\",\"name\":\"Ari Holtzman\"},{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"50730674\",\"name\":\"Joyce Chai\"},{\"authorId\":\"1412841514\",\"name\":\"M. Lapata\"},{\"authorId\":\"2672644\",\"name\":\"A. Lazaridou\"},{\"authorId\":\"143823227\",\"name\":\"Jonathan May\"},{\"authorId\":\"17109242\",\"name\":\"Aleksandr Nisnevich\"},{\"authorId\":\"30017846\",\"name\":\"Nicolas Pinto\"},{\"authorId\":\"153160559\",\"name\":\"Joseph P. Turian\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.703\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d\",\"title\":\"Experience Grounds Language\",\"url\":\"https://www.semanticscholar.org/paper/bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d\",\"venue\":\"EMNLP\",\"year\":2020}],\"corpusId\":51876975,\"doi\":\"10.18653/v1/P18-1238\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":41,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"b4df354db88a70183a64dbc9e56cf14e7669a6c0\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"47087291\",\"name\":\"Z. Yang\"},{\"authorId\":\"30556331\",\"name\":\"Y. Yuan\"},{\"authorId\":\"9287688\",\"name\":\"Yuexin Wu\"},{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61d2dda8d96a10a714636475c7589bd149bda053\",\"title\":\"Review Networks for Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/61d2dda8d96a10a714636475c7589bd149bda053\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145717790\",\"name\":\"C. Chambers\"},{\"authorId\":\"2123949\",\"name\":\"A. Raniwala\"},{\"authorId\":\"48166357\",\"name\":\"F. Perry\"},{\"authorId\":\"144478848\",\"name\":\"Stephen Adams\"},{\"authorId\":\"49234292\",\"name\":\"Robert R. Henry\"},{\"authorId\":\"143979415\",\"name\":\"R. Bradshaw\"},{\"authorId\":\"1896887\",\"name\":\"Nathan Weizenbaum\"}],\"doi\":\"10.1145/1806596.1806638\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b6dd340fb5442e0c31d73f40e241fdd73d42330\",\"title\":\"FlumeJava: easy, efficient data-parallel pipelines\",\"url\":\"https://www.semanticscholar.org/paper/3b6dd340fb5442e0c31d73f40e241fdd73d42330\",\"venue\":\"PLDI '10\",\"year\":2010},{\"arxivId\":\"1412.3555\",\"authors\":[{\"authorId\":\"8270717\",\"name\":\"J. Chung\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"adfcf065e15fd3bc9badf6145034c84dfb08f204\",\"title\":\"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\",\"url\":\"https://www.semanticscholar.org/paper/adfcf065e15fd3bc9badf6145034c84dfb08f204\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1511.06732\",\"authors\":[{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"35c1668dc64d24a28c6041978e5fcca754eb2f4b\",\"title\":\"Sequence Level Training with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/35c1668dc64d24a28c6041978e5fcca754eb2f4b\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1411.4952\",\"authors\":[{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3346186\",\"name\":\"Forrest N. Iandola\"},{\"authorId\":\"2100612\",\"name\":\"R. Srivastava\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"144189092\",\"name\":\"John C. Platt\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"}],\"doi\":\"10.1109/CVPR.2015.7298754\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"title\":\"From captions to visual concepts and back\",\"url\":\"https://www.semanticscholar.org/paper/15f102c3c9f4d4fe6ba105e221df48c6e8902b3b\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"145297531\",\"name\":\"A. Lai\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1162/tacl_a_00166\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"44040913380206991b1991daf1192942e038fe31\",\"title\":\"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\"url\":\"https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":\"1611.10012\",\"authors\":[{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"40303375\",\"name\":\"V. Rathod\"},{\"authorId\":\"145546535\",\"name\":\"Chen Sun\"},{\"authorId\":\"2717876\",\"name\":\"Menglong Zhu\"},{\"authorId\":\"34786378\",\"name\":\"A. Balan\"},{\"authorId\":\"50706340\",\"name\":\"Alireza Fathi\"},{\"authorId\":\"33091759\",\"name\":\"Ian S. Fischer\"},{\"authorId\":\"3282833\",\"name\":\"Z. Wojna\"},{\"authorId\":\"49992891\",\"name\":\"Y. Song\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"145601650\",\"name\":\"K. Murphy\"}],\"doi\":\"10.1109/CVPR.2017.351\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a312a573ef81793d56401e932ef6c9498791a3d1\",\"title\":\"Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors\",\"url\":\"https://www.semanticscholar.org/paper/a312a573ef81793d56401e932ef6c9498791a3d1\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1611.08321\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"2560579\",\"name\":\"J. Xu\"},{\"authorId\":\"39554931\",\"name\":\"Yushi Jing\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc18cb42289fd570a06896b5543b085ebabee57b\",\"title\":\"Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images\",\"url\":\"https://www.semanticscholar.org/paper/cc18cb42289fd570a06896b5543b085ebabee57b\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3141511\",\"name\":\"S. Banerjee\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"title\":\"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments\",\"url\":\"https://www.semanticscholar.org/paper/0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"venue\":\"IEEvaluation@ACL\",\"year\":2005},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Franz Josef Och\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"agenet classification with deep convolutional neural networks\",\"url\":\"\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"},{\"authorId\":\"2002316\",\"name\":\"F. Och\"}],\"doi\":\"10.3115/1218955.1219032\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ca86842aad16797d0fe0323358f3beb1ac6a5c6\",\"title\":\"Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics\",\"url\":\"https://www.semanticscholar.org/paper/9ca86842aad16797d0fe0323358f3beb1ac6a5c6\",\"venue\":\"ACL\",\"year\":2004},{\"arxivId\":\"1602.07261\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"122113652\",\"name\":\"Alexander Amir Alemi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b5c26ab8767d046cb6e32d959fdf726aee89bb62\",\"title\":\"Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\",\"url\":\"https://www.semanticscholar.org/paper/b5c26ab8767d046cb6e32d959fdf726aee89bb62\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1607.08822\",\"authors\":[{\"authorId\":\"6965856\",\"name\":\"Peter Anderson\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"145177220\",\"name\":\"Mark Johnson\"},{\"authorId\":\"145273587\",\"name\":\"Stephen Gould\"}],\"doi\":\"10.1007/978-3-319-46454-1_24\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"title\":\"SPICE: Semantic Propositional Image Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/1c54acd7d9ed8017acdc5674c9b7faac738fd651\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":\"1709.09346\",\"authors\":[{\"authorId\":\"145534769\",\"name\":\"N. Ding\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9bcef44c5a383499f80df861744465e62f6ba33f\",\"title\":\"Cold-Start Reinforcement Learning with Softmax Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/9bcef44c5a383499f80df861744465e62f6ba33f\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2358515\",\"name\":\"Gabriel Murray\"},{\"authorId\":\"47996780\",\"name\":\"S. Renals\"},{\"authorId\":\"50717217\",\"name\":\"J. Carletta\"},{\"authorId\":\"81479509\",\"name\":\"J. D. Moore\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4774432f02ef4c5285952dd8c7daff0852c3a601\",\"title\":\"Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization\",\"url\":\"https://www.semanticscholar.org/paper/4774432f02ef4c5285952dd8c7daff0852c3a601\",\"venue\":\"ACL 2005\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1613/jair.3994\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9814df8bd00ba999c4d1e305a7e9bca579dc7c75\",\"title\":\"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/9814df8bd00ba999c4d1e305a7e9bca579dc7c75\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1734693\",\"name\":\"John C. Duchi\"},{\"authorId\":\"34840427\",\"name\":\"Elad Hazan\"},{\"authorId\":\"1740765\",\"name\":\"Y. Singer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"413c1142de9d91804d6d11c67ff3fed59c9fc279\",\"title\":\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/413c1142de9d91804d6d11c67ff3fed59c9fc279\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.1613/jair.4900\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c162d791b63682d928c09578bd38c3dd61f78c8c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures\",\"url\":\"https://www.semanticscholar.org/paper/c162d791b63682d928c09578bd38c3dd61f78c8c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2016},{\"arxivId\":\"1612.00370\",\"authors\":[{\"authorId\":\"47130333\",\"name\":\"Siqi Liu\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"145361612\",\"name\":\"N. Ye\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.1109/ICCV.2017.100\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"665a311c538fc021c27acd3953f171924cc5905c\",\"title\":\"Optimization of image description metrics using policy gradient methods\",\"url\":\"https://www.semanticscholar.org/paper/665a311c538fc021c27acd3953f171924cc5905c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153302678\",\"name\":\"Jia Deng\"},{\"authorId\":\"47680287\",\"name\":\"W. Dong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2009.5206848\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b47265245e8db53a553049dcb27ed3e495fd625\",\"title\":\"ImageNet: A large-scale hierarchical image database\",\"url\":\"https://www.semanticscholar.org/paper/1b47265245e8db53a553049dcb27ed3e495fd625\",\"venue\":\"CVPR 2009\",\"year\":2009},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.1561/2200000006\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e60ff004dde5c13ec53087872cfcdd12e85beb57\",\"title\":\"Learning Deep Architectures for AI\",\"url\":\"https://www.semanticscholar.org/paper/e60ff004dde5c13ec53087872cfcdd12e85beb57\",\"venue\":\"Found. Trends Mach. Learn.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2015.7298932\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"title\":\"Deep visual-semantic alignments for generating image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"venue\":\"CVPR\",\"year\":2015}],\"title\":\"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning\",\"topics\":[{\"topic\":\"Feature extraction\",\"topicId\":\"4259\",\"url\":\"https://www.semanticscholar.org/topic/4259\"},{\"topic\":\"Transformer\",\"topicId\":\"6977\",\"url\":\"https://www.semanticscholar.org/topic/6977\"},{\"topic\":\"Alt attribute\",\"topicId\":\"1027120\",\"url\":\"https://www.semanticscholar.org/topic/1027120\"},{\"topic\":\"Learnability\",\"topicId\":\"4569\",\"url\":\"https://www.semanticscholar.org/topic/4569\"},{\"topic\":\"Filter (signal processing)\",\"topicId\":\"455583\",\"url\":\"https://www.semanticscholar.org/topic/455583\"}],\"url\":\"https://www.semanticscholar.org/paper/b4df354db88a70183a64dbc9e56cf14e7669a6c0\",\"venue\":\"ACL\",\"year\":2018}\n"