"{\"abstract\":\"To create accessible content for deaf users, we investigate automatically synthesizing animations of American Sign Language (ASL), including grammatically important facial expressions and head movements. Based on recordings of humans performing various types of syntactic face and head movements (which include idiosyncratic variation), we evaluate the efficacy of Continuous Profile Models (CPMs) at identifying an essential \\u201clatent trace\\u201d of the performance, for use in producing ASL animations. A metric-based evaluation and a study with deaf users indicated that this approach was more effective than a prior method for producing animations.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\",\"url\":\"https://www.semanticscholar.org/author/1745229\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\",\"url\":\"https://www.semanticscholar.org/author/1747703\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"2007214491\",\"name\":\"Utkarsh Dwivedi\"},{\"authorId\":\"1393356238\",\"name\":\"Sravya Amancherla\"},{\"authorId\":\"35684404\",\"name\":\"Mayanka Jha\"},{\"authorId\":\"41124627\",\"name\":\"Riya Chanduka\"}],\"doi\":\"10.1145/3373625.3418026\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f6ec17a76e1173c96824e9a0d367b95c9f421dd\",\"title\":\"IncluSet: A Data Surfacing Repository for Accessibility Datasets\",\"url\":\"https://www.semanticscholar.org/paper/7f6ec17a76e1173c96824e9a0d367b95c9f421dd\",\"venue\":\"ASSETS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"}],\"doi\":\"10.21437/SLPAT.2016-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"169b30a45a41dff6a57d696248880e946fa56354\",\"title\":\"Selecting Exemplar Recordings of American Sign Language Non-Manual Expressions for Animation Synthesis Based on Manual Sign Timing\",\"url\":\"https://www.semanticscholar.org/paper/169b30a45a41dff6a57d696248880e946fa56354\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1908.08597\",\"authors\":[{\"authorId\":\"36472974\",\"name\":\"Danielle Bragg\"},{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"40617694\",\"name\":\"Mary Bellard\"},{\"authorId\":\"1842645\",\"name\":\"L. Berke\"},{\"authorId\":\"3522402\",\"name\":\"Patrick Boudreault\"},{\"authorId\":\"1704441\",\"name\":\"Annelies Braffort\"},{\"authorId\":\"2726828\",\"name\":\"Naomi K. Caselli\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"},{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"37531591\",\"name\":\"T. Verhoef\"},{\"authorId\":\"93533005\",\"name\":\"C. Vogler\"},{\"authorId\":\"144844426\",\"name\":\"M. Morris\"}],\"doi\":\"10.1145/3308561.3353774\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c7fd5973a2cb3f7c6a5df0120400bc47b2f2bacf\",\"title\":\"Sign Language Recognition, Generation, and Translation: An Interdisciplinary Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c7fd5973a2cb3f7c6a5df0120400bc47b2f2bacf\",\"venue\":\"ASSETS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1666259463\",\"name\":\"Heike Brock\"},{\"authorId\":\"39293019\",\"name\":\"S. Nishina\"}],\"doi\":\"10.1145/3334480.3375222\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa79622509d0b77c2890c51270bba27fefdb415a\",\"title\":\"Quantifying Sign Avatar Perception: How Imperfect is Insufficient?\",\"url\":\"https://www.semanticscholar.org/paper/aa79622509d0b77c2890c51270bba27fefdb415a\",\"venue\":\"CHI Extended Abstracts\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1697808\",\"name\":\"Barbara Leporini\"},{\"authorId\":\"27539155\",\"name\":\"Eleonora Palmucci\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43a6edfc2f875b318b166700ff737157c1801c64\",\"title\":\"Designing Smartglasses Applications for People with Low Vision\",\"url\":\"https://www.semanticscholar.org/paper/43a6edfc2f875b318b166700ff737157c1801c64\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2499101\",\"name\":\"M. E. D. A. Cardoso\"},{\"authorId\":\"2125066\",\"name\":\"S. M. Peres\"},{\"authorId\":\"15431887\",\"name\":\"Fernando de Almeida Freitas\"},{\"authorId\":\"2918012\",\"name\":\"F. Barbosa\"},{\"authorId\":\"120384538\",\"name\":\"Clodoaldo A. M. Lima\"},{\"authorId\":\"1707003\",\"name\":\"P. Hung\"}],\"doi\":\"10.24251/hicss.2020.184\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"feac094d9af71f9a7839b4b12d6eeb7131241d5f\",\"title\":\"Automatic Segmentation of Grammatical Facial Expressions in Sign Language: Towards an Inclusive Communication Experience\",\"url\":\"https://www.semanticscholar.org/paper/feac094d9af71f9a7839b4b12d6eeb7131241d5f\",\"venue\":\"HICSS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1397309951\",\"name\":\"Luis Naranjo-Zeled\\u00f3n\"},{\"authorId\":\"145467727\",\"name\":\"J. Peral\"},{\"authorId\":\"152666680\",\"name\":\"A. Ferr\\u00e1ndez\"},{\"authorId\":\"1397309948\",\"name\":\"M. Chac\\u00f3n-Rivas\"}],\"doi\":\"10.3390/electronics8091047\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f3b0a20aa5bcc89ea234341d06af0b69b69dfc3\",\"title\":\"A Systematic Mapping of Translation-Enabling Technologies for Sign Languages\",\"url\":\"https://www.semanticscholar.org/paper/3f3b0a20aa5bcc89ea234341d06af0b69b69dfc3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1666259463\",\"name\":\"Heike Brock\"},{\"authorId\":\"1712219541\",\"name\":\"Felix Law\"},{\"authorId\":\"1764429\",\"name\":\"K. Nakadai\"},{\"authorId\":\"50117721\",\"name\":\"Yuji Nagashima\"}],\"doi\":\"10.1145/3377552\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e9383b1b76ffd3e19b646ba057af88cdc21bb1f4\",\"title\":\"Learning Three-dimensional Skeleton Data from Sign Language Video\",\"url\":\"https://www.semanticscholar.org/paper/e9383b1b76ffd3e19b646ba057af88cdc21bb1f4\",\"venue\":\"ACM Trans. Intell. Syst. Technol.\",\"year\":2020}],\"corpusId\":17987597,\"doi\":\"10.18653/v1/p16-1196\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"52bb55ec9c47bfc4e57f470d611031653f11b0e8\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matt Huenerfauth\"},{\"authorId\":null,\"name\":\"Hernisa Kacorri.\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Release of experimental stimuli and questions for evaluating facial expressions in animations of american sign language\",\"url\":\"\",\"venue\":\"Proceedings of the 6th Workshop on the Representation and Processing of Sign Languages:\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"144769406\",\"name\":\"Allen Harper\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"}],\"doi\":\"10.1007/978-3-319-07509-9_52\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"af24a4fa41f9fdfdf10a1d2fa0df1d9d4c81b5d9\",\"title\":\"Measuring the Perception of Facial Expressions in American Sign Language Animations with Eye Tracking\",\"url\":\"https://www.semanticscholar.org/paper/af24a4fa41f9fdfdf10a1d2fa0df1d9d4c81b5d9\",\"venue\":\"HCI\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"38ba50c5590908729bedbfd89d919b103ca24072\",\"title\":\"TR-2015001: A Survey and Critique of Facial Expression Synthesis in Sign Language Animation\",\"url\":\"https://www.semanticscholar.org/paper/38ba50c5590908729bedbfd89d919b103ca24072\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3153387\",\"name\":\"M. Filhol\"},{\"authorId\":\"35711611\",\"name\":\"M. Hadjadj\"},{\"authorId\":\"2465487\",\"name\":\"Benoit Testu\"}],\"doi\":\"10.1007/s10209-015-0413-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f803d467e0dccc11f1a89f91b9901a1da1701669\",\"title\":\"A rule triggering system for automatic text-to-sign translation\",\"url\":\"https://www.semanticscholar.org/paper/f803d467e0dccc11f1a89f91b9901a1da1701669\",\"venue\":\"Universal Access in the Information Society\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1401779806\",\"name\":\"C. Baker-Shenk\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3dc31646436bf43f7c35ee84c51b860028e8949d\",\"title\":\"A Microanalysis of the Nonmanual Components of Questions in American Sign Language\",\"url\":\"https://www.semanticscholar.org/paper/3dc31646436bf43f7c35ee84c51b860028e8949d\",\"venue\":\"\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1732359\",\"name\":\"C. Neidle\"},{\"authorId\":\"38079056\",\"name\":\"Jingjing Liu\"},{\"authorId\":\"50677931\",\"name\":\"B. Liu\"},{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"2467082\",\"name\":\"C. Vogler\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"fe961cbe4be0a35becd2d722f9f364ec3c26bd34\",\"title\":\"Computer-based tracking, analysis, and visualization of linguistically significant nonmanual events in American Sign Language (ASL)\",\"url\":\"https://www.semanticscholar.org/paper/fe961cbe4be0a35becd2d722f9f364ec3c26bd34\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2756081\",\"name\":\"Sarah Ebling\"},{\"authorId\":\"1740734\",\"name\":\"J. Glauert\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"212aeb9b228f1059ce2acc3c0d4bef8b450f957b\",\"title\":\"Exploiting the full potential of JASigning to build an avatar signing train announcements\",\"url\":\"https://www.semanticscholar.org/paper/212aeb9b228f1059ce2acc3c0d4bef8b450f957b\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A Appendix: Supplemental Material In Section 2.3, we made use of a freely available CPM implementation available from http\",\"url\":\"\",\"venue\":\"MAT- LAB, Version 8.5.0\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760519\",\"name\":\"J. Listgarten\"},{\"authorId\":\"1764325\",\"name\":\"R. Neal\"},{\"authorId\":\"9330607\",\"name\":\"S. Roweis\"},{\"authorId\":\"2143959\",\"name\":\"A. Emili\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"08ca9cd5311b19a86fad014f76dec552e72ca1ae\",\"title\":\"Multiple Alignment of Continuous Time Series\",\"url\":\"https://www.semanticscholar.org/paper/08ca9cd5311b19a86fad014f76dec552e72ca1ae\",\"venue\":\"NIPS\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"},{\"authorId\":\"3044430\",\"name\":\"Yilei Xu\"}],\"doi\":\"10.1007/978-0-387-73003-5_90\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"976cc64700d799876da3227d488f61ad0528d865\",\"title\":\"Face Tracking\",\"url\":\"https://www.semanticscholar.org/paper/976cc64700d799876da3227d488f61ad0528d865\",\"venue\":\"Encyclopedia of Biometrics\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"IsoIec\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Information technology-Coding of audio-visual objects-Part 2: Visual. ISO 14496-2:1999, International Organization for Standardization\",\"url\":\"\",\"venue\":\"\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"},{\"authorId\":\"2756081\",\"name\":\"Sarah Ebling\"},{\"authorId\":\"7210463\",\"name\":\"K. Patel\"},{\"authorId\":\"49301423\",\"name\":\"Mackenzie Willard\"}],\"doi\":\"10.1145/2700648.2809860\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab\",\"title\":\"Demographic and Experiential Factors Influencing Acceptance of Sign Language Animation by Deaf Users\",\"url\":\"https://www.semanticscholar.org/paper/e5aa37a7a87b1f3ea9afab6791e02fe2aa8872ab\",\"venue\":\"ASSETS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"39227955\",\"name\":\"Pengfei Lu\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"}],\"doi\":\"10.1007/978-3-642-39188-0_55\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0bdb55cf60f4735ce5349b8431cc98724640be5c\",\"title\":\"Evaluating Facial Expressions in American Sign Language Animations for Accessible Online Information\",\"url\":\"https://www.semanticscholar.org/paper/0bdb55cf60f4735ce5349b8431cc98724640be5c\",\"venue\":\"HCI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"2148322\",\"name\":\"A. Syed\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"},{\"authorId\":\"1732359\",\"name\":\"C. Neidle\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"288c79edcfc9f1aff8bca674283e65792003cca1\",\"title\":\"Centroid-Based Exemplar Selection of ASL Non-Manual Expressions using Multidimensional Dynamic Time Warping and MPEG4 Features\",\"url\":\"https://www.semanticscholar.org/paper/288c79edcfc9f1aff8bca674283e65792003cca1\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50748347\",\"name\":\"J. A. L\\u00f3pez del Val\"},{\"authorId\":\"48728449\",\"name\":\"J. P. Alonso P\\u00e9rez de Agreda\"}],\"doi\":\"10.7551/mitpress/8764.003.0012\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9de58adc7b0be24ac751ec78c5a0c1adbd91be77\",\"title\":\"[Principal components analysis].\",\"url\":\"https://www.semanticscholar.org/paper/9de58adc7b0be24ac751ec78c5a0c1adbd91be77\",\"venue\":\"Atencion primaria\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50508287\",\"name\":\"C. Schmidt\"},{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"},{\"authorId\":\"3104165\",\"name\":\"Thomas Hoyoux\"},{\"authorId\":\"1772389\",\"name\":\"J. Piater\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0bc3e6618786c5133b7f8b0033f8917e61b42a91\",\"title\":\"Enhancing gloss-based corpora with facial features using active appearance models\",\"url\":\"https://www.semanticscholar.org/paper/0bc3e6618786c5133b7f8b0033f8917e61b42a91\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"}],\"doi\":\"10.18653/v1/W15-5106\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"924c999be82f28f612d66a1c8e2a971ef01941a2\",\"title\":\"Evaluating a Dynamic Time Warping Based Scoring Algorithm for Facial Expressions in ASL Animations\",\"url\":\"https://www.semanticscholar.org/paper/924c999be82f28f612d66a1c8e2a971ef01941a2\",\"venue\":\"SLPAT@Interspeech\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"},{\"authorId\":\"39227955\",\"name\":\"Pengfei Lu\"},{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"}],\"doi\":\"10.1145/2517038\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d3e00f004e26693e26cfc2ea46cefb7298fa9bc\",\"title\":\"Effect of Displaying Human Videos During an Evaluation Study of American Sign Language Animation\",\"url\":\"https://www.semanticscholar.org/paper/7d3e00f004e26693e26cfc2ea46cefb7298fa9bc\",\"venue\":\"TACC\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738443\",\"name\":\"S. Gibet\"},{\"authorId\":\"145374281\",\"name\":\"N. Courty\"},{\"authorId\":\"2701613\",\"name\":\"Kyle Duarte\"},{\"authorId\":\"1678906\",\"name\":\"T. L. Naour\"}],\"doi\":\"10.1145/2030365.2030371\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edf3d6174068e0f1f930a030dc30ea06d1791586\",\"title\":\"The SignCom system for data-driven animation of interactive virtual signers: Methodology and Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/edf3d6174068e0f1f930a030dc30ea06d1791586\",\"venue\":\"TIIS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760519\",\"name\":\"J. Listgarten\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b3390de9db193e1f98710d657c7dfef1a91914af\",\"title\":\"Analysis of sibling time series data: alignment and difference detection\",\"url\":\"https://www.semanticscholar.org/paper/b3390de9db193e1f98710d657c7dfef1a91914af\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carol Bloomquist Traxler.\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The stanford achievement test: National norming and performance standards for deaf and hard-of-hearing students\",\"url\":\"\",\"venue\":\"Journal of deaf studies and deaf education, 5(4):337\\u2013348.\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"32551332f9d882d8ea8a51738e70f110292c7929\",\"title\":\"Augmenting EMBR Virtual Human Animation System with MPEG-4 Controls for Producing ASL Facial Expressions\",\"url\":\"https://www.semanticscholar.org/paper/32551332f9d882d8ea8a51738e70f110292c7929\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145708229\",\"name\":\"D. Stein\"},{\"authorId\":\"50508287\",\"name\":\"C. Schmidt\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.1007/s10590-012-9125-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e422eddc4ecb78b34c9cb658a60362da18f0d45b\",\"title\":\"Analysis, preparation, and optimization of statistical sign language machine translation\",\"url\":\"https://www.semanticscholar.org/paper/e422eddc4ecb78b34c9cb658a60362da18f0d45b\",\"venue\":\"Machine Translation\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Karl Pearson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Principal components analysis. The London\",\"url\":\"\",\"venue\":\"Edinburgh, and Dublin Philosophical Magazine and Journal of Science\",\"year\":1901},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Technologies Visage\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Face tracking. https://visagetechnologies.com/products-andservices/visagesdk/facetrack\",\"url\":\"\",\"venue\":\"Face tracking. https://visagetechnologies.com/products-andservices/visagesdk/facetrack\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6463600\",\"name\":\"G. Dunteman\"}],\"doi\":\"10.4135/9781412985475\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c253dc792141518b89dc3ef49681b0424dbf5f79\",\"title\":\"Principal Components Analysis\",\"url\":\"https://www.semanticscholar.org/paper/c253dc792141518b89dc3ef49681b0424dbf5f79\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1747703\",\"name\":\"Matt Huenerfauth\"},{\"authorId\":\"1745229\",\"name\":\"Hernisa Kacorri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c65a49e151b43249ad65aa091b05af25a0161921\",\"title\":\"Best practices for conducting evaluations of sign language animation\",\"url\":\"https://www.semanticscholar.org/paper/c65a49e151b43249ad65aa091b05af25a0161921\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1869727\",\"name\":\"A. Poritz\"}],\"doi\":\"10.1109/ICASSP.1988.196495\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6627d8efde3ed55e34ccee059eb6cdac99bb2fe\",\"title\":\"Hidden Markov models: a guided tour\",\"url\":\"https://www.semanticscholar.org/paper/a6627d8efde3ed55e34ccee059eb6cdac99bb2fe\",\"venue\":\"ICASSP-88., International Conference on Acoustics, Speech, and Signal Processing\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2812935\",\"name\":\"A. H\\u00e9loir\"},{\"authorId\":\"145616714\",\"name\":\"M. Kipp\"}],\"doi\":\"10.1007/978-3-642-04380-2_43\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f0ff5d9beb65b490c79cde1c8cafeaa1210fcee\",\"title\":\"EMBR: A realtime animation engine for interactive embodied agents\",\"url\":\"https://www.semanticscholar.org/paper/6f0ff5d9beb65b490c79cde1c8cafeaa1210fcee\",\"venue\":\"2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops\",\"year\":2009}],\"title\":\"Continuous Profile Models in ASL Syntactic Facial Expression Synthesis\",\"topics\":[{\"topic\":\"Humans\",\"topicId\":\"732\",\"url\":\"https://www.semanticscholar.org/topic/732\"}],\"url\":\"https://www.semanticscholar.org/paper/52bb55ec9c47bfc4e57f470d611031653f11b0e8\",\"venue\":\"ACL\",\"year\":2016}\n"