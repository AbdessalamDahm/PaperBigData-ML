"{\"abstract\":\"We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.\",\"arxivId\":\"1905.07799\",\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\",\"url\":\"https://www.semanticscholar.org/author/2265067\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\",\"url\":\"https://www.semanticscholar.org/author/3024698\"},{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\",\"url\":\"https://www.semanticscholar.org/author/2329288\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\",\"url\":\"https://www.semanticscholar.org/author/2319608\"}],\"citationVelocity\":43,\"citations\":[{\"arxivId\":\"1911.03872\",\"authors\":[{\"authorId\":\"47367640\",\"name\":\"Yann Dubois\"},{\"authorId\":\"67175437\",\"name\":\"Gautier Dagan\"},{\"authorId\":\"3449411\",\"name\":\"Dieuwke Hupkes\"},{\"authorId\":\"2552871\",\"name\":\"Elia Bruni\"}],\"doi\":\"10.18653/v1/2020.acl-main.39\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a6a77b94ee928112cace3c9a4ef77202a45d266\",\"title\":\"Location Attention for Extrapolation to Longer Sequences\",\"url\":\"https://www.semanticscholar.org/paper/4a6a77b94ee928112cace3c9a4ef77202a45d266\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2004.08483\",\"authors\":[{\"authorId\":\"1643737606\",\"name\":\"Joshua Ainslie\"},{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"},{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"3377849\",\"name\":\"V. Cvicek\"},{\"authorId\":\"144211029\",\"name\":\"Zachary Kenneth Fisher\"},{\"authorId\":\"1899637431\",\"name\":\"Philip Pham\"},{\"authorId\":\"101210026\",\"name\":\"Anirudh Ravula\"},{\"authorId\":\"144074891\",\"name\":\"S. Sanghai\"},{\"authorId\":\"145196279\",\"name\":\"Qifan Wang\"},{\"authorId\":\"1624030637\",\"name\":\"L. Yang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"56676aef356ebb13cba77fc9e4d70760fbc151f5\",\"title\":\"ETC: Encoding Long and Structured Inputs in Transformers\",\"url\":\"https://www.semanticscholar.org/paper/56676aef356ebb13cba77fc9e4d70760fbc151f5\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2006.16236\",\"authors\":[{\"authorId\":\"3493855\",\"name\":\"Angelos Katharopoulos\"},{\"authorId\":\"2992087\",\"name\":\"Apoorv Vyas\"},{\"authorId\":\"1680075\",\"name\":\"Nikolaos Pappas\"},{\"authorId\":\"116272138\",\"name\":\"Franccois Fleuret\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac173aba7e854b16042aa09bb33d4618c3fef89c\",\"title\":\"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\",\"url\":\"https://www.semanticscholar.org/paper/ac173aba7e854b16042aa09bb33d4618c3fef89c\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a93b2a79347228c539cb37e5334a1f04ac5da4fd\",\"title\":\"DiscoMT 2019 The Fourth Workshop on Discourse in Machine Translation Proceedings of the Workshop\",\"url\":\"https://www.semanticscholar.org/paper/a93b2a79347228c539cb37e5334a1f04ac5da4fd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1909.11556\",\"authors\":[{\"authorId\":\"144270981\",\"name\":\"Angela Fan\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1\",\"title\":\"Reducing Transformer Depth on Demand with Structured Dropout\",\"url\":\"https://www.semanticscholar.org/paper/f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1909.01562\",\"authors\":[{\"authorId\":\"145485960\",\"name\":\"Jie Hao\"},{\"authorId\":\"48631170\",\"name\":\"Xing Wang\"},{\"authorId\":\"34720053\",\"name\":\"Shuming Shi\"},{\"authorId\":\"145087095\",\"name\":\"J. Zhang\"},{\"authorId\":\"2909321\",\"name\":\"Zhaopeng Tu\"}],\"doi\":\"10.18653/v1/D19-1135\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a940e88117d19160f276e9b6ae014df699495a3f\",\"title\":\"Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons\",\"url\":\"https://www.semanticscholar.org/paper/a940e88117d19160f276e9b6ae014df699495a3f\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2646110\",\"name\":\"S. Park\"},{\"authorId\":\"70308241\",\"name\":\"G. Kim\"},{\"authorId\":\"2885812\",\"name\":\"Jihyo Lee\"},{\"authorId\":\"3409691\",\"name\":\"Junbum Cha\"},{\"authorId\":\"2059904\",\"name\":\"J. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49bbb17c48c8461dbae10e08b17a4d81cb654236\",\"title\":\"Group-Transformer: Towards A Lightweight Character-level Language Model\",\"url\":\"https://www.semanticscholar.org/paper/49bbb17c48c8461dbae10e08b17a4d81cb654236\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1910.08435\",\"authors\":[{\"authorId\":\"144270981\",\"name\":\"Angela Fan\"},{\"authorId\":\"1794075\",\"name\":\"Claire Gardent\"},{\"authorId\":\"2929661\",\"name\":\"C. Braud\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"}],\"doi\":\"10.18653/v1/D19-1428\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83fac78857c7e65fe10a11a798674dd3cd259c1d\",\"title\":\"Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs\",\"url\":\"https://www.semanticscholar.org/paper/83fac78857c7e65fe10a11a798674dd3cd259c1d\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2012.15832\",\"authors\":[{\"authorId\":null,\"name\":\"Ofir Press\"},{\"authorId\":\"1685669\",\"name\":\"N. A. Smith\"},{\"authorId\":\"152153375\",\"name\":\"M. Lewis\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"df92600ed898d7823067223ca3f097157c435bad\",\"title\":\"Shortformer: Better Language Modeling using Shorter Inputs\",\"url\":\"https://www.semanticscholar.org/paper/df92600ed898d7823067223ca3f097157c435bad\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.09483\",\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"},{\"authorId\":\"71184965\",\"name\":\"J. Xu\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"51225788\",\"name\":\"Liangchen Luo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bc53c49ae68adacf2d5be2fa795bcb879e2717a\",\"title\":\"MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/3bc53c49ae68adacf2d5be2fa795bcb879e2717a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9538ae749417eb39af04658e15f65e05ee3012f3\",\"title\":\"Pointwise Conv Projection Shared Projection Output Representations Attention Input Representations\",\"url\":\"https://www.semanticscholar.org/paper/9538ae749417eb39af04658e15f65e05ee3012f3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2227827\",\"name\":\"Tsendsuren Munkhdalai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2954f315ee45feab491852ade78f845cef62abe\",\"title\":\"Sparse Meta Networks for Sequential Adaptation and its Application to Adaptive Language Modelling\",\"url\":\"https://www.semanticscholar.org/paper/a2954f315ee45feab491852ade78f845cef62abe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.07074\",\"authors\":[{\"authorId\":\"48304805\",\"name\":\"Xiaofei Sun\"},{\"authorId\":\"32637368\",\"name\":\"C. Fan\"},{\"authorId\":\"1879521408\",\"name\":\"Zijun Sun\"},{\"authorId\":\"65844131\",\"name\":\"Yuxian Meng\"},{\"authorId\":\"93192602\",\"name\":\"Fei Wu\"},{\"authorId\":\"5183779\",\"name\":\"J. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd3761ac4795b3840dabb325b6171e6368542576\",\"title\":\"Summarize, Outline, and Elaborate: Long-Text Generation via Hierarchical Supervision from Extractive Summaries\",\"url\":\"https://www.semanticscholar.org/paper/cd3761ac4795b3840dabb325b6171e6368542576\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.00294\",\"authors\":[{\"authorId\":\"3179080\",\"name\":\"Yunsu Kim\"},{\"authorId\":\"1391186266\",\"name\":\"Thanh Tran\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.18653/v1/D19-6503\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc18676ef52caf30004ba7a2ce884b84c2a5e552\",\"title\":\"When and Why is Document-level Context Useful in Neural Machine Translation?\",\"url\":\"https://www.semanticscholar.org/paper/fc18676ef52caf30004ba7a2ce884b84c2a5e552\",\"venue\":\"DiscoMT@EMNLP\",\"year\":2019},{\"arxivId\":\"2004.11854\",\"authors\":[{\"authorId\":\"48335426\",\"name\":\"Biao Zhang\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"2082372\",\"name\":\"Rico Sennrich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2f36f14419565a0fed3032b7f1d1811daf6702e\",\"title\":\"On Sparsifying Encoder Outputs in Sequence-to-Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/c2f36f14419565a0fed3032b7f1d1811daf6702e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.00015\",\"authors\":[{\"authorId\":\"146783606\",\"name\":\"Gon\\u00e7alo M. Correia\"},{\"authorId\":\"2114966\",\"name\":\"Vlad Niculae\"},{\"authorId\":\"145644643\",\"name\":\"Andr\\u00e9 F. T. Martins\"}],\"doi\":\"10.18653/v1/D19-1223\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"f6390beca54411b06f3bde424fb983a451789733\",\"title\":\"Adaptively Sparse Transformers\",\"url\":\"https://www.semanticscholar.org/paper/f6390beca54411b06f3bde424fb983a451789733\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1576100042\",\"name\":\"A. Rodriguez\"}],\"doi\":\"10.2200/s01046ed1v01y202009cac053\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74795e7a50e166a23795df26d46b4ba5338bc6d2\",\"title\":\"Deep Learning Systems: Algorithms, Compilers, and Processors for Large-Scale Production\",\"url\":\"https://www.semanticscholar.org/paper/74795e7a50e166a23795df26d46b4ba5338bc6d2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.09833\",\"authors\":[{\"authorId\":\"2845020\",\"name\":\"Xiaoya Li\"},{\"authorId\":\"65844131\",\"name\":\"Yuxian Meng\"},{\"authorId\":\"5439717\",\"name\":\"Qinghong Han\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"},{\"authorId\":\"47786716\",\"name\":\"J. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"81b984e256320b3586492610f85486d635205179\",\"title\":\"SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection\",\"url\":\"https://www.semanticscholar.org/paper/81b984e256320b3586492610f85486d635205179\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2010.00854\",\"authors\":[{\"authorId\":\"2465658\",\"name\":\"Patrick Xia\"},{\"authorId\":\"50425845\",\"name\":\"Shijie Wu\"},{\"authorId\":\"7536576\",\"name\":\"Benjamin Van Durme\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.608\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dae4641eed6ddbe0a781ab5e78daf8204e60f397\",\"title\":\"Which *BERT? A Survey Organizing Contextualized Encoders\",\"url\":\"https://www.semanticscholar.org/paper/dae4641eed6ddbe0a781ab5e78daf8204e60f397\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2007.10434\",\"authors\":[{\"authorId\":\"116506812\",\"name\":\"Bhaskar Mitra\"},{\"authorId\":\"97393346\",\"name\":\"Sebastian Hofst\\u00e4tter\"},{\"authorId\":\"2499986\",\"name\":\"Hamed Zamani\"},{\"authorId\":\"72386923\",\"name\":\"Nick Craswell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2555cfe0f77d55cc0d887da00fd58857d0c6edd5\",\"title\":\"Conformer-Kernel with Query Term Independence for Document Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/2555cfe0f77d55cc0d887da00fd58857d0c6edd5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"LITE TRANSFORMER\"},{\"authorId\":\"1390573666\",\"name\":\"Zhanghao Wu\"},{\"authorId\":\"150357233\",\"name\":\"Zhijian Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8af925f4edf45131b5b6fed8aa655089d58692fa\",\"title\":\"LONG-SHORT RANGE ATTENTION\",\"url\":\"https://www.semanticscholar.org/paper/8af925f4edf45131b5b6fed8aa655089d58692fa\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145485960\",\"name\":\"Jie Hao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"64e2a920b8b070fc02b06a5c3f870f0a915b82ab\",\"title\":\"Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons\",\"url\":\"https://www.semanticscholar.org/paper/64e2a920b8b070fc02b06a5c3f870f0a915b82ab\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2010.06065\",\"authors\":[{\"authorId\":\"1576489304\",\"name\":\"Zonghai Yao\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"3915988\",\"name\":\"H. Pan\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.228\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17a042c93305ac0f9c8b5940434add8bcc61ff8a\",\"title\":\"Zero-shot Entity Linking with Efficient Long Range Sequence Modeling\",\"url\":\"https://www.semanticscholar.org/paper/17a042c93305ac0f9c8b5940434add8bcc61ff8a\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2004.12297\",\"authors\":[{\"authorId\":\"1988888746\",\"name\":\"Liu Yang\"},{\"authorId\":\"1791201\",\"name\":\"Mingyang Zhang\"},{\"authorId\":\"1753450171\",\"name\":\"Cheng Li\"},{\"authorId\":\"120309663\",\"name\":\"Mike Bendersky\"},{\"authorId\":\"1763978\",\"name\":\"Marc Najork\"}],\"doi\":\"10.1145/3340531.3411908\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"984e437300732867b7bec9e79ff8322abeefea5b\",\"title\":\"Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching\",\"url\":\"https://www.semanticscholar.org/paper/984e437300732867b7bec9e79ff8322abeefea5b\",\"venue\":\"CIKM\",\"year\":2020},{\"arxivId\":\"1911.04070\",\"authors\":[{\"authorId\":\"3060913\",\"name\":\"Zihao Ye\"},{\"authorId\":\"3187768\",\"name\":\"Qipeng Guo\"},{\"authorId\":\"47594426\",\"name\":\"Q. Gan\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":null,\"name\":\"Zheng Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2e14e84ccec924ed770b58108ad1d9de6f0ca295\",\"title\":\"BP-Transformer: Modelling Long-Range Context via Binary Partitioning\",\"url\":\"https://www.semanticscholar.org/paper/2e14e84ccec924ed770b58108ad1d9de6f0ca295\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37843592\",\"name\":\"Xiaochuan Zhang\"},{\"authorId\":\"48424065\",\"name\":\"Wenjie Sun\"},{\"authorId\":\"66453931\",\"name\":\"Jian-Min Pang\"},{\"authorId\":\"51013547\",\"name\":\"Fudong Liu\"},{\"authorId\":\"102749841\",\"name\":\"Zhen Ma\"}],\"doi\":\"10.14722/bar.2020.23002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d663c090d79d2d6685ce684e73e00063c33632e5\",\"title\":\"Similarity Metric Method for Binary Basic Blocks of Cross-Instruction Set Architecture\",\"url\":\"https://www.semanticscholar.org/paper/d663c090d79d2d6685ce684e73e00063c33632e5\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145573466\",\"name\":\"Ming Ding\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"46741049\",\"name\":\"J. Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a5fa6e7565dca654eab9372ace4b1ba7f63655f7\",\"title\":\"CogLTX: Applying BERT to Long Texts\",\"url\":\"https://www.semanticscholar.org/paper/a5fa6e7565dca654eab9372ace4b1ba7f63655f7\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2010.08412\",\"authors\":[{\"authorId\":\"153651238\",\"name\":\"Matthew Khoury\"},{\"authorId\":\"26916003\",\"name\":\"R. Dangovski\"},{\"authorId\":\"98860637\",\"name\":\"L. Ou\"},{\"authorId\":\"1683562\",\"name\":\"Preslav Nakov\"},{\"authorId\":\"49745990\",\"name\":\"Y. Shen\"},{\"authorId\":\"145341956\",\"name\":\"L. Jing\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.640\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4c1fedaefad0fb6fb971d4434b6a186f839b8f9\",\"title\":\"Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications\",\"url\":\"https://www.semanticscholar.org/paper/a4c1fedaefad0fb6fb971d4434b6a186f839b8f9\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151240818\",\"name\":\"Ursin Brunner\"},{\"authorId\":\"48229635\",\"name\":\"Kurt Stockinger\"}],\"doi\":\"10.5441/002/edbt.2020.58\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4e6e90c32b30eae80d42fc91261715752ef29c7d\",\"title\":\"Entity Matching with Transformer Architectures - A Step Forward in Data Integration\",\"url\":\"https://www.semanticscholar.org/paper/4e6e90c32b30eae80d42fc91261715752ef29c7d\",\"venue\":\"EDBT\",\"year\":2020},{\"arxivId\":\"2011.14027\",\"authors\":[{\"authorId\":\"3369052\",\"name\":\"Jack Lanchantin\"},{\"authorId\":\"1785372925\",\"name\":\"Tianlu Wang\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"121817403\",\"name\":\"Yanjun Qi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"395f76ffadf04c9711358b70928d0a313351985c\",\"title\":\"General Multi-label Image Classification with Transformers\",\"url\":\"https://www.semanticscholar.org/paper/395f76ffadf04c9711358b70928d0a313351985c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.05150\",\"authors\":[{\"authorId\":\"46181066\",\"name\":\"Iz Beltagy\"},{\"authorId\":\"39139825\",\"name\":\"Matthew E. Peters\"},{\"authorId\":\"2527954\",\"name\":\"Arman Cohan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"71b6394ad5654f5cd0fba763768ba4e523f7bbca\",\"title\":\"Longformer: The Long-Document Transformer\",\"url\":\"https://www.semanticscholar.org/paper/71b6394ad5654f5cd0fba763768ba4e523f7bbca\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.10120\",\"authors\":[{\"authorId\":\"150111176\",\"name\":\"Gaetan Hadjeres\"},{\"authorId\":\"3468368\",\"name\":\"L\\u00e9opold Crestel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de7032ee638659b93e0b5e3684ab0327f3ba16fb\",\"title\":\"Vector Quantized Contrastive Predictive Coding for Template-based Music Generation\",\"url\":\"https://www.semanticscholar.org/paper/de7032ee638659b93e0b5e3684ab0327f3ba16fb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.05507\",\"authors\":[{\"authorId\":\"34269227\",\"name\":\"Jack W. Rae\"},{\"authorId\":\"13759734\",\"name\":\"Anna Potapenko\"},{\"authorId\":\"35880964\",\"name\":\"Siddhant M. Jayakumar\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f51497f463566581874c941353dd9d80069c5b77\",\"title\":\"Compressive Transformers for Long-Range Sequence Modelling\",\"url\":\"https://www.semanticscholar.org/paper/f51497f463566581874c941353dd9d80069c5b77\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2006.04862\",\"authors\":[{\"authorId\":\"2674870\",\"name\":\"Chulhee Yun\"},{\"authorId\":\"2504795\",\"name\":\"Yin-Wen Chang\"},{\"authorId\":\"1798880\",\"name\":\"Srinadh Bhojanapalli\"},{\"authorId\":\"2241094\",\"name\":\"A. Rawat\"},{\"authorId\":\"1981186\",\"name\":\"S. Reddi\"},{\"authorId\":\"2794322\",\"name\":\"S. Kumar\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7ab95243ca74829c3b7893ddbf17fd5f31fa96a9\",\"title\":\"$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers\",\"url\":\"https://www.semanticscholar.org/paper/7ab95243ca74829c3b7893ddbf17fd5f31fa96a9\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1912.11637\",\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"50317060\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"143725038\",\"name\":\"Qi Su\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"title\":\"Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection\",\"url\":\"https://www.semanticscholar.org/paper/b03cf6324ecf7a295a4aeae5970c88d1a1c3f336\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.00072\",\"authors\":[{\"authorId\":\"152500865\",\"name\":\"A. Ivanov\"},{\"authorId\":\"2134146\",\"name\":\"Nikoli Dryden\"},{\"authorId\":\"1402921119\",\"name\":\"Tal Ben-Nun\"},{\"authorId\":\"1484986152\",\"name\":\"Shigang Li\"},{\"authorId\":\"1713648\",\"name\":\"Torsten Hoefler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cb897a4bdc4c1d60a0333a0d0b4c957e480cc3f3\",\"title\":\"Data Movement Is All You Need: A Case Study on Optimizing Transformers\",\"url\":\"https://www.semanticscholar.org/paper/cb897a4bdc4c1d60a0333a0d0b4c957e480cc3f3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.06537\",\"authors\":[{\"authorId\":\"1490937486\",\"name\":\"Hao Peng\"},{\"authorId\":\"4671928\",\"name\":\"Roy Schwartz\"},{\"authorId\":\"7379232\",\"name\":\"Dianqi Li\"},{\"authorId\":\"1685669\",\"name\":\"N. A. Smith\"}],\"doi\":\"10.18653/v1/2020.acl-main.587\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d97cd476bef990352ae921e4c7c5ae1222e9b8da\",\"title\":\"A Mixture of h-1 Heads is Better than h Heads\",\"url\":\"https://www.semanticscholar.org/paper/d97cd476bef990352ae921e4c7c5ae1222e9b8da\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2230087\",\"name\":\"Tsunghan Wu\"},{\"authorId\":\"1741146311\",\"name\":\"Chun-Chen Hsieh\"},{\"authorId\":\"47558836\",\"name\":\"Yen-Hao Chen\"},{\"authorId\":\"1388262677\",\"name\":\"Po-Han Chi\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a98279ca685d7c0ab0a59a241c6c625f9fd7f503\",\"title\":\"Hand-crafted Attention is All You Need? A Study of Attention on Self-supervised Audio Transformer\",\"url\":\"https://www.semanticscholar.org/paper/a98279ca685d7c0ab0a59a241c6c625f9fd7f503\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.02646\",\"authors\":[{\"authorId\":\"1992634637\",\"name\":\"Songyang Zhang\"},{\"authorId\":\"2484788\",\"name\":\"Houwen Peng\"},{\"authorId\":\"2027130177\",\"name\":\"J. Fu\"},{\"authorId\":\"38534822\",\"name\":\"Y. Lu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"261582574b9e039be1518bc7c8e405a4af75a41a\",\"title\":\"Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/261582574b9e039be1518bc7c8e405a4af75a41a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4861083\",\"name\":\"A. Fan\"},{\"authorId\":\"37502184\",\"name\":\"P. Stock\"},{\"authorId\":\"143853801\",\"name\":\"B. Graham\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"46307001\",\"name\":\"R. Gribonval\"},{\"authorId\":\"1681054\",\"name\":\"H. J\\u00e9gou\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"37127a02d129cb733377458d2f155997e3fd622f\",\"title\":\"Training with Quantization Noise for Extreme Fixed-Point Compression\",\"url\":\"https://www.semanticscholar.org/paper/37127a02d129cb733377458d2f155997e3fd622f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1643737606\",\"name\":\"Joshua Ainslie\"},{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"},{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"38552691\",\"name\":\"Philip Pham\"},{\"authorId\":\"101210026\",\"name\":\"Anirudh Ravula\"},{\"authorId\":\"144074891\",\"name\":\"S. Sanghai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"445ce5b0be7661c0a83dfa5421abd45eabcc0988\",\"title\":\"ETC: Encoding Long and Structured Data in Transformers\",\"url\":\"https://www.semanticscholar.org/paper/445ce5b0be7661c0a83dfa5421abd45eabcc0988\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.06097\",\"authors\":[{\"authorId\":\"2992833\",\"name\":\"Shuohang Wang\"},{\"authorId\":\"48206987\",\"name\":\"L. Zhou\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"50580345\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"51444591\",\"name\":\"Yuwei Fang\"},{\"authorId\":\"2419809\",\"name\":\"S. Sun\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0cd82dfae930ac4b57c0e959f744f2d10bf87649\",\"title\":\"Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding\",\"url\":\"https://www.semanticscholar.org/paper/0cd82dfae930ac4b57c0e959f744f2d10bf87649\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00581\",\"authors\":[{\"authorId\":\"50324141\",\"name\":\"Sandeep Subramanian\"},{\"authorId\":\"2939803\",\"name\":\"Ronan Collobert\"},{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"90841478\",\"name\":\"Y-Lan Boureau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f2fc9ef411846dd577c26225ce93f50bb1fa760b\",\"title\":\"Multi-scale Transformer Language Models\",\"url\":\"https://www.semanticscholar.org/paper/f2fc9ef411846dd577c26225ce93f50bb1fa760b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.03040\",\"authors\":[{\"authorId\":\"39886114\",\"name\":\"Ethan M. Rudd\"},{\"authorId\":\"144380304\",\"name\":\"A. Abdallah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f9c869e7a40f5a6c58dd47720f0a044cf8fba602\",\"title\":\"Training Transformers for Information Security Tasks: A Case Study on Malicious URL Prediction\",\"url\":\"https://www.semanticscholar.org/paper/f9c869e7a40f5a6c58dd47720f0a044cf8fba602\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51115516\",\"name\":\"L. Yang\"},{\"authorId\":\"1791201\",\"name\":\"Mingyang Zhang\"},{\"authorId\":\"1753450171\",\"name\":\"Cheng Li\"},{\"authorId\":\"120309663\",\"name\":\"Mike Bendersky\"},{\"authorId\":\"1763978\",\"name\":\"Marc Najork\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f90f41416534efbd13a368d6383865b252584aec\",\"title\":\"Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Document Matching\",\"url\":\"https://www.semanticscholar.org/paper/f90f41416534efbd13a368d6383865b252584aec\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.11423\",\"authors\":[{\"authorId\":\"3375440\",\"name\":\"Stephen Merity\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b9ed2fd3237539b0ad539dad8bdee15efbe0a26e\",\"title\":\"Single Headed Attention RNN: Stop Thinking With Your Head\",\"url\":\"https://www.semanticscholar.org/paper/b9ed2fd3237539b0ad539dad8bdee15efbe0a26e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36288283\",\"name\":\"E. Beck\"},{\"authorId\":\"144490010\",\"name\":\"R. Schl\\u00fcter\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.21437/interspeech.2020-1164\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0fe7bf6c84a7494a6331646936d017148f989c34\",\"title\":\"LVCSR with Transformer Language Models\",\"url\":\"https://www.semanticscholar.org/paper/0fe7bf6c84a7494a6331646936d017148f989c34\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1911.03864\",\"authors\":[{\"authorId\":\"40170001\",\"name\":\"Ofir Press\"},{\"authorId\":\"1685669\",\"name\":\"N. A. Smith\"},{\"authorId\":\"39455775\",\"name\":\"Omer Levy\"}],\"doi\":\"10.18653/v1/2020.acl-main.270\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1b5de55d57b53c91889a8c57f1fd7159bc6b4b10\",\"title\":\"Improving Transformer Models by Reordering their Sublayers\",\"url\":\"https://www.semanticscholar.org/paper/1b5de55d57b53c91889a8c57f1fd7159bc6b4b10\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2007.04825\",\"authors\":[{\"authorId\":\"2992087\",\"name\":\"Apoorv Vyas\"},{\"authorId\":\"3493855\",\"name\":\"Angelos Katharopoulos\"},{\"authorId\":\"116272138\",\"name\":\"Franccois Fleuret\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"cd4ffe5e014601a3d6b64121355d29a730591490\",\"title\":\"Fast Transformers with Clustered Attention\",\"url\":\"https://www.semanticscholar.org/paper/cd4ffe5e014601a3d6b64121355d29a730591490\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390788836\",\"name\":\"Guangxiang Zhao\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"},{\"authorId\":\"71184965\",\"name\":\"J. Xu\"},{\"authorId\":\"50316564\",\"name\":\"Zhiyuan Zhang\"},{\"authorId\":\"51225788\",\"name\":\"Liangchen Luo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1e5c21b863903dd9283192699d9e7a99f7ef0e2\",\"title\":\"MUSE: PARALLEL MULTI-SCALE ATTENTION\",\"url\":\"https://www.semanticscholar.org/paper/b1e5c21b863903dd9283192699d9e7a99f7ef0e2\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1909.00383\",\"authors\":[{\"authorId\":\"48631170\",\"name\":\"Xing Wang\"},{\"authorId\":\"2909321\",\"name\":\"Zhaopeng Tu\"},{\"authorId\":\"1800190\",\"name\":\"Longyue Wang\"},{\"authorId\":\"34720053\",\"name\":\"Shuming Shi\"}],\"doi\":\"10.18653/v1/D19-1145\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ca4e7ed290617aab8b4d99294d10b0ac8372967\",\"title\":\"Self-Attention with Structural Position Representations\",\"url\":\"https://www.semanticscholar.org/paper/4ca4e7ed290617aab8b4d99294d10b0ac8372967\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2008.07669\",\"authors\":[{\"authorId\":\"39499001\",\"name\":\"A. Gu\"},{\"authorId\":\"24593911\",\"name\":\"Tri Dao\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"},{\"authorId\":\"1755572\",\"name\":\"A. Rudra\"},{\"authorId\":\"144988097\",\"name\":\"C. R\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66a99c1ee4c664f04eafee47c43f6b1b342f1342\",\"title\":\"HiPPO: Recurrent Memory with Optimal Polynomial Projections\",\"url\":\"https://www.semanticscholar.org/paper/66a99c1ee4c664f04eafee47c43f6b1b342f1342\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2003.05997\",\"authors\":[{\"authorId\":\"39788470\",\"name\":\"Aurko Roy\"},{\"authorId\":\"2814161\",\"name\":\"M. Saffar\"},{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"2529182\",\"name\":\"David Grangier\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"657329c633709dd1ac34a30d57341b186b1a47c2\",\"title\":\"Efficient Content-Based Sparse Attention with Routing Transformers\",\"url\":\"https://www.semanticscholar.org/paper/657329c633709dd1ac34a30d57341b186b1a47c2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.08708\",\"authors\":[{\"authorId\":\"144377631\",\"name\":\"J. Parker\"},{\"authorId\":\"51318916\",\"name\":\"Shakti Kumar\"},{\"authorId\":\"1643691005\",\"name\":\"Joe Roussy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f23a3f541f2dd99fab7f9fb9eb1b1fe7419e73f7\",\"title\":\"Adaptive Attention Span in Computer Vision\",\"url\":\"https://www.semanticscholar.org/paper/f23a3f541f2dd99fab7f9fb9eb1b1fe7419e73f7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"40902044\",\"name\":\"Aswin Shanmugam Subramanian\"},{\"authorId\":\"47871517\",\"name\":\"Pengcheng Guo\"},{\"authorId\":\"1826565368\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"1471364901\",\"name\":\"Yuya Fujita\"},{\"authorId\":\"35976156\",\"name\":\"M. Omachi\"}],\"doi\":\"10.21437/interspeech.2020-2816\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bef4548a43fca8a7410734e4200157d50e257a29\",\"title\":\"End-to-End ASR with Adaptive Span Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/bef4548a43fca8a7410734e4200157d50e257a29\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1973629764\",\"name\":\"Y. Zhu\"},{\"authorId\":\"2911924\",\"name\":\"P. Haghani\"},{\"authorId\":\"1500649019\",\"name\":\"Anshuman Tripathi\"},{\"authorId\":\"1720857\",\"name\":\"B. Ramabhadran\"},{\"authorId\":\"35315013\",\"name\":\"Brian Farris\"},{\"authorId\":\"2094929\",\"name\":\"Hainan Xu\"},{\"authorId\":\"1500648246\",\"name\":\"Han Lu\"},{\"authorId\":\"2670103\",\"name\":\"H. Sak\"},{\"authorId\":\"48749428\",\"name\":\"I. Leal\"},{\"authorId\":\"1500645596\",\"name\":\"Neeraj Gaur\"},{\"authorId\":\"47690405\",\"name\":\"P. Moreno\"},{\"authorId\":\"47332302\",\"name\":\"Qian Zhang\"}],\"doi\":\"10.21437/interspeech.2020-2847\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b837a453d4046100c84b1232f705c1b08903400c\",\"title\":\"Multilingual Speech Recognition with Self-Attention Structured Parameterization\",\"url\":\"https://www.semanticscholar.org/paper/b837a453d4046100c84b1232f705c1b08903400c\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2009.06857\",\"authors\":[{\"authorId\":\"51891020\",\"name\":\"Aran Komatsuzaki\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f89e2f4fb44e30b1adb08d153bf22b063597f896\",\"title\":\"Current Limitations of Language Models: What You Need is Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/f89e2f4fb44e30b1adb08d153bf22b063597f896\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2646110\",\"name\":\"S. Park\"},{\"authorId\":\"70308241\",\"name\":\"G. Kim\"},{\"authorId\":\"2885812\",\"name\":\"Jihyo Lee\"},{\"authorId\":\"3409691\",\"name\":\"Junbum Cha\"},{\"authorId\":\"102361212\",\"name\":\"J. Kim\"},{\"authorId\":\"3178268\",\"name\":\"Hwal-Suk Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"763654e1d3b875d9f914b5171e765ef53e7cf271\",\"title\":\"Scale down Transformer by Grouping Features for a Lightweight Character-level Language Model\",\"url\":\"https://www.semanticscholar.org/paper/763654e1d3b875d9f914b5171e765ef53e7cf271\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152500865\",\"name\":\"Andrei Ivanov\"},{\"authorId\":\"2134146\",\"name\":\"Nikoli Dryden\"},{\"authorId\":\"1402921119\",\"name\":\"Tal Ben-Nun\"},{\"authorId\":\"1484986152\",\"name\":\"Shigang Li\"},{\"authorId\":\"1713648\",\"name\":\"Torsten Hoefler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"990d042905b8a41f601d3fdd975bc4051a852bcf\",\"title\":\"Data Movement Is All You Need: A Case Study of Transformer Networks\",\"url\":\"https://www.semanticscholar.org/paper/990d042905b8a41f601d3fdd975bc4051a852bcf\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.11886\",\"authors\":[{\"authorId\":\"1390573666\",\"name\":\"Zhanghao Wu\"},{\"authorId\":\"47781592\",\"name\":\"Zhijian Liu\"},{\"authorId\":\"93318099\",\"name\":\"J. Lin\"},{\"authorId\":\"49417466\",\"name\":\"Yujun Lin\"},{\"authorId\":\"1484743787\",\"name\":\"Song Han\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8474155e4a0085c44b12817d5da5a20853b56285\",\"title\":\"Lite Transformer with Long-Short Range Attention\",\"url\":\"https://www.semanticscholar.org/paper/8474155e4a0085c44b12817d5da5a20853b56285\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2003.01371\",\"authors\":[{\"authorId\":\"47422763\",\"name\":\"Qichen Li\"},{\"authorId\":\"1975227\",\"name\":\"Xiaoke Jiang\"},{\"authorId\":\"144390806\",\"name\":\"J. Xia\"},{\"authorId\":null,\"name\":\"Jian Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb247f9f74a19e90b913d9ef3e8f3ef27b9cb118\",\"title\":\"Meta-Embeddings Based On Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/eb247f9f74a19e90b913d9ef3e8f3ef27b9cb118\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.12442\",\"authors\":[{\"authorId\":\"144745718\",\"name\":\"Stephen Roller\"},{\"authorId\":\"2656573\",\"name\":\"Y.-Lan Boureau\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"},{\"authorId\":\"31461304\",\"name\":\"Emily Dinan\"},{\"authorId\":\"4861083\",\"name\":\"A. Fan\"},{\"authorId\":\"2121780\",\"name\":\"D. Gunning\"},{\"authorId\":\"3092435\",\"name\":\"Da Ju\"},{\"authorId\":\"6649233\",\"name\":\"Margaret Li\"},{\"authorId\":\"1753626755\",\"name\":\"Spencer Poff\"},{\"authorId\":\"1422035486\",\"name\":\"Pratik Ringshia\"},{\"authorId\":\"35752280\",\"name\":\"Kurt Shuster\"},{\"authorId\":\"51324296\",\"name\":\"Eric Michael Smith\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"39219656\",\"name\":\"Jack Urbanek\"},{\"authorId\":\"49160304\",\"name\":\"M. Williamson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ebf79630966e36761f2275990075384fdcb8d3a7\",\"title\":\"Open-Domain Conversational Agents: Current Progress, Open Problems, and Future Directions\",\"url\":\"https://www.semanticscholar.org/paper/ebf79630966e36761f2275990075384fdcb8d3a7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.07027\",\"authors\":[{\"authorId\":\"74884989\",\"name\":\"D. Yoshida\"},{\"authorId\":\"37907837\",\"name\":\"A. Ettinger\"},{\"authorId\":\"1700980\",\"name\":\"Kevin Gimpel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54bc3e055d05e44c010febc669e8dea394643efc\",\"title\":\"Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size\",\"url\":\"https://www.semanticscholar.org/paper/54bc3e055d05e44c010febc669e8dea394643efc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.10260\",\"authors\":[{\"authorId\":\"3106437\",\"name\":\"Alessandro Raganato\"},{\"authorId\":\"3080637\",\"name\":\"Yves Scherrer\"},{\"authorId\":\"113391779\",\"name\":\"Jorg Tiedemann\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.49\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ca05087fc36e7917c0ff9a88e89bae35960704ab\",\"title\":\"Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/ca05087fc36e7917c0ff9a88e89bae35960704ab\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1911.12287\",\"authors\":[{\"authorId\":\"1432234296\",\"name\":\"Giannis Daras\"},{\"authorId\":\"2624088\",\"name\":\"Augustus Odena\"},{\"authorId\":\"120811666\",\"name\":\"Han Zhang\"},{\"authorId\":\"1718469\",\"name\":\"A. Dimakis\"}],\"doi\":\"10.1109/CVPR42600.2020.01454\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ab27dfc0ad306486ee1219e12bf5c6a0036c3ee\",\"title\":\"Your Local GAN: Designing Two Dimensional Local Attention Mechanisms for Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/4ab27dfc0ad306486ee1219e12bf5c6a0036c3ee\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2004.03761\",\"authors\":[{\"authorId\":\"51318916\",\"name\":\"Shakti Kumar\"},{\"authorId\":\"144377631\",\"name\":\"J. Parker\"},{\"authorId\":\"1620500016\",\"name\":\"Panteha Naderian\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"561408d38f4d16a3156a06c3c83e1fcabc2ae07c\",\"title\":\"Adaptive Transformers in RL\",\"url\":\"https://www.semanticscholar.org/paper/561408d38f4d16a3156a06c3c83e1fcabc2ae07c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.05174\",\"authors\":[{\"authorId\":\"2230087\",\"name\":\"Tsunghan Wu\"},{\"authorId\":\"1741146311\",\"name\":\"Chun-Chen Hsieh\"},{\"authorId\":\"47558836\",\"name\":\"Yen-Hao Chen\"},{\"authorId\":\"1388262677\",\"name\":\"Po-Han Chi\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"25f6815dee1f1e2086c3f0d2409000c5257efcec\",\"title\":\"Input-independent Attention Weights Are Expressive Enough: A Study of Attention in Self-supervised Audio Transformers\",\"url\":\"https://www.semanticscholar.org/paper/25f6815dee1f1e2086c3f0d2409000c5257efcec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1912.05625\",\"authors\":[{\"authorId\":\"3387623\",\"name\":\"Seonwoo Min\"},{\"authorId\":\"66049376\",\"name\":\"Seunghyun Park\"},{\"authorId\":\"48388901\",\"name\":\"S. Kim\"},{\"authorId\":\"31400130\",\"name\":\"Hyun-Soo Choi\"},{\"authorId\":\"2999019\",\"name\":\"S. Yoon\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3401104ef5a06ea03aa764d80865693707fd65b1\",\"title\":\"Pre-Training of Deep Bidirectional Protein Sequence Representations with Structural Information\",\"url\":\"https://www.semanticscholar.org/paper/3401104ef5a06ea03aa764d80865693707fd65b1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1911.02972\",\"authors\":[{\"authorId\":\"40125294\",\"name\":\"Jiezhong Qiu\"},{\"authorId\":\"46389865\",\"name\":\"Hao Ma\"},{\"authorId\":\"39455775\",\"name\":\"Omer Levy\"},{\"authorId\":\"3156075\",\"name\":\"S. Yih\"},{\"authorId\":\"2096387\",\"name\":\"Sinong Wang\"},{\"authorId\":\"143805718\",\"name\":\"Jie Tang\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.232\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3da5dac5f51acab98c7c46e94a8536dd7fef163c\",\"title\":\"Blockwise Self-Attention for Long Document Understanding\",\"url\":\"https://www.semanticscholar.org/paper/3da5dac5f51acab98c7c46e94a8536dd7fef163c\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2003.00130\",\"authors\":[{\"authorId\":\"51932270\",\"name\":\"James Wallbridge\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa36de77434fbb47f03ca4a4202f0237b509ee55\",\"title\":\"Transformers for Limit Order Books\",\"url\":\"https://www.semanticscholar.org/paper/aa36de77434fbb47f03ca4a4202f0237b509ee55\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.03897\",\"authors\":[{\"authorId\":\"19677945\",\"name\":\"Yaoyiran Li\"},{\"authorId\":\"144924128\",\"name\":\"Jing Jiang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"863bc093e87a2963a0f6b5fc6c2d8b7451b8a7b2\",\"title\":\"Two-Headed Monster And Crossed Co-Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/863bc093e87a2963a0f6b5fc6c2d8b7451b8a7b2\",\"venue\":\"AACL\",\"year\":2020},{\"arxivId\":\"2007.12146\",\"authors\":[{\"authorId\":\"66536530\",\"name\":\"Yash Kant\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606382599\",\"name\":\"Peter Anderson\"},{\"authorId\":\"5153264\",\"name\":\"A. Schwing\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"}],\"doi\":\"10.1007/978-3-030-58545-7_41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f92dc7d9a61f7b8b5b7cb58f3895541e266ce057\",\"title\":\"Spatially Aware Multimodal Transformers for TextVQA\",\"url\":\"https://www.semanticscholar.org/paper/f92dc7d9a61f7b8b5b7cb58f3895541e266ce057\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2005.14187\",\"authors\":[{\"authorId\":\"35446689\",\"name\":\"Hanrui Wang\"},{\"authorId\":\"1390573666\",\"name\":\"Zhanghao Wu\"},{\"authorId\":\"47781592\",\"name\":\"Zhijian Liu\"},{\"authorId\":\"81623205\",\"name\":\"H. Cai\"},{\"authorId\":\"20515689\",\"name\":\"Ligeng Zhu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"143840277\",\"name\":\"Song Han\"}],\"doi\":\"10.18653/v1/2020.acl-main.686\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7\",\"title\":\"HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/ef8d788a904ed66bd8e30ffa69bc3ea1fe57dda7\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2001.04451\",\"authors\":[{\"authorId\":\"143808231\",\"name\":\"Nikita Kitaev\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"6639036\",\"name\":\"Anselm Levskaya\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"055fd6a9f7293269f1b22c1470e63bd02d8d9500\",\"title\":\"Reformer: The Efficient Transformer\",\"url\":\"https://www.semanticscholar.org/paper/055fd6a9f7293269f1b22c1470e63bd02d8d9500\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2009.11523\",\"authors\":[{\"authorId\":\"1680075\",\"name\":\"Nikolaos Pappas\"},{\"authorId\":\"46244238\",\"name\":\"Phoebe Mulcaire\"},{\"authorId\":\"144365875\",\"name\":\"Noah A. Smith\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.96\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"53d5930ecd9dcc3eb79ef576f61fea248602e850\",\"title\":\"Grounded Compositional Outputs for Adaptive Language Modeling\",\"url\":\"https://www.semanticscholar.org/paper/53d5930ecd9dcc3eb79ef576f61fea248602e850\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2007.14062\",\"authors\":[{\"authorId\":\"1769795177\",\"name\":\"Manzil Zaheer\"},{\"authorId\":\"1947314\",\"name\":\"Guru Guruganesh\"},{\"authorId\":\"89890133\",\"name\":\"Kumar Avinava Dubey\"},{\"authorId\":\"1643737606\",\"name\":\"Joshua Ainslie\"},{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"},{\"authorId\":\"38552691\",\"name\":\"Philip Pham\"},{\"authorId\":\"101210026\",\"name\":\"Anirudh Ravula\"},{\"authorId\":\"145196279\",\"name\":\"Qifan Wang\"},{\"authorId\":\"1624030637\",\"name\":\"L. Yang\"},{\"authorId\":\"143629707\",\"name\":\"Amr Ahmed\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3\",\"title\":\"Big Bird: Transformers for Longer Sequences\",\"url\":\"https://www.semanticscholar.org/paper/044e13d7dd4e0655eb76f0bd00b2c1bdb44e2be3\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2004.07320\",\"authors\":[{\"authorId\":\"4861083\",\"name\":\"A. Fan\"},{\"authorId\":\"37502184\",\"name\":\"P. Stock\"},{\"authorId\":\"143853801\",\"name\":\"B. Graham\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"46307001\",\"name\":\"R. Gribonval\"},{\"authorId\":\"1681054\",\"name\":\"H. J\\u00e9gou\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0171ad4cc87cc7db25b4ec3169e293eed9a13b39\",\"title\":\"Training with Quantization Noise for Extreme Model Compression\",\"url\":\"https://www.semanticscholar.org/paper/0171ad4cc87cc7db25b4ec3169e293eed9a13b39\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.03356\",\"authors\":[{\"authorId\":\"1754374187\",\"name\":\"Jack Rae\"},{\"authorId\":\"153234306\",\"name\":\"A. Razavi\"}],\"doi\":\"10.18653/v1/2020.acl-main.672\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3df83a60f55c64b40e6dbcd99cf9f67894a0736e\",\"title\":\"Do Transformers Need Deep Long-Range Memory?\",\"url\":\"https://www.semanticscholar.org/paper/3df83a60f55c64b40e6dbcd99cf9f67894a0736e\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef78ffbb79353ecb6e9a1cd7c4e20a886fc1b470\",\"title\":\"COMBINER: INDUCTIVELY LEARNING TREE STRUC-\",\"url\":\"https://www.semanticscholar.org/paper/ef78ffbb79353ecb6e9a1cd7c4e20a886fc1b470\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1907.01470\",\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"1830914\",\"name\":\"Guillaume Lample\"},{\"authorId\":\"1681054\",\"name\":\"H. J\\u00e9gou\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"830995ef17cc291c13f42dfd9f462137de1d2179\",\"title\":\"Augmenting Self-attention with Persistent Memory\",\"url\":\"https://www.semanticscholar.org/paper/830995ef17cc291c13f42dfd9f462137de1d2179\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2002.09402\",\"authors\":[{\"authorId\":\"4861083\",\"name\":\"A. Fan\"},{\"authorId\":\"46183616\",\"name\":\"Thibaut Lavril\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8890eeda67d02117a589b0ba41c69419c40c7d5e\",\"title\":\"Accessing Higher-level Representations in Sequential Transformers with Feedback Memory\",\"url\":\"https://www.semanticscholar.org/paper/8890eeda67d02117a589b0ba41c69419c40c7d5e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.07486\",\"authors\":[{\"authorId\":\"51229603\",\"name\":\"Prajjwal Bhargava\"}],\"doi\":\"10.18653/v1/2020.acl-srw.1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43ea1db66536f930bde2aa18d1224e52ec95418d\",\"title\":\"Adaptive Transformers for Learning Multimodal Representations\",\"url\":\"https://www.semanticscholar.org/paper/43ea1db66536f930bde2aa18d1224e52ec95418d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2011.14203\",\"authors\":[{\"authorId\":\"1387249067\",\"name\":\"Thierry Tambe\"},{\"authorId\":\"2029486869\",\"name\":\"Coleman Hooper\"},{\"authorId\":\"3329554\",\"name\":\"L. Pentecost\"},{\"authorId\":\"38866509\",\"name\":\"En-Yu Yang\"},{\"authorId\":\"95873043\",\"name\":\"M. Donato\"},{\"authorId\":\"51918868\",\"name\":\"Victor Sanh\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"},{\"authorId\":\"1877046993\",\"name\":\"D. Brooks\"},{\"authorId\":\"1879090052\",\"name\":\"Gu-Yeon Wei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"65aebca660a5f95579041f28f4138299e3dff032\",\"title\":\"EdgeBERT: Optimizing On-Chip Inference for Multi-Task NLP\",\"url\":\"https://www.semanticscholar.org/paper/65aebca660a5f95579041f28f4138299e3dff032\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.05315\",\"authors\":[{\"authorId\":\"1432234296\",\"name\":\"Giannis Daras\"},{\"authorId\":\"143808231\",\"name\":\"Nikita Kitaev\"},{\"authorId\":\"2624088\",\"name\":\"Augustus Odena\"},{\"authorId\":\"1718469\",\"name\":\"A. Dimakis\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0f709acf38eb27702b0fbce1215db0ebaa2de2b\",\"title\":\"SMYRF: Efficient Attention using Asymmetric Clustering\",\"url\":\"https://www.semanticscholar.org/paper/c0f709acf38eb27702b0fbce1215db0ebaa2de2b\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2010.12683\",\"authors\":[{\"authorId\":\"88939986\",\"name\":\"Jyun-Yu Jiang\"},{\"authorId\":\"144628574\",\"name\":\"Chenyan Xiong\"},{\"authorId\":\"1820785734\",\"name\":\"Chia-Jung Lee\"},{\"authorId\":\"115438571\",\"name\":\"W. Wang\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.412\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9eb7860ae9777997ed17f6de623b0ab08cfc2df5\",\"title\":\"Long Document Ranking with Query-Directed Sparse Transformer\",\"url\":\"https://www.semanticscholar.org/paper/9eb7860ae9777997ed17f6de623b0ab08cfc2df5\",\"venue\":\"EMNLP\",\"year\":2020}],\"corpusId\":159041867,\"doi\":\"10.18653/v1/P19-1032\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":7,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"f4238bd2385a52413ccbacfd9e409a650235bd13\",\"references\":[{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1808.04444\",\"authors\":[{\"authorId\":\"1388360943\",\"name\":\"Rami Al-Rfou\"},{\"authorId\":\"51221461\",\"name\":\"Dokook Choe\"},{\"authorId\":\"40832517\",\"name\":\"Noah Constant\"},{\"authorId\":\"51150315\",\"name\":\"Mandy Guo\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"}],\"doi\":\"10.1609/aaai.v33i01.33013159\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b9de9599d7241459db9213b5cdd7059696f5ef8d\",\"title\":\"Character-Level Language Modeling with Deeper Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/b9de9599d7241459db9213b5cdd7059696f5ef8d\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1901.02860\",\"authors\":[{\"authorId\":\"3422912\",\"name\":\"Zihang Dai\"},{\"authorId\":\"47087291\",\"name\":\"Z. Yang\"},{\"authorId\":\"35729970\",\"name\":\"Yiming Yang\"},{\"authorId\":\"143712374\",\"name\":\"J. Carbonell\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":\"10.18653/v1/P19-1285\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c4744a7c2bb298e4a52289a1e085c71cc3d37bc6\",\"title\":\"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context\",\"url\":\"https://www.semanticscholar.org/paper/c4744a7c2bb298e4a52289a1e085c71cc3d37bc6\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1603.08983\",\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"04cca8e341a5da42b29b0bc831cb25a0f784fa01\",\"title\":\"Adaptive Computation Time for Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/04cca8e341a5da42b29b0bc831cb25a0f784fa01\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e\",\"title\":\"End-To-End Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1803.02155\",\"authors\":[{\"authorId\":\"38759328\",\"name\":\"Peter Shaw\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"}],\"doi\":\"10.18653/v1/N18-2074\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c8efcc854d97dfc2a42b83316a2109f9d166e43f\",\"title\":\"Self-Attention with Relative Position Representations\",\"url\":\"https://www.semanticscholar.org/paper/c8efcc854d97dfc2a42b83316a2109f9d166e43f\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7412686\",\"name\":\"Raphael Shu\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":\"10.18653/v1/W17-3201\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f902ce46b2e1307fe4d0895e2a99ad8399f47d40\",\"title\":\"An Empirical Study of Adequate Vision Span for Attention-Based Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/f902ce46b2e1307fe4d0895e2a99ad8399f47d40\",\"venue\":\"NMT@ACL\",\"year\":2017},{\"arxivId\":\"1508.04025\",\"authors\":[{\"authorId\":\"1821711\",\"name\":\"Thang Luong\"},{\"authorId\":\"143950636\",\"name\":\"Hieu Pham\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.18653/v1/D15-1166\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93499a7c7f699b6630a86fad964536f9423bb6d0\",\"title\":\"Effective Approaches to Attention-based Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/93499a7c7f699b6630a86fad964536f9423bb6d0\",\"venue\":\"EMNLP\",\"year\":2015},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zihang Dai\"},{\"authorId\":null,\"name\":\"Zhilin Yang\"},{\"authorId\":null,\"name\":\"Yiming Yang\"},{\"authorId\":null,\"name\":\"Jaime G. Carbonell\"},{\"authorId\":null,\"name\":\"Quoc V. Le\"},{\"authorId\":null,\"name\":\"Ruslan Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Transformer-xl: Attentive language\",\"url\":\"\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1611.06188\",\"authors\":[{\"authorId\":\"2262249\",\"name\":\"Yacine Jernite\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":null,\"name\":\"Tomas Mikolov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6746a18b2820f757334f75bc95428b3ea58d6603\",\"title\":\"Variable Computation in Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6746a18b2820f757334f75bc95428b3ea58d6603\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matt Mahoney.\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Large text compression benchmark\",\"url\":\"\",\"venue\":\"URL: http://www. mattmahoney. net/text/text. html.\",\"year\":2011}],\"title\":\"Adaptive Attention Span in Transformers\",\"topics\":[{\"topic\":\"Transformer\",\"topicId\":\"6977\",\"url\":\"https://www.semanticscholar.org/topic/6977\"},{\"topic\":\"Transformers\",\"topicId\":\"927204\",\"url\":\"https://www.semanticscholar.org/topic/927204\"},{\"topic\":\"Memory footprint\",\"topicId\":\"158486\",\"url\":\"https://www.semanticscholar.org/topic/158486\"},{\"topic\":\"Language model\",\"topicId\":\"26812\",\"url\":\"https://www.semanticscholar.org/topic/26812\"},{\"topic\":\"Time complexity\",\"topicId\":\"3448\",\"url\":\"https://www.semanticscholar.org/topic/3448\"},{\"topic\":\"Performance\",\"topicId\":\"3097\",\"url\":\"https://www.semanticscholar.org/topic/3097\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"}],\"url\":\"https://www.semanticscholar.org/paper/f4238bd2385a52413ccbacfd9e409a650235bd13\",\"venue\":\"ACL\",\"year\":2019}\n"