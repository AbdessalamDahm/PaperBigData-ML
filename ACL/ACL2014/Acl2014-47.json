"{\"abstract\":\"We use single-agent and multi-agent Reinforcement Learning (RL) for learning dialogue policies in a resource allocation negotiation scenario. Two agents learn concurrently by interacting with each other without any need for simulated users (SUs) to train against or corpora to learn from. In particular, we compare the Qlearning, Policy Hill-Climbing (PHC) and Win or Learn Fast Policy Hill-Climbing (PHC-WoLF) algorithms, varying the scenario complexity (state space size), the number of training episodes, the learning rate, and the exploration rate. Our results show that generally Q-learning fails to converge whereas PHC and PHC-WoLF always converge and perform similarly. We also show that very high gradually decreasing exploration rates are required for convergence. We conclude that multiagent RL of dialogue policies is a promising alternative to using single-agent RL and SUs or learning directly from corpora.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\",\"url\":\"https://www.semanticscholar.org/author/3194430\"},{\"authorId\":\"46384245\",\"name\":\"Claire Nelson\",\"url\":\"https://www.semanticscholar.org/author/46384245\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\",\"url\":\"https://www.semanticscholar.org/author/144518646\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"31728436\",\"name\":\"Niklas Rach\"},{\"authorId\":\"1720942\",\"name\":\"W. Minker\"},{\"authorId\":\"2295429\",\"name\":\"Stefan Ultes\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"32b10b3101ecbcf29dc2974ce2b2bd699c95dcbf\",\"title\":\"Towards an Argumentative Dialogue System\",\"url\":\"https://www.semanticscholar.org/paper/32b10b3101ecbcf29dc2974ce2b2bd699c95dcbf\",\"venue\":\"CMNA@ICAIL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710287\",\"name\":\"Alexandros Papangelis\"},{\"authorId\":\"1800944\",\"name\":\"M. Kotti\"},{\"authorId\":\"3101750\",\"name\":\"Y. Stylianou\"}],\"doi\":\"10.1109/ICASSP.2017.7953110\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b44e59222c7401116d5469ff6b7f2cdd76ed521\",\"title\":\"Predicting dialogue success, naturalness, and length with acoustic features\",\"url\":\"https://www.semanticscholar.org/paper/3b44e59222c7401116d5469ff6b7f2cdd76ed521\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1806041\",\"name\":\"H. Cuay\\u00e1huitl\"},{\"authorId\":\"1796426\",\"name\":\"Simon Keizer\"},{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"}],\"doi\":\"10.1007/978-3-319-39402-2_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fce61fa969e3ff48773547a35ec01d012207a40\",\"title\":\"Learning to Trade in Strategic Board Games\",\"url\":\"https://www.semanticscholar.org/paper/4fce61fa969e3ff48773547a35ec01d012207a40\",\"venue\":\"CGW/GIGA@IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145849898\",\"name\":\"G. Xiao\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6c91f44110ea593478ed787fbde64677f182cc4\",\"title\":\"A Comparison of Reinforcement Learning Methodologies in Two-Party and Three-Party Negotiation Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/a6c91f44110ea593478ed787fbde64677f182cc4\",\"venue\":\"FLAIRS Conference\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105943\",\"name\":\"I. Efstathiou\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"813d64aee4459966764c26c27ffd971be1fab9d3\",\"title\":\"Reinforcement learning for trading dialogue agents in non-cooperative negotiations\",\"url\":\"https://www.semanticscholar.org/paper/813d64aee4459966764c26c27ffd971be1fab9d3\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3228958\",\"name\":\"A. Malchanau\"},{\"authorId\":\"144090767\",\"name\":\"V. Petukhova\"},{\"authorId\":\"1786202\",\"name\":\"H. Bunt\"}],\"doi\":\"10.5087/DAD.2018.202\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2292f8ce4906ccf633c8ebf5045f043215d57c50\",\"title\":\"Towards Integration of Cognitive Models in Dialogue Management: Designing the Virtual Negotiation Coach Application\",\"url\":\"https://www.semanticscholar.org/paper/2292f8ce4906ccf633c8ebf5045f043215d57c50\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710287\",\"name\":\"Alexandros Papangelis\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"}],\"doi\":\"10.18653/v1/W15-4621\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f4b364973a6447e00300b0a5c97482890fb94eb6\",\"title\":\"Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies\",\"url\":\"https://www.semanticscholar.org/paper/f4b364973a6447e00300b0a5c97482890fb94eb6\",\"venue\":\"SIGDIAL Conference\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3450836\",\"name\":\"Merwan Barlier\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.18653/v1/w15-4602\",\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4f1343e4e501110adcba483a16ac2a75127bc11b\",\"title\":\"Human-Machine Dialogue as a Stochastic Game\",\"url\":\"https://www.semanticscholar.org/paper/4f1343e4e501110adcba483a16ac2a75127bc11b\",\"venue\":\"SIGDIAL Conference\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"3403009\",\"name\":\"Aude Genevay\"}],\"doi\":\"10.1007/978-981-10-2585-3_33\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"de3a43fd012ec7bd037b91302226bdb7e9434f60\",\"title\":\"The Negotiation Dialogue Game\",\"url\":\"https://www.semanticscholar.org/paper/de3a43fd012ec7bd037b91302226bdb7e9434f60\",\"venue\":\"IWSDS\",\"year\":2016},{\"arxivId\":\"1907.05507\",\"authors\":[{\"authorId\":\"1710287\",\"name\":\"Alexandros Papangelis\"},{\"authorId\":null,\"name\":\"Yi-Chia Wang\"},{\"authorId\":\"34890911\",\"name\":\"Piero Molino\"},{\"authorId\":\"1748051\",\"name\":\"G. T\\u00fcr\"}],\"doi\":\"10.18653/v1/W19-5912\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e3adfc4de6e65ffb8e179790c5693aac0af2cfff\",\"title\":\"Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3adfc4de6e65ffb8e179790c5693aac0af2cfff\",\"venue\":\"SIGdial\",\"year\":2019},{\"arxivId\":\"1809.07066\",\"authors\":[{\"authorId\":\"31622211\",\"name\":\"V. Sunder\"},{\"authorId\":\"3213990\",\"name\":\"L. Vig\"},{\"authorId\":\"48415127\",\"name\":\"A. Chatterjee\"},{\"authorId\":\"143725466\",\"name\":\"G. Shroff\"}],\"doi\":\"10.1007/978-981-15-5869-6_5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa134d9e696c5fb49dba2937c7f1ac17504d76c4\",\"title\":\"Prosocial or Selfish? Agents with different behaviors for Contract Negotiation using Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/aa134d9e696c5fb49dba2937c7f1ac17504d76c4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3223669\",\"name\":\"Wendong Ge\"},{\"authorId\":null,\"name\":\"Bo Xu\"}],\"doi\":\"10.18653/v1/W15-4648\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c7103cbe504fe1d5738328bf9c62833f5f5f0234\",\"title\":\"Dialogue Management based on Multi-domain Corpus\",\"url\":\"https://www.semanticscholar.org/paper/c7103cbe504fe1d5738328bf9c62833f5f5f0234\",\"venue\":\"SIGDIAL Conference\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40544534\",\"name\":\"P. Chaffey\"},{\"authorId\":\"122518132\",\"name\":\"Ron Artstein\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"1852064\",\"name\":\"K. A. Pollard\"},{\"authorId\":\"40353538\",\"name\":\"Setareh Nasihati Gilani\"},{\"authorId\":\"46622929\",\"name\":\"David M. Krum\"},{\"authorId\":\"144859571\",\"name\":\"D. Nelson\"},{\"authorId\":\"47699420\",\"name\":\"Kevin Huynh\"},{\"authorId\":\"49406416\",\"name\":\"Alesia Gainer\"},{\"authorId\":\"1725411556\",\"name\":\"Seyed Hossein Alavi\"},{\"authorId\":\"2860406\",\"name\":\"Rhys Yahata\"},{\"authorId\":\"3201827\",\"name\":\"A. Leuski\"},{\"authorId\":\"98882740\",\"name\":\"V. Yanov\"},{\"authorId\":\"1784659\",\"name\":\"D. Traum\"}],\"doi\":\"10.1117/12.2557573\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"135edd73dbed8a50d0075416cdc403c7fb8b1b5c\",\"title\":\"Human swarm interaction using plays, audibles, and a virtual spokesperson\",\"url\":\"https://www.semanticscholar.org/paper/135edd73dbed8a50d0075416cdc403c7fb8b1b5c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753737454\",\"name\":\"Parth Thosani\"},{\"authorId\":\"1753737450\",\"name\":\"Manas Sinkar\"},{\"authorId\":\"1753737452\",\"name\":\"Jaydeep Vaghasiya\"},{\"authorId\":\"2812129\",\"name\":\"R. Shankarmani\"}],\"doi\":\"10.1109/ICICCS48265.2020.9120912\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd4ef554c57ae58be87fb0d543811757ccc875dd\",\"title\":\"A Self Learning Chat-Bot From User Interactions and Preferences\",\"url\":\"https://www.semanticscholar.org/paper/fd4ef554c57ae58be87fb0d543811757ccc875dd\",\"venue\":\"2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS)\",\"year\":2020},{\"arxivId\":\"1709.06136\",\"authors\":[{\"authorId\":\"143830417\",\"name\":\"Bing Liu\"},{\"authorId\":\"5347612\",\"name\":\"I. Lane\"}],\"doi\":\"10.1109/ASRU.2017.8268975\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93040b08acb1fcaf7564e843aa0433c048a8d638\",\"title\":\"Iterative policy learning in end-to-end trainable task-oriented neural dialog models\",\"url\":\"https://www.semanticscholar.org/paper/93040b08acb1fcaf7564e843aa0433c048a8d638\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3027595\",\"name\":\"T. Hiraoka\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"2074997\",\"name\":\"E. Nouri\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\"},{\"authorId\":\"145223960\",\"name\":\"S. Nakamura\"}],\"doi\":\"10.18653/v1/W15-4605\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"feb4245aa7eaec9eaf225cc0d0676e41b9308a37\",\"title\":\"Reinforcement Learning in Multi-Party Trading Dialog\",\"url\":\"https://www.semanticscholar.org/paper/feb4245aa7eaec9eaf225cc0d0676e41b9308a37\",\"venue\":\"SIGDIAL Conference\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1692242\",\"name\":\"B. Grosz\"}],\"doi\":\"10.1162/COLI_a_00313\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"53a599e03d02447104487cc259f0e79c21255482\",\"title\":\"Smart Enough to Talk With Us? Foundations and Challenges for Dialogue Capable AI Systems\",\"url\":\"https://www.semanticscholar.org/paper/53a599e03d02447104487cc259f0e79c21255482\",\"venue\":\"Computational Linguistics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40544534\",\"name\":\"P. Chaffey\"},{\"authorId\":\"2038490\",\"name\":\"Ron Artstein\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"1852064\",\"name\":\"K. A. Pollard\"},{\"authorId\":\"40353538\",\"name\":\"Setareh Nasihati Gilani\"},{\"authorId\":\"46622929\",\"name\":\"David M. Krum\"},{\"authorId\":\"46973789\",\"name\":\"D. Nelson\"},{\"authorId\":\"50722273\",\"name\":\"K. Huynh\"},{\"authorId\":\"49406416\",\"name\":\"Alesia Gainer\"},{\"authorId\":\"143654999\",\"name\":\"S. H. Alavi\"},{\"authorId\":\"2860406\",\"name\":\"Rhys Yahata\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d25237ead956b9a9331e2ff65168e62e06d0af4a\",\"title\":\"Developing a Virtual Reality Wildfire Simulation to Analyze Human Communication and Interaction with a Robotic Swarm During Emergencies\",\"url\":\"https://www.semanticscholar.org/paper/d25237ead956b9a9331e2ff65168e62e06d0af4a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3027595\",\"name\":\"T. Hiraoka\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"2074997\",\"name\":\"E. Nouri\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\"},{\"authorId\":\"145223960\",\"name\":\"S. Nakamura\"}],\"doi\":\"10.1527/TJSAI.B-FC1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa564e2f9e13bfab81ec43a13393ca267b3a4e56\",\"title\":\"Reinforcement Learning of Multi-Party Trading Dialog Policies\",\"url\":\"https://www.semanticscholar.org/paper/aa564e2f9e13bfab81ec43a13393ca267b3a4e56\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"2001.06463\",\"authors\":[{\"authorId\":\"1710287\",\"name\":\"Alexandros Papangelis\"},{\"authorId\":\"2171886\",\"name\":\"M. Namazifar\"},{\"authorId\":\"144011343\",\"name\":\"C. Khatri\"},{\"authorId\":null,\"name\":\"Yi-Chia Wang\"},{\"authorId\":\"34890911\",\"name\":\"Piero Molino\"},{\"authorId\":\"5108268\",\"name\":\"G. Tur\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1555808ba7d522cefe1518d4bd9e2cf4cc6389e\",\"title\":\"Plato Dialogue System: A Flexible Conversational AI Research Platform\",\"url\":\"https://www.semanticscholar.org/paper/e1555808ba7d522cefe1518d4bd9e2cf4cc6389e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39503804\",\"name\":\"Jiyun Luo\"},{\"authorId\":\"1797492\",\"name\":\"Xuchu Dong\"},{\"authorId\":\"10237116\",\"name\":\"G. Yang\"}],\"doi\":\"10.1145/2808194.2809468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5eb39988017c8ff6af6ed5a6b5befe558a29f90b\",\"title\":\"Learning to Reinforce Search Effectiveness\",\"url\":\"https://www.semanticscholar.org/paper/5eb39988017c8ff6af6ed5a6b5befe558a29f90b\",\"venue\":\"ICTIR\",\"year\":2015},{\"arxivId\":\"2004.03809\",\"authors\":[{\"authorId\":\"51055574\",\"name\":\"Ryuichi Takanobu\"},{\"authorId\":\"93513536\",\"name\":\"Runze Liang\"},{\"authorId\":\"1730108\",\"name\":\"Minlie Huang\"}],\"doi\":\"10.18653/v1/2020.acl-main.59\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c4e8f111b4cb6cdfe0f718d0d81ca138c0e9464e\",\"title\":\"Multi-Agent Task-Oriented Dialog Policy Learning with Role-Aware Reward Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/c4e8f111b4cb6cdfe0f718d0d81ca138c0e9464e\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3403009\",\"name\":\"Aude Genevay\"},{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2b4b3f3f6f2d6a2150f684ef562b47206b0433d9\",\"title\":\"Transfer Learning for User Adaptation in Spoken Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/2b4b3f3f6f2d6a2150f684ef562b47206b0433d9\",\"venue\":\"AAMAS\",\"year\":2016}],\"corpusId\":10681279,\"doi\":\"10.3115/v1/P14-1047\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"71b7fe9bb2a2f03a000b9d7e6b2bb7898a3be1da\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"1737616\",\"name\":\"D. Litman\"}],\"doi\":\"10.1016/j.specom.2008.05.002\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32b81042b423910a99fce9a3a92d4c44826e9100\",\"title\":\"A Reinforcement Learning approach to evaluating state representations in spoken dialogue systems\",\"url\":\"https://www.semanticscholar.org/paper/32b81042b423910a99fce9a3a92d4c44826e9100\",\"venue\":\"Speech Commun.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michail G. Lagoudakis\"},{\"authorId\":null,\"name\":\"Ronald Parr.\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Leastsquares policy iteration\",\"url\":\"\",\"venue\":\"Journal of Machine Learning Research, 4:1107\\u20131149.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1752407\",\"name\":\"P. Heeman\"}],\"doi\":\"10.1109/ASRU.2009.5373413\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e180a6f2b8dcb6f48988dae70b951f83b01dafa\",\"title\":\"Representing the Reinforcement Learning state in a negotiation dialogue\",\"url\":\"https://www.semanticscholar.org/paper/3e180a6f2b8dcb6f48988dae70b951f83b01dafa\",\"venue\":\"2009 IEEE Workshop on Automatic Speech Recognition & Understanding\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2057050\",\"name\":\"Y. Engel\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1766683\",\"name\":\"R. Meir\"}],\"doi\":\"10.1145/1102351.1102377\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba4bcd8c4ebb1427d335c23d50249d370778ae49\",\"title\":\"Reinforcement learning with Gaussian processes\",\"url\":\"https://www.semanticscholar.org/paper/ba4bcd8c4ebb1427d335c23d50249d370778ae49\",\"venue\":\"ICML '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a918a27162a5014206955f6d8823b7bf8ec1e3f2\",\"title\":\"Reinforcement Learning of Argumentation Dialogue Policies in Negotiation\",\"url\":\"https://www.semanticscholar.org/paper/a918a27162a5014206955f6d8823b7bf8ec1e3f2\",\"venue\":\"INTERSPEECH\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1768624\",\"name\":\"Milica Gasic\"},{\"authorId\":\"1789736\",\"name\":\"Filip Jurc\\u00edcek\"},{\"authorId\":\"145462220\",\"name\":\"B. Thomson\"},{\"authorId\":\"1736727\",\"name\":\"Kai Yu\"},{\"authorId\":\"145259603\",\"name\":\"S. Young\"}],\"doi\":\"10.1109/ASRU.2011.6163950\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f73259e33e0ca9372774d121aad351e3e37f457e\",\"title\":\"On-line policy optimisation of spoken dialogue systems via live interaction with human subjects\",\"url\":\"https://www.semanticscholar.org/paper/f73259e33e0ca9372774d121aad351e3e37f457e\",\"venue\":\"2011 IEEE Workshop on Automatic Speech Recognition & Understanding\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709638\",\"name\":\"L. Busoniu\"},{\"authorId\":\"1705222\",\"name\":\"Robert Babu\\u0161ka\"},{\"authorId\":\"1724741\",\"name\":\"B. D. Schutter\"}],\"doi\":\"10.1109/TSMCC.2007.913919\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4aece8df7bd59e2fbfedbf5729bba41abc56d870\",\"title\":\"A Comprehensive Survey of Multiagent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4aece8df7bd59e2fbfedbf5729bba41abc56d870\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"},{\"authorId\":\"1796426\",\"name\":\"Simon Keizer\"},{\"authorId\":\"31436700\",\"name\":\"X. Liu\"},{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5fde8a669445cb444d5e7aff49e1dca23ee1a75a\",\"title\":\"Adaptive Information Presentation for Spoken Dialogue Systems : Evaluation with human subjects\",\"url\":\"https://www.semanticscholar.org/paper/5fde8a669445cb444d5e7aff49e1dca23ee1a75a\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32639375\",\"name\":\"Teruhisa Misu\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"3201827\",\"name\":\"A. Leuski\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3dcb123c2fa47808e3f0e2af2c67c72d5da38914\",\"title\":\"Reinforcement Learning of Question-Answering Dialogue Policies for Virtual Museum Guides\",\"url\":\"https://www.semanticscholar.org/paper/3dcb123c2fa47808e3f0e2af2c67c72d5da38914\",\"venue\":\"SIGDIAL Conference\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1789736\",\"name\":\"Filip Jurc\\u00edcek\"},{\"authorId\":\"145462220\",\"name\":\"B. Thomson\"},{\"authorId\":\"145259603\",\"name\":\"S. Young\"}],\"doi\":\"10.1016/j.csl.2011.09.004\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"649aa08f540628f1fc02445526c661bae8396578\",\"title\":\"Reinforcement learning for parameter estimation in statistical spoken dialogue systems\",\"url\":\"https://www.semanticscholar.org/paper/649aa08f540628f1fc02445526c661bae8396578\",\"venue\":\"Comput. Speech Lang.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1768624\",\"name\":\"Milica Gasic\"},{\"authorId\":\"145133553\",\"name\":\"Matthew Henderson\"},{\"authorId\":\"145462220\",\"name\":\"B. Thomson\"},{\"authorId\":\"1934588\",\"name\":\"Pirros Tsiakoulis\"},{\"authorId\":\"145259603\",\"name\":\"S. Young\"}],\"doi\":\"10.1109/SLT.2012.6424165\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2753726fd32c1e4644ffb70e9af2784d4ba6c4f\",\"title\":\"Policy optimisation of POMDP-based dialogue systems without state space compression\",\"url\":\"https://www.semanticscholar.org/paper/e2753726fd32c1e4644ffb70e9af2784d4ba6c4f\",\"venue\":\"2012 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"144915758\",\"name\":\"James Henderson\"}],\"doi\":\"10.1109/SLT.2006.326774\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"190c34935b8939511bd821e089b77c9bc60c29f1\",\"title\":\"EVALUATING EFFECTIVENESS AND PORTABILITY OF REINFORCEMENT LEARNED DIALOGUE STRATEGIES WITH REAL USERS: THE TALK TOWNINFO EVALUATION\",\"url\":\"https://www.semanticscholar.org/paper/190c34935b8939511bd821e089b77c9bc60c29f1\",\"venue\":\"2006 IEEE Spoken Language Technology Workshop\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144771028\",\"name\":\"C. Claus\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38d35d5581e58dca4e9458501e65c1f85ca754d5\",\"title\":\"The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems\",\"url\":\"https://www.semanticscholar.org/paper/38d35d5581e58dca4e9458501e65c1f85ca754d5\",\"venue\":\"AAAI/IAAI\",\"year\":1998},{\"arxivId\":\"1106.0676\",\"authors\":[{\"authorId\":\"113303087\",\"name\":\"M. Kearns\"},{\"authorId\":\"123019850\",\"name\":\"D. Litman\"},{\"authorId\":\"100689924\",\"name\":\"S. Singh\"},{\"authorId\":\"49523502\",\"name\":\"M. Walker\"}],\"doi\":\"10.1613/jair.859\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb53b7c13156e3acacb47c1e51d93cefeabfaeb0\",\"title\":\"Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System\",\"url\":\"https://www.semanticscholar.org/paper/eb53b7c13156e3acacb47c1e51d93cefeabfaeb0\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1784072\",\"name\":\"M. Lagoudakis\"},{\"authorId\":\"145726861\",\"name\":\"R. Parr\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b750a17921d32936425e05f8b00b96569e2fc5a6\",\"title\":\"Least-Squares Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/b750a17921d32936425e05f8b00b96569e2fc5a6\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32639375\",\"name\":\"Teruhisa Misu\"},{\"authorId\":\"2332462\",\"name\":\"K. Sugiura\"},{\"authorId\":\"34942650\",\"name\":\"K. Ohtake\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"1799065\",\"name\":\"Hideki Kashioka\"},{\"authorId\":\"1805595\",\"name\":\"H. Kawai\"},{\"authorId\":\"145223960\",\"name\":\"S. Nakamura\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f83d3dfc6c47c28284ac9bc18832038c8af79200\",\"title\":\"Modeling Spoken Decision Making Dialogue and Optimization of its Dialogue Strategy\",\"url\":\"https://www.semanticscholar.org/paper/f83d3dfc6c47c28284ac9bc18832038c8af79200\",\"venue\":\"SIGDIAL Conference\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1768624\",\"name\":\"Milica Gasic\"},{\"authorId\":\"3027805\",\"name\":\"C. Breslin\"},{\"authorId\":\"145133553\",\"name\":\"Matthew Henderson\"},{\"authorId\":\"144184704\",\"name\":\"Dongho Kim\"},{\"authorId\":\"1778989\",\"name\":\"Martin Szummer\"},{\"authorId\":\"145462220\",\"name\":\"B. Thomson\"},{\"authorId\":\"1934588\",\"name\":\"Pirros Tsiakoulis\"},{\"authorId\":\"145259603\",\"name\":\"S. Young\"}],\"doi\":\"10.1109/ICASSP.2013.6639297\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"1e0ede6f60b1106070e041211133c634a2e4f991\",\"title\":\"On-line policy optimisation of Bayesian spoken dialogue systems via human interaction\",\"url\":\"https://www.semanticscholar.org/paper/1e0ede6f60b1106070e041211133c634a2e4f991\",\"venue\":\"2013 IEEE International Conference on Acoustics, Speech and Signal Processing\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144915758\",\"name\":\"James Henderson\"},{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"}],\"doi\":\"10.1162/coli.2008.07-028-R2-05-82\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab71bdfb49d4ccafce2e345893bb29284905dc3b\",\"title\":\"Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets\",\"url\":\"https://www.semanticscholar.org/paper/ab71bdfb49d4ccafce2e345893bb29284905dc3b\",\"venue\":\"Computational Linguistics\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2074997\",\"name\":\"E. Nouri\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"144518646\",\"name\":\"D. Traum\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"184ecd47d94d27711fe81c168b23096766a7b47a\",\"title\":\"A Cultural Decision-Making Model for Negotiation based on Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/184ecd47d94d27711fe81c168b23096766a7b47a\",\"venue\":\"CogSci\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2339084\",\"name\":\"Lucie Daubigney\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"33778205\",\"name\":\"S. Chandramohan\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.1109/JSTSP.2012.2229257\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1e0dbdfc5d458f9206f4086733f48f39c2fbd51\",\"title\":\"A Comprehensive Reinforcement Learning Framework for Dialogue Management Optimization\",\"url\":\"https://www.semanticscholar.org/paper/b1e0dbdfc5d458f9206f4086733f48f39c2fbd51\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1794606\",\"name\":\"Praveen Paruchuri\"},{\"authorId\":\"48337881\",\"name\":\"Nilanjan Chakraborty\"},{\"authorId\":\"1713240\",\"name\":\"Roie Zivan\"},{\"authorId\":\"9076478\",\"name\":\"K. Sycara\"},{\"authorId\":\"144652072\",\"name\":\"M. Dud\\u00edk\"},{\"authorId\":\"122494419\",\"name\":\"Geoff Gordon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"889595d2eebd7f630fe67e743c9006916cb38d6c\",\"title\":\"POMDP based Negotiation Modeling\",\"url\":\"https://www.semanticscholar.org/paper/889595d2eebd7f630fe67e743c9006916cb38d6c\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50032031\",\"name\":\"Yi Ma\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"30fdc360e5d4d2357f5ddca757fd9d7627a6fced\",\"title\":\"User Goal Change Model for Spoken Dialog State Tracking\",\"url\":\"https://www.semanticscholar.org/paper/30fdc360e5d4d2357f5ddca757fd9d7627a6fced\",\"venue\":\"HLT-NAACL\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3274000\",\"name\":\"J. Hu\"},{\"authorId\":\"1796536\",\"name\":\"Michael P. Wellman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"770393848fc170d92fa4666f192c8e86dd5e02b0\",\"title\":\"Multiagent Reinforcement Learning: Theoretical Framework and an Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/770393848fc170d92fa4666f192c8e86dd5e02b0\",\"venue\":\"ICML\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1016/S0004-3702(02)00121-2\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"05e4f9007e75d3e426ee2d6414a852e9cca8bcb2\",\"title\":\"Multiagent learning using a variable learning rate\",\"url\":\"https://www.semanticscholar.org/paper/05e4f9007e75d3e426ee2d6414a852e9cca8bcb2\",\"venue\":\"Artif. Intell.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84c909eba11f2041c14a324b5d46809ccdba6635\",\"title\":\"Reinforcement Learning of Two-Issue Negotiation Dialogue Policies\",\"url\":\"https://www.semanticscholar.org/paper/84c909eba11f2041c14a324b5d46809ccdba6635\",\"venue\":\"SIGDIAL Conference\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3472959\",\"name\":\"C. Rasmussen\"},{\"authorId\":\"145715698\",\"name\":\"C. K. Williams\"}],\"doi\":\"10.1007/978-3-540-28650-9_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82266f6103bade9005ec555ed06ba20b5210ff22\",\"title\":\"Gaussian Processes for Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/82266f6103bade9005ec555ed06ba20b5210ff22\",\"venue\":\"Adaptive computation and machine learning\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"},{\"authorId\":\"1796426\",\"name\":\"Simon Keizer\"},{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"},{\"authorId\":\"31436700\",\"name\":\"X. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"043dcd9a0f7eca08b7b5287243db1bbc9b155806\",\"title\":\"Adaptive Information Presentation for Spoken Dialogue Systems: Evaluation with real users\",\"url\":\"https://www.semanticscholar.org/paper/043dcd9a0f7eca08b7b5287243db1bbc9b155806\",\"venue\":\"ENLG\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"2033965\",\"name\":\"M. Wolters\"},{\"authorId\":\"2050835\",\"name\":\"J. Moore\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd02d7d0384bcce75c5ae381750c3e60323de9c5\",\"title\":\"Learning Dialogue Strategies from Older and Younger Simulated Users\",\"url\":\"https://www.semanticscholar.org/paper/dd02d7d0384bcce75c5ae381750c3e60323de9c5\",\"venue\":\"SIGDIAL Conference\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33778205\",\"name\":\"S. Chandramohan\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"50653913\",\"name\":\"F. Lef\\u00e8vre\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.1007/978-1-4614-8280-2_31\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f18045c83377225243d1f0045dbbf441998f003\",\"title\":\"Co-adaptation in Spoken Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/0f18045c83377225243d1f0045dbbf441998f003\",\"venue\":\"Natural Interaction with Robots, Knowbots and Smartphones, Putting Spoken Dialog Systems into Practice\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1805256\",\"name\":\"J. Schatzmann\"},{\"authorId\":\"1777891\",\"name\":\"K. Weilhammer\"},{\"authorId\":\"2568798\",\"name\":\"Matthew N. Stuttle\"},{\"authorId\":\"145259603\",\"name\":\"S. Young\"}],\"doi\":\"10.1017/S0269888906000944\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e3d8e925ec4f938620a8d76bb50a1b2fc95e7f5e\",\"title\":\"A survey of statistical user simulation techniques for reinforcement-learning of dialogue management strategies\",\"url\":\"https://www.semanticscholar.org/paper/e3d8e925ec4f938620a8d76bb50a1b2fc95e7f5e\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"},{\"authorId\":\"144915758\",\"name\":\"James Henderson\"},{\"authorId\":\"1782798\",\"name\":\"Oliver Lemon\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f89760c4fc1aadbef441a6e1fe6ce0b9411f1c38\",\"title\":\"User simulation for spoken dialogue systems: learning and evaluation\",\"url\":\"https://www.semanticscholar.org/paper/f89760c4fc1aadbef441a6e1fe6ce0b9411f1c38\",\"venue\":\"INTERSPEECH\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49317886\",\"name\":\"H. Ai\"},{\"authorId\":\"1737616\",\"name\":\"D. Litman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b7eabd3f48c62240465af8ec3e34d26eeaac64c5\",\"title\":\"Assessing Dialog System User Simulation Evaluation Measures Using Human Judges\",\"url\":\"https://www.semanticscholar.org/paper/b7eabd3f48c62240465af8ec3e34d26eeaac64c5\",\"venue\":\"ACL\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145462220\",\"name\":\"B. Thomson\"},{\"authorId\":\"145259603\",\"name\":\"S. Young\"}],\"doi\":\"10.1016/j.csl.2009.07.003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b99d66f53eed00320320acce63a22f0c48465d8b\",\"title\":\"Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems\",\"url\":\"https://www.semanticscholar.org/paper/b99d66f53eed00320320acce63a22f0c48465d8b\",\"venue\":\"Comput. Speech Lang.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145372005\",\"name\":\"M. English\"},{\"authorId\":\"1752407\",\"name\":\"P. Heeman\"}],\"doi\":\"10.3115/1220575.1220702\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"87b2f3ab89fc9231cfa6f8a17f4253de67c4c3d8\",\"title\":\"Learning Mixed Initiative Dialog Strategies By Using Reinforcement Learning On Both Conversants\",\"url\":\"https://www.semanticscholar.org/paper/87b2f3ab89fc9231cfa6f8a17f4253de67c4c3d8\",\"venue\":\"HLT/EMNLP\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50293251\",\"name\":\"J. Williams\"},{\"authorId\":\"96397022\",\"name\":\"S. Young\"}],\"doi\":\"10.1109/TASL.2007.902050\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"989b971dffe1242e27f9d4ef4b9f430fe0c62f49\",\"title\":\"Scaling POMDPs for Spoken Dialog Management\",\"url\":\"https://www.semanticscholar.org/paper/989b971dffe1242e27f9d4ef4b9f430fe0c62f49\",\"venue\":\"IEEE Transactions on Audio, Speech, and Language Processing\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710287\",\"name\":\"Alexandros Papangelis\"},{\"authorId\":\"3194430\",\"name\":\"Kallirroi Georgila\"}],\"doi\":\"10.18653/v1/W15-4621\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4b364973a6447e00300b0a5c97482890fb94eb6\",\"title\":\"Reinforcement Learning of Multi-Issue Negotiation Dialogue Policies\",\"url\":\"https://www.semanticscholar.org/paper/f4b364973a6447e00300b0a5c97482890fb94eb6\",\"venue\":\"SIGDIAL Conference\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1731937\",\"name\":\"Min Chi\"},{\"authorId\":\"1797292\",\"name\":\"K. VanLehn\"},{\"authorId\":\"1737616\",\"name\":\"D. Litman\"},{\"authorId\":\"1730942\",\"name\":\"P. Jordan\"}],\"doi\":\"10.1007/s11257-010-9093-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7a12907a5502ab31132adb41b73a5220cb26161\",\"title\":\"Empirically evaluating the application of reinforcement learning to the induction of effective and adaptive pedagogical strategies\",\"url\":\"https://www.semanticscholar.org/paper/e7a12907a5502ab31132adb41b73a5220cb26161\",\"venue\":\"User Modeling and User-Adapted Interaction\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1015330.1015430\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"title\":\"Apprenticeship learning via inverse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1691444\",\"name\":\"H. Hastie\"}],\"doi\":\"10.1017/S0269888912000343\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d5bf1c9d46bc3158bda10874c387218b299ff53\",\"title\":\"A survey on metrics for the evaluation of user simulations\",\"url\":\"https://www.semanticscholar.org/paper/1d5bf1c9d46bc3158bda10874c387218b299ff53\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Ga\\u0161i\\u0107\"},{\"authorId\":null,\"name\":\"Filip Jur\\u010d\\u0131\\u0301\\u010dek\"},{\"authorId\":null,\"name\":\"Blaise Thomson\"},{\"authorId\":null,\"name\":\"Kai Yu\"},{\"authorId\":null,\"name\":\"Steve Young\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"On-line policy optimisation\",\"url\":\"\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"47271859\",\"name\":\"J. Williams\"},{\"authorId\":\"5246203\",\"name\":\"S. Balakrishnan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a950d7836e101e7d649791714d8383a804a6f671\",\"title\":\"Reinforcement learning for dialog management using least-squares Policy iteration and fast feature selection\",\"url\":\"https://www.semanticscholar.org/paper/a950d7836e101e7d649791714d8383a804a6f671\",\"venue\":\"INTERSPEECH\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50027-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"title\":\"Markov Games as a Framework for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73745042\",\"name\":\"Heriberto Cuay\"},{\"authorId\":\"3198238\",\"name\":\"Nina Dethlefs\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cb2e78addc22ce2e7bb405f1baa9aa555118f4ec\",\"title\":\"Hierarchical Multiagent Reinforcement Learning for Coordinating Verbal and Non-verbal Actions in Robots\",\"url\":\"https://www.semanticscholar.org/paper/cb2e78addc22ce2e7bb405f1baa9aa555118f4ec\",\"venue\":\"\",\"year\":2012}],\"title\":\"Single-Agent vs. Multi-Agent Techniques for Concurrent Reinforcement Learning of Negotiation Dialogue Policies\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Text corpus\",\"topicId\":\"14829\",\"url\":\"https://www.semanticscholar.org/topic/14829\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Converge\",\"topicId\":\"205534\",\"url\":\"https://www.semanticscholar.org/topic/205534\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Hill climbing\",\"topicId\":\"51492\",\"url\":\"https://www.semanticscholar.org/topic/51492\"},{\"topic\":\"State space\",\"topicId\":\"6115\",\"url\":\"https://www.semanticscholar.org/topic/6115\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Agent-based model\",\"topicId\":\"4774\",\"url\":\"https://www.semanticscholar.org/topic/4774\"}],\"url\":\"https://www.semanticscholar.org/paper/71b7fe9bb2a2f03a000b9d7e6b2bb7898a3be1da\",\"venue\":\"ACL\",\"year\":2014}\n"