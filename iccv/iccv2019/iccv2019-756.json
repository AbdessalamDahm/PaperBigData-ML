"{\"abstract\":\"Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.\",\"arxivId\":\"1904.01766\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\",\"url\":\"https://www.semanticscholar.org/author/144762505\"},{\"authorId\":\"49588480\",\"name\":\"A. Myers\",\"url\":\"https://www.semanticscholar.org/author/49588480\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\",\"url\":\"https://www.semanticscholar.org/author/1856025\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\",\"url\":\"https://www.semanticscholar.org/author/1702318\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\",\"url\":\"https://www.semanticscholar.org/author/2462253\"}],\"citationVelocity\":93,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"39356242\",\"name\":\"Andrea Amelio Ravelli\"},{\"authorId\":\"1715983\",\"name\":\"Oier Lopez de Lacalle\"},{\"authorId\":\"1733049\",\"name\":\"Eneko Agirre\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42238855eb05afbc3cd9b68fb673eb8190c1c60f\",\"title\":\"A Comparison of Representation Models in a Non-Conventional Semantic Similarity Scenario\",\"url\":\"https://www.semanticscholar.org/paper/42238855eb05afbc3cd9b68fb673eb8190c1c60f\",\"venue\":\"CLiC-it\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"50172036\",\"name\":\"X. Wu\"},{\"authorId\":\"1993634137\",\"name\":\"Shen Ge\"},{\"authorId\":\"47958349\",\"name\":\"X. Zhang\"},{\"authorId\":\"144934703\",\"name\":\"Wei Fan\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":\"10.1145/3394171.3414004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"title\":\"Bridging the Gap between Vision and Language Domains for Improved Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0f24a830cdacb07415f335784cd0a7e81eb42f3a\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1455114388\",\"name\":\"Hrituraj Singh\"},{\"authorId\":\"35223379\",\"name\":\"Sumit Shekhar\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.264\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"baff151a5f1cb378d3627b88ff7d1b3afd29db45\",\"title\":\"STL-CQA: Structure-based Transformers with Localization and Encoding for Chart Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/baff151a5f1cb378d3627b88ff7d1b3afd29db45\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2011.13917\",\"authors\":[{\"authorId\":\"13700560\",\"name\":\"J. J. Sun\"},{\"authorId\":\"3413206\",\"name\":\"A. Kennedy\"},{\"authorId\":\"12094027\",\"name\":\"Eric Zhan\"},{\"authorId\":\"1740159\",\"name\":\"Yisong Yue\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"624ab884243e3c9e97ac159064d4ad3ecbe19ff1\",\"title\":\"Task Programming: Learning Data Efficient Behavior Representations\",\"url\":\"https://www.semanticscholar.org/paper/624ab884243e3c9e97ac159064d4ad3ecbe19ff1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38273401\",\"name\":\"X. Wu\"},{\"authorId\":\"144395119\",\"name\":\"D. Chen\"},{\"authorId\":\"143605211\",\"name\":\"Yuan He\"},{\"authorId\":\"49930315\",\"name\":\"H. Xue\"},{\"authorId\":\"1727111\",\"name\":\"Mingli Song\"},{\"authorId\":\"1505834084\",\"name\":\"Feng Mao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb8a2f20aff35966e7e40fc5c4e398a6287e0570\",\"title\":\"Hybrid Sequence Encoder for Text Based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/eb8a2f20aff35966e7e40fc5c4e398a6287e0570\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"153731335\",\"name\":\"Zhe Gan\"},{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"33c10383189c118465f8b40e8dba9213f57fa570\",\"title\":\"UNITER: Learning UNiversal Image-TExt Representations\",\"url\":\"https://www.semanticscholar.org/paper/33c10383189c118465f8b40e8dba9213f57fa570\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2002.03808\",\"authors\":[{\"authorId\":\"51278980\",\"name\":\"June-Woo Kim\"},{\"authorId\":\"1390683035\",\"name\":\"Ho-Young Jung\"},{\"authorId\":\"91899029\",\"name\":\"Minho Lee\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207653\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd306911a0ef639ba674c05a1656ec281552ed91\",\"title\":\"Vocoder-free End-to-End Voice Conversion with Transformer Network\",\"url\":\"https://www.semanticscholar.org/paper/bd306911a0ef639ba674c05a1656ec281552ed91\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1908.06066\",\"authors\":[{\"authorId\":\"150112700\",\"name\":\"Gen Li\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143795948\",\"name\":\"Yuejian Fang\"},{\"authorId\":\"71790825\",\"name\":\"Daxin Jiang\"},{\"authorId\":\"143849622\",\"name\":\"M. Zhou\"}],\"doi\":\"10.1609/AAAI.V34I07.6795\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"title\":\"Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/2bc1c8bd00bbf7401afcb5460277840fd8bab029\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5685594\",\"name\":\"G. Lorre\"},{\"authorId\":\"2962220\",\"name\":\"J. Rabarisoa\"},{\"authorId\":\"19258632\",\"name\":\"A. Orcesi\"},{\"authorId\":\"2910432\",\"name\":\"Samia Ainouz\"},{\"authorId\":\"10451773\",\"name\":\"St\\u00e9phane Canu\"}],\"doi\":\"10.1109/WACV45572.2020.9093278\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"90f84c25039d6c69bd25e70c719251aeacc50978\",\"title\":\"Temporal Contrastive Pretraining for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/90f84c25039d6c69bd25e70c719251aeacc50978\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2004.14338\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.709\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c7d2ed602ef41531912d5f27383a61ab4510c3d\",\"title\":\"Beyond Instructional Videos: Probing for More Diverse Visual-Textual Grounding on YouTube\",\"url\":\"https://www.semanticscholar.org/paper/7c7d2ed602ef41531912d5f27383a61ab4510c3d\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2004.00849\",\"authors\":[{\"authorId\":\"47272083\",\"name\":\"Zhicheng Huang\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"1453953482\",\"name\":\"Bei Liu\"},{\"authorId\":\"143890169\",\"name\":\"Dongmei Fu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c188874316557d501369e611a96cafc8058dffa\",\"title\":\"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers\",\"url\":\"https://www.semanticscholar.org/paper/5c188874316557d501369e611a96cafc8058dffa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2077989\",\"name\":\"Rahul C. Deo\"}],\"doi\":\"10.1161/CIRCULATIONAHA.120.050583\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0bff32c9b92b6cf76acc426a0be4eb0d930a300\",\"title\":\"Machine Learning in Medicine: Will This Time Be Different?\",\"url\":\"https://www.semanticscholar.org/paper/c0bff32c9b92b6cf76acc426a0be4eb0d930a300\",\"venue\":\"Circulation\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"94567368\",\"name\":\"C. Sun\"},{\"authorId\":\"9943923\",\"name\":\"Fabien Baradel\"},{\"authorId\":\"144816684\",\"name\":\"K. Murphy\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"025a0dc4a2a98742f1b410b6318a46de2c854b22\",\"title\":\"Learning Video Representations using Contrastive Bidirectional Transformer\",\"url\":\"https://www.semanticscholar.org/paper/025a0dc4a2a98742f1b410b6318a46de2c854b22\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2005.00200\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1664725279\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1520007550\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.161\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"title\":\"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491241424\",\"name\":\"Ao Liu\"},{\"authorId\":\"145403756\",\"name\":\"Shuai Yuan\"},{\"authorId\":\"1390466968\",\"name\":\"C. Zhang\"},{\"authorId\":\"1830449995\",\"name\":\"Congjian Luo\"},{\"authorId\":\"80944474\",\"name\":\"Yaqing Liao\"},{\"authorId\":\"50139529\",\"name\":\"Kun Bai\"},{\"authorId\":\"1683510\",\"name\":\"Zenglin Xu\"}],\"doi\":\"10.1145/3397271.3401247\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8dd60362b36fe59897990bdb481016a4eddd2d0f\",\"title\":\"Multi-Level Multimodal Transformer Network for Multimodal Recipe Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/8dd60362b36fe59897990bdb481016a4eddd2d0f\",\"venue\":\"SIGIR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46900590\",\"name\":\"H. Lee\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"122704930\",\"name\":\"Paul Natsev\"}],\"doi\":\"10.1109/CVPR42600.2020.00684\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3eed5cfc6311279a72e539037e36a375a186bff\",\"title\":\"Large Scale Video Representation Learning via Relational Graph Clustering\",\"url\":\"https://www.semanticscholar.org/paper/c3eed5cfc6311279a72e539037e36a375a186bff\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2004.13931\",\"authors\":[{\"authorId\":\"153504695\",\"name\":\"Hao Zhang\"},{\"authorId\":\"1735962\",\"name\":\"Aixin Sun\"},{\"authorId\":\"1492128584\",\"name\":\"Wei Jing\"},{\"authorId\":\"10638646\",\"name\":\"Joey Tianyi Zhou\"}],\"doi\":\"10.18653/v1/2020.acl-main.585\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5a975dcd3dba2a11830e5595d4c4659441cb6836\",\"title\":\"Span-based Localizing Network for Natural Language Video Localization\",\"url\":\"https://www.semanticscholar.org/paper/5a975dcd3dba2a11830e5595d4c4659441cb6836\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2012.12877\",\"authors\":[{\"authorId\":\"146941532\",\"name\":\"Hugo Touvron\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"},{\"authorId\":\"3271933\",\"name\":\"M. Douze\"},{\"authorId\":\"1403239967\",\"name\":\"F. Massa\"},{\"authorId\":\"3469062\",\"name\":\"Alexandre Sablayrolles\"},{\"authorId\":\"1681054\",\"name\":\"H. J\\u00e9gou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c13be599a8aef4f7ffb467e03fde65e855a366c\",\"title\":\"Training data-efficient image transformers&distillation through attention\",\"url\":\"https://www.semanticscholar.org/paper/8c13be599a8aef4f7ffb467e03fde65e855a366c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2002.00163\",\"authors\":[{\"authorId\":\"15401738\",\"name\":\"Zekang Li\"},{\"authorId\":\"115419547\",\"name\":\"Zongjia Li\"},{\"authorId\":\"27672597\",\"name\":\"Jinchao Zhang\"},{\"authorId\":\"49771779\",\"name\":\"Yang Feng\"},{\"authorId\":\"150954670\",\"name\":\"Cheng Niu\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f4e0723e048941ea73c77a7c69dbb731ef8de750\",\"title\":\"Bridging Text and Video: A Universal Multimodal Transformer for Video-Audio Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/f4e0723e048941ea73c77a7c69dbb731ef8de750\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1443835648\",\"name\":\"Michael Snower\"},{\"authorId\":null,\"name\":\"Youssef Mejjatti\"},{\"authorId\":\"8278433\",\"name\":\"Aaron Gokaslan\"},{\"authorId\":\"101568984\",\"name\":\"Zejiang Shen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2b4852b995046dae2b9c855904a53df123b4c388\",\"title\":\"Improving Unpaired Object Translation for Unaligned Domains\",\"url\":\"https://www.semanticscholar.org/paper/2b4852b995046dae2b9c855904a53df123b4c388\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.06351\",\"authors\":[{\"authorId\":\"17866105\",\"name\":\"Fuli Luo\"},{\"authorId\":\"46709826\",\"name\":\"Pengcheng Yang\"},{\"authorId\":\"50341802\",\"name\":\"S. Li\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d9b7620f9b9653ada1a7ce36b0d6617f5979fff2\",\"title\":\"CAPT: Contrastive Pre-Training for Learning Denoised Sequence Representations\",\"url\":\"https://www.semanticscholar.org/paper/d9b7620f9b9653ada1a7ce36b0d6617f5979fff2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.00781\",\"authors\":[{\"authorId\":\"2030527072\",\"name\":\"Anirudh Tunga\"},{\"authorId\":\"71070791\",\"name\":\"Sai Vidyaranya Nuthalapati\"},{\"authorId\":\"1768610\",\"name\":\"J. Wachs\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e0d7289231dc4b1cd822186690b426810da620b\",\"title\":\"Pose-based Sign Language Recognition using GCN and BERT\",\"url\":\"https://www.semanticscholar.org/paper/2e0d7289231dc4b1cd822186690b426810da620b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.15086\",\"authors\":[{\"authorId\":\"3322871\",\"name\":\"Zhuosheng Zhang\"},{\"authorId\":\"151486082\",\"name\":\"Haojie Yu\"},{\"authorId\":null,\"name\":\"Hai Zhao\"},{\"authorId\":\"102696161\",\"name\":\"Rui Wang\"},{\"authorId\":null,\"name\":\"Masao Utiyama\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5b8d4b52f736976221652b572770042fc31a830\",\"title\":\"Accurate Word Representations with Universal Visual Guidance\",\"url\":\"https://www.semanticscholar.org/paper/c5b8d4b52f736976221652b572770042fc31a830\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.08271\",\"authors\":[{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"153345698\",\"name\":\"Tianxiang Sun\"},{\"authorId\":\"26339093\",\"name\":\"Yige Xu\"},{\"authorId\":\"95329799\",\"name\":\"Yunfan Shao\"},{\"authorId\":\"145493218\",\"name\":\"Ning Dai\"},{\"authorId\":\"152638818\",\"name\":\"Xuanjing Huang\"}],\"doi\":\"10.1007/s11431-020-1647-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e06b36f1076b3497992b558d7707f053161dc840\",\"title\":\"Pre-trained Models for Natural Language Processing: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/e06b36f1076b3497992b558d7707f053161dc840\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.02971\",\"authors\":[{\"authorId\":\"3322871\",\"name\":\"Zhuosheng Zhang\"},{\"authorId\":\"108085542\",\"name\":\"Rui Wang\"},{\"authorId\":\"2849740\",\"name\":\"Kehai Chen\"},{\"authorId\":\"1802277\",\"name\":\"M. Utiyama\"},{\"authorId\":\"1698363\",\"name\":\"Eiichiro Sumita\"},{\"authorId\":\"97628269\",\"name\":\"Hai Zhao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5bca8ca79077db1b49bcb4869c98e77ad40878bd\",\"title\":\"Probing Contextualized Sentence Representations with Visual Awareness\",\"url\":\"https://www.semanticscholar.org/paper/5bca8ca79077db1b49bcb4869c98e77ad40878bd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.05743\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"9943923\",\"name\":\"Fabien Baradel\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c\",\"title\":\"Contrastive Bidirectional Transformer for Temporal Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/f259bc7ef31c4ec7dd041c94bfd6b2f93b99b47c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2012.06567\",\"authors\":[{\"authorId\":\"1805946041\",\"name\":\"Yi Zhu\"},{\"authorId\":\"21657733\",\"name\":\"X. Li\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"22539483\",\"name\":\"Chongruo Wu\"},{\"authorId\":\"152781163\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2397422\",\"name\":\"Joseph Tighe\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"49140510\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"title\":\"A Comprehensive Study of Deep Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.08673\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c8f6878dac0c5c99c192ded715d2e864eed7302\",\"title\":\"A Closer Look at the Robustness of Vision-and-Language Pre-trained Models\",\"url\":\"https://www.semanticscholar.org/paper/3c8f6878dac0c5c99c192ded715d2e864eed7302\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.00597\",\"authors\":[{\"authorId\":\"2007582232\",\"name\":\"Simon Ging\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"1835025\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"80089ad641bae28b0e57771afef181b60011069e\",\"title\":\"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/80089ad641bae28b0e57771afef181b60011069e\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2012.01002\",\"authors\":[{\"authorId\":\"2030527923\",\"name\":\"Xiayu Zhong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0646fda8bd8003df23dcd53caec403c9fd840a7\",\"title\":\"Classification of Multimodal Hate Speech - The Winning Solution of Hateful Memes Challenge\",\"url\":\"https://www.semanticscholar.org/paper/f0646fda8bd8003df23dcd53caec403c9fd840a7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.13198\",\"authors\":[{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"11882893\",\"name\":\"A. Yang\"},{\"authorId\":\"29343468\",\"name\":\"Yichang Zhang\"},{\"authorId\":\"1618186344\",\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9779ddeb6a8a9de0f7e104d8742728aa14578d6\",\"title\":\"InterBERT: An Effective Multi-Modal Pretraining Approach via Vision-and-Language Interaction\",\"url\":\"https://www.semanticscholar.org/paper/b9779ddeb6a8a9de0f7e104d8742728aa14578d6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.08508\",\"authors\":[{\"authorId\":\"1399191196\",\"name\":\"David Ding\"},{\"authorId\":\"145783676\",\"name\":\"Felix Hill\"},{\"authorId\":\"35030998\",\"name\":\"A. Santoro\"},{\"authorId\":\"46378362\",\"name\":\"M. Botvinick\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"02791e807dc9a91f854a1f3d5f6005122a546109\",\"title\":\"Object-based attention for spatio-temporal reasoning: Outperforming neuro-symbolic models with flexible distributed architectures\",\"url\":\"https://www.semanticscholar.org/paper/02791e807dc9a91f854a1f3d5f6005122a546109\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.08200\",\"authors\":[{\"authorId\":\"2813326\",\"name\":\"Wanyun Cui\"},{\"authorId\":\"3423969\",\"name\":\"G. Zheng\"},{\"authorId\":\"115438571\",\"name\":\"W. Wang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.444\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dcc03cce582910458ea6ed58745952b020cf3893\",\"title\":\"Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning\",\"url\":\"https://www.semanticscholar.org/paper/dcc03cce582910458ea6ed58745952b020cf3893\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.10390\",\"authors\":[{\"authorId\":null,\"name\":\"Rufin VanRullen\"},{\"authorId\":null,\"name\":\"Ryota Kanai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69135a269b8d2536bc4f3caa03b465ceb9356192\",\"title\":\"Deep Learning and the Global Workspace Theory\",\"url\":\"https://www.semanticscholar.org/paper/69135a269b8d2536bc4f3caa03b465ceb9356192\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.03977\",\"authors\":[{\"authorId\":\"145282222\",\"name\":\"C. Zhang\"},{\"authorId\":\"8387085\",\"name\":\"Zichao Yang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"144718783\",\"name\":\"Li Deng\"}],\"doi\":\"10.1109/JSTSP.2020.2987728\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"415efb7b4d9d1e5b64dbaf3fe4229ad462acce71\",\"title\":\"Multimodal Intelligence: Representation Learning, Information Fusion, and Applications\",\"url\":\"https://www.semanticscholar.org/paper/415efb7b4d9d1e5b64dbaf3fe4229ad462acce71\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144629424\",\"name\":\"A. Kolesnikov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a32335a738576d9e71f58a70e677761e3c9f2eaf\",\"title\":\"Transformer Encoder MLP Head Vision Transformer ( ViT ) * Linear Projection of Flattened Patches * Extra learnable\",\"url\":\"https://www.semanticscholar.org/paper/a32335a738576d9e71f58a70e677761e3c9f2eaf\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.03019\",\"authors\":[{\"authorId\":\"49019589\",\"name\":\"Zhuoran Shen\"},{\"authorId\":\"4689792\",\"name\":\"I. Bello\"},{\"authorId\":\"39963722\",\"name\":\"Raviteja Vemulapalli\"},{\"authorId\":\"34760532\",\"name\":\"Xuhui Jia\"},{\"authorId\":\"2534188\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b\",\"title\":\"Global Self-Attention Networks for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/64b9be00f4eecd465b4e8e46e2ab7624d7eaeb2b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.05691\",\"authors\":[{\"authorId\":\"122460701\",\"name\":\"Tianhao Li\"},{\"authorId\":\"119770705\",\"name\":\"L. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2783c13471060300e62a1eed458d9d2888b5b5be\",\"title\":\"Learning Spatiotemporal Features via Video and Text Pair Discrimination\",\"url\":\"https://www.semanticscholar.org/paper/2783c13471060300e62a1eed458d9d2888b5b5be\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.02783\",\"authors\":[{\"authorId\":\"143902495\",\"name\":\"Michael Tschannen\"},{\"authorId\":\"2941141\",\"name\":\"Josip Djolonga\"},{\"authorId\":\"39687627\",\"name\":\"M. Ritter\"},{\"authorId\":\"32694028\",\"name\":\"Aravindh Mahendran\"},{\"authorId\":\"2815290\",\"name\":\"N. Houlsby\"},{\"authorId\":\"1802148\",\"name\":\"S. Gelly\"},{\"authorId\":\"34302129\",\"name\":\"M. Lucic\"}],\"doi\":\"10.1109/cvpr42600.2020.01382\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"797d52dd7819b15a5d171fd5bedad9cc911cf756\",\"title\":\"Self-Supervised Learning of Video-Induced Visual Invariances\",\"url\":\"https://www.semanticscholar.org/paper/797d52dd7819b15a5d171fd5bedad9cc911cf756\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.09392\",\"authors\":[{\"authorId\":\"35299091\",\"name\":\"Yansong Tang\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":null,\"name\":\"Jie Zhou\"}],\"doi\":\"10.1109/TPAMI.2020.2980824\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c974b4547ce5df86f07c428abb6206ad5e52fc0d\",\"title\":\"Comprehensive Instructional Video Analysis: The COIN Dataset and Performance Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/c974b4547ce5df86f07c428abb6206ad5e52fc0d\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1907.01172\",\"authors\":[{\"authorId\":\"4251916\",\"name\":\"C. Chang\"},{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"2068265\",\"name\":\"Danfei Xu\"},{\"authorId\":\"3419364\",\"name\":\"E. Adeli\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1007/978-3-030-58621-8_20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71992656ff50c56adcbe6e99fca7eecac446d70a\",\"title\":\"Procedure Planning in Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/71992656ff50c56adcbe6e99fca7eecac446d70a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119589314\",\"name\":\"A. Rives\"},{\"authorId\":\"3322061\",\"name\":\"S. Goyal\"},{\"authorId\":\"48116599\",\"name\":\"J. Meier\"},{\"authorId\":\"35578711\",\"name\":\"Demi Guo\"},{\"authorId\":\"40511414\",\"name\":\"Myle Ott\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"2845100\",\"name\":\"Jerry Ma\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":\"10.1101/622803\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18a93dc1558bf9d7534d0b416633cebaf75c1145\",\"title\":\"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences\",\"url\":\"https://www.semanticscholar.org/paper/18a93dc1558bf9d7534d0b416633cebaf75c1145\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7885575\",\"name\":\"E. Amrani\"},{\"authorId\":\"1397958297\",\"name\":\"R. Ben-Ari\"},{\"authorId\":\"14871837\",\"name\":\"I. Shapira\"},{\"authorId\":\"120159179\",\"name\":\"Tal Hakim\"},{\"authorId\":\"49791471\",\"name\":\"A. Bronstein\"}],\"doi\":\"10.1109/CVPRW50498.2020.00485\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da031a350f80eb23b8cdbce32ec3b0b792cd7557\",\"title\":\"Self-Supervised Object Detection and Retrieval Using Unlabeled Videos\",\"url\":\"https://www.semanticscholar.org/paper/da031a350f80eb23b8cdbce32ec3b0b792cd7557\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":\"2005.09175\",\"authors\":[{\"authorId\":\"153204991\",\"name\":\"Feng Qi\"},{\"authorId\":\"1942580\",\"name\":\"Guanjun Jiang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0b9f5bf7b183c39558713524c0213e1f59e919b2\",\"title\":\"Human-like general language processing\",\"url\":\"https://www.semanticscholar.org/paper/0b9f5bf7b183c39558713524c0213e1f59e919b2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2387987\",\"name\":\"Jieh-Sheng Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a02da304a0143b7f014e1cb59bc15090e9de622a\",\"title\":\"PatentTransformer: A Framework for Personalized Patent Claim Generation\",\"url\":\"https://www.semanticscholar.org/paper/a02da304a0143b7f014e1cb59bc15090e9de622a\",\"venue\":\"JURIX\",\"year\":2019},{\"arxivId\":\"2007.10703\",\"authors\":[{\"authorId\":\"31638576\",\"name\":\"A. Arnab\"},{\"authorId\":\"1491624845\",\"name\":\"Chen Sun\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1007/978-3-030-58607-2_44\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d9660f127a880ec9757aedc90509524d744c15a\",\"title\":\"Uncertainty-Aware Weakly Supervised Action Detection from Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/7d9660f127a880ec9757aedc90509524d744c15a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.01843\",\"authors\":[{\"authorId\":\"8350409\",\"name\":\"I. Padhi\"},{\"authorId\":\"1999174380\",\"name\":\"Yair Schiff\"},{\"authorId\":\"2576373\",\"name\":\"I. Melnyk\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"},{\"authorId\":\"2211263\",\"name\":\"Youssef Mroueh\"},{\"authorId\":\"1839363\",\"name\":\"Pierre L. Dognin\"},{\"authorId\":\"39320489\",\"name\":\"J. Ross\"},{\"authorId\":\"145093191\",\"name\":\"Ravi Nair\"},{\"authorId\":\"40021617\",\"name\":\"E. Altman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b055049c568be70d6a762679cdb93f630d5d6e6\",\"title\":\"Tabular Transformers for Modeling Multivariate Time Series\",\"url\":\"https://www.semanticscholar.org/paper/1b055049c568be70d6a762679cdb93f630d5d6e6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.09199\",\"authors\":[{\"authorId\":\"41020711\",\"name\":\"Andrew Rouditchenko\"},{\"authorId\":\"1394839535\",\"name\":\"Angie Boggust\"},{\"authorId\":\"30507748\",\"name\":\"David Harwath\"},{\"authorId\":\"47669634\",\"name\":\"Dhiraj Joshi\"},{\"authorId\":\"70913918\",\"name\":\"S. Thomas\"},{\"authorId\":\"3104038\",\"name\":\"Kartik Audhkhasi\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"144707379\",\"name\":\"Brian Kingsbury\"},{\"authorId\":\"1774515\",\"name\":\"M. Picheny\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"},{\"authorId\":\"152450847\",\"name\":\"J. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1141ac4a1443ae7b44266a84a7f042f38759ac47\",\"title\":\"AVLnet: Learning Audio-Visual Language Representations from Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/1141ac4a1443ae7b44266a84a7f042f38759ac47\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.11237\",\"authors\":[{\"authorId\":\"35399640\",\"name\":\"D\\u00eddac Sur\\u00eds\"},{\"authorId\":\"32486555\",\"name\":\"D. Epstein\"},{\"authorId\":\"153172090\",\"name\":\"Huai-zhong Ji\"},{\"authorId\":\"70351911\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"45e5ce07582b9f500ec7de8e28286eccc639a0a7\",\"title\":\"Learning to Learn Words from Narrated Video\",\"url\":\"https://www.semanticscholar.org/paper/45e5ce07582b9f500ec7de8e28286eccc639a0a7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.03538\",\"authors\":[{\"authorId\":\"31937047\",\"name\":\"Sara Beery\"},{\"authorId\":\"3490569\",\"name\":\"Guanhang Wu\"},{\"authorId\":\"40303375\",\"name\":\"V. Rathod\"},{\"authorId\":\"69423660\",\"name\":\"Ronny Votel\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"}],\"doi\":\"10.1109/cvpr42600.2020.01309\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9ac36b31c7c4ea9dc8b1962ea80ed5f117430cee\",\"title\":\"Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/9ac36b31c7c4ea9dc8b1962ea80ed5f117430cee\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.16228\",\"authors\":[{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"39257069\",\"name\":\"A. Recasens\"},{\"authorId\":\"145721402\",\"name\":\"Ros\\u00e1lia G. Schneider\"},{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"16092809\",\"name\":\"Jason Ramapuram\"},{\"authorId\":\"3364908\",\"name\":\"J. Fauw\"},{\"authorId\":\"1466466597\",\"name\":\"Lucas Smaira\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4174f03c7d8d9add62ae4ecd0ec90efba680b7ae\",\"title\":\"Self-Supervised MultiModal Versatile Networks\",\"url\":\"https://www.semanticscholar.org/paper/4174f03c7d8d9add62ae4ecd0ec90efba680b7ae\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1909.11740\",\"authors\":[{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1877430\",\"name\":\"A. E. Kholy\"},{\"authorId\":\"83147159\",\"name\":\"Faisal Ahmed\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":\"10.1007/978-3-030-58577-8_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8a305b9366608d54452ac30459ee57b4f5cf1c9\",\"title\":\"UNITER: UNiversal Image-TExt Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/d8a305b9366608d54452ac30459ee57b4f5cf1c9\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2005.09801\",\"authors\":[{\"authorId\":\"7642720\",\"name\":\"Dehong Gao\"},{\"authorId\":\"81235644\",\"name\":\"Linbo Jin\"},{\"authorId\":\"50462177\",\"name\":\"B. Chen\"},{\"authorId\":\"2642333\",\"name\":\"Minghui Qiu\"},{\"authorId\":\"1491201634\",\"name\":\"Yi Wei\"},{\"authorId\":\"46972251\",\"name\":\"Y. Hu\"},{\"authorId\":\"3705643\",\"name\":\"H. Wang\"}],\"doi\":\"10.1145/3397271.3401430\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"efe4f7ca1a9e533a3a96cca0bca556f8e153e9b3\",\"title\":\"FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/efe4f7ca1a9e533a3a96cca0bca556f8e153e9b3\",\"venue\":\"SIGIR\",\"year\":2020},{\"arxivId\":\"2004.03708\",\"authors\":[{\"authorId\":\"80389349\",\"name\":\"Zhuowan Li\"},{\"authorId\":\"2536742\",\"name\":\"Quan Hung Tran\"},{\"authorId\":\"2712573\",\"name\":\"Long Mai\"},{\"authorId\":\"145527698\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/CVPR42600.2020.00350\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"title\":\"Context-Aware Group Captioning via Self-Attention and Contrastive Features\",\"url\":\"https://www.semanticscholar.org/paper/477b70ed4753745e2700dd2791c3a5fb966f8b64\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29545186\",\"name\":\"M. Hosseinzadeh\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":\"10.1109/cvpr42600.2020.00365\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef6db51cb736116266025eb1eab2fb4f36b75310\",\"title\":\"Composed Query Image Retrieval Using Locally Bounded Features\",\"url\":\"https://www.semanticscholar.org/paper/ef6db51cb736116266025eb1eab2fb4f36b75310\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1912.03502\",\"authors\":[{\"authorId\":\"150151163\",\"name\":\"Jieh-Sheng Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b18e50747e88953e86046a7bec5afc17bf2649a0\",\"title\":\"Personalized Patent Claim Generation and Measurement\",\"url\":\"https://www.semanticscholar.org/paper/b18e50747e88953e86046a7bec5afc17bf2649a0\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150282005\",\"name\":\"Rizard Renanda Adhi Pramono\"},{\"authorId\":\"30477181\",\"name\":\"Yie-Tarng Chen\"},{\"authorId\":\"122315920\",\"name\":\"Wen-Hsien Fang\"}],\"doi\":\"10.1007/978-3-030-58452-8_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"33f62e6f851da560037f1ed008d2eb51bb80f062\",\"title\":\"Empowering Relational Network by Self-attention Augmented Conditional Random Fields for Group Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/33f62e6f851da560037f1ed008d2eb51bb80f062\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1908.02265\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"title\":\"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/65a9c7b0800c86a196bc14e7621ff895cc6ab287\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145876154\",\"name\":\"Bin Huang\"},{\"authorId\":\"1993657480\",\"name\":\"Siao Tang\"},{\"authorId\":\"1993657377\",\"name\":\"Guangyao Shen\"},{\"authorId\":\"1993611266\",\"name\":\"Guohao Li\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3422852.3423484\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f642ddb69f712e2b517cb6c6d9a062506991d1ed\",\"title\":\"Commonsense Learning: An Indispensable Path towards Human-centric Multimedia\",\"url\":\"https://www.semanticscholar.org/paper/f642ddb69f712e2b517cb6c6d9a062506991d1ed\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.01473\",\"authors\":[{\"authorId\":\"10007273\",\"name\":\"Qiaolin Xia\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"144934143\",\"name\":\"Dongdong Zhang\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"49575302\",\"name\":\"Zhifang Sui\"},{\"authorId\":\"144530394\",\"name\":\"Edward Cui\"},{\"authorId\":\"1490606819\",\"name\":\"Taroon Bharti\"},{\"authorId\":null,\"name\":\"Xin Liu\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"title\":\"XGPT: Cross-modal Generative Pre-Training for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1d0d9550ecd2bece6a34fe1ffd12fb7504e7aaa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.08155\",\"authors\":[{\"authorId\":\"51056532\",\"name\":\"Zhangyin Feng\"},{\"authorId\":\"51223794\",\"name\":\"Daya Guo\"},{\"authorId\":\"39483833\",\"name\":\"Duyu Tang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"2674998\",\"name\":\"X. Feng\"},{\"authorId\":\"50175330\",\"name\":\"Ming Gong\"},{\"authorId\":\"24962156\",\"name\":\"Linjun Shou\"},{\"authorId\":\"152277111\",\"name\":\"B. Qin\"},{\"authorId\":\"152244126\",\"name\":\"T. Liu\"},{\"authorId\":\"71790825\",\"name\":\"Daxin Jiang\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.139\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93697712f2929727d18cfee30a5e35b055062a26\",\"title\":\"CodeBERT: A Pre-Trained Model for Programming and Natural Languages\",\"url\":\"https://www.semanticscholar.org/paper/93697712f2929727d18cfee30a5e35b055062a26\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2004.09936\",\"authors\":[{\"authorId\":\"1645254617\",\"name\":\"Tanja Bunk\"},{\"authorId\":\"15629035\",\"name\":\"Daksh Varshneya\"},{\"authorId\":\"36992437\",\"name\":\"Vladimir Vlasov\"},{\"authorId\":\"143799158\",\"name\":\"A. Nichol\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c51b99f18f0ca9e1e2fea7132bf5654299817d6c\",\"title\":\"DIET: Lightweight Language Understanding for Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/c51b99f18f0ca9e1e2fea7132bf5654299817d6c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1007/978-3-030-58526-6_28\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bcb053622ef73ccebbfce3d7a8663b15e0c33a8\",\"title\":\"Learning Actionness via Long-Range Temporal Order Verification\",\"url\":\"https://www.semanticscholar.org/paper/1bcb053622ef73ccebbfce3d7a8663b15e0c33a8\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390626344\",\"name\":\"Zhiyuan Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"title\":\"Cross-Modal Representation\",\"url\":\"https://www.semanticscholar.org/paper/7cd0c93b77bd2cf085c02c7a98118b6b43592110\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.11278\",\"authors\":[{\"authorId\":\"2706729\",\"name\":\"Jaemin Cho\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"34846449\",\"name\":\"D. Schwenk\"},{\"authorId\":\"2548384\",\"name\":\"Hannaneh Hajishirzi\"},{\"authorId\":\"2684226\",\"name\":\"Aniruddha Kembhavi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.707\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5fb7a72807af36a7d39049346b3feb422a50c3c\",\"title\":\"X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers\",\"url\":\"https://www.semanticscholar.org/paper/e5fb7a72807af36a7d39049346b3feb422a50c3c\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2006.06195\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"50580345\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"1431754650\",\"name\":\"C. Zhu\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8dc49a2041a8a269fbf64911a4f2c8cef6738a5c\",\"title\":\"Large-Scale Adversarial Training for Vision-and-Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/8dc49a2041a8a269fbf64911a4f2c8cef6738a5c\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50580169\",\"name\":\"Y. Chen\"},{\"authorId\":\"144784813\",\"name\":\"S. Gong\"},{\"authorId\":\"1809420\",\"name\":\"Loris Bazzani\"}],\"doi\":\"10.1109/CVPR42600.2020.00307\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78f69364531794550130389342b7bc0ff785b7e9\",\"title\":\"Image Search With Text Feedback by Visiolinguistic Attention Learning\",\"url\":\"https://www.semanticscholar.org/paper/78f69364531794550130389342b7bc0ff785b7e9\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2005.12833\",\"authors\":[{\"authorId\":\"51226934\",\"name\":\"Laila Rasmy\"},{\"authorId\":\"1500391802\",\"name\":\"Yang Xiang\"},{\"authorId\":\"47661144\",\"name\":\"Ziqian Xie\"},{\"authorId\":\"152206015\",\"name\":\"C. Tao\"},{\"authorId\":\"50789152\",\"name\":\"D. Zhi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d4de0fa45aeddc31142e6a24666d06ed7923f1e\",\"title\":\"Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction\",\"url\":\"https://www.semanticscholar.org/paper/5d4de0fa45aeddc31142e6a24666d06ed7923f1e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.10639\",\"authors\":[{\"authorId\":\"151352107\",\"name\":\"Valentin Gabeur\"},{\"authorId\":\"1491624845\",\"name\":\"Chen Sun\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1007/978-3-030-58548-8_13\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6871f6c5437a747fae75a19962f418d234ce2dc1\",\"title\":\"Multi-modal Transformer for Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6871f6c5437a747fae75a19962f418d234ce2dc1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"146144922\",\"name\":\"Akari Ninomiya\"},{\"authorId\":\"2875233\",\"name\":\"T. Ozaki\"}],\"doi\":\"10.1145/3379175.3391710\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d205d70043e129f56b94427aa39a0c50359ba630\",\"title\":\"Cooking Recipe Analysis based on Sequences of Distributed Representation on Procedure Texts and Associated Images\",\"url\":\"https://www.semanticscholar.org/paper/d205d70043e129f56b94427aa39a0c50359ba630\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.07203\",\"authors\":[{\"authorId\":\"3443095\",\"name\":\"Bruno Korbar\"},{\"authorId\":\"40052301\",\"name\":\"F. Petroni\"},{\"authorId\":\"3102850\",\"name\":\"Rohit Girdhar\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a2bf96de7889360dce11b9fc80f578a56e63dcfc\",\"title\":\"Video Understanding as Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/a2bf96de7889360dce11b9fc80f578a56e63dcfc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.05787\",\"authors\":[{\"authorId\":\"153515321\",\"name\":\"Wasifur Rahman\"},{\"authorId\":\"2811524\",\"name\":\"M. Hasan\"},{\"authorId\":\"1459934320\",\"name\":\"Sangwu Lee\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"},{\"authorId\":\"1429197894\",\"name\":\"Chengfeng Mao\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"},{\"authorId\":\"1491348598\",\"name\":\"Ehsan Hoque\"}],\"doi\":\"10.18653/v1/2020.acl-main.214\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ecaddd3df8793c1b7050cb4c6b359c041bedc57\",\"title\":\"Integrating Multimodal Information in Large Pretrained Transformers\",\"url\":\"https://www.semanticscholar.org/paper/0ecaddd3df8793c1b7050cb4c6b359c041bedc57\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2006.15319\",\"authors\":[{\"authorId\":\"143725625\",\"name\":\"Hung Le\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/2020.acl-main.518\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"af73d7815f13794223384004096ff4fc62c3d4a9\",\"title\":\"Video-Grounded Dialogues with Pretrained Generation Language Models\",\"url\":\"https://www.semanticscholar.org/paper/af73d7815f13794223384004096ff4fc62c3d4a9\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2002.10832\",\"authors\":[{\"authorId\":\"90745780\",\"name\":\"Thomas Scialom\"},{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"1502219404\",\"name\":\"Paul-Alexis Dray\"},{\"authorId\":\"1767493\",\"name\":\"Jacopo Staiano\"},{\"authorId\":\"67192547\",\"name\":\"P. Gallinari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b32d02d7f6fa954e7ae500f94f6a5efd113751a4\",\"title\":\"BERT Can See Out of the Box: On the Cross-modal Transferability of Text Representations\",\"url\":\"https://www.semanticscholar.org/paper/b32d02d7f6fa954e7ae500f94f6a5efd113751a4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.09805\",\"authors\":[{\"authorId\":\"1491401265\",\"name\":\"Shuang Ma\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"1381192475\",\"name\":\"Daniel McDuff\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7df5dd31f2ce600a4be589dff4d6a758be262324\",\"title\":\"Learning Audio-Visual Representations with Active Contrastive Coding\",\"url\":\"https://www.semanticscholar.org/paper/7df5dd31f2ce600a4be589dff4d6a758be262324\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.01392\",\"authors\":[{\"authorId\":\"1413822807\",\"name\":\"Mert Bulent Sariyildiz\"},{\"authorId\":\"144781195\",\"name\":\"J. Perez\"},{\"authorId\":\"2295553\",\"name\":\"Diane Larlus\"}],\"doi\":\"10.1007/978-3-030-58598-3_10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0181353f32b1ad3ac6bc59838c69b0e5c64137a\",\"title\":\"Learning Visual Representations with Caption Annotations\",\"url\":\"https://www.semanticscholar.org/paper/b0181353f32b1ad3ac6bc59838c69b0e5c64137a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2006.15489\",\"authors\":[{\"authorId\":\"49984891\",\"name\":\"Ceyuan Yang\"},{\"authorId\":\"121983635\",\"name\":\"Yinghao Xu\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3cb65b26e407e7464223e9568010965fe73ae61f\",\"title\":\"Video Representation Learning with Visual Tempo Consistency\",\"url\":\"https://www.semanticscholar.org/paper/3cb65b26e407e7464223e9568010965fe73ae61f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.07093\",\"authors\":[{\"authorId\":\"11542215\",\"name\":\"Kazuki Miyazawa\"},{\"authorId\":\"46949537\",\"name\":\"T. Aoki\"},{\"authorId\":\"3130434\",\"name\":\"Takato Horii\"},{\"authorId\":\"1491117340\",\"name\":\"Takayuki Nagai\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bc3ad9b70feca27c3bf7374628d52c52a491657\",\"title\":\"lamBERT: Language and Action Learning Using Multimodal BERT\",\"url\":\"https://www.semanticscholar.org/paper/6bc3ad9b70feca27c3bf7374628d52c52a491657\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1994374184\",\"name\":\"Anthony Gillioz\"},{\"authorId\":\"144558519\",\"name\":\"Jacky Casas\"},{\"authorId\":\"1802011\",\"name\":\"E. Mugellini\"},{\"authorId\":\"1799647\",\"name\":\"Omar Abou Khaled\"}],\"doi\":\"10.15439/2020F20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b49bcd9fd53158a57d158550461bcb72ac83fbc\",\"title\":\"Overview of the Transformer-based Models for NLP Tasks\",\"url\":\"https://www.semanticscholar.org/paper/7b49bcd9fd53158a57d158550461bcb72ac83fbc\",\"venue\":\"2020 15th Conference on Computer Science and Information Systems (FedCSIS)\",\"year\":2020},{\"arxivId\":\"2004.12238\",\"authors\":[{\"authorId\":\"1424748326\",\"name\":\"A. Kumar\"},{\"authorId\":\"9214782\",\"name\":\"Trisha Mittal\"},{\"authorId\":\"1699159\",\"name\":\"D. Manocha\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43d992c17b53e75e46a29c9866ac1c0966bd5bf4\",\"title\":\"MCQA: Multimodal Co-attention Based Network for Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/43d992c17b53e75e46a29c9866ac1c0966bd5bf4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.05402\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"145602574\",\"name\":\"L. Wang\"},{\"authorId\":\"1752875\",\"name\":\"Y. Shen\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.acl-main.233\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"70557ea6b65846fc30729ceed224acd4ac64ca5d\",\"title\":\"MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/70557ea6b65846fc30729ceed224acd4ac64ca5d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2010.11459\",\"authors\":[{\"authorId\":\"1471349938\",\"name\":\"Prateek Verma\"},{\"authorId\":\"152251359\",\"name\":\"J. Smith\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2205c75be12090b1d46023e6622b9a4458c1c62\",\"title\":\"A Framework for Contrastive and Generative Learning of Audio Representations\",\"url\":\"https://www.semanticscholar.org/paper/d2205c75be12090b1d46023e6622b9a4458c1c62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"92326466\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/cvpr42600.2020.01059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"title\":\"Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c5882ba83265093f2625fcebaf41bcdae4548a1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2011.13922\",\"authors\":[{\"authorId\":\"1612421029\",\"name\":\"Yicong Hong\"},{\"authorId\":\"1509240145\",\"name\":\"Qi Wu\"},{\"authorId\":\"35653798\",\"name\":\"Yuankai Qi\"},{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"47873182\",\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d16bce335338372c1927f69d4b1f667a330b59d2\",\"title\":\"A Recurrent Vision-and-Language BERT for Navigation\",\"url\":\"https://www.semanticscholar.org/paper/d16bce335338372c1927f69d4b1f667a330b59d2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.06597\",\"authors\":[{\"authorId\":\"47773127\",\"name\":\"S. Yuan\"},{\"authorId\":\"1423652601\",\"name\":\"Ke Bai\"},{\"authorId\":\"50811121\",\"name\":\"Liqun Chen\"},{\"authorId\":\"48378494\",\"name\":\"Yizhe Zhang\"},{\"authorId\":\"46387857\",\"name\":\"Chenyang Tao\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"1807489212\",\"name\":\"Guoyin Wang\"},{\"authorId\":\"51030446\",\"name\":\"R. Henao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1af06d2c4a129f9335159db8bb1455414705bed1\",\"title\":\"Weakly supervised cross-domain alignment with optimal transport\",\"url\":\"https://www.semanticscholar.org/paper/1af06d2c4a129f9335159db8bb1455414705bed1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.04298\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"47792365\",\"name\":\"Y. Asano\"},{\"authorId\":\"145891577\",\"name\":\"Ruth Fong\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ddbaa34c574124a91fa3bc217e232e17668e84c\",\"title\":\"Multi-modal Self-Supervision from Generalized Data Transformations\",\"url\":\"https://www.semanticscholar.org/paper/8ddbaa34c574124a91fa3bc217e232e17668e84c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.08100\",\"authors\":[{\"authorId\":\"28130078\",\"name\":\"P. P. Liang\"},{\"authorId\":\"47841931\",\"name\":\"Irene Z Li\"},{\"authorId\":\"5866249\",\"name\":\"E. Zheng\"},{\"authorId\":\"144529448\",\"name\":\"Yao Chong Lim\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.18653/v1/2020.acl-main.488\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d965ed237a3b4592ecefdb618c29f63adedff76\",\"title\":\"Towards Debiasing Sentence Representations\",\"url\":\"https://www.semanticscholar.org/paper/0d965ed237a3b4592ecefdb618c29f63adedff76\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"1791344388\",\"name\":\"Lei Ji\"},{\"authorId\":\"1783553\",\"name\":\"Zhen-dong Niu\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/3394171.3413498\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"title\":\"Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning\",\"url\":\"https://www.semanticscholar.org/paper/de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11165439\",\"name\":\"Y. Yao\"},{\"authorId\":\"1396239754\",\"name\":\"Ver\\u00f3nica P\\u00e9rez-Rosas\"},{\"authorId\":\"1898814\",\"name\":\"Mohamed Abouelenien\"},{\"authorId\":\"1803815\",\"name\":\"Mihai Burzo\"}],\"doi\":\"10.1145/3382507.3418821\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fab0028cab9949f7924241b922998811218882a0\",\"title\":\"MORSE: MultimOdal sentiment analysis for Real-life SEttings\",\"url\":\"https://www.semanticscholar.org/paper/fab0028cab9949f7924241b922998811218882a0\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":\"2004.13278\",\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1785083\",\"name\":\"Michael R. Lyu\"},{\"authorId\":\"145310663\",\"name\":\"Irwin King\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.269\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"06c7269c10125589d2599f684b751b1640f7a0cc\",\"title\":\"VD-BERT: A Unified Vision and Dialog Transformer with BERT\",\"url\":\"https://www.semanticscholar.org/paper/06c7269c10125589d2599f684b751b1640f7a0cc\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2012.12556\",\"authors\":[{\"authorId\":\"3826388\",\"name\":\"Kai Han\"},{\"authorId\":null,\"name\":\"Yunhe Wang\"},{\"authorId\":\"14499178\",\"name\":\"H. Chen\"},{\"authorId\":\"1736061\",\"name\":\"Xinghao Chen\"},{\"authorId\":\"8572788\",\"name\":\"Jianyuan Guo\"},{\"authorId\":\"152613920\",\"name\":\"Zhenhua Liu\"},{\"authorId\":\"103603255\",\"name\":\"Yehui Tang\"},{\"authorId\":\"1569696821\",\"name\":\"An Xiao\"},{\"authorId\":\"1691522\",\"name\":\"Chunjing Xu\"},{\"authorId\":\"6898202\",\"name\":\"Yixing Xu\"},{\"authorId\":\"40993275\",\"name\":\"Z. Yang\"},{\"authorId\":\"46868350\",\"name\":\"Yiman Zhang\"},{\"authorId\":\"145047838\",\"name\":\"Dacheng Tao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"49e17ad5bf10eb17f4c35a93a1588a6f0f8760db\",\"title\":\"A Survey on Visual Transformer\",\"url\":\"https://www.semanticscholar.org/paper/49e17ad5bf10eb17f4c35a93a1588a6f0f8760db\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.11760\",\"authors\":[{\"authorId\":\"24040986\",\"name\":\"Gabriel Huang\"},{\"authorId\":\"1560385163\",\"name\":\"Bo Pang\"},{\"authorId\":\"2062703\",\"name\":\"Z. Zhu\"},{\"authorId\":\"66193113\",\"name\":\"C. Rivera\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"title\":\"Multimodal Pretraining for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"2010.11929\",\"authors\":[{\"authorId\":\"1382344214\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"39611591\",\"name\":\"Lucas Beyer\"},{\"authorId\":\"144629422\",\"name\":\"Alexander Kolesnikov\"},{\"authorId\":\"3319373\",\"name\":\"Dirk Weissenborn\"},{\"authorId\":\"30969374\",\"name\":\"Xiaohua Zhai\"},{\"authorId\":\"2465270\",\"name\":\"Thomas Unterthiner\"},{\"authorId\":\"3226635\",\"name\":\"M. Dehghani\"},{\"authorId\":\"46352821\",\"name\":\"Matthias Minderer\"},{\"authorId\":\"2280399\",\"name\":\"Georg Heigold\"},{\"authorId\":\"1802148\",\"name\":\"S. Gelly\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"2815290\",\"name\":\"N. Houlsby\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7b15fa1b8d413fbe14ef7a97f651f47f5aff3903\",\"title\":\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\",\"url\":\"https://www.semanticscholar.org/paper/7b15fa1b8d413fbe14ef7a97f651f47f5aff3903\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03501\",\"authors\":[{\"authorId\":\"3436466\",\"name\":\"Palash Goyal\"},{\"authorId\":\"144756035\",\"name\":\"Saurabh Sahu\"},{\"authorId\":\"46848045\",\"name\":\"S. Ghosh\"},{\"authorId\":\"117523938\",\"name\":\"C. Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0b9fad2013d5ef8f4f64ce3e99f9b355f87a83c\",\"title\":\"Cross-modal Learning for Multi-modal Video Categorization\",\"url\":\"https://www.semanticscholar.org/paper/f0b9fad2013d5ef8f4f64ce3e99f9b355f87a83c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.05054\",\"authors\":[{\"authorId\":\"114577307\",\"name\":\"C. Alberti\"},{\"authorId\":\"50602231\",\"name\":\"Jeffrey Ling\"},{\"authorId\":\"123052390\",\"name\":\"Michael Collins\"},{\"authorId\":\"1781409\",\"name\":\"D. Reitter\"}],\"doi\":\"10.18653/v1/D19-1219\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b82153bf85d5d1edd3f170aace830e5328ca9ed0\",\"title\":\"Fusion of Detected Objects in Text for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b82153bf85d5d1edd3f170aace830e5328ca9ed0\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"50272388\",\"name\":\"Ioana Croitoru\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"title\":\"A hierarchical approach to vision-based language generation: from simple sentences to complex natural language\",\"url\":\"https://www.semanticscholar.org/paper/9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2012.00451\",\"authors\":[{\"authorId\":\"153276988\",\"name\":\"Antoine Yang\"},{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"title\":\"Just Ask: Learning to Answer Questions from Millions of Narrated Videos\",\"url\":\"https://www.semanticscholar.org/paper/b1b6ff0df21818ac8c4f61d86141da48188f36b3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.08824\",\"authors\":[{\"authorId\":\"2878414\",\"name\":\"Xueliang Zhao\"},{\"authorId\":\"145717875\",\"name\":\"Wei Wu\"},{\"authorId\":\"46747953\",\"name\":\"Can Xu\"},{\"authorId\":\"8801869\",\"name\":\"Chongyang Tao\"},{\"authorId\":\"9072379\",\"name\":\"Dongyan Zhao\"},{\"authorId\":\"1845885740\",\"name\":\"Rui Yan\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.272\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3447a432f724aa36595643446acda5b78943db19\",\"title\":\"Knowledge-Grounded Dialogue Generation with Pre-trained Language Models\",\"url\":\"https://www.semanticscholar.org/paper/3447a432f724aa36595643446acda5b78943db19\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.02824\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144721617\",\"name\":\"Y. Asano\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"145414740\",\"name\":\"J. Henriques\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"title\":\"Support-set bottlenecks for video-text representation learning\",\"url\":\"https://www.semanticscholar.org/paper/78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.02472\",\"authors\":[{\"authorId\":\"3361240\",\"name\":\"Manling Li\"},{\"authorId\":\"2778637\",\"name\":\"Alireza Zareian\"},{\"authorId\":\"145653968\",\"name\":\"Q. Zeng\"},{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"152347526\",\"name\":\"Di Lu\"},{\"authorId\":\"153172090\",\"name\":\"Huai-zhong Ji\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":\"10.18653/v1/2020.acl-main.230\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04f7834936bf8f455f804c4d84b52fcffc6784ee\",\"title\":\"Cross-media Structured Common Space for Multimedia Event Extraction\",\"url\":\"https://www.semanticscholar.org/paper/04f7834936bf8f455f804c4d84b52fcffc6784ee\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"1909.02950\",\"authors\":[{\"authorId\":\"1743722\",\"name\":\"Douwe Kiela\"},{\"authorId\":\"47686397\",\"name\":\"Suvrat Bhooshan\"},{\"authorId\":\"22593971\",\"name\":\"Hamed Firooz\"},{\"authorId\":\"1389630028\",\"name\":\"Davide Testuggine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f3003f95798c70675bcea8dc523f89895522e66\",\"title\":\"Supervised Multimodal Bitransformers for Classifying Images and Text\",\"url\":\"https://www.semanticscholar.org/paper/0f3003f95798c70675bcea8dc523f89895522e66\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"112858943\",\"name\":\"Z. Liu\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"}],\"doi\":\"10.1007/978-981-15-5573-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"title\":\"Representation Learning for Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/2f1107ea6b9b4147a808e9ae27757e214685e90a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.04124\",\"authors\":[{\"authorId\":\"40176903\",\"name\":\"Sangho Lee\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"},{\"authorId\":\"6555176\",\"name\":\"Thomas Breuel\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3655a6344ffb57838fd7f7acce651f36d3a9d526\",\"title\":\"Parameter Efficient Multimodal Transformers for Video Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/3655a6344ffb57838fd7f7acce651f36d3a9d526\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.14631\",\"authors\":[{\"authorId\":\"70448703\",\"name\":\"F. Liu\"},{\"authorId\":\"50982032\",\"name\":\"Ge Li\"},{\"authorId\":\"46316986\",\"name\":\"Y. Zhao\"},{\"authorId\":\"143897680\",\"name\":\"Zhi Jin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d\",\"title\":\"Multi-task Learning based Pre-trained Language Model for Code Completion\",\"url\":\"https://www.semanticscholar.org/paper/49cf6a22a5dac5bc98b653534af65ffa0bc0e76d\",\"venue\":\"2020 35th IEEE/ACM International Conference on Automated Software Engineering (ASE)\",\"year\":2020},{\"arxivId\":\"2005.07310\",\"authors\":[{\"authorId\":\"1701219797\",\"name\":\"Jize Cao\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"46700583\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.1007/978-3-030-58539-6_34\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26cfb57a9722599b361858d454ec816420723e36\",\"title\":\"Behind the Scene: Revealing the Secrets of Pre-trained Vision-and-Language Models\",\"url\":\"https://www.semanticscholar.org/paper/26cfb57a9722599b361858d454ec816420723e36\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.10638\",\"authors\":[{\"authorId\":\"3314779\",\"name\":\"Weituo Hao\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1109/cvpr42600.2020.01315\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"046d1e193faeeb404b4a7812d3c635aa5e1ecccd\",\"title\":\"Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training\",\"url\":\"https://www.semanticscholar.org/paper/046d1e193faeeb404b4a7812d3c635aa5e1ecccd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2004.06165\",\"authors\":[{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"1629039205\",\"name\":\"Xi Yin\"},{\"authorId\":\"1978606\",\"name\":\"C. Li\"},{\"authorId\":\"50049779\",\"name\":\"X. Hu\"},{\"authorId\":\"9325940\",\"name\":\"Pengchuan Zhang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"29957038\",\"name\":\"Longguang Wang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"145307652\",\"name\":\"Li Dong\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1007/978-3-030-58577-8_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57\",\"title\":\"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1912.08226\",\"authors\":[{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"144255105\",\"name\":\"M. Stefanini\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"261570379fd841b426a4c51e8004f2cf9f1df771\",\"title\":\"M2: Meshed-Memory Transformer for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/261570379fd841b426a4c51e8004f2cf9f1df771\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2012.04977\",\"authors\":[{\"authorId\":\"50549794\",\"name\":\"Wei-Bo Zhang\"},{\"authorId\":\"50084529\",\"name\":\"Guihua Liu\"},{\"authorId\":\"93591272\",\"name\":\"Zhuohua Li\"},{\"authorId\":\"151499378\",\"name\":\"F. Zhu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf0a74e19a6a43b437ec59dd9d786afec03228b7\",\"title\":\"Hateful Memes Detection via Complementary Visual and Linguistic Networks\",\"url\":\"https://www.semanticscholar.org/paper/bf0a74e19a6a43b437ec59dd9d786afec03228b7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.04446\",\"authors\":[{\"authorId\":\"2018337565\",\"name\":\"Jia Guo\"},{\"authorId\":\"1431754650\",\"name\":\"C. Zhu\"},{\"authorId\":\"46316984\",\"name\":\"Yilun Zhao\"},{\"authorId\":\"3405101\",\"name\":\"He-Da Wang\"},{\"authorId\":\"72285615\",\"name\":\"Yao Hu\"},{\"authorId\":\"101131295\",\"name\":\"Xiaofei He\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6695d3b92e7cd7f2359f698a09c7b3dc37996329\",\"title\":\"LAMP: Label Augmented Multimodal Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/6695d3b92e7cd7f2359f698a09c7b3dc37996329\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.08708\",\"authors\":[{\"authorId\":\"2728646\",\"name\":\"Hantao Huang\"},{\"authorId\":\"145421604\",\"name\":\"T. Han\"},{\"authorId\":\"72549949\",\"name\":\"Wei Han\"},{\"authorId\":\"48986588\",\"name\":\"D. Yap\"},{\"authorId\":\"32312400\",\"name\":\"C. Chiang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"91550d1490caa39f723d3efb5d1b78b6f8b7c6cf\",\"title\":\"Answer-checking in Context: A Multi-modal FullyAttention Network for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/91550d1490caa39f723d3efb5d1b78b6f8b7c6cf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.15780\",\"authors\":[{\"authorId\":\"3360992\",\"name\":\"Cristina Garbacea\"},{\"authorId\":\"1743469\",\"name\":\"Q. Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"64da659c0687762359226b4cf455520c78acd165\",\"title\":\"Neural Language Generation: Formulation, Methods, and Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/64da659c0687762359226b4cf455520c78acd165\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50987563\",\"name\":\"Sangmin Park\"},{\"authorId\":\"97243906\",\"name\":\"Young-gab Kim\"}],\"doi\":\"10.1016/j.inffus.2020.10.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65dc69953ef141871f1701d68196aa278566fc16\",\"title\":\"Survey and challenges of story generation models - A multimodal perspective with five steps: Data embedding, topic modeling, storyline generation, draft story generation, and story evaluation\",\"url\":\"https://www.semanticscholar.org/paper/65dc69953ef141871f1701d68196aa278566fc16\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"1912.04487\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":\"10.1109/cvpr42600.2020.01047\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"title\":\"Listen to Look: Action Recognition by Previewing Audio\",\"url\":\"https://www.semanticscholar.org/paper/c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.16934\",\"authors\":[{\"authorId\":\"40471592\",\"name\":\"Fei Yu\"},{\"authorId\":\"11713158\",\"name\":\"Jiji Tang\"},{\"authorId\":\"2318321\",\"name\":\"Weichong Yin\"},{\"authorId\":\"144825828\",\"name\":\"Y. Sun\"},{\"authorId\":null,\"name\":\"Hao Tian\"},{\"authorId\":\"120155201\",\"name\":\"Hua Wu\"},{\"authorId\":\"144270729\",\"name\":\"Haifeng Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e34cf702b9c90889e268380572bec782280b59c3\",\"title\":\"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph\",\"url\":\"https://www.semanticscholar.org/paper/e34cf702b9c90889e268380572bec782280b59c3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.02930\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/K19-1039\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"title\":\"A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":\"1908.07490\",\"authors\":[{\"authorId\":\"3218666\",\"name\":\"Hao Hao Tan\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D19-1514\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"79c93274429d6355959f1e4374c2147bb81ea649\",\"title\":\"LXMERT: Learning Cross-Modality Encoder Representations from Transformers\",\"url\":\"https://www.semanticscholar.org/paper/79c93274429d6355959f1e4374c2147bb81ea649\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864744\",\"name\":\"Wen-Jun Zeng\"}],\"doi\":\"10.1017/atsip.2019.26\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e83046ed4d7c021b19801bc747c4cc1a6fb7df91\",\"title\":\"Toward human-centric deep video understanding\",\"url\":\"https://www.semanticscholar.org/paper/e83046ed4d7c021b19801bc747c4cc1a6fb7df91\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51016420\",\"name\":\"Gorjan Radevski\"},{\"authorId\":\"144481186\",\"name\":\"Guillem Collell\"},{\"authorId\":\"100781843\",\"name\":\"Marie-Francine Moens\"},{\"authorId\":\"1704728\",\"name\":\"T. Tuytelaars\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.408\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ee9d43e028de1f895f3333a74d863425ee4acdc\",\"title\":\"Decoding language spatial relations to 2D spatial arrangements\",\"url\":\"https://www.semanticscholar.org/paper/8ee9d43e028de1f895f3333a74d863425ee4acdc\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2010.14104\",\"authors\":[{\"authorId\":\"1423718676\",\"name\":\"Bj\\u00f6rn Bebensee\"},{\"authorId\":\"152705134\",\"name\":\"B. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8684d9c94351f758af1a51ae0e93b9e5d76c354d\",\"title\":\"Co-attentional Transformers for Story-Based Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/8684d9c94351f758af1a51ae0e93b9e5d76c354d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.08395\",\"authors\":[{\"authorId\":\"4852522\",\"name\":\"Tariq Habib Afridi\"},{\"authorId\":\"47686775\",\"name\":\"Aftab Alam\"},{\"authorId\":\"1645748271\",\"name\":\"M. N. Khan\"},{\"authorId\":\"1649689679\",\"name\":\"Jawad Khan\"},{\"authorId\":\"2806926\",\"name\":\"Young-Koo Lee\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b0903f184dfccc813d8165d0f69c87f79520014e\",\"title\":\"A Multimodal Memes Classification: A Survey and Open Research Issues\",\"url\":\"https://www.semanticscholar.org/paper/b0903f184dfccc813d8165d0f69c87f79520014e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2262359\",\"name\":\"Yuxuan Wang\"},{\"authorId\":\"8471176\",\"name\":\"Yutai Hou\"},{\"authorId\":\"2256319\",\"name\":\"W. Che\"},{\"authorId\":\"152244126\",\"name\":\"T. Liu\"}],\"doi\":\"10.1007/s13042-020-01069-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"12aa5c0c9fa3077f41f153cdccfa2aacf8940be3\",\"title\":\"From static to dynamic word representations: a survey\",\"url\":\"https://www.semanticscholar.org/paper/12aa5c0c9fa3077f41f153cdccfa2aacf8940be3\",\"venue\":\"Int. J. Mach. Learn. Cybern.\",\"year\":2020},{\"arxivId\":\"2003.05162\",\"authors\":[{\"authorId\":\"1500408667\",\"name\":\"Zhiyuan Fang\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.61\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1794bc353c94e8d708476132eb326fe3af51c2e6\",\"title\":\"Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1794bc353c94e8d708476132eb326fe3af51c2e6\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70026472\",\"name\":\"T. Yu\"},{\"authorId\":\"6470580\",\"name\":\"Y. Yang\"},{\"authorId\":\"47002715\",\"name\":\"Y. Li\"},{\"authorId\":\"49794949\",\"name\":\"Xiaodong Chen\"},{\"authorId\":\"1893044063\",\"name\":\"Mingming Sun\"},{\"authorId\":\"144785135\",\"name\":\"P. Li\"}],\"doi\":\"10.1145/3394486.3403297\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a1d603b0860ca5a27a2813be1e0a0d1277fa44a\",\"title\":\"Combo-Attention Network for Baidu Video Advertising\",\"url\":\"https://www.semanticscholar.org/paper/8a1d603b0860ca5a27a2813be1e0a0d1277fa44a\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"1909.00121\",\"authors\":[{\"authorId\":\"49178142\",\"name\":\"H. Chen\"},{\"authorId\":\"145468578\",\"name\":\"Ke Lin\"},{\"authorId\":\"1772128\",\"name\":\"A. Maye\"},{\"authorId\":\"47786863\",\"name\":\"J. Li\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.3389/frobt.2020.475767\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"304f94dbe2ed228309e86298766ad24d9b6c6747\",\"title\":\"A Semantics-Assisted Video Captioning Model Trained With Scheduled Sampling\",\"url\":\"https://www.semanticscholar.org/paper/304f94dbe2ed228309e86298766ad24d9b6c6747\",\"venue\":\"Frontiers in Robotics and AI\",\"year\":2020},{\"arxivId\":\"2006.13268\",\"authors\":[{\"authorId\":\"51436197\",\"name\":\"Erion cCano\"},{\"authorId\":\"151158933\",\"name\":\"Ondvrej Bojar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"275073c5cf7f373dd22e3aa822678839e09e6dab\",\"title\":\"Automating Text Naturalness Evaluation of NLG Systems\",\"url\":\"https://www.semanticscholar.org/paper/275073c5cf7f373dd22e3aa822678839e09e6dab\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.12146\",\"authors\":[{\"authorId\":\"66536530\",\"name\":\"Yash Kant\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1606382599\",\"name\":\"Peter Anderson\"},{\"authorId\":\"5153264\",\"name\":\"A. Schwing\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"37825612\",\"name\":\"Harsh Agrawal\"}],\"doi\":\"10.1007/978-3-030-58545-7_41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f92dc7d9a61f7b8b5b7cb58f3895541e266ce057\",\"title\":\"Spatially Aware Multimodal Transformers for TextVQA\",\"url\":\"https://www.semanticscholar.org/paper/f92dc7d9a61f7b8b5b7cb58f3895541e266ce057\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738707459\",\"name\":\"Frederico Belcavello\"},{\"authorId\":\"1738707461\",\"name\":\"Marcelo Viridiano\"},{\"authorId\":\"1738713510\",\"name\":\"Alexandre Diniz da Costa\"},{\"authorId\":\"1788718\",\"name\":\"E. Matos\"},{\"authorId\":\"1805988908\",\"name\":\"Tiago Timponi Torrent\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b59f563b96f54f48ff37810c18e1e592d4033e77\",\"title\":\"Frame-Based Annotation of Multimodal Corpora: Tracking (A)Synchronies in Meaning Construction\",\"url\":\"https://www.semanticscholar.org/paper/b59f563b96f54f48ff37810c18e1e592d4033e77\",\"venue\":\"FRAMENET\",\"year\":2020},{\"arxivId\":\"2005.00706\",\"authors\":[{\"authorId\":\"40027632\",\"name\":\"F. F. Xu\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"3109653\",\"name\":\"Junyi Du\"},{\"authorId\":\"1700325\",\"name\":\"Graham Neubig\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"}],\"doi\":\"10.18653/v1/2020.nlpbt-1.4\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"358d7d6333d3edd530e37efd8004cb9da8cfd5d4\",\"title\":\"A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos\",\"url\":\"https://www.semanticscholar.org/paper/358d7d6333d3edd530e37efd8004cb9da8cfd5d4\",\"venue\":\"NLPBT\",\"year\":2020},{\"arxivId\":\"2012.14740\",\"authors\":[{\"authorId\":\"1561676722\",\"name\":\"Yang Xu\"},{\"authorId\":\"3032611\",\"name\":\"Yiheng Xu\"},{\"authorId\":null,\"name\":\"Tengchao Lv\"},{\"authorId\":null,\"name\":\"Lei Cui\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":null,\"name\":\"Guoxin Wang\"},{\"authorId\":\"38534822\",\"name\":\"Y. Lu\"},{\"authorId\":null,\"name\":\"Dinei Florencio\"},{\"authorId\":\"1706673\",\"name\":\"C. Zhang\"},{\"authorId\":null,\"name\":\"Wanxiang Che\"},{\"authorId\":null,\"name\":\"Min Zhang\"},{\"authorId\":\"114451627\",\"name\":\"L. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf561b8cb782f0f18b3fcb9ee5f969f4314889b2\",\"title\":\"LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding\",\"url\":\"https://www.semanticscholar.org/paper/bf561b8cb782f0f18b3fcb9ee5f969f4314889b2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.09920\",\"authors\":[{\"authorId\":\"1911972\",\"name\":\"Tanmay Gupta\"},{\"authorId\":\"3214848\",\"name\":\"Arash Vahdat\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"},{\"authorId\":\"47008546\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2433269\",\"name\":\"Derek Hoiem\"}],\"doi\":\"10.1007/978-3-030-58580-8_44\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"14e6c84fd88badeaa969c27d4cdada764877afff\",\"title\":\"Contrastive Learning for Weakly Supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/14e6c84fd88badeaa969c27d4cdada764877afff\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2006.13297\",\"authors\":[{\"authorId\":\"1389570375\",\"name\":\"Arijit Sehanobish\"},{\"authorId\":\"2400638\",\"name\":\"H. Corzo\"},{\"authorId\":\"103697788\",\"name\":\"Onur Kara\"},{\"authorId\":\"7385683\",\"name\":\"D. V. Dijk\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"890b3b42b4f988a204d990f46c67b34d6c2c8aa0\",\"title\":\"Learning Potentials of Quantum Systems using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/890b3b42b4f988a204d990f46c67b34d6c2c8aa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.15464\",\"authors\":[{\"authorId\":\"1720851638\",\"name\":\"Li Tao\"},{\"authorId\":\"1524733293\",\"name\":\"Xueting Wang\"},{\"authorId\":\"145572095\",\"name\":\"T. Yamasaki\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5ada370b12a174a45b27c2e40d01cd47155bb9d4\",\"title\":\"Self-Supervised Video Representation Using Pretext-Contrastive Learning\",\"url\":\"https://www.semanticscholar.org/paper/5ada370b12a174a45b27c2e40d01cd47155bb9d4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.13784\",\"authors\":[{\"authorId\":\"73441526\",\"name\":\"R. Tan\"},{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c0d69d243c08eaa1d9030c95541a2ae23e8eb85\",\"title\":\"LoGAN: Latent Graph Co-Attention Network for Weakly-Supervised Video Moment Retrieval.\",\"url\":\"https://www.semanticscholar.org/paper/3c0d69d243c08eaa1d9030c95541a2ae23e8eb85\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.03557\",\"authors\":[{\"authorId\":\"32562635\",\"name\":\"Liunian Harold Li\"},{\"authorId\":\"2064210\",\"name\":\"Mark Yatskar\"},{\"authorId\":\"144508458\",\"name\":\"Da Yin\"},{\"authorId\":\"1793529\",\"name\":\"C. Hsieh\"},{\"authorId\":\"2782886\",\"name\":\"Kai-Wei Chang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\"title\":\"VisualBERT: A Simple and Performant Baseline for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/5aec474c31a2f4b74703c6f786c0a8ff85c450da\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.12435\",\"authors\":[{\"authorId\":\"9268397\",\"name\":\"Xuehai He\"},{\"authorId\":\"48569200\",\"name\":\"Zhuo Cai\"},{\"authorId\":\"1629217804\",\"name\":\"Wenlan Wei\"},{\"authorId\":\"49890963\",\"name\":\"Yichen Zhang\"},{\"authorId\":\"37756359\",\"name\":\"Luntian Mou\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"},{\"authorId\":\"40526720\",\"name\":\"Pengtao Xie\"}],\"doi\":\"10.36227/techrxiv.13127537.v1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72e0dccf59f126a64f970fe9f4712b3221a3be8c\",\"title\":\"Pathological Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/72e0dccf59f126a64f970fe9f4712b3221a3be8c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.03186\",\"authors\":[{\"authorId\":\"7885575\",\"name\":\"E. Amrani\"},{\"authorId\":\"1397958297\",\"name\":\"R. Ben-Ari\"},{\"authorId\":\"143679933\",\"name\":\"Daniel Rotman\"},{\"authorId\":\"144858358\",\"name\":\"Alex Bronstein\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"777873ef6d23c2cdd7dfd6c4834eb56769a25bb1\",\"title\":\"Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning\",\"url\":\"https://www.semanticscholar.org/paper/777873ef6d23c2cdd7dfd6c4834eb56769a25bb1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.10151\",\"authors\":[{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"14487640\",\"name\":\"Ari Holtzman\"},{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"50730674\",\"name\":\"Joyce Chai\"},{\"authorId\":\"1412841514\",\"name\":\"M. Lapata\"},{\"authorId\":\"2672644\",\"name\":\"A. Lazaridou\"},{\"authorId\":\"143823227\",\"name\":\"Jonathan May\"},{\"authorId\":\"17109242\",\"name\":\"Aleksandr Nisnevich\"},{\"authorId\":\"30017846\",\"name\":\"Nicolas Pinto\"},{\"authorId\":\"153160559\",\"name\":\"Joseph P. Turian\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.703\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d\",\"title\":\"Experience Grounds Language\",\"url\":\"https://www.semanticscholar.org/paper/bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98795336\",\"name\":\"Kristofer Krus\"},{\"authorId\":\"143644394\",\"name\":\"T. Polishchuk\"},{\"authorId\":\"144575187\",\"name\":\"V. Polishchuk\"}],\"doi\":\"10.1109/AIDA-AT48540.2020.9049170\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69a0cc4ca710795f72a031312424d3c8b765f8c3\",\"title\":\"Identifying Interesting Moments in Controllers Work Video via Dimensionality Reduction\",\"url\":\"https://www.semanticscholar.org/paper/69a0cc4ca710795f72a031312424d3c8b765f8c3\",\"venue\":\"2020 International Conference on Artificial Intelligence and Data Analytics for Air Transportation (AIDA-AT)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1474245162\",\"name\":\"Alison Emily Chow\"},{\"authorId\":\"2542272\",\"name\":\"Michael Gr\\u00fcninger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3654f9c444f06ca3025864b2e83b7ca4c965d70\",\"title\":\"Multimodal Event Recognition with an Ontology for Cooking Recipes\",\"url\":\"https://www.semanticscholar.org/paper/c3654f9c444f06ca3025864b2e83b7ca4c965d70\",\"venue\":\"JOWO\",\"year\":2019},{\"arxivId\":\"1912.06430\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"1466466597\",\"name\":\"Lucas Smaira\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/cvpr42600.2020.00990\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fb40df31aa7177c9d009478479db61c39caebd54\",\"title\":\"End-to-End Learning of Visual Representations From Uncurated Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/fb40df31aa7177c9d009478479db61c39caebd54\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2008.06884\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1753617513\",\"name\":\"Tan Wang\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1704030\",\"name\":\"J. Zhu\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413518\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"080ee4e93438f8b8cbdd894eef15af71f0c30097\",\"title\":\"DeVLBert: Learning Deconfounded Visio-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/080ee4e93438f8b8cbdd894eef15af71f0c30097\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2010.15225\",\"authors\":[{\"authorId\":\"39745976\",\"name\":\"Dylan Ebert\"},{\"authorId\":\"2949185\",\"name\":\"Ellie Pavlick\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b29610253e316ca976d346cec8ad0ca22869b75c\",\"title\":\"A Visuospatial Dataset for Naturalistic Verb Learning\",\"url\":\"https://www.semanticscholar.org/paper/b29610253e316ca976d346cec8ad0ca22869b75c\",\"venue\":\"STARSEM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35996608\",\"name\":\"Junyang Lin\"},{\"authorId\":\"11882893\",\"name\":\"A. Yang\"},{\"authorId\":\"29343468\",\"name\":\"Yichang Zhang\"},{\"authorId\":\"49723003\",\"name\":\"J. Liu\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee4918cc9b1dc28007454490fbe8366ec017b33d\",\"title\":\"InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/ee4918cc9b1dc28007454490fbe8366ec017b33d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.06870\",\"authors\":[{\"authorId\":\"50816334\",\"name\":\"D. Ye\"},{\"authorId\":\"2427350\",\"name\":\"Yankai Lin\"},{\"authorId\":\"153140528\",\"name\":\"Jiaju Du\"},{\"authorId\":\"49047064\",\"name\":\"Zhenghao Liu\"},{\"authorId\":\"1753344\",\"name\":\"M. Sun\"},{\"authorId\":\"1390626344\",\"name\":\"Zhiyuan Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.582\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d51e3280fb8d0678a2d2770672d78dc6dcf1c5b9\",\"title\":\"Coreferential Reasoning Learning for Language Representation\",\"url\":\"https://www.semanticscholar.org/paper/d51e3280fb8d0678a2d2770672d78dc6dcf1c5b9\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2011.09046\",\"authors\":[{\"authorId\":\"1490732820\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"145327825\",\"name\":\"Mingde Zhao\"},{\"authorId\":\"40600020\",\"name\":\"Sheide Chammas\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"39543696\",\"name\":\"Fei Sha\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe21d30afa60f8ab72da309ca0a80eee1ac07a66\",\"title\":\"A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus\",\"url\":\"https://www.semanticscholar.org/paper/fe21d30afa60f8ab72da309ca0a80eee1ac07a66\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.02496\",\"authors\":[{\"authorId\":\"145062296\",\"name\":\"Zihang Jiang\"},{\"authorId\":\"23476952\",\"name\":\"Weihao Yu\"},{\"authorId\":\"18119920\",\"name\":\"Daquan Zhou\"},{\"authorId\":\"1713312\",\"name\":\"Y. Chen\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f78bbe15f14fe4d69b7f8ee611136c19318dd63\",\"title\":\"ConvBERT: Improving BERT with Span-based Dynamic Convolution\",\"url\":\"https://www.semanticscholar.org/paper/9f78bbe15f14fe4d69b7f8ee611136c19318dd63\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2008.03800\",\"authors\":[{\"authorId\":\"47519958\",\"name\":\"Rui Qian\"},{\"authorId\":\"1486442460\",\"name\":\"Tianjian Meng\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"},{\"authorId\":\"37144787\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"3154495\",\"name\":\"H. Wang\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"1387716705\",\"name\":\"Yin Cui\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1f3a9a431dc5bf813455ac89df5c0a4e747bc553\",\"title\":\"Spatiotemporal Contrastive Video Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/1f3a9a431dc5bf813455ac89df5c0a4e747bc553\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1908.08530\",\"authors\":[{\"authorId\":\"145499378\",\"name\":\"Weijie Su\"},{\"authorId\":\"2578924\",\"name\":\"X. Zhu\"},{\"authorId\":\"47746274\",\"name\":\"Y. Cao\"},{\"authorId\":\"48218753\",\"name\":\"B. Li\"},{\"authorId\":\"152309485\",\"name\":\"Lewei Lu\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"3304536\",\"name\":\"Jifeng Dai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2527626c11a84f15709e943fbfa2356e19930e3b\",\"title\":\"VL-BERT: Pre-training of Generic Visual-Linguistic Representations\",\"url\":\"https://www.semanticscholar.org/paper/2527626c11a84f15709e943fbfa2356e19930e3b\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50754085\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"49479171\",\"name\":\"Chen Fang\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"2047181\",\"name\":\"Seokhwan Kim\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"144015161\",\"name\":\"Y. Fu\"}],\"doi\":\"10.1109/cvpr42600.2020.01254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1a780c6219996c8481c117056efcf071cbfbd15\",\"title\":\"Screencast Tutorial Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b1a780c6219996c8481c117056efcf071cbfbd15\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.14727\",\"authors\":[{\"authorId\":\"7164154\",\"name\":\"Polina Zablotskaia\"},{\"authorId\":\"51150385\",\"name\":\"E. A. Dominici\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"20812150\",\"name\":\"Andreas M. Lehrmann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6de213be7dd45672852ae946440d639cc3d8ddb7\",\"title\":\"Unsupervised Video Decomposition using Spatio-temporal Iterative Inference\",\"url\":\"https://www.semanticscholar.org/paper/6de213be7dd45672852ae946440d639cc3d8ddb7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.04259\",\"authors\":[{\"authorId\":\"1474350644\",\"name\":\"Jiyang Gao\"},{\"authorId\":\"1491624845\",\"name\":\"Chen Sun\"},{\"authorId\":\"47940821\",\"name\":\"Hang Zhao\"},{\"authorId\":\"102569870\",\"name\":\"Y. Shen\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"46652068\",\"name\":\"Congcong Li\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/CVPR42600.2020.01154\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"296c808abc2b11c6b7cc44237357b6590bac4f35\",\"title\":\"VectorNet: Encoding HD Maps and Agent Dynamics From Vectorized Representation\",\"url\":\"https://www.semanticscholar.org/paper/296c808abc2b11c6b7cc44237357b6590bac4f35\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2006.11747\",\"authors\":[{\"authorId\":\"1500408667\",\"name\":\"Zhiyuan Fang\"},{\"authorId\":\"34362536\",\"name\":\"S. Kong\"},{\"authorId\":\"1521319166\",\"name\":\"Zhe Wang\"},{\"authorId\":\"143800213\",\"name\":\"Charless C. Fowlkes\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a792ff56eeed530fab1935168510cbb16b0f1b68\",\"title\":\"Weak Supervision and Referring Attention for Temporal-Textual Association Learning\",\"url\":\"https://www.semanticscholar.org/paper/a792ff56eeed530fab1935168510cbb16b0f1b68\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.10460\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143672098\",\"name\":\"Bei Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"35119829\",\"name\":\"R. Song\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"7474269\",\"name\":\"Ping-Ping Lin\"},{\"authorId\":\"47099153\",\"name\":\"Xiaoyu Qi\"},{\"authorId\":\"50096056\",\"name\":\"C. Wang\"},{\"authorId\":\"97807965\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1145/3343031.3350571\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e0779bca8faec33918338f98c0014e387993d388\",\"title\":\"Neural Storyboard Artist: Visualizing Stories with Coherent Image Sequences\",\"url\":\"https://www.semanticscholar.org/paper/e0779bca8faec33918338f98c0014e387993d388\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1912.02379\",\"authors\":[{\"authorId\":\"46258988\",\"name\":\"Vishvak S. Murahari\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"}],\"doi\":\"10.1007/978-3-030-58523-5_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"604d7678235f5bb6039794e382d12058cecf8070\",\"title\":\"Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline\",\"url\":\"https://www.semanticscholar.org/paper/604d7678235f5bb6039794e382d12058cecf8070\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.07526\",\"authors\":[{\"authorId\":\"3451494\",\"name\":\"Ana Marasovi\\u0107\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"39227408\",\"name\":\"Ronan Le Bras\"},{\"authorId\":\"1685669\",\"name\":\"N. A. Smith\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.253\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c9940a17504a3b83bd1e9d613b095ddb204d2ad0\",\"title\":\"Natural Language Rationales with Full-Stack Visual Reasoning: From Pixels to Semantic Frames to Commonsense Graphs\",\"url\":\"https://www.semanticscholar.org/paper/c9940a17504a3b83bd1e9d613b095ddb204d2ad0\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2006.13662\",\"authors\":[{\"authorId\":\"47792365\",\"name\":\"Y. Asano\"},{\"authorId\":null,\"name\":\"Mandela Patrick\"},{\"authorId\":\"49359942\",\"name\":\"C. Rupprecht\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"008ba3c574f8e5085dfb1eef0342e8b408565e45\",\"title\":\"Labelling unlabelled videos from scratch with multi-modal self-supervision\",\"url\":\"https://www.semanticscholar.org/paper/008ba3c574f8e5085dfb1eef0342e8b408565e45\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2002.06353\",\"authors\":[{\"authorId\":\"35347136\",\"name\":\"Huaishao Luo\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"66782928\",\"name\":\"Tianrui Li\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4243555758433880a67b15b50f752b1e2a8c4609\",\"title\":\"UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation\",\"url\":\"https://www.semanticscholar.org/paper/4243555758433880a67b15b50f752b1e2a8c4609\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.01028\",\"authors\":[{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"1413822807\",\"name\":\"Mert Bulent Sariyildiz\"},{\"authorId\":\"147699085\",\"name\":\"No'e Pion\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"2295553\",\"name\":\"Diane Larlus\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"87b008a6289fa22c72e1726a8929e815dfbbc65f\",\"title\":\"Hard Negative Mixing for Contrastive Learning\",\"url\":\"https://www.semanticscholar.org/paper/87b008a6289fa22c72e1726a8929e815dfbbc65f\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47127499\",\"name\":\"R. Rao\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"144478564\",\"name\":\"Elnaz Nouri\"},{\"authorId\":\"1780951\",\"name\":\"Debadeepta Dey\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":\"10.1109/CVPRW50498.2020.00486\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0b9c8fa914be1c59f3d5ad1a5e7e41164e4ca5b2\",\"title\":\"Quality and Relevance Metrics for Selection of Multimodal Pretraining Data\",\"url\":\"https://www.semanticscholar.org/paper/0b9c8fa914be1c59f3d5ad1a5e7e41164e4ca5b2\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":\"2005.06035\",\"authors\":[{\"authorId\":\"46882405\",\"name\":\"Chen Zheng\"},{\"authorId\":\"144919537\",\"name\":\"Quan Guo\"},{\"authorId\":\"2190934\",\"name\":\"Parisa Kordjamshidi\"}],\"doi\":\"10.18653/v1/2020.acl-main.683\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b293e4fd06e8a0f3ea7e46c4dafb575b288515e2\",\"title\":\"Cross-Modality Relevance for Reasoning on Language and Vision\",\"url\":\"https://www.semanticscholar.org/paper/b293e4fd06e8a0f3ea7e46c4dafb575b288515e2\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2009.08018\",\"authors\":[{\"authorId\":\"153000866\",\"name\":\"Xiyan Fu\"},{\"authorId\":\"71563095\",\"name\":\"J. Wang\"},{\"authorId\":\"2881049\",\"name\":\"Zhenglu Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"89232937e3325a078e211015f2ec88660e8fcca0\",\"title\":\"Multi-modal Summarization for Video-containing Documents\",\"url\":\"https://www.semanticscholar.org/paper/89232937e3325a078e211015f2ec88660e8fcca0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.13487\",\"authors\":[{\"authorId\":\"40457423\",\"name\":\"Y. Liu\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"title\":\"Use What You Have: Video retrieval using representations from collaborative experts\",\"url\":\"https://www.semanticscholar.org/paper/b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1852333699\",\"name\":\"Bin Hu\"},{\"authorId\":\"46966185\",\"name\":\"K. Guo\"},{\"authorId\":\"47539466\",\"name\":\"J. Zhang\"},{\"authorId\":\"144008544\",\"name\":\"S. Ren\"},{\"authorId\":\"1695160\",\"name\":\"J. Ma\"}],\"doi\":\"10.1109/MC.2020.2996409\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4c420217b08ef71d3222d23e6b3689b0ae45b26\",\"title\":\"Exploring a Humanoid Video-Understanding Algorithm Guided by Behavior\",\"url\":\"https://www.semanticscholar.org/paper/c4c420217b08ef71d3222d23e6b3689b0ae45b26\",\"venue\":\"Computer\",\"year\":2020},{\"arxivId\":\"2002.12204\",\"authors\":[{\"authorId\":\"134814700\",\"name\":\"T. Wang\"},{\"authorId\":\"50560698\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"32222907\",\"name\":\"Qianru Sun\"}],\"doi\":\"10.1109/cvpr42600.2020.01077\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d9d0197c6d9304d6bc179ac21370c49a33fc2388\",\"title\":\"Visual Commonsense R-CNN\",\"url\":\"https://www.semanticscholar.org/paper/d9d0197c6d9304d6bc179ac21370c49a33fc2388\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2002.10832\",\"authors\":[{\"authorId\":\"90745780\",\"name\":\"Thomas Scialom\"},{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"1502219404\",\"name\":\"Paul-Alexis Dray\"},{\"authorId\":\"1767493\",\"name\":\"Jacopo Staiano\"},{\"authorId\":\"67192547\",\"name\":\"P. Gallinari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6364653539facfdd70837f460b20c62f7ca8a6d\",\"title\":\"What BERT Sees: Cross-Modal Transfer for Visual Question Generation\",\"url\":\"https://www.semanticscholar.org/paper/d6364653539facfdd70837f460b20c62f7ca8a6d\",\"venue\":\"INLG\",\"year\":2020},{\"arxivId\":\"2011.12870\",\"authors\":[{\"authorId\":\"1960607091\",\"name\":\"Yi Zhou\"},{\"authorId\":\"8157338\",\"name\":\"Zhenhao Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"14e5d984af4ac965579644bd0ea4ae42587b6a00\",\"title\":\"Multimodal Learning for Hateful Memes Detection\",\"url\":\"https://www.semanticscholar.org/paper/14e5d984af4ac965579644bd0ea4ae42587b6a00\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.07517\",\"authors\":[{\"authorId\":\"49606678\",\"name\":\"Jianan Wang\"},{\"authorId\":\"49730189\",\"name\":\"Boyang Li\"},{\"authorId\":\"46515715\",\"name\":\"Xiangyu Fan\"},{\"authorId\":\"1845592115\",\"name\":\"J. Lin\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f5cbd60f07a19afc3566376e404a490865e5def\",\"title\":\"Data-efficient Alignment of Multimodal Sequences by Aligning Gradient Updates and Internal Feature Distributions\",\"url\":\"https://www.semanticscholar.org/paper/5f5cbd60f07a19afc3566376e404a490865e5def\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.00626\",\"authors\":[{\"authorId\":\"47002659\",\"name\":\"Yanchao Li\"},{\"authorId\":\"1720749879\",\"name\":\"Miao Liu\"},{\"authorId\":\"50779871\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a48cde936f7dea115cbda99a08e12dbdf45d13fb\",\"title\":\"In the Eye of the Beholder: Gaze and Actions in First Person Video\",\"url\":\"https://www.semanticscholar.org/paper/a48cde936f7dea115cbda99a08e12dbdf45d13fb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.04208\",\"authors\":[{\"authorId\":\"153000035\",\"name\":\"M. Bain\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"152853748\",\"name\":\"A. Brown\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"239c34ab213229725c6428e63b1315f2e8cdcbc8\",\"title\":\"Condensed Movies: Story Based Retrieval with Contextual Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/239c34ab213229725c6428e63b1315f2e8cdcbc8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.12765\",\"authors\":[{\"authorId\":\"74634930\",\"name\":\"Issa Annamoradnejad\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf2e2ee8f2b4e15a655af51571203726ce3be36a\",\"title\":\"ColBERT: Using BERT Sentence Embedding for Humor Detection\",\"url\":\"https://www.semanticscholar.org/paper/bf2e2ee8f2b4e15a655af51571203726ce3be36a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.11059\",\"authors\":[{\"authorId\":\"48206987\",\"name\":\"L. Zhou\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"},{\"authorId\":\"35431603\",\"name\":\"H. Hu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.1609/AAAI.V34I07.7005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"title\":\"Unified Vision-Language Pre-Training for Image Captioning and VQA\",\"url\":\"https://www.semanticscholar.org/paper/088ef8f1bdd733673672a055017ee3dd0e70f2cf\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2925b1a75c72728d2d90157c1d1057cbc5da8b9\",\"title\":\"Structured Procedural Knowledge Extraction from Cooking Videos\",\"url\":\"https://www.semanticscholar.org/paper/d2925b1a75c72728d2d90157c1d1057cbc5da8b9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.05710\",\"authors\":[{\"authorId\":\"14454974\",\"name\":\"P. H. Seo\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"title\":\"Look Before you Speak: Visually Contextualized Utterances\",\"url\":\"https://www.semanticscholar.org/paper/b1f6397717d3cbf84e89081a47205f8ed8395d22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.12829\",\"authors\":[{\"authorId\":\"33806547\",\"name\":\"C. Tran\"},{\"authorId\":\"20132361\",\"name\":\"Changhan Wang\"},{\"authorId\":\"1825565215\",\"name\":\"Yuqing Tang\"},{\"authorId\":\"145761718\",\"name\":\"Y. Tang\"},{\"authorId\":\"145503806\",\"name\":\"J. Pino\"},{\"authorId\":\"2745494\",\"name\":\"X. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78a5633e79018a7f55b9831320162906121d7321\",\"title\":\"Cross-Modal Transfer Learning for Multilingual Speech-to-Text Translation\",\"url\":\"https://www.semanticscholar.org/paper/78a5633e79018a7f55b9831320162906121d7321\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2029680149\",\"name\":\"Jiajia Duan\"},{\"authorId\":\"47941266\",\"name\":\"H. Zhao\"},{\"authorId\":\"31085414\",\"name\":\"Qian Zhou\"},{\"authorId\":\"144326259\",\"name\":\"M. Qiu\"},{\"authorId\":\"49353848\",\"name\":\"Meiqin Liu\"}],\"doi\":\"10.1109/SmartCloud49737.2020.00030\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a2c44f0b729740c5e0aadff833f8031919cf75a8\",\"title\":\"A Study of Pre-trained Language Models in Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/a2c44f0b729740c5e0aadff833f8031919cf75a8\",\"venue\":\"2020 IEEE International Conference on Smart Cloud (SmartCloud)\",\"year\":2020},{\"arxivId\":\"2012.07000\",\"authors\":[{\"authorId\":\"145144396\",\"name\":\"Dandan Song\"},{\"authorId\":\"4274864\",\"name\":\"Siyi Ma\"},{\"authorId\":\"2037300711\",\"name\":\"Zhanchen Sun\"},{\"authorId\":\"3389167\",\"name\":\"S. Yang\"},{\"authorId\":\"31364087\",\"name\":\"Lejian Liao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7912b8bb86a6d32ed355651d05ff0cbf37e9504e\",\"title\":\"KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/7912b8bb86a6d32ed355651d05ff0cbf37e9504e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35399640\",\"name\":\"D\\u00eddac Sur\\u00eds\"},{\"authorId\":\"32486555\",\"name\":\"D. Epstein\"},{\"authorId\":\"153172090\",\"name\":\"Huai-zhong Ji\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"}],\"doi\":\"10.1007/978-3-030-58526-6_26\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"007ca8ca7a68451c32da034c72a06238434843c1\",\"title\":\"Learning to Learn Words from Visual Scenes\",\"url\":\"https://www.semanticscholar.org/paper/007ca8ca7a68451c32da034c72a06238434843c1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1911.12377\",\"authors\":[{\"authorId\":\"67344892\",\"name\":\"Federico Landi\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"40186452\",\"name\":\"M. Corsini\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb4e5d203efae1f0838bb59e31026ce3e7511f87\",\"title\":\"Perceive, Transform, and Act: Multi-Modal Attention Networks for Vision-and-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/bb4e5d203efae1f0838bb59e31026ce3e7511f87\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.01127\",\"authors\":[{\"authorId\":\"48511110\",\"name\":\"Tianqi Liu\"},{\"authorId\":\"1441128337\",\"name\":\"Qizhan Shao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7706ed62e51487ee1fab56f932f5274bdeaea171\",\"title\":\"BERT for Large-scale Video Segment Classification with Test-time Augmentation\",\"url\":\"https://www.semanticscholar.org/paper/7706ed62e51487ee1fab56f932f5274bdeaea171\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.03768\",\"authors\":[{\"authorId\":\"33516562\",\"name\":\"Mohit Shridhar\"},{\"authorId\":\"2854297\",\"name\":\"Xingdi Yuan\"},{\"authorId\":\"40638665\",\"name\":\"Marc-Alexandre C\\u00f4t\\u00e9\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"3382568\",\"name\":\"Adam Trischler\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"398a0625e8707a0b41ac58eaec51e8feb87dd7cb\",\"title\":\"ALFWorld: Aligning Text and Embodied Environments for Interactive Learning\",\"url\":\"https://www.semanticscholar.org/paper/398a0625e8707a0b41ac58eaec51e8feb87dd7cb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2640975\",\"name\":\"Dagmar Gromann\"}],\"doi\":\"10.3233/sw-190373\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3248121914962cf63116ed91cacede86dbb7cc94\",\"title\":\"Neural language models for the multilingual, transcultural, and multimodal Semantic Web\",\"url\":\"https://www.semanticscholar.org/paper/3248121914962cf63116ed91cacede86dbb7cc94\",\"venue\":\"Semantic Web\",\"year\":2020},{\"arxivId\":\"2011.07231\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/cvpr42600.2020.00877\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8cda672bd5487ec2c67d5c217dc84ed8fb786640\",\"title\":\"ActBERT: Learning Global-Local Video-Text Representations\",\"url\":\"https://www.semanticscholar.org/paper/8cda672bd5487ec2c67d5c217dc84ed8fb786640\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2003.10066\",\"authors\":[{\"authorId\":\"2237192\",\"name\":\"Koichiro Yoshino\"},{\"authorId\":\"97189605\",\"name\":\"Kohei Wakimoto\"},{\"authorId\":\"1490936400\",\"name\":\"Yuta Nishimura\"},{\"authorId\":\"50068540\",\"name\":\"S. Nakamura\"}],\"doi\":\"10.1007/978-981-15-8395-7_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4be01587dbdf7cfacc4f83776f419a459e957ce\",\"title\":\"Caption Generation of Robot Behaviors based on Unsupervised Learning of Action Segments\",\"url\":\"https://www.semanticscholar.org/paper/e4be01587dbdf7cfacc4f83776f419a459e957ce\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":102483628,\"doi\":\"10.1109/ICCV.2019.00756\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":22,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"c41a11c0e9b8b92b4faaf97749841170b760760a\",\"references\":[{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1806773\",\"name\":\"I. Misra\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1007/978-3-319-46448-0_32\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d4e3616d0b27957c4107ae877dc0dd4504b69ab\",\"title\":\"Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification\",\"url\":\"https://www.semanticscholar.org/paper/6d4e3616d0b27957c4107ae877dc0dd4504b69ab\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1606.07873\",\"authors\":[{\"authorId\":\"143928130\",\"name\":\"J. Walker\"},{\"authorId\":\"2786693\",\"name\":\"C. Doersch\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"145670946\",\"name\":\"M. Hebert\"}],\"doi\":\"10.1007/978-3-319-46478-7_51\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e96c4626e4b3d09727bcbfbdb2672dd9b886743\",\"title\":\"An Uncertain Future: Forecasting from Static Images Using Variational Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/5e96c4626e4b3d09727bcbfbdb2672dd9b886743\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1503.01558\",\"authors\":[{\"authorId\":\"3274291\",\"name\":\"Jonathan Malmaud\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"145239860\",\"name\":\"Vivek Rathod\"},{\"authorId\":\"40939880\",\"name\":\"Nicholas Johnston\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":\"10.3115/v1/N15-1015\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"59ba3b1b31e2f8adb18fb886ad3fc087081c0e38\",\"title\":\"What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision\",\"url\":\"https://www.semanticscholar.org/paper/59ba3b1b31e2f8adb18fb886ad3fc087081c0e38\",\"venue\":\"HLT-NAACL\",\"year\":2015},{\"arxivId\":\"1707.02968\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"1781242\",\"name\":\"Abhinav Shrivastava\"},{\"authorId\":\"49551046\",\"name\":\"S. Singh\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1109/ICCV.2017.97\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8760bc7631c0cb04e7138254e9fd6451b7def8ca\",\"title\":\"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era\",\"url\":\"https://www.semanticscholar.org/paper/8760bc7631c0cb04e7138254e9fd6451b7def8ca\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Aurelio Ranzato\"},{\"authorId\":null,\"name\":\"Alex Graves\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep unsupervised learning\",\"url\":\"\",\"venue\":\"NIPS Tutorial,\",\"year\":2018},{\"arxivId\":\"1705.08421\",\"authors\":[{\"authorId\":\"39599498\",\"name\":\"C. Gu\"},{\"authorId\":\"94567368\",\"name\":\"C. Sun\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"2997956\",\"name\":\"C. Pantofaru\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"1758054\",\"name\":\"Y. Li\"},{\"authorId\":\"2262946\",\"name\":\"S. Ricco\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"153652146\",\"name\":\"J. Malik\"}],\"doi\":\"10.1109/CVPR.2018.00633\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54c7c3909c7e1e827befdbe8d2595a3b196ba1b8\",\"title\":\"AVA: A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions\",\"url\":\"https://www.semanticscholar.org/paper/54c7c3909c7e1e827befdbe8d2595a3b196ba1b8\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1609.02612\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.13016/M26GIH-TNYZ\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"title\":\"Generating Videos with Scene Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1607.02586\",\"authors\":[{\"authorId\":\"3222730\",\"name\":\"Tianfan Xue\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"18555073\",\"name\":\"K. Bouman\"},{\"authorId\":\"36668046\",\"name\":\"B. Freeman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"397e9b56e46d3cc34af1525493e597facb104570\",\"title\":\"Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/397e9b56e46d3cc34af1525493e597facb104570\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1802.07687\",\"authors\":[{\"authorId\":\"40081727\",\"name\":\"Emily L. Denton\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"de3b9eb697feed3d097e3f671afe395f48c1ab76\",\"title\":\"Stochastic Video Generation with a Learned Prior\",\"url\":\"https://www.semanticscholar.org/paper/de3b9eb697feed3d097e3f671afe395f48c1ab76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1705.06950\",\"authors\":[{\"authorId\":\"21028601\",\"name\":\"W. Kay\"},{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"11809518\",\"name\":\"Brian Zhang\"},{\"authorId\":\"38961760\",\"name\":\"Chloe Hillier\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"143740871\",\"name\":\"F. Viola\"},{\"authorId\":\"143897708\",\"name\":\"T. Green\"},{\"authorId\":\"2830305\",\"name\":\"T. Back\"},{\"authorId\":\"1820908\",\"name\":\"A. Natsev\"},{\"authorId\":\"2573615\",\"name\":\"Mustafa Suleyman\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6\",\"title\":\"The Kinetics Human Action Video Dataset\",\"url\":\"https://www.semanticscholar.org/paper/86e1bdbfd13b9ed137e4c4b8b459a3980eb257f6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1506.09215\",\"authors\":[{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"143688116\",\"name\":\"Nishant Agrawal\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1388317459\",\"name\":\"S. Lacoste-Julien\"}],\"doi\":\"10.1109/CVPR.2016.495\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e17ba2b5d0769e7f2602d859ea77a153846cf27d\",\"title\":\"Unsupervised Learning from Narrated Instruction Videos\",\"url\":\"https://www.semanticscholar.org/paper/e17ba2b5d0769e7f2602d859ea77a153846cf27d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1801.03150\",\"authors\":[{\"authorId\":\"95743023\",\"name\":\"Mathew Monfort\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"3298267\",\"name\":\"Sarah Adel Bargal\"},{\"authorId\":\"50112310\",\"name\":\"Alex Andonian\"},{\"authorId\":\"12082007\",\"name\":\"Tom Yan\"},{\"authorId\":\"40544169\",\"name\":\"K. Ramakrishnan\"},{\"authorId\":\"49860655\",\"name\":\"L. Brown\"},{\"authorId\":\"33421444\",\"name\":\"Quanfu Fan\"},{\"authorId\":\"1891570\",\"name\":\"Dan Gutfreund\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"}],\"doi\":\"10.1109/TPAMI.2019.2901464\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"07174c2f209f15cacf9ad3422b48652df286be69\",\"title\":\"Moments in Time Dataset: One Million Videos for Event Understanding\",\"url\":\"https://www.semanticscholar.org/paper/07174c2f209f15cacf9ad3422b48652df286be69\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1610.09001\",\"authors\":[{\"authorId\":\"3152281\",\"name\":\"Y. Aytar\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"title\":\"SoundNet: Learning Sound Representations from Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1903.02874\",\"authors\":[{\"authorId\":\"35299091\",\"name\":\"Yansong Tang\"},{\"authorId\":\"50792340\",\"name\":\"Dajun Ding\"},{\"authorId\":\"39358728\",\"name\":\"Yongming Rao\"},{\"authorId\":\"145473095\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2118333\",\"name\":\"Danyang Zhang\"},{\"authorId\":\"48096213\",\"name\":\"L. Zhao\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"49640256\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/CVPR.2019.00130\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"title\":\"COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yangqing Jia\"},{\"authorId\":null,\"name\":\"Pierre Sermanet\"},{\"authorId\":null,\"name\":\"E. Scott\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Reed , Dragomir Anguelov , Dumitru Erhan , Vincent Vanhoucke , and Andrew Rabinovich . Going deeper with convolutions\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. X. Lee\"},{\"authorId\":null,\"name\":\"R. Zhang\"},{\"authorId\":null,\"name\":\"F. Ebert\"},{\"authorId\":null,\"name\":\"P. Abbeel\"},{\"authorId\":null,\"name\":\"C. Finn\"},{\"authorId\":null,\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Stochastic adversarial video\",\"url\":\"\",\"venue\":\"prediction. arXiv:1804.01523,\",\"year\":2018},{\"arxivId\":\"1802.05365\",\"authors\":[{\"authorId\":\"39139825\",\"name\":\"Matthew E. Peters\"},{\"authorId\":\"50043859\",\"name\":\"Mark Neumann\"},{\"authorId\":\"2136562\",\"name\":\"Mohit Iyyer\"},{\"authorId\":\"40642935\",\"name\":\"Matt Gardner\"},{\"authorId\":\"143997772\",\"name\":\"Christopher Clark\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":\"10.18653/v1/N18-1202\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3febb2bed8865945e7fddc99efd791887bb7e14f\",\"title\":\"Deep contextualized word representations\",\"url\":\"https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1608.07017\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-319-46448-0_48\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"93a87dfa72f22fba14ef243a62c7d0a6906dfed7\",\"title\":\"Ambient Sound Provides Supervision for Visual Learning\",\"url\":\"https://www.semanticscholar.org/paper/93a87dfa72f22fba14ef243a62c7d0a6906dfed7\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1609.08144\",\"authors\":[{\"authorId\":\"1780996\",\"name\":\"Y. Wu\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"3153147\",\"name\":\"Wolfgang Macherey\"},{\"authorId\":\"2048712\",\"name\":\"M. Krikun\"},{\"authorId\":\"145144022\",\"name\":\"Yuan Cao\"},{\"authorId\":\"145312182\",\"name\":\"Q. Gao\"},{\"authorId\":\"113439369\",\"name\":\"Klaus Macherey\"},{\"authorId\":\"2367620\",\"name\":\"Jeff Klingner\"},{\"authorId\":\"145825976\",\"name\":\"Apurva Shah\"},{\"authorId\":\"145657834\",\"name\":\"M. Johnson\"},{\"authorId\":\"2600217\",\"name\":\"X. Liu\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"2776283\",\"name\":\"S. Gouws\"},{\"authorId\":\"2739610\",\"name\":\"Y. Kato\"},{\"authorId\":\"1765329\",\"name\":\"Taku Kudo\"},{\"authorId\":\"1754386\",\"name\":\"H. Kazawa\"},{\"authorId\":\"144077726\",\"name\":\"K. Stevens\"},{\"authorId\":\"35066890\",\"name\":\"G. Kurian\"},{\"authorId\":\"35173708\",\"name\":\"Nishant Patil\"},{\"authorId\":\"47824598\",\"name\":\"W. Wang\"},{\"authorId\":\"39660914\",\"name\":\"C. Young\"},{\"authorId\":\"27070552\",\"name\":\"J. Smith\"},{\"authorId\":\"2909504\",\"name\":\"Jason Riesa\"},{\"authorId\":\"29951847\",\"name\":\"Alex Rudnick\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"48342565\",\"name\":\"Macduff Hughes\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\",\"title\":\"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1712.09374\",\"authors\":[{\"authorId\":\"49453213\",\"name\":\"Hang Zhao\"},{\"authorId\":\"3305169\",\"name\":\"Zhicheng Yan\"},{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd9157331104a0708aa4f8ae79b7651a5be797c6\",\"title\":\"SLAC: A Sparsely Labeled Dataset for Action Classification and Localization\",\"url\":\"https://www.semanticscholar.org/paper/bd9157331104a0708aa4f8ae79b7651a5be797c6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1810.04805\",\"authors\":[{\"authorId\":\"39172707\",\"name\":\"J. Devlin\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"3259253\",\"name\":\"Kristina Toutanova\"}],\"doi\":\"10.18653/v1/N19-1423\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\"title\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"url\":\"https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":\"1804.00819\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"34872128\",\"name\":\"Yingbo Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.1109/CVPR.2018.00911\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"35ed258aede3df17ee20a6635364cb5fd2461049\",\"title\":\"End-to-End Dense Video Captioning with Masked Transformer\",\"url\":\"https://www.semanticscholar.org/paper/35ed258aede3df17ee20a6635364cb5fd2461049\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1710.11252\",\"authors\":[{\"authorId\":\"3365707\",\"name\":\"M. Babaeizadeh\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"143775101\",\"name\":\"R. Campbell\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"59d86da5c5936e7a236678bf5eaaa7753c226fb1\",\"title\":\"Stochastic Variational Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/59d86da5c5936e7a236678bf5eaaa7753c226fb1\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1817030\",\"name\":\"Saining Xie\"},{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"144035504\",\"name\":\"Zhuowen Tu\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4fa0d73b8ba114578744c2ebaf610d2ca9694f45\",\"title\":\"Rethinking Spatiotemporal Feature Learning For Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/4fa0d73b8ba114578744c2ebaf610d2ca9694f45\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1901.07291\",\"authors\":[{\"authorId\":\"1830914\",\"name\":\"Guillaume Lample\"},{\"authorId\":\"2480903\",\"name\":\"Alexis Conneau\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc\",\"title\":\"Cross-lingual Language Model Pretraining\",\"url\":\"https://www.semanticscholar.org/paper/ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1109/CVPR.2016.18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"932ac3707e1ed84ab67526692a1ef8f064f24ab5\",\"title\":\"Anticipating Visual Representations from Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/932ac3707e1ed84ab67526692a1ef8f064f24ab5\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6e6f47c4b2109e7824cd475336c3676faf9b113e\",\"title\":\"Baby Talk : Understanding and Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/6e6f47c4b2109e7824cd475336c3676faf9b113e\",\"venue\":\"\",\"year\":2011},{\"arxivId\":\"1707.04993\",\"authors\":[{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00165\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"title\":\"MoCoGAN: Decomposing Motion and Content for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1409.4842\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"46641766\",\"name\":\"W. Liu\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"48840704\",\"name\":\"Scott Reed\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"}],\"doi\":\"10.1109/CVPR.2015.7298594\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"title\":\"Going deeper with convolutions\",\"url\":\"https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1804.01523\",\"authors\":[{\"authorId\":\"49250083\",\"name\":\"Alex X. Lee\"},{\"authorId\":\"2844849\",\"name\":\"Richard Zhang\"},{\"authorId\":\"27535721\",\"name\":\"Frederik Ebert\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f67271a1edea3bf80c64c2d54e2a0a57612a567\",\"title\":\"Stochastic Adversarial Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/9f67271a1edea3bf80c64c2d54e2a0a57612a567\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1512.08512\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"145358192\",\"name\":\"E. Adelson\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1109/CVPR.2016.264\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac640c2d0f33fb3ab49f37b26982948fc31e3191\",\"title\":\"Visually Indicated Sounds\",\"url\":\"https://www.semanticscholar.org/paper/ac640c2d0f33fb3ab49f37b26982948fc31e3191\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2015.7298932\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"title\":\"Deep visual-semantic alignments for generating image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/ebd1f6822d1dbb13bb813ff83a3490e0439fc9e4\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":\"1902.04094\",\"authors\":[{\"authorId\":\"144906624\",\"name\":\"Alex Wang\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"}],\"doi\":\"10.18653/v1/W19-2304\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\",\"title\":\"BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model\",\"url\":\"https://www.semanticscholar.org/paper/d79ac7a7bafdc9a782fb8c53285ca11c7f2e3f18\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1703.09788\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e10a5e0baf2aa87d804795af071808a9377cc80a\",\"title\":\"Towards Automatic Learning of Procedures From Web Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/e10a5e0baf2aa87d804795af071808a9377cc80a\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1511.05440\",\"authors\":[{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2341378\",\"name\":\"C. Couprie\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"title\":\"Deep multi-scale video prediction beyond mean square error\",\"url\":\"https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1803.09845\",\"authors\":[{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"94908120\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2018.00754\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"title\":\"Neural Baby Talk\",\"url\":\"https://www.semanticscholar.org/paper/3bf09b2e2639add154a9fe6ff98cc373d3e90e4e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1805.02834\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"46184233\",\"name\":\"N. Louis\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1288aaf45ff85916ccef13668ceba421273a3c36\",\"title\":\"Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction\",\"url\":\"https://www.semanticscholar.org/paper/1288aaf45ff85916ccef13668ceba421273a3c36\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/ICCV.2017.83\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"title\":\"Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017}],\"title\":\"VideoBERT: A Joint Model for Video and Language Representation Learning\",\"topics\":[{\"topic\":\"Vector quantization\",\"topicId\":\"14152\",\"url\":\"https://www.semanticscholar.org/topic/14152\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Speech recognition\",\"topicId\":\"2869\",\"url\":\"https://www.semanticscholar.org/topic/2869\"},{\"topic\":\"Linguistics\",\"topicId\":\"11047\",\"url\":\"https://www.semanticscholar.org/topic/11047\"},{\"topic\":\"Supervised learning\",\"topicId\":\"8357\",\"url\":\"https://www.semanticscholar.org/topic/8357\"},{\"topic\":\"Language model\",\"topicId\":\"26812\",\"url\":\"https://www.semanticscholar.org/topic/26812\"},{\"topic\":\"Modal logic\",\"topicId\":\"61528\",\"url\":\"https://www.semanticscholar.org/topic/61528\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Inspiration function\",\"topicId\":\"16\",\"url\":\"https://www.semanticscholar.org/topic/16\"}],\"url\":\"https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019}\n"