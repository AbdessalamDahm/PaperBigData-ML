"{\"abstract\":\"We focus on multi-modal fusion for egocentric action recognition, and propose a novel architecture for multi-modal temporal-binding, i.e. the combination of modalities within a range of temporal offsets. We train the architecture with three modalities -- RGB, Flow and Audio -- and combine them with mid-level fusion alongside sparse temporal sampling of fused representations. In contrast with previous works, modalities are fused before temporal aggregation, with shared modality fusion weights over time. Our proposed architecture is trained end-to-end, outperforming individual modalities as well as late-fusion of modalities. We demonstrate the importance of audio in egocentric vision, on per-class basis, for identifying actions as well as interacting objects. Our method achieves state of the art results on both the seen and unseen test sets of the largest egocentric dataset: EPIC-Kitchens, on all metrics using the public leaderboard.\",\"arxivId\":\"1908.08498\",\"authors\":[{\"authorId\":\"48842721\",\"name\":\"Evangelos Kazakos\",\"url\":\"https://www.semanticscholar.org/author/48842721\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\",\"url\":\"https://www.semanticscholar.org/author/19263506\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\",\"url\":\"https://www.semanticscholar.org/author/1688869\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\",\"url\":\"https://www.semanticscholar.org/author/145089978\"}],\"citationVelocity\":22,\"citations\":[{\"arxivId\":\"2005.00343\",\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"48842721\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":\"10.1109/TPAMI.2020.2991965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1badccbe4a3cbf8662b924a97bbeea14fe2f1ac7\",\"title\":\"The EPIC-KITCHENS Dataset: Collection, Challenges and Baselines\",\"url\":\"https://www.semanticscholar.org/paper/1badccbe4a3cbf8662b924a97bbeea14fe2f1ac7\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"40454588\",\"name\":\"J. Frahm\"}],\"doi\":\"10.1007/978-3-030-58577-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"title\":\"Computer Vision \\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\\u201328, 2020, Proceedings, Part XXX\",\"url\":\"https://www.semanticscholar.org/paper/08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49507095\",\"name\":\"Haoming Xu\"},{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"1443732549\",\"name\":\"Qingyao Wu\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1145/3394171.3413581\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d0ffb0a8c69ec211f91bda8f5e9f2ed5d5b42bf\",\"title\":\"Cross-Modal Relation-Aware Networks for Audio-Visual Event Localization\",\"url\":\"https://www.semanticscholar.org/paper/1d0ffb0a8c69ec211f91bda8f5e9f2ed5d5b42bf\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2010.05654\",\"authors\":[{\"authorId\":\"39714216\",\"name\":\"F. Ragusa\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"2444519\",\"name\":\"S. Livatino\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a58a0732664b97b471b795df5812f98f24840490\",\"title\":\"The MECCANO Dataset: Understanding Human-Object Interactions from Egocentric Videos in an Industrial-like Domain\",\"url\":\"https://www.semanticscholar.org/paper/a58a0732664b97b471b795df5812f98f24840490\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1596808016\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"50118837\",\"name\":\"Y. Wu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/tpami.2020.3015894\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7401d3895e1cb78c34fa25db12409fdff56b1661\",\"title\":\"Symbiotic Attention for Egocentric Action Recognition with Object-centric Alignment.\",\"url\":\"https://www.semanticscholar.org/paper/7401d3895e1cb78c34fa25db12409fdff56b1661\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1404356633\",\"name\":\"Federica Gerina\"},{\"authorId\":\"1752587993\",\"name\":\"Silvia M. Massa\"},{\"authorId\":\"1752545546\",\"name\":\"Francesca Moi\"},{\"authorId\":\"1752584925\",\"name\":\"Diego Reforgiato Recupero\"},{\"authorId\":\"1752546103\",\"name\":\"Daniele Riboni\"}],\"doi\":\"10.1186/s13673-020-00235-9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0b7851bcb6b85208267e48acd832d7c39819312\",\"title\":\"Recognition of cooking activities through air quality sensor data for supporting food journaling\",\"url\":\"https://www.semanticscholar.org/paper/b0b7851bcb6b85208267e48acd832d7c39819312\",\"venue\":\"Human-centric Computing and Information Sciences\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000124994\",\"name\":\"Mo Sun\"},{\"authorId\":\"81752599\",\"name\":\"J. Li\"},{\"authorId\":\"151503256\",\"name\":\"Hui Feng\"},{\"authorId\":\"2000123285\",\"name\":\"Wei Gou\"},{\"authorId\":\"153784017\",\"name\":\"Haifeng Shen\"},{\"authorId\":\"152226504\",\"name\":\"J. Tang\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"2778556\",\"name\":\"Jie-ping Ye\"}],\"doi\":\"10.1145/3382507.3417971\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d0afe00d375df35342afd0723689aa9570c28863\",\"title\":\"Multi-modal Fusion Using Spatio-temporal and Static Features for Group Emotion Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d0afe00d375df35342afd0723689aa9570c28863\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":\"2007.04364\",\"authors\":[{\"authorId\":\"1802044762\",\"name\":\"Andreea Birhala\"},{\"authorId\":\"152792826\",\"name\":\"C. Ristea\"},{\"authorId\":\"51898972\",\"name\":\"Anamaria Radoi\"},{\"authorId\":\"1423728454\",\"name\":\"Liviu Cristian Du\\u0163u\"}],\"doi\":\"10.1109/TSP49548.2020.9163474\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5cc8f66bc7cfe3e20faf9c7dece404a300502dde\",\"title\":\"Temporal aggregation of audio-visual modalities for emotion recognition\",\"url\":\"https://www.semanticscholar.org/paper/5cc8f66bc7cfe3e20faf9c7dece404a300502dde\",\"venue\":\"2020 43rd International Conference on Telecommunications and Signal Processing (TSP)\",\"year\":2020},{\"arxivId\":\"2007.01883\",\"authors\":[{\"authorId\":\"1397974082\",\"name\":\"Juan-Manuel Perez-Rua\"},{\"authorId\":\"3098817\",\"name\":\"Antoine Toisoul\"},{\"authorId\":\"145944235\",\"name\":\"B. Mart\\u00ednez\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"48571183\",\"name\":\"Liyong Zhang\"},{\"authorId\":\"2171228\",\"name\":\"Xiatian Zhu\"},{\"authorId\":\"145406420\",\"name\":\"Tao Xiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dbdf7284a37d6bc9c68e3fe17596850b9cd6a1bb\",\"title\":\"Egocentric Action Recognition by Video Attention and Temporal Context\",\"url\":\"https://www.semanticscholar.org/paper/dbdf7284a37d6bc9c68e3fe17596850b9cd6a1bb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48760802\",\"name\":\"D. Kumar\"},{\"authorId\":\"153264095\",\"name\":\"C. Kumar\"},{\"authorId\":\"71665304\",\"name\":\"Chun Wei. Seah\"},{\"authorId\":\"50019156\",\"name\":\"Si-Yu Xia\"},{\"authorId\":\"144197068\",\"name\":\"M. Shao\"}],\"doi\":\"10.1145/3394171.3413531\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2c5b58c3aa283518b4cbd483584e936ca0d3884\",\"title\":\"Finding Achilles' Heel: Adversarial Attack on Multi-modal Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a2c5b58c3aa283518b4cbd483584e936ca0d3884\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2006.01595\",\"authors\":[{\"authorId\":\"1822214\",\"name\":\"Haytham M. Fayek\"},{\"authorId\":\"39862695\",\"name\":\"Anurag Kumar\"}],\"doi\":\"10.24963/ijcai.2020/78\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5bc24361d1f1ec16451d9c9531cfb45b99ea6a1f\",\"title\":\"Large Scale Audiovisual Learning of Sounds with Weakly Labeled Data\",\"url\":\"https://www.semanticscholar.org/paper/5bc24361d1f1ec16451d9c9531cfb45b99ea6a1f\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2012.06567\",\"authors\":[{\"authorId\":\"1805946041\",\"name\":\"Yi Zhu\"},{\"authorId\":\"21657733\",\"name\":\"X. Li\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"22539483\",\"name\":\"Chongruo Wu\"},{\"authorId\":\"152781163\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2397422\",\"name\":\"Joseph Tighe\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"49140510\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"title\":\"A Comprehensive Study of Deep Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.11866\",\"authors\":[{\"authorId\":\"2595189\",\"name\":\"Zehua Sun\"},{\"authorId\":\"120809631\",\"name\":\"Jiwang Liu\"},{\"authorId\":\"143969578\",\"name\":\"Qiuhong Ke\"},{\"authorId\":\"1877377\",\"name\":\"H. Rahmani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"816236bf3219363bfe4b847363e137b1fe6712e7\",\"title\":\"Human Action Recognition from Various Data Modalities: A Review\",\"url\":\"https://www.semanticscholar.org/paper/816236bf3219363bfe4b847363e137b1fe6712e7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50118603\",\"name\":\"Y. Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"144652817\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"7179962\",\"name\":\"Yan-qi Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TIP.2020.3040521\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"acfa6fcd7343c14e3f150c0541cc94e672cf1a73\",\"title\":\"Learning to Anticipate Egocentric Actions by Imagination\",\"url\":\"https://www.semanticscholar.org/paper/acfa6fcd7343c14e3f150c0541cc94e672cf1a73\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2021},{\"arxivId\":\"2003.04298\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"47792365\",\"name\":\"Y. Asano\"},{\"authorId\":\"145891577\",\"name\":\"Ruth Fong\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1681543\",\"name\":\"G. Zweig\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ddbaa34c574124a91fa3bc217e232e17668e84c\",\"title\":\"Multi-modal Self-Supervision from Generalized Data Transformations\",\"url\":\"https://www.semanticscholar.org/paper/8ddbaa34c574124a91fa3bc217e232e17668e84c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3443768\",\"name\":\"M. A. Rahman\"},{\"authorId\":null,\"name\":\"Robert Lagani\\u00e8re\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a70b46841bdf48f8a15508f1f48b51937d082efa\",\"title\":\"Mid-level Fusion for End-to-End Temporal Activity Detection in Untrimmed Video\",\"url\":\"https://www.semanticscholar.org/paper/a70b46841bdf48f8a15508f1f48b51937d082efa\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2008.09890\",\"authors\":[{\"authorId\":\"1677780022\",\"name\":\"Dima Damen\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e6fd410f1d327580f704de83f5c8e073a711c93\",\"title\":\"Supervision Levels Scale (SLS)\",\"url\":\"https://www.semanticscholar.org/paper/8e6fd410f1d327580f704de83f5c8e073a711c93\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1740811711\",\"name\":\"Shinya Michibata\"},{\"authorId\":\"2256574\",\"name\":\"K. Inoue\"},{\"authorId\":\"2593208\",\"name\":\"M. Yoshioka\"},{\"authorId\":\"1733070603\",\"name\":\"Atsushi Hashimoto\"}],\"doi\":\"10.1145/3379175.3391712\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66bead45f2fff3f9175658eb036f1a38031f2ba7\",\"title\":\"Cooking Activity Recognition in Egocentric Videos with a Hand Mask Image Branch in the Multi-stream CNN\",\"url\":\"https://www.semanticscholar.org/paper/66bead45f2fff3f9175658eb036f1a38031f2ba7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"1734802895\",\"name\":\"Dipika Singhania\"},{\"authorId\":\"144031869\",\"name\":\"A. Yao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e77735f1131cdb42db5a03093e4d15ebce34d473\",\"title\":\"Temporal Aggregate Representations for Long Term Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/e77735f1131cdb42db5a03093e4d15ebce34d473\",\"venue\":\"ECCV 2020\",\"year\":2020},{\"arxivId\":\"2001.08740\",\"authors\":[{\"authorId\":\"2299381\",\"name\":\"Fanyi Xiao\"},{\"authorId\":\"144756076\",\"name\":\"Y. Lee\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"153652147\",\"name\":\"J. Malik\"},{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"80685dc522d30b18f3feb97d6c977f71fa746325\",\"title\":\"Audiovisual SlowFast Networks for Video Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80685dc522d30b18f3feb97d6c977f71fa746325\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.11154\",\"authors\":[{\"authorId\":\"1825768521\",\"name\":\"Kamalesh Palanisamy\"},{\"authorId\":\"1734802895\",\"name\":\"Dipika Singhania\"},{\"authorId\":\"1803321310\",\"name\":\"Angela Yao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dfa8b5cfd28a8b461c1a1e1bb13e3c6e28a24a28\",\"title\":\"Rethinking CNN Models for Audio Classification\",\"url\":\"https://www.semanticscholar.org/paper/dfa8b5cfd28a8b461c1a1e1bb13e3c6e28a24a28\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.00626\",\"authors\":[{\"authorId\":\"47002659\",\"name\":\"Yanchao Li\"},{\"authorId\":\"1720749879\",\"name\":\"Miao Liu\"},{\"authorId\":\"50779871\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a48cde936f7dea115cbda99a08e12dbdf45d13fb\",\"title\":\"In the Eye of the Beholder: Gaze and Actions in First Person Video\",\"url\":\"https://www.semanticscholar.org/paper/a48cde936f7dea115cbda99a08e12dbdf45d13fb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.03137\",\"authors\":[{\"authorId\":\"144652817\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"47096706\",\"name\":\"Y. Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1609/AAAI.V34I07.6907\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4bca2665f80765d25e71796c928dd20963e0b26e\",\"title\":\"Symbiotic Attention with Privileged Information for Egocentric Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/4bca2665f80765d25e71796c928dd20963e0b26e\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1912.00381\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":\"10.1109/cvpr42600.2020.00118\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"57cee868188127305f966a178ca22025b397d911\",\"title\":\"Gate-Shift Networks for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/57cee868188127305f966a178ca22025b397d911\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1789820\",\"name\":\"N. Wake\"},{\"authorId\":\"7997400\",\"name\":\"Iori Yanokura\"},{\"authorId\":\"1882605\",\"name\":\"Kazuhiro Sasabuchi\"},{\"authorId\":\"66117417\",\"name\":\"K. Ikeuchi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bc00a91901a936be3b99ad8cfbce2625e690d416\",\"title\":\"Verbal Focus-of-Attention System for Learning-from-Demonstration\",\"url\":\"https://www.semanticscholar.org/paper/bc00a91901a936be3b99ad8cfbce2625e690d416\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.04487\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":\"10.1109/cvpr42600.2020.01047\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"title\":\"Listen to Look: Action Recognition by Previewing Audio\",\"url\":\"https://www.semanticscholar.org/paper/c4128ba8deeb67e731cf52fb98addcfddd487e55\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1596808016\",\"name\":\"Xiaohan Wang\"},{\"authorId\":\"50118837\",\"name\":\"Y. Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca2c5474491f0a7117e7ca05fc2bdf49deff4b68\",\"title\":\"Symbiotic Attention: UTS-Baidu Submission to the EPIC-Kitchens 2020 Action Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/ca2c5474491f0a7117e7ca05fc2bdf49deff4b68\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.13725\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"7855312\",\"name\":\"S. Escalera\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a2e870d3561756bdfd4daeb0c5735434b484a7dd\",\"title\":\"FBK-HUPBA Submission to the EPIC-Kitchens Action Recognition 2020 Challenge\",\"url\":\"https://www.semanticscholar.org/paper/a2e870d3561756bdfd4daeb0c5735434b484a7dd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71013766\",\"name\":\"Mathilde Brousmiche\"},{\"authorId\":\"47322308\",\"name\":\"S. Dupont\"},{\"authorId\":\"1993689197\",\"name\":\"Jean Rout\"}],\"doi\":\"10.1145/3422852.3423486\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"204190d66b85145bacc42001b760ff91b84d5443\",\"title\":\"Intra and Inter-modality Interactions for Audio-visual Event Detection\",\"url\":\"https://www.semanticscholar.org/paper/204190d66b85145bacc42001b760ff91b84d5443\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.12102\",\"authors\":[{\"authorId\":\"1773047\",\"name\":\"Qing Gao\"},{\"authorId\":\"3356854\",\"name\":\"Ming-Tao Pei\"},{\"authorId\":\"2187859\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c195c6aa7269a7d4b64d24b079e3fd61d97f6e9c\",\"title\":\"Do You Live a Healthy Life? Analyzing Lifestyle by Visual Life Logging\",\"url\":\"https://www.semanticscholar.org/paper/c195c6aa7269a7d4b64d24b079e3fd61d97f6e9c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.00830\",\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"1734802895\",\"name\":\"Dipika Singhania\"},{\"authorId\":\"1803321310\",\"name\":\"Angela Yao\"}],\"doi\":\"10.1007/978-3-030-58517-4_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c8f191fa7b5c51af0810b0996a0ef25b4db4d9a\",\"title\":\"Temporal Aggregate Representations for Long-Range Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/6c8f191fa7b5c51af0810b0996a0ef25b4db4d9a\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2012.01641\",\"authors\":[{\"authorId\":\"1452981772\",\"name\":\"Lei Zhang\"},{\"authorId\":\"143741775\",\"name\":\"F. Zhou\"},{\"authorId\":\"145673161\",\"name\":\"W. Wei\"},{\"authorId\":\"48379046\",\"name\":\"Y. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"648ad8757436e0d954a5319138d85fb64edea26e\",\"title\":\"Meta-Generating Deep Attentive Metric for Few-shot Classification\",\"url\":\"https://www.semanticscholar.org/paper/648ad8757436e0d954a5319138d85fb64edea26e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.10558\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"40580714\",\"name\":\"Dingzeyu Li\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-58580-8_26\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3867340091c920dc5f8ba462197fa5bc924a98c4\",\"title\":\"Unified Multisensory Perception: Weakly-Supervised Audio-Visual Video Parsing\",\"url\":\"https://www.semanticscholar.org/paper/3867340091c920dc5f8ba462197fa5bc924a98c4\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2012.04124\",\"authors\":[{\"authorId\":\"40176903\",\"name\":\"Sangho Lee\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"},{\"authorId\":\"6555176\",\"name\":\"Thomas Breuel\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3655a6344ffb57838fd7f7acce651f36d3a9d526\",\"title\":\"Parameter Efficient Multimodal Transformers for Video Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/3655a6344ffb57838fd7f7acce651f36d3a9d526\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1944880380\",\"name\":\"Harrison Taylor\"},{\"authorId\":\"151473559\",\"name\":\"Liam Hiley\"},{\"authorId\":\"1944881984\",\"name\":\"Jack Furby\"},{\"authorId\":\"1762890\",\"name\":\"A. Preece\"},{\"authorId\":\"2215679\",\"name\":\"Dave Braines\"}],\"doi\":\"10.23919/FUSION45008.2020.9190215\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"403f487d325dad5f179c154603a8141ae5b908bd\",\"title\":\"VADR: Discriminative Multimodal Explanations for Situational Understanding\",\"url\":\"https://www.semanticscholar.org/paper/403f487d325dad5f179c154603a8141ae5b908bd\",\"venue\":\"2020 IEEE 23rd International Conference on Information Fusion (FUSION)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7634810\",\"name\":\"Weiyao Wang\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"}],\"doi\":\"10.1109/CVPR42600.2020.01271\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f6caf91f731fab861ef420f680cf691f12f70134\",\"title\":\"What Makes Training Multi-Modal Classification Networks Hard?\",\"url\":\"https://www.semanticscholar.org/paper/f6caf91f731fab861ef420f680cf691f12f70134\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2005.02190\",\"authors\":[{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"}],\"doi\":\"10.1109/TPAMI.2020.2992889\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"07cf6c4c1714a0cd88e5c1566aac9df40e111db7\",\"title\":\"Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video\",\"url\":\"https://www.semanticscholar.org/paper/07cf6c4c1714a0cd88e5c1566aac9df40e111db7\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"2001.04463\",\"authors\":[{\"authorId\":\"151500851\",\"name\":\"K. Deng\"},{\"authorId\":\"3294630\",\"name\":\"Aayush Bansal\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1dcee6a820b9d23c9cca11b1de96dd14f97e9f2a\",\"title\":\"Unsupervised Any-to-Many Audiovisual Synthesis via Exemplar Autoencoders\",\"url\":\"https://www.semanticscholar.org/paper/1dcee6a820b9d23c9cca11b1de96dd14f97e9f2a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.13256\",\"authors\":[{\"authorId\":\"1677780022\",\"name\":\"Dima Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"48842721\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"153155867\",\"name\":\"Jian Ma\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":\"10.5523/bris.2g1n6qdydwa9u22shpxqzp0t8m\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c1a80daadab9c3bc3fdf183c669070ba7a3fd37\",\"title\":\"Rescaling Egocentric Vision\",\"url\":\"https://www.semanticscholar.org/paper/6c1a80daadab9c3bc3fdf183c669070ba7a3fd37\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48003030\",\"name\":\"Haibin Yu\"},{\"authorId\":\"49166770\",\"name\":\"W. Jia\"},{\"authorId\":\"48459110\",\"name\":\"Li Zhang\"},{\"authorId\":\"144645724\",\"name\":\"M. Pan\"},{\"authorId\":\"35588611\",\"name\":\"Yuan-yuan Liu\"},{\"authorId\":\"31179730\",\"name\":\"Mingui Sun\"}],\"doi\":\"10.1007/s12652-020-02241-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ed03223d7702f28b41f7f61398050fdf711bfbb1\",\"title\":\"A hierarchical parallel fusion framework for egocentric ADL recognition based on discernment frame partitioning and belief coarsening\",\"url\":\"https://www.semanticscholar.org/paper/ed03223d7702f28b41f7f61398050fdf711bfbb1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.04687\",\"authors\":[{\"authorId\":\"2678268\",\"name\":\"P. Wu\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"9754502\",\"name\":\"Yujia Shi\"},{\"authorId\":\"5264927\",\"name\":\"Yujia Sun\"},{\"authorId\":\"1802542506\",\"name\":\"Fangtao Shao\"},{\"authorId\":\"48551946\",\"name\":\"Zhaoyang Wu\"},{\"authorId\":\"40615725\",\"name\":\"Zhiwei Yang\"}],\"doi\":\"10.1007/978-3-030-58577-8_20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f28873be3601c5a2736996eba543cf51950a381\",\"title\":\"Not only Look, but also Listen: Learning Multimodal Violence Detection under Weak Supervision\",\"url\":\"https://www.semanticscholar.org/paper/8f28873be3601c5a2736996eba543cf51950a381\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.10916\",\"authors\":[{\"authorId\":\"90683745\",\"name\":\"K. Panchal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"07c9e3c2481a074ed44b7967e49d2a7d75c6f06c\",\"title\":\"Hierachical Delta-Attention Method for Multimodal Fusion\",\"url\":\"https://www.semanticscholar.org/paper/07c9e3c2481a074ed44b7967e49d2a7d75c6f06c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.12391\",\"authors\":[{\"authorId\":\"1756108\",\"name\":\"N. Anantrasirichai\"},{\"authorId\":\"2073462\",\"name\":\"D. Bull\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3670d55a696dea9234660149b490e22fb5c2a3a5\",\"title\":\"Artificial Intelligence in the Creative Industries: A Review\",\"url\":\"https://www.semanticscholar.org/paper/3670d55a696dea9234660149b490e22fb5c2a3a5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.01616\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"153246625\",\"name\":\"Chang\\u2019an Chen\"},{\"authorId\":\"9187007\",\"name\":\"Z. Al-Halah\"},{\"authorId\":\"2368282\",\"name\":\"Carl Schissler\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-030-58545-7_38\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33006edd74d92453903df5a53fa25539047c1850\",\"title\":\"VisualEchoes: Spatial Image Representation Learning through Echolocation\",\"url\":\"https://www.semanticscholar.org/paper/33006edd74d92453903df5a53fa25539047c1850\",\"venue\":\"ECCV\",\"year\":2020}],\"corpusId\":201306082,\"doi\":\"10.1109/ICCV.2019.00559\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"7c9de67cc76aeecddcd07e8898acea3ef4eba738\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2776623\",\"name\":\"N. Qian\"}],\"doi\":\"10.1016/S0893-6080(98)00116-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"735d4220d5579cc6afe956d9f6ea501a96ae99e2\",\"title\":\"On the momentum term in gradient descent learning algorithms\",\"url\":\"https://www.semanticscholar.org/paper/735d4220d5579cc6afe956d9f6ea501a96ae99e2\",\"venue\":\"Neural Networks\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":\"10.1109/CVPR.2012.6248010\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9e81caf9dd31b893ebbee3970c312619b7eac7bf\",\"title\":\"Detecting activities of daily living in first-person camera views\",\"url\":\"https://www.semanticscholar.org/paper/9e81caf9dd31b893ebbee3970c312619b7eac7bf\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":\"1706.06905\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1238d0c296c1263afa958ccc1bff3d65e6430be3\",\"title\":\"Learnable pooling with Context Gating for video classification\",\"url\":\"https://www.semanticscholar.org/paper/1238d0c296c1263afa958ccc1bff3d65e6430be3\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ruohan Gao\"},{\"authorId\":null,\"name\":\"Kristen Grauman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"5D visual sound\",\"url\":\"\",\"venue\":\"CVPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49455017\",\"name\":\"Yipin Zhou\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/ICCV.2015.511\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d12c567768b401f8d7f1ad532a2b23320ca29b49\",\"title\":\"Temporal Perception and Prediction in Ego-Centric Video\",\"url\":\"https://www.semanticscholar.org/paper/d12c567768b401f8d7f1ad532a2b23320ca29b49\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1704.02895\",\"authors\":[{\"authorId\":\"3102850\",\"name\":\"Rohit Girdhar\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/CVPR.2017.337\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"63213d080a43660ac59ea12e3c35e6953f6d7ce8\",\"title\":\"ActionVLAD: Learning Spatio-Temporal Aggregation for Action Classification\",\"url\":\"https://www.semanticscholar.org/paper/63213d080a43660ac59ea12e3c35e6953f6d7ce8\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1804.02748\",\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"12387007\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc50c9392fd23b6c88915177c6ae904a498aacea\",\"title\":\"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset\",\"url\":\"https://www.semanticscholar.org/paper/fc50c9392fd23b6c88915177c6ae904a498aacea\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1711.08496\",\"authors\":[{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"50112310\",\"name\":\"Alex Andonian\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-030-01246-5_49\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ad12d3ee186403b856639b58d7797aa4b89a6c7\",\"title\":\"Temporal Relational Reasoning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8ad12d3ee186403b856639b58d7797aa4b89a6c7\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1807.11794\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"75fac714ccc1b616048c9bb6ca918c58317c56a1\",\"title\":\"Attention is All We Need: Nailing Down Object-centric Attention for Egocentric Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/75fac714ccc1b616048c9bb6ca918c58317c56a1\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":\"1804.02516\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3448af861bf5d44ce7ab6b25002504815212252e\",\"title\":\"Learning a Text-Video Embedding from Incomplete and Heterogeneous Data\",\"url\":\"https://www.semanticscholar.org/paper/3448af861bf5d44ce7ab6b25002504815212252e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1807.00230\",\"authors\":[{\"authorId\":\"3443095\",\"name\":\"Bruno Korbar\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e057a9b195f0ea2d1c5a1e88ff9606f9b67ef8b\",\"title\":\"Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization\",\"url\":\"https://www.semanticscholar.org/paper/2e057a9b195f0ea2d1c5a1e88ff9606f9b67ef8b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1804.03641\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1007/978-3-030-01231-1_39\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"title\":\"Audio-Visual Scene Analysis with Self-Supervised Multisensory Features\",\"url\":\"https://www.semanticscholar.org/paper/171f8f1090ef0533ff470ed5a4d31ecfefcc74be\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1610.09001\",\"authors\":[{\"authorId\":\"3152281\",\"name\":\"Y. Aytar\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"title\":\"SoundNet: Learning Sound Representations from Unlabeled Video\",\"url\":\"https://www.semanticscholar.org/paper/7ab8d3af6f78f9c9f64a2f2d38471401ad0988a9\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1803.03849\",\"authors\":[{\"authorId\":\"40895287\",\"name\":\"Arda Senocak\"},{\"authorId\":\"66808667\",\"name\":\"Tae-Hyun Oh\"},{\"authorId\":\"3053231\",\"name\":\"J. Kim\"},{\"authorId\":\"1715634\",\"name\":\"Ming-Hsuan Yang\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"}],\"doi\":\"10.1109/CVPR.2018.00458\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b91d738cd1f5d550c5b27f328e55308a0a73b2d2\",\"title\":\"Learning to Localize Sound Source in Visual Scenes\",\"url\":\"https://www.semanticscholar.org/paper/b91d738cd1f5d550c5b27f328e55308a0a73b2d2\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1899753\",\"name\":\"Ryo Yonetani\"},{\"authorId\":\"37991449\",\"name\":\"Kris M. Kitani\"},{\"authorId\":\"9467266\",\"name\":\"Y. Sato\"}],\"doi\":\"10.1109/CVPR.2016.288\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"16461bc1f06c781b5357cb1157248d28dc3672a0\",\"title\":\"Recognizing Micro-Actions and Reactions from Paired Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/16461bc1f06c781b5357cb1157248d28dc3672a0\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48071615\",\"name\":\"Huda Alamri\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa6c59f0cab5f8dcc899a356d364ce51626536a8\",\"title\":\"Audio Visual Scene-aware dialog (AVSD) Track for Natural Language Generation in DSTC7\",\"url\":\"https://www.semanticscholar.org/paper/aa6c59f0cab5f8dcc899a356d364ce51626536a8\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1702.01992\",\"authors\":[{\"authorId\":\"3258998\",\"name\":\"John Arevalo\"},{\"authorId\":\"1794626\",\"name\":\"T. Solorio\"},{\"authorId\":\"1400883928\",\"name\":\"M. Montes-y-G\\u00f3mez\"},{\"authorId\":\"145580599\",\"name\":\"Fabio A. Gonz\\u00e1lez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"37e3721940352df07faebd87620732c05b458985\",\"title\":\"Gated Multimodal Units for Information Fusion\",\"url\":\"https://www.semanticscholar.org/paper/37e3721940352df07faebd87620732c05b458985\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1712.06651\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01246-5_27\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfc504536e8434eb008680343abb77010965169e\",\"title\":\"Objects that Sound\",\"url\":\"https://www.semanticscholar.org/paper/dfc504536e8434eb008680343abb77010965169e\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1608.07017\",\"authors\":[{\"authorId\":\"144956994\",\"name\":\"Andrew Owens\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"2324658\",\"name\":\"J. McDermott\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.1007/978-3-319-46448-0_48\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"93a87dfa72f22fba14ef243a62c7d0a6906dfed7\",\"title\":\"Ambient Sound Provides Supervision for Visual Learning\",\"url\":\"https://www.semanticscholar.org/paper/93a87dfa72f22fba14ef243a62c7d0a6906dfed7\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1804.00326\",\"authors\":[{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2018.00879\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2c75658b080a9baaac20db39af86016ffa36f6f0\",\"title\":\"Seeing Voices and Hearing Faces: Cross-Modal Biometric Matching\",\"url\":\"https://www.semanticscholar.org/paper/2c75658b080a9baaac20db39af86016ffa36f6f0\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48039353\",\"name\":\"Suriya Singh\"},{\"authorId\":\"145676233\",\"name\":\"Chetan Arora\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1109/CVPR.2016.287\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"28efff48583dfbcf8e6e552f5720278db1ae0fe3\",\"title\":\"First Person Action Recognition Using Deep Learned Descriptors\",\"url\":\"https://www.semanticscholar.org/paper/28efff48583dfbcf8e6e552f5720278db1ae0fe3\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2527741\",\"name\":\"Sibo Song\"},{\"authorId\":\"1802086\",\"name\":\"V. Chandrasekhar\"},{\"authorId\":\"1709001\",\"name\":\"B. Mandal\"},{\"authorId\":\"35718875\",\"name\":\"L. Li\"},{\"authorId\":\"6516914\",\"name\":\"J. Lim\"},{\"authorId\":\"34094551\",\"name\":\"G. S. Babu\"},{\"authorId\":\"2307813\",\"name\":\"P. P. San\"},{\"authorId\":\"143770929\",\"name\":\"N. Cheung\"}],\"doi\":\"10.1109/CVPRW.2016.54\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a9168bd32550e06f2cd23105f82f6735fb2edf4\",\"title\":\"Multimodal Multi-Stream Deep Learning for Egocentric Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6a9168bd32550e06f2cd23105f82f6735fb2edf4\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2016},{\"arxivId\":\"1608.00859\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-46484-8_2\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"title\":\"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1807.10066\",\"authors\":[{\"authorId\":\"3102850\",\"name\":\"Rohit Girdhar\"},{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"2786693\",\"name\":\"C. Doersch\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6fb3940ddd658e549a111870f10ca77ba3c4cf37\",\"title\":\"A Better Baseline for AVA\",\"url\":\"https://www.semanticscholar.org/paper/6fb3940ddd658e549a111870f10ca77ba3c4cf37\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3407277\",\"name\":\"Adam Paszke\"},{\"authorId\":\"39793298\",\"name\":\"S. Gross\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"114250963\",\"name\":\"G. Chanan\"},{\"authorId\":\"50064334\",\"name\":\"E. Yang\"},{\"authorId\":\"81505016\",\"name\":\"Zachary Devito\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"3050846\",\"name\":\"Alban Desmaison\"},{\"authorId\":\"3029482\",\"name\":\"L. Antiga\"},{\"authorId\":\"1977806\",\"name\":\"A. Lerer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b36a5bb1707bb9c70025294b3a310138aae8327a\",\"title\":\"Automatic differentiation in PyTorch\",\"url\":\"https://www.semanticscholar.org/paper/b36a5bb1707bb9c70025294b3a310138aae8327a\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ruohan Gao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Kristen Grauman . 2 . 5 D visual sound\",\"url\":\"\",\"venue\":\"ActivityNet Workshop at CVPR\",\"year\":2016},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144756076\",\"name\":\"Y. Lee\"},{\"authorId\":\"34724702\",\"name\":\"Joydeep Ghosh\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2012.6247820\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"616a23ebf79e35033c84797993943013c5dde5a0\",\"title\":\"Discovering important people and objects for egocentric video summarization\",\"url\":\"https://www.semanticscholar.org/paper/616a23ebf79e35033c84797993943013c5dde5a0\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":\"1605.03688\",\"authors\":[{\"authorId\":\"2238622\",\"name\":\"Minghuang Ma\"},{\"authorId\":\"2681569\",\"name\":\"Haoqi Fan\"},{\"authorId\":\"37991449\",\"name\":\"Kris M. Kitani\"}],\"doi\":\"10.1109/CVPR.2016.209\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"444eba373d46c9f7d58cad74989ec9109b0d5219\",\"title\":\"Going Deeper into First-Person Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/444eba373d46c9f7d58cad74989ec9109b0d5219\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145835943\",\"name\":\"Ryan A. Stevenson\"},{\"authorId\":\"144678507\",\"name\":\"M. Wilson\"},{\"authorId\":\"144379294\",\"name\":\"A. Powers\"},{\"authorId\":\"2813609\",\"name\":\"M. Wallace\"}],\"doi\":\"10.1007/s00221-012-3387-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"049e940947a6394c24caa1d5ddf92f91fff66efb\",\"title\":\"The effects of visual training on multisensory temporal processing\",\"url\":\"https://www.semanticscholar.org/paper/049e940947a6394c24caa1d5ddf92f91fff66efb\",\"venue\":\"Experimental Brain Research\",\"year\":2012},{\"arxivId\":\"1804.09627\",\"authors\":[{\"authorId\":\"34280810\",\"name\":\"Gunnar A. Sigurdsson\"},{\"authorId\":null,\"name\":\"Abhinav Gupta\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"}],\"doi\":\"10.1145/3265987.3265995\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5bce81f9f0a4b962ae39205be9985bc5128fce9\",\"title\":\"Actor and Observer: Joint Modeling of First and Third-Person Videos\",\"url\":\"https://www.semanticscholar.org/paper/e5bce81f9f0a4b962ae39205be9985bc5128fce9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2436014\",\"name\":\"P. M\\u00e9gevand\"},{\"authorId\":\"2064758\",\"name\":\"S. Molholm\"},{\"authorId\":\"32676481\",\"name\":\"Ashabari Nayak\"},{\"authorId\":\"3229047\",\"name\":\"J. Foxe\"}],\"doi\":\"10.1371/journal.pone.0071608\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba84b1bfde94971a11828205e35740fe9f32842f\",\"title\":\"Recalibration of the Multisensory Temporal Window of Integration Results from Changing Task Demands\",\"url\":\"https://www.semanticscholar.org/paper/ba84b1bfde94971a11828205e35740fe9f32842f\",\"venue\":\"PloS one\",\"year\":2013},{\"arxivId\":\"1705.08168\",\"authors\":[{\"authorId\":\"2299479\",\"name\":\"R. Arandjelovi\\u0107\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/ICCV.2017.73\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b5f696f73c1264ccb8e97d3b738a2342ecd6bee\",\"title\":\"Look, Listen and Learn\",\"url\":\"https://www.semanticscholar.org/paper/9b5f696f73c1264ccb8e97d3b738a2342ecd6bee\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681054\",\"name\":\"H. J\\u00e9gou\"},{\"authorId\":\"3271933\",\"name\":\"M. Douze\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"144565371\",\"name\":\"P. P\\u00e9rez\"}],\"doi\":\"10.1109/CVPR.2010.5540039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"400e09ceca374f0621335f84a4daf2049d5902be\",\"title\":\"Aggregating local descriptors into a compact image representation\",\"url\":\"https://www.semanticscholar.org/paper/400e09ceca374f0621335f84a4daf2049d5902be\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":\"1604.06573\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2016.213\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d9aced120e530484609164c836da64548693484\",\"title\":\"Convolutional Two-Stream Network Fusion for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"47119743\",\"name\":\"X. Wang\"},{\"authorId\":\"145222820\",\"name\":\"H. Ye\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1145/2964284.2964328\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"899be93e14d991017b1f8a4afdf907cbc03cf300\",\"title\":\"Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/899be93e14d991017b1f8a4afdf907cbc03cf300\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1711.09550\",\"authors\":[{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"48033101\",\"name\":\"Xiao Liu\"},{\"authorId\":\"35247507\",\"name\":\"Shilei Wen\"}],\"doi\":\"10.1109/CVPR.2018.00817\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5406fd98aa22bc2a0c1a8bc2a58ca3eb7a91155d\",\"title\":\"Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/5406fd98aa22bc2a0c1a8bc2a58ca3eb7a91155d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1703.09026\",\"authors\":[{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"},{\"authorId\":\"1398236231\",\"name\":\"W. Mayol-Cuevas\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":\"10.1109/iccv.2017.314\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b02b4fb293eae3a2d87df98a132b3d701f44a579\",\"title\":\"Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video\",\"url\":\"https://www.semanticscholar.org/paper/b02b4fb293eae3a2d87df98a132b3d701f44a579\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1812.04204\",\"authors\":[{\"authorId\":\"3387849\",\"name\":\"Ruohan Gao\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2019.00041\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b809837560cf11937ee857338eb1a7ccd2abc7b2\",\"title\":\"2.5D Visual Sound\",\"url\":\"https://www.semanticscholar.org/paper/b809837560cf11937ee857338eb1a7ccd2abc7b2\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2813609\",\"name\":\"M. Wallace\"},{\"authorId\":\"145835943\",\"name\":\"Ryan A. Stevenson\"}],\"doi\":\"10.1016/j.neuropsychologia.2014.08.005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"895c84076db92b9dc4cd4cac0fe85a877ea61e95\",\"title\":\"The construct of the multisensory temporal binding window and its dysregulation in developmental disabilities\",\"url\":\"https://www.semanticscholar.org/paper/895c84076db92b9dc4cd4cac0fe85a877ea61e95\",\"venue\":\"Neuropsychologia\",\"year\":2014},{\"arxivId\":\"1705.07750\",\"authors\":[{\"authorId\":\"35681810\",\"name\":\"J. Carreira\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2017.502\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"title\":\"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\",\"url\":\"https://www.semanticscholar.org/paper/b61a3f8b80bbd44f24544dc915f52fd30bbdf485\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Rohit Girdhar\"},{\"authorId\":null,\"name\":\"Joo Carreira\"},{\"authorId\":null,\"name\":\"Carl Doersch\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Andrew Zisserman. A better baseline for ava\",\"url\":\"\",\"venue\":\"ActivityNet Workshop at CVPR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Herve Jegou\"},{\"authorId\":null,\"name\":\"Matthijs Douze\"},{\"authorId\":null,\"name\":\"Cordelia Schmid\"},{\"authorId\":null,\"name\":\"Patrick Perez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"2 . 5 D visual sound Actionvlad : Learning spatio - temporal aggregation for action classification Batch normalization : Accelerating deep network training by reducing internal co - variate shift\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"48033101\",\"name\":\"Xiao Liu\"},{\"authorId\":\"48515099\",\"name\":\"Y. Li\"},{\"authorId\":\"50984378\",\"name\":\"F. Li\"},{\"authorId\":\"35247507\",\"name\":\"Shilei Wen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"686970efdcef5d11a4d2b1f5d77d5515d766f53a\",\"title\":\"Multimodal Keyless Attention Fusion for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/686970efdcef5d11a4d2b1f5d77d5515d766f53a\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1805.00833\",\"authors\":[{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1007/978-3-030-01261-8_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2344004f0e1409c0c9473d071a5cfd74bff0a5d\",\"title\":\"Learnable PINs: Cross-Modal Embeddings for Person Identity\",\"url\":\"https://www.semanticscholar.org/paper/a2344004f0e1409c0c9473d071a5cfd74bff0a5d\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145089978\",\"name\":\"D. Damen\"},{\"authorId\":\"3067867\",\"name\":\"T. Leelasawassuk\"},{\"authorId\":\"2356868\",\"name\":\"Osian Haines\"},{\"authorId\":\"3336943\",\"name\":\"A. Calway\"},{\"authorId\":\"1398236231\",\"name\":\"W. Mayol-Cuevas\"}],\"doi\":\"10.5244/C.28.30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d37a63088d0f296b3dc5f587944cd3c65d1e52e\",\"title\":\"You-Do, I-Learn: Discovering Task Relevant Objects and their Modes of Interaction from Multi-User Egocentric Video\",\"url\":\"https://www.semanticscholar.org/paper/6d37a63088d0f296b3dc5f587944cd3c65d1e52e\",\"venue\":\"BMVC\",\"year\":2014},{\"arxivId\":\"1711.07430\",\"authors\":[{\"authorId\":\"8131625\",\"name\":\"W. Lin\"},{\"authorId\":\"145071344\",\"name\":\"Yang Mi\"},{\"authorId\":\"49388002\",\"name\":\"J. Wu\"},{\"authorId\":\"144392311\",\"name\":\"Ke Lu\"},{\"authorId\":\"144045763\",\"name\":\"Hongkai Xiong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4f22d424292594c164a5f72846ac2cbfe94cd69f\",\"title\":\"Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion\",\"url\":\"https://www.semanticscholar.org/paper/4f22d424292594c164a5f72846ac2cbfe94cd69f\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144959150\",\"name\":\"C. Parise\"},{\"authorId\":\"144248893\",\"name\":\"C. Spence\"},{\"authorId\":\"1741502\",\"name\":\"M. Ernst\"}],\"doi\":\"10.1016/j.cub.2011.11.039\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c853c1248ef9f70057afa71599674c4b2c0d6a4\",\"title\":\"When Correlation Implies Causation in Multisensory Integration\",\"url\":\"https://www.semanticscholar.org/paper/9c853c1248ef9f70057afa71599674c4b2c0d6a4\",\"venue\":\"Current Biology\",\"year\":2012}],\"title\":\"EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition\",\"topics\":[{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Sampling (signal processing)\",\"topicId\":\"7839\",\"url\":\"https://www.semanticscholar.org/topic/7839\"},{\"topic\":\"Modality (human\\u2013computer interaction)\",\"topicId\":\"462\",\"url\":\"https://www.semanticscholar.org/topic/462\"},{\"topic\":\"Sparse matrix\",\"topicId\":\"126\",\"url\":\"https://www.semanticscholar.org/topic/126\"},{\"topic\":\"Modal logic\",\"topicId\":\"61528\",\"url\":\"https://www.semanticscholar.org/topic/61528\"},{\"topic\":\"Multimodal interaction\",\"topicId\":\"42592\",\"url\":\"https://www.semanticscholar.org/topic/42592\"},{\"topic\":\"Binding problem\",\"topicId\":\"614689\",\"url\":\"https://www.semanticscholar.org/topic/614689\"}],\"url\":\"https://www.semanticscholar.org/paper/7c9de67cc76aeecddcd07e8898acea3ef4eba738\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019}\n"