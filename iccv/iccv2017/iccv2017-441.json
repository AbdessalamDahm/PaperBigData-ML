"{\"abstract\":\"Current methods for video description are based on encoder-decoder sentence generation using recurrent neural networks (RNNs). Recent work has demonstrated the advantages of integrating temporal attention mechanisms into these models, in which the decoder network predicts each word in the description by selectively giving more weight to encoded features from specific time frames. Such methods typically use two different types of features: image features (from an object classification model), and motion features (from an action recognition model), combined by naive concatenation in the model input. Because different feature modalities may carry task-relevant information at different times, fusing them by naive concatenation may limit the model's ability to dynamically determine the relevance of each type of feature to different parts of the description. In this paper, we incorporate audio features in addition to the image and motion features. To fuse these three modalities, we introduce a multimodal attention model that can selectively utilize features from different modalities for each word in the output description. Combining our new multimodal attention model with standard temporal attention outperforms state-of-the-art methods on two standard datasets: YouTube2Text and MSR-VTT.\",\"arxivId\":\"1701.03126\",\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\",\"url\":\"https://www.semanticscholar.org/author/1765212\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\",\"url\":\"https://www.semanticscholar.org/author/145443186\"},{\"authorId\":\"1747615\",\"name\":\"Teng-Yok Lee\",\"url\":\"https://www.semanticscholar.org/author/1747615\"},{\"authorId\":\"7969330\",\"name\":\"Ziming Zhang\",\"url\":\"https://www.semanticscholar.org/author/7969330\"},{\"authorId\":\"145222187\",\"name\":\"B. Harsham\",\"url\":\"https://www.semanticscholar.org/author/145222187\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\",\"url\":\"https://www.semanticscholar.org/author/2387467\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\",\"url\":\"https://www.semanticscholar.org/author/34749896\"},{\"authorId\":\"145441213\",\"name\":\"K. Sumi\",\"url\":\"https://www.semanticscholar.org/author/145441213\"}],\"citationVelocity\":54,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"87301211\",\"name\":\"X. Zhang\"},{\"authorId\":\"102556299\",\"name\":\"Boming Li\"},{\"authorId\":\"153409166\",\"name\":\"C. Song\"},{\"authorId\":\"32151849\",\"name\":\"Zhengwen Huang\"},{\"authorId\":\"47003172\",\"name\":\"Y. Li\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"99acbda960d2c5fad7bf5e1340e14edf034ded34\",\"title\":\"SASRM: A Semantic and Attention Spatio-temporal Recurrent Model for Next Location Prediction\",\"url\":\"https://www.semanticscholar.org/paper/99acbda960d2c5fad7bf5e1340e14edf034ded34\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TMM.2019.2896515\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"title\":\"Generating Video Descriptions With Latent Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48637710\",\"name\":\"Yongqing Zhu\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1145/3343031.3350932\",\"intent\":[\"background\",\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"title\":\"Attention-based Densely Connected LSTM for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"145211780\",\"name\":\"Bin Fan\"},{\"authorId\":\"1380311632\",\"name\":\"Shinming Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.18653/v1/D19-1213\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"title\":\"Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag\",\"url\":\"https://www.semanticscholar.org/paper/ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145488619\",\"name\":\"M. Liu\"},{\"authorId\":null,\"name\":\"Xiang Wang\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"1748939\",\"name\":\"B. Chen\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1145/3209978.3210003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"519da94369c1d87e09c592f239b55cc9486b5b7c\",\"title\":\"Attentive Moment Retrieval in Videos\",\"url\":\"https://www.semanticscholar.org/paper/519da94369c1d87e09c592f239b55cc9486b5b7c\",\"venue\":\"SIGIR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2018/164\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f4821a615f08fdad69957a19366c79d939bfd5f\",\"title\":\"Video Captioning with Tube Features\",\"url\":\"https://www.semanticscholar.org/paper/2f4821a615f08fdad69957a19366c79d939bfd5f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1007/s00530-018-0598-5\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"title\":\"Multi-guiding long short-term memory for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"venue\":\"Multimedia Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51123172\",\"name\":\"Darshana Priyasad\"},{\"authorId\":\"34735743\",\"name\":\"T. Fernando\"},{\"authorId\":\"1980700\",\"name\":\"Simon Denman\"},{\"authorId\":\"1729760\",\"name\":\"S. Sridharan\"},{\"authorId\":\"3140440\",\"name\":\"C. Fookes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"48d5edd8d705f5c51f01a1d958de375cca8b06f5\",\"title\":\"Memory Based Attentive Fusion\",\"url\":\"https://www.semanticscholar.org/paper/48d5edd8d705f5c51f01a1d958de375cca8b06f5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50873936\",\"name\":\"Jingbei Li\"},{\"authorId\":\"3860920\",\"name\":\"Zhiyong Wu\"},{\"authorId\":\"9582233\",\"name\":\"Runnan Li\"},{\"authorId\":\"9450377\",\"name\":\"Mingxing Xu\"},{\"authorId\":\"38717208\",\"name\":\"Kehua Lei\"},{\"authorId\":\"7239047\",\"name\":\"Lianhong Cai\"}],\"doi\":\"10.1007/978-3-319-94361-9_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8976d049d9360d08d54dd146e9f8b92cce4eaeb9\",\"title\":\"Multi-modal Multi-scale Speech Expression Evaluation in Computer-Assisted Language Learning\",\"url\":\"https://www.semanticscholar.org/paper/8976d049d9360d08d54dd146e9f8b92cce4eaeb9\",\"venue\":\"AIMS\",\"year\":2018},{\"arxivId\":\"1904.04357\",\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"49469577\",\"name\":\"X. Zhang\"},{\"authorId\":\"50202300\",\"name\":\"Shu Zhang\"},{\"authorId\":\"46314996\",\"name\":\"Wensheng Wang\"},{\"authorId\":null,\"name\":\"Chi Zhang\"},{\"authorId\":\"46675463\",\"name\":\"Heng Huang\"}],\"doi\":\"10.1109/CVPR.2019.00210\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c5d99eff1377e141be293336a14ffddb323c364\",\"title\":\"Heterogeneous Memory Enhanced Multimodal Attention Model for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/5c5d99eff1377e141be293336a14ffddb323c364\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"87725970\",\"name\":\"Yuanchao Li\"},{\"authorId\":null,\"name\":\"Tianyu Zhao\"},{\"authorId\":\"48947005\",\"name\":\"Xun Shen\"}],\"doi\":\"10.1145/3371382.3378261\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"1b78f9b92c6258585dcd066217acf8d5b8503852\",\"title\":\"Attention-Based Multimodal Fusion for Estimating Human Emotion in Real-World HRI\",\"url\":\"https://www.semanticscholar.org/paper/1b78f9b92c6258585dcd066217acf8d5b8503852\",\"venue\":\"HRI\",\"year\":2020},{\"arxivId\":\"2006.11405\",\"authors\":[{\"authorId\":\"9148956\",\"name\":\"Chongyang Bai\"},{\"authorId\":\"2028262\",\"name\":\"H. Chen\"},{\"authorId\":\"39703734\",\"name\":\"Srijan Kumar\"},{\"authorId\":\"1702139\",\"name\":\"J. Leskovec\"},{\"authorId\":\"1728462\",\"name\":\"V. Subrahmanian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51819e6b428676c373d97f267d18b8d4a25020fd\",\"title\":\"M2P2: Multimodal Persuasion Prediction using Adaptive Fusion\",\"url\":\"https://www.semanticscholar.org/paper/51819e6b428676c373d97f267d18b8d4a25020fd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1807.10018\",\"authors\":[{\"authorId\":\"51152390\",\"name\":\"Yilei Xiong\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01252-6_29\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"title\":\"Move Forward and Tell: A Progressive Generator of Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1901.03461\",\"authors\":[{\"authorId\":\"2237192\",\"name\":\"Koichiro Yoshino\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"121436310\",\"name\":\"Julien Perez\"},{\"authorId\":\"1405511901\",\"name\":\"Luis Fernando D'Haro\"},{\"authorId\":\"1725498\",\"name\":\"L. Polymenakos\"},{\"authorId\":\"144543562\",\"name\":\"R. Chulaka Gunasekara\"},{\"authorId\":\"2598433\",\"name\":\"Walter S. Lasecki\"},{\"authorId\":\"1727211\",\"name\":\"Jonathan K. Kummerfeld\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"3125776\",\"name\":\"Chris Brockett\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"},{\"authorId\":\"71886367\",\"name\":\"Xiang Gao\"},{\"authorId\":\"2809915\",\"name\":\"H. AlAmri\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69004f329149096b2b672083e4ee4268bb7fef74\",\"title\":\"Dialog System Technology Challenge 7\",\"url\":\"https://www.semanticscholar.org/paper/69004f329149096b2b672083e4ee4268bb7fef74\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2009.05636\",\"authors\":[{\"authorId\":\"8664360\",\"name\":\"J. Wittenbach\"},{\"authorId\":\"3242748\",\"name\":\"Brian Dalessandro\"},{\"authorId\":\"8954363\",\"name\":\"C. Bruss\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"77c6bdddf20c4552421c980d90a91f78d4e968c6\",\"title\":\"Machine Learning for Temporal Data in Finance: Challenges and Opportunities\",\"url\":\"https://www.semanticscholar.org/paper/77c6bdddf20c4552421c980d90a91f78d4e968c6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.09849\",\"authors\":[{\"authorId\":\"31743563\",\"name\":\"A. Falcon\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"},{\"authorId\":\"46887324\",\"name\":\"Giuseppe Serra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"926045fee007662f4365ef7e68ccc94776491899\",\"title\":\"Data augmentation techniques for the Video Question Answering task\",\"url\":\"https://www.semanticscholar.org/paper/926045fee007662f4365ef7e68ccc94776491899\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.1109/ASRU.2017.8268968\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a8bed0a96d9ad3e5c7ecbaaf2a8967e034e72cb3\",\"title\":\"Early and late integration of audio features for automatic video description\",\"url\":\"https://www.semanticscholar.org/paper/a8bed0a96d9ad3e5c7ecbaaf2a8967e034e72cb3\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":\"1803.07950\",\"authors\":[{\"authorId\":\"4322411\",\"name\":\"L. Li\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/WACV.2019.00042\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"title\":\"End-to-End Video Captioning With Multitask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1384783507\",\"name\":\"Anni Li\"},{\"authorId\":\"2601046\",\"name\":\"J. Qi\"},{\"authorId\":\"153176123\",\"name\":\"Huchuan Lu\"}],\"doi\":\"10.1016/j.neucom.2020.06.021\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a551e9bce44d57c390dc34aa40d023cb9a889ee\",\"title\":\"Multi-attention guided feature fusion network for salient object detection\",\"url\":\"https://www.semanticscholar.org/paper/7a551e9bce44d57c390dc34aa40d023cb9a889ee\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"119738090\",\"name\":\"M. Liu\"},{\"authorId\":\"33977299\",\"name\":\"X. Song\"}],\"doi\":\"10.2200/s00938ed1v01y201907ivm020\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2837a59970edfb24d69e8f6c08ceaa888dbed95\",\"title\":\"Multimodal Learning toward Micro-Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/a2837a59970edfb24d69e8f6c08ceaa888dbed95\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144039832\",\"name\":\"P. Barros\"},{\"authorId\":\"2236890\",\"name\":\"Manfred Eppe\"},{\"authorId\":\"2988592\",\"name\":\"G. Parisi\"},{\"authorId\":\"144227938\",\"name\":\"X. Liu\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.3389/frobt.2019.00137\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"afbc0881addf58dd82afb2580d0fecd42f65acd6\",\"title\":\"Expectation Learning for Stimulus Prediction Across Modalities Improves Unisensory Classification\",\"url\":\"https://www.semanticscholar.org/paper/afbc0881addf58dd82afb2580d0fecd42f65acd6\",\"venue\":\"Front. Robot. AI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145878079\",\"name\":\"Vivek Sharma\"}],\"doi\":\"10.5445/IR/1000119819\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3605e0e6ea610576fa85fcf8737b9f20ddd87180\",\"title\":\"Self-supervised Face Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/3605e0e6ea610576fa85fcf8737b9f20ddd87180\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1903.06496\",\"authors\":[{\"authorId\":\"1414405933\",\"name\":\"Juan-Manuel P\\u00e9rez-R\\u00faa\"},{\"authorId\":\"26339425\",\"name\":\"Valentin Vielzeuf\"},{\"authorId\":\"2642628\",\"name\":\"S. Pateux\"},{\"authorId\":\"2341854\",\"name\":\"M. Baccouche\"},{\"authorId\":\"82117876\",\"name\":\"F. Jurie\"}],\"doi\":\"10.1109/CVPR.2019.00713\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5cd45ee1e91ba7a68d2a18d0735a75ed021766a\",\"title\":\"MFAS: Multimodal Fusion Architecture Search\",\"url\":\"https://www.semanticscholar.org/paper/e5cd45ee1e91ba7a68d2a18d0735a75ed021766a\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2372326\",\"name\":\"Md. Mostafa Kamal Sarker\"},{\"authorId\":\"2698334\",\"name\":\"Hatem A. Rashwan\"},{\"authorId\":\"2961376\",\"name\":\"Farhan Akram\"},{\"authorId\":\"32212960\",\"name\":\"E. Talavera\"},{\"authorId\":\"46231365\",\"name\":\"Syeda Furruka Banu\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"},{\"authorId\":\"143844336\",\"name\":\"D. Puig\"}],\"doi\":\"10.1109/ACCESS.2019.2902225\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60eed1cd8570d8c53e2ef3d0ba909139299f08c2\",\"title\":\"Recognizing Food Places in Egocentric Photo-Streams Using Multi-Scale Atrous Convolutional Networks and Self-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/60eed1cd8570d8c53e2ef3d0ba909139299f08c2\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1787270\",\"name\":\"D. Vazhenina\"},{\"authorId\":\"1753161751\",\"name\":\"Konstantin Markov\"}],\"doi\":\"10.3390/electronics9071157\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"575c7f6162452652ed6e416612a585bfa875f4ec\",\"title\":\"End-to-End Noisy Speech Recognition Using Fourier and Hilbert Spectrum Features\",\"url\":\"https://www.semanticscholar.org/paper/575c7f6162452652ed6e416612a585bfa875f4ec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2001.09485\",\"authors\":[{\"authorId\":\"1430732489\",\"name\":\"Cong Bao\"},{\"authorId\":\"2931718\",\"name\":\"Z. Fountas\"},{\"authorId\":\"38792093\",\"name\":\"Temitayo A. Olugbade\"},{\"authorId\":\"1398541310\",\"name\":\"N. Bianchi-Berthouze\"}],\"doi\":\"10.1145/3382507.3418849\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4cd487c03e52adeb3919b147f6f9772daf69007\",\"title\":\"Multimodal Data Fusion based on the Global Workspace Theory\",\"url\":\"https://www.semanticscholar.org/paper/b4cd487c03e52adeb3919b147f6f9772daf69007\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":\"2001.05840\",\"authors\":[{\"authorId\":\"36325868\",\"name\":\"L. Shi\"},{\"authorId\":\"1947101\",\"name\":\"Shijie Geng\"},{\"authorId\":\"2198730\",\"name\":\"K. Shuang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"51263928\",\"name\":\"Songxiang Liu\"},{\"authorId\":\"153933134\",\"name\":\"Peng Gao\"},{\"authorId\":\"47374777\",\"name\":\"S. Su\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053595\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad6500f4e4548be232e8027cfa648577e8e0ca4b\",\"title\":\"Multi-Layer Content Interaction Through Quaternion Product for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/ad6500f4e4548be232e8027cfa648577e8e0ca4b\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/TIP.2020.2988435\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"title\":\"Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2579920\",\"name\":\"Haojie Li\"},{\"authorId\":\"66939932\",\"name\":\"Sihang Wu\"},{\"authorId\":\"1860840\",\"name\":\"Shuangping Huang\"},{\"authorId\":\"144847940\",\"name\":\"K. Lam\"},{\"authorId\":\"67023635\",\"name\":\"Xiao-Fen Xing\"}],\"doi\":\"10.1109/ACCESS.2019.2958405\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"529ce7b768a1cd88d3bafe93a581ccfd0c441402\",\"title\":\"Deep Motion-Appearance Convolutions for Robust Visual Tracking\",\"url\":\"https://www.semanticscholar.org/paper/529ce7b768a1cd88d3bafe93a581ccfd0c441402\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3252486\",\"name\":\"Madhura Jayaratne\"},{\"authorId\":\"47848775\",\"name\":\"D. de Silva\"},{\"authorId\":\"143775049\",\"name\":\"D. Alahakoon\"}],\"doi\":\"10.1109/TASE.2019.2910508\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d964b51d2e86a3f87746b7e9bb1347ef9c0358c2\",\"title\":\"Unsupervised Machine Learning Based Scalable Fusion for Active Perception\",\"url\":\"https://www.semanticscholar.org/paper/d964b51d2e86a3f87746b7e9bb1347ef9c0358c2\",\"venue\":\"IEEE Transactions on Automation Science and Engineering\",\"year\":2019},{\"arxivId\":\"1804.05448\",\"authors\":[{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.18653/v1/N18-2125\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"title\":\"Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":\"1912.10132\",\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b6e21f2a308c02d943107ff260ffc2ce6f13180\",\"title\":\"Exploring Context, Attention and Audio Features for Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/5b6e21f2a308c02d943107ff260ffc2ce6f13180\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1404341452\",\"name\":\"Adria Mallol-Ragolta\"},{\"authorId\":\"143889734\",\"name\":\"Ziping Zhao\"},{\"authorId\":\"113705775\",\"name\":\"Lukas Stappen\"},{\"authorId\":\"49249279\",\"name\":\"Nicholas Cummins\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"}],\"doi\":\"10.21437/interspeech.2019-2036\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8db761dc173e30b0882390892fe92af7acd11208\",\"title\":\"A Hierarchical Attention Network-Based Approach for Depression Detection from Transcribed Clinical Interviews\",\"url\":\"https://www.semanticscholar.org/paper/8db761dc173e30b0882390892fe92af7acd11208\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"2005.08182\",\"authors\":[{\"authorId\":\"20421748\",\"name\":\"M. S. Grover\"},{\"authorId\":\"50793081\",\"name\":\"Y. Kumar\"},{\"authorId\":\"70155812\",\"name\":\"Sumit Sarin\"},{\"authorId\":\"52111730\",\"name\":\"Payman Vafaee\"},{\"authorId\":\"38730526\",\"name\":\"Mika Hama\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2e6c656f00192133b183df7fa28f3be374fbe20\",\"title\":\"Multi-modal Automated Speech Scoring using Attention Fusion\",\"url\":\"https://www.semanticscholar.org/paper/c2e6c656f00192133b183df7fa28f3be374fbe20\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6022603\",\"name\":\"Shichuan Zhang\"},{\"authorId\":\"1387822299\",\"name\":\"Zengming Tang\"},{\"authorId\":\"49349259\",\"name\":\"H. Pan\"},{\"authorId\":\"151499994\",\"name\":\"Xinyu Wei\"},{\"authorId\":\"101573196\",\"name\":\"Jun Huang\"}],\"doi\":\"10.1145/3343031.3356074\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed43a7ab9a72c60cc37f69e3e22a52405a0e31ba\",\"title\":\"A Hierarchical Framwork with Improved Loss for Large-scale Multi-modal Video Identification\",\"url\":\"https://www.semanticscholar.org/paper/ed43a7ab9a72c60cc37f69e3e22a52405a0e31ba\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1905.02963\",\"authors\":[{\"authorId\":\"145114776\",\"name\":\"L. Sun\"},{\"authorId\":\"143721383\",\"name\":\"Bing Li\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":\"10.1109/ICME.2019.00226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"title\":\"Multimodal Semantic Attention Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.1016/j.csl.2020.101095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"661f04ecc734ced906e16980a6143c814ce085ed\",\"title\":\"Hierarchical multimodal attention for end-to-end audio-visual scene-aware dialogue response generation\",\"url\":\"https://www.semanticscholar.org/paper/661f04ecc734ced906e16980a6143c814ce085ed\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"96066534\",\"name\":\"P. Joshi\"},{\"authorId\":\"51497543\",\"name\":\"Chitwan Saharia\"},{\"authorId\":\"1557378944\",\"name\":\"V. Singh\"},{\"authorId\":\"51267359\",\"name\":\"Digvijaysingh Gautam\"},{\"authorId\":\"145799547\",\"name\":\"Ganesh Ramakrishnan\"},{\"authorId\":\"1557645545\",\"name\":\"P. Jyothi\"}],\"doi\":\"10.1109/ICCVW.2019.00459\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"title\":\"A Tale of Two Modalities for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31534381\",\"name\":\"M. Fortin\"},{\"authorId\":\"1399443272\",\"name\":\"B. Chaib-draa\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e858a6e55bbb86b7ab4360455b728a7466f891bc\",\"title\":\"1 Problem Formulation and Overview of the Proposed Framework\",\"url\":\"https://www.semanticscholar.org/paper/e858a6e55bbb86b7ab4360455b728a7466f891bc\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145829609\",\"name\":\"Hung T. Le\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd6a2c7bbb54ad5c1350eb02718211fe86e125e5\",\"title\":\"End-to-End Multimodal Dialog Systems with Hierarchical Multimodal Attention on Video Features\",\"url\":\"https://www.semanticscholar.org/paper/fd6a2c7bbb54ad5c1350eb02718211fe86e125e5\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3239379\",\"name\":\"J. Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1007/978-3-030-05710-7_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"title\":\"Hierarchical Vision-Language Alignment for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":\"1911.06394\",\"authors\":[{\"authorId\":\"2047181\",\"name\":\"Seokhwan Kim\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"66161659\",\"name\":\"Chulaka Gunasekara\"},{\"authorId\":\"48601642\",\"name\":\"S. Lee\"},{\"authorId\":\"144179710\",\"name\":\"A. Atkinson\"},{\"authorId\":\"1780690\",\"name\":\"Baolin Peng\"},{\"authorId\":\"1944614\",\"name\":\"Hannes Schulz\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"2887412\",\"name\":\"Jin-chao Li\"},{\"authorId\":\"51172009\",\"name\":\"Mahmoud Adada\"},{\"authorId\":\"1730108\",\"name\":\"Minlie Huang\"},{\"authorId\":\"1753717\",\"name\":\"Luis Lastras\"},{\"authorId\":\"1727211\",\"name\":\"Jonathan K. Kummerfeld\"},{\"authorId\":\"2598433\",\"name\":\"Walter S. Lasecki\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"2188497\",\"name\":\"Abhinav Rastogi\"},{\"authorId\":\"9761407\",\"name\":\"Xiaoxue Zang\"},{\"authorId\":\"31801337\",\"name\":\"Srinivas Sunkara\"},{\"authorId\":\"47495799\",\"name\":\"Raghav Gupta\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b3b9a9de1ff0d36e66b41c2f5942cca8116efe95\",\"title\":\"The Eighth Dialog System Technology Challenge\",\"url\":\"https://www.semanticscholar.org/paper/b3b9a9de1ff0d36e66b41c2f5942cca8116efe95\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49685502\",\"name\":\"J. Lee\"},{\"authorId\":\"1769295\",\"name\":\"Junmo Kim\"}],\"doi\":\"10.1109/ICCE-ASIA.2018.8552140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"22409d9471b426e0bcac3f850aa16ad158b355a7\",\"title\":\"Improving Video Captioning with Non-Local Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/22409d9471b426e0bcac3f850aa16ad158b355a7\",\"venue\":\"2018 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia)\",\"year\":2018},{\"arxivId\":\"1905.13448\",\"authors\":[{\"authorId\":\"153030413\",\"name\":\"Xuenan Xu\"},{\"authorId\":\"2451839\",\"name\":\"H. Dinkel\"},{\"authorId\":\"3000684\",\"name\":\"Mengyue Wu\"},{\"authorId\":\"143819768\",\"name\":\"K. Yu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f3f733d760a9b5ae8453874571765e9e17defdb\",\"title\":\"Audio Caption in a Car Setting with a Sentence-Level Loss.\",\"url\":\"https://www.semanticscholar.org/paper/9f3f733d760a9b5ae8453874571765e9e17defdb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b02dba59a087f16d8286aec5e6481d5952a37df5\",\"title\":\"CMU Sinbad\\u2019s Submission for the DSTC7 AVSD Challenge\",\"url\":\"https://www.semanticscholar.org/paper/b02dba59a087f16d8286aec5e6481d5952a37df5\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1403865587\",\"name\":\"Aske R. Lejb\\u00f8lle\"},{\"authorId\":\"1803459\",\"name\":\"Kamal Nasrollahi\"},{\"authorId\":\"26910589\",\"name\":\"B. Krogh\"},{\"authorId\":\"1700569\",\"name\":\"T. Moeslund\"}],\"doi\":\"10.1109/TIFS.2019.2938870\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2722014e9455965025147d3b9669a65727cd2d55\",\"title\":\"Person Re-Identification Using Spatial and Layer-Wise Attention\",\"url\":\"https://www.semanticscholar.org/paper/2722014e9455965025147d3b9669a65727cd2d55\",\"venue\":\"IEEE Transactions on Information Forensics and Security\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102523405\",\"name\":\"J. Lee\"},{\"authorId\":\"1390565629\",\"name\":\"Yekang Lee\"},{\"authorId\":\"1911697\",\"name\":\"Sihyeon Seong\"},{\"authorId\":\"97531942\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"153275028\",\"name\":\"Sungjin Kim\"},{\"authorId\":\"1769295\",\"name\":\"Junmo Kim\"}],\"doi\":\"10.1109/ICIP.2019.8803143\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"67a85632e96bbeb0748100d9a570d06e75b0e99b\",\"title\":\"Capturing Long-Range Dependencies in Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/67a85632e96bbeb0748100d9a570d06e75b0e99b\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":\"1912.10131\",\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8e36c89ab4339079dd9871895693410eab09423\",\"title\":\"Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/b8e36c89ab4339079dd9871895693410eab09423\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TCSVT.2018.2867286\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"title\":\"Dual-Stream Recurrent Neural Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1557331957\",\"name\":\"Bairong Zhuang\"},{\"authorId\":\"40007645\",\"name\":\"Wenbo Wang\"},{\"authorId\":\"49018339\",\"name\":\"T. Shinozaki\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"87ef8177df87849287b0e968b77f6f00f1d3cec7\",\"title\":\"Investigation of Attention-Based Multimodal Fusion and Maximum Mutual Information Objective for DSTC7 Track3\",\"url\":\"https://www.semanticscholar.org/paper/87ef8177df87849287b0e968b77f6f00f1d3cec7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tino Fuhrmann\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"845313c7e1949837770f3cf632cf18f71bccb530\",\"title\":\"Using Scene-Aware Voice Dialogs in Human-Drone Interaction\",\"url\":\"https://www.semanticscholar.org/paper/845313c7e1949837770f3cf632cf18f71bccb530\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152230242\",\"name\":\"Feng Xing\"},{\"authorId\":\"1784002\",\"name\":\"C. Guo\"}],\"doi\":\"10.1109/BigDataCongress.2019.00016\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b69f86bf6e2840cc9eeffb4a874a618b84762fcd\",\"title\":\"Mining Semantic Information in Rumor Detection via a Deep Visual Perception Based Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b69f86bf6e2840cc9eeffb4a874a618b84762fcd\",\"venue\":\"2019 IEEE International Congress on Big Data (BigDataCongress)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48521241\",\"name\":\"Ye Ma\"},{\"authorId\":\"1918283\",\"name\":\"Xinxing Li\"},{\"authorId\":\"9450377\",\"name\":\"Mingxing Xu\"},{\"authorId\":\"144202060\",\"name\":\"Jia Jia\"},{\"authorId\":\"7239047\",\"name\":\"Lianhong Cai\"}],\"doi\":\"10.1145/3123266.3123408\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a46f1d0019e43effc52e2b85fa91ece8ef93eba\",\"title\":\"Multi-scale Context Based Attention for Dynamic Music Emotion Prediction\",\"url\":\"https://www.semanticscholar.org/paper/7a46f1d0019e43effc52e2b85fa91ece8ef93eba\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3863922\",\"name\":\"C. Yan\"},{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"48631703\",\"name\":\"Xingzheng Wang\"},{\"authorId\":\"5094646\",\"name\":\"Y. Zhang\"},{\"authorId\":\"145922541\",\"name\":\"Xinhong Hao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144954808\",\"name\":\"Q. Dai\"}],\"doi\":\"10.1109/TMM.2019.2924576\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d\",\"title\":\"STAT: Spatial-Temporal Attention Mechanism for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"}],\"doi\":\"10.1016/j.csl.2020.101093\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9094fc5d46fe4b81c9b5157b5768ed8e0c955d0d\",\"title\":\"Transfer learning for multimodal dialog\",\"url\":\"https://www.semanticscholar.org/paper/9094fc5d46fe4b81c9b5157b5768ed8e0c955d0d\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"}],\"doi\":\"10.1109/ICCVW.2019.00536\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"title\":\"EgoVQA - An Egocentric Video Question Answering Benchmark Dataset\",\"url\":\"https://www.semanticscholar.org/paper/71f3a2632d924f29ca6eb2e789f8ff6d46250c82\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1965909970\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"1755773\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930562\",\"name\":\"G. Chen\"},{\"authorId\":\"153016830\",\"name\":\"J. Guo\"}],\"doi\":\"10.1109/ACCESS.2020.3021857\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"title\":\"Understanding Objects in Video: Object-Oriented Video Captioning via Structured Trajectory and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":\"10.1016/j.csl.2020.101102\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a00a2b6eb505a172a36de81dc803a9d45597bc8a\",\"title\":\"Investigating topics, audio representations and attention for multimodal scene-aware dialog\",\"url\":\"https://www.semanticscholar.org/paper/a00a2b6eb505a172a36de81dc803a9d45597bc8a\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"7549094\",\"name\":\"Yuze Zhang\"},{\"authorId\":\"144680113\",\"name\":\"Yi Pan\"},{\"authorId\":\"50079594\",\"name\":\"Xiaoyue Li\"},{\"authorId\":\"145697902\",\"name\":\"C. Zhang\"},{\"authorId\":\"50007086\",\"name\":\"Rong Yuan\"},{\"authorId\":null,\"name\":\"Di Wu\"},{\"authorId\":\"47824876\",\"name\":\"W. Wang\"},{\"authorId\":\"145525190\",\"name\":\"J. Pei\"},{\"authorId\":\"1748032\",\"name\":\"Heng Huang\"}],\"doi\":\"10.1145/3292500.3330662\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d8272d3ee319cf205ec96597f7a828eecde1da8\",\"title\":\"Multi-Horizon Time Series Forecasting with Temporal Attention Learning\",\"url\":\"https://www.semanticscholar.org/paper/5d8272d3ee319cf205ec96597f7a828eecde1da8\",\"venue\":\"KDD\",\"year\":2019},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1906.04375\",\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/CVPR.2019.00852\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"title\":\"Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8ad1c0d40b416ac52522d3cf110f574b71c0db6\",\"title\":\"DSTC 7-AVSD : Scene-Aware Video-Dialogue Systems with Dual Attention\",\"url\":\"https://www.semanticscholar.org/paper/c8ad1c0d40b416ac52522d3cf110f574b71c0db6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1910.00628\",\"authors\":[{\"authorId\":\"51036510\",\"name\":\"A. Narayanan\"},{\"authorId\":\"71124982\",\"name\":\"Avinash Siravuru\"},{\"authorId\":\"2086607\",\"name\":\"B. Dariush\"}],\"doi\":\"10.1109/LRA.2020.2967738\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"997a31a9a32a1769adccbf4a5f77d69ac29fe229\",\"title\":\"Gated Recurrent Fusion to Learn Driving Behavior from Temporal Multimodal Data\",\"url\":\"https://www.semanticscholar.org/paper/997a31a9a32a1769adccbf4a5f77d69ac29fe229\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":\"2011.00192\",\"authors\":[{\"authorId\":\"66442354\",\"name\":\"Haochen Liu\"},{\"authorId\":\"14631164\",\"name\":\"Zhiwei Liu\"},{\"authorId\":\"4574975\",\"name\":\"Zhongqin Wu\"},{\"authorId\":\"1736632\",\"name\":\"Jiliang Tang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6a8fca8e7139b2c73233ad812c09e7f74830d3b6\",\"title\":\"Personalized Multimodal Feedback Generation in Education\",\"url\":\"https://www.semanticscholar.org/paper/6a8fca8e7139b2c73233ad812c09e7f74830d3b6\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145082091\",\"name\":\"Jianbo Zheng\"},{\"authorId\":\"1747615\",\"name\":\"Teng-Yok Lee\"},{\"authorId\":\"48931599\",\"name\":\"Chen Feng\"},{\"authorId\":\"113778242\",\"name\":\"Xiaohua Lit\"},{\"authorId\":\"7969330\",\"name\":\"Ziming Zhang\"}],\"doi\":\"10.1109/ICPR.2018.8545607\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d399b86d4c67472fd65e9466359a43ef2f914f97\",\"title\":\"Robust Attentional Pooling via Feature Selection\",\"url\":\"https://www.semanticscholar.org/paper/d399b86d4c67472fd65e9466359a43ef2f914f97\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":\"1911.09989\",\"authors\":[{\"authorId\":\"1429191721\",\"name\":\"Menatallh Hammad\"},{\"authorId\":\"1429191719\",\"name\":\"May Hammad\"},{\"authorId\":\"31358369\",\"name\":\"M. ElShenawy\"}],\"doi\":\"10.1007/978-3-030-59830-3_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"title\":\"Characterizing the impact of using features extracted from pre-trained models on the quality of video captioning sequence-to-sequence models\",\"url\":\"https://www.semanticscholar.org/paper/86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"venue\":\"ICPRAI\",\"year\":2020},{\"arxivId\":\"2012.11866\",\"authors\":[{\"authorId\":\"2595189\",\"name\":\"Zehua Sun\"},{\"authorId\":\"120809631\",\"name\":\"Jiwang Liu\"},{\"authorId\":\"143969578\",\"name\":\"Qiuhong Ke\"},{\"authorId\":\"1877377\",\"name\":\"H. Rahmani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"816236bf3219363bfe4b847363e137b1fe6712e7\",\"title\":\"Human Action Recognition from Various Data Modalities: A Review\",\"url\":\"https://www.semanticscholar.org/paper/816236bf3219363bfe4b847363e137b1fe6712e7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46207379\",\"name\":\"Yu Zhang\"},{\"authorId\":\"1774721\",\"name\":\"Cong Phuoc Huynh\"},{\"authorId\":\"1684869\",\"name\":\"K. N. Ngan\"}],\"doi\":\"10.1109/TGRS.2019.2908679\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c0aa60d151eeb031895b05bad4a66de1337cfc7b\",\"title\":\"Feature Fusion With Predictive Weighting for Spectral Image Classification and Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/c0aa60d151eeb031895b05bad4a66de1337cfc7b\",\"venue\":\"IEEE Transactions on Geoscience and Remote Sensing\",\"year\":2019},{\"arxivId\":\"1806.00525\",\"authors\":[{\"authorId\":\"2809915\",\"name\":\"H. AlAmri\"},{\"authorId\":\"51002409\",\"name\":\"Vincent Cartillier\"},{\"authorId\":\"143826364\",\"name\":\"Raphael Gontijo Lopes\"},{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"48094509\",\"name\":\"J. Wang\"},{\"authorId\":\"21472040\",\"name\":\"Irfan Essa\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e7240d11872af602aabb103c4f2f307006250a0f\",\"title\":\"Audio Visual Scene-Aware Dialog (AVSD) Challenge at DSTC7\",\"url\":\"https://www.semanticscholar.org/paper/e7240d11872af602aabb103c4f2f307006250a0f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"}],\"doi\":\"10.21437/interspeech.2019-3143\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f78c136471778771c29fb385d3a8c1a1def28de1\",\"title\":\"Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/f78c136471778771c29fb385d3a8c1a1def28de1\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98831710\",\"name\":\"B. Yang\"},{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"86010b6cb557103eda7e28fa2b497c9a9697fa8d\",\"title\":\"Non-Autoregressive Video Captioning with Iterative Refinement\",\"url\":\"https://www.semanticscholar.org/paper/86010b6cb557103eda7e28fa2b497c9a9697fa8d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49868702\",\"name\":\"Ran Wei\"},{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1016/j.jvcir.2020.102751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"title\":\"Exploiting the local temporal information for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"2007.08076\",\"authors\":[{\"authorId\":\"51123172\",\"name\":\"Darshana Priyasad\"},{\"authorId\":\"34735743\",\"name\":\"T. Fernando\"},{\"authorId\":\"1980700\",\"name\":\"Simon Denman\"},{\"authorId\":\"1729760\",\"name\":\"S. Sridharan\"},{\"authorId\":\"3140440\",\"name\":\"C. Fookes\"}],\"doi\":\"10.1016/j.inffus.2020.10.005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"08189ce7ec387a47b490113a4040dec6f65a254e\",\"title\":\"Memory based fusion for multi-modal deep learning.\",\"url\":\"https://www.semanticscholar.org/paper/08189ce7ec387a47b490113a4040dec6f65a254e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"39024a168ea1511821e5af17bdf838bf4afb3db8\",\"title\":\"FDU Participation in TRECVID 2019 VTT Task\",\"url\":\"https://www.semanticscholar.org/paper/39024a168ea1511821e5af17bdf838bf4afb3db8\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2003.01607\",\"authors\":[{\"authorId\":\"3207112\",\"name\":\"A. Reiter\"},{\"authorId\":\"51502783\",\"name\":\"Menglin Jia\"},{\"authorId\":\"48220341\",\"name\":\"Pu Yang\"},{\"authorId\":\"38760573\",\"name\":\"Ser-Nam Lim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"051ad18077e955ccac22f0d7d33a6f993d3ca5e2\",\"title\":\"Deep Multi-Modal Sets\",\"url\":\"https://www.semanticscholar.org/paper/051ad18077e955ccac22f0d7d33a6f993d3ca5e2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26339425\",\"name\":\"Valentin Vielzeuf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0501b8a99270a20c7536ed2f6df6569413810f6d\",\"title\":\"Apprentissage neuronal profond pour l'analyse de contenus multimodaux et temporels. (Deep learning for multimodal and temporal contents analysis)\",\"url\":\"https://www.semanticscholar.org/paper/0501b8a99270a20c7536ed2f6df6569413810f6d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2003.03501\",\"authors\":[{\"authorId\":\"3436466\",\"name\":\"Palash Goyal\"},{\"authorId\":\"144756035\",\"name\":\"Saurabh Sahu\"},{\"authorId\":\"46848045\",\"name\":\"S. Ghosh\"},{\"authorId\":\"117523938\",\"name\":\"C. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0b9fad2013d5ef8f4f64ce3e99f9b355f87a83c\",\"title\":\"Cross-modal Learning for Multi-modal Video Categorization\",\"url\":\"https://www.semanticscholar.org/paper/f0b9fad2013d5ef8f4f64ce3e99f9b355f87a83c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1813915\",\"name\":\"S. Liu\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3240508.3240667\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"849642b4701ac11c035326069f707f23a51a6f1a\",\"title\":\"SibNet: Sibling Convolutional Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/849642b4701ac11c035326069f707f23a51a6f1a\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1887997\",\"name\":\"Yin-wei Wei\"},{\"authorId\":\"98285513\",\"name\":\"Xiang Wang\"},{\"authorId\":\"15880069\",\"name\":\"W. Guan\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"33383055\",\"name\":\"Zhouchen Lin\"},{\"authorId\":\"1748939\",\"name\":\"B. Chen\"}],\"doi\":\"10.1109/TIP.2019.2923608\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ae81e7ef50d01a076ef7967653ae006cdc825638\",\"title\":\"Neural Multimodal Cooperative Learning Toward Micro-Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/ae81e7ef50d01a076ef7967653ae006cdc825638\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1911.05186\",\"authors\":[{\"authorId\":\"7878341\",\"name\":\"Wubo Li\"},{\"authorId\":\"9276071\",\"name\":\"Wei Zou\"},{\"authorId\":\"1898780\",\"name\":\"Xiangang Li\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"962ec0fc31498001bdd011effb4ba73621dc0a8a\",\"title\":\"TCT: A Cross-supervised Learning Method for Multimodal Sequence Representation\",\"url\":\"https://www.semanticscholar.org/paper/962ec0fc31498001bdd011effb4ba73621dc0a8a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40055538\",\"name\":\"C. Yang\"},{\"authorId\":\"1519971304\",\"name\":\"Xiaochan Wang\"},{\"authorId\":\"145942580\",\"name\":\"B. Jiang\"}],\"doi\":\"10.1109/ACCESS.2020.2989473\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7efb2bc1d440320de5c38b3c3b1529a93b4dca90\",\"title\":\"Sentiment Enhanced Multi-Modal Hashtag Recommendation for Micro-Videos\",\"url\":\"https://www.semanticscholar.org/paper/7efb2bc1d440320de5c38b3c3b1529a93b4dca90\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51036510\",\"name\":\"Athma Narayanan\"},{\"authorId\":\"145608726\",\"name\":\"Avinash Siravuru\"},{\"authorId\":\"2086607\",\"name\":\"Behzad Dariush\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6f3e3703491e8e193dbe24b0dd832767c0a9fada\",\"title\":\"Temporal Multimodal Fusion for Driver Behavior Prediction Tasks using Gated Recurrent Fusion Units\",\"url\":\"https://www.semanticscholar.org/paper/6f3e3703491e8e193dbe24b0dd832767c0a9fada\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1016/j.neucom.2019.08.042\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"00b350e4211dd5ed4791744920e664880cd3fd3a\",\"title\":\"Recurrent convolutional video captioning with global and local attention\",\"url\":\"https://www.semanticscholar.org/paper/00b350e4211dd5ed4791744920e664880cd3fd3a\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100576986\",\"name\":\"Yue Liu\"},{\"authorId\":\"122024145\",\"name\":\"Xin Wang\"},{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3350986\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bd195e11cb554cb5f3a7ac6aab8111d03cb46c6\",\"title\":\"Cross-Modal Dual Learning for Sentence-to-Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/3bd195e11cb554cb5f3a7ac6aab8111d03cb46c6\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31469067\",\"name\":\"Jia-Ming Wang\"},{\"authorId\":\"153140559\",\"name\":\"Jun Du\"},{\"authorId\":\"47539230\",\"name\":\"Jian-Shu Zhang\"},{\"authorId\":\"50219146\",\"name\":\"Zi-Rui Wang\"}],\"doi\":\"10.1109/ICDAR.2019.00191\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8231c3a1bfbb2ef309ba7b41abab9b8d6306bba\",\"title\":\"Multi-modal Attention Network for Handwritten Mathematical Expression Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f8231c3a1bfbb2ef309ba7b41abab9b8d6306bba\",\"venue\":\"2019 International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2019},{\"arxivId\":\"1907.01166\",\"authors\":[{\"authorId\":\"143725625\",\"name\":\"Hung Le\"},{\"authorId\":\"36187119\",\"name\":\"Doyen Sahoo\"},{\"authorId\":\"2185019\",\"name\":\"Nancy F. Chen\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/P19-1564\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"594ad264d6b92afb9d13cb56ad8ffadba94a9f7a\",\"title\":\"Multimodal Transformer Networks for End-to-End Video-Grounded Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/594ad264d6b92afb9d13cb56ad8ffadba94a9f7a\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398869664\",\"name\":\"Abdulrahman Al-Molegi\"},{\"authorId\":\"3447134\",\"name\":\"Mohammed Jabreel\"},{\"authorId\":\"1398869662\",\"name\":\"A. Mart\\u00ednez-Ballest\\u00e9\"}],\"doi\":\"10.1016/j.patrec.2018.05.015\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6579c0298d055f1c09be99c7329727d37f09556\",\"title\":\"Move, Attend and Predict: An attention-based neural model for people's movement prediction\",\"url\":\"https://www.semanticscholar.org/paper/c6579c0298d055f1c09be99c7329727d37f09556\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":\"2005.00192\",\"authors\":[{\"authorId\":\"2065332\",\"name\":\"H. Lee\"},{\"authorId\":\"144517919\",\"name\":\"Seunghyun Yoon\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"153586399\",\"name\":\"Doo Soon Kim\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"27582486\",\"name\":\"Joongbo Shin\"},{\"authorId\":\"1731707\",\"name\":\"K. Jung\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb9332fab74f30ce4d4bc2a12fa67fcd658fd460\",\"title\":\"KPQA: A Metric for Generative Question Answering Using Keyphrase Weights.\",\"url\":\"https://www.semanticscholar.org/paper/eb9332fab74f30ce4d4bc2a12fa67fcd658fd460\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405511901\",\"name\":\"Luis Fernando D'Haro\"},{\"authorId\":\"2237192\",\"name\":\"Koichiro Yoshino\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1725498\",\"name\":\"L. Polymenakos\"},{\"authorId\":\"1727211\",\"name\":\"Jonathan K. Kummerfeld\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"71886367\",\"name\":\"Xiang Gao\"}],\"doi\":\"10.1016/j.csl.2020.101068\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2da92688aef91371424f0e3ca5482c9f8dbe67b7\",\"title\":\"Overview of the seventh Dialog System Technology Challenge: DSTC7\",\"url\":\"https://www.semanticscholar.org/paper/2da92688aef91371424f0e3ca5482c9f8dbe67b7\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886528\",\"name\":\"Guolong Wang\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"},{\"authorId\":\"2168639\",\"name\":\"Kaiping Xu\"},{\"authorId\":\"145489794\",\"name\":\"K. Huang\"},{\"authorId\":\"19204816\",\"name\":\"Shuxiong Ye\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"title\":\"Bridge Video and Text with Cascade Syntactic Structure\",\"url\":\"https://www.semanticscholar.org/paper/b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":\"1903.00755\",\"authors\":[{\"authorId\":\"7969330\",\"name\":\"Ziming Zhang\"},{\"authorId\":\"35544303\",\"name\":\"Anil Kag\"},{\"authorId\":\"143972875\",\"name\":\"Alan Sullivan\"},{\"authorId\":\"1699322\",\"name\":\"Venkatesh Saligrama\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bcc1454eb13ec9179bb28b28faef100987bfadf1\",\"title\":\"Equilibrated Recurrent Neural Network: Neuronal Time-Delayed Self-Feedback Improves Accuracy and Stability\",\"url\":\"https://www.semanticscholar.org/paper/bcc1454eb13ec9179bb28b28faef100987bfadf1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1911.00212\",\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.18653/v1/D19-1207\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"55f546209c01530a7717d4170aa24947c6b92775\",\"title\":\"Low-Rank HOCA: Efficient High-Order Cross-Modal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55f546209c01530a7717d4170aa24947c6b92775\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"12257085\",\"name\":\"Pang-Chi Huang\"},{\"authorId\":\"3596543\",\"name\":\"Chi-Yu Yang\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ASRU.2017.8268961\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"title\":\"Seeing and hearing too: Audio representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1993562639\",\"name\":\"Saurabh Sahu\"},{\"authorId\":\"3436466\",\"name\":\"Palash Goyal\"},{\"authorId\":\"46848045\",\"name\":\"S. Ghosh\"},{\"authorId\":\"1699113\",\"name\":\"C. Lee\"}],\"doi\":\"10.1145/3394171.3413756\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71d117718ebfd5dcde01ed844debc9d33d03e8c9\",\"title\":\"Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models\",\"url\":\"https://www.semanticscholar.org/paper/71d117718ebfd5dcde01ed844debc9d33d03e8c9\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31534381\",\"name\":\"M. Fortin\"},{\"authorId\":\"1399443272\",\"name\":\"B. Chaib-draa\"}],\"doi\":\"10.5220/0007313503680376\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6abb74f2bc74b704df86c3d1ca7837d71dddb3f8\",\"title\":\"Multimodal Sentiment Analysis: A Multitask Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/6abb74f2bc74b704df86c3d1ca7837d71dddb3f8\",\"venue\":\"ICPRAM\",\"year\":2019},{\"arxivId\":\"2101.00359\",\"authors\":[{\"authorId\":null,\"name\":\"Mingjian Zhu\"},{\"authorId\":null,\"name\":\"Chenrui Duan\"},{\"authorId\":\"1409820051\",\"name\":\"Changbin Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"title\":\"Video Captioning in Compressed Video\",\"url\":\"https://www.semanticscholar.org/paper/93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"1812.08407\",\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"51011510\",\"name\":\"Juan Jose Alvarado Leanos\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"title\":\"Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"50080046\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/TIP.2019.2916757\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"title\":\"CAM-RNN: Co-Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"1910.02930\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/K19-1039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"title\":\"A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018191\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"title\":\"Motion Guided Spatial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.08041\",\"authors\":[{\"authorId\":\"2879849\",\"name\":\"Ruizhi Li\"},{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"144921407\",\"name\":\"Sri Harish Mallidi\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1738798\",\"name\":\"H. Hermansky\"}],\"doi\":\"10.1109/TASLP.2019.2959721\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"84f2cfbc142ad3165ea3bcacd189a3d1110660e0\",\"title\":\"Multi-Stream End-to-End Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/84f2cfbc142ad3165ea3bcacd189a3d1110660e0\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144952644\",\"name\":\"D. Y. Choi\"},{\"authorId\":\"1687433\",\"name\":\"Deok-Hwan Kim\"},{\"authorId\":\"10774886\",\"name\":\"B. Song\"}],\"doi\":\"10.1109/ACCESS.2020.3036877\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"29af086d2a9e80b5528c55e6e99c63364a7281e1\",\"title\":\"Multimodal Attention Network for Continuous-Time Emotion Recognition Using Video and EEG Signals\",\"url\":\"https://www.semanticscholar.org/paper/29af086d2a9e80b5528c55e6e99c63364a7281e1\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2004.02205\",\"authors\":[{\"authorId\":\"145878079\",\"name\":\"Vivek Sharma\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"49157259\",\"name\":\"R. Stiefelhagen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e908719ae2a09e3726300df65bcd31dfddea5a86\",\"title\":\"Deep Multimodal Feature Encoding for Video Ordering\",\"url\":\"https://www.semanticscholar.org/paper/e908719ae2a09e3726300df65bcd31dfddea5a86\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144364295\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"title\":\"TVT: Two-View Transformer Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"venue\":\"ACML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48071615\",\"name\":\"Huda Alamri\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1606364265\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa6c59f0cab5f8dcc899a356d364ce51626536a8\",\"title\":\"Audio Visual Scene-aware dialog (AVSD) Track for Natural Language Generation in DSTC7\",\"url\":\"https://www.semanticscholar.org/paper/aa6c59f0cab5f8dcc899a356d364ce51626536a8\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1811.04897\",\"authors\":[{\"authorId\":\"2879849\",\"name\":\"Ruizhi Li\"},{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"1865337\",\"name\":\"Sri Harish Reddy Mallidi\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"1738798\",\"name\":\"H. Hermansky\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f17e182fcb7fbbff2257824174ed6f7df512a42b\",\"title\":\"Multi-encoder multi-resolution framework for end-to-end speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/f17e182fcb7fbbff2257824174ed6f7df512a42b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"152578624\",\"name\":\"C. Peters\"}],\"doi\":\"10.1109/RO-MAN46459.2019.8956425\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb988f0a9995d95c556694a5e6dbaa8f2d25370f\",\"title\":\"AppGAN: Generative Adversarial Networks for Generating Robot Approach Behaviors into Small Groups of People\",\"url\":\"https://www.semanticscholar.org/paper/bb988f0a9995d95c556694a5e6dbaa8f2d25370f\",\"venue\":\"2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)\",\"year\":2019},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.09833\",\"authors\":[{\"authorId\":\"94281814\",\"name\":\"Fa-Ting Hong\"},{\"authorId\":\"1823519002\",\"name\":\"Xuanteng Huang\"},{\"authorId\":\"50135134\",\"name\":\"Weihong Li\"},{\"authorId\":\"3333315\",\"name\":\"W. Zheng\"}],\"doi\":\"10.1007/978-3-030-58601-0_21\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4079558004efd97ddb20ea160909f7fa97d689c2\",\"title\":\"MINI-Net: Multiple Instance Ranking Network for Video Highlight Detection\",\"url\":\"https://www.semanticscholar.org/paper/4079558004efd97ddb20ea160909f7fa97d689c2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"153489843\",\"name\":\"T. Chen\"},{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"145909567\",\"name\":\"Wanli Yu\"}],\"doi\":\"10.1109/ACCESS.2020.3010872\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"title\":\"Attention-Based Convolutional LSTM for Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"title\":\"Image Input OR Video Hierarchical LSTMs with Adaptive Attention ( hLSTMat ) Feature Extraction Generated Captions Losses\",\"url\":\"https://www.semanticscholar.org/paper/e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1704.04613\",\"authors\":[{\"authorId\":\"145905113\",\"name\":\"X. Bai\"},{\"authorId\":\"2181925\",\"name\":\"Mingkun Yang\"},{\"authorId\":\"10344582\",\"name\":\"Pengyuan Lyu\"},{\"authorId\":\"9510649\",\"name\":\"Yongchao Xu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/ACCESS.2018.2878899\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4285cd81d5d5c91f6322bade859364827c404f21\",\"title\":\"Integrating Scene Text and Visual Appearance for Fine-Grained Image Classification\",\"url\":\"https://www.semanticscholar.org/paper/4285cd81d5d5c91f6322bade859364827c404f21\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1807.03658\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"}],\"doi\":\"10.1016/j.neucom.2020.08.035\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"title\":\"Video Captioning with Boundary-aware Hierarchical Language Decoding and Joint Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"2011768695\",\"name\":\"Junjun Guo\"},{\"authorId\":\"2409659\",\"name\":\"Shengxiang Gao\"},{\"authorId\":\"121854326\",\"name\":\"Zhengtao Yu\"}],\"doi\":\"10.1016/j.patcog.2020.107702\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6686fadf7f7ef2283cc9286095db281f8520ec04\",\"title\":\"Enhancing the alignment between target words and corresponding frames for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/6686fadf7f7ef2283cc9286095db281f8520ec04\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50251712\",\"name\":\"Ziqi Zhou\"},{\"authorId\":\"1387835192\",\"name\":\"Xinna Guo\"},{\"authorId\":\"3183928\",\"name\":\"W. Yang\"},{\"authorId\":\"2475959\",\"name\":\"Y. Shi\"},{\"authorId\":\"6578587\",\"name\":\"L. Zhou\"},{\"authorId\":\"1716548\",\"name\":\"L. Wang\"},{\"authorId\":\"152790163\",\"name\":\"M. Yang\"}],\"doi\":\"10.1007/978-3-030-32692-0_69\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b801e798302ae970f0645138e19932f2dd83c810\",\"title\":\"Cross-Modal Attention-Guided Convolutional Network for Multi-modal Cardiac Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/b801e798302ae970f0645138e19932f2dd83c810\",\"venue\":\"MLMI@MICCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22423271\",\"name\":\"Mingyang Geng\"},{\"authorId\":\"2400793\",\"name\":\"Shuqi Liu\"},{\"authorId\":\"40503508\",\"name\":\"Z. Wu\"}],\"doi\":\"10.3390/s19040823\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e93ec7595cb9fb3b239f51429b33430ea987127a\",\"title\":\"Sensor Fusion-Based Cooperative Trail Following for Autonomous Multi-Robot System \\u2020\",\"url\":\"https://www.semanticscholar.org/paper/e93ec7595cb9fb3b239f51429b33430ea987127a\",\"venue\":\"Sensors\",\"year\":2019},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1816785\",\"name\":\"G. Wichern\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":\"1747615\",\"name\":\"Teng-Yok Lee\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e561b60c6aea937f9d98ee336dde01abd1ff651\",\"title\":\"Multimodal Attention for Fusion of Audio and Spatiotemporal Features for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/8e561b60c6aea937f9d98ee336dde01abd1ff651\",\"venue\":\"CVPR Workshops\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"40574775\",\"name\":\"Sruthi Gorantla\"},{\"authorId\":\"1382302885\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"}],\"doi\":\"10.1109/MIPR.2018.00043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4aa5191088edafc2d3ae6232d9db4145d0099529\",\"title\":\"Self-Attentive Feature-Level Fusion for Multimodal Emotion Detection\",\"url\":\"https://www.semanticscholar.org/paper/4aa5191088edafc2d3ae6232d9db4145d0099529\",\"venue\":\"2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451396\",\"name\":\"Nikos Papasarantopoulos\"},{\"authorId\":\"2875615\",\"name\":\"Lea Frermann\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"},{\"authorId\":\"40146204\",\"name\":\"Shay B. Cohen\"}],\"doi\":\"10.18653/v1/D19-1212\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"187e69900484453fda35d853cdc8c5a298ecbd24\",\"title\":\"Partners in Crime: Multi-view Sequential Inference for Movie Understanding\",\"url\":\"https://www.semanticscholar.org/paper/187e69900484453fda35d853cdc8c5a298ecbd24\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"50678151\",\"name\":\"Bingtao Liu\"},{\"authorId\":\"7590116\",\"name\":\"C. Yan\"}],\"doi\":\"10.1145/3123266.3123354\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"title\":\"Video Description with Spatial-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1905.13448\",\"authors\":[{\"authorId\":\"153030413\",\"name\":\"Xuenan Xu\"},{\"authorId\":\"2451839\",\"name\":\"H. Dinkel\"},{\"authorId\":\"3000684\",\"name\":\"Mengyue Wu\"},{\"authorId\":\"1736727\",\"name\":\"Kai Yu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"346ed15dca43e302f51571bdb1349380604d1ff8\",\"title\":\"What does a Car-ssette tape tell?\",\"url\":\"https://www.semanticscholar.org/paper/346ed15dca43e302f51571bdb1349380604d1ff8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1811.12500\",\"authors\":[{\"authorId\":\"3449409\",\"name\":\"Tiehang Duan\"},{\"authorId\":\"145553880\",\"name\":\"Qi Lou\"},{\"authorId\":\"1696384\",\"name\":\"S. Srihari\"},{\"authorId\":\"2834988\",\"name\":\"X. Xie\"}],\"doi\":\"10.1007/978-3-030-16142-2_6\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c138f927708ea50aebcfbc07430e1f734693655\",\"title\":\"Sequential Embedding Induced Text Clustering, a Non-parametric Bayesian Approach\",\"url\":\"https://www.semanticscholar.org/paper/2c138f927708ea50aebcfbc07430e1f734693655\",\"venue\":\"PAKDD\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145859178\",\"name\":\"J. Zheng\"},{\"authorId\":\"145184141\",\"name\":\"Yifan Wang\"},{\"authorId\":\"46448456\",\"name\":\"X. Zhang\"},{\"authorId\":\"47056922\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/GLOBALSIP.2018.8646539\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f041269e0b0b3d3940dfa91ddaf25f702a50981d\",\"title\":\"CLASSIFICATION OF SEVERELY OCCLUDED IMAGE SEQUENCES VIA CONVOLUTIONAL RECURRENT NEURAL NETWORKS\",\"url\":\"https://www.semanticscholar.org/paper/f041269e0b0b3d3940dfa91ddaf25f702a50981d\",\"venue\":\"2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)\",\"year\":2018},{\"arxivId\":\"1903.01534\",\"authors\":[{\"authorId\":\"48240322\",\"name\":\"Changhao Chen\"},{\"authorId\":\"145895122\",\"name\":\"S. Rosa\"},{\"authorId\":\"2666898\",\"name\":\"Yishu Miao\"},{\"authorId\":\"11614724\",\"name\":\"Chris Xiaoxuan Lu\"},{\"authorId\":\"39533001\",\"name\":\"Wei Wu\"},{\"authorId\":\"34401562\",\"name\":\"A. Markham\"},{\"authorId\":\"3641238\",\"name\":\"A. Trigoni\"}],\"doi\":\"10.1109/CVPR.2019.01079\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ea0ff9044c2dbb4adc32dd9113a52053ff7eb36\",\"title\":\"Selective Sensor Fusion for Neural Visual-Inertial Odometry\",\"url\":\"https://www.semanticscholar.org/paper/2ea0ff9044c2dbb4adc32dd9113a52053ff7eb36\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1808.07659\",\"authors\":[{\"authorId\":\"30156979\",\"name\":\"Haoxuan You\"},{\"authorId\":\"95444098\",\"name\":\"Y. Feng\"},{\"authorId\":\"145592290\",\"name\":\"R. Ji\"},{\"authorId\":\"35350470\",\"name\":\"Yue Gao\"}],\"doi\":\"10.1145/3240508.3240702\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c838ffc6c4b63a577c9cb3d8d86c8ba14915640\",\"title\":\"PVNet: A Joint Convolutional Network of Point Cloud and Multi-View for 3D Shape Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1c838ffc6c4b63a577c9cb3d8d86c8ba14915640\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"2010.10839\",\"authors\":[{\"authorId\":\"7878341\",\"name\":\"Wubo Li\"},{\"authorId\":\"46197764\",\"name\":\"D. Jiang\"},{\"authorId\":\"9276071\",\"name\":\"Wei Zou\"},{\"authorId\":\"1898780\",\"name\":\"Xiangang Li\"}],\"doi\":\"10.21437/interspeech.2020-2359\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5bb5c693a69cc10bc9d4dcc48eb96bae3d0600a\",\"title\":\"TMT: A Transformer-based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/f5bb5c693a69cc10bc9d4dcc48eb96bae3d0600a\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"2841633\",\"name\":\"Hyunmin Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.18653/v1/N19-1011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4798919e74411d87f7745840e45b8bcf61128ff\",\"title\":\"AudioCaps: Generating Captions for Audios in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/c4798919e74411d87f7745840e45b8bcf61128ff\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46580562\",\"name\":\"Stefan Braun\"},{\"authorId\":\"145243593\",\"name\":\"D. Neil\"},{\"authorId\":\"24033690\",\"name\":\"Jithendar Anumula\"},{\"authorId\":\"9314936\",\"name\":\"Enea Ceolini\"},{\"authorId\":\"1704961\",\"name\":\"Shih-Chii Liu\"}],\"doi\":\"10.1109/IJCNN.2019.8852396\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f7706104e1d3e1c5c18874350de6d35334fd8fed\",\"title\":\"Attention-driven Multi-sensor Selection\",\"url\":\"https://www.semanticscholar.org/paper/f7706104e1d3e1c5c18874350de6d35334fd8fed\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"1804.09066\",\"authors\":[{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"145264990\",\"name\":\"K. Singh\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1007/978-3-030-01216-8_43\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa63893b34f523973d0692dc74ff22512daac322\",\"title\":\"ECO: Efficient Convolutional Network for Online Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/aa63893b34f523973d0692dc74ff22512daac322\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3038673\",\"name\":\"Tsubasa Ochiai\"},{\"authorId\":\"1690812\",\"name\":\"M. Delcroix\"},{\"authorId\":\"1899649\",\"name\":\"K. Kinoshita\"},{\"authorId\":\"2555553\",\"name\":\"A. Ogawa\"},{\"authorId\":\"144805536\",\"name\":\"T. Nakatani\"}],\"doi\":\"10.21437/interspeech.2019-1513\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d09d1c92ba1e0497a8414c596fe46ff512c7e1e\",\"title\":\"Multimodal SpeakerBeam: Single Channel Target Speech Extraction with Audio-Visual Speaker Clues\",\"url\":\"https://www.semanticscholar.org/paper/0d09d1c92ba1e0497a8414c596fe46ff512c7e1e\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1904.08607\",\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"103278467\",\"name\":\"Minuk Ma\"},{\"authorId\":\"4604969\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"2561991\",\"name\":\"S. Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/CVPR.2019.00853\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b621afeef5888f8f71fd6ca97a62daa0d0cb6d69\",\"title\":\"Progressive Attention Memory Network for Movie Story Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b621afeef5888f8f71fd6ca97a62daa0d0cb6d69\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1581606067\",\"name\":\"Amey Arvind Bhile\"},{\"authorId\":\"144088162\",\"name\":\"V. Hole\"}],\"doi\":\"10.1007/978-3-030-37051-0_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"065544f24bcf2094042355193364cb9d951ef065\",\"title\":\"Real-Time Environment Description Application for Visually Challenged People\",\"url\":\"https://www.semanticscholar.org/paper/065544f24bcf2094042355193364cb9d951ef065\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f6dbe07a311cbc756eb6d38528d70eb66663311\",\"title\":\"Audio Visual Scene-Aware Dialog Track in DSTC8\",\"url\":\"https://www.semanticscholar.org/paper/8f6dbe07a311cbc756eb6d38528d70eb66663311\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7299839\",\"name\":\"Huadong Tan\"},{\"authorId\":\"89136398\",\"name\":\"Guang Wu\"},{\"authorId\":\"152226296\",\"name\":\"Pengcheng Zhao\"},{\"authorId\":\"47558599\",\"name\":\"Yan-Xiang Chen\"}],\"doi\":\"10.1109/ICASSP40776.2020.9052918\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8fd8e6b8ebe89bd7500d2639fcf42f52e3e771fa\",\"title\":\"Spectrogram Analysis Via Self-Attention for Realizing Cross-Model Visual-Audio Generation\",\"url\":\"https://www.semanticscholar.org/paper/8fd8e6b8ebe89bd7500d2639fcf42f52e3e771fa\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"1812.03849\",\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1688516\",\"name\":\"Jingdong Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"title\":\"Weakly Supervised Dense Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1591133899\",\"name\":\"Yiwei Zhang\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3351094\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0b12c784965baac88b6597890303fa834fa9eea\",\"title\":\"Watch, Reason and Code: Learning to Represent Videos Using Program\",\"url\":\"https://www.semanticscholar.org/paper/c0b12c784965baac88b6597890303fa834fa9eea\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2009.04070\",\"authors\":[{\"authorId\":\"151486141\",\"name\":\"Junghyun Koo\"},{\"authorId\":\"1576777538\",\"name\":\"J. H. Lee\"},{\"authorId\":\"1932568766\",\"name\":\"Jaewoo Pyo\"},{\"authorId\":\"4352594\",\"name\":\"Yu-Jin Jo\"},{\"authorId\":\"34674393\",\"name\":\"K. Lee\"}],\"doi\":\"10.21437/interspeech.2020-3153\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9387b3b224065c9d0f41f681f62c4290c46f90b8\",\"title\":\"Exploiting Multi-Modal Features From Pre-trained Networks for Alzheimer's Dementia Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9387b3b224065c9d0f41f681f62c4290c46f90b8\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394741222\",\"name\":\"Yuling Gui\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"97522088\",\"name\":\"Ye Zhao\"}],\"doi\":\"10.1145/3347319.3356839\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"title\":\"Semantic Enhanced Encoder-Decoder Network (SEN) for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1804.04527\",\"authors\":[{\"authorId\":\"22314218\",\"name\":\"Silvio Giancola\"},{\"authorId\":\"41022271\",\"name\":\"Mohieddine Amine\"},{\"authorId\":\"41015552\",\"name\":\"Tarek Dghaily\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1109/CVPRW.2018.00223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"982f2025925062aeafac07ae015c9ed273e4d3d6\",\"title\":\"SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos\",\"url\":\"https://www.semanticscholar.org/paper/982f2025925062aeafac07ae015c9ed273e4d3d6\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34885156\",\"name\":\"C. Heath\"},{\"authorId\":\"3151995\",\"name\":\"Hemanth Venkateswara\"},{\"authorId\":\"1717453\",\"name\":\"T. McDaniel\"},{\"authorId\":\"70993850\",\"name\":\"S. Panchanathan\"}],\"doi\":\"10.1109/GlobalSIP45357.2019.8969089\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0a155ef7a0d540f42fb0649e57107d52c5c614f\",\"title\":\"Using Multimodal Data for Automated Fidelity Evaluation in Pivotal Response Treatment Videos\",\"url\":\"https://www.semanticscholar.org/paper/c0a155ef7a0d540f42fb0649e57107d52c5c614f\",\"venue\":\"2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"144179578\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a212be7ec1ff75ecfee52c7c49c73d7244a87eb7\",\"title\":\"Video Scene-Aware Dialog Track in DSTC 7\",\"url\":\"https://www.semanticscholar.org/paper/a212be7ec1ff75ecfee52c7c49c73d7244a87eb7\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1708.01015\",\"authors\":[{\"authorId\":\"143651538\",\"name\":\"Stefan Braun\"},{\"authorId\":\"145243593\",\"name\":\"D. Neil\"},{\"authorId\":\"9314936\",\"name\":\"Enea Ceolini\"},{\"authorId\":\"24033690\",\"name\":\"Jithendar Anumula\"},{\"authorId\":\"1704961\",\"name\":\"Shih-Chii Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"911e277f26bccd87807d84128e232fad655eb686\",\"title\":\"Sensor Transformation Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/911e277f26bccd87807d84128e232fad655eb686\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1704.02163\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"},{\"authorId\":\"87972149\",\"name\":\"Sergi Soler\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1016/j.jvcir.2017.11.022\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"title\":\"Egocentric video description based on temporally-linked sequences\",\"url\":\"https://www.semanticscholar.org/paper/a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144560801\",\"name\":\"Wenzhong Guo\"},{\"authorId\":\"120465682\",\"name\":\"J. Wang\"},{\"authorId\":\"49183986\",\"name\":\"S. Wang\"}],\"doi\":\"10.1109/ACCESS.2019.2916887\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c192c7d1d94e7a64de7e18e2f2fdffbf2909fcff\",\"title\":\"Deep Multimodal Representation Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/c192c7d1d94e7a64de7e18e2f2fdffbf2909fcff\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1811.04903\",\"authors\":[{\"authorId\":\"50142326\",\"name\":\"X. Wang\"},{\"authorId\":\"2879849\",\"name\":\"Ruizhi Li\"},{\"authorId\":\"144921407\",\"name\":\"Sri Harish Mallidi\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"1738798\",\"name\":\"H. Hermansky\"}],\"doi\":\"10.1109/ICASSP.2019.8682650\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf50833a46839d3932663b472d6145418f9d0bd6\",\"title\":\"Stream Attention-based Multi-array End-to-end Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bf50833a46839d3932663b472d6145418f9d0bd6\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1909.09944\",\"authors\":[{\"authorId\":\"49172303\",\"name\":\"T. Rahman\"},{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00900\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7a2de516a4e628a30036193d71faac7240d553ef\",\"title\":\"Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a2de516a4e628a30036193d71faac7240d553ef\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48116016\",\"name\":\"J. Ren\"},{\"authorId\":\"50550351\",\"name\":\"W. Zhang\"}],\"doi\":\"10.1007/S11760-019-01449-W\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1581301ddb2b9b76c10b31eef101733ffebc46f8\",\"title\":\"CLOSE: Coupled content\\u2013semantic embedding\",\"url\":\"https://www.semanticscholar.org/paper/1581301ddb2b9b76c10b31eef101733ffebc46f8\",\"venue\":\"Signal Image Video Process.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2673097\",\"name\":\"Kazuma Murao\"},{\"authorId\":\"145298778\",\"name\":\"K. Kobayashi\"},{\"authorId\":\"145111241\",\"name\":\"Hayato Kobayashi\"},{\"authorId\":\"3321637\",\"name\":\"Taichi Yatsuka\"},{\"authorId\":\"2008318\",\"name\":\"T. Masuyama\"},{\"authorId\":\"51205649\",\"name\":\"Tatsuru Higurashi\"},{\"authorId\":\"2743336\",\"name\":\"Y. Tabuchi\"}],\"doi\":\"10.18653/v1/N19-2010\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"89b30b5719569f82c849ce99c5c4c9aad17f5f2a\",\"title\":\"A Case Study on Neural Headline Generation for Editing Support\",\"url\":\"https://www.semanticscholar.org/paper/89b30b5719569f82c849ce99c5c4c9aad17f5f2a\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"2001.05614\",\"authors\":[{\"authorId\":\"49178142\",\"name\":\"H. Chen\"},{\"authorId\":\"38376468\",\"name\":\"J. Li\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.3233/FAIA200204\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f0c33980d7c011a8c657afb825220632e17b1568\",\"title\":\"Delving Deeper into the Decoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f0c33980d7c011a8c657afb825220632e17b1568\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":\"1806.08409\",\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"2809915\",\"name\":\"H. AlAmri\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":\"1816785\",\"name\":\"G. Wichern\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"51002409\",\"name\":\"Vincent Cartillier\"},{\"authorId\":\"143826364\",\"name\":\"Raphael Gontijo Lopes\"},{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"21472040\",\"name\":\"Irfan Essa\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/ICASSP.2019.8682583\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"85c22ce1a62973a4b64bbcde26748893d61d01e4\",\"title\":\"End-to-end Audio Visual Scene-aware Dialog Using Multimodal Attention-based Video Features\",\"url\":\"https://www.semanticscholar.org/paper/85c22ce1a62973a4b64bbcde26748893d61d01e4\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"1808.04108\",\"authors\":[{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"144300094\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ICASSP.2019.8682383\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"908c342d66137f5e70544e8204951f28cb02deb0\",\"title\":\"Towards Audio to Scene Image Synthesis Using Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/908c342d66137f5e70544e8204951f28cb02deb0\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":\"2007.11888\",\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"1796216614\",\"name\":\"Siyu Huang\"},{\"authorId\":\"1796254\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.24963/ijcai.2020/88\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"884be34dd5d2ea78940da96d2813be7768933857\",\"title\":\"SBAT: Video Captioning with Sparse Boundary-Aware Transformer\",\"url\":\"https://www.semanticscholar.org/paper/884be34dd5d2ea78940da96d2813be7768933857\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46608477\",\"name\":\"Ningning Guo\"},{\"authorId\":\"31833173\",\"name\":\"H. Liu\"},{\"authorId\":\"47420475\",\"name\":\"Linhua Jiang\"}],\"doi\":\"10.1109/ICARM.2019.8834066\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"02ac11a09db5d0f8a52d428e6d9ab64dba0535cf\",\"title\":\"Attention-based Visual-Audio Fusion for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/02ac11a09db5d0f8a52d428e6d9ab64dba0535cf\",\"venue\":\"2019 IEEE 4th International Conference on Advanced Robotics and Mechatronics (ICARM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3418004\",\"name\":\"Spyridon Thermos\"},{\"authorId\":\"33961149\",\"name\":\"G. Papadopoulos\"},{\"authorId\":\"1747572\",\"name\":\"P. Daras\"},{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"}],\"doi\":\"10.1109/ICIP.2018.8451158\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b79d9ce2824f2ddeb2bdbf425c0496010437fbb\",\"title\":\"Attention-Enhanced Sensorimotor Object Recognition\",\"url\":\"https://www.semanticscholar.org/paper/4b79d9ce2824f2ddeb2bdbf425c0496010437fbb\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40617978\",\"name\":\"X. Wang\"},{\"authorId\":\"49986692\",\"name\":\"Yanan Gu\"},{\"authorId\":\"49779966\",\"name\":\"Xinbo Gao\"},{\"authorId\":\"36893758\",\"name\":\"Zheng Hui\"}],\"doi\":\"10.1016/J.NEUCOM.2019.06.078\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e6ec2b7af6431da2bee5e2397705c863c1ee7bcf\",\"title\":\"Dual residual attention module network for single image super resolution\",\"url\":\"https://www.semanticscholar.org/paper/e6ec2b7af6431da2bee5e2397705c863c1ee7bcf\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"1911.12018\",\"authors\":[{\"authorId\":\"98831710\",\"name\":\"B. Yang\"},{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"145586191\",\"name\":\"Can Zhang\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9191773630826b15a86148453365aae7703aec6b\",\"title\":\"Non-Autoregressive Coarse-to-Fine Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9191773630826b15a86148453365aae7703aec6b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.08250\",\"authors\":[{\"authorId\":\"30606918\",\"name\":\"George Sterpu\"},{\"authorId\":\"1814175\",\"name\":\"Christian Saam\"},{\"authorId\":\"34530970\",\"name\":\"N. Harte\"}],\"doi\":\"10.1109/TASLP.2020.2980436\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"08b75f2df56b0ddf1fa054931d4b20027fe6f791\",\"title\":\"How to Teach DNNs to Pay Attention to the Visual Modality in Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/08b75f2df56b0ddf1fa054931d4b20027fe6f791\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":\"2007.03848\",\"authors\":[{\"authorId\":\"1947101\",\"name\":\"Shijie Geng\"},{\"authorId\":\"144740494\",\"name\":\"Peng Gao\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03598364626c419d3a2578b5c22403f0dd246e99\",\"title\":\"Spatio-Temporal Scene Graphs for Video Dialog\",\"url\":\"https://www.semanticscholar.org/paper/03598364626c419d3a2578b5c22403f0dd246e99\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":6388927,\"doi\":\"10.1109/ICCV.2017.450\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":15,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"08903ceeee6420992d30ff3f3b8b4830118af4d9\",\"references\":[{\"arxivId\":\"1506.08909\",\"authors\":[{\"authorId\":\"2054294\",\"name\":\"Ryan Lowe\"},{\"authorId\":\"3236233\",\"name\":\"Nissan Pow\"},{\"authorId\":\"35224828\",\"name\":\"I. Serban\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":\"10.18653/v1/w15-4640\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"916441619914101258c71669b5ccc36424b54a6c\",\"title\":\"The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/916441619914101258c71669b5ccc36424b54a6c\",\"venue\":\"SIGDIAL Conference\",\"year\":2015},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"50302694\",\"name\":\"Bret A. Harsham\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"},{\"authorId\":\"2018427\",\"name\":\"Yusuke Koji\"},{\"authorId\":\"2595954\",\"name\":\"Y. Fujii\"},{\"authorId\":\"47225398\",\"name\":\"Yuki Furumoto\"}],\"doi\":\"10.1109/ICME.2016.7552966\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f01f4808263ecfa221f856c34d3420166dbf5930\",\"title\":\"Driver confusion status detection using recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/f01f4808263ecfa221f856c34d3420166dbf5930\",\"venue\":\"2016 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2016},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Mao\"},{\"authorId\":null,\"name\":\"W Xu\"},{\"authorId\":null,\"name\":\"Y Yang\"},{\"authorId\":null,\"name\":\"J Wang\"},{\"authorId\":null,\"name\":\"A L Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep captioning with multimodal recurrent neural networks (mrnn ). CoRR, abs/1412\",\"url\":\"\",\"venue\":\"Deep captioning with multimodal recurrent neural networks (mrnn ). CoRR, abs/1412\",\"year\":2014},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1506.07503\",\"authors\":[{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1862138\",\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b624504240fa52ab76167acfe3156150ca01cf3b\",\"title\":\"Attention-Based Models for Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/b624504240fa52ab76167acfe3156150ca01cf3b\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\",\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3117618\",\"name\":\"Seiya Tokui\"},{\"authorId\":\"1812144\",\"name\":\"Kenta Oono\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"67156902beca9bc90b728c8d5dd4ac9d8b27d3a3\",\"title\":\"Chainer : a Next-Generation Open Source Framework for Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/67156902beca9bc90b728c8d5dd4ac9d8b27d3a3\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1409.1259\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/W14-4012\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1eb09fecd75eb27825dce4f964b97f4f5cc399d7\",\"title\":\"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches\",\"url\":\"https://www.semanticscholar.org/paper/1eb09fecd75eb27825dce4f964b97f4f5cc399d7\",\"venue\":\"SSST@EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741200\",\"name\":\"J. Movellan\"},{\"authorId\":\"3040175\",\"name\":\"P. Mineiro\"}],\"doi\":\"10.1023/A:1007468413059\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7badd4c72a82ab0a3712a9da0347b7dd6ccffe1\",\"title\":\"Robust Sensor Fusion: Analysis and Application to Audio Visual Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f7badd4c72a82ab0a3712a9da0347b7dd6ccffe1\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M Lin\"},{\"authorId\":null,\"name\":\"Q Chen\"},{\"authorId\":null,\"name\":\"S Yan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Network in network. CoRR, abs/1312\",\"url\":\"\",\"venue\":\"Network in network. CoRR, abs/1312\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1212.5701\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8729441d734782c3ed532a7d2d9611b438c0a09a\",\"title\":\"ADADELTA: An Adaptive Learning Rate Method\",\"url\":\"https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"F. Pereira\"},{\"authorId\":null,\"name\":\"L. Bottou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"agenet classification with deep convolutional neural networks\",\"url\":\"\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1408.5093\",\"authors\":[{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"3049736\",\"name\":\"S. Karayev\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1145/2647868.2654889\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bdb186ec4726e00a8051119636d4df3b94043b5\",\"title\":\"Caffe: Convolutional Architecture for Fast Feature Embedding\",\"url\":\"https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Q. Chen M. Lin\"},{\"authorId\":null,\"name\":\"S. Yan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"agenet classification with deep convolutional neural networks\",\"url\":\"\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1312.4400\",\"authors\":[{\"authorId\":\"143953684\",\"name\":\"M. Lin\"},{\"authorId\":\"35370244\",\"name\":\"Q. Chen\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5e83ab70d0cbc003471e87ec306d27d9c80ecb16\",\"title\":\"Network In Network\",\"url\":\"https://www.semanticscholar.org/paper/5e83ab70d0cbc003471e87ec306d27d9c80ecb16\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1708671\",\"name\":\"G. Gravier\"},{\"authorId\":\"145492758\",\"name\":\"S. Axelrod\"},{\"authorId\":\"1688852\",\"name\":\"G. Potamianos\"},{\"authorId\":\"2264160\",\"name\":\"C. Neti\"}],\"doi\":\"10.1109/ICASSP.2002.5743873\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9b641945f6695a3d7501a1cd66ebfe76ef1d305a\",\"title\":\"Maximum entropy and MCE based HMM stream weight estimation for audio-visual ASR\",\"url\":\"https://www.semanticscholar.org/paper/9b641945f6695a3d7501a1cd66ebfe76ef1d305a\",\"venue\":\"2002 IEEE International Conference on Acoustics, Speech, and Signal Processing\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"},{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"}],\"doi\":\"10.21437/Interspeech.2016-380\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"title\":\"Generating Natural Video Descriptions via Multimodal Processing\",\"url\":\"https://www.semanticscholar.org/paper/2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"venue\":\"INTERSPEECH\",\"year\":2016},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32533286\",\"name\":\"M. Hennecke\"},{\"authorId\":\"2586918\",\"name\":\"D. Stork\"},{\"authorId\":\"144739049\",\"name\":\"K. V. Prasad\"}],\"doi\":\"10.1007/978-3-662-13015-5_25\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f48dd812be36503ea9dada921fdc642c939e6f27\",\"title\":\"Visionary Speech: Looking Ahead to Practical Speechreading Systems\",\"url\":\"https://www.semanticscholar.org/paper/f48dd812be36503ea9dada921fdc642c939e6f27\",\"venue\":\"\",\"year\":1996},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"I. Sutskever A. Krizhevsky\"},{\"authorId\":null,\"name\":\"G. E. Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"agenet classification with deep convolutional neural networks\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1508.04395\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"1862138\",\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":\"2616163\",\"name\":\"Philemon Brakel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.1109/ICASSP.2016.7472618\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"878ba5458e9e51f0b341fd9117fa0b43ef4096d3\",\"title\":\"End-to-end attention-based large vocabulary speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/878ba5458e9e51f0b341fd9117fa0b43ef4096d3\",\"venue\":\"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D Bahdanau\"},{\"authorId\":null,\"name\":\"K Cho\"},{\"authorId\":null,\"name\":\"Y Bengio\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Neural machine translation by jointly learning to align and translate. CoRR, abs/1409\",\"url\":\"\",\"venue\":\"Neural machine translation by jointly learning to align and translate. CoRR, abs/1409\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. Hori\"},{\"authorId\":null,\"name\":\"T. Hori\"},{\"authorId\":null,\"name\":\"T. Lee\"},{\"authorId\":null,\"name\":\"K. Sumi\"},{\"authorId\":null,\"name\":\"J. R. Hershey\"},{\"authorId\":null,\"name\":\"T. K. Marks\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Attention-based multimodal fusion for video description\",\"url\":\"\",\"venue\":\"CoRR, abs/1701.03126,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1689269\",\"name\":\"C. Liu\"}],\"doi\":\"10.1109/CVPR.2011.5995407\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3afbb0e64fcb70496b44b30b76fac9456cc51e34\",\"title\":\"Action recognition by dense trajectories\",\"url\":\"https://www.semanticscholar.org/paper/3afbb0e64fcb70496b44b30b76fac9456cc51e34\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Tieleman\"},{\"authorId\":null,\"name\":\"G. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Lecture 6.5\\u2014RmsProp: Divide the gradient by a running average of its recent magnitude\",\"url\":\"\",\"venue\":\"COURSERA: Neural Networks for Machine Learning,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"48016277\",\"name\":\"Hai Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"145222187\",\"name\":\"B. Harsham\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"},{\"authorId\":\"2018427\",\"name\":\"Yusuke Koji\"},{\"authorId\":\"40319314\",\"name\":\"Y. Jing\"},{\"authorId\":\"9031926\",\"name\":\"Zhaocheng Zhu\"},{\"authorId\":\"9037196\",\"name\":\"T. Aikawa\"}],\"doi\":\"10.1109/SLT.2016.7846317\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8e580fcf34ee6da50989bbde685634018cbe224\",\"title\":\"Dialog state tracking with attention-based sequence-to-sequence learning\",\"url\":\"https://www.semanticscholar.org/paper/f8e580fcf34ee6da50989bbde685634018cbe224\",\"venue\":\"2016 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2016}],\"title\":\"Attention-Based Multimodal Fusion for Video Description\",\"topics\":[{\"topic\":\"Multimodal interaction\",\"topicId\":\"42592\",\"url\":\"https://www.semanticscholar.org/topic/42592\"},{\"topic\":\"Recurrent neural network\",\"topicId\":\"16115\",\"url\":\"https://www.semanticscholar.org/topic/16115\"},{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"Concatenation\",\"topicId\":\"2262\",\"url\":\"https://www.semanticscholar.org/topic/2262\"},{\"topic\":\"Modality (human\\u2013computer interaction)\",\"topicId\":\"462\",\"url\":\"https://www.semanticscholar.org/topic/462\"},{\"topic\":\"Introspection\",\"topicId\":\"124535\",\"url\":\"https://www.semanticscholar.org/topic/124535\"},{\"topic\":\"Encoder\",\"topicId\":\"16744\",\"url\":\"https://www.semanticscholar.org/topic/16744\"},{\"topic\":\"Data structure alignment\",\"topicId\":\"227498\",\"url\":\"https://www.semanticscholar.org/topic/227498\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Relevance\",\"topicId\":\"503\",\"url\":\"https://www.semanticscholar.org/topic/503\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Microsoft Research\",\"topicId\":\"73897\",\"url\":\"https://www.semanticscholar.org/topic/73897\"},{\"topic\":\"Oracle Fusion Architecture\",\"topicId\":\"4475853\",\"url\":\"https://www.semanticscholar.org/topic/4475853\"}],\"url\":\"https://www.semanticscholar.org/paper/08903ceeee6420992d30ff3f3b8b4830118af4d9\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017}\n"