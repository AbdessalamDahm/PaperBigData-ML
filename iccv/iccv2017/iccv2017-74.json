"{\"abstract\":\"Most natural videos contain numerous events. For example, in a video of a \\u201cman playing a piano\\u201d, the video might also contain \\u201canother man dancing\\u201d or \\u201ca crowd clapping\\u201d. We introduce the task of dense-captioning events, which involves both detecting and describing events in a video. We propose a new model that is able to identify all events in a single pass of the video while simultaneously describing the detected events with natural language. Our model introduces a variant of an existing proposal module that is designed to capture both short as well as long events that span minutes. To capture the dependencies between the events in a video, our model introduces a new captioning module that uses contextual information from past and future events to jointly describe all events. We also introduce ActivityNet Captions, a large-scale benchmark for dense-captioning events. ActivityNet Captions contains 20k videos amounting to 849 video hours with 100k total descriptions, each with its unique start and end time. Finally, we report performances of our model for dense-captioning events, video retrieval and localization.\",\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\",\"url\":\"https://www.semanticscholar.org/author/145237361\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\",\"url\":\"https://www.semanticscholar.org/author/1382195702\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\",\"url\":\"https://www.semanticscholar.org/author/3260219\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\",\"url\":\"https://www.semanticscholar.org/author/48004138\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\",\"url\":\"https://www.semanticscholar.org/author/9200530\"}],\"citationVelocity\":74,\"citations\":[{\"arxivId\":\"1911.09345\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"1747500\",\"name\":\"A. Mian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"title\":\"Empirical Autopsy of Deep Video Captioning Frameworks\",\"url\":\"https://www.semanticscholar.org/paper/62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"50688017\",\"name\":\"L. Ji\"},{\"authorId\":\"3887469\",\"name\":\"Yaobo Liang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"46915168\",\"name\":\"P. Chen\"},{\"authorId\":\"46764518\",\"name\":\"Zhendong Niu\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"}],\"doi\":\"10.18653/v1/P19-1641\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ea57ffa3e13400cad53dc061887a6fbbd45e7f12\",\"title\":\"Dense Procedure Captioning in Narrated Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/ea57ffa3e13400cad53dc061887a6fbbd45e7f12\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1907.12763\",\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"150234800\",\"name\":\"Mattia Soldan\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"145160922\",\"name\":\"Bryan Russell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e12a3e3f3f383f222b5d2007802d7b7944364301\",\"title\":\"Temporal Localization of Moments in Video Collections with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/e12a3e3f3f383f222b5d2007802d7b7944364301\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.13913\",\"authors\":[{\"authorId\":\"144568152\",\"name\":\"David M. Chan\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8ac8179dbdd256e514700b076673b6d5b9083251\",\"title\":\"Active Learning for Video Description With Cluster-Regularized Ensemble Ranking\",\"url\":\"https://www.semanticscholar.org/paper/8ac8179dbdd256e514700b076673b6d5b9083251\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7332901\",\"name\":\"Gan Sun\"},{\"authorId\":\"145702758\",\"name\":\"Yang Cong\"},{\"authorId\":\"49681152\",\"name\":\"L. Wang\"},{\"authorId\":\"2788685\",\"name\":\"Z. Ding\"},{\"authorId\":\"152987474\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/ICCVW.2019.00126\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c83c8a91b67768adcfa8cfc661e1093bc8a25fc8\",\"title\":\"Online Multi-Task Clustering for Human Motion Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/c83c8a91b67768adcfa8cfc661e1093bc8a25fc8\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"title\":\"Multi-Modal Deep Learning to Understand Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47897687\",\"name\":\"H. Tan\"},{\"authorId\":\"47297550\",\"name\":\"Hongyuan Zhu\"},{\"authorId\":\"6516914\",\"name\":\"J. Lim\"},{\"authorId\":\"1694051\",\"name\":\"Cheston Tan\"}],\"doi\":\"10.1016/J.CVIU.2020.103107\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d79912ee3469c1c93f2c726f9a9b4baf7682ec5f\",\"title\":\"A comprehensive survey of procedural video datasets\",\"url\":\"https://www.semanticscholar.org/paper/d79912ee3469c1c93f2c726f9a9b4baf7682ec5f\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2021},{\"arxivId\":\"1904.01766\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"49588480\",\"name\":\"A. Myers\"},{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"1702318\",\"name\":\"Kevin Murphy\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2019.00756\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c41a11c0e9b8b92b4faaf97749841170b760760a\",\"title\":\"VideoBERT: A Joint Model for Video and Language Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/c41a11c0e9b8b92b4faaf97749841170b760760a\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30557120\",\"name\":\"Heeseung Kwon\"},{\"authorId\":\"7992455\",\"name\":\"W. Shim\"},{\"authorId\":\"72643925\",\"name\":\"Minsu Cho\"}],\"doi\":\"10.1109/ICCVW.2019.00192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8de23a4528ae7f569161d295cbda1619d28abca\",\"title\":\"Temporal U-Nets for Video Summarization with Scene and Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f8de23a4528ae7f569161d295cbda1619d28abca\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"35084211\",\"name\":\"M. Lewis\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D18-1117\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"title\":\"A Dataset for Telling the Stories of Social Media Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2444704\",\"name\":\"Xindi Shang\"},{\"authorId\":\"79723716\",\"name\":\"D. Di\"},{\"authorId\":\"66358686\",\"name\":\"J. Xiao\"},{\"authorId\":\"144149886\",\"name\":\"Yu Cao\"},{\"authorId\":\"2028727\",\"name\":\"X. Yang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1145/3323873.3325056\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa0388f5373a2f17a3c456346a52427c887667ea\",\"title\":\"Annotating Objects and Relations in User-Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/fa0388f5373a2f17a3c456346a52427c887667ea\",\"venue\":\"ICMR\",\"year\":2019},{\"arxivId\":\"1806.08251\",\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8f27df2d4fb7dd7ed5587640dcbe4dc1eb37acfb\",\"title\":\"Unseen Action Recognition with Multimodal Learning\",\"url\":\"https://www.semanticscholar.org/paper/8f27df2d4fb7dd7ed5587640dcbe4dc1eb37acfb\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1907.13487\",\"authors\":[{\"authorId\":\"40457423\",\"name\":\"Y. Liu\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"title\":\"Use What You Have: Video retrieval using representations from collaborative experts\",\"url\":\"https://www.semanticscholar.org/paper/b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1903.02874\",\"authors\":[{\"authorId\":\"35299091\",\"name\":\"Yansong Tang\"},{\"authorId\":\"50792340\",\"name\":\"Dajun Ding\"},{\"authorId\":\"39358728\",\"name\":\"Yongming Rao\"},{\"authorId\":\"145473095\",\"name\":\"Y. Zheng\"},{\"authorId\":\"2118333\",\"name\":\"Danyang Zhang\"},{\"authorId\":\"48096213\",\"name\":\"L. Zhao\"},{\"authorId\":\"1697700\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"49640256\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/CVPR.2019.00130\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"title\":\"COIN: A Large-Scale Dataset for Comprehensive Instructional Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/e27e78c33288728f66f7dab2fe2696ddbc5c1026\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2001.06891\",\"authors\":[{\"authorId\":\"1742291\",\"name\":\"Zixing Zhang\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"90148415\",\"name\":\"Yang Zhao\"},{\"authorId\":\"50621207\",\"name\":\"Q. Wang\"},{\"authorId\":\"46936306\",\"name\":\"H. Liu\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1109/cvpr42600.2020.01068\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a709bb1b4bce51de5aa67a362ea69e51a43d1e5\",\"title\":\"Where Does It Exist: Spatio-Temporal Video Grounding for Multi-Form Sentences\",\"url\":\"https://www.semanticscholar.org/paper/0a709bb1b4bce51de5aa67a362ea69e51a43d1e5\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3442125\",\"name\":\"Ines Chami\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f2d8795acbc19d235da6300a3f0f587066ae485d\",\"title\":\"CS 231 N Final Report : Single-Stream Action Proposals in Videos\",\"url\":\"https://www.semanticscholar.org/paper/f2d8795acbc19d235da6300a3f0f587066ae485d\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1906.12158\",\"authors\":[{\"authorId\":\"47294375\",\"name\":\"Zhu Zhang\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"145510896\",\"name\":\"Z. Lin\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"}],\"doi\":\"10.24963/ijcai.2019/609\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b2ae2e1f9148e6a05128773c16868cae7e99dc2\",\"title\":\"Open-Ended Long-Form Video Question Answering via Hierarchical Convolutional Self-Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/1b2ae2e1f9148e6a05128773c16868cae7e99dc2\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1804.00819\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"34872128\",\"name\":\"Yingbo Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.1109/CVPR.2018.00911\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"35ed258aede3df17ee20a6635364cb5fd2461049\",\"title\":\"End-to-End Dense Video Captioning with Masked Transformer\",\"url\":\"https://www.semanticscholar.org/paper/35ed258aede3df17ee20a6635364cb5fd2461049\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47569376\",\"name\":\"Shijie Yang\"},{\"authorId\":\"73596205\",\"name\":\"L. Li\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"49356099\",\"name\":\"Dechao Meng\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3343031.3350859\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"title\":\"Structured Stochastic Recurrent Network for Linguistic Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.00744\",\"authors\":[{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"1614038854\",\"name\":\"Yang Liu\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"47107270\",\"name\":\"E. Coto\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"151352107\",\"name\":\"Valentin Gabeur\"},{\"authorId\":\"1612977414\",\"name\":\"Chen Sun\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"1389284356\",\"name\":\"Kaixu Cui\"},{\"authorId\":\"87652983\",\"name\":\"H. Liu\"},{\"authorId\":\"1808091339\",\"name\":\"Chen Wang\"},{\"authorId\":\"3040310\",\"name\":\"Y. Jiang\"},{\"authorId\":\"145912650\",\"name\":\"X. Hao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2921b34b99c6150a7625acdbdd99504c2789f7a2\",\"title\":\"The End-of-End-to-End: A Video Understanding Pentathlon Challenge (2020)\",\"url\":\"https://www.semanticscholar.org/paper/2921b34b99c6150a7625acdbdd99504c2789f7a2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"145303641\",\"name\":\"Yu Xing\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"145201124\",\"name\":\"Xiang Ren\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8b7cb2e9d884427ef50b564d97d3fd403953afa6\",\"title\":\"COMMONGEN: Towards Generative Commonsense Reasoning via A Constrained Text Generation Challenge\",\"url\":\"https://www.semanticscholar.org/paper/8b7cb2e9d884427ef50b564d97d3fd403953afa6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.01121\",\"authors\":[{\"authorId\":\"3396987\",\"name\":\"Sharon Zhou\"},{\"authorId\":\"39504881\",\"name\":\"M. Gordon\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"89315058\",\"name\":\"Austin Narcomey\"},{\"authorId\":\"3312398\",\"name\":\"Durim Morina\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"717d85bd48aca5d950a4da4bc63b647a57233075\",\"title\":\"HYPE: Human eYe Perceptual Evaluation of Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/717d85bd48aca5d950a4da4bc63b647a57233075\",\"venue\":\"DGS@ICLR\",\"year\":2019},{\"arxivId\":\"1808.05326\",\"authors\":[{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"4671928\",\"name\":\"Roy Schwartz\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/D18-1009\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"af5c4b80fbf847f69a202ba5a780a3dd18c1a027\",\"title\":\"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference\",\"url\":\"https://www.semanticscholar.org/paper/af5c4b80fbf847f69a202ba5a780a3dd18c1a027\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1906.03327\",\"authors\":[{\"authorId\":\"19200186\",\"name\":\"Antoine Miech\"},{\"authorId\":\"35838466\",\"name\":\"D. Zhukov\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1109/ICCV.2019.00272\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9311779489e597315488749ee6c386bfa3f3512e\",\"title\":\"HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips\",\"url\":\"https://www.semanticscholar.org/paper/9311779489e597315488749ee6c386bfa3f3512e\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1711.06370\",\"authors\":[{\"authorId\":\"3194022\",\"name\":\"Bohan Zhuang\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2018.00447\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7299465d70181e423480fdb252aa2e28c18aa012\",\"title\":\"Parallel Attention: A Unified Framework for Visual Object Discovery Through Dialogs and Queries\",\"url\":\"https://www.semanticscholar.org/paper/7299465d70181e423480fdb252aa2e28c18aa012\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47751104\",\"name\":\"Dali Yang\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"}],\"doi\":\"10.1109/ICIP.2018.8451740\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"758d1c17569eea2a698cac31b2d9d2a772c84322\",\"title\":\"Hierarchical Context Encoding for Events Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/758d1c17569eea2a698cac31b2d9d2a772c84322\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":\"1904.03885\",\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":null,\"name\":\"Abhinav Shrivastava\"},{\"authorId\":\"2852035\",\"name\":\"V. Morariu\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.18653/v1/W19-1802\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e164e75632e23a7fba6a46d2ee2dc328720601af\",\"title\":\"Referring to Objects in Videos using Spatio-Temporal Identifying Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e164e75632e23a7fba6a46d2ee2dc328720601af\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"1791344388\",\"name\":\"Lei Ji\"},{\"authorId\":\"1783553\",\"name\":\"Zhen-dong Niu\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/3394171.3413498\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"title\":\"Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning\",\"url\":\"https://www.semanticscholar.org/paper/de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"34779291\",\"name\":\"Fuchen Long\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"153216896\",\"name\":\"D. Li\"},{\"authorId\":\"153040576\",\"name\":\"T. Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c3ecbfb72986111f3489704e9fe4a12175b0240\",\"title\":\"MSR Asia MSM at ActivityNet Challenge 2017: Trimmed Action Recognition, Temporal Action Proposals and Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/6c3ecbfb72986111f3489704e9fe4a12175b0240\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"title\":\"LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2004.00760\",\"authors\":[{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.14288/1.0392691\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"title\":\"Consistent Multiple Sequence Decoding\",\"url\":\"https://www.semanticscholar.org/paper/5e7139debfcff8c193bc0141302218fe0d4c8a32\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41030694\",\"name\":\"Huanyu Yu\"},{\"authorId\":\"3392007\",\"name\":\"Shuo Cheng\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"7272302\",\"name\":\"Minsi Wang\"},{\"authorId\":\"40430880\",\"name\":\"J. Zhang\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"}],\"doi\":\"10.1109/CVPR.2018.00629\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f5876f67129a80a1ee753f715efcd2e2109bf432\",\"title\":\"Fine-Grained Video Captioning for Sports Narrative\",\"url\":\"https://www.semanticscholar.org/paper/f5876f67129a80a1ee753f715efcd2e2109bf432\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1901.09107\",\"authors\":[{\"authorId\":\"2809915\",\"name\":\"H. AlAmri\"},{\"authorId\":\"51002409\",\"name\":\"Vincent Cartillier\"},{\"authorId\":\"50317425\",\"name\":\"Abhishek Das\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"153149395\",\"name\":\"P. Anderson\"},{\"authorId\":\"21472040\",\"name\":\"Irfan Essa\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"}],\"doi\":\"10.1109/CVPR.2019.00774\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"044c56af7005c2013ce24c7199af716319378d7f\",\"title\":\"Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/044c56af7005c2013ce24c7199af716319378d7f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390916430\",\"name\":\"Huan Liu\"},{\"authorId\":\"2817677\",\"name\":\"Qinghua Zheng\"},{\"authorId\":\"3326677\",\"name\":\"Minnan Luo\"},{\"authorId\":\"152193942\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"152299623\",\"name\":\"C. Yan\"},{\"authorId\":\"46518251\",\"name\":\"L. Yao\"}],\"doi\":\"10.1016/j.knosys.2020.106432\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6072b5407daf1db4871fa27bdac7f63407019091\",\"title\":\"Memory transformation networks for weakly supervised visual classification\",\"url\":\"https://www.semanticscholar.org/paper/6072b5407daf1db4871fa27bdac7f63407019091\",\"venue\":\"Knowl. Based Syst.\",\"year\":2020},{\"arxivId\":\"2007.08751\",\"authors\":[{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"}],\"doi\":\"10.1007/978-3-030-58523-5_34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"773e7d33411fc2cdd6829356b7ce8ed34e14cd65\",\"title\":\"Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/773e7d33411fc2cdd6829356b7ce8ed34e14cd65\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.07236\",\"authors\":[{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"1389646918\",\"name\":\"Edison Marrese-Taylor\"},{\"authorId\":\"9120887\",\"name\":\"F. Saleh\"},{\"authorId\":\"40124570\",\"name\":\"Hongdong Li\"},{\"authorId\":\"47873182\",\"name\":\"S. Gould\"}],\"doi\":\"10.1109/WACV45572.2020.9093328\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03df26255781ebb71d9430e1b2aaabf8e1af9990\",\"title\":\"Proposal-free Temporal Moment Localization of a Natural-Language Query in Video using Guided Attention\",\"url\":\"https://www.semanticscholar.org/paper/03df26255781ebb71d9430e1b2aaabf8e1af9990\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1812.03849\",\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1688516\",\"name\":\"Jingdong Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"title\":\"Weakly Supervised Dense Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1904.02755\",\"authors\":[{\"authorId\":\"3288111\",\"name\":\"S. Ghosh\"},{\"authorId\":\"50714560\",\"name\":\"A. Agarwal\"},{\"authorId\":\"27456119\",\"name\":\"Zarana Parekh\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.18653/v1/N19-1198\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ddf69a6ef015a1b0668ca48a486c4cc7e22a7d9c\",\"title\":\"ExCL: Extractive Clip Localization Using Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/ddf69a6ef015a1b0668ca48a486c4cc7e22a7d9c\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144677557\",\"name\":\"C. Li\"},{\"authorId\":\"47827559\",\"name\":\"Yuming Zhao\"},{\"authorId\":\"151474865\",\"name\":\"S. Peng\"},{\"authorId\":\"73708274\",\"name\":\"J. Chen\"}],\"doi\":\"10.1109/ICIP.2019.8802929\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c943d780edef5c07526e745ac678bfcea79b7bfb\",\"title\":\"Bidirectional Single-Stream Temporal Sentence Query Localization in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/c943d780edef5c07526e745ac678bfcea79b7bfb\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":\"2001.09099\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.1007/978-3-030-58589-1_27\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"37dd9c725cb800fd5f336a5227f02c160b8723cb\",\"title\":\"TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/37dd9c725cb800fd5f336a5227f02c160b8723cb\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mehrdad Hosseinzadeh\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"title\":\"Video Captioning of Future Frames\",\"url\":\"https://www.semanticscholar.org/paper/da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1910.02930\",\"authors\":[{\"authorId\":\"2689239\",\"name\":\"Jack Hessel\"},{\"authorId\":\"48157646\",\"name\":\"Bo Pang\"},{\"authorId\":\"39815369\",\"name\":\"Z. Zhu\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":\"10.18653/v1/K19-1039\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"title\":\"A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/659e2f1d54b88252bcf08c4f3d54c0832a181c3e\",\"venue\":\"CoNLL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153228843\",\"name\":\"Chujie Lu\"},{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"71208047\",\"name\":\"C. Tan\"},{\"authorId\":\"47056890\",\"name\":\"Xiao-Lin Li\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.18653/v1/D19-1518\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"872091517b0bfad0e9bc1826d4668022d1d57953\",\"title\":\"DEBUG: A Dense Bottom-Up Grounding Approach for Natural Language Video Localization\",\"url\":\"https://www.semanticscholar.org/paper/872091517b0bfad0e9bc1826d4668022d1d57953\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"2010.06035\",\"authors\":[{\"authorId\":\"112873019\",\"name\":\"Jaylin Herskovitz\"},{\"authorId\":\"47875818\",\"name\":\"J. Wu\"},{\"authorId\":\"144289340\",\"name\":\"Samuel White\"},{\"authorId\":\"48453720\",\"name\":\"A. Pavel\"},{\"authorId\":\"47098111\",\"name\":\"G. Reyes\"},{\"authorId\":\"2582404\",\"name\":\"Anhong Guo\"},{\"authorId\":\"1744846\",\"name\":\"Jeffrey P. Bigham\"}],\"doi\":\"10.1145/3373625.3417006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8\",\"title\":\"Making Mobile Augmented Reality Applications Accessible\",\"url\":\"https://www.semanticscholar.org/paper/8d4702b2f7ebb13d75d2def7ada05968f9cc5fc8\",\"venue\":\"ASSETS\",\"year\":2020},{\"arxivId\":\"1907.05092\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"50678073\",\"name\":\"Bei Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8b7bf64b2c7372aa82d32424aacc6f4a86215433\",\"title\":\"Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8b7bf64b2c7372aa82d32424aacc6f4a86215433\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1808.03766\",\"authors\":[{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"8983218\",\"name\":\"S. Buch\"},{\"authorId\":\"3409955\",\"name\":\"C. D. Dao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5468c96e3846da23c26b59c28c313506bffbf7ce\",\"title\":\"The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary\",\"url\":\"https://www.semanticscholar.org/paper/5468c96e3846da23c26b59c28c313506bffbf7ce\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2003.04865\",\"authors\":[{\"authorId\":\"3087214\",\"name\":\"Yutaro Shigeto\"},{\"authorId\":\"31678456\",\"name\":\"Y. Yoshikawa\"},{\"authorId\":\"2996464\",\"name\":\"Jiaqing Lin\"},{\"authorId\":\"39702069\",\"name\":\"A. Takeuchi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"title\":\"Video Caption Dataset for Describing Human Actions in Japanese\",\"url\":\"https://www.semanticscholar.org/paper/46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":\"2006.13256\",\"authors\":[{\"authorId\":\"1677780022\",\"name\":\"Dima Damen\"},{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"1729739\",\"name\":\"G. Farinella\"},{\"authorId\":\"1792681\",\"name\":\"Antonino Furnari\"},{\"authorId\":\"48842721\",\"name\":\"Evangelos Kazakos\"},{\"authorId\":\"153155867\",\"name\":\"Jian Ma\"},{\"authorId\":\"3420479\",\"name\":\"D. Moltisanti\"},{\"authorId\":\"47077615\",\"name\":\"J. Munro\"},{\"authorId\":\"2682004\",\"name\":\"T. Perrett\"},{\"authorId\":\"50065546\",\"name\":\"W. Price\"},{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"}],\"doi\":\"10.5523/bris.2g1n6qdydwa9u22shpxqzp0t8m\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c1a80daadab9c3bc3fdf183c669070ba7a3fd37\",\"title\":\"Rescaling Egocentric Vision\",\"url\":\"https://www.semanticscholar.org/paper/6c1a80daadab9c3bc3fdf183c669070ba7a3fd37\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2008.02448\",\"authors\":[{\"authorId\":\"51912474\",\"name\":\"Xiaoye Qu\"},{\"authorId\":\"1855095179\",\"name\":\"Pengwei Tang\"},{\"authorId\":\"3008849\",\"name\":\"Zhikang Zhou\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"}],\"doi\":\"10.1145/3394171.3414053\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a4d78df0b175e3cbf811e652184f6b1e9d11caca\",\"title\":\"Fine-grained Iterative Attention Network for Temporal Language Localization in Videos\",\"url\":\"https://www.semanticscholar.org/paper/a4d78df0b175e3cbf811e652184f6b1e9d11caca\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1582890834\",\"name\":\"Zekun Yang\"},{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"2427516\",\"name\":\"Chenhui Chu\"},{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"1748743\",\"name\":\"H. Takemura\"}],\"doi\":\"10.1109/WACV45572.2020.9093596\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39b2d8b8233a53dc7eadb819c52213369dff8648\",\"title\":\"BERT Representations for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/39b2d8b8233a53dc7eadb819c52213369dff8648\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1904.03493\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"46255707\",\"name\":\"Lei Li\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"title\":\"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research\",\"url\":\"https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1904.11238\",\"authors\":[{\"authorId\":\"40063957\",\"name\":\"Eric Arazo Sanchez\"},{\"authorId\":\"2531432\",\"name\":\"Diego Ortego\"},{\"authorId\":\"83107779\",\"name\":\"P. Albert\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eeecea3097cf5629eb72a06e5caaf24d774adce7\",\"title\":\"Unsupervised label noise modeling and loss correction\",\"url\":\"https://www.semanticscholar.org/paper/eeecea3097cf5629eb72a06e5caaf24d774adce7\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1802.10250\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACV.2019.00048\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"99cdb10443a0543be3466c9231ff922bcc996843\",\"title\":\"Joint Event Detection and Description in Continuous Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/99cdb10443a0543be3466c9231ff922bcc996843\",\"venue\":\"2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46447373\",\"name\":\"X. Zhang\"},{\"authorId\":\"48019166\",\"name\":\"J. Zhao\"},{\"authorId\":null,\"name\":\"Long Chen\"},{\"authorId\":\"71074804\",\"name\":\"W. Wang\"}],\"doi\":\"10.1007/978-3-030-64221-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd8930e23b8aa474df701469a4e9d8c8098c9432\",\"title\":\"Advances in Neural Networks \\u2013 ISNN 2020: 17th International Symposium on Neural Networks, ISNN 2020, Cairo, Egypt, December 4\\u20136, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/cd8930e23b8aa474df701469a4e9d8c8098c9432\",\"venue\":\"ISNN\",\"year\":2020},{\"arxivId\":\"2011.09046\",\"authors\":[{\"authorId\":\"1490732820\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"145327825\",\"name\":\"Mingde Zhao\"},{\"authorId\":\"40600020\",\"name\":\"Sheide Chammas\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"39543696\",\"name\":\"Fei Sha\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fe21d30afa60f8ab72da309ca0a80eee1ac07a66\",\"title\":\"A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus\",\"url\":\"https://www.semanticscholar.org/paper/fe21d30afa60f8ab72da309ca0a80eee1ac07a66\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2359832\",\"name\":\"Hongya Wang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"46772808\",\"name\":\"X. Chen\"},{\"authorId\":\"2352456\",\"name\":\"Z. Xiong\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3394171.3413975\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b8d5e6f888c4e165fea9bab809239f7fe5fef65\",\"title\":\"Dual Path Interaction Network for Video Moment Localization\",\"url\":\"https://www.semanticscholar.org/paper/7b8d5e6f888c4e165fea9bab809239f7fe5fef65\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"2841633\",\"name\":\"Hyunmin Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.18653/v1/N19-1011\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4798919e74411d87f7745840e45b8bcf61128ff\",\"title\":\"AudioCaps: Generating Captions for Audios in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/c4798919e74411d87f7745840e45b8bcf61128ff\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98579574\",\"name\":\"Daizong Liu\"},{\"authorId\":\"51912474\",\"name\":\"Xiaoye Qu\"},{\"authorId\":\"1912074118\",\"name\":\"Jianfeng Dong\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"83d2f3b57aa07c499c3a19d22fe4f4ef655e9030\",\"title\":\"Reasoning Step-by-Step: Temporal Sentence Localization in Videos via Deep Rectification-Modulation Network\",\"url\":\"https://www.semanticscholar.org/paper/83d2f3b57aa07c499c3a19d22fe4f4ef655e9030\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4210401\",\"name\":\"Junqing Yu\"},{\"authorId\":\"121240872\",\"name\":\"Yixin Huang\"},{\"authorId\":\"49990818\",\"name\":\"Yunfeng He\"}],\"doi\":\"10.1145/3265845.3265847\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fcba855978c51aacd4af52dda4568ec8e1e633a\",\"title\":\"Snooker Video Event Detection Using Multimodal Features\",\"url\":\"https://www.semanticscholar.org/paper/5fcba855978c51aacd4af52dda4568ec8e1e633a\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2002.06353\",\"authors\":[{\"authorId\":\"35347136\",\"name\":\"Huaishao Luo\"},{\"authorId\":\"144906579\",\"name\":\"Lei Ji\"},{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"15086992\",\"name\":\"H. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"66782928\",\"name\":\"Tianrui Li\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4243555758433880a67b15b50f752b1e2a8c4609\",\"title\":\"UniViLM: A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation\",\"url\":\"https://www.semanticscholar.org/paper/4243555758433880a67b15b50f752b1e2a8c4609\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58565-5_36\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"853069fa3f976fe368858ce4650b6348a17a3764\",\"title\":\"Hierarchical Visual-Textual Graph for Temporal Activity Localization via Language\",\"url\":\"https://www.semanticscholar.org/paper/853069fa3f976fe368858ce4650b6348a17a3764\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2006.14262\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"title\":\"SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1810.07212\",\"authors\":[{\"authorId\":\"3047890\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"}],\"doi\":\"10.1007/978-3-030-01261-8_23\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ea133d0067740902bc26a082c842d9e7ba48ecf6\",\"title\":\"Cross-Modal and Hierarchical Modeling of Video and Text\",\"url\":\"https://www.semanticscholar.org/paper/ea133d0067740902bc26a082c842d9e7ba48ecf6\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500655884\",\"name\":\"Amulya Shivanath\"},{\"authorId\":\"8079893\",\"name\":\"M. Sumana\"}],\"doi\":\"10.1109/ICCES45898.2019.9002156\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ba891acdd147336163c4ddec847f799d91b040d\",\"title\":\"Extracting Primitive Tasks from Procedural Videos using Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/3ba891acdd147336163c4ddec847f799d91b040d\",\"venue\":\"2019 International Conference on Communication and Electronics Systems (ICCES)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47837986\",\"name\":\"An Yan\"},{\"authorId\":\"47120131\",\"name\":\"X. Wang\"},{\"authorId\":\"2093485\",\"name\":\"Jiangtao Feng\"},{\"authorId\":null,\"name\":\"Lei Li\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a88e77a9574bcb20021d0dd4a4b8d729d85696f2\",\"title\":\"Beyond Monolingual Vision-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/a88e77a9574bcb20021d0dd4a4b8d729d85696f2\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1807.03480\",\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"4734949\",\"name\":\"Suraj Nair\"},{\"authorId\":\"2068265\",\"name\":\"Danfei Xu\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"1873736\",\"name\":\"Animesh Garg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2019.00876\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5df5561b55ab872f2b3df559ddd475299f660b42\",\"title\":\"Neural Task Graphs: Generalizing to Unseen Tasks From a Single Video Demonstration\",\"url\":\"https://www.semanticscholar.org/paper/5df5561b55ab872f2b3df559ddd475299f660b42\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114498698\",\"name\":\"Ankush Manocha\"},{\"authorId\":\"50631862\",\"name\":\"R. Singh\"}],\"doi\":\"10.1007/s11042-019-7700-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad6493b57050955d9686b3c1fc84a19195852f8c\",\"title\":\"Computer vision based working environment monitoring to analyze Generalized Anxiety Disorder (GAD)\",\"url\":\"https://www.semanticscholar.org/paper/ad6493b57050955d9686b3c1fc84a19195852f8c\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1910.14303\",\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"152309770\",\"name\":\"Lin Ma\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"1743698\",\"name\":\"Wenyu Liu\"},{\"authorId\":\"145583985\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1109/tpami.2020.3038993\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"613634071acd170fe5c20600f8d49662a8c3b23f\",\"title\":\"Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos\",\"url\":\"https://www.semanticscholar.org/paper/613634071acd170fe5c20600f8d49662a8c3b23f\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2004.07514\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"72643925\",\"name\":\"Minsu Cho\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR42600.2020.01082\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e3d5b20e5df692deb80d9e100e4f34c1a8f8031\",\"title\":\"Local-Global Video-Text Interactions for Temporal Grounding\",\"url\":\"https://www.semanticscholar.org/paper/7e3d5b20e5df692deb80d9e100e4f34c1a8f8031\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50506129\",\"name\":\"E. Barati\"},{\"authorId\":\"2410994\",\"name\":\"Xue-wen Chen\"}],\"doi\":\"10.1145/3343031.3351037\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cf8a3f260fbe4ee104380437cd576a556dccd290\",\"title\":\"Critic-based Attention Network for Event-based Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cf8a3f260fbe4ee104380437cd576a556dccd290\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2011.11071\",\"authors\":[{\"authorId\":\"2028596941\",\"name\":\"Andreea-Maria Oncescu\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1614034792\",\"name\":\"Yang Liu\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"title\":\"QuerYD: A video dataset with high-quality textual and audio narrations\",\"url\":\"https://www.semanticscholar.org/paper/6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5781871\",\"name\":\"Jiaqi Su\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"title\":\"Study of Video Captioning Problem\",\"url\":\"https://www.semanticscholar.org/paper/511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46900590\",\"name\":\"H. Lee\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"122704930\",\"name\":\"Paul Natsev\"}],\"doi\":\"10.1109/CVPR42600.2020.00684\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c3eed5cfc6311279a72e539037e36a375a186bff\",\"title\":\"Large Scale Video Representation Learning via Relational Graph Clustering\",\"url\":\"https://www.semanticscholar.org/paper/c3eed5cfc6311279a72e539037e36a375a186bff\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1904.09936\",\"authors\":[{\"authorId\":\"29132542\",\"name\":\"Meera Hahn\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"144177248\",\"name\":\"James M. Rehg\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1378f07a9229f09c7bf6c3be6ade4765403c0ca6\",\"title\":\"Tripping through time: Efficient Localization of Activities in Videos\",\"url\":\"https://www.semanticscholar.org/paper/1378f07a9229f09c7bf6c3be6ade4765403c0ca6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1806.08854\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"10713620\",\"name\":\"J. Qiu\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9a9c92a56e388997adb513305a4259798506b7f5\",\"title\":\"RUC+CMU: System Report for Dense Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/9a9c92a56e388997adb513305a4259798506b7f5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"121638702\",\"name\":\"Jungin Park\"},{\"authorId\":\"82536700\",\"name\":\"J. Lee\"},{\"authorId\":\"9535835\",\"name\":\"Sangryul Jeon\"},{\"authorId\":\"144442279\",\"name\":\"K. Sohn\"}],\"doi\":\"10.1109/ICCVW.2019.00193\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62d6e66c6a97540064c3de51b455cdc8fd7f0bdc\",\"title\":\"Video Summarization by Learning Relationships between Action and Scene\",\"url\":\"https://www.semanticscholar.org/paper/62d6e66c6a97540064c3de51b455cdc8fd7f0bdc\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":\"1809.09294\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":null,\"name\":\"Zhuang Liu\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/TPAMI.2019.2922181\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1c98035c32e783b7450f1779af572a65e6fc23f\",\"title\":\"Object Detection from Scratch with Deep Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f1c98035c32e783b7450f1779af572a65e6fc23f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1909.05316\",\"authors\":[{\"authorId\":\"145919382\",\"name\":\"J. Hu\"},{\"authorId\":\"145215470\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"32556571\",\"name\":\"J. Liu\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1700325\",\"name\":\"Graham Neubig\"}],\"doi\":\"10.1609/AAAI.V34I05.6305\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f416f27ba8ae0be32bb4c9a3a50995965a09c449\",\"title\":\"What Makes A Good Story? Designing Composite Rewards for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/f416f27ba8ae0be32bb4c9a3a50995965a09c449\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49120765\",\"name\":\"R. Singh\"},{\"authorId\":\"1387052806\",\"name\":\"Ankur Sonawane\"},{\"authorId\":\"33188415\",\"name\":\"R. Srivastava\"}],\"doi\":\"10.1007/s00530-019-00635-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45a7a721cfe3f179c5bf86ae08340b1f38fdde48\",\"title\":\"Recent evolution of modern datasets for human activity recognition: a deep survey\",\"url\":\"https://www.semanticscholar.org/paper/45a7a721cfe3f179c5bf86ae08340b1f38fdde48\",\"venue\":\"Multimedia Systems\",\"year\":2019},{\"arxivId\":\"2011.07735\",\"authors\":[{\"authorId\":\"40016108\",\"name\":\"Aman Chadha\"},{\"authorId\":\"2025073690\",\"name\":\"Gurneet Arora\"},{\"authorId\":\"2025065763\",\"name\":\"Navpreet Kaloty\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"title\":\"iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"BiGRU BiGRU\"},{\"authorId\":null,\"name\":\"BiGRU\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd8a531ddff2ba788d02555af04632c46f31ecf0\",\"title\":\"Multi-Stage Cross-modal Interaction Module d ) Moment Retrieval Module q \\\" q # q $ q ) Query\",\"url\":\"https://www.semanticscholar.org/paper/dd8a531ddff2ba788d02555af04632c46f31ecf0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1491414917\",\"name\":\"Xin Yan\"},{\"authorId\":\"4004957\",\"name\":\"W. Cheng\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.1145/3366710\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"title\":\"Multichannel Attention Refinement for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7576095\",\"name\":\"Zikai Song\"},{\"authorId\":\"4210401\",\"name\":\"Junqing Yu\"},{\"authorId\":\"51048063\",\"name\":\"Hengyou Cai\"},{\"authorId\":\"15429809\",\"name\":\"Yangliu Hu\"},{\"authorId\":\"12650221\",\"name\":\"Y. Chen\"}],\"doi\":\"10.1007/978-3-030-37731-1_42\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"59c5b9e22d093811ca44959e66d14cae0928c737\",\"title\":\"Fine-Grain Level Sports Video Search Engine\",\"url\":\"https://www.semanticscholar.org/paper/59c5b9e22d093811ca44959e66d14cae0928c737\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":\"1909.09944\",\"authors\":[{\"authorId\":\"49172303\",\"name\":\"T. Rahman\"},{\"authorId\":\"3443241\",\"name\":\"Bicheng Xu\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00900\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7a2de516a4e628a30036193d71faac7240d553ef\",\"title\":\"Watch, Listen and Tell: Multi-Modal Weakly Supervised Dense Event Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a2de516a4e628a30036193d71faac7240d553ef\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"48093314\",\"name\":\"Jing-Wen Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3394171.3413908\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"title\":\"Controllable Video Captioning with an Exemplar Sentence\",\"url\":\"https://www.semanticscholar.org/paper/40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2009.08043\",\"authors\":[{\"authorId\":\"46207897\",\"name\":\"Seonhoon Kim\"},{\"authorId\":\"1946727719\",\"name\":\"Seohyeong Jeong\"},{\"authorId\":\"93705260\",\"name\":\"Eun-Byul Kim\"},{\"authorId\":\"34693670\",\"name\":\"Inho Kang\"},{\"authorId\":\"71494716\",\"name\":\"Nojun Kwak\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"626f0d48747f919be2d282cca125f8ded96e500b\",\"title\":\"Self-supervised pre-training and contrastive representation learning for multiple-choice video QA\",\"url\":\"https://www.semanticscholar.org/paper/626f0d48747f919be2d282cca125f8ded96e500b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47795983\",\"name\":\"Yu-Lan Yang\"},{\"authorId\":\"51484605\",\"name\":\"Z. Li\"},{\"authorId\":\"1519001806\",\"name\":\"Gangyan Zeng\"}],\"doi\":\"10.1109/ICCST50977.2020.00123\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"895fdebe3a2584fa87ee7da9eea47f131c09375f\",\"title\":\"A Survey of Temporal Activity Localization via Language in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/895fdebe3a2584fa87ee7da9eea47f131c09375f\",\"venue\":\"2020 International Conference on Culture-oriented Science & Technology (ICCST)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1581863540\",\"name\":\"Aidean Sharghi Karganroodi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"550b08b659d3b8e7f45bdc09602af2184791d082\",\"title\":\"Visual-Textual Video Synopsis Generation\",\"url\":\"https://www.semanticscholar.org/paper/550b08b659d3b8e7f45bdc09602af2184791d082\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2006.11693\",\"authors\":[{\"authorId\":\"1563987322\",\"name\":\"Teng Wang\"},{\"authorId\":\"39458374\",\"name\":\"H. Zheng\"},{\"authorId\":\"47730643\",\"name\":\"Mingjing Yu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d2383f16ecc732601af6c3929e8a3abfca193d87\",\"title\":\"Dense-Captioning Events in Videos: SYSU Submission to ActivityNet Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/d2383f16ecc732601af6c3929e8a3abfca193d87\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1805.08191\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"38504661\",\"name\":\"J. Wang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":\"10.1609/aaai.v33i01.33018465\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b02822cfbc50d17ec5220a19556be9d601c132\",\"title\":\"Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation\",\"url\":\"https://www.semanticscholar.org/paper/c2b02822cfbc50d17ec5220a19556be9d601c132\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2006.07896\",\"authors\":[{\"authorId\":\"40607664\",\"name\":\"Yuqing Song\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8c9c256c33ed4db6b83321c516025b1feb62ddfb\",\"title\":\"Team RUC_AIM3 Technical Report at Activitynet 2020 Task 2: Exploring Sequential Events Detection for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/8c9c256c33ed4db6b83321c516025b1feb62ddfb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.01403\",\"authors\":[{\"authorId\":\"98579574\",\"name\":\"Daizong Liu\"},{\"authorId\":\"51912474\",\"name\":\"Xiaoye Qu\"},{\"authorId\":\"4029028\",\"name\":\"Xiao-Yang Liu\"},{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"},{\"authorId\":\"2956815\",\"name\":\"Zichuan Xu\"}],\"doi\":\"10.1145/3394171.3414026\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2b632209923bfe3452ac19f23b46c70455fae465\",\"title\":\"Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization\",\"url\":\"https://www.semanticscholar.org/paper/2b632209923bfe3452ac19f23b46c70455fae465\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.14598\",\"authors\":[{\"authorId\":\"1753647133\",\"name\":\"Chen Zhao\"},{\"authorId\":\"1872964\",\"name\":\"Ali K. Thabet\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b19e442f6d313c211b522a791252de2c2468063b\",\"title\":\"Video Self-Stitching Graph Network for Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/b19e442f6d313c211b522a791252de2c2468063b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1711.06330\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":\"10.1109/CVPR.2018.00710\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"66aebb3af16aaa78579344784212ae10f60ec27e\",\"title\":\"Attend and Interact: Higher-Order Object Interactions for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/66aebb3af16aaa78579344784212ae10f60ec27e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2011.11479\",\"authors\":[{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"22314218\",\"name\":\"Silvio Giancola\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5388388db25f0d40ef6612333a0279373f8dddcf\",\"title\":\"TSP: Temporally-Sensitive Pretraining of Video Encoders for Localization Tasks\",\"url\":\"https://www.semanticscholar.org/paper/5388388db25f0d40ef6612333a0279373f8dddcf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2007565041\",\"name\":\"Vighnesh Reddy Konda\"},{\"authorId\":\"2007593167\",\"name\":\"Mayur Warialani\"},{\"authorId\":\"2007595554\",\"name\":\"Rakesh Prasanth Achari\"},{\"authorId\":\"2006916144\",\"name\":\"Varad Bhatnagar\"},{\"authorId\":\"2006759688\",\"name\":\"Jayaprakash Akula\"},{\"authorId\":\"1409054881\",\"name\":\"P. Jyothi\"},{\"authorId\":\"1859063226\",\"name\":\"Ganesh Ramakrishnan\"},{\"authorId\":\"2561045\",\"name\":\"Gholamreza Haffari\"},{\"authorId\":\"48101909\",\"name\":\"Pankaj Singh\"}],\"doi\":\"10.21437/interspeech.2020-3157\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8582e5f71abb5b99c2c71e706ae3b50386286b6f\",\"title\":\"Caption Alignment for Low Resource Audio-Visual Data\",\"url\":\"https://www.semanticscholar.org/paper/8582e5f71abb5b99c2c71e706ae3b50386286b6f\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"1901.06829\",\"authors\":[{\"authorId\":\"2192303\",\"name\":\"D. He\"},{\"authorId\":\"1749527\",\"name\":\"Xiang Zhao\"},{\"authorId\":\"2864855\",\"name\":\"Jizhou Huang\"},{\"authorId\":\"50984378\",\"name\":\"F. Li\"},{\"authorId\":\"48033101\",\"name\":\"Xiao Liu\"},{\"authorId\":\"35247507\",\"name\":\"Shilei Wen\"}],\"doi\":\"10.1609/aaai.v33i01.33018393\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6fd7e8aa1a031923e3580752e4ab9163e45fe41c\",\"title\":\"Read, Watch, and Move: Reinforcement Learning for Temporally Grounding Natural Language Descriptions in Videos\",\"url\":\"https://www.semanticscholar.org/paper/6fd7e8aa1a031923e3580752e4ab9163e45fe41c\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2005.00200\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1664725279\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1520007550\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"title\":\"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2011.11760\",\"authors\":[{\"authorId\":\"24040986\",\"name\":\"Gabriel Huang\"},{\"authorId\":\"1560385163\",\"name\":\"Bo Pang\"},{\"authorId\":\"2062703\",\"name\":\"Z. Zhu\"},{\"authorId\":\"66193113\",\"name\":\"C. Rivera\"},{\"authorId\":\"1737285\",\"name\":\"Radu Soricut\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"title\":\"Multimodal Pretraining for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2bacd2f2a70d756f108ad889b6bcddc79cc1ce51\",\"venue\":\"AACL/IJCNLP\",\"year\":2020},{\"arxivId\":\"1908.00707\",\"authors\":[{\"authorId\":\"3235708\",\"name\":\"Guoqiang Gong\"},{\"authorId\":\"9693996\",\"name\":\"Liangfeng Zheng\"},{\"authorId\":\"144654776\",\"name\":\"Kun Bai\"},{\"authorId\":\"145353089\",\"name\":\"Y. Mu\"}],\"doi\":\"10.1109/icme46284.2020.9102850\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bba21a50bdd896cc4ebddcb8b6ce807f4a26287\",\"title\":\"Scale Matters: Temporal Scale Aggregation Network For Precise Action Localization In Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/3bba21a50bdd896cc4ebddcb8b6ce807f4a26287\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"1905.07830\",\"authors\":[{\"authorId\":\"2545335\",\"name\":\"Rowan Zellers\"},{\"authorId\":\"14487640\",\"name\":\"Ari Holtzman\"},{\"authorId\":\"3312309\",\"name\":\"Yonatan Bisk\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/P19-1472\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad\",\"title\":\"HellaSwag: Can a Machine Really Finish Your Sentence?\",\"url\":\"https://www.semanticscholar.org/paper/8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1803.00057\",\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":\"10.1109/CVPR.2018.00912\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"title\":\"A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1909.00239\",\"authors\":[{\"authorId\":\"1759094\",\"name\":\"Mingfei Gao\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.18653/v1/D19-1157\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a758828d2865592fb7ee0c95fe4d2517cf405196\",\"title\":\"WSLLN: Weakly Supervised Natural Language Localization Networks\",\"url\":\"https://www.semanticscholar.org/paper/a758828d2865592fb7ee0c95fe4d2517cf405196\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1804.07014\",\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1609/aaai.v33i01.33019159\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"31bb920739f22b4865161f75692785decfea470c\",\"title\":\"To Find Where You Talk: Temporal Sentence Localization in Video with Attention Based Location Regression\",\"url\":\"https://www.semanticscholar.org/paper/31bb920739f22b4865161f75692785decfea470c\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1591133899\",\"name\":\"Yiwei Zhang\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3351094\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c0b12c784965baac88b6597890303fa834fa9eea\",\"title\":\"Watch, Reason and Code: Learning to Represent Videos Using Program\",\"url\":\"https://www.semanticscholar.org/paper/c0b12c784965baac88b6597890303fa834fa9eea\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1906.02497\",\"authors\":[{\"authorId\":\"1764508\",\"name\":\"Z. Zhang\"},{\"authorId\":\"34064720\",\"name\":\"Zhijie Lin\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"123034558\",\"name\":\"Z. Xiao\"}],\"doi\":\"10.1145/3331184.3331235\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fb53803897d3df3e1f43a43a753ee88a64517c47\",\"title\":\"Cross-Modal Interaction Networks for Query-Based Moment Retrieval in Videos\",\"url\":\"https://www.semanticscholar.org/paper/fb53803897d3df3e1f43a43a753ee88a64517c47\",\"venue\":\"SIGIR\",\"year\":2019},{\"arxivId\":\"2010.03403\",\"authors\":[{\"authorId\":\"1490652152\",\"name\":\"Jiwei Wei\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1524912498\",\"name\":\"Yang Yang\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":null,\"name\":\"Zheng Wang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/CVPR42600.2020.01302\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"title\":\"Universal Weighting Metric Learning for Cross-Modal Matching\",\"url\":\"https://www.semanticscholar.org/paper/dd860c3f8a195d06f64ecf36ef6a78397eb883bd\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1909.05010\",\"authors\":[{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"}],\"doi\":\"10.1609/AAAI.V34I07.6897\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"aa6c925c5f2fe61cdf8e09dd4ecbf1e7d220eac0\",\"title\":\"Temporally Grounding Language Queries in Videos by Contextual Boundary-aware Prediction\",\"url\":\"https://www.semanticscholar.org/paper/aa6c925c5f2fe61cdf8e09dd4ecbf1e7d220eac0\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1706.04261\",\"authors\":[{\"authorId\":\"38962424\",\"name\":\"Raghav Goyal\"},{\"authorId\":\"3127597\",\"name\":\"S. Kahou\"},{\"authorId\":\"1748421\",\"name\":\"Vincent Michalski\"},{\"authorId\":\"7654960\",\"name\":\"Joanna Materzynska\"},{\"authorId\":\"12929417\",\"name\":\"S. Westphal\"},{\"authorId\":\"2233986\",\"name\":\"Heuna Kim\"},{\"authorId\":\"7241984\",\"name\":\"V. Haenel\"},{\"authorId\":\"47544625\",\"name\":\"Ingo Fr\\u00fcnd\"},{\"authorId\":\"19265538\",\"name\":\"Peter Yianilos\"},{\"authorId\":\"1414405239\",\"name\":\"Moritz Mueller-Freitag\"},{\"authorId\":\"143931146\",\"name\":\"F. Hoppe\"},{\"authorId\":\"2020614\",\"name\":\"Christian Thurau\"},{\"authorId\":\"2443288\",\"name\":\"I. Bax\"},{\"authorId\":\"1710604\",\"name\":\"R. Memisevic\"}],\"doi\":\"10.1109/ICCV.2017.622\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b68811a9b5cafe4795a11c1048541750068b7ad0\",\"title\":\"The \\u201cSomething Something\\u201d Video Database for Learning and Evaluating Visual Common Sense\",\"url\":\"https://www.semanticscholar.org/paper/b68811a9b5cafe4795a11c1048541750068b7ad0\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1908.06327\",\"authors\":[{\"authorId\":\"38727845\",\"name\":\"A. Burns\"},{\"authorId\":\"73441526\",\"name\":\"R. Tan\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"}],\"doi\":\"10.1109/ICCV.2019.00757\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71634edd80ce000b3d1e462137fcfa8c2b377943\",\"title\":\"Language Features Matter: Effective Language Representations for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/71634edd80ce000b3d1e462137fcfa8c2b377943\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"49418159\",\"name\":\"Y. Wang\"},{\"authorId\":\"145546921\",\"name\":\"J. Qin\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/TCSVT.2019.2894161\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c75881e226b5d57e1c5570bf8e51a93bdada9e8\",\"title\":\"stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6c75881e226b5d57e1c5570bf8e51a93bdada9e8\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34935264\",\"name\":\"Chiradeep Roy\"},{\"authorId\":\"82348650\",\"name\":\"Mahesh Shanbhag\"},{\"authorId\":\"50165674\",\"name\":\"M. Nourani\"},{\"authorId\":\"34108618\",\"name\":\"Tahrima Rahman\"},{\"authorId\":\"18123696\",\"name\":\"Samia Kabir\"},{\"authorId\":\"143754991\",\"name\":\"V. Gogate\"},{\"authorId\":\"2273098\",\"name\":\"N. Ruozzi\"},{\"authorId\":\"1777991\",\"name\":\"Eric D. Ragan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49f9ce554c738aa125766b1e036f35d33bb61727\",\"title\":\"Explainable Activity Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/49f9ce554c738aa125766b1e036f35d33bb61727\",\"venue\":\"IUI Workshops\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49551061\",\"name\":\"Shubham Singh\"},{\"authorId\":\"40339392\",\"name\":\"R. Kaushal\"},{\"authorId\":\"3170352\",\"name\":\"A. Buduru\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":\"10.1145/3297280.3297487\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a9bfb8ea06d011900d7facc93ee77e5733b8870\",\"title\":\"KidsGUARD: fine grained approach for child unsafe video representation and detection\",\"url\":\"https://www.semanticscholar.org/paper/6a9bfb8ea06d011900d7facc93ee77e5733b8870\",\"venue\":\"SAC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41016725\",\"name\":\"Thomas Kipf\"},{\"authorId\":\"47002813\",\"name\":\"Yujia Li\"},{\"authorId\":\"2791430\",\"name\":\"Hanjun Dai\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"1398105826\",\"name\":\"Alvaro Sanchez-Gonzalez\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"2019153\",\"name\":\"P. Battaglia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fbf03bf621ffee283911e765d525a75fc0d11bae\",\"title\":\"CompILE: Compositional Imitation Learning and Execution\",\"url\":\"https://www.semanticscholar.org/paper/fbf03bf621ffee283911e765d525a75fc0d11bae\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2007876320\",\"name\":\"Ngoc Phuoc An Vo\"},{\"authorId\":\"2873078\",\"name\":\"Irene Manotas\"},{\"authorId\":\"1757683\",\"name\":\"V. Sheinin\"},{\"authorId\":\"1698656\",\"name\":\"O. Popescu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c8946a24452394e8710e7aac3fdd11b45e7d899\",\"title\":\"Identifying Motion Entities in Natural Language and A Case Study for Named Entity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5c8946a24452394e8710e7aac3fdd11b45e7d899\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1711.06666\",\"authors\":[{\"authorId\":\"9085797\",\"name\":\"Keren Ye\"},{\"authorId\":\"1770205\",\"name\":\"Adriana Kovashka\"}],\"doi\":\"10.1007/978-3-030-01267-0_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1be2c16060974f66e5366872ebbee21325075e8\",\"title\":\"ADVISE: Symbolism and External Knowledge for Decoding Advertisements\",\"url\":\"https://www.semanticscholar.org/paper/e1be2c16060974f66e5366872ebbee21325075e8\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1711.11135\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/CVPR.2018.00443\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"74b284a66e75b65f5970d05bac000fe91243ee49\",\"title\":\"Video Captioning via Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/74b284a66e75b65f5970d05bac000fe91243ee49\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2005.03356\",\"authors\":[{\"authorId\":\"117172343\",\"name\":\"Seongho Choi\"},{\"authorId\":\"2943489\",\"name\":\"Kyoung-Woon On\"},{\"authorId\":\"15353659\",\"name\":\"Yu-Jung Heo\"},{\"authorId\":\"1679974562\",\"name\":\"Ahjeong Seo\"},{\"authorId\":\"1680054988\",\"name\":\"Youwon Jang\"},{\"authorId\":\"153311117\",\"name\":\"Seungchan Lee\"},{\"authorId\":\"1491775096\",\"name\":\"Minsu Lee\"},{\"authorId\":\"152705134\",\"name\":\"B. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d21241b930b005847cf4350294c61d6c29ccd9f\",\"title\":\"DramaQA: Character-Centered Video Story Understanding with Hierarchical QA\",\"url\":\"https://www.semanticscholar.org/paper/4d21241b930b005847cf4350294c61d6c29ccd9f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.04208\",\"authors\":[{\"authorId\":\"153000035\",\"name\":\"M. Bain\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"152853748\",\"name\":\"A. Brown\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"239c34ab213229725c6428e63b1315f2e8cdcbc8\",\"title\":\"Condensed Movies: Story Based Retrieval with Contextual Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/239c34ab213229725c6428e63b1315f2e8cdcbc8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1910.11301\",\"authors\":[{\"authorId\":\"47837986\",\"name\":\"An Yan\"},{\"authorId\":\"72541452\",\"name\":\"X. Wang\"},{\"authorId\":\"2093485\",\"name\":\"Jiangtao Feng\"},{\"authorId\":null,\"name\":\"Lei Li\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a273479d5ede7a27cf144a18cfbf0542b92fa12\",\"title\":\"Cross-Lingual Vision-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/2a273479d5ede7a27cf144a18cfbf0542b92fa12\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"506ea19145838a035e7dba535519fb40a3a0018c\",\"title\":\"Learning Shared Multimodal Embeddings with Unpaired Data\",\"url\":\"https://www.semanticscholar.org/paper/506ea19145838a035e7dba535519fb40a3a0018c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1910.11009\",\"authors\":[{\"authorId\":\"47424372\",\"name\":\"Yu Xiong\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":\"10357054\",\"name\":\"L. Guo\"},{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/ICCV.2019.00469\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"87699cff38982712ddb0b2349313077779a5d0ff\",\"title\":\"A Graph-Based Framework to Bridge Movies and Synopses\",\"url\":\"https://www.semanticscholar.org/paper/87699cff38982712ddb0b2349313077779a5d0ff\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"1926578\",\"name\":\"Joon-Young Lee\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1007/978-3-030-01252-6_13\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9258ae3ad05e77555d3459dfbcfa9c70440c5f88\",\"title\":\"What Do I Annotate Next? An Empirical Study of Active Learning for Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/9258ae3ad05e77555d3459dfbcfa9c70440c5f88\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2009.08614\",\"authors\":[{\"authorId\":\"2017491\",\"name\":\"J. Wu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"50016941\",\"name\":\"Xiao-Guang Han\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":\"10.1145/3394171.3413862\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"949b698f4aaaeea923d451db8175c5b464520f27\",\"title\":\"Reinforcement Learning for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/949b698f4aaaeea923d451db8175c5b464520f27\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49941672\",\"name\":\"Zhiming Hu\"},{\"authorId\":\"50699057\",\"name\":\"Ning Ye\"},{\"authorId\":\"48976987\",\"name\":\"C. Phillips\"},{\"authorId\":\"49772724\",\"name\":\"Tim Capes\"},{\"authorId\":\"1703622\",\"name\":\"I. Mohomed\"}],\"doi\":\"10.1145/3429357.3430518\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9e2e00fdee406eb9a4f7b341a3be5866c8f99822\",\"title\":\"mmFilter: Language-Guided Video Analytics at the Edge\",\"url\":\"https://www.semanticscholar.org/paper/9e2e00fdee406eb9a4f7b341a3be5866c8f99822\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.07048\",\"authors\":[{\"authorId\":\"2250163\",\"name\":\"Yijun Song\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"152309770\",\"name\":\"Lin Ma\"},{\"authorId\":\"1564034697\",\"name\":\"Zhou Yu\"},{\"authorId\":null,\"name\":\"Jun Yu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e655c524630b0fb37f11b01468dab5477c58db0f\",\"title\":\"Weakly-Supervised Multi-Level Attentional Reconstruction Network for Grounding Textual Queries in Videos\",\"url\":\"https://www.semanticscholar.org/paper/e655c524630b0fb37f11b01468dab5477c58db0f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1809.01696\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.18653/v1/D18-1167\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e7e1313061b0d56364bd2c41f017deb954bb05db\",\"title\":\"TVQA: Localized, Compositional Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/e7e1313061b0d56364bd2c41f017deb954bb05db\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47243105\",\"name\":\"P. Zhang\"},{\"authorId\":\"9308544\",\"name\":\"Chunmiao Yuan\"},{\"authorId\":\"3386073\",\"name\":\"Kunliang Liu\"},{\"authorId\":\"46676502\",\"name\":\"Y. Sun\"},{\"authorId\":\"2607232\",\"name\":\"Jiayu Liang\"},{\"authorId\":\"151470674\",\"name\":\"Guanghao Jin\"},{\"authorId\":\"46584793\",\"name\":\"J. Wang\"}],\"doi\":\"10.1007/978-3-030-35231-8_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"801a3ada495732f95b34bce187de9dd41a40d10d\",\"title\":\"Fast Video Clip Retrieval Method via Language Query\",\"url\":\"https://www.semanticscholar.org/paper/801a3ada495732f95b34bce187de9dd41a40d10d\",\"venue\":\"ADMA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2873078\",\"name\":\"Irene Manotas\"},{\"authorId\":\"2007876320\",\"name\":\"Ngoc Phuoc An Vo\"},{\"authorId\":\"1757683\",\"name\":\"V. Sheinin\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.88\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"00d479b957f87fd3328e6ba8b9f23a4e5ccbf56e\",\"title\":\"LiMiT: The Literal Motion in Text Dataset\",\"url\":\"https://www.semanticscholar.org/paper/00d479b957f87fd3328e6ba8b9f23a4e5ccbf56e\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2003.11618\",\"authors\":[{\"authorId\":\"48211835\",\"name\":\"J. Liu\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"35729970\",\"name\":\"Yiming Yang\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":\"10.1109/cvpr42600.2020.01091\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"704ec27b8399df574a96da338c428a923509385e\",\"title\":\"Violin: A Large-Scale Dataset for Video-and-Language Inference\",\"url\":\"https://www.semanticscholar.org/paper/704ec27b8399df574a96da338c428a923509385e\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Weiying Wang\"},{\"authorId\":null,\"name\":\"Yongcheng Wang\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.18653/v1/D19-1517\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"34e7db6ffcdfeffc29eab0622384ed412d9b4558\",\"title\":\"YouMakeup: A Large-Scale Domain-Specific Multimodal Dataset for Fine-Grained Semantic Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/34e7db6ffcdfeffc29eab0622384ed412d9b4558\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1910.02602\",\"authors\":[{\"authorId\":\"1383481973\",\"name\":\"Yan Bin Ng\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42b7b4bb2f96b21c33541a83606917be5eb6abbb\",\"title\":\"Human Action Sequence Classification\",\"url\":\"https://www.semanticscholar.org/paper/42b7b4bb2f96b21c33541a83606917be5eb6abbb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1803.10699\",\"authors\":[{\"authorId\":\"144529493\",\"name\":\"Li Ding\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1109/CVPR.2018.00681\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c6ce420976f958e7582a2f452c3a541faa82074\",\"title\":\"Weakly-Supervised Action Segmentation with Iterative Soft Boundary Assignment\",\"url\":\"https://www.semanticscholar.org/paper/6c6ce420976f958e7582a2f452c3a541faa82074\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2004.13931\",\"authors\":[{\"authorId\":\"153504695\",\"name\":\"Hao Zhang\"},{\"authorId\":\"1735962\",\"name\":\"Aixin Sun\"},{\"authorId\":\"1492128584\",\"name\":\"Wei Jing\"},{\"authorId\":\"10638646\",\"name\":\"Joey Tianyi Zhou\"}],\"doi\":\"10.18653/v1/2020.acl-main.585\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5a975dcd3dba2a11830e5595d4c4659441cb6836\",\"title\":\"Span-based Localizing Network for Natural Language Video Localization\",\"url\":\"https://www.semanticscholar.org/paper/5a975dcd3dba2a11830e5595d4c4659441cb6836\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"93353108\",\"name\":\"Cheng Chen\"},{\"authorId\":\"1649999106\",\"name\":\"Xiaodong Gu\"}],\"doi\":\"10.1007/978-3-030-64221-1_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6b6e4860665d9185dfd80af63d93334582c56be\",\"title\":\"Semantic Modulation Based Residual Network for Temporal Language Queries Grounding in Video\",\"url\":\"https://www.semanticscholar.org/paper/c6b6e4860665d9185dfd80af63d93334582c56be\",\"venue\":\"ISNN\",\"year\":2020},{\"arxivId\":\"2009.01039\",\"authors\":[{\"authorId\":\"40974493\",\"name\":\"Alessio Sarullo\"},{\"authorId\":\"47279171\",\"name\":\"Tingting Mu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c945cff9a378aca8ebe5659afc8a0338010770df\",\"title\":\"Zero-Shot Human-Object Interaction Recognition via Affordance Graphs\",\"url\":\"https://www.semanticscholar.org/paper/c945cff9a378aca8ebe5659afc8a0338010770df\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00375\",\"authors\":[{\"authorId\":\"152985547\",\"name\":\"Z. Li\"},{\"authorId\":\"40560502\",\"name\":\"W. Wang\"},{\"authorId\":\"121544228\",\"name\":\"Z. Li\"},{\"authorId\":\"48355651\",\"name\":\"Yifei Huang\"},{\"authorId\":\"2003804019\",\"name\":\"Yoichi Sato\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9338b7de4b849cb094aa4cbd5b85f9935a4ae00\",\"title\":\"Towards Visually Explaining Video Understanding Networks with Perturbation\",\"url\":\"https://www.semanticscholar.org/paper/b9338b7de4b849cb094aa4cbd5b85f9935a4ae00\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.06992\",\"authors\":[{\"authorId\":\"2547290\",\"name\":\"Jingwei Ji\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/cvpr42600.2020.01025\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1242ba8fdb994b82a0575dc92f30f7b26a75707\",\"title\":\"Action Genome: Actions As Compositions of Spatio-Temporal Scene Graphs\",\"url\":\"https://www.semanticscholar.org/paper/d1242ba8fdb994b82a0575dc92f30f7b26a75707\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1703.09788\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e10a5e0baf2aa87d804795af071808a9377cc80a\",\"title\":\"Towards Automatic Learning of Procedures From Web Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/e10a5e0baf2aa87d804795af071808a9377cc80a\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"}],\"doi\":\"10.1007/978-3-030-00764-5_8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2036b394a5dff537df48d6db62a0d61491c92046\",\"title\":\"iMakeup: Makeup Instructional Video Dataset for Fine-Grained Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2036b394a5dff537df48d6db62a0d61491c92046\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":\"1806.08251\",\"authors\":[{\"authorId\":\"8797855\",\"name\":\"A. Piergiovanni\"},{\"authorId\":\"1766489\",\"name\":\"M. Ryoo\"}],\"doi\":\"10.1109/WACV45572.2020.9093612\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dbbcf8af70db7533f76e8e55f108fcc6af6e0c0e\",\"title\":\"Learning Multimodal Representations for Unseen Activities\",\"url\":\"https://www.semanticscholar.org/paper/dbbcf8af70db7533f76e8e55f108fcc6af6e0c0e\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2011.00597\",\"authors\":[{\"authorId\":\"2007582232\",\"name\":\"Simon Ging\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"1835025\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"80089ad641bae28b0e57771afef181b60011069e\",\"title\":\"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/80089ad641bae28b0e57771afef181b60011069e\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3232074\",\"name\":\"L. Pishdad\"},{\"authorId\":\"2364338\",\"name\":\"Federico Fancellu\"},{\"authorId\":\"2019013527\",\"name\":\"Ran Zhang\"},{\"authorId\":\"1775745\",\"name\":\"A. Fazly\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"890f4892d9fc6856556ad8ee16e8e3916affe084\",\"title\":\"How coherent are neural models of coherence?\",\"url\":\"https://www.semanticscholar.org/paper/890f4892d9fc6856556ad8ee16e8e3916affe084\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1804.08274\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2018.00782\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"title\":\"Jointly Localizing and Describing Events for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27575517\",\"name\":\"Dian Shao\"},{\"authorId\":\"145984817\",\"name\":\"Yu Xiong\"},{\"authorId\":\"47827957\",\"name\":\"Y. Zhao\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01240-3_13\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9925eb898e7484ae289b675785313ddb47bb20bd\",\"title\":\"Find and Focus: Retrieve and Localize Video Events with Natural Language Queries\",\"url\":\"https://www.semanticscholar.org/paper/9925eb898e7484ae289b675785313ddb47bb20bd\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"48116710\",\"name\":\"Suraj Nair\"},{\"authorId\":\"2068265\",\"name\":\"Danfei Xu\"},{\"authorId\":null,\"name\":\"Yuke Zhu\"},{\"authorId\":null,\"name\":\"Animesh Garg\"},{\"authorId\":\"3216322\",\"name\":\"Li Fei-Fei\"},{\"authorId\":null,\"name\":\"Silvio Savarese\"},{\"authorId\":null,\"name\":\"Juan Carlos Niebles\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e15d8e8d14c4ff75c617cdd80f0cc88159b36cbb\",\"title\":\"NTG Generator Demo Interpreter Graph Completion Network NTG Execution Engine Conjugate Task Graph Visual Observation ( o ) Node Localizer Edge Classifier Action ( a ) Demonstration\",\"url\":\"https://www.semanticscholar.org/paper/e15d8e8d14c4ff75c617cdd80f0cc88159b36cbb\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1812.01483\",\"authors\":[{\"authorId\":\"41016725\",\"name\":\"Thomas Kipf\"},{\"authorId\":\"47002813\",\"name\":\"Yujia Li\"},{\"authorId\":\"2791430\",\"name\":\"Hanjun Dai\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"Pushmeet Kohli\"},{\"authorId\":\"2019153\",\"name\":\"P. Battaglia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"683599f260a877fef5e97a643852b854ae3db9a1\",\"title\":\"Compositional Imitation Learning: Explaining and executing one task at a time\",\"url\":\"https://www.semanticscholar.org/paper/683599f260a877fef5e97a643852b854ae3db9a1\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2007.10639\",\"authors\":[{\"authorId\":\"151352107\",\"name\":\"Valentin Gabeur\"},{\"authorId\":\"1491624845\",\"name\":\"Chen Sun\"},{\"authorId\":\"72492981\",\"name\":\"Alahari Karteek\"},{\"authorId\":\"153433844\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1007/978-3-030-58548-8_13\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6871f6c5437a747fae75a19962f418d234ce2dc1\",\"title\":\"Multi-modal Transformer for Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6871f6c5437a747fae75a19962f418d234ce2dc1\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.06647\",\"authors\":[{\"authorId\":\"153634296\",\"name\":\"Matthew Hutchinson\"},{\"authorId\":\"74882299\",\"name\":\"Vijay Gadepally\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"65955a106905afb90a2a2fa74e48c6d6d597892f\",\"title\":\"Video Action Understanding: A Tutorial\",\"url\":\"https://www.semanticscholar.org/paper/65955a106905afb90a2a2fa74e48c6d6d597892f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.03545\",\"authors\":[{\"authorId\":\"147992804\",\"name\":\"Runhao Zeng\"},{\"authorId\":\"2342663\",\"name\":\"H. Xu\"},{\"authorId\":\"51026885\",\"name\":\"W. Huang\"},{\"authorId\":\"4965440\",\"name\":\"Peihao Chen\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"}],\"doi\":\"10.1109/cvpr42600.2020.01030\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"355403d7ce4b625307fd3ebb2beea269ecc15213\",\"title\":\"Dense Regression Network for Video Grounding\",\"url\":\"https://www.semanticscholar.org/paper/355403d7ce4b625307fd3ebb2beea269ecc15213\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1910.06426\",\"authors\":[{\"authorId\":\"23999166\",\"name\":\"Shuangjie Xu\"},{\"authorId\":\"143979421\",\"name\":\"F. Xu\"},{\"authorId\":\"153655416\",\"name\":\"Yu Cheng\"},{\"authorId\":\"145232778\",\"name\":\"Pan Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dc9b8dbd11e3786d6f8dee167a8b8353a0b8ffd1\",\"title\":\"Tell-the-difference: Fine-grained Visual Descriptor via a Discriminating Referee\",\"url\":\"https://www.semanticscholar.org/paper/dc9b8dbd11e3786d6f8dee167a8b8353a0b8ffd1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2001.04809\",\"authors\":[{\"authorId\":\"47964485\",\"name\":\"Joshua Y. Kim\"},{\"authorId\":\"1430679970\",\"name\":\"Greyson Y. Kim\"},{\"authorId\":\"1763220\",\"name\":\"K. Yacef\"}],\"doi\":\"10.1007/978-3-030-35288-2_25\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4e80234c85027ea3062c27eb27c5d9066050a39\",\"title\":\"Detecting Depression in Dyadic Conversations with Multimodal Narratives and Visualizations\",\"url\":\"https://www.semanticscholar.org/paper/e4e80234c85027ea3062c27eb27c5d9066050a39\",\"venue\":\"Australasian Conference on Artificial Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81781019\",\"name\":\"Minho Shim\"},{\"authorId\":\"40832988\",\"name\":\"Y. H. Kim\"},{\"authorId\":\"32850725\",\"name\":\"Kyungmin Kim\"},{\"authorId\":\"1754380\",\"name\":\"S. Kim\"}],\"doi\":\"10.1007/978-3-030-01267-0_25\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"041115cb5509466f7449451709387268a008aba2\",\"title\":\"Teaching Machines to Understand Baseball Games: Large-Scale Baseball Video Database for Multiple Video Understanding Tasks\",\"url\":\"https://www.semanticscholar.org/paper/041115cb5509466f7449451709387268a008aba2\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"2003.10606\",\"authors\":[{\"authorId\":\"152229386\",\"name\":\"Arka Sadhu\"},{\"authorId\":\"115727183\",\"name\":\"K. Chen\"},{\"authorId\":\"66449153\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/CVPR42600.2020.01043\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"70ea5d98141cd845d1c8131f25ddbc77f962f3d8\",\"title\":\"Video Object Grounding Using Semantic Roles in Language Description\",\"url\":\"https://www.semanticscholar.org/paper/70ea5d98141cd845d1c8131f25ddbc77f962f3d8\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1804.05113\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1609/AAAI.V33I01.33019062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"title\":\"Multilevel Language and Vision Integration for Text-to-Clip Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46314993\",\"name\":\"Wei-ying Wang\"},{\"authorId\":\"1994473516\",\"name\":\"Jieting Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3394171.3413890\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b116c8cd44a34f440f260a890e0600b61d92c262\",\"title\":\"VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation\",\"url\":\"https://www.semanticscholar.org/paper/b116c8cd44a34f440f260a890e0600b61d92c262\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97cf2d8581eb7ec0fcd2ead5410473d8440eb3a6\",\"title\":\"Supplementary Material for CVPR 2018 paper # 330\",\"url\":\"https://www.semanticscholar.org/paper/97cf2d8581eb7ec0fcd2ead5410473d8440eb3a6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"}],\"doi\":\"10.25781/KAUST-VR909\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b5c549fd9172f2d1be80cc77b0de9e90d3416463\",\"title\":\"Efficient Localization of Human Actions and Moments in Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5c549fd9172f2d1be80cc77b0de9e90d3416463\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4210401\",\"name\":\"Junqing Yu\"},{\"authorId\":\"51048125\",\"name\":\"Aiping Lei\"},{\"authorId\":\"15429809\",\"name\":\"Yangliu Hu\"}],\"doi\":\"10.1007/978-3-030-05716-9_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"168ebf77527329bc2dbd0ce82bddb9905c3aa7ee\",\"title\":\"Soccer Video Event Detection Based on Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/168ebf77527329bc2dbd0ce82bddb9905c3aa7ee\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":\"1906.02467\",\"authors\":[{\"authorId\":\"48567197\",\"name\":\"Zhou Yu\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1720236\",\"name\":\"J. Yu\"},{\"authorId\":\"144478231\",\"name\":\"T. Yu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1609/aaai.v33i01.33019127\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4f2c1af57c056102806a184517313804f66e7447\",\"title\":\"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/4f2c1af57c056102806a184517313804f66e7447\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2007.15829\",\"authors\":[{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"48708586\",\"name\":\"Zijian Wang\"},{\"authorId\":\"1852415\",\"name\":\"Zheng Zhang\"},{\"authorId\":\"51278862\",\"name\":\"Mahsa Baktashmotlagh\"}],\"doi\":\"10.1145/3394171.3413897\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c312bbdb66bd72643c57213cb167250cadb384a0\",\"title\":\"Adversarial Bipartite Graph Learning for Video Domain Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/c312bbdb66bd72643c57213cb167250cadb384a0\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1807.09986\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1007/978-3-030-01216-8_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"title\":\"Recurrent Fusion Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1912.08741\",\"authors\":[{\"authorId\":\"2531432\",\"name\":\"Diego Ortego\"},{\"authorId\":\"107621684\",\"name\":\"Eric Arazo\"},{\"authorId\":\"83107779\",\"name\":\"P. Albert\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d225b29a28b6154a90f29df8df74bed1614b2028\",\"title\":\"Towards Robust Learning with Different Label Noise Distributions\",\"url\":\"https://www.semanticscholar.org/paper/d225b29a28b6154a90f29df8df74bed1614b2028\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50987563\",\"name\":\"Sangmin Park\"},{\"authorId\":\"97243906\",\"name\":\"Young-gab Kim\"}],\"doi\":\"10.1016/j.inffus.2020.10.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65dc69953ef141871f1701d68196aa278566fc16\",\"title\":\"Survey and challenges of story generation models - A multimodal perspective with five steps: Data embedding, topic modeling, storyline generation, draft story generation, and story evaluation\",\"url\":\"https://www.semanticscholar.org/paper/65dc69953ef141871f1701d68196aa278566fc16\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2502450\",\"name\":\"C. Hsu\"},{\"authorId\":\"1583964783\",\"name\":\"Abbas Zeitoun\"},{\"authorId\":\"2141495\",\"name\":\"Guang-He Lee\"},{\"authorId\":\"1785714\",\"name\":\"D. Katabi\"},{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ab699bc0208256d01f7cf5cb1290e2632279534\",\"title\":\"Self-Supervised Learning of Appliance Usage\",\"url\":\"https://www.semanticscholar.org/paper/3ab699bc0208256d01f7cf5cb1290e2632279534\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1908.04052\",\"authors\":[{\"authorId\":\"152338671\",\"name\":\"Yiitan Yuan\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3350985\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"745242c746b6f379048f6dbdfc009181d9027a60\",\"title\":\"Sentence Specified Dynamic Video Thumbnail Generation\",\"url\":\"https://www.semanticscholar.org/paper/745242c746b6f379048f6dbdfc009181d9027a60\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2009.11232\",\"authors\":[{\"authorId\":\"1432778730\",\"name\":\"Binjie Zhang\"},{\"authorId\":\"1940342\",\"name\":\"Y. Li\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"2091174\",\"name\":\"Pin Jiang\"},{\"authorId\":\"1387190008\",\"name\":\"Ying Shan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1e645df446ccdc985b85864ac0b91b053090c14d\",\"title\":\"A Simple Yet Effective Method for Video Temporal Grounding with Cross-Modality Attention\",\"url\":\"https://www.semanticscholar.org/paper/1e645df446ccdc985b85864ac0b91b053090c14d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b47776ecc194616d5ae789357ac69b1298e47ae\",\"title\":\"Frames CNN Low-level Encoder ( Bi-LSTM ) High-level Encoder ( LSTM ) Worker Manager Internal Critic Environment segment signal goal state reward action HRL Agent context context\",\"url\":\"https://www.semanticscholar.org/paper/1b47776ecc194616d5ae789357ac69b1298e47ae\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1807.10018\",\"authors\":[{\"authorId\":\"51152390\",\"name\":\"Yilei Xiong\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01252-6_29\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"title\":\"Move Forward and Tell: A Progressive Generator of Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1912.11872\",\"authors\":[{\"authorId\":\"153040576\",\"name\":\"T. Mei\"},{\"authorId\":\"101586660\",\"name\":\"W. Zhang\"},{\"authorId\":\"48577275\",\"name\":\"Ting Yao\"}],\"doi\":\"10.1017/ATSIP.2020.10\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"title\":\"Vision and Language: from Visual Perception to Content Creation\",\"url\":\"https://www.semanticscholar.org/paper/3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc57062ad3e622f5ab074a283afcfea745a2d6cc\",\"title\":\"Team RUC AI\\u00b7M: Technical Report in Video Pentathlon Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/cc57062ad3e622f5ab074a283afcfea745a2d6cc\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.06260\",\"authors\":[{\"authorId\":\"145112187\",\"name\":\"Cristian Rodriguez-Opazo\"},{\"authorId\":\"1389646918\",\"name\":\"Edison Marrese-Taylor\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"71200893\",\"name\":\"H. Li\"},{\"authorId\":\"143685465\",\"name\":\"S. Gould\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a7177ae93ed91f3bf34875205396ec97f0873bb7\",\"title\":\"DORi: Discovering Object Relationship for Moment Localization of a Natural-Language Query in Video\",\"url\":\"https://www.semanticscholar.org/paper/a7177ae93ed91f3bf34875205396ec97f0873bb7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35687142\",\"name\":\"Daichi Horita\"},{\"authorId\":\"1681659\",\"name\":\"K. Yanai\"}],\"doi\":\"10.1007/978-3-030-41404-7_44\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d682b993762623c1fea5c91e19429df04a4c87d\",\"title\":\"SSA-GAN: End-to-End Time-Lapse Video Generation with Spatial Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d682b993762623c1fea5c91e19429df04a4c87d\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"50272388\",\"name\":\"Ioana Croitoru\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"title\":\"A hierarchical approach to vision-based language generation: from simple sentences to complex natural language\",\"url\":\"https://www.semanticscholar.org/paper/9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2003.12633\",\"authors\":[{\"authorId\":\"24339915\",\"name\":\"Davis Gilton\"},{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"145952380\",\"name\":\"R. Willett\"},{\"authorId\":\"46208708\",\"name\":\"G. Shakhnarovich\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"title\":\"Detection and Description of Change in Visual Streams\",\"url\":\"https://www.semanticscholar.org/paper/dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.02824\",\"authors\":[{\"authorId\":\"1379929116\",\"name\":\"Mandela Patrick\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"144721617\",\"name\":\"Y. Asano\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"},{\"authorId\":\"145414740\",\"name\":\"J. Henriques\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"title\":\"Support-set bottlenecks for video-text representation learning\",\"url\":\"https://www.semanticscholar.org/paper/78bc767ebd02c0cc690fdb334c37bf64cfaf0115\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.05162\",\"authors\":[{\"authorId\":\"1500408667\",\"name\":\"Zhiyuan Fang\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.61\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1794bc353c94e8d708476132eb326fe3af51c2e6\",\"title\":\"Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1794bc353c94e8d708476132eb326fe3af51c2e6\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1911.03705\",\"authors\":[{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"143977316\",\"name\":\"M. Shen\"},{\"authorId\":\"150341221\",\"name\":\"Wangchunshu Zhou\"},{\"authorId\":\"1557324013\",\"name\":\"Pei Zhou\"},{\"authorId\":\"1857797\",\"name\":\"Chandra Bhagavatula\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"1384550891\",\"name\":\"X. Ren\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.165\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"title\":\"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/fc366c5a6e6aaf3fe718be09d9b6fb8924f1a7bf\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2001.06680\",\"authors\":[{\"authorId\":\"71170299\",\"name\":\"J. Wu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"101219260\",\"name\":\"Si Liu\"},{\"authorId\":\"49478124\",\"name\":\"L. Lin\"}],\"doi\":\"10.1609/AAAI.V34I07.6924\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8e744b1f33b1f3f53fcffb1dafd592e992694ff\",\"title\":\"Tree-Structured Policy based Progressive Reinforcement Learning for Temporally Language Grounding in Video\",\"url\":\"https://www.semanticscholar.org/paper/b8e744b1f33b1f3f53fcffb1dafd592e992694ff\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"26559284\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"title\":\"Captioning Near-Future Activity Sequences\",\"url\":\"https://www.semanticscholar.org/paper/dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":null,\"name\":\"Ning Xu\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"title\":\"Tianjin University and National University of Singapore at TRECVID 2017: Video to Text Description\",\"url\":\"https://www.semanticscholar.org/paper/c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":\"1909.03099\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7598ad186218f5d43fde37acd3d17f897283c3b7\",\"title\":\"Abductive Reasoning as Self-Supervision for Common Sense Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7598ad186218f5d43fde37acd3d17f897283c3b7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1704.02163\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"},{\"authorId\":\"87972149\",\"name\":\"Sergi Soler\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1016/j.jvcir.2017.11.022\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"title\":\"Egocentric video description based on temporally-linked sequences\",\"url\":\"https://www.semanticscholar.org/paper/a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":\"1905.03966\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"49050519\",\"name\":\"Jiyuan Zhang\"},{\"authorId\":\"47119038\",\"name\":\"X. Wang\"},{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/CVPR.2019.00854\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"title\":\"Memory-Attended Recurrent Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1812.05634\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00676\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"title\":\"Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"49730189\",\"name\":\"Boyang Li\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACVW.2019.00011\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"32b3e8f7a673801d6bcfb482a72c52c78e96b006\",\"title\":\"Joint Event Detection and Description in Continuous Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/32b3e8f7a673801d6bcfb482a72c52c78e96b006\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"101332807\",\"name\":\"O. S. Amosov\"},{\"authorId\":\"46543203\",\"name\":\"Y. Ivanov\"},{\"authorId\":\"122224609\",\"name\":\"S. Zhiganov\"}],\"doi\":\"10.1109/FAREASTCON.2018.8602625\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9217c555d584b9fdb4f811aae28fa03137f90eb3\",\"title\":\"Detection and Recognition of Emergency Situations in Continuous Video Stream of Information and Telecommunication Systems\",\"url\":\"https://www.semanticscholar.org/paper/9217c555d584b9fdb4f811aae28fa03137f90eb3\",\"venue\":\"2018 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50117915\",\"name\":\"Y. Wu\"},{\"authorId\":\"37108050\",\"name\":\"S. Drucker\"},{\"authorId\":\"3041721\",\"name\":\"M. Philipose\"},{\"authorId\":\"40125198\",\"name\":\"L. Ravindranath\"}],\"doi\":\"10.1145/3209900.3209909\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8494ea1faf92982a2c6bf07674eeb6201a13b70d\",\"title\":\"Querying Videos Using DNN Generated Labels\",\"url\":\"https://www.semanticscholar.org/paper/8494ea1faf92982a2c6bf07674eeb6201a13b70d\",\"venue\":\"HILDA@SIGMOD\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":\"2010.03667\",\"authors\":[{\"authorId\":\"48453720\",\"name\":\"A. Pavel\"},{\"authorId\":\"47098111\",\"name\":\"G. Reyes\"},{\"authorId\":\"1744846\",\"name\":\"Jeffrey P. Bigham\"}],\"doi\":\"10.1145/3379337.3415864\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"72d00737548215582bc4860ca26f04d6ccdbcdf2\",\"title\":\"Rescribe: Authoring and Automatically Editing Audio Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/72d00737548215582bc4860ca26f04d6ccdbcdf2\",\"venue\":\"UIST\",\"year\":2020},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2003304806\",\"name\":\"Mauricio-Andres Zamora-Hernandez\"},{\"authorId\":\"2003285868\",\"name\":\"Jose Andrez Chaves Ceciliano\"},{\"authorId\":\"145399025\",\"name\":\"Alonso Granados\"},{\"authorId\":\"1410269918\",\"name\":\"John Alejandro Castro-Vargas\"},{\"authorId\":\"2031782865\",\"name\":\"Jos\\u00ed Garc\\u00eda Rodr\\u00edguez\"},{\"authorId\":\"1754830\",\"name\":\"J. L\\u00f3pez\"}],\"doi\":\"10.1007/978-3-030-57802-2_76\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff70b785887b883f965be3fb93a129ec1dd9c238\",\"title\":\"Manufacturing Description Language for Process Control in Industry 4.0\",\"url\":\"https://www.semanticscholar.org/paper/ff70b785887b883f965be3fb93a129ec1dd9c238\",\"venue\":\"SOCO\",\"year\":2020},{\"arxivId\":\"2005.05402\",\"authors\":[{\"authorId\":\"46665218\",\"name\":\"Jie Lei\"},{\"authorId\":\"145602574\",\"name\":\"L. Wang\"},{\"authorId\":\"1752875\",\"name\":\"Y. Shen\"},{\"authorId\":\"144580027\",\"name\":\"Dong Yu\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.acl-main.233\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"70557ea6b65846fc30729ceed224acd4ac64ca5d\",\"title\":\"MART: Memory-Augmented Recurrent Transformer for Coherent Video Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/70557ea6b65846fc30729ceed224acd4ac64ca5d\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2008.06880\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":null,\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413880\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"title\":\"Poet: Product-oriented Video Captioner for E-commerce\",\"url\":\"https://www.semanticscholar.org/paper/72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1904.03597\",\"authors\":[{\"authorId\":\"46584859\",\"name\":\"Jiangliu Wang\"},{\"authorId\":\"2840852\",\"name\":\"Jianbo Jiao\"},{\"authorId\":\"2780029\",\"name\":\"Linchao Bao\"},{\"authorId\":\"2548483\",\"name\":\"Shengfeng He\"},{\"authorId\":\"46398631\",\"name\":\"Yunhui Liu\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2019.00413\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eb2f5e00049eb5bd36bea402c2818fe4bbfe0a0e\",\"title\":\"Self-Supervised Spatio-Temporal Representation Learning for Videos by Predicting Motion and Appearance Statistics\",\"url\":\"https://www.semanticscholar.org/paper/eb2f5e00049eb5bd36bea402c2818fe4bbfe0a0e\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1911.05186\",\"authors\":[{\"authorId\":\"7878341\",\"name\":\"Wubo Li\"},{\"authorId\":\"9276071\",\"name\":\"Wei Zou\"},{\"authorId\":\"1898780\",\"name\":\"Xiangang Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"962ec0fc31498001bdd011effb4ba73621dc0a8a\",\"title\":\"TCT: A Cross-supervised Learning Method for Multimodal Sequence Representation\",\"url\":\"https://www.semanticscholar.org/paper/962ec0fc31498001bdd011effb4ba73621dc0a8a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":\"10.1007/978-3-030-31756-0_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"title\":\"The Encoder-Decoder Framework and Its Applications\",\"url\":\"https://www.semanticscholar.org/paper/0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1912.06617\",\"authors\":[{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1398236231\",\"name\":\"W. Mayol-Cuevas\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":\"10.1109/CVPR42600.2020.00095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bc01f26b29282855e7cc997a737aa72697a4cac\",\"title\":\"Action Modifiers: Learning From Adverbs in Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/1bc01f26b29282855e7cc997a737aa72697a4cac\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1905.11954\",\"authors\":[{\"authorId\":\"2117357\",\"name\":\"Chengxu Zhuang\"},{\"authorId\":\"50112310\",\"name\":\"Alex Andonian\"},{\"authorId\":\"40657572\",\"name\":\"D. Yamins\"}],\"doi\":\"10.1109/CVPR42600.2020.00958\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"02cd7e1a888fedd25337a4598f332c5203091e71\",\"title\":\"Unsupervised Learning From Video With Deep Neural Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/02cd7e1a888fedd25337a4598f332c5203091e71\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1903.01489\",\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/s11042-018-7040-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1344317f255a9d338fb80f276126951b9644f7e3\",\"title\":\"M-VAD names: a dataset for video captioning with naming\",\"url\":\"https://www.semanticscholar.org/paper/1344317f255a9d338fb80f276126951b9644f7e3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":\"2008.09791\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-58589-1_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"title\":\"Identity-Aware Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9361887\",\"name\":\"Pravin Nagar\"},{\"authorId\":\"67153756\",\"name\":\"Mansi Khemka\"},{\"authorId\":\"38772597\",\"name\":\"C. Arora\"}],\"doi\":\"10.1145/3394171.3413713\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"25df842658e7adb7535f4d154d49bb9961b0eb6e\",\"title\":\"Concept Drift Detection for Multivariate Data Streams and Temporal Segmentation of Daylong Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/25df842658e7adb7535f4d154d49bb9961b0eb6e\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2004.05573\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"47824843\",\"name\":\"W. Wang\"},{\"authorId\":\"1630359492\",\"name\":\"Ludan Ruan\"},{\"authorId\":\"49539732\",\"name\":\"Linli Yao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78605f537f6c4a8c5206bdd4f57cea22d9750703\",\"title\":\"YouMakeup VQA Challenge: Towards Fine-grained Action Understanding in Domain-Specific Videos\",\"url\":\"https://www.semanticscholar.org/paper/78605f537f6c4a8c5206bdd4f57cea22d9750703\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1802.01144\",\"authors\":[{\"authorId\":\"144865353\",\"name\":\"B. Pang\"},{\"authorId\":\"15376265\",\"name\":\"Kaiwen Zha\"},{\"authorId\":\"1830034\",\"name\":\"Cewu Lu\"}],\"doi\":\"10.1109/CVPRW.2018.00308\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"35684ac29ea0936635da9d9944659e4cdb7e2d0d\",\"title\":\"Human Action Adverb Recognition: ADHA Dataset and a Three-Stream Hybrid Model\",\"url\":\"https://www.semanticscholar.org/paper/35684ac29ea0936635da9d9944659e4cdb7e2d0d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"title\":\"Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1809.08381\",\"authors\":[{\"authorId\":\"29132542\",\"name\":\"Meera Hahn\"},{\"authorId\":\"31601235\",\"name\":\"Nataniel Ruiz\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"144177248\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7c9b3eb8737d29a779a0e21e355543c22ccca49\",\"title\":\"Learning to Localize and Align Fine-Grained Actions to Sparse Instructions\",\"url\":\"https://www.semanticscholar.org/paper/a7c9b3eb8737d29a779a0e21e355543c22ccca49\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":null,\"name\":\"Abhinav Shrivastava\"},{\"authorId\":\"2852035\",\"name\":\"V. Morariu\"},{\"authorId\":\"145855898\",\"name\":\"L. Davis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ef6dbe27031265a4663721883008f4c3e6a89c4e\",\"title\":\"Supplementary Material : Referring to Object in Video using Spatio-Temporal Identifying Description\",\"url\":\"https://www.semanticscholar.org/paper/ef6dbe27031265a4663721883008f4c3e6a89c4e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2151048\",\"name\":\"Taiki Miyanishi\"},{\"authorId\":\"8425523\",\"name\":\"J. Hirayama\"},{\"authorId\":\"34772057\",\"name\":\"Takuya Maekawa\"},{\"authorId\":\"1716788\",\"name\":\"M. Kawanabe\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"686b14ce7f02516730de03f459cadb223a03765f\",\"title\":\"Generating an Event Timeline About Daily Activities From a Semantic Concept Stream\",\"url\":\"https://www.semanticscholar.org/paper/686b14ce7f02516730de03f459cadb223a03765f\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1911.03705\",\"authors\":[{\"authorId\":\"51583409\",\"name\":\"Bill Yuchen Lin\"},{\"authorId\":\"153150499\",\"name\":\"Ming Shen\"},{\"authorId\":\"97953933\",\"name\":\"Y. Xing\"},{\"authorId\":\"144032123\",\"name\":\"P. Zhou\"},{\"authorId\":\"145201126\",\"name\":\"X. Ren\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"727c9d3846ebd80a9138d0e6c9e995d9afc1d312\",\"title\":\"CommonGen: A Constrained Text Generation Dataset Towards Generative Commonsense Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/727c9d3846ebd80a9138d0e6c9e995d9afc1d312\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2012.00366\",\"authors\":[{\"authorId\":\"9610143\",\"name\":\"Zhihao Fan\"},{\"authorId\":\"2171182\",\"name\":\"Yeyun Gong\"},{\"authorId\":\"2712533\",\"name\":\"Zhongyu Wei\"},{\"authorId\":\"50695377\",\"name\":\"Siyuan Wang\"},{\"authorId\":\"121240634\",\"name\":\"Yameng Huang\"},{\"authorId\":\"49097406\",\"name\":\"Jian Jiao\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"2027657172\",\"name\":\"Ruofei Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"023b508aca0e776f6face93548fedb8921cf35ab\",\"title\":\"An Enhanced Knowledge Injection Model for Commonsense Generation\",\"url\":\"https://www.semanticscholar.org/paper/023b508aca0e776f6face93548fedb8921cf35ab\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114276298\",\"name\":\"Soichiro Fujita\"},{\"authorId\":\"8351786\",\"name\":\"Tsutomu Hirao\"},{\"authorId\":\"2300756\",\"name\":\"Hidetaka Kamigaito\"},{\"authorId\":\"144859189\",\"name\":\"M. Okumura\"},{\"authorId\":\"2364073\",\"name\":\"M. Nagata\"}],\"doi\":\"10.1007/978-3-030-58539-6_31\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5a4c5fa5a25cff3c65e74f64504819683353ef1e\",\"title\":\"SODA: Story Oriented Dense Video Captioning Evaluation Framework\",\"url\":\"https://www.semanticscholar.org/paper/5a4c5fa5a25cff3c65e74f64504819683353ef1e\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1912.03590\",\"authors\":[{\"authorId\":\"3178508\",\"name\":\"Songyang Zhang\"},{\"authorId\":\"2484788\",\"name\":\"Houwen Peng\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1609/AAAI.V34I07.6984\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cef4a58c08816c3d3bd56826a418508720d4b20d\",\"title\":\"Learning 2D Temporal Adjacent Networks for Moment Localization with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/cef4a58c08816c3d3bd56826a418508720d4b20d\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2011.10132\",\"authors\":[{\"authorId\":\"51218991\",\"name\":\"Sisi Qu\"},{\"authorId\":\"150234800\",\"name\":\"Mattia Soldan\"},{\"authorId\":\"97375393\",\"name\":\"Mengmeng Xu\"},{\"authorId\":\"1402915033\",\"name\":\"Jesper Tegn\\u00e9r\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"075ce64105cc54cec2b54c24f8f890f2c62d090b\",\"title\":\"VLG-Net: Video-Language Graph Matching Network for Video Grounding\",\"url\":\"https://www.semanticscholar.org/paper/075ce64105cc54cec2b54c24f8f890f2c62d090b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":\"1711.06354\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"title\":\"Grounded Objects and Interactions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"venue\":\"ArXiv\",\"year\":2017}],\"corpusId\":1026139,\"doi\":\"10.1109/ICCV.2017.83\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":73,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2013.340\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"title\":\"A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching\",\"url\":\"https://www.semanticscholar.org/paper/a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":\"1411.6031\",\"authors\":[{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/CVPR.2015.7298676\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"295a1c000c5e3fd5959bb250e8376a06efa405b1\",\"title\":\"Finding action tubes\",\"url\":\"https://www.semanticscholar.org/paper/295a1c000c5e3fd5959bb250e8376a06efa405b1\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1406.5824\",\"authors\":[{\"authorId\":\"34149749\",\"name\":\"Serena Yeung\"},{\"authorId\":\"50706340\",\"name\":\"Alireza Fathi\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a31da018246f4d67c5702574b7c16e14d261541\",\"title\":\"VideoSET: Video Summary Evaluation through Text\",\"url\":\"https://www.semanticscholar.org/paper/3a31da018246f4d67c5702574b7c16e14d261541\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1611.06949\",\"authors\":[{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"3355264\",\"name\":\"Kevin D. Tang\"},{\"authorId\":\"1706007\",\"name\":\"Jianchao Yang\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"}],\"doi\":\"10.1109/CVPR.2017.214\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21fa67345e49642b8ebb22a59c4b2799a56e996f\",\"title\":\"Dense Captioning with Joint Inference and Visual Context\",\"url\":\"https://www.semanticscholar.org/paper/21fa67345e49642b8ebb22a59c4b2799a56e996f\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1403.6173\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"113090874\",\"name\":\"W. Qiu\"},{\"authorId\":\"33985877\",\"name\":\"Annemarie Friedrich\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-11752-2_15\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"889e723cd6d581e120ee6776b231fdf69707ab50\",\"title\":\"Coherent Multi-sentence Video Description with Variable Level of Detail\",\"url\":\"https://www.semanticscholar.org/paper/889e723cd6d581e120ee6776b231fdf69707ab50\",\"venue\":\"GCPR\",\"year\":2014},{\"arxivId\":\"1510.01442\",\"authors\":[{\"authorId\":\"46402216\",\"name\":\"Huan Yang\"},{\"authorId\":\"2450889\",\"name\":\"B. Wang\"},{\"authorId\":\"145676588\",\"name\":\"Stephen Lin\"},{\"authorId\":\"2242717\",\"name\":\"D. Wipf\"},{\"authorId\":\"1697293\",\"name\":\"M. Guo\"},{\"authorId\":\"143632999\",\"name\":\"B. Guo\"}],\"doi\":\"10.1109/ICCV.2015.526\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"faafe2a76dbb9a5a1468b1a02b1f0f09ced8587e\",\"title\":\"Unsupervised Extraction of Video Highlights via Robust Recurrent Auto-Encoders\",\"url\":\"https://www.semanticscholar.org/paper/faafe2a76dbb9a5a1468b1a02b1f0f09ced8587e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1602.07332\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"40591424\",\"name\":\"J. Kravitz\"},{\"authorId\":\"6000646\",\"name\":\"Stephanie Chen\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"33642044\",\"name\":\"L. Li\"},{\"authorId\":\"1388344504\",\"name\":\"David A. Shamma\"},{\"authorId\":\"1379506718\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-016-0981-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"title\":\"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations\",\"url\":\"https://www.semanticscholar.org/paper/afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"143836295\",\"name\":\"W. Xu\"},{\"authorId\":\"41216159\",\"name\":\"Ming Yang\"},{\"authorId\":\"144782042\",\"name\":\"Kai Yu\"}],\"doi\":\"10.1109/TPAMI.2012.59\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"title\":\"3D Convolutional Neural Networks for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8193421\",\"name\":\"Yicong Tian\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":\"10.1109/CVPR.2013.341\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eea7842025dad6b4cf53445c161538536020b412\",\"title\":\"Spatiotemporal Deformable Part Models for Action Detection\",\"url\":\"https://www.semanticscholar.org/paper/eea7842025dad6b4cf53445c161538536020b412\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3304525\",\"name\":\"Alexandre Alahi\"},{\"authorId\":\"2957685\",\"name\":\"Kratarth Goel\"},{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"2364487\",\"name\":\"Alexandre Robicquet\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"}],\"doi\":\"10.1109/CVPR.2016.110\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e11a020f0d2942d09127daf1ce7e658d3bf67291\",\"title\":\"Social LSTM: Human Trajectory Prediction in Crowded Spaces\",\"url\":\"https://www.semanticscholar.org/paper/e11a020f0d2942d09127daf1ce7e658d3bf67291\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1007/978-3-319-46487-9_47\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c24bbbc5139eb2f8c5d0579174dbeae5cbaedbfc\",\"title\":\"DAPs: Deep Action Proposals for Action Understanding\",\"url\":\"https://www.semanticscholar.org/paper/c24bbbc5139eb2f8c5d0579174dbeae5cbaedbfc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Karaman\"},{\"authorId\":null,\"name\":\"L. Seidenari\"},{\"authorId\":null,\"name\":\"A. Del Bimbo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Fast saliency based pooling of fisher encoded dense trajectories\",\"url\":\"\",\"venue\":\"In ECCV THUMOS Workshop,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1766254\",\"name\":\"Olivier Duchenne\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"144570279\",\"name\":\"Francis R. Bach\"},{\"authorId\":\"144189388\",\"name\":\"J. Ponce\"}],\"doi\":\"10.1109/ICCV.2009.5459279\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b67efc8f41fcb3458f6ca5482e73bcc929323a09\",\"title\":\"Automatic annotation of human actions in video\",\"url\":\"https://www.semanticscholar.org/paper/b67efc8f41fcb3458f6ca5482e73bcc929323a09\",\"venue\":\"2009 IEEE 12th International Conference on Computer Vision\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.112\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97398356607115f78d677663a682363eec3302d7\",\"title\":\"Highlight Detection with Pairwise Deep Ranking for First-Person Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/97398356607115f78d677663a682363eec3302d7\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2b42f83a720bd4156113ba5350add2df2673daf0\",\"title\":\"Action Recognition and Detection by Combining Motion and Appearance Features\",\"url\":\"https://www.semanticscholar.org/paper/2b42f83a720bd4156113ba5350add2df2673daf0\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1505.05914\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"title\":\"A Multi-scale Multiple Instance Video Description Network\",\"url\":\"https://www.semanticscholar.org/paper/08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"1753355\",\"name\":\"Z. Harchaoui\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/TPAMI.2013.65\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"404352f5c18d4aca97f0cb660a31bf5d0df3fe0c\",\"title\":\"Temporal Localization of Actions with Actoms\",\"url\":\"https://www.semanticscholar.org/paper/404352f5c18d4aca97f0cb660a31bf5d0df3fe0c\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"24138684\",\"name\":\"Dominikus Wetzel\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"}],\"doi\":\"10.1162/tacl_a_00207\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"title\":\"Grounding Action Descriptions in Videos\",\"url\":\"https://www.semanticscholar.org/paper/21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47237027\",\"name\":\"Andreas Geiger\"},{\"authorId\":\"37108776\",\"name\":\"Philip Lenz\"},{\"authorId\":\"1760556\",\"name\":\"C. Stiller\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":\"10.1177/0278364913491297\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"79b949d9b35c3f51dd20fb5c746cc81fc87147eb\",\"title\":\"Vision meets robotics: The KITTI dataset\",\"url\":\"https://www.semanticscholar.org/paper/79b949d9b35c3f51dd20fb5c746cc81fc87147eb\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2013},{\"arxivId\":\"1602.04506\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"6000646\",\"name\":\"Stephanie Chen\"},{\"authorId\":\"40591424\",\"name\":\"J. Kravitz\"},{\"authorId\":\"1760364\",\"name\":\"D. Shamma\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"}],\"doi\":\"10.1145/2858036.2858115\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d1949fa1a0ad3caef53e71e9f6ebbcaa69a1891\",\"title\":\"Embracing Error to Enable Rapid Crowdsourcing\",\"url\":\"https://www.semanticscholar.org/paper/9d1949fa1a0ad3caef53e71e9f6ebbcaa69a1891\",\"venue\":\"CHI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"120896463\",\"name\":\"Chih-Wei Chen\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-642-15552-9_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"994a7b903b937f8b177c035db86852091fd26aa7\",\"title\":\"Modeling Temporal Structure of Decomposable Motion Segments for Activity Classification\",\"url\":\"https://www.semanticscholar.org/paper/994a7b903b937f8b177c035db86852091fd26aa7\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"}],\"doi\":\"10.1007/978-3-319-10602-1_37\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d9c9b8194ac81f97bfedb7d15124e7b80c3c3d68\",\"title\":\"Video Action Detection with Relational Dynamic-Poselets\",\"url\":\"https://www.semanticscholar.org/paper/d9c9b8194ac81f97bfedb7d15124e7b80c3c3d68\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1611.06607\",\"authors\":[{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"153365679\",\"name\":\"J. Johnson\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2017.356\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a7011346ce939e3251915e92ae2f252e4c7f777\",\"title\":\"A Hierarchical Approach for Generating Descriptive Image Paragraphs\",\"url\":\"https://www.semanticscholar.org/paper/3a7011346ce939e3251915e92ae2f252e4c7f777\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"2757535\",\"name\":\"Jordi Vallmitjana\"},{\"authorId\":\"1690152\",\"name\":\"Amanda Stent\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"}],\"doi\":\"10.1109/CVPR.2015.7299154\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cbf89cb4e107fb59e119ae619bcfe48e1964e033\",\"title\":\"TVSum: Summarizing web videos using titles\",\"url\":\"https://www.semanticscholar.org/paper/cbf89cb4e107fb59e119ae619bcfe48e1964e033\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1506.04714\",\"authors\":[{\"authorId\":\"144348441\",\"name\":\"Dinesh Jayaraman\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2016.418\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32e6b6106ee2cc90126dcd8996d4a06374afe69b\",\"title\":\"Slow and Steady Feature Analysis: Higher Order Temporal Coherence in Video\",\"url\":\"https://www.semanticscholar.org/paper/32e6b6106ee2cc90126dcd8996d4a06374afe69b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1686917\",\"name\":\"Wu Liu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"3038150\",\"name\":\"Cherry Che\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2015.7298994\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"562fbe1f8b8a77fbeb2adea42476752246b610e7\",\"title\":\"Multi-task deep visual-semantic embedding for video thumbnail selection\",\"url\":\"https://www.semanticscholar.org/paper/562fbe1f8b8a77fbeb2adea42476752246b610e7\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"title\":\"Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework\",\"url\":\"https://www.semanticscholar.org/paper/1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tomas Mikolov\"},{\"authorId\":\"2245567\",\"name\":\"M. Karafi\\u00e1t\"},{\"authorId\":\"1816892\",\"name\":\"L. Burget\"},{\"authorId\":\"1899242\",\"name\":\"J. \\u010cernock\\u00fd\"},{\"authorId\":\"2803071\",\"name\":\"S. Khudanpur\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"9819b600a828a57e1cde047bbe710d3446b30da5\",\"title\":\"Recurrent neural network based language model\",\"url\":\"https://www.semanticscholar.org/paper/9819b600a828a57e1cde047bbe710d3446b30da5\",\"venue\":\"INTERSPEECH\",\"year\":2010},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145136049\",\"name\":\"W. Wolf\"}],\"doi\":\"10.1109/ICASSP.1996.543588\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"16a48cbc05cff687e099266f9ab2d0b5ee508406\",\"title\":\"Key frame selection by motion analysis\",\"url\":\"https://www.semanticscholar.org/paper/16a48cbc05cff687e099266f9ab2d0b5ee508406\",\"venue\":\"1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1910299\",\"name\":\"Oren Boiman\"},{\"authorId\":\"144611617\",\"name\":\"M. Irani\"}],\"doi\":\"10.1007/s11263-006-0009-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86078efdd0d1bbfaf4b7f821e973f607429751fc\",\"title\":\"Detecting Irregularities in Images and in Video\",\"url\":\"https://www.semanticscholar.org/paper/86078efdd0d1bbfaf4b7f821e973f607429751fc\",\"venue\":\"Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710918\",\"name\":\"J. Yamato\"},{\"authorId\":\"1708785\",\"name\":\"J. Ohya\"},{\"authorId\":\"46562570\",\"name\":\"K. Ishii\"}],\"doi\":\"10.1109/CVPR.1992.223161\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45336e96c04ea005b203ff3fc84aa4f4159e8cb0\",\"title\":\"Recognizing human action in time-sequential images using hidden Markov model\",\"url\":\"https://www.semanticscholar.org/paper/45336e96c04ea005b203ff3fc84aa4f4159e8cb0\",\"venue\":\"Proceedings 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":1992},{\"arxivId\":\"1604.01753\",\"authors\":[{\"authorId\":\"34280810\",\"name\":\"Gunnar A. Sigurdsson\"},{\"authorId\":\"2668759\",\"name\":\"G. Varol\"},{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1007/978-3-319-46448-0_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21334d1aac5422da88780f8e24e181bfa15ef0e1\",\"title\":\"Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding\",\"url\":\"https://www.semanticscholar.org/paper/21334d1aac5422da88780f8e24e181bfa15ef0e1\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2712738\",\"name\":\"C. Sch\\u00fcldt\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"3033284\",\"name\":\"B. Caputo\"}],\"doi\":\"10.1109/icpr.2004.1334462\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"b480f6a3750b4cebaf1db205692c8321d45926a2\",\"title\":\"Recognizing human actions: a local SVM approach\",\"url\":\"https://www.semanticscholar.org/paper/b480f6a3750b4cebaf1db205692c8321d45926a2\",\"venue\":\"Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2015.7298698\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a28efacb92d16e6e0dd4d87b5aca91b28be8853\",\"title\":\"ActivityNet: A large-scale video benchmark for human activity understanding\",\"url\":\"https://www.semanticscholar.org/paper/0a28efacb92d16e6e0dd4d87b5aca91b28be8853\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1511.07571\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.494\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"title\":\"DenseCap: Fully Convolutional Localization Networks for Dense Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d7ce5665a72c0b607f484c1b448875f02ddfac3b\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3095774\",\"name\":\"Dan Oneata\"},{\"authorId\":\"1721683\",\"name\":\"J. Verbeek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/CVPR.2014.326\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"19aa7a6f8709f2e7b4341a43c45edfe4e1964ca5\",\"title\":\"Efficient Action Localization with Approximately Normalized Fisher Vectors\",\"url\":\"https://www.semanticscholar.org/paper/19aa7a6f8709f2e7b4341a43c45edfe4e1964ca5\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49121996\",\"name\":\"William M. Marsden\"}],\"doi\":\"10.1017/CBO9781139207249.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3d2218b17e7898a222e5fc2079a3f1531990708f\",\"title\":\"I and J\",\"url\":\"https://www.semanticscholar.org/paper/3d2218b17e7898a222e5fc2079a3f1531990708f\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/CVPR.2009.5206557\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c26906b6dab02083ffd01fd27d9087597999bc0e\",\"title\":\"Actions in context\",\"url\":\"https://www.semanticscholar.org/paper/c26906b6dab02083ffd01fd27d9087597999bc0e\",\"venue\":\"CVPR\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"3261451\",\"name\":\"Benjamin Rozenfeld\"}],\"doi\":\"10.1109/CVPR.2008.4587756\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f86767732f76f478d5845f2e59f99ba106e9265\",\"title\":\"Learning realistic human actions from movies\",\"url\":\"https://www.semanticscholar.org/paper/0f86767732f76f478d5845f2e59f99ba106e9265\",\"venue\":\"2008 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2008},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123446103\",\"name\":\"Hilde Kuehne\"},{\"authorId\":\"119268487\",\"name\":\"Hueihan Jhuang\"},{\"authorId\":\"1930964\",\"name\":\"E. Garrote\"},{\"authorId\":\"145031878\",\"name\":\"T. Poggio\"},{\"authorId\":\"1981539\",\"name\":\"Thomas Serre\"}],\"doi\":\"10.1109/ICCV.2011.6126543\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8b3b8848a311c501e704c45c6d50430ab7068956\",\"title\":\"HMDB: A large video database for human motion recognition\",\"url\":\"https://www.semanticscholar.org/paper/8b3b8848a311c501e704c45c6d50430ab7068956\",\"venue\":\"2011 International Conference on Computer Vision\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1976171\",\"name\":\"D. Goldman\"},{\"authorId\":\"143800609\",\"name\":\"Brian Curless\"},{\"authorId\":\"1745260\",\"name\":\"D. Salesin\"},{\"authorId\":\"1679223\",\"name\":\"S. Seitz\"}],\"doi\":\"10.1145/1179352.1141967\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0df0acbd41f388b6212b2b9f52febceb91253937\",\"title\":\"Schematic storyboarding for video visualization and editing\",\"url\":\"https://www.semanticscholar.org/paper/0df0acbd41f388b6212b2b9f52febceb91253937\",\"venue\":\"ACM Trans. Graph.\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1718558\",\"name\":\"H. Zhang\"},{\"authorId\":\"49388282\",\"name\":\"Jianhua Wu\"},{\"authorId\":\"40588714\",\"name\":\"Di Zhong\"},{\"authorId\":\"1743808\",\"name\":\"S. Smoliar\"}],\"doi\":\"10.1016/S0031-3203(96)00109-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f98fb60a5869514215f2a8776fa942b2a1bdd6b3\",\"title\":\"An integrated system for content-based video retrieval and browsing\",\"url\":\"https://www.semanticscholar.org/paper/f98fb60a5869514215f2a8776fa942b2a1bdd6b3\",\"venue\":\"Pattern Recognit.\",\"year\":1997},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1608.02367\",\"authors\":[{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"},{\"authorId\":\"3111194\",\"name\":\"J. Heikkil\\u00e4\"},{\"authorId\":\"1771769\",\"name\":\"N. Yokoya\"}],\"doi\":\"10.1007/978-3-319-46604-0_46\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa76f655c2ad655080593a191c4b479ab9f18117\",\"title\":\"Learning Joint Representations of Videos and Sentences with Web Image Search\",\"url\":\"https://www.semanticscholar.org/paper/aa76f655c2ad655080593a191c4b479ab9f18117\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Gorban\"},{\"authorId\":null,\"name\":\"H. Idrees\"},{\"authorId\":null,\"name\":\"Y.-G. Jiang\"},{\"authorId\":null,\"name\":\"A. Roshan Zamir\"},{\"authorId\":null,\"name\":\"I. Laptev\"},{\"authorId\":null,\"name\":\"M. Shah\"},{\"authorId\":null,\"name\":\"R. Sukthankar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"THUMOS challenge: Action recognition with a large number of classes\",\"url\":\"\",\"venue\":\"http://www.thumos.info/,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3502855\",\"name\":\"M. Marszalek\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/cvprw.2009.5206557\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1fd485daa491c0debcd900b3f6bc141c3883812d\",\"title\":\"Actions in context\",\"url\":\"https://www.semanticscholar.org/paper/1fd485daa491c0debcd900b3f6bc141c3883812d\",\"venue\":\"2009 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2009},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34827715\",\"name\":\"N. Salehi\"},{\"authorId\":\"3142146\",\"name\":\"L. Irani\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"3153970\",\"name\":\"A. Alkhatib\"},{\"authorId\":\"3195620\",\"name\":\"Eva Ogbe\"},{\"authorId\":\"1844994\",\"name\":\"Kristy Milland\"},{\"authorId\":\"1914904\",\"name\":\"Clickhappier\"}],\"doi\":\"10.1145/2702123.2702508\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03d4ce5a1066d72cd20fe3af744fe985d024b095\",\"title\":\"We Are Dynamo: Overcoming Stalling and Friction in Collective Action for Crowd Workers\",\"url\":\"https://www.semanticscholar.org/paper/03d4ce5a1066d72cd20fe3af744fe985d024b095\",\"venue\":\"CHI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":\"10.1109/CVPR.2014.85\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c0f1b49fd8a5b2ca8116e613a5da10babe5e564\",\"title\":\"Parsing Videos of Actions with Segmental Grammars\",\"url\":\"https://www.semanticscholar.org/paper/7c0f1b49fd8a5b2ca8116e613a5da10babe5e564\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3037160\",\"name\":\"Michael Gygli\"},{\"authorId\":\"145551629\",\"name\":\"H. Grabner\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/CVPR.2015.7298928\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cfcb9bcc1e8b4d3451578398aeb37f0fa5614632\",\"title\":\"Video summarization by learning submodular mixtures of objectives\",\"url\":\"https://www.semanticscholar.org/paper/cfcb9bcc1e8b4d3451578398aeb37f0fa5614632\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1109/CVPR.2016.211\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bac994dda1385cd709e08e24170c711d8c573676\",\"title\":\"Fast Temporal Activity Proposals for Efficient Detection of Human Actions in Untrimmed Videos\",\"url\":\"https://www.semanticscholar.org/paper/bac994dda1385cd709e08e24170c711d8c573676\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3214848\",\"name\":\"Arash Vahdat\"},{\"authorId\":\"144664894\",\"name\":\"B. Gao\"},{\"authorId\":\"2413711\",\"name\":\"M. Ranjbar\"},{\"authorId\":\"10771328\",\"name\":\"G. Mori\"}],\"doi\":\"10.1109/ICCVW.2011.6130458\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ca0ddc53ea91bf5d30288f6224bd73981b548817\",\"title\":\"A discriminative key pose sequence model for recognizing human interactions\",\"url\":\"https://www.semanticscholar.org/paper/ca0ddc53ea91bf5d30288f6224bd73981b548817\",\"venue\":\"2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2308737\",\"name\":\"C. Baldassano\"},{\"authorId\":\"16309044\",\"name\":\"J. Chen\"},{\"authorId\":\"6157581\",\"name\":\"A. Zadbood\"},{\"authorId\":\"1791723\",\"name\":\"Jonathan W. Pillow\"},{\"authorId\":\"1787630\",\"name\":\"U. Hasson\"},{\"authorId\":\"1780319\",\"name\":\"K. Norman\"}],\"doi\":\"10.1016/j.neuron.2017.06.041\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"63ae4d0bae5cea37c57131eafd89fe119fbe58be\",\"title\":\"Discovering Event Structure in Continuous Narrative Perception and Memory\",\"url\":\"https://www.semanticscholar.org/paper/63ae4d0bae5cea37c57131eafd89fe119fbe58be\",\"venue\":\"Neuron\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"3105474\",\"name\":\"Vignesh R. Paramathayalan\"},{\"authorId\":\"1742248\",\"name\":\"P. Moulin\"}],\"doi\":\"10.1109/CVPR.2014.102\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1640e42e0f1a40c5777d19866f763a62bd591624\",\"title\":\"Multiple Granularity Analysis for Fine-Grained Action Detection\",\"url\":\"https://www.semanticscholar.org/paper/1640e42e0f1a40c5777d19866f763a62bd591624\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2012.6247801\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"49435aab7cdf259335725acc96691f755e436f55\",\"title\":\"A database for fine grained activity detection of cooking activities\",\"url\":\"https://www.semanticscholar.org/paper/49435aab7cdf259335725acc96691f755e436f55\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012}],\"title\":\"Dense-Captioning Events in Videos\",\"topics\":[{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Internationalization and localization\",\"topicId\":\"69706\",\"url\":\"https://www.semanticscholar.org/topic/69706\"},{\"topic\":\"Performance\",\"topicId\":\"3097\",\"url\":\"https://www.semanticscholar.org/topic/3097\"},{\"topic\":\"Sensor\",\"topicId\":\"1117\",\"url\":\"https://www.semanticscholar.org/topic/1117\"}],\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017}\n"