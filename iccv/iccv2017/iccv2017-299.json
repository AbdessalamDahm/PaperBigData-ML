"{\"abstract\":\"In this paper, we propose a generative model, Temporal Generative Adversarial Nets (TGAN), which can learn a semantic representation of unlabeled videos, and is capable of generating videos. Unlike existing Generative Adversarial Nets (GAN)-based methods that generate videos with a single generator consisting of 3D deconvolutional layers, our model exploits two different types of generators: a temporal generator and an image generator. The temporal generator takes a single latent variable as input and outputs a set of latent variables, each of which corresponds to an image frame in a video. The image generator transforms a set of such latent variables into a video. To deal with instability in training of GAN with such advanced networks, we adopt a recently proposed model, Wasserstein GAN, and propose a novel method to train it stably in an end-to-end manner. The experimental results demonstrate the effectiveness of our methods.\",\"arxivId\":\"1611.06624\",\"authors\":[{\"authorId\":\"144648787\",\"name\":\"M. Saito\",\"url\":\"https://www.semanticscholar.org/author/144648787\"},{\"authorId\":\"8252749\",\"name\":\"E. Matsumoto\",\"url\":\"https://www.semanticscholar.org/author/8252749\"},{\"authorId\":\"3083107\",\"name\":\"Shunta Saito\",\"url\":\"https://www.semanticscholar.org/author/3083107\"}],\"citationVelocity\":60,\"citations\":[{\"arxivId\":\"1807.11152\",\"authors\":[{\"authorId\":\"49984891\",\"name\":\"Ceyuan Yang\"},{\"authorId\":\"40072288\",\"name\":\"Zhe Wang\"},{\"authorId\":\"22689408\",\"name\":\"Xinge Zhu\"},{\"authorId\":\"145544640\",\"name\":\"C. Huang\"},{\"authorId\":\"1788070\",\"name\":\"J. Shi\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01249-6_13\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da219d1f43cc00de6c5a411e47feca956f88645f\",\"title\":\"Pose Guided Human Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/da219d1f43cc00de6c5a411e47feca956f88645f\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153725817\",\"name\":\"Yuki Nakahira\"},{\"authorId\":\"3063432\",\"name\":\"K. Kawamoto\"}],\"doi\":\"10.1109/ICIP.2019.8803764\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3fe732ce588319a744e2b59c143b4958b54dbd06\",\"title\":\"DCVGAN: Depth Conditional Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/3fe732ce588319a744e2b59c143b4958b54dbd06\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699643\",\"name\":\"C. Pollett\"},{\"authorId\":\"50082102\",\"name\":\"L. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd355230db355e11d66aed1625cda46082178fe7\",\"title\":\"GAN-BASED PHOTO VIDEO SYNTHESIS\",\"url\":\"https://www.semanticscholar.org/paper/dd355230db355e11d66aed1625cda46082178fe7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1812.08861\",\"authors\":[{\"authorId\":\"10753214\",\"name\":\"Aliaksandr Siarohin\"},{\"authorId\":\"3099587\",\"name\":\"St\\u00e9phane Lathuili\\u00e8re\"},{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"40811261\",\"name\":\"E. Ricci\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1109/CVPR.2019.00248\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"edce7f037c840b7db2612f47c35ae374c4a80e3a\",\"title\":\"Animating Arbitrary Objects via Deep Motion Transfer\",\"url\":\"https://www.semanticscholar.org/paper/edce7f037c840b7db2612f47c35ae374c4a80e3a\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46489265\",\"name\":\"Yu-Ying Yeh\"},{\"authorId\":\"1614039983\",\"name\":\"Y. Liu\"},{\"authorId\":\"37811787\",\"name\":\"Wei-Chen Chiu\"},{\"authorId\":null,\"name\":\"Yu-Chiang Frank Wang\"}],\"doi\":\"10.1109/TETCI.2020.2968599\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d96db0e2164f464ef9dc5dd030672ea3e3c02d57\",\"title\":\"Static2Dynamic: Video Inference From a Deep Glimpse\",\"url\":\"https://www.semanticscholar.org/paper/d96db0e2164f464ef9dc5dd030672ea3e3c02d57\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46793780\",\"name\":\"Ximeng Sun\"},{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"99cd8f9920ba94ec9d85517f4975e1c4bee3c60a\",\"title\":\"A Two-Stream Variational Adversarial Network for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/99cd8f9920ba94ec9d85517f4975e1c4bee3c60a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82623793\",\"name\":\"Sameerah Talafha\"},{\"authorId\":\"1966492\",\"name\":\"Banafsheh Rekabdar\"},{\"authorId\":\"2698203\",\"name\":\"Chinwe Ekenna\"},{\"authorId\":\"2278522\",\"name\":\"Christos Mousas\"}],\"doi\":\"10.1109/ICSC.2020.00014\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e8b89b33935d2eb9098a568640bfb5ce6160f81\",\"title\":\"Attentional Adversarial Variational Video Generation via Decomposing Motion and Content\",\"url\":\"https://www.semanticscholar.org/paper/8e8b89b33935d2eb9098a568640bfb5ce6160f81\",\"venue\":\"2020 IEEE 14th International Conference on Semantic Computing (ICSC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391189572\",\"name\":\"Sinan Wang\"},{\"authorId\":\"1425074554\",\"name\":\"Xinyang Chen\"},{\"authorId\":null,\"name\":\"Yunbo Wang\"},{\"authorId\":\"35776445\",\"name\":\"Mingsheng Long\"},{\"authorId\":\"1519286448\",\"name\":\"J. Wang\"}],\"doi\":\"10.1109/cvpr42600.2020.00923\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4a608ca9679a6fd03c3a63c5dcc0bfcb64b62a1\",\"title\":\"Progressive Adversarial Networks for Fine-Grained Domain Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/f4a608ca9679a6fd03c3a63c5dcc0bfcb64b62a1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1711.09618\",\"authors\":[{\"authorId\":\"8197937\",\"name\":\"K. Ohnishi\"},{\"authorId\":\"48333400\",\"name\":\"S. Yamamoto\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e25c57a6395b7c8861088699eef1a56b6f1d70ad\",\"title\":\"Hierarchical Video Generation from Orthogonal Information: Optical Flow and Texture\",\"url\":\"https://www.semanticscholar.org/paper/e25c57a6395b7c8861088699eef1a56b6f1d70ad\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1907.10087\",\"authors\":[{\"authorId\":\"26997269\",\"name\":\"Naima Otberdout\"},{\"authorId\":\"2909056\",\"name\":\"M. Daoudi\"},{\"authorId\":\"46243487\",\"name\":\"A. Kacem\"},{\"authorId\":\"2062946\",\"name\":\"Lahoucine Ballihi\"},{\"authorId\":\"2507859\",\"name\":\"S. Berretti\"}],\"doi\":\"10.1109/TPAMI.2020.3002500\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4ea07e4cf344ed5fbc2aa833eec5b6bf2f6b759\",\"title\":\"Dynamic Facial Expression Generation on Hilbert Hypersphere with Conditional Wasserstein Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/b4ea07e4cf344ed5fbc2aa833eec5b6bf2f6b759\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118923780\",\"name\":\"\\u00c1lvarez\"},{\"authorId\":\"103771726\",\"name\":\"Emilien\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5548a56c62a10aff49212fc59ab2577a8b9a8d77\",\"title\":\"MAchine learning for environmental Time Series ( MATS ) Coordinator :\",\"url\":\"https://www.semanticscholar.org/paper/5548a56c62a10aff49212fc59ab2577a8b9a8d77\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1907.10244\",\"authors\":[{\"authorId\":\"98080420\",\"name\":\"H. Lee\"},{\"authorId\":\"48271129\",\"name\":\"T. Kim\"},{\"authorId\":\"3191728\",\"name\":\"T. Chung\"},{\"authorId\":\"48322708\",\"name\":\"D. Pak\"},{\"authorId\":\"2417978\",\"name\":\"Yuseok Ban\"},{\"authorId\":\"39847092\",\"name\":\"Sangyoun Lee\"}],\"doi\":\"10.1109/cvpr42600.2020.00536\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"af480bfec4eb0a8182ad52c0d0699fb86da8609b\",\"title\":\"AdaCoF: Adaptive Collaboration of Flows for Video Frame Interpolation\",\"url\":\"https://www.semanticscholar.org/paper/af480bfec4eb0a8182ad52c0d0699fb86da8609b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2008.04776\",\"authors\":[{\"authorId\":\"73329364\",\"name\":\"Jiangning Zhang\"},{\"authorId\":\"145194966\",\"name\":\"Chao Xu\"},{\"authorId\":\"1391190989\",\"name\":\"L. Liu\"},{\"authorId\":\"47446949\",\"name\":\"M. Wang\"},{\"authorId\":\"1490938675\",\"name\":\"Xia Wu\"},{\"authorId\":\"93006732\",\"name\":\"Y. Liu\"},{\"authorId\":\"122376816\",\"name\":\"Yunliang Jiang\"}],\"doi\":\"10.1007/978-3-030-58558-7_18\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c1847cbb4064ca05593f7b408783e9f183953488\",\"title\":\"DTVNet: Dynamic Time-lapse Video Generation via Single Still Image\",\"url\":\"https://www.semanticscholar.org/paper/c1847cbb4064ca05593f7b408783e9f183953488\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"2388085\",\"name\":\"M. K\\u00f6rner\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0fc9b202107bafa4b755c913c904d8ab046b8113\",\"title\":\"FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing Autoencoder GANs\",\"url\":\"https://www.semanticscholar.org/paper/0fc9b202107bafa4b755c913c904d8ab046b8113\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1679427\",\"name\":\"Marina L. Gavrilova\"},{\"authorId\":\"144891213\",\"name\":\"Jian Chang\"},{\"authorId\":\"41183001\",\"name\":\"Nadia Magnenat Thalmann\"},{\"authorId\":\"3278789\",\"name\":\"Eckhard Hitzer\"},{\"authorId\":\"145102331\",\"name\":\"Hiroshi Ishikawa\"}],\"doi\":\"10.1007/978-3-030-22514-8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8a4583c3db9ffb78c8942b0054024465033a76d2\",\"title\":\"Advances in Computer Graphics\",\"url\":\"https://www.semanticscholar.org/paper/8a4583c3db9ffb78c8942b0054024465033a76d2\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2019},{\"arxivId\":\"1902.06162\",\"authors\":[{\"authorId\":\"29724362\",\"name\":\"Longlong Jing\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/tpami.2020.2992393\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c94ee7df6bc2bfcac76703be4f059a79010f7e5\",\"title\":\"Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/4c94ee7df6bc2bfcac76703be4f059a79010f7e5\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66438378\",\"name\":\"Kongtao Zhu\"},{\"authorId\":\"3300934\",\"name\":\"Xiwei Liu\"},{\"authorId\":\"27391286\",\"name\":\"Hongxue Yang\"}],\"doi\":\"10.1109/CAC.2018.8623645\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f571725ffc18c6249702ab457b287495302a4e68\",\"title\":\"A Survey of Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/f571725ffc18c6249702ab457b287495302a4e68\",\"venue\":\"2018 Chinese Automation Congress (CAC)\",\"year\":2018},{\"arxivId\":\"1802.05957\",\"authors\":[{\"authorId\":\"3213400\",\"name\":\"Takeru Miyato\"},{\"authorId\":\"1984831\",\"name\":\"T. Kataoka\"},{\"authorId\":\"2877296\",\"name\":\"Masanori Koyama\"},{\"authorId\":\"51462146\",\"name\":\"Y. Yoshida\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"84de7d27e2f6160f634a483e8548c499a2cda7fa\",\"title\":\"Spectral Normalization for Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/84de7d27e2f6160f634a483e8548c499a2cda7fa\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1911.07806\",\"authors\":[{\"authorId\":\"11519650\",\"name\":\"Yuge Shi\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"},{\"authorId\":\"143750009\",\"name\":\"R. Hartley\"}],\"doi\":\"10.1007/978-3-030-01249-6_19\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3200539538eca54a85223bf0ec4f3ed132d0493\",\"title\":\"Action Anticipation with RBF Kernelized Feature Mapping RNN\",\"url\":\"https://www.semanticscholar.org/paper/b3200539538eca54a85223bf0ec4f3ed132d0493\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1805.09313\",\"authors\":[{\"authorId\":\"2160245\",\"name\":\"Konstantinos Vougioukas\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f722b0a7a9b7709d693b9d39195c779832a943fe\",\"title\":\"End-to-End Speech-Driven Facial Animation with Temporal GANs\",\"url\":\"https://www.semanticscholar.org/paper/f722b0a7a9b7709d693b9d39195c779832a943fe\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":\"1812.01874\",\"authors\":[{\"authorId\":\"5437547\",\"name\":\"Qiyang Hu\"},{\"authorId\":\"1822120361\",\"name\":\"Adrian Walchli\"},{\"authorId\":\"21529935\",\"name\":\"Tiziano Portenier\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"},{\"authorId\":\"144707901\",\"name\":\"P. Favaro\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bb74e29321772ea815f88769d31a902a2c3e996\",\"title\":\"Learning to Take Directions One Step at a Time\",\"url\":\"https://www.semanticscholar.org/paper/6bb74e29321772ea815f88769d31a902a2c3e996\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1904.03445\",\"authors\":[{\"authorId\":\"2096834\",\"name\":\"Lukasz Struski\"},{\"authorId\":\"145541197\",\"name\":\"J. Tabor\"},{\"authorId\":\"3245500\",\"name\":\"I. Podolak\"},{\"authorId\":\"46662285\",\"name\":\"Aleksandra Nowak\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b478876cbb734111175591eda91fbe7a53d3bef\",\"title\":\"Interpolation in generative models\",\"url\":\"https://www.semanticscholar.org/paper/7b478876cbb734111175591eda91fbe7a53d3bef\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144620586\",\"name\":\"X. Wu\"},{\"authorId\":\"144024533\",\"name\":\"Kun Xu\"},{\"authorId\":\"144003456\",\"name\":\"P. Hall\"}],\"doi\":\"10.23919/TST.2017.8195348\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"722514cf193ea8b301475de9da5a0061f2e47bdd\",\"title\":\"A survey of image synthesis and editing with generative adversarial networks\",\"url\":\"https://www.semanticscholar.org/paper/722514cf193ea8b301475de9da5a0061f2e47bdd\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"2003.00196\",\"authors\":[{\"authorId\":\"10753214\",\"name\":\"Aliaksandr Siarohin\"},{\"authorId\":\"3099587\",\"name\":\"St\\u00e9phane Lathuili\\u00e8re\"},{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"40811261\",\"name\":\"E. Ricci\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"797389ca052efd160ed759d7ef7adf9c30a917d6\",\"title\":\"First Order Motion Model for Image Animation\",\"url\":\"https://www.semanticscholar.org/paper/797389ca052efd160ed759d7ef7adf9c30a917d6\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819153751\",\"name\":\"Pengfei Sun\"},{\"authorId\":\"145809273\",\"name\":\"Xin Su\"},{\"authorId\":\"1818552572\",\"name\":\"Shangqi Guo\"},{\"authorId\":\"144180417\",\"name\":\"Feng Chen\"}],\"doi\":\"10.1007/s10489-020-01750-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c64e475888c76f3313f9eeebeeb38eb4d9fb0dd\",\"title\":\"Cycle representation-disentangling network: learning to completely disentangle spatial-temporal features in video\",\"url\":\"https://www.semanticscholar.org/paper/6c64e475888c76f3313f9eeebeeb38eb4d9fb0dd\",\"venue\":\"Applied Intelligence\",\"year\":2020},{\"arxivId\":\"1912.06606\",\"authors\":[{\"authorId\":\"1466503743\",\"name\":\"X. Ren\"},{\"authorId\":null,\"name\":\"Haoran Li\"},{\"authorId\":\"48783196\",\"name\":\"Zijian Huang\"},{\"authorId\":\"143832240\",\"name\":\"Qifeng Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3803023eb13d986b8776d5856df70e801efd74da\",\"title\":\"Music-oriented Dance Video Synthesis with Pose Perceptual Loss\",\"url\":\"https://www.semanticscholar.org/paper/3803023eb13d986b8776d5856df70e801efd74da\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.05519\",\"authors\":[{\"authorId\":\"2210804\",\"name\":\"Nathanael Perraudin\"},{\"authorId\":\"48294059\",\"name\":\"Ankit Srivastava\"},{\"authorId\":\"40401747\",\"name\":\"A. Lucchi\"},{\"authorId\":\"49287712\",\"name\":\"T. Kacprzak\"},{\"authorId\":\"47483298\",\"name\":\"T. Hofmann\"},{\"authorId\":\"123213185\",\"name\":\"A. R\\u00e9fr\\u00e9gier\"}],\"doi\":\"10.1186/s40668-019-0032-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a9ea825bc7114a8c2eb3505c5b5ab6f59fbd0d62\",\"title\":\"Cosmological N-body simulations: a challenge for scalable generative models\",\"url\":\"https://www.semanticscholar.org/paper/a9ea825bc7114a8c2eb3505c5b5ab6f59fbd0d62\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1612.02136\",\"authors\":[{\"authorId\":\"47828117\",\"name\":\"Tong Che\"},{\"authorId\":\"3305402\",\"name\":\"Yanran Li\"},{\"authorId\":\"12782441\",\"name\":\"Athul Paul Jacob\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"50135831\",\"name\":\"W. Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f0fb306a3ea4e31f59b18ffeb497054ea934ba6a\",\"title\":\"Mode Regularized Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/f0fb306a3ea4e31f59b18ffeb497054ea934ba6a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48531958\",\"name\":\"Jonathan Rosenthal\"},{\"authorId\":\"113138355\",\"name\":\"Saturday\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2f58c94d7158c89af03a16b48678ce37b37192af\",\"title\":\"Generative Temporal Models for Cosmology Master Thesis\",\"url\":\"https://www.semanticscholar.org/paper/2f58c94d7158c89af03a16b48678ce37b37192af\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1452981772\",\"name\":\"Lei Zhang\"},{\"authorId\":\"1699643\",\"name\":\"C. Pollett\"},{\"authorId\":\"2508779\",\"name\":\"P. Heller\"}],\"doi\":null,\"intent\":[\"methodology\",\"result\"],\"isInfluential\":true,\"paperId\":\"c43302d64d1ae8092cf6f0758cf1981f83b5b7a7\",\"title\":\"Video Synthesis from the StyleGAN Latent Space\",\"url\":\"https://www.semanticscholar.org/paper/c43302d64d1ae8092cf6f0758cf1981f83b5b7a7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1903.11633\",\"authors\":[{\"authorId\":\"4056993\",\"name\":\"J. P. Robinson\"},{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"145002061\",\"name\":\"N. Zhang\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"},{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"}],\"doi\":\"10.1109/ICCV.2019.01020\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d684c197c06139319bf575e2498fe040d9ea32f\",\"title\":\"Laplace Landmark Localization\",\"url\":\"https://www.semanticscholar.org/paper/5d684c197c06139319bf575e2498fe040d9ea32f\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1812.10265\",\"authors\":[{\"authorId\":\"144772331\",\"name\":\"X. Zheng\"},{\"authorId\":\"144152334\",\"name\":\"Yanqing Guo\"},{\"authorId\":\"32885778\",\"name\":\"Huaibo Huang\"},{\"authorId\":\"31256513\",\"name\":\"Yi Li\"},{\"authorId\":\"143712929\",\"name\":\"R. He\"}],\"doi\":\"10.1007/s11263-020-01308-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"562b21e58805061f898c421848ec837a70d3017d\",\"title\":\"A Survey of Deep Facial Attribute Analysis\",\"url\":\"https://www.semanticscholar.org/paper/562b21e58805061f898c421848ec837a70d3017d\",\"venue\":\"International Journal of Computer Vision\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":\"10.1007/978-981-13-0716-4_3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a52bf74e6bcbba58e80fbba11f96c1af61f890a\",\"title\":\"From Recognition to Generation Using Deep Learning: A Case Study with Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/4a52bf74e6bcbba58e80fbba11f96c1af61f890a\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"2011.02638\",\"authors\":[{\"authorId\":\"31173467\",\"name\":\"K. Liu\"},{\"authorId\":\"10830369\",\"name\":\"G. Cao\"},{\"authorId\":\"143741775\",\"name\":\"F. Zhou\"},{\"authorId\":\"3185721\",\"name\":\"Bozhi Liu\"},{\"authorId\":\"144408268\",\"name\":\"Jiang Duan\"},{\"authorId\":\"1713506\",\"name\":\"Guoping Qiu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d47978a3549763a48ca480180bd78c3ec6206f37\",\"title\":\"Disentangling Latent Space for Unsupervised Semantic Face Editing\",\"url\":\"https://www.semanticscholar.org/paper/d47978a3549763a48ca480180bd78c3ec6206f37\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8080821\",\"name\":\"Megha Nawhal\"},{\"authorId\":\"2453402\",\"name\":\"Meng-Yao Zhai\"},{\"authorId\":\"20812150\",\"name\":\"Andreas M. Lehrmann\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"53ea5e0448c309c3614bba25bac58f46f06690c7\",\"title\":\"Zero-Shot Generation of Human-Object Interaction Videos\",\"url\":\"https://www.semanticscholar.org/paper/53ea5e0448c309c3614bba25bac58f46f06690c7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2208488\",\"name\":\"Bernhard Kratzwald\"},{\"authorId\":\"145314891\",\"name\":\"Z. Huang\"},{\"authorId\":\"35268081\",\"name\":\"D. Paudel\"},{\"authorId\":\"71879671\",\"name\":\"A. Dinesh\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d0b936f643f7462068517e0a840e775d6bd4abfb\",\"title\":\"Improving Video Generation for Multi-functional Applications.\",\"url\":\"https://www.semanticscholar.org/paper/d0b936f643f7462068517e0a840e775d6bd4abfb\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1812.01874\",\"authors\":[{\"authorId\":\"5437547\",\"name\":\"Qiyang Hu\"},{\"authorId\":\"52148417\",\"name\":\"Adrian Waelchli\"},{\"authorId\":\"21529935\",\"name\":\"Tiziano Portenier\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"},{\"authorId\":\"145646305\",\"name\":\"P. Favaro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"949ec7afb546060a8c8929462b2eb7bdb468f30a\",\"title\":\"Video Synthesis from a Single Image and Motion Stroke\",\"url\":\"https://www.semanticscholar.org/paper/949ec7afb546060a8c8929462b2eb7bdb468f30a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1909.00475\",\"authors\":[{\"authorId\":\"47231927\",\"name\":\"G. Balakrishnan\"},{\"authorId\":\"3046516\",\"name\":\"Adrian V. Dalca\"},{\"authorId\":\"46917346\",\"name\":\"Amy Zhao\"},{\"authorId\":\"1724429\",\"name\":\"J. Guttag\"},{\"authorId\":\"145403226\",\"name\":\"F. Durand\"},{\"authorId\":\"1768236\",\"name\":\"W. Freeman\"}],\"doi\":\"10.1109/ICCV.2019.00026\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"374162524b3fffc1f926c05c5539f01a5d542014\",\"title\":\"Visual Deprojection: Probabilistic Recovery of Collapsed Dimensions\",\"url\":\"https://www.semanticscholar.org/paper/374162524b3fffc1f926c05c5539f01a5d542014\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1804.03429\",\"authors\":[{\"authorId\":\"2399563\",\"name\":\"Chongxuan Li\"},{\"authorId\":\"1678311\",\"name\":\"M. Welling\"},{\"authorId\":\"145254043\",\"name\":\"J. Zhu\"},{\"authorId\":\"49846744\",\"name\":\"Bo Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f17c56540118af065cf2a72f4b56d5b17d84f5ca\",\"title\":\"Graphical Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/f17c56540118af065cf2a72f4b56d5b17d84f5ca\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1808.06601\",\"authors\":[{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"47062069\",\"name\":\"Guilin Liu\"},{\"authorId\":\"29955511\",\"name\":\"A. Tao\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2301680\",\"name\":\"Bryan Catanzaro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5b55f410365bb889c25a9f0354f2b86ec61c4f0\",\"title\":\"Video-to-Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/c5b55f410365bb889c25a9f0354f2b86ec61c4f0\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48096253\",\"name\":\"Long Zhao\"},{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"144978989\",\"name\":\"Y. Tian\"},{\"authorId\":\"1379758006\",\"name\":\"Mubbasir Kapadia\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1007/s11263-020-01328-9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0ca9eb353e33e37e725a8f89bd07ca1fc09a285\",\"title\":\"Towards Image-to-Video Translation: A Structure-Aware Approach via Multi-stage Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/f0ca9eb353e33e37e725a8f89bd07ca1fc09a285\",\"venue\":\"International Journal of Computer Vision\",\"year\":2020},{\"arxivId\":\"1901.11384\",\"authors\":[{\"authorId\":\"153804922\",\"name\":\"Isabela Albuquerque\"},{\"authorId\":\"144903711\",\"name\":\"J. Monteiro\"},{\"authorId\":\"2632038\",\"name\":\"T. Falk\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15767267a679da3a41e37f19fdcf68e0c2eb86ad\",\"title\":\"Learning to navigate image manifolds induced by generative adversarial networks for unsupervised video generation\",\"url\":\"https://www.semanticscholar.org/paper/15767267a679da3a41e37f19fdcf68e0c2eb86ad\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"1382657362\",\"name\":\"Xavier Alameda-Pineda\"},{\"authorId\":\"144742695\",\"name\":\"D. Xu\"},{\"authorId\":\"40811261\",\"name\":\"E. Ricci\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1109/TMM.2019.2963621\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"92027cb7282f2cae56f223ca313150f7e9ce0055\",\"title\":\"Learning How to Smile: Expression Video Generation With Conditional Adversarial Recurrent Nets\",\"url\":\"https://www.semanticscholar.org/paper/92027cb7282f2cae56f223ca313150f7e9ce0055\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1905.08474\",\"authors\":[{\"authorId\":\"46183801\",\"name\":\"Yakov Miron\"},{\"authorId\":\"120106969\",\"name\":\"Yona Coscas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"22c4be3b470348c2d846e24cec30f7a3ed1de20c\",\"title\":\"S-Flow GAN\",\"url\":\"https://www.semanticscholar.org/paper/22c4be3b470348c2d846e24cec30f7a3ed1de20c\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yaohui Wang\"},{\"authorId\":\"105070829\",\"name\":\"P. Bilinski\"},{\"authorId\":\"69929964\",\"name\":\"F. Br\\u00e9mond\"},{\"authorId\":\"3299530\",\"name\":\"A. Dantcheva\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8cfb522acca27ee7b0cdd1e4a8f5af492ac5c87d\",\"title\":\"G3AN: This video does not exist. Disentangling motion and appearance for video generation\",\"url\":\"https://www.semanticscholar.org/paper/8cfb522acca27ee7b0cdd1e4a8f5af492ac5c87d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.03250\",\"authors\":[{\"authorId\":\"2985797\",\"name\":\"U. Tantipongpipat\"},{\"authorId\":\"1451646307\",\"name\":\"Chris Waites\"},{\"authorId\":\"27640659\",\"name\":\"Digvijay Boob\"},{\"authorId\":\"1451646488\",\"name\":\"Amaresh Ankit Siva\"},{\"authorId\":\"49326047\",\"name\":\"R. Cummings\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a17ec0b8e806fcffb0bbfcfe3cc7ae6d11cc096\",\"title\":\"Differentially Private Mixed-Type Data Generation For Unsupervised Learning\",\"url\":\"https://www.semanticscholar.org/paper/7a17ec0b8e806fcffb0bbfcfe3cc7ae6d11cc096\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1712.03534\",\"authors\":[{\"authorId\":\"2017906\",\"name\":\"Wissam J. Baddar\"},{\"authorId\":\"2707832\",\"name\":\"Geonmo Gu\"},{\"authorId\":\"2909533\",\"name\":\"S. Lee\"},{\"authorId\":\"7251290\",\"name\":\"Yong Man Ro\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"deb0cbced8a5986e0771209cf786b0ebdedc119d\",\"title\":\"Dynamics Transfer GAN: Generating Video by Transferring Arbitrary Temporal Dynamics from a Source Video to a Single Target Image\",\"url\":\"https://www.semanticscholar.org/paper/deb0cbced8a5986e0771209cf786b0ebdedc119d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40617564\",\"name\":\"W. Li\"},{\"authorId\":\"51305314\",\"name\":\"Zehuan Yuan\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"},{\"authorId\":\"1697065\",\"name\":\"C. Wang\"}],\"doi\":\"10.1109/icme46284.2020.9102778\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d3f072ef328c2a9fd0649ce8f27fab8839ff4bdd\",\"title\":\"Moflowgan: Video Generation With Flow Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d3f072ef328c2a9fd0649ce8f27fab8839ff4bdd\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":\"1912.07991\",\"authors\":[{\"authorId\":\"1469066868\",\"name\":\"Yatin Dandi\"},{\"authorId\":\"19089337\",\"name\":\"Aniket Das\"},{\"authorId\":\"1466543874\",\"name\":\"Soumye Singhal\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"145593549\",\"name\":\"P. Rai\"}],\"doi\":\"10.1109/WACV45572.2020.9093308\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c33a293453240a02ee335b5b27d2b23c9882fb1b\",\"title\":\"Jointly Trained Image and Video Generation using Residual Vectors\",\"url\":\"https://www.semanticscholar.org/paper/c33a293453240a02ee335b5b27d2b23c9882fb1b\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2008.05856\",\"authors\":[{\"authorId\":\"48002920\",\"name\":\"Hongyuan Yu\"},{\"authorId\":\"144368930\",\"name\":\"Yan Huang\"},{\"authorId\":\"150192018\",\"name\":\"Lihong Pi\"},{\"authorId\":\"97846606\",\"name\":\"L. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8ac4c4c5dacd16a3a06fe75f95564af0a1d0a79\",\"title\":\"Recurrent Deconvolutional Generative Adversarial Networks with Application to Text Guided Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e8ac4c4c5dacd16a3a06fe75f95564af0a1d0a79\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.04254\",\"authors\":[{\"authorId\":\"70508259\",\"name\":\"S. Chen\"},{\"authorId\":\"50264653\",\"name\":\"Peng Zhang\"},{\"authorId\":\"1744228\",\"name\":\"Xinge You\"},{\"authorId\":\"2647338\",\"name\":\"Qinmu Peng\"},{\"authorId\":null,\"name\":\"Xin Liu\"},{\"authorId\":\"47060588\",\"name\":\"Zehong Cao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fdcb735c637139de69c5871846f738c736c9e70\",\"title\":\"Similarity-DT: Kernel Similarity Embedding for Dynamic Texture Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/5fdcb735c637139de69c5871846f738c736c9e70\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2003.07101\",\"authors\":[{\"authorId\":\"35836678\",\"name\":\"Moritz Kampelm\\u00fchler\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"}],\"doi\":\"10.1109/WACV45572.2020.9093440\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d8a25f0bbc7c757a563f674502c2b38f3dc1366\",\"title\":\"Synthesizing human-like sketches from natural images using a conditional convolutional decoder\",\"url\":\"https://www.semanticscholar.org/paper/7d8a25f0bbc7c757a563f674502c2b38f3dc1366\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48390722\",\"name\":\"Gong Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f128ca4614fdb69ddf89870ca6fd6f975da885f1\",\"title\":\"Artificial intelligence in music : composition and emotion\",\"url\":\"https://www.semanticscholar.org/paper/f128ca4614fdb69ddf89870ca6fd6f975da885f1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17659319\",\"name\":\"Zackary P. T. Sin\"},{\"authorId\":\"2774268\",\"name\":\"P. H. Ng\"},{\"authorId\":\"1738911\",\"name\":\"S. Shiu\"},{\"authorId\":\"145288211\",\"name\":\"K. Chung\"},{\"authorId\":\"1714454\",\"name\":\"H. Leong\"}],\"doi\":\"10.1007/978-3-030-22514-8_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"725d8d026e1b9be741c10ae6bc0608ccea0d8ddb\",\"title\":\"Multi-level Motion-Informed Approach for Video Generation with Key Frames\",\"url\":\"https://www.semanticscholar.org/paper/725d8d026e1b9be741c10ae6bc0608ccea0d8ddb\",\"venue\":\"CGI\",\"year\":2019},{\"arxivId\":\"1912.02401\",\"authors\":[{\"authorId\":\"8080821\",\"name\":\"Megha Nawhal\"},{\"authorId\":\"1936990\",\"name\":\"Mengyao Zhai\"},{\"authorId\":\"20812150\",\"name\":\"Andreas M. Lehrmann\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"10771328\",\"name\":\"G. Mori\"}],\"doi\":\"10.1007/978-3-030-58610-2_23\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4ebb4985c6da258fe47a63d1f9dc1ae491e6db62\",\"title\":\"Generating Videos of Zero-Shot Compositions of Actions and Objects\",\"url\":\"https://www.semanticscholar.org/paper/4ebb4985c6da258fe47a63d1f9dc1ae491e6db62\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2004.01546\",\"authors\":[{\"authorId\":\"34735743\",\"name\":\"T. Fernando\"},{\"authorId\":\"1729760\",\"name\":\"S. Sridharan\"},{\"authorId\":\"3282778\",\"name\":\"Mitchell McLaren\"},{\"authorId\":\"51123172\",\"name\":\"Darshana Priyasad\"},{\"authorId\":\"1980700\",\"name\":\"Simon Denman\"},{\"authorId\":\"3140440\",\"name\":\"C. Fookes\"}],\"doi\":\"10.1109/TASLP.2020.2982297\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfcf9e6f8662dde85f2da78e0adbcbe199f49e1e\",\"title\":\"Temporarily-Aware Context Modeling Using Generative Adversarial Networks for Speech Activity Detection\",\"url\":\"https://www.semanticscholar.org/paper/dfcf9e6f8662dde85f2da78e0adbcbe199f49e1e\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2020},{\"arxivId\":\"1904.02912\",\"authors\":[{\"authorId\":\"40897201\",\"name\":\"Tsun-Hsuan Wang\"},{\"authorId\":\"94288126\",\"name\":\"Y. Cheng\"},{\"authorId\":\"49044307\",\"name\":\"Chieh Hubert Lin\"},{\"authorId\":\"1803730\",\"name\":\"Hwann-Tzong Chen\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2019.01059\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb0a151cf8740c66b461ff809e086b91f14bd23b\",\"title\":\"Point-to-Point Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/cb0a151cf8740c66b461ff809e086b91f14bd23b\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1903.00913\",\"authors\":[{\"authorId\":\"37145669\",\"name\":\"Lu Sheng\"},{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"47093626\",\"name\":\"Jiaming Guo\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"},{\"authorId\":\"48631549\",\"name\":\"X. Wang\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"545f3b0c15a0a555fda0d2377646c0c70090c185\",\"title\":\"Unsupervised Bi-directional Flow-based Video Generation from one Snapshot\",\"url\":\"https://www.semanticscholar.org/paper/545f3b0c15a0a555fda0d2377646c0c70090c185\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.03864\",\"authors\":[{\"authorId\":\"2007745319\",\"name\":\"Cade Gordon\"},{\"authorId\":\"2326758\",\"name\":\"Natalie Parde\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac4e977aba1731ce1bd5ddeb9ea466fa86c10a5f\",\"title\":\"Latent Neural Differential Equations for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/ac4e977aba1731ce1bd5ddeb9ea466fa86c10a5f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.10699\",\"authors\":[{\"authorId\":\"50415746\",\"name\":\"Shruti Vyas\"},{\"authorId\":\"2116440\",\"name\":\"Y. Rawat\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fc941fbbc906bf7834a21eea520e2d2277aa57f5\",\"title\":\"Time-Aware and View-Aware Video Rendering for Unsupervised Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/fc941fbbc906bf7834a21eea520e2d2277aa57f5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145557680\",\"name\":\"Luca Melis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8db6be3938c5c77bba78818de274675a72ac79e\",\"title\":\"Building and evaluating privacy-preserving data processing systems\",\"url\":\"https://www.semanticscholar.org/paper/b8db6be3938c5c77bba78818de274675a72ac79e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1808.05174\",\"authors\":[{\"authorId\":\"3294630\",\"name\":\"Aayush Bansal\"},{\"authorId\":\"2863531\",\"name\":\"Shugao Ma\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"1774867\",\"name\":\"Yaser Sheikh\"}],\"doi\":\"10.1007/978-3-030-01228-1_8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8ad3c383a79e85159098112127300dfd08c21319\",\"title\":\"Recycle-GAN: Unsupervised Video Retargeting\",\"url\":\"https://www.semanticscholar.org/paper/8ad3c383a79e85159098112127300dfd08c21319\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1909.11975\",\"authors\":[{\"authorId\":\"39380386\",\"name\":\"Jianwen Xie\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"},{\"authorId\":\"39092098\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/TPAMI.2019.2934852\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4b2207e6de1e759ac7a7f45abe7a0b1d1da4558\",\"title\":\"Learning Energy-based Spatial-Temporal Generative ConvNets for Dynamic Patterns\",\"url\":\"https://www.semanticscholar.org/paper/a4b2207e6de1e759ac7a7f45abe7a0b1d1da4558\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51149606\",\"name\":\"Jeffrey M. Ede\"},{\"authorId\":\"31436765\",\"name\":\"J. J. Peters\"},{\"authorId\":\"22404881\",\"name\":\"J. Sloan\"},{\"authorId\":\"40586446\",\"name\":\"R. Beanland\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"754c25ff4730b57a9a1787ce8f530ff28295971f\",\"title\":\"Exit Wavefunction Reconstruction from Single Transmission Electron Micrographs with Deep Learning [pre-print]\",\"url\":\"https://www.semanticscholar.org/paper/754c25ff4730b57a9a1787ce8f530ff28295971f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17659319\",\"name\":\"Zackary P. T. Sin\"},{\"authorId\":\"2774268\",\"name\":\"P. H. Ng\"},{\"authorId\":\"1738911\",\"name\":\"S. Shiu\"},{\"authorId\":\"145288211\",\"name\":\"K. Chung\"},{\"authorId\":\"1714454\",\"name\":\"H. Leong\"}],\"doi\":\"10.1145/3297280.3297301\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1cf969cca9a3dcbe042ecf0a4a0ec5bb80022c1\",\"title\":\"2D character animating networks: bringing static characters to move via motion transfer\",\"url\":\"https://www.semanticscholar.org/paper/a1cf969cca9a3dcbe042ecf0a4a0ec5bb80022c1\",\"venue\":\"SAC\",\"year\":2019},{\"arxivId\":\"1909.09283\",\"authors\":[{\"authorId\":\"9284940\",\"name\":\"Harshala Gammulle\"},{\"authorId\":\"34735743\",\"name\":\"T. Fernando\"},{\"authorId\":\"1980700\",\"name\":\"Simon Denman\"},{\"authorId\":\"1729760\",\"name\":\"S. Sridharan\"},{\"authorId\":\"3140440\",\"name\":\"C. Fookes\"}],\"doi\":\"10.1109/WACV.2019.00027\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ced33344402d74f69367caa163e94918f307ab78\",\"title\":\"Coupled Generative Adversarial Network for Continuous Fine-Grained Action Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/ced33344402d74f69367caa163e94918f307ab78\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2028218872\",\"name\":\"Kara Marie Schatz\"},{\"authorId\":\"1607110764\",\"name\":\"Erik Quintanilla\"},{\"authorId\":\"50415746\",\"name\":\"Shruti Vyas\"},{\"authorId\":\"2116440\",\"name\":\"Y. Rawat\"}],\"doi\":\"10.1007/978-3-030-58583-9_25\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"64eba0de822754df1bc1303f96163265dea9ac4f\",\"title\":\"A Recurrent Transformer Network for Novel View Action Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/64eba0de822754df1bc1303f96163265dea9ac4f\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47932716\",\"name\":\"X. Huang\"},{\"authorId\":\"151474565\",\"name\":\"Mingjie Wang\"},{\"authorId\":\"1473876432\",\"name\":\"M. Gong\"}],\"doi\":\"10.1007/S00371-020-01982-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef6467ddba06210c7cd0677955234e2e850274f0\",\"title\":\"Fine-grained talking face generation with video reinterpretation\",\"url\":\"https://www.semanticscholar.org/paper/ef6467ddba06210c7cd0677955234e2e850274f0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49901552\",\"name\":\"Gong Chen\"},{\"authorId\":\"47909587\",\"name\":\"Y. Liu\"},{\"authorId\":\"2812440\",\"name\":\"S. Zhong\"},{\"authorId\":\"46447747\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1145/3240508.3240604\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f9cd1278bda3f3db4b08a5908916d15b842eb47e\",\"title\":\"Musicality-Novelty Generative Adversarial Nets for Algorithmic Composition\",\"url\":\"https://www.semanticscholar.org/paper/f9cd1278bda3f3db4b08a5908916d15b842eb47e\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9638825\",\"name\":\"N. Ronquillo\"},{\"authorId\":\"1745184\",\"name\":\"Josh Harguess\"}],\"doi\":\"10.1109/AIPR.2018.8707431\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f23d889dce3453da1795a5463869efc76ae0507f\",\"title\":\"On Evaluating Video-based Generative Adversarial Networks (GANs)\",\"url\":\"https://www.semanticscholar.org/paper/f23d889dce3453da1795a5463869efc76ae0507f\",\"venue\":\"2018 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)\",\"year\":2018},{\"arxivId\":\"1909.12400\",\"authors\":[{\"authorId\":\"1389556562\",\"name\":\"V. Yushchenko\"},{\"authorId\":\"1942495\",\"name\":\"Nikita Araslanov\"},{\"authorId\":\"46840930\",\"name\":\"S. Roth\"}],\"doi\":\"10.1109/ICCVW.2019.00190\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa6bb4f4ad12ca1ff14faff0c2147de3be059e79\",\"title\":\"Markov Decision Process for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/aa6bb4f4ad12ca1ff14faff0c2147de3be059e79\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48002920\",\"name\":\"Hongyuan Yu\"},{\"authorId\":\"13557066\",\"name\":\"Y. Huang\"},{\"authorId\":\"150192018\",\"name\":\"Lihong Pi\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"}],\"doi\":\"10.1007/978-3-030-31723-2_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c6e9b53df1c21649d21fd1d77765fc20154a3a3\",\"title\":\"Recurrent Deconvolutional Generative Adversarial Networks with Application to Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/9c6e9b53df1c21649d21fd1d77765fc20154a3a3\",\"venue\":\"PRCV\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145116188\",\"name\":\"Rajat Arora\"},{\"authorId\":\"144756076\",\"name\":\"Y. Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7223fb56eeece5806f8d25718bbc78386e17f19f\",\"title\":\"SinGAN-GIF: Learning a Generative Video Model from a Single GIF\",\"url\":\"https://www.semanticscholar.org/paper/7223fb56eeece5806f8d25718bbc78386e17f19f\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409705906\",\"name\":\"S. Palazzo\"},{\"authorId\":\"2441118\",\"name\":\"C. Spampinato\"},{\"authorId\":\"82619398\",\"name\":\"P. D'Oro\"},{\"authorId\":\"35906202\",\"name\":\"D. Giordano\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":\"10.1007/978-3-030-11012-3_37\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d3fccec7ab3d5e1a630c108e8a1284cf38fc53d9\",\"title\":\"Generating Synthetic Video Sequences by Explicitly Modeling Object Motion\",\"url\":\"https://www.semanticscholar.org/paper/d3fccec7ab3d5e1a630c108e8a1284cf38fc53d9\",\"venue\":\"ECCV Workshops\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50218076\",\"name\":\"Z. Wang\"},{\"authorId\":\"51300955\",\"name\":\"Taiping Yao\"},{\"authorId\":\"48056150\",\"name\":\"Huawei Wei\"},{\"authorId\":\"51303943\",\"name\":\"Shanyan Guan\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"}],\"doi\":\"10.1007/978-3-030-00767-6_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf52f10c6f23f99cda01b7c33541f4dab1f7667d\",\"title\":\"Multi-person/Group Interactive Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/bf52f10c6f23f99cda01b7c33541f4dab1f7667d\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2274692\",\"name\":\"Abhishek Aich\"},{\"authorId\":\"150015249\",\"name\":\"Akash Gupta\"},{\"authorId\":\"1819152\",\"name\":\"R. Panda\"},{\"authorId\":\"1605999515\",\"name\":\"Rakib Hyder\"},{\"authorId\":\"27996204\",\"name\":\"M. Asif\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34697d133f69201a5fab15fdf60dacda01314bd2\",\"title\":\"Non-Adversarial Video Synthesis with Learned Priors (Supplementary Material)\",\"url\":\"https://www.semanticscholar.org/paper/34697d133f69201a5fab15fdf60dacda01314bd2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31993415\",\"name\":\"A. Clark\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"83769658\",\"name\":\"K. Simonyan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"12c37cb419121cdb43f2c6620303932f43e2e1b7\",\"title\":\"Adversarial Video Generation on Complex Datasets\",\"url\":\"https://www.semanticscholar.org/paper/12c37cb419121cdb43f2c6620303932f43e2e1b7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1898782\",\"name\":\"Shoichiro Yamaguchi\"},{\"authorId\":\"2877296\",\"name\":\"Masanori Koyama\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"81473ba213bae1aa188e03e54f91319914159c9c\",\"title\":\"Distributional Concavity Regularization for GANs\",\"url\":\"https://www.semanticscholar.org/paper/81473ba213bae1aa188e03e54f91319914159c9c\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"2011.02250\",\"authors\":[{\"authorId\":\"51235324\",\"name\":\"Nuha Aldausari\"},{\"authorId\":\"145313633\",\"name\":\"A. Sowmya\"},{\"authorId\":\"47600265\",\"name\":\"N. Marcus\"},{\"authorId\":\"4911295\",\"name\":\"G. Mohammadi\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b\",\"title\":\"Video Generative Adversarial Networks: A Review\",\"url\":\"https://www.semanticscholar.org/paper/5c1b1ab734ebc6ba56ea5d8af9d01a9fe8e0ba8b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.10587\",\"authors\":[{\"authorId\":\"39380386\",\"name\":\"Jianwen Xie\"},{\"authorId\":\"9659905\",\"name\":\"Ruiqi Gao\"},{\"authorId\":\"49774254\",\"name\":\"Zilong Zheng\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"},{\"authorId\":\"39092098\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1609/aaai.v33i01.33015498\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7b4b2787aed8b1652f5268c6b7dbfd63d9795939\",\"title\":\"Learning Dynamic Generator Model by Alternating Back-Propagation Through Time\",\"url\":\"https://www.semanticscholar.org/paper/7b4b2787aed8b1652f5268c6b7dbfd63d9795939\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"84200540\",\"name\":\"Lu Sheng\"},{\"authorId\":\"1720856277\",\"name\":\"Junting Pan\"},{\"authorId\":\"48605271\",\"name\":\"J. Guo\"},{\"authorId\":\"144478191\",\"name\":\"J. Shao\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":\"10.1007/s11263-020-01334-x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"991649f2903d05a27243a7d26016c4df735a6fbb\",\"title\":\"High-Quality Video Generation from Static Structural Annotations\",\"url\":\"https://www.semanticscholar.org/paper/991649f2903d05a27243a7d26016c4df735a6fbb\",\"venue\":\"International Journal of Computer Vision\",\"year\":2020},{\"arxivId\":\"1812.00452\",\"authors\":[{\"authorId\":null,\"name\":\"Hang Gao\"},{\"authorId\":\"3286703\",\"name\":\"Huazhe Xu\"},{\"authorId\":\"46193391\",\"name\":\"Qi-Zhi Cai\"},{\"authorId\":\"46886239\",\"name\":\"R. Wang\"},{\"authorId\":\"1807197\",\"name\":\"F. Yu\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/ICCV.2019.00910\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ce71c5b4c959c34715503a5980e457e700db9e70\",\"title\":\"Disentangling Propagation and Generation for Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ce71c5b4c959c34715503a5980e457e700db9e70\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1908.10515\",\"authors\":[{\"authorId\":\"3098768\",\"name\":\"Junhyuk Kim\"},{\"authorId\":\"2572894\",\"name\":\"C. Lee\"}],\"doi\":\"10.1016/j.jcp.2019.109216\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"36f7cfd15ec92239e0cdd3a034702c29010d972c\",\"title\":\"Deep unsupervised learning of turbulence for inflow generation at various Reynolds numbers\",\"url\":\"https://www.semanticscholar.org/paper/36f7cfd15ec92239e0cdd3a034702c29010d972c\",\"venue\":\"J. Comput. Phys.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49479754\",\"name\":\"Linlin Liu\"},{\"authorId\":\"49724537\",\"name\":\"Haijun Zhang\"},{\"authorId\":\"48670367\",\"name\":\"X. Xu\"},{\"authorId\":\"51092685\",\"name\":\"Z. Zhang\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":\"10.1109/TNNLS.2019.2944979\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"63d04a47e567878d272669e543bc1a2c22b5adea\",\"title\":\"Collocating Clothes With Generative Adversarial Networks Cosupervised by Categories and Attributes: A Multidiscriminator Framework\",\"url\":\"https://www.semanticscholar.org/paper/63d04a47e567878d272669e543bc1a2c22b5adea\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39380386\",\"name\":\"Jianwen Xie\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"},{\"authorId\":\"39092098\",\"name\":\"Y. Wu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"879eca33fa23688eb0b586e6b84a3a30351541c5\",\"title\":\"Dynamic Pattern Synthesis by Spatial-Temporal Generative ConvNet\",\"url\":\"https://www.semanticscholar.org/paper/879eca33fa23688eb0b586e6b84a3a30351541c5\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2006.15327\",\"authors\":[{\"authorId\":\"48319922\",\"name\":\"A. Bar\"},{\"authorId\":\"46796686\",\"name\":\"Roei Herzig\"},{\"authorId\":\"122024152\",\"name\":\"Xiaolong Wang\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1786843\",\"name\":\"A. Globerson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cdc4ae0adcff94e9ea4591f25b81f88c93cb4fbe\",\"title\":\"Compositional Video Synthesis with Action Graphs\",\"url\":\"https://www.semanticscholar.org/paper/cdc4ae0adcff94e9ea4591f25b81f88c93cb4fbe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1707.04993\",\"authors\":[{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00165\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"title\":\"MoCoGAN: Decomposing Motion and Content for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144898072\",\"name\":\"Xiao Jin\"},{\"authorId\":\"2989256\",\"name\":\"Peiguang Jing\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/ACCESS.2018.2867370\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"878de992815b4e491a9795239f525b5647986d17\",\"title\":\"AMFNet: An Adversarial Network for Median Filtering Detection\",\"url\":\"https://www.semanticscholar.org/paper/878de992815b4e491a9795239f525b5647986d17\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1811.09245\",\"authors\":[{\"authorId\":\"1725418556\",\"name\":\"Masaki Saito\"},{\"authorId\":\"3083107\",\"name\":\"Shunta Saito\"},{\"authorId\":\"152400765\",\"name\":\"Masanori Koyama\"},{\"authorId\":\"3456592\",\"name\":\"S. Kobayashi\"}],\"doi\":\"10.1007/s11263-020-01333-y\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"14b175654024c6b57653239674305fe91bca89a1\",\"title\":\"Train Sparsely, Generate Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN\",\"url\":\"https://www.semanticscholar.org/paper/14b175654024c6b57653239674305fe91bca89a1\",\"venue\":\"International Journal of Computer Vision\",\"year\":2020},{\"arxivId\":\"1906.01689\",\"authors\":[{\"authorId\":\"134493188\",\"name\":\"Maximilian Werhahn\"},{\"authorId\":\"47779150\",\"name\":\"Y. Xie\"},{\"authorId\":\"32464128\",\"name\":\"Mengyu Chu\"},{\"authorId\":\"1786445\",\"name\":\"N. Th\\u00fcrey\"}],\"doi\":\"10.1145/3340251\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03129ebc427191e55f710877bd91de99dc85ffc3\",\"title\":\"A Multi-Pass GAN for Fluid Flow Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/03129ebc427191e55f710877bd91de99dc85ffc3\",\"venue\":\"PACMCGIT\",\"year\":2019},{\"arxivId\":\"2006.08571\",\"authors\":[{\"authorId\":\"40084973\",\"name\":\"Tianlin Xu\"},{\"authorId\":\"49432923\",\"name\":\"Li K Wenliang\"},{\"authorId\":\"40484552\",\"name\":\"M. Munn\"},{\"authorId\":\"1902556\",\"name\":\"B. Acciaio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"53073458ad3f387f6c97af28467749c9f148c9cd\",\"title\":\"COT-GAN: Generating Sequential Data via Causal Optimal Transport\",\"url\":\"https://www.semanticscholar.org/paper/53073458ad3f387f6c97af28467749c9f148c9cd\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2003.09565\",\"authors\":[{\"authorId\":\"2274692\",\"name\":\"Abhishek Aich\"},{\"authorId\":\"150015249\",\"name\":\"Akash Gupta\"},{\"authorId\":\"1819152\",\"name\":\"R. Panda\"},{\"authorId\":\"12212948\",\"name\":\"Rakib Hyder\"},{\"authorId\":\"27996204\",\"name\":\"M. Asif\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1109/cvpr42600.2020.00613\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6a55aad6a51662da2d1e454a41878935a86a5d4a\",\"title\":\"Non-Adversarial Video Synthesis with Learned Priors\",\"url\":\"https://www.semanticscholar.org/paper/6a55aad6a51662da2d1e454a41878935a86a5d4a\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1812.01037\",\"authors\":[{\"authorId\":\"46793780\",\"name\":\"Ximeng Sun\"},{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACV45572.2020.9093557\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"05c43f1791787e78e343536a65a2853af699fe68\",\"title\":\"TwoStreamVAN: Improving Motion Modeling in Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/05c43f1791787e78e343536a65a2853af699fe68\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"145371957\",\"name\":\"Chang Xu\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/TAI.2020.3031581\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8ad1580246c895760198888ff7c0ee0237114f7\",\"title\":\"Self-Supervised Pose Adaptation for Cross-Domain Image Animation\",\"url\":\"https://www.semanticscholar.org/paper/f8ad1580246c895760198888ff7c0ee0237114f7\",\"venue\":\"IEEE Transactions on Artificial Intelligence\",\"year\":2020},{\"arxivId\":\"1906.06337\",\"authors\":[{\"authorId\":\"2160245\",\"name\":\"Konstantinos Vougioukas\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":\"10.1007/s11263-019-01251-8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f06ab43069480c09d7170075a6ea74669a71f139\",\"title\":\"Realistic Speech-Driven Facial Animation with GANs\",\"url\":\"https://www.semanticscholar.org/paper/f06ab43069480c09d7170075a6ea74669a71f139\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"1810.01325\",\"authors\":[{\"authorId\":\"51466944\",\"name\":\"Sandra Aigner\"},{\"authorId\":\"119567230\",\"name\":\"Marco Korner\"}],\"doi\":\"10.5194/isprs-archives-xlii-2-w16-3-2019\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947\",\"title\":\"FutureGAN: Anticipating the Future Frames of Video Sequences using Spatio-Temporal 3d Convolutions in Progressively Growing GANs\",\"url\":\"https://www.semanticscholar.org/paper/b6cad0b0ffd5d7b5ec1db60173c194ae2a9ec947\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1785411921\",\"name\":\"Ping-Sung Cheng\"},{\"authorId\":\"1785372532\",\"name\":\"Chieh-Ying Lai\"},{\"authorId\":\"50205803\",\"name\":\"C. Chang\"},{\"authorId\":\"50385111\",\"name\":\"Shu-Fen Chiou\"},{\"authorId\":\"3682532\",\"name\":\"Yu-Chieh Yang\"}],\"doi\":\"10.1145/3399871.3399888\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a44c61b43c50190d4dec9f00580bad93c9231eb7\",\"title\":\"A Variant Model of TGAN for Music Generation\",\"url\":\"https://www.semanticscholar.org/paper/a44c61b43c50190d4dec9f00580bad93c9231eb7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.11294\",\"authors\":[{\"authorId\":\"39380386\",\"name\":\"Jianwen Xie\"},{\"authorId\":\"9659905\",\"name\":\"Ruiqi Gao\"},{\"authorId\":\"49774254\",\"name\":\"Zilong Zheng\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"},{\"authorId\":\"39092098\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1609/AAAI.V34I07.6931\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d905d787372549ea445cc0c87c9fcb9de4ffa8a1\",\"title\":\"Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns\",\"url\":\"https://www.semanticscholar.org/paper/d905d787372549ea445cc0c87c9fcb9de4ffa8a1\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1811.09245\",\"authors\":[{\"authorId\":\"144648787\",\"name\":\"M. Saito\"},{\"authorId\":\"3083107\",\"name\":\"Shunta Saito\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ef29d5c85b70fd9dbe04a7b839fbc7d413b161e6\",\"title\":\"TGANv2: Efficient Training of Large Models for Video Generation with Multiple Subsampling Layers\",\"url\":\"https://www.semanticscholar.org/paper/ef29d5c85b70fd9dbe04a7b839fbc7d413b161e6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.10240\",\"authors\":[{\"authorId\":\"2469811\",\"name\":\"Yunpeng Li\"},{\"authorId\":\"3181733\",\"name\":\"Dominik Roblek\"},{\"authorId\":\"1749128\",\"name\":\"M. Tagliasacchi\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"063700c45e10362f5642c08849348c41ee5b08a3\",\"title\":\"From Here to There: Video Inbetweening Using Direct 3D Convolutions\",\"url\":\"https://www.semanticscholar.org/paper/063700c45e10362f5642c08849348c41ee5b08a3\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2318546\",\"name\":\"Shiping Wen\"},{\"authorId\":\"49663634\",\"name\":\"W. Liu\"},{\"authorId\":\"46286125\",\"name\":\"Y. Yang\"},{\"authorId\":\"145582475\",\"name\":\"T. Huang\"},{\"authorId\":\"145043786\",\"name\":\"Z. Zeng\"}],\"doi\":\"10.1109/TCSVT.2018.2867934\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"84e5e611baa362ccce571eef737c98de7a5331b5\",\"title\":\"Generating Realistic Videos From Keyframes With Concatenated GANs\",\"url\":\"https://www.semanticscholar.org/paper/84e5e611baa362ccce571eef737c98de7a5331b5\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35687142\",\"name\":\"Daichi Horita\"},{\"authorId\":\"1681659\",\"name\":\"K. Yanai\"}],\"doi\":\"10.1007/978-3-030-41404-7_44\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d682b993762623c1fea5c91e19429df04a4c87d\",\"title\":\"SSA-GAN: End-to-End Time-Lapse Video Generation with Spatial Self-Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d682b993762623c1fea5c91e19429df04a4c87d\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":\"1802.01873\",\"authors\":[{\"authorId\":\"47825302\",\"name\":\"Wei Wang\"},{\"authorId\":\"1382657362\",\"name\":\"Xavier Alameda-Pineda\"},{\"authorId\":\"145851646\",\"name\":\"D. Xu\"},{\"authorId\":\"40811261\",\"name\":\"E. Ricci\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1109/CVPR.2018.00740\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb13cbbaa7647177746ab86983273317dd5bcf51\",\"title\":\"Every Smile is Unique: Landmark-Guided Diverse Smile Generation\",\"url\":\"https://www.semanticscholar.org/paper/cb13cbbaa7647177746ab86983273317dd5bcf51\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1912.05523\",\"authors\":[{\"authorId\":null,\"name\":\"Yaohui Wang\"},{\"authorId\":\"105070829\",\"name\":\"P. Bilinski\"},{\"authorId\":\"69929964\",\"name\":\"F. Br\\u00e9mond\"},{\"authorId\":\"3299530\",\"name\":\"A. Dantcheva\"}],\"doi\":\"10.1109/CVPR42600.2020.00531\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"33392bb15145ba1c7681061ce891f2e49354ca17\",\"title\":\"G3AN: Disentangling Appearance and Motion for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/33392bb15145ba1c7681061ce891f2e49354ca17\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1802.06739\",\"authors\":[{\"authorId\":\"2962279\",\"name\":\"L. Xie\"},{\"authorId\":\"3002019\",\"name\":\"Kaixiang Lin\"},{\"authorId\":\"49183901\",\"name\":\"S. Wang\"},{\"authorId\":\"1682816\",\"name\":\"Fei Wang\"},{\"authorId\":\"145487992\",\"name\":\"Jiayu Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2df45c89dfb41d611af08806d891cd1eaff472cb\",\"title\":\"Differentially Private Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/2df45c89dfb41d611af08806d891cd1eaff472cb\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1803.11264\",\"authors\":[{\"authorId\":\"1916516\",\"name\":\"M. Khodabandeh\"},{\"authorId\":\"3227254\",\"name\":\"Hamid Reza Vaezi Joze\"},{\"authorId\":\"15623770\",\"name\":\"I. Zharkov\"},{\"authorId\":\"3811436\",\"name\":\"V. Pradeep\"}],\"doi\":\"10.1109/CVPRW.2018.00194\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74b8f8e27450891985df8ac8e083e81a8a922cd9\",\"title\":\"DIY Human Action Dataset Generation\",\"url\":\"https://www.semanticscholar.org/paper/74b8f8e27450891985df8ac8e083e81a8a922cd9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2160245\",\"name\":\"Konstantinos Vougioukas\"},{\"authorId\":\"2403354\",\"name\":\"S. Petridis\"},{\"authorId\":\"145387780\",\"name\":\"M. Pantic\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aed99738a2ad6d99bad710bdf4938de3403221be\",\"title\":\"End-to-End Speech-Driven Realistic Facial Animation with Temporal GANs\",\"url\":\"https://www.semanticscholar.org/paper/aed99738a2ad6d99bad710bdf4938de3403221be\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":\"2004.03142\",\"authors\":[{\"authorId\":\"48116039\",\"name\":\"Jian Ren\"},{\"authorId\":\"1752091\",\"name\":\"M. Chai\"},{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"49479171\",\"name\":\"Chen Fang\"},{\"authorId\":\"50457502\",\"name\":\"Xiaohui Shen\"},{\"authorId\":\"47988297\",\"name\":\"Jianchao Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dce3f80b0e1f43aa6fa94288710da0a5bdae2a9d\",\"title\":\"Human Motion Transfer from Poses in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/dce3f80b0e1f43aa6fa94288710da0a5bdae2a9d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9328269\",\"name\":\"Ngoc-Dung T. Tieu\"},{\"authorId\":\"40415016\",\"name\":\"Huy H. Nguyen\"},{\"authorId\":\"2912817\",\"name\":\"Hoang-Quoc Nguyen-Son\"},{\"authorId\":\"1716857\",\"name\":\"J. Yamagishi\"},{\"authorId\":\"1678602\",\"name\":\"I. Echizen\"}],\"doi\":\"10.1016/J.JISA.2019.03.002\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a15a3813040ee1a5a181ed781290a6a18d953777\",\"title\":\"Spatio-temporal generative adversarial network for gait anonymization\",\"url\":\"https://www.semanticscholar.org/paper/a15a3813040ee1a5a181ed781290a6a18d953777\",\"venue\":\"J. Inf. Secur. Appl.\",\"year\":2019},{\"arxivId\":\"2006.10704\",\"authors\":[{\"authorId\":\"145411379\",\"name\":\"R. Rakhimov\"},{\"authorId\":\"9937997\",\"name\":\"Denis Volkhonskiy\"},{\"authorId\":\"145235439\",\"name\":\"A. Artemov\"},{\"authorId\":\"145516498\",\"name\":\"D. Zorin\"},{\"authorId\":\"51139941\",\"name\":\"Evgeny Burnaev\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef25fe928b5a9f53296a938339dff5aeffcf6e7b\",\"title\":\"Latent Video Transformer\",\"url\":\"https://www.semanticscholar.org/paper/ef25fe928b5a9f53296a938339dff5aeffcf6e7b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1523761688\",\"name\":\"Aziz Siyaev\"},{\"authorId\":\"144089347\",\"name\":\"G. Jo\"}],\"doi\":\"10.1007/978-3-030-41964-6_26\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ef69c67e3129526ad616985a38fbd51f360d218\",\"title\":\"GOHAG: GANs Orchestration for Human Actions Generation\",\"url\":\"https://www.semanticscholar.org/paper/2ef69c67e3129526ad616985a38fbd51f360d218\",\"venue\":\"ACIIDS\",\"year\":2020},{\"arxivId\":\"2008.03555\",\"authors\":[{\"authorId\":\"1866277225\",\"name\":\"Sandeep Inuganti\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7513b5d0ab916491aef4df9baf18aa76ce3c021a\",\"title\":\"Assisting Scene Graph Generation with Self-Supervision.\",\"url\":\"https://www.semanticscholar.org/paper/7513b5d0ab916491aef4df9baf18aa76ce3c021a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2714590\",\"name\":\"Yoshihiro Nagano\"},{\"authorId\":\"3430922\",\"name\":\"Ryo Karakida\"},{\"authorId\":\"145029939\",\"name\":\"M. Okada\"}],\"doi\":\"10.1038/s41598-020-72593-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b3050ebe42e05b23a4e88afaddc4424cbd8b00f5\",\"title\":\"Collective dynamics of repeated inference in variational autoencoder rapidly find cluster structure\",\"url\":\"https://www.semanticscholar.org/paper/b3050ebe42e05b23a4e88afaddc4424cbd8b00f5\",\"venue\":\"Scientific reports\",\"year\":2020},{\"arxivId\":\"1910.11104\",\"authors\":[{\"authorId\":\"1382655643\",\"name\":\"Facundo Tuesca\"},{\"authorId\":\"2588271\",\"name\":\"Lucas C. Uzal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6e1ad81593de36b622e5d5c9b44e00496c73c240\",\"title\":\"Exploiting video sequences for unsupervised disentangling in generative adversarial networks\",\"url\":\"https://www.semanticscholar.org/paper/6e1ad81593de36b622e5d5c9b44e00496c73c240\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2033136\",\"name\":\"Jinghuan Wen\"},{\"authorId\":\"119905044\",\"name\":\"Huimin Ma\"},{\"authorId\":\"9160831\",\"name\":\"X. Luo\"}],\"doi\":\"10.1007/s00371-019-01738-y\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7085edafd3218d20b4787fbbf0b64c64b9b1ab0b\",\"title\":\"Deep generative smoke simulator: connecting simulated and real data\",\"url\":\"https://www.semanticscholar.org/paper/7085edafd3218d20b4787fbbf0b64c64b9b1ab0b\",\"venue\":\"The Visual Computer\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48446876\",\"name\":\"Huajun Liu\"},{\"authorId\":\"1414569274\",\"name\":\"Chao Li\"},{\"authorId\":\"1825780805\",\"name\":\"Dian Lei\"},{\"authorId\":\"153138255\",\"name\":\"Q. Zhu\"}],\"doi\":\"10.1007/s00371-020-01913-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3ae2a3e7c30150817c2e3c8561eb60e49f26a25\",\"title\":\"Unsupervised video-to-video translation with preservation of frame modification tendency\",\"url\":\"https://www.semanticscholar.org/paper/e3ae2a3e7c30150817c2e3c8561eb60e49f26a25\",\"venue\":\"The Visual Computer\",\"year\":2020},{\"arxivId\":\"1908.07683\",\"authors\":[{\"authorId\":\"150176607\",\"name\":\"KwanYong Park\"},{\"authorId\":\"2262209\",\"name\":\"S. Woo\"},{\"authorId\":\"24028009\",\"name\":\"Dahun Kim\"},{\"authorId\":\"3160154\",\"name\":\"Donghyeon Cho\"},{\"authorId\":\"98758720\",\"name\":\"I. Kweon\"}],\"doi\":\"10.1145/3343031.3350864\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e12184b519c14d6964d1ab45ff13fd87f24891c5\",\"title\":\"Preserving Semantic and Temporal Consistency for Unpaired Video-to-Video Translation\",\"url\":\"https://www.semanticscholar.org/paper/e12184b519c14d6964d1ab45ff13fd87f24891c5\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1910.06699\",\"authors\":[{\"authorId\":\"37868227\",\"name\":\"C. D. Souza\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"3407519\",\"name\":\"Y. Cabon\"},{\"authorId\":\"26734366\",\"name\":\"N. Murray\"},{\"authorId\":\"3940956\",\"name\":\"A. Pe\\u00f1a\"}],\"doi\":\"10.1007/s11263-019-01222-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"099bdf9cf1c59c7f5ed201759157fb505f51a762\",\"title\":\"Generating Human Action Videos by Coupling 3D Game Engines and Probabilistic Graphical Models\",\"url\":\"https://www.semanticscholar.org/paper/099bdf9cf1c59c7f5ed201759157fb505f51a762\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"1912.08860\",\"authors\":[{\"authorId\":\"26432578\",\"name\":\"Emmanuel Kahembwe\"},{\"authorId\":\"47172195\",\"name\":\"S. Ramamoorthy\"}],\"doi\":\"10.1016/j.neunet.2020.09.016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f313226533edea306c4de79df945cc5a90d153c\",\"title\":\"Lower Dimensional Kernels for Video Discriminators\",\"url\":\"https://www.semanticscholar.org/paper/1f313226533edea306c4de79df945cc5a90d153c\",\"venue\":\"Neural Networks\",\"year\":2020},{\"arxivId\":\"1701.06264\",\"authors\":[{\"authorId\":\"2272096\",\"name\":\"Guo-Jun Qi\"}],\"doi\":\"10.1007/s11263-019-01265-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b71ab38d3ac6eee0da5e68ef666d33d1028bad8\",\"title\":\"Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities\",\"url\":\"https://www.semanticscholar.org/paper/4b71ab38d3ac6eee0da5e68ef666d33d1028bad8\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1395873384\",\"name\":\"Rui Zhao\"},{\"authorId\":\"1563539653\",\"name\":\"Hui Su\"},{\"authorId\":\"144116884\",\"name\":\"Qiang Ji\"}],\"doi\":\"10.1109/cvpr42600.2020.00626\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1fd40a93a0cdac88f53bf5612fa1604a897eea5\",\"title\":\"Bayesian Adversarial Human Motion Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/b1fd40a93a0cdac88f53bf5612fa1604a897eea5\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1903.04480\",\"authors\":[{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":null,\"name\":\"Chengyu Wang\"},{\"authorId\":\"143904396\",\"name\":\"Xu Jia\"},{\"authorId\":\"144478188\",\"name\":\"J. Shao\"},{\"authorId\":\"37145669\",\"name\":\"Lu Sheng\"},{\"authorId\":\"1721677\",\"name\":\"J. Yan\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CVPR.2019.00385\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f66d531439f5847afa7b31f49db87a44b788690\",\"title\":\"Video Generation From Single Semantic Label Map\",\"url\":\"https://www.semanticscholar.org/paper/2f66d531439f5847afa7b31f49db87a44b788690\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1466503743\",\"name\":\"X. Ren\"},{\"authorId\":\"49403981\",\"name\":\"H. Li\"},{\"authorId\":\"1993645766\",\"name\":\"Zijian Huang\"},{\"authorId\":\"1559427865\",\"name\":\"Qifeng Chen\"}],\"doi\":\"10.1145/3394171.3413932\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d3af080ac123a224a40ab0bb2929d63061451ee\",\"title\":\"Self-supervised Dance Video Synthesis Conditioned on Music\",\"url\":\"https://www.semanticscholar.org/paper/6d3af080ac123a224a40ab0bb2929d63061451ee\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1802.05637\",\"authors\":[{\"authorId\":\"3213400\",\"name\":\"Takeru Miyato\"},{\"authorId\":\"2877296\",\"name\":\"Masanori Koyama\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ab559473a01836e72b9fb9393d6e07c5745528f3\",\"title\":\"cGANs with Projection Discriminator\",\"url\":\"https://www.semanticscholar.org/paper/ab559473a01836e72b9fb9393d6e07c5745528f3\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"2004.01823\",\"authors\":[{\"authorId\":\"143672737\",\"name\":\"A. Mu\\u00f1oz\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"49965376\",\"name\":\"Max Argus\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"31ce95cc12c4e58fd5d7051d9797589859d5dda1\",\"title\":\"Temporal Shift GAN for Large Scale Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/31ce95cc12c4e58fd5d7051d9797589859d5dda1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.01272\",\"authors\":[{\"authorId\":\"122744867\",\"name\":\"S\\u00e9bastien Ehrhardt\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"2535033\",\"name\":\"Aron Monszpart\"},{\"authorId\":\"3468172\",\"name\":\"Martin Engelcke\"},{\"authorId\":\"1834086\",\"name\":\"I. Posner\"},{\"authorId\":\"152908887\",\"name\":\"Niloy Mitra\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26fa7cfa8fcadfcab6ccc187d417b66b420d90a8\",\"title\":\"RELATE: Physically Plausible Multi-Object Scene Synthesis Using Structured Latent Spaces\",\"url\":\"https://www.semanticscholar.org/paper/26fa7cfa8fcadfcab6ccc187d417b66b420d90a8\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2001.04758\",\"authors\":[{\"authorId\":\"145153296\",\"name\":\"Hao Zhu\"},{\"authorId\":\"100602595\",\"name\":\"Mandi Luo\"},{\"authorId\":\"77886607\",\"name\":\"Rui Wang\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"144282794\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"title\":\"Deep Audio-Visual Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7cabfa7362ebb57c77380caa57aa17fd7195605c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819450790\",\"name\":\"Qi Chen\"},{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"49251914\",\"name\":\"Jun-kai Chen\"},{\"authorId\":\"8277017\",\"name\":\"Q. Wu\"},{\"authorId\":\"133678193\",\"name\":\"A. van den Hengel\"},{\"authorId\":\"2823637\",\"name\":\"Mingkui Tan\"}],\"doi\":\"10.1109/TIP.2020.3003227\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a2418fd9c453492f1ca833d5595571249a3ee55\",\"title\":\"Scripted Video Generation With a Bottom-Up Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/5a2418fd9c453492f1ca833d5595571249a3ee55\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1702.07983\",\"authors\":[{\"authorId\":\"47828117\",\"name\":\"Tong Che\"},{\"authorId\":\"3305402\",\"name\":\"Yanran Li\"},{\"authorId\":\"39501182\",\"name\":\"R. Zhang\"},{\"authorId\":\"40482726\",\"name\":\"R. Devon Hjelm\"},{\"authorId\":\"50135831\",\"name\":\"W. Li\"},{\"authorId\":\"1809614\",\"name\":\"Y. Song\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fc0ea6db600850908264652e1a5d7904f66ca58\",\"title\":\"Maximum-Likelihood Augmented Discrete Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/4fc0ea6db600850908264652e1a5d7904f66ca58\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1903.12161\",\"authors\":[{\"authorId\":\"1413064976\",\"name\":\"S. Caelles\"},{\"authorId\":\"49107901\",\"name\":\"Albert Pumarola\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"},{\"authorId\":\"49743313\",\"name\":\"A. Sanfeliu\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c8b83e7e26a22891cab3907079f392366cf4cfa4\",\"title\":\"Fast video object segmentation with Spatio-Temporal GANs\",\"url\":\"https://www.semanticscholar.org/paper/c8b83e7e26a22891cab3907079f392366cf4cfa4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1712.04195\",\"authors\":[{\"authorId\":\"2714590\",\"name\":\"Yoshihiro Nagano\"},{\"authorId\":\"3430922\",\"name\":\"Ryo Karakida\"},{\"authorId\":\"145029940\",\"name\":\"Masato Okada\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9372d249731e8e312e9017a70f980e169192e165\",\"title\":\"Concept Formation and Dynamics of Repeated Inference in Deep Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/9372d249731e8e312e9017a70f980e169192e165\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"147998545\",\"name\":\"Stefan Ainetter\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"}],\"doi\":\"10.3217/978-3-85125-652-9-03\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7f224be6f39aca5bbdb08671127aa825fc8815fe\",\"title\":\"A Spatiotemporal Generative Adversarial Network to Generate Human Action Videos\",\"url\":\"https://www.semanticscholar.org/paper/7f224be6f39aca5bbdb08671127aa825fc8815fe\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50819441\",\"name\":\"R. Qiu\"},{\"authorId\":\"72863851\",\"name\":\"Danilo Vasconcellos Vargas\"},{\"authorId\":\"1485772943\",\"name\":\"Kouich Sakurai\"}],\"doi\":\"10.1109/CANDARW.2019.00037\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"151cd66d9fe95b13465084d1407824260ec8e057\",\"title\":\"Frame Difference Generative Adversarial Networks: Clearer Contour Video Generating\",\"url\":\"https://www.semanticscholar.org/paper/151cd66d9fe95b13465084d1407824260ec8e057\",\"venue\":\"2019 Seventh International Symposium on Computing and Networking Workshops (CANDARW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152718961\",\"name\":\"Yun-Chieh Tien\"},{\"authorId\":\"3228778\",\"name\":\"Chen-Min Hsu\"},{\"authorId\":\"145120643\",\"name\":\"Fang Yu\"}],\"doi\":\"10.1007/978-3-030-30484-3_50\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21f6362b98032eee26a5238c2cabc6a1f65f2731\",\"title\":\"HiSeqGAN: Hierarchical Sequence Synthesis and Prediction\",\"url\":\"https://www.semanticscholar.org/paper/21f6362b98032eee26a5238c2cabc6a1f65f2731\",\"venue\":\"ICANN\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143672737\",\"name\":\"A. Mu\\u00f1oz\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"49965376\",\"name\":\"Max Argus\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1233d892716e3c8efab5a72bef14c8a4bdf668d4\",\"title\":\"Multi-Variate Temporal GAN for Large Scale Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/1233d892716e3c8efab5a72bef14c8a4bdf668d4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.08786\",\"authors\":[{\"authorId\":\"79372499\",\"name\":\"Subin Jeon\"},{\"authorId\":\"7532506\",\"name\":\"Seonghyeon Nam\"},{\"authorId\":\"3451982\",\"name\":\"S. Oh\"},{\"authorId\":\"1754380\",\"name\":\"S. Kim\"}],\"doi\":\"10.1007/978-3-030-58586-0_18\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d30ae95b116c2b7c2214dbf0d3301ffb410d5d8d\",\"title\":\"Cross-Identity Motion Transfer for Arbitrary Objects through Pose-Attentive Video Reassembling\",\"url\":\"https://www.semanticscholar.org/paper/d30ae95b116c2b7c2214dbf0d3301ffb410d5d8d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.02808\",\"authors\":[{\"authorId\":\"47637491\",\"name\":\"M. Lakhal\"},{\"authorId\":\"1804261\",\"name\":\"D. Boscaini\"},{\"authorId\":\"1753989\",\"name\":\"Fabio Poiesi\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"},{\"authorId\":\"153142893\",\"name\":\"A. Cavallaro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f21fec81c7eb3f8f657d88eee43ae99fa4d46260\",\"title\":\"Novel-View Human Action Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/f21fec81c7eb3f8f657d88eee43ae99fa4d46260\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1804.08264\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3123266.3127905\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"883224c3b28b0563a393746066738f52e6fcc70d\",\"title\":\"To Create What You Tell: Generating Videos from Captions\",\"url\":\"https://www.semanticscholar.org/paper/883224c3b28b0563a393746066738f52e6fcc70d\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3032880\",\"name\":\"L. Zhang\"},{\"authorId\":\"144630617\",\"name\":\"N. Wei\"},{\"authorId\":\"30667673\",\"name\":\"Xuhao Du\"},{\"authorId\":\"47673269\",\"name\":\"S. Wang\"}],\"doi\":\"10.3390/APP9112292\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fbf2ab218c75e0be347dacd8b0c012959832d082\",\"title\":\"A size-controlled AFGAN model for ship acoustic fault expansion\",\"url\":\"https://www.semanticscholar.org/paper/fbf2ab218c75e0be347dacd8b0c012959832d082\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1612.03777\",\"authors\":[{\"authorId\":\"31656404\",\"name\":\"N. Sedaghat\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f42dca4a4426e5873a981712102aa961be34539a\",\"title\":\"Next-Flow: Hybrid Multi-Tasking with Next-Frame Prediction to Boost Optical-Flow Estimation in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/f42dca4a4426e5873a981712102aa961be34539a\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1902.03442\",\"authors\":[{\"authorId\":\"3406363\",\"name\":\"Michal U\\u0159i\\u010d\\u00e1\\u0159\"},{\"authorId\":\"2088180\",\"name\":\"P. Kr\\u00edzek\"},{\"authorId\":\"2687885\",\"name\":\"D. Hurych\"},{\"authorId\":\"143995880\",\"name\":\"Ibrahim Sobh\"},{\"authorId\":\"2601522\",\"name\":\"S. Yogamani\"},{\"authorId\":\"143880484\",\"name\":\"P. Denny\"}],\"doi\":\"10.2352/ISSN.2470-1173.2019.15.AVM-048\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9656a10c9d4a35fd8d4ad994bc03fcd69231ff22\",\"title\":\"Yes, we GAN: Applying Adversarial Techniques for Autonomous Driving\",\"url\":\"https://www.semanticscholar.org/paper/9656a10c9d4a35fd8d4ad994bc03fcd69231ff22\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1804.01523\",\"authors\":[{\"authorId\":\"49250083\",\"name\":\"Alex X. Lee\"},{\"authorId\":\"2844849\",\"name\":\"Richard Zhang\"},{\"authorId\":\"27535721\",\"name\":\"Frederik Ebert\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9f67271a1edea3bf80c64c2d54e2a0a57612a567\",\"title\":\"Stochastic Adversarial Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/9f67271a1edea3bf80c64c2d54e2a0a57612a567\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13763932\",\"name\":\"Dongxu Wei\"},{\"authorId\":\"144838755\",\"name\":\"Xiaowei Xu\"},{\"authorId\":\"1888007\",\"name\":\"Haibin Shen\"},{\"authorId\":\"47942157\",\"name\":\"K. Huang\"}],\"doi\":\"10.1109/tmm.2020.3011290\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1df6cc2507dd90adaafe5f9915a261558b88008c\",\"title\":\"GAC-GAN: A General Method for Appearance-Controllable Human Video Motion Transfer\",\"url\":\"https://www.semanticscholar.org/paper/1df6cc2507dd90adaafe5f9915a261558b88008c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2009.02018\",\"authors\":[{\"authorId\":\"48693251\",\"name\":\"D. Kim\"},{\"authorId\":\"50001046\",\"name\":\"Donggyu Joo\"},{\"authorId\":\"1769295\",\"name\":\"Junmo Kim\"}],\"doi\":\"10.1109/ACCESS.2020.3017881\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"49e9fa120e4cdcaa2aebea59d3e882fd8ef0bd0a\",\"title\":\"TiVGAN: Text to Image to Video Generation With Step-by-Step Evolutionary Generator\",\"url\":\"https://www.semanticscholar.org/paper/49e9fa120e4cdcaa2aebea59d3e882fd8ef0bd0a\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2012.07304\",\"authors\":[{\"authorId\":\"1488670226\",\"name\":\"N. Kumar\"},{\"authorId\":\"14085625\",\"name\":\"Srishti Goel\"},{\"authorId\":\"34275551\",\"name\":\"A. Narang\"},{\"authorId\":\"2036952884\",\"name\":\"Brejesh Lall\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc9ec9cde734084888a6e529488aed42abbd2c8e\",\"title\":\"Multi Modal Adaptive Normalization for Audio to Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/dc9ec9cde734084888a6e529488aed42abbd2c8e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100486091\",\"name\":\"D. Dirvanauskas\"},{\"authorId\":\"1922138\",\"name\":\"R. Maskeli\\u016bnas\"},{\"authorId\":\"48103505\",\"name\":\"V. Raudonis\"},{\"authorId\":\"1801351\",\"name\":\"Robertas Dama\\u0161evi\\u010dius\"},{\"authorId\":\"1710827\",\"name\":\"R. Scherer\"}],\"doi\":\"10.3390/s19163578\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1849392799a75c4a5a2b15c55050f56a9265c6f3\",\"title\":\"HEMIGEN: Human Embryo Image Generator Based on Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/1849392799a75c4a5a2b15c55050f56a9265c6f3\",\"venue\":\"Sensors\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144783081\",\"name\":\"You Xie\"},{\"authorId\":\"35358480\",\"name\":\"Erik Franz\"},{\"authorId\":\"32464128\",\"name\":\"Mengyu Chu\"},{\"authorId\":\"1786445\",\"name\":\"N. Th\\u00fcrey\"}],\"doi\":\"10.1145/3197517.3201304\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2e0605ba5cfe30864a514c8a69460dbb59d68df3\",\"title\":\"tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow\",\"url\":\"https://www.semanticscholar.org/paper/2e0605ba5cfe30864a514c8a69460dbb59d68df3\",\"venue\":\"ACM Trans. Graph.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102509914\",\"name\":\"Radamanthys Stivaktakis\"},{\"authorId\":\"47952527\",\"name\":\"G. Tsagkatakis\"},{\"authorId\":\"1694755\",\"name\":\"P. Tsakalides\"}],\"doi\":\"10.3390/make2030017\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d3d8366c5186914be2cfbf43a6457df6d83db861\",\"title\":\"Semantic Predictive Coding with Arbitrated Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/d3d8366c5186914be2cfbf43a6457df6d83db861\",\"venue\":\"Mach. Learn. Knowl. Extr.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46756194\",\"name\":\"Cheng Yu\"},{\"authorId\":\"47825552\",\"name\":\"Wenmin Wang\"},{\"authorId\":\"134233854\",\"name\":\"Jianhao Yan\"}],\"doi\":\"10.1109/ACCESS.2020.3008523\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2c5b4b49dd322a4ffe50448e8810880c0f79453b\",\"title\":\"Self-Supervised Animation Synthesis Through Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/2c5b4b49dd322a4ffe50448e8810880c0f79453b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2001.10938\",\"authors\":[{\"authorId\":\"51149606\",\"name\":\"Jeffrey M. Ede\"},{\"authorId\":\"31436765\",\"name\":\"J. J. Peters\"},{\"authorId\":\"22404881\",\"name\":\"J. Sloan\"},{\"authorId\":\"40586446\",\"name\":\"R. Beanland\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"992a8f8b087ab467c13ac0b6a39340011e1698e0\",\"title\":\"Exit Wavefunction Reconstruction from Single Transmisson Electron Micrographs with Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/992a8f8b087ab467c13ac0b6a39340011e1698e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.01261\",\"authors\":[{\"authorId\":\"48333400\",\"name\":\"S. Yamamoto\"},{\"authorId\":\"1399435786\",\"name\":\"Antonio Tejero-de-Pablos\"},{\"authorId\":\"3250559\",\"name\":\"Y. Ushiku\"},{\"authorId\":\"1790553\",\"name\":\"T. Harada\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e8de3d97d14344d13d15701eea8d6a27c696e27a\",\"title\":\"Conditional Video Generation Using Action-Appearance Captions\",\"url\":\"https://www.semanticscholar.org/paper/e8de3d97d14344d13d15701eea8d6a27c696e27a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1709.06298\",\"authors\":[{\"authorId\":\"9093463\",\"name\":\"Hao-Wen Dong\"},{\"authorId\":\"37188394\",\"name\":\"Wen-Yi Hsiao\"},{\"authorId\":\"9922427\",\"name\":\"Li-Chia Yang\"},{\"authorId\":\"1689230\",\"name\":\"Y. Yang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f83ef3250ba1166d7c1c7585da7dd78e0641fae7\",\"title\":\"MuseGAN: Multi-track Sequential Generative Adversarial Networks for Symbolic Music Generation and Accompaniment\",\"url\":\"https://www.semanticscholar.org/paper/f83ef3250ba1166d7c1c7585da7dd78e0641fae7\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"2007.08509\",\"authors\":[{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36775862\",\"name\":\"K. Sapra\"},{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"}],\"doi\":\"10.1007/978-3-030-58598-3_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"633a7a0c92dd8c254b65a11b759b99c157e115c7\",\"title\":\"World-Consistent Video-to-Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/633a7a0c92dd8c254b65a11b759b99c157e115c7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2007.12130\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"2479187\",\"name\":\"Moitreya Chatterjee\"},{\"authorId\":\"145237406\",\"name\":\"N. Ahuja\"}],\"doi\":\"10.1007/978-3-030-58583-9_42\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"title\":\"Sound2Sight: Generating Visual Dynamics from Sound and Context\",\"url\":\"https://www.semanticscholar.org/paper/02f3ced09497c5db59985b2a5db9d3d0aebe5074\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.13074\",\"authors\":[{\"authorId\":\"152967315\",\"name\":\"P. Zhou\"},{\"authorId\":\"3041937\",\"name\":\"Lingxi Xie\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"1400120070\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fed4395fca860d20a64781d1fff717270465693\",\"title\":\"Omni-GAN: On the Secrets of cGANs and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/5fed4395fca860d20a64781d1fff717270465693\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.06571\",\"authors\":[{\"authorId\":\"31993415\",\"name\":\"A. Clark\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"24262bd12149b0b8322e0691fa30ec1a4c06b9a8\",\"title\":\"Efficient Video Generation on Complex Datasets\",\"url\":\"https://www.semanticscholar.org/paper/24262bd12149b0b8322e0691fa30ec1a4c06b9a8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37145669\",\"name\":\"Lu Sheng\"},{\"authorId\":\"7588865\",\"name\":\"Junting Pan\"},{\"authorId\":\"47093626\",\"name\":\"Jiaming Guo\"},{\"authorId\":\"145965208\",\"name\":\"J. Shao\"},{\"authorId\":null,\"name\":\"Xiaogang Wang\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"545f3b0c15a0a555fda0d2377646c0c70090c185\",\"title\":\"Tgt . ( a ) GT sequence ( b ) ImagineFlow ( c ) Backward warping Before After\",\"url\":\"https://www.semanticscholar.org/paper/545f3b0c15a0a555fda0d2377646c0c70090c185\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1910.12713\",\"authors\":[{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"29955511\",\"name\":\"A. Tao\"},{\"authorId\":\"47062069\",\"name\":\"Guilin Liu\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2301680\",\"name\":\"Bryan Catanzaro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bea92a69b48287caa3a25ff3dfe727bed8888348\",\"title\":\"Few-shot Video-to-Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/bea92a69b48287caa3a25ff3dfe727bed8888348\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66283463\",\"name\":\"Flow\"},{\"authorId\":\"9071227\",\"name\":\"You Xie\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6c110e724ac33243f9d02ac061007057e0b30adf\",\"title\":\"tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow\",\"url\":\"https://www.semanticscholar.org/paper/6c110e724ac33243f9d02ac061007057e0b30adf\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8303994\",\"name\":\"Honggang Xie\"},{\"authorId\":\"144033365\",\"name\":\"Jinsheng Xiao\"},{\"authorId\":\"3389703\",\"name\":\"Junfeng Lei\"},{\"authorId\":\"2268470\",\"name\":\"Wenjuan Xie\"},{\"authorId\":\"1729664\",\"name\":\"Reinhard Klette\"}],\"doi\":\"10.1007/978-981-15-3651-9\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"42636140273d2a8bdeed36df1cd4d8b56f62270c\",\"title\":\"Pattern Recognition: ACPR 2019 Workshops, Auckland, New Zealand, November 26, 2019, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/42636140273d2a8bdeed36df1cd4d8b56f62270c\",\"venue\":\"ACPR Workshops\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46602660\",\"name\":\"Thanh-Hai Tran\"},{\"authorId\":\"1565673788\",\"name\":\"Viet-Dung Bach\"},{\"authorId\":\"1909194\",\"name\":\"H. Doan\"}],\"doi\":\"10.1007/978-981-15-3651-9_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a0c50fae31bfc064de96820520a4bcc8ae3f7ab\",\"title\":\"vi-MoCoGAN: A Variant of MoCoGAN for Video Generation of Human Hand Gestures Under Different Viewpoints\",\"url\":\"https://www.semanticscholar.org/paper/5a0c50fae31bfc064de96820520a4bcc8ae3f7ab\",\"venue\":\"ACPR Workshops\",\"year\":2019},{\"arxivId\":\"1905.12043\",\"authors\":[{\"authorId\":\"50383147\",\"name\":\"M. Doukas\"},{\"authorId\":\"1790503\",\"name\":\"V. Sharmanska\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2f7e7adbbd2326c7ac7cbb67fa43214108bb372\",\"title\":\"Video-to-Video Translation for Visual Speech Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/a2f7e7adbbd2326c7ac7cbb67fa43214108bb372\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2008.09646\",\"authors\":[{\"authorId\":\"1581479411\",\"name\":\"Abhinav Sagar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b367a582b8921997bbbea433bf99c4671568b44c\",\"title\":\"HRVGAN: High Resolution Video Generation using Spatio-Temporal GAN\",\"url\":\"https://www.semanticscholar.org/paper/b367a582b8921997bbbea433bf99c4671568b44c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.14695\",\"authors\":[{\"authorId\":\"6044420\",\"name\":\"Risheng Huang\"},{\"authorId\":\"152148573\",\"name\":\"L. Shen\"},{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"1766319\",\"name\":\"Chu-Hsing Lin\"},{\"authorId\":\"2711717\",\"name\":\"Haozhi Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd9089461dadc59bbb3e40a90f9f281830d3bd6b\",\"title\":\"Adaptive Compact Attention For Few-shot Video-to-video Translation\",\"url\":\"https://www.semanticscholar.org/paper/dd9089461dadc59bbb3e40a90f9f281830d3bd6b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.05214\",\"authors\":[{\"authorId\":\"152298373\",\"name\":\"Sergiu Oprea\"},{\"authorId\":\"1410236705\",\"name\":\"P. Martinez-Gonzalez\"},{\"authorId\":\"1397392435\",\"name\":\"Alberto Garcia-Garcia\"},{\"authorId\":\"1410269918\",\"name\":\"John Alejandro Castro-Vargas\"},{\"authorId\":\"1405686926\",\"name\":\"S. Orts-Escolano\"},{\"authorId\":\"1429069120\",\"name\":\"J. Garci\\u0301a-Rodri\\u0301guez\"},{\"authorId\":\"1689415\",\"name\":\"Antonis A. Argyros\"}],\"doi\":\"10.1109/TPAMI.2020.3045007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ded24959bbd7715073fa700d5898cdbb2b8353f7\",\"title\":\"A Review on Deep Learning Techniques for Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ded24959bbd7715073fa700d5898cdbb2b8353f7\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"1803.09092\",\"authors\":[{\"authorId\":\"2441118\",\"name\":\"C. Spampinato\"},{\"authorId\":\"1681089\",\"name\":\"S. Palazzo\"},{\"authorId\":\"1403265508\",\"name\":\"Salvatore D\\u2019Oro\"},{\"authorId\":\"2004177\",\"name\":\"F. Murabito\"},{\"authorId\":\"35906202\",\"name\":\"D. Giordano\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eacba5e8fbafb1302866c0860fc260a2bdfff232\",\"title\":\"VOS-GAN: Adversarial Learning of Visual-Temporal Dynamics for Unsupervised Dense Prediction in Videos\",\"url\":\"https://www.semanticscholar.org/paper/eacba5e8fbafb1302866c0860fc260a2bdfff232\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1961262790\",\"name\":\"Yiwei Fu\"},{\"authorId\":\"1961307935\",\"name\":\"Shiraj Sen\"},{\"authorId\":\"1961309452\",\"name\":\"Johan Reimann\"},{\"authorId\":\"35030051\",\"name\":\"Charles Theurer\"}],\"doi\":\"10.1109/ICRA40945.2020.9196858\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e8abb38214f2de8f68fa59be41548bebf6e0fa5\",\"title\":\"Spatiotemporal Representation Learning with GAN Trained LSTM-LSTM Networks\",\"url\":\"https://www.semanticscholar.org/paper/3e8abb38214f2de8f68fa59be41548bebf6e0fa5\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hao Zhu\"},{\"authorId\":\"4520695\",\"name\":\"A. Zheng\"},{\"authorId\":\"32885778\",\"name\":\"Huaibo Huang\"},{\"authorId\":\"143712929\",\"name\":\"R. He\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"3af11867986df5acc2c07467307d13924ba640bd\",\"title\":\"High-Resolution Talking Face Generation via Mutual Information Approximation\",\"url\":\"https://www.semanticscholar.org/paper/3af11867986df5acc2c07467307d13924ba640bd\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5458428\",\"name\":\"M. Chen\"},{\"authorId\":\"121900636\",\"name\":\"Changbo Wang\"},{\"authorId\":\"47968194\",\"name\":\"Ligang Liu\"}],\"doi\":\"10.1111/cgf.13859\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"63cb1daa3b2276eb8872790c6b1f3a03994f2ac9\",\"title\":\"Deep Video\\u2010Based Performance Synthesis from Sparse Multi\\u2010View Capture\",\"url\":\"https://www.semanticscholar.org/paper/63cb1daa3b2276eb8872790c6b1f3a03994f2ac9\",\"venue\":\"Comput. Graph. Forum\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1791319\",\"name\":\"Shangfei Wang\"},{\"authorId\":\"102592577\",\"name\":\"Zhuangqiang Zheng\"},{\"authorId\":\"145195545\",\"name\":\"S. Yin\"},{\"authorId\":\"46478407\",\"name\":\"J. Yang\"},{\"authorId\":\"50426357\",\"name\":\"Q. Ji\"}],\"doi\":\"10.1109/TPAMI.2019.2911937\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a493b38d3bc13dd34c4788de6e6e88554392902f\",\"title\":\"A Novel Dynamic Model Capturing Spatial and Temporal Patterns for Facial Expression Analysis\",\"url\":\"https://www.semanticscholar.org/paper/a493b38d3bc13dd34c4788de6e6e88554392902f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1812.01717\",\"authors\":[{\"authorId\":\"2465270\",\"name\":\"Thomas Unterthiner\"},{\"authorId\":\"3440930\",\"name\":\"Sjoerd van Steenkiste\"},{\"authorId\":\"2006889\",\"name\":\"Karol Kurach\"},{\"authorId\":\"52153018\",\"name\":\"Rapha\\u00ebl Marinier\"},{\"authorId\":\"144859281\",\"name\":\"M. Michalski\"},{\"authorId\":\"1802148\",\"name\":\"S. Gelly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b59233aab8364186603967bc12d88af48cc0992d\",\"title\":\"Towards Accurate Generative Models of Video: A New Metric & Challenges\",\"url\":\"https://www.semanticscholar.org/paper/b59233aab8364186603967bc12d88af48cc0992d\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2003.04035\",\"authors\":[{\"authorId\":\"152831141\",\"name\":\"Pauline Luc\"},{\"authorId\":\"31993415\",\"name\":\"A. Clark\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"40550616\",\"name\":\"D. Casas\"},{\"authorId\":\"2895238\",\"name\":\"Yotam Doron\"},{\"authorId\":\"51042571\",\"name\":\"Albin Cassirer\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e93e16a1a9f060841956ae2eca735f5cce457c8c\",\"title\":\"Transformation-based Adversarial Video Prediction on Large-Scale Data\",\"url\":\"https://www.semanticscholar.org/paper/e93e16a1a9f060841956ae2eca735f5cce457c8c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.12226\",\"authors\":[{\"authorId\":\"47509360\",\"name\":\"Shir Gur\"},{\"authorId\":\"19310335\",\"name\":\"Sagie Benaim\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d29bcdbb700a35634ac72e7694a396b26db2527\",\"title\":\"Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample\",\"url\":\"https://www.semanticscholar.org/paper/7d29bcdbb700a35634ac72e7694a396b26db2527\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50415746\",\"name\":\"Shruti Vyas\"},{\"authorId\":\"2116440\",\"name\":\"Y. Rawat\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":\"10.1007/978-3-030-58583-9_26\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"85f4601fb7cc495f33c80750f30b0e44c7ab6a91\",\"title\":\"Multi-view Action Recognition Using Cross-View Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/85f4601fb7cc495f33c80750f30b0e44c7ab6a91\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1708.05980\",\"authors\":[{\"authorId\":\"8268761\",\"name\":\"T. Marwah\"},{\"authorId\":\"47351893\",\"name\":\"G. Mittal\"},{\"authorId\":\"1699429\",\"name\":\"V. Balasubramanian\"}],\"doi\":\"10.1109/ICCV.2017.159\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1491b98d8e9ccca13bec883f94d935d2dff24053\",\"title\":\"Attentive Semantic Video Generation Using Captions\",\"url\":\"https://www.semanticscholar.org/paper/1491b98d8e9ccca13bec883f94d935d2dff24053\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"2008.02793\",\"authors\":[{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"},{\"authorId\":\"47932904\",\"name\":\"Xun Huang\"},{\"authorId\":\"150167366\",\"name\":\"Jiahui Yu\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"555e7029e8fd3b759300edfd2a4b7dfa2fd34a48\",\"title\":\"Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications\",\"url\":\"https://www.semanticscholar.org/paper/555e7029e8fd3b759300edfd2a4b7dfa2fd34a48\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.10954\",\"authors\":[{\"authorId\":\"46193344\",\"name\":\"Mohammad Rami Koujan\"},{\"authorId\":\"50383147\",\"name\":\"M. Doukas\"},{\"authorId\":\"2931390\",\"name\":\"A. Roussos\"},{\"authorId\":\"1379747201\",\"name\":\"S. Zafeiriou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4a787b7177fec50da643a662a57db64ccc91ffc\",\"title\":\"Head2Head: Video-based Neural Head Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/e4a787b7177fec50da643a662a57db64ccc91ffc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66933647\",\"name\":\"Debapriya Hazra\"},{\"authorId\":\"1730636\",\"name\":\"Yungcheol Byun\"}],\"doi\":\"10.3390/electronics9081312\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c03f51a33b2ffe479f2995482143207f889f2c9b\",\"title\":\"Upsampling Real-Time, Low-Resolution CCTV Videos Using Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/c03f51a33b2ffe479f2995482143207f889f2c9b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.07842\",\"authors\":[{\"authorId\":\"1488670226\",\"name\":\"N. Kumar\"},{\"authorId\":\"14085625\",\"name\":\"Srishti Goel\"},{\"authorId\":\"34275551\",\"name\":\"A. Narang\"},{\"authorId\":\"1491237789\",\"name\":\"Hasan Mujtaba\"}],\"doi\":\"10.1109/CVPRW50498.2020.00393\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61c52febf4c0a12f20326af58633077a9e218ce3\",\"title\":\"Robust One Shot Audio to Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/61c52febf4c0a12f20326af58633077a9e218ce3\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2441118\",\"name\":\"C. Spampinato\"},{\"authorId\":\"1409705906\",\"name\":\"S. Palazzo\"},{\"authorId\":\"82619398\",\"name\":\"P. D'Oro\"},{\"authorId\":\"1390194542\",\"name\":\"D. Giordano\"},{\"authorId\":\"147598837\",\"name\":\"M. Shah\"}],\"doi\":\"10.1007/s11263-019-01246-5\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d50b45969bef86bf29bcaf9052beade2282fa094\",\"title\":\"Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos\",\"url\":\"https://www.semanticscholar.org/paper/d50b45969bef86bf29bcaf9052beade2282fa094\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"2010.15075\",\"authors\":[{\"authorId\":\"11016722\",\"name\":\"Noushin Hajarolasvadi\"},{\"authorId\":\"32286131\",\"name\":\"M. Ram\\u00edrez\"},{\"authorId\":\"40986317\",\"name\":\"Wesley Beccaro\"},{\"authorId\":\"2128977\",\"name\":\"H. Demirel\"}],\"doi\":\"10.1109/ACCESS.2020.3042328\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5122bc767bcbdde57761c64973c72e945ba7bd22\",\"title\":\"Generative Adversarial Networks in Human Emotion Synthesis: A Review\",\"url\":\"https://www.semanticscholar.org/paper/5122bc767bcbdde57761c64973c72e945ba7bd22\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1910.09139\",\"authors\":[{\"authorId\":\"7164154\",\"name\":\"Polina Zablotskaia\"},{\"authorId\":\"10753214\",\"name\":\"Aliaksandr Siarohin\"},{\"authorId\":\"47705564\",\"name\":\"B. Zhao\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e2706b994697b1653a5bf72c64061f517c062f86\",\"title\":\"DwNet: Dense warp-based network for pose-guided human video generation\",\"url\":\"https://www.semanticscholar.org/paper/e2706b994697b1653a5bf72c64061f517c062f86\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47637491\",\"name\":\"M. Lakhal\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"},{\"authorId\":\"153142893\",\"name\":\"A. Cavallaro\"}],\"doi\":\"10.1109/ICCV.2019.00767\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a1f7dd6c85c4c53b05ae6833ee5541f29a24946\",\"title\":\"View-LSTM: Novel-View Video Synthesis Through View Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/7a1f7dd6c85c4c53b05ae6833ee5541f29a24946\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1808.06847\",\"authors\":[{\"authorId\":\"3451442\",\"name\":\"Kfir Aberman\"},{\"authorId\":\"5807605\",\"name\":\"M. Shi\"},{\"authorId\":\"1400217664\",\"name\":\"Jing Liao\"},{\"authorId\":\"1684384\",\"name\":\"Dani Lischinski\"},{\"authorId\":\"1748939\",\"name\":\"B. Chen\"},{\"authorId\":\"1388323541\",\"name\":\"D. Cohen-Or\"}],\"doi\":\"10.1111/cgf.13632\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c14e77fc10f133358903019f45132d694682ab88\",\"title\":\"Deep Video\\u2010Based Performance Cloning\",\"url\":\"https://www.semanticscholar.org/paper/c14e77fc10f133358903019f45132d694682ab88\",\"venue\":\"Comput. Graph. Forum\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151368062\",\"name\":\"Yukitaka Tsuchiya\"},{\"authorId\":\"21583404\",\"name\":\"Takahiro Itazuri\"},{\"authorId\":\"41015901\",\"name\":\"R. Natsume\"},{\"authorId\":\"48333526\",\"name\":\"S. Yamamoto\"},{\"authorId\":\"1707631\",\"name\":\"T. Kato\"},{\"authorId\":\"1731030\",\"name\":\"S. Morishima\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c1fb788324287f7e7d84606f32f1878196764b36\",\"title\":\"Generating Video from Single Image and Sound\",\"url\":\"https://www.semanticscholar.org/paper/c1fb788324287f7e7d84606f32f1878196764b36\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143624101\",\"name\":\"R. Zhao\"},{\"authorId\":\"50426357\",\"name\":\"Q. Ji\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"22fcd10806e3dc89f862c55bbbf81edc957f3929\",\"title\":\"An Adversarial Hierarchical Hidden Markov Model for Human Pose Modeling and Generation\",\"url\":\"https://www.semanticscholar.org/paper/22fcd10806e3dc89f862c55bbbf81edc957f3929\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"2001.03569\",\"authors\":[{\"authorId\":\"7667912\",\"name\":\"L. Duan\"},{\"authorId\":\"41127426\",\"name\":\"Jiaying Liu\"},{\"authorId\":\"1898172\",\"name\":\"W. Yang\"},{\"authorId\":\"50592933\",\"name\":\"Tiejun Huang\"},{\"authorId\":\"48385803\",\"name\":\"W. Gao\"}],\"doi\":\"10.1109/TIP.2020.3016485\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c1007dd5518ac0ce41f2d8f72ed8c5fd4971caf\",\"title\":\"Video Coding for Machines: A Paradigm of Collaborative Compression and Intelligent Analytics\",\"url\":\"https://www.semanticscholar.org/paper/4c1007dd5518ac0ce41f2d8f72ed8c5fd4971caf\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2001.06937\",\"authors\":[{\"authorId\":\"48603577\",\"name\":\"Jie Gui\"},{\"authorId\":\"1757186\",\"name\":\"Z. Sun\"},{\"authorId\":\"145868453\",\"name\":\"Yonggang Wen\"},{\"authorId\":\"145047838\",\"name\":\"Dacheng Tao\"},{\"authorId\":\"2778556\",\"name\":\"Jie-ping Ye\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea\",\"title\":\"A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications\",\"url\":\"https://www.semanticscholar.org/paper/fbc5486a1ffb9039dbb5046b84f0eb32e4ce8eea\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.06628\",\"authors\":[{\"authorId\":\"121544228\",\"name\":\"Z. Li\"},{\"authorId\":\"1813796\",\"name\":\"Zhaopeng Cui\"},{\"authorId\":\"1821761\",\"name\":\"M. Oswald\"},{\"authorId\":\"88550057\",\"name\":\"Marc Pollefeys\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bbaa5a95aba6cccb7c9885db0329c413f12bbb22\",\"title\":\"Street-view Panoramic Video Synthesis from a Single Satellite Image\",\"url\":\"https://www.semanticscholar.org/paper/bbaa5a95aba6cccb7c9885db0329c413f12bbb22\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":6945308,\"doi\":\"10.1109/ICCV.2017.308\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":36,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"062c41dad67bb68fefd9ff0c5c4d296e796004dc\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3117618\",\"name\":\"Seiya Tokui\"},{\"authorId\":\"1812144\",\"name\":\"Kenta Oono\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"67156902beca9bc90b728c8d5dd4ac9d8b27d3a3\",\"title\":\"Chainer : a Next-Generation Open Source Framework for Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/67156902beca9bc90b728c8d5dd4ac9d8b27d3a3\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1768464\",\"name\":\"David Stavens\"},{\"authorId\":\"144867807\",\"name\":\"S. Thrun\"}],\"doi\":\"10.1109/CVPR.2010.5539773\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a94783b5f57bfb44a432af043d309b226954ad3b\",\"title\":\"Unsupervised learning of invariant features using video\",\"url\":\"https://www.semanticscholar.org/paper/a94783b5f57bfb44a432af043d309b226954ad3b\",\"venue\":\"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\",\"year\":2010},{\"arxivId\":\"1608.07724\",\"authors\":[{\"authorId\":\"49455017\",\"name\":\"Yipin Zhou\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1007/978-3-319-46484-8_16\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5bf3b2a5591f2a2f0c648dc5a1f4ff3bd6b9102a\",\"title\":\"Learning Temporal Transformations from Time-Lapse Videos\",\"url\":\"https://www.semanticscholar.org/paper/5bf3b2a5591f2a2f0c648dc5a1f4ff3bd6b9102a\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0228810a988f6b8f06337e14f564e2fd3f6e1056\",\"title\":\"The Recurrent Temporal Restricted Boltzmann Machine\",\"url\":\"https://www.semanticscholar.org/paper/0228810a988f6b8f06337e14f564e2fd3f6e1056\",\"venue\":\"NIPS\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Nair\"},{\"authorId\":null,\"name\":\"G. E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Rectified Linear Units\",\"url\":\"\",\"venue\":\"Improve Restricted Boltzmann Machines. ICML,\",\"year\":2010},{\"arxivId\":\"1502.04623\",\"authors\":[{\"authorId\":\"144717963\",\"name\":\"K. Gregor\"},{\"authorId\":\"1841008\",\"name\":\"Ivo Danihelka\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"1748523\",\"name\":\"Danilo Jimenez Rezende\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2785f66c20fbdf30ec26c0931584c6d6a0f4fca\",\"title\":\"DRAW: A Recurrent Neural Network For Image Generation\",\"url\":\"https://www.semanticscholar.org/paper/a2785f66c20fbdf30ec26c0931584c6d6a0f4fca\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c74e230a5a6fd5e2db6ace765ce38afe65f96214\",\"title\":\"Learning Multilevel Distributed Representations for High-Dimensional Sequences\",\"url\":\"https://www.semanticscholar.org/paper/c74e230a5a6fd5e2db6ace765ce38afe65f96214\",\"venue\":\"AISTATS\",\"year\":2007},{\"arxivId\":\"1412.6056\",\"authors\":[{\"authorId\":\"2558463\",\"name\":\"R. Goroshin\"},{\"authorId\":\"143627859\",\"name\":\"Joan Bruna\"},{\"authorId\":\"2704494\",\"name\":\"J. Tompson\"},{\"authorId\":\"2060028\",\"name\":\"D. Eigen\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":\"10.1109/ICCV.2015.465\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53e14bb909ef71388c8ca189c9db84b52af2db44\",\"title\":\"Unsupervised Learning of Spatiotemporally Coherent Metrics\",\"url\":\"https://www.semanticscholar.org/paper/53e14bb909ef71388c8ca189c9db84b52af2db44\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1302.4389\",\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"b7b915d508987b73b61eccd2b237e7ed099a2d29\",\"title\":\"Maxout Networks\",\"url\":\"https://www.semanticscholar.org/paper/b7b915d508987b73b61eccd2b237e7ed099a2d29\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2012.6247801\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"49435aab7cdf259335725acc96691f755e436f55\",\"title\":\"A database for fine grained activity detection of cooking activities\",\"url\":\"https://www.semanticscholar.org/paper/49435aab7cdf259335725acc96691f755e436f55\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"367f2c63a6f6a10b3b64b8729d601e69337ee3cc\",\"title\":\"Rectifier Nonlinearities Improve Neural Network Acoustic Models\",\"url\":\"https://www.semanticscholar.org/paper/367f2c63a6f6a10b3b64b8729d601e69337ee3cc\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Tieleman\"},{\"authorId\":null,\"name\":\"G. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lecture 6.5 - RmsProp: Divide the gradient by a running average of its recent magnitude\",\"url\":\"\",\"venue\":\"COURSERA: Neural Networks for Machine Learning,\",\"year\":2012},{\"arxivId\":\"1412.6604\",\"authors\":[{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"143627859\",\"name\":\"Joan Bruna\"},{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2939803\",\"name\":\"Ronan Collobert\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"355f98e4827a1b6ad3f29d07ea2bcf9ad078295c\",\"title\":\"Video (language) modeling: a baseline for generative models of natural videos\",\"url\":\"https://www.semanticscholar.org/paper/355f98e4827a1b6ad3f29d07ea2bcf9ad078295c\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145557639\",\"name\":\"V. Nair\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a538b05ebb01a40323997629e171c91aa28b8e2f\",\"title\":\"Rectified Linear Units Improve Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":\"1604.07379\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"2562966\",\"name\":\"Philipp Kr\\u00e4henb\\u00fchl\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2016.278\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d0effebfa4bed19b6ba41f3af5b7e5b6890de87\",\"title\":\"Context Encoders: Feature Learning by Inpainting\",\"url\":\"https://www.semanticscholar.org/paper/7d0effebfa4bed19b6ba41f3af5b7e5b6890de87\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2841331\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"3332944\",\"name\":\"Maxim Tatarchenko\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1109/TPAMI.2016.2567384\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c3d7d5a9a9eafa084913b8c83a013c81d50479f\",\"title\":\"Learning to Generate Chairs, Tables and Cars with Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/1c3d7d5a9a9eafa084913b8c83a013c81d50479f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H. Pirsiavash\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Generating Videos with Scene Dynamics Action Recognition by Dense Trajectories\",\"url\":\"\",\"venue\":\"\",\"year\":2011},{\"arxivId\":\"1606.03498\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"571b0750085ae3d939525e62af510ee2cee9d5ea\",\"title\":\"Improved Techniques for Training GANs\",\"url\":\"https://www.semanticscholar.org/paper/571b0750085ae3d939525e62af510ee2cee9d5ea\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1602.05110\",\"authors\":[{\"authorId\":\"2903841\",\"name\":\"D. Im\"},{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"145079150\",\"name\":\"Hui Jiang\"},{\"authorId\":\"1710604\",\"name\":\"R. Memisevic\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"32c09d2933a4638b034343f9be20544dacf6031f\",\"title\":\"Generating images with recurrent adversarial networks\",\"url\":\"https://www.semanticscholar.org/paper/32c09d2933a4638b034343f9be20544dacf6031f\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1610.00527\",\"authors\":[{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1841008\",\"name\":\"Ivo Danihelka\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"b01871c114b122340209562972ff515b86b16ccf\",\"title\":\"Video Pixel Networks\",\"url\":\"https://www.semanticscholar.org/paper/b01871c114b122340209562972ff515b86b16ccf\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1511.07289\",\"authors\":[{\"authorId\":\"34917892\",\"name\":\"Djork-Arn\\u00e9 Clevert\"},{\"authorId\":\"2465270\",\"name\":\"Thomas Unterthiner\"},{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f63e917638553414526a0cc8550de4ad2d83fe7a\",\"title\":\"Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\",\"url\":\"https://www.semanticscholar.org/paper/f63e917638553414526a0cc8550de4ad2d83fe7a\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948848\",\"name\":\"S. Sadanand\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2012.6247806\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d90cb88d89408daf4a0fe5ac341a6b9db747a556\",\"title\":\"Action bank: A high-level representation of activity in video\",\"url\":\"https://www.semanticscholar.org/paper/d90cb88d89408daf4a0fe5ac341a6b9db747a556\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":\"1312.6114\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"1678311\",\"name\":\"M. Welling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"title\":\"Auto-Encoding Variational Bayes\",\"url\":\"https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":\"1505.04868\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"}],\"doi\":\"10.1109/CVPR.2015.7299059\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":false,\"paperId\":\"df658828fb4146877f1031ec9d07052f7eb31186\",\"title\":\"Action recognition with trajectory-pooled deep-convolutional descriptors\",\"url\":\"https://www.semanticscholar.org/paper/df658828fb4146877f1031ec9d07052f7eb31186\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1511.06434\",\"authors\":[{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"2096458\",\"name\":\"Luke Metz\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8388f1be26329fa45e5807e968a641ce170ea078\",\"title\":\"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8388f1be26329fa45e5807e968a641ce170ea078\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"9330607\",\"name\":\"S. Roweis\"}],\"doi\":\"10.7551/mitpress/7503.003.0173\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"497a80b2813cffb17f46af50e621a71505094528\",\"title\":\"Modeling Human Motion Using Binary Latent Variables\",\"url\":\"https://www.semanticscholar.org/paper/497a80b2813cffb17f46af50e621a71505094528\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2860351\",\"name\":\"Will Y. Zou\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1682028\",\"name\":\"Shenghuo Zhu\"},{\"authorId\":\"144782042\",\"name\":\"Kai Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aefc7c708269b874182a5c877fb6dae06da210d4\",\"title\":\"Deep Learning of Invariant Features via Simulated Fixations in Video\",\"url\":\"https://www.semanticscholar.org/paper/aefc7c708269b874182a5c877fb6dae06da210d4\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":\"1511.05440\",\"authors\":[{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2341378\",\"name\":\"C. Couprie\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"title\":\"Deep multi-scale video prediction beyond mean square error\",\"url\":\"https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1502.01852\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/ICCV.2015.123\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d6f2f611da110b5b5061731be3fc4c7f45d8ee23\",\"title\":\"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\",\"url\":\"https://www.semanticscholar.org/paper/d6f2f611da110b5b5061731be3fc4c7f45d8ee23\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1506.05751\",\"authors\":[{\"authorId\":\"40081727\",\"name\":\"Emily L. Denton\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"47900aca2f0b50da3010ad59b394c870f0e6c02e\",\"title\":\"Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/47900aca2f0b50da3010ad59b394c870f0e6c02e\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1608.00859\",\"authors\":[{\"authorId\":\"33345248\",\"name\":\"L. Wang\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"1915826\",\"name\":\"Zhe Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-319-46484-8_2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"title\":\"Temporal Segment Networks: Towards Good Practices for Deep Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ea3d7de6c0880e14455b9acb28f1bc1234321456\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1701.07875\",\"authors\":[{\"authorId\":\"2877311\",\"name\":\"Mart\\u00edn Arjovsky\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"52184096\",\"name\":\"L. Bottou\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2f85b7376769473d2bed56f855f115e23d727094\",\"title\":\"Wasserstein GAN\",\"url\":\"https://www.semanticscholar.org/paper/2f85b7376769473d2bed56f855f115e23d727094\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1603.05631\",\"authors\":[{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1007/978-3-319-46493-0_20\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9c763df6843aba88d7fb3ab3c55a5937a5f39276\",\"title\":\"Generative Image Modeling Using Style and Structure Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/9c763df6843aba88d7fb3ab3c55a5937a5f39276\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"143836295\",\"name\":\"W. Xu\"},{\"authorId\":\"41216159\",\"name\":\"Ming Yang\"},{\"authorId\":\"144782042\",\"name\":\"Kai Yu\"}],\"doi\":\"10.1109/TPAMI.2012.59\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"title\":\"3D Convolutional Neural Networks for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"1785346\",\"name\":\"Roger B. Grosse\"},{\"authorId\":\"2615814\",\"name\":\"R. Ranganath\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1553374.1553453\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e80f755bcbf10479afd2338cec05211fdbd325c\",\"title\":\"Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations\",\"url\":\"https://www.semanticscholar.org/paper/1e80f755bcbf10479afd2338cec05211fdbd325c\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":\"1507.08750\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4257bc131c36504a04382290cbc27ca8bb27813\",\"title\":\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c11626ae08706e6185fceff0a6d05e4bfd6bd06\",\"title\":\"Unsupervised Learning of Visual Representations using Videos\",\"url\":\"https://www.semanticscholar.org/paper/6c11626ae08706e6185fceff0a6d05e4bfd6bd06\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1411.4038\",\"authors\":[{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2572683\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"317aee7fc081f2b137a85c4f20129007fd8e717e\",\"title\":\"Fully Convolutional Networks for Semantic Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/317aee7fc081f2b137a85c4f20129007fd8e717e\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1609.02612\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.13016/M26GIH-TNYZ\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"title\":\"Generating Videos with Scene Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1689269\",\"name\":\"C. Liu\"}],\"doi\":\"10.1109/CVPR.2011.5995407\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3afbb0e64fcb70496b44b30b76fac9456cc51e34\",\"title\":\"Action recognition by dense trajectories\",\"url\":\"https://www.semanticscholar.org/paper/3afbb0e64fcb70496b44b30b76fac9456cc51e34\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":\"1503.08909\",\"authors\":[{\"authorId\":\"2340579\",\"name\":\"J. Ng\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"}],\"doi\":\"10.1109/CVPR.2015.7299101\",\"intent\":[\"methodology\",\"result\"],\"isInfluential\":false,\"paperId\":\"5418b2a482720e013d487a385c26fae0f017c6a6\",\"title\":\"Beyond short snippets: Deep networks for video classification\",\"url\":\"https://www.semanticscholar.org/paper/5418b2a482720e013d487a385c26fae0f017c6a6\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1502.04681\",\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"title\":\"Unsupervised Learning of Video Representations using LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1411.1784\",\"authors\":[{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"353ecf7b66b3e9ff5e9f41145a147e899a2eea5c\",\"title\":\"Conditional Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/353ecf7b66b3e9ff5e9f41145a147e899a2eea5c\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1608.06993\",\"authors\":[{\"authorId\":\"143983679\",\"name\":\"Gao Huang\"},{\"authorId\":null,\"name\":\"Zhuang Liu\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":\"10.1109/CVPR.2017.243\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5694e46284460a648fe29117cbc55f6c9be3fa3c\",\"title\":\"Densely Connected Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/5694e46284460a648fe29117cbc55f6c9be3fa3c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"},{\"authorId\":\"2860351\",\"name\":\"Will Y. Zou\"},{\"authorId\":\"32408341\",\"name\":\"Serena Y. Yeung\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1109/CVPR.2011.5995496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42269d0438c0ae4ca892334946ed779999691074\",\"title\":\"Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis\",\"url\":\"https://www.semanticscholar.org/paper/42269d0438c0ae4ca892334946ed779999691074\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"85021c84383d18a7a4434d76dc8135fc6bdc0aa6\",\"title\":\"Deep Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/85021c84383d18a7a4434d76dc8135fc6bdc0aa6\",\"venue\":\"AISTATS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"145715698\",\"name\":\"C. K. Williams\"},{\"authorId\":\"33652486\",\"name\":\"J. Winn\"}],\"doi\":\"10.1007/s11263-013-0669-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2375de37c64bb47795beb28b763f923afd85a638\",\"title\":\"The Shape Boltzmann Machine: A Strong Model of Object Shape\",\"url\":\"https://www.semanticscholar.org/paper/2375de37c64bb47795beb28b763f923afd85a638\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":\"1604.04382\",\"authors\":[{\"authorId\":\"153228286\",\"name\":\"Chuan Li\"},{\"authorId\":\"1723149\",\"name\":\"M. Wand\"}],\"doi\":\"10.1007/978-3-319-46487-9_43\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"102a2096ba2e2947dc252445f764e7583b557680\",\"title\":\"Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/102a2096ba2e2947dc252445f764e7583b557680\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"2428034\",\"name\":\"C. Bregler\"}],\"doi\":\"10.1007/978-3-642-15567-3_11\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d476b96be73fccc61f2076befbf5a468caa4180\",\"title\":\"Convolutional Learning of Spatio-temporal Features\",\"url\":\"https://www.semanticscholar.org/paper/4d476b96be73fccc61f2076befbf5a468caa4180\",\"venue\":\"ECCV\",\"year\":2010}],\"title\":\"Temporal Generative Adversarial Nets with Singular Value Clipping\",\"topics\":[{\"topic\":\"Generative model\",\"topicId\":\"37177\",\"url\":\"https://www.semanticscholar.org/topic/37177\"},{\"topic\":\"Latent variable\",\"topicId\":\"79039\",\"url\":\"https://www.semanticscholar.org/topic/79039\"},{\"topic\":\"Glossary of computer graphics\",\"topicId\":\"12790\",\"url\":\"https://www.semanticscholar.org/topic/12790\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Motion interpolation\",\"topicId\":\"21402\",\"url\":\"https://www.semanticscholar.org/topic/21402\"},{\"topic\":\"Brian\",\"topicId\":\"2605\",\"url\":\"https://www.semanticscholar.org/topic/2605\"},{\"topic\":\"Instability\",\"topicId\":\"4779\",\"url\":\"https://www.semanticscholar.org/topic/4779\"},{\"topic\":\"OS-tan\",\"topicId\":\"3555107\",\"url\":\"https://www.semanticscholar.org/topic/3555107\"}],\"url\":\"https://www.semanticscholar.org/paper/062c41dad67bb68fefd9ff0c5c4d296e796004dc\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017}\n"