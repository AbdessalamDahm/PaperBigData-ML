"{\"abstract\":\"Real-world videos often have complex dynamics, methods for generating open-domain video descriptions should be sensitive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).\",\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\",\"url\":\"https://www.semanticscholar.org/author/1811430\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\",\"url\":\"https://www.semanticscholar.org/author/34849128\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\",\"url\":\"https://www.semanticscholar.org/author/7408951\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\",\"url\":\"https://www.semanticscholar.org/author/1797655\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\",\"url\":\"https://www.semanticscholar.org/author/1753210\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\",\"url\":\"https://www.semanticscholar.org/author/2903226\"}],\"citationVelocity\":203,\"citations\":[{\"arxivId\":\"1610.04062\",\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"119745921\",\"name\":\"Dong Zhang\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fb31d0375552500bd494af20ab0c3109c9be3d2\",\"title\":\"Video Fill in the Blank with Merging LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/9fb31d0375552500bd494af20ab0c3109c9be3d2\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2017/307\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e33bc5c83f2cea403a5521385ee8e2794b311275\",\"title\":\"MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e33bc5c83f2cea403a5521385ee8e2794b311275\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49890994\",\"name\":\"Y. Zhang\"},{\"authorId\":\"144343547\",\"name\":\"W. Xiao\"}],\"doi\":\"10.1109/ACCESS.2018.2865589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0298eb372adaff137da6f6051273e13a76db4c39\",\"title\":\"Keyphrase Generation Based on Deep Seq2seq Model\",\"url\":\"https://www.semanticscholar.org/paper/0298eb372adaff137da6f6051273e13a76db4c39\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49370397\",\"name\":\"D. Wang\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"}],\"doi\":\"10.1109/ICBK.2017.26\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4d22c000c12aaedadcf075736dfc998dea932f06\",\"title\":\"Video Captioning with Semantic Information from the Knowledge Base\",\"url\":\"https://www.semanticscholar.org/paper/4d22c000c12aaedadcf075736dfc998dea932f06\",\"venue\":\"2017 IEEE International Conference on Big Knowledge (ICBK)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3128328\",\"name\":\"Manfei Liu\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"},{\"authorId\":\"2002678\",\"name\":\"Zecheng Xie\"}],\"doi\":\"10.1109/ICDAR.2017.114\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"db98e1dcf4621c5e0fb8c11be509029577485bd2\",\"title\":\"PS-LSTM: Capturing Essential Sequential Online Information with Path Signature and LSTM for Writer Identification\",\"url\":\"https://www.semanticscholar.org/paper/db98e1dcf4621c5e0fb8c11be509029577485bd2\",\"venue\":\"2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40dd2b9aace337467c6e1e269d0cb813442313d7\",\"title\":\"Localizing spatially and temporally objects and actions in videos. (Localiser spatio-temporallement des objets et des actions dans des vid\\u00e9os)\",\"url\":\"https://www.semanticscholar.org/paper/40dd2b9aace337467c6e1e269d0cb813442313d7\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1812.00108\",\"authors\":[{\"authorId\":\"46419391\",\"name\":\"M. Elfeki\"},{\"authorId\":\"3377097\",\"name\":\"A. Sharghi\"},{\"authorId\":\"1976152\",\"name\":\"Srikrishna Karanam\"},{\"authorId\":\"3311781\",\"name\":\"Z. Wu\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eda39b775a0e393d41d099a09c10913c3ee65849\",\"title\":\"Multi-View Egocentric Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/eda39b775a0e393d41d099a09c10913c3ee65849\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1911.05449\",\"authors\":[{\"authorId\":\"23604913\",\"name\":\"Liqi Yan\"},{\"authorId\":\"151478904\",\"name\":\"Mingjian Zhu\"},{\"authorId\":\"9792456\",\"name\":\"Changbin Yu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fa00475c5819723b767e51e3b342d1ff4c64159f\",\"title\":\"Crowd Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/fa00475c5819723b767e51e3b342d1ff4c64159f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"49338773\",\"name\":\"Y. Zhan\"},{\"authorId\":\"27718163\",\"name\":\"Y. Yang\"},{\"authorId\":\"1410252832\",\"name\":\"Xing Xu\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TIP.2019.2952088\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ac2b6006886e6693ce85a5c935abceef90ae3b89\",\"title\":\"A Context Knowledge Map Guided Coarse-to-Fine Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ac2b6006886e6693ce85a5c935abceef90ae3b89\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7184058\",\"name\":\"Daehun Kim\"},{\"authorId\":\"1565662585\",\"name\":\"Joungmin Beh\"},{\"authorId\":\"152829188\",\"name\":\"Y. Chen\"},{\"authorId\":\"81950948\",\"name\":\"Hanseok Ko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa160f17a17bbf8bb9ee1aa81b7c2a4975c39885\",\"title\":\"KU-ISPL TRECVID 2017 VTT System\",\"url\":\"https://www.semanticscholar.org/paper/fa160f17a17bbf8bb9ee1aa81b7c2a4975c39885\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742975\",\"name\":\"Zhenrong Deng\"},{\"authorId\":\"2011644117\",\"name\":\"Fuxin Ma\"},{\"authorId\":\"122211567\",\"name\":\"Rushi Lan\"},{\"authorId\":\"122211566\",\"name\":\"Rushi Lan\"},{\"authorId\":\"1727001\",\"name\":\"W. Huang\"},{\"authorId\":\"2405346\",\"name\":\"X. Luo\"}],\"doi\":\"10.1016/j.neucom.2020.02.102\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1abdc6e3006d540d5f40e037c2fdf1ea328be12\",\"title\":\"A Two-stage Chinese text summarization algorithm using keyword information and adversarial learning\",\"url\":\"https://www.semanticscholar.org/paper/f1abdc6e3006d540d5f40e037c2fdf1ea328be12\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.03493\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"46255707\",\"name\":\"Lei Li\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/ICCV.2019.00468\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"title\":\"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research\",\"url\":\"https://www.semanticscholar.org/paper/28b74bb7c8b08cceb2430ec2d54dfa0f3225d796\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123726071\",\"name\":\"Lingbo Mo\"},{\"authorId\":\"1792246\",\"name\":\"C. Zhang\"},{\"authorId\":\"144105871\",\"name\":\"Y. Ji\"},{\"authorId\":\"144909678\",\"name\":\"Zheng Hu\"}],\"doi\":\"10.1007/978-3-030-20870-7_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a050168894dc08ee5b21c62c9085cef633cf32cf\",\"title\":\"Adversarial Learning for Visual Storytelling with Sense Group Partition\",\"url\":\"https://www.semanticscholar.org/paper/a050168894dc08ee5b21c62c9085cef633cf32cf\",\"venue\":\"ACCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144150628\",\"name\":\"Kai Xu\"},{\"authorId\":\"40615963\",\"name\":\"Fengbo Ren\"}],\"doi\":\"10.1109/WACV.2018.00187\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6691abf2cf7c521596330b03297e4ba370efe76\",\"title\":\"CSVideoNet: A Real-Time End-to-End Learning Framework for High-Frame-Rate Video Compressive Sensing\",\"url\":\"https://www.semanticscholar.org/paper/d6691abf2cf7c521596330b03297e4ba370efe76\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a58dedf7192e991b74145da928cae679d0998ce\",\"title\":\"Survey of Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/3a58dedf7192e991b74145da928cae679d0998ce\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46583706\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"40476140\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1109/CVPR.2018.00784\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b910a6f687a4e56062dc326786cee297bd60e8c1\",\"title\":\"M3: Multimodal Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b910a6f687a4e56062dc326786cee297bd60e8c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1812.03849\",\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"2978255\",\"name\":\"Wen-bing Huang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1688516\",\"name\":\"Jingdong Wang\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"},{\"authorId\":\"50882910\",\"name\":\"Junzhou Huang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"title\":\"Weakly Supervised Dense Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8735ac2324b5aeaa3a8418af97eb82e9aa1910cb\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48195668\",\"name\":\"Jin Yuan\"},{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"46448210\",\"name\":\"Xiangnan Zhang\"},{\"authorId\":\"47814961\",\"name\":\"Y. Ding\"},{\"authorId\":\"145673165\",\"name\":\"Wei Wei\"}],\"doi\":\"10.1109/BigMM.2018.8499357\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad8d0432bdc1fcefbd7ebc8badea8aceec16fbdf\",\"title\":\"Video Captioning with Semantic Guiding\",\"url\":\"https://www.semanticscholar.org/paper/ad8d0432bdc1fcefbd7ebc8badea8aceec16fbdf\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30075807\",\"name\":\"L. Zaman\"},{\"authorId\":\"9377338\",\"name\":\"S. Sumpeno\"},{\"authorId\":\"2517400\",\"name\":\"M. Hariadi\"}],\"doi\":\"10.1109/CENIM.2018.8710992\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"186f687b1d978e041279214f8913d0b7edbde622\",\"title\":\"Training Strategies for Remo Dance on Long Short-Term Memory Generative Model\",\"url\":\"https://www.semanticscholar.org/paper/186f687b1d978e041279214f8913d0b7edbde622\",\"venue\":\"2018 International Conference on Computer Engineering, Network and Intelligent Multimedia (CENIM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153164376\",\"name\":\"Liping Xie\"},{\"authorId\":\"1399606597\",\"name\":\"C. Gong\"},{\"authorId\":\"50561931\",\"name\":\"Jinxia Zhang\"},{\"authorId\":\"39155455\",\"name\":\"Shuo Shan\"},{\"authorId\":\"3140817\",\"name\":\"Haikun Wei\"}],\"doi\":\"10.1007/978-3-030-41299-9_53\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ee492d03344fadb15d7ceb708abba2394877767\",\"title\":\"Semi-supervised Early Event Detection\",\"url\":\"https://www.semanticscholar.org/paper/4ee492d03344fadb15d7ceb708abba2394877767\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40087835\",\"name\":\"Cong Duy Vu Hoang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e288af83f750ccfc3da958628d59c72c0a2cf96\",\"title\":\"On the use of prior and external knowledge in neural sequence models\",\"url\":\"https://www.semanticscholar.org/paper/0e288af83f750ccfc3da958628d59c72c0a2cf96\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28646227\",\"name\":\"Tielin Zhang\"},{\"authorId\":\"144492744\",\"name\":\"Y. Zeng\"},{\"authorId\":\"31184900\",\"name\":\"Ruihan Pan\"},{\"authorId\":\"1811211084\",\"name\":\"Mengting Shi\"},{\"authorId\":\"144863217\",\"name\":\"Enmeng Lu\"}],\"doi\":\"10.1007/s12559-020-09753-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"074a927fb2dc7f481e7cef4e55ad1b6a234df8da\",\"title\":\"Brain-Inspired Active Learning Architecture for Procedural Knowledge Understanding Based on Human-Robot Interaction\",\"url\":\"https://www.semanticscholar.org/paper/074a927fb2dc7f481e7cef4e55ad1b6a234df8da\",\"venue\":\"Cognitive Computation\",\"year\":2020},{\"arxivId\":\"2008.06880\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":null,\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413880\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"title\":\"Poet: Product-oriented Video Captioner for E-commerce\",\"url\":\"https://www.semanticscholar.org/paper/72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2802864\",\"name\":\"H. Gao\"},{\"authorId\":\"143979239\",\"name\":\"T. Oates\"}],\"doi\":\"10.1007/978-3-030-10925-7_32\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ac052778fc853ba9e52a12c38ac036d2d558bf5\",\"title\":\"On Finer Control of Information Flow in LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/2ac052778fc853ba9e52a12c38ac036d2d558bf5\",\"venue\":\"ECML/PKDD\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145222076\",\"name\":\"Tianyu Zhang\"},{\"authorId\":\"2366119\",\"name\":\"Weiqing Min\"},{\"authorId\":\"27730334\",\"name\":\"Ying Zhu\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1145/3394171.3413964\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10066d89b96b8baac518c994f23278b817994a12\",\"title\":\"An Egocentric Action Anticipation Framework via Fusing Intuition and Analysis\",\"url\":\"https://www.semanticscholar.org/paper/10066d89b96b8baac518c994f23278b817994a12\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2706269\",\"name\":\"Chengxiang Yin\"},{\"authorId\":\"152949437\",\"name\":\"Jian Tang\"},{\"authorId\":\"48559420\",\"name\":\"Zhiyuan Xu\"},{\"authorId\":null,\"name\":\"Yanzhi Wang\"}],\"doi\":\"10.1109/TNNLS.2019.2938015\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d796ff29f8798c418d5374a6632231f02233dbba\",\"title\":\"Memory Augmented Deep Recurrent Neural Network for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/d796ff29f8798c418d5374a6632231f02233dbba\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471721735\",\"name\":\"Umair Ahmad Khan\"},{\"authorId\":\"1409307726\",\"name\":\"Miguel \\u00c1. Mart\\u00ednez-Del-Amor\"},{\"authorId\":\"49912813\",\"name\":\"Saleh M. Altowaijri\"},{\"authorId\":\"144396106\",\"name\":\"A. Ahmed\"},{\"authorId\":\"3233695\",\"name\":\"A. Rahman\"},{\"authorId\":\"2234879\",\"name\":\"Najm Us Sama\"},{\"authorId\":\"48043100\",\"name\":\"K. Haseeb\"},{\"authorId\":\"50421651\",\"name\":\"Naveed Islam\"}],\"doi\":\"10.1109/ACCESS.2019.2963535\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ffabdc4909395baa486fd56153b98911647e205\",\"title\":\"Movie Tags Prediction and Segmentation Using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/5ffabdc4909395baa486fd56153b98911647e205\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144933637\",\"name\":\"S. Sen\"},{\"authorId\":\"145291370\",\"name\":\"A. Raghunathan\"}],\"doi\":\"10.1109/TCAD.2018.2858362\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bc80d702c6b7bb8f7990ee2e82cc4f01a25ae539\",\"title\":\"Approximate Computing for Long Short Term Memory (LSTM) Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/bc80d702c6b7bb8f7990ee2e82cc4f01a25ae539\",\"venue\":\"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems\",\"year\":2018},{\"arxivId\":\"1907.03049\",\"authors\":[{\"authorId\":null,\"name\":\"Yu-Siang Wang\"},{\"authorId\":\"71309591\",\"name\":\"Hung-Ting Su\"},{\"authorId\":\"150053992\",\"name\":\"Chen-Hsi Chang\"},{\"authorId\":\"143822897\",\"name\":\"Zhe Yu Liu\"},{\"authorId\":\"31871157\",\"name\":\"Winston Hsu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"091ad302f5381bd131b41a57e16d802ff4ab9668\",\"title\":\"Video Question Generation via Cross-Modal Self-Attention Networks Learning\",\"url\":\"https://www.semanticscholar.org/paper/091ad302f5381bd131b41a57e16d802ff4ab9668\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394475099\",\"name\":\"Beg\\u00fcm \\u00c7itamak\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"}],\"doi\":\"10.1109/SIU.2019.8806555\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"title\":\"MSVD-Turkish: A Large-Scale Dataset for Video Captioning in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/dc1ea2bad25bd8ee0c152682b422c30d8849934e\",\"venue\":\"2019 27th Signal Processing and Communications Applications Conference (SIU)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92104112\",\"name\":\"Tangming Chen\"},{\"authorId\":\"9398015\",\"name\":\"Qike Zhao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1007/978-3-030-33982-1_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cf546bfb933bddbd9e7017525d9621405e549b9\",\"title\":\"Boundary Detector Encoder and Decoder with Soft Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1cf546bfb933bddbd9e7017525d9621405e549b9\",\"venue\":\"APWeb/WAIM Workshops\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30532805\",\"name\":\"Qingle Huang\"},{\"authorId\":\"2928799\",\"name\":\"Zicheng Liao\"}],\"doi\":\"10.5244/c.31.126\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"61cf3b276defcc82ccba3566da4a44a88740f013\",\"title\":\"A Convolutional Temporal Encoder for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/61cf3b276defcc82ccba3566da4a44a88740f013\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9260404\",\"name\":\"Xiaotong Du\"},{\"authorId\":\"46685438\",\"name\":\"J. Yuan\"},{\"authorId\":\"1840183\",\"name\":\"H. Liu\"}],\"doi\":\"10.1007/978-3-030-00021-9_40\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"544ccc01b63be4a68fb3f2c318ee14b5fc036c37\",\"title\":\"Attention-Based Bidirectional Recurrent Neural Networks for Description Generation of Videos\",\"url\":\"https://www.semanticscholar.org/paper/544ccc01b63be4a68fb3f2c318ee14b5fc036c37\",\"venue\":\"ICCCS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47569376\",\"name\":\"Shijie Yang\"},{\"authorId\":\"73596205\",\"name\":\"L. Li\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"49356099\",\"name\":\"Dechao Meng\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3343031.3350859\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"title\":\"Structured Stochastic Recurrent Network for Linguistic Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145859691\",\"name\":\"J. Sheela\"},{\"authorId\":\"108731783\",\"name\":\"B. Janet\"}],\"doi\":\"10.1007/s12652-020-02412-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bc96240f6ea1df47d84a8cb8edfc82c96928e1a0\",\"title\":\"An abstractive summary generation system for customer reviews and news article using deep learning\",\"url\":\"https://www.semanticscholar.org/paper/bc96240f6ea1df47d84a8cb8edfc82c96928e1a0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.09049\",\"authors\":[{\"authorId\":\"1810689822\",\"name\":\"Ganchao Tan\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.24963/ijcai.2020/104\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"title\":\"Learning to Discretely Compose Reasoning Module Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.06162\",\"authors\":[{\"authorId\":\"29724362\",\"name\":\"Longlong Jing\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/tpami.2020.2992393\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c94ee7df6bc2bfcac76703be4f059a79010f7e5\",\"title\":\"Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/4c94ee7df6bc2bfcac76703be4f059a79010f7e5\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"3076466\",\"name\":\"X. Liu\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":\"10.1007/978-3-030-00776-8_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"title\":\"Video Captioning Based on the Spatial-Temporal Saliency Tracing\",\"url\":\"https://www.semanticscholar.org/paper/ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2502363\",\"name\":\"Gagan Kanojia\"},{\"authorId\":\"145853779\",\"name\":\"S. Raman\"}],\"doi\":\"10.1016/j.patrec.2018.11.014\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8174e51ee117cbdfb0358eaceaa2b16ac7a91c9\",\"title\":\"DeepImSeq: Deep image sequencing for unsynchronized cameras\",\"url\":\"https://www.semanticscholar.org/paper/b8174e51ee117cbdfb0358eaceaa2b16ac7a91c9\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jing Wang\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8218640e95bb2d925a617b1c3012eed7d209351\",\"title\":\"Show, Reward and Tell: Automatic Generation of Narrative Paragraph From Photo Stream by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/b8218640e95bb2d925a617b1c3012eed7d209351\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"2008.10966\",\"authors\":[{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"47268124\",\"name\":\"T. Li\"},{\"authorId\":\"46499812\",\"name\":\"Yuan Yuan\"},{\"authorId\":\"1785714\",\"name\":\"D. Katabi\"}],\"doi\":\"10.1007/978-3-030-58536-5_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4\",\"title\":\"In-Home Daily-Life Captioning Using Radio Signals\",\"url\":\"https://www.semanticscholar.org/paper/fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2010.05264\",\"authors\":[{\"authorId\":\"2653622\",\"name\":\"Junfu Pu\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"1400188907\",\"name\":\"Hezhen Hu\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"}],\"doi\":\"10.1145/3394171.3413931\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce50fa888551b640fe3dddc57289c27f325c029b\",\"title\":\"Boosting Continuous Sign Language Recognition via Cross Modality Augmentation\",\"url\":\"https://www.semanticscholar.org/paper/ce50fa888551b640fe3dddc57289c27f325c029b\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1903.10869\",\"authors\":[{\"authorId\":\"145062693\",\"name\":\"Anh Nguyen\"},{\"authorId\":\"3354627\",\"name\":\"Thanh-Toan Do\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"},{\"authorId\":\"1745158\",\"name\":\"D. Caldwell\"},{\"authorId\":\"145887349\",\"name\":\"N. Tsagarakis\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fda87f56010b1e8d05a52a166fdb2750b4dec39b\",\"title\":\"V2CNet: A Deep Learning Framework to Translate Videos to Commands for Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/fda87f56010b1e8d05a52a166fdb2750b4dec39b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144364295\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"title\":\"TVT: Two-View Transformer Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"venue\":\"ACML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":\"10.1007/978-3-319-46604-0_33\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"95df7770a5036c87104df23f333aa05e67723cdc\",\"title\":\"DeepDiary: Automatically Captioning Lifelogging Image Streams\",\"url\":\"https://www.semanticscholar.org/paper/95df7770a5036c87104df23f333aa05e67723cdc\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34575578\",\"name\":\"Y. Fan\"},{\"authorId\":\"3090848\",\"name\":\"Xiangju Lu\"},{\"authorId\":\"144760436\",\"name\":\"Dian Li\"},{\"authorId\":\"2816557\",\"name\":\"Yuanliu Liu\"}],\"doi\":\"10.1145/2993148.2997632\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"92527ace7f75188b5ec209ff7d59f431343075e4\",\"title\":\"Video-based emotion recognition using CNN-RNN and C3D hybrid networks\",\"url\":\"https://www.semanticscholar.org/paper/92527ace7f75188b5ec209ff7d59f431343075e4\",\"venue\":\"ICMI\",\"year\":2016},{\"arxivId\":\"1707.06436\",\"authors\":[{\"authorId\":\"1730200\",\"name\":\"H. Kataoka\"},{\"authorId\":\"3393640\",\"name\":\"Soma Shirakabe\"},{\"authorId\":null,\"name\":\"Yun He\"},{\"authorId\":\"9935341\",\"name\":\"S. Ueta\"},{\"authorId\":\"5014206\",\"name\":\"T. Suzuki\"},{\"authorId\":\"49897653\",\"name\":\"K. Abe\"},{\"authorId\":\"2554424\",\"name\":\"Asako Kanezaki\"},{\"authorId\":\"49133490\",\"name\":\"Shinichiro Morita\"},{\"authorId\":\"22219521\",\"name\":\"Toshiyuki Yabe\"},{\"authorId\":\"50544018\",\"name\":\"Yoshihiro Kanehara\"},{\"authorId\":\"22174281\",\"name\":\"Hiroya Yatsuyanagi\"},{\"authorId\":\"1692565\",\"name\":\"S. Maruyama\"},{\"authorId\":\"10756539\",\"name\":\"Ryousuke Takasawa\"},{\"authorId\":\"3217653\",\"name\":\"Masataka Fuchida\"},{\"authorId\":\"2642022\",\"name\":\"Y. Miyashita\"},{\"authorId\":\"34935749\",\"name\":\"Kazushige Okayasu\"},{\"authorId\":\"20505300\",\"name\":\"Yuta Matsuzaki\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a9ca15aebadad4b7cfa18c5af91c431d6045b86\",\"title\":\"cvpaper.challenge in 2016: Futuristic Computer Vision through 1, 600 Papers Survey\",\"url\":\"https://www.semanticscholar.org/paper/8a9ca15aebadad4b7cfa18c5af91c431d6045b86\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1807.10018\",\"authors\":[{\"authorId\":\"51152390\",\"name\":\"Yilei Xiong\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01252-6_29\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"title\":\"Move Forward and Tell: A Progressive Generator of Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1808.02559\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"153188107\",\"name\":\"Jongseok Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1007/978-3-030-01234-2_29\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8befcd91c24038e5c26df0238d26e2311b21719a\",\"title\":\"A Joint Sequence Fusion Model for Video Question Answering and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8befcd91c24038e5c26df0238d26e2311b21719a\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1491414917\",\"name\":\"Xin Yan\"},{\"authorId\":\"4004957\",\"name\":\"W. Cheng\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.1145/3366710\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"title\":\"Multichannel Attention Refinement for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49162124\",\"name\":\"D. Gao\"},{\"authorId\":\"47023543\",\"name\":\"Xiaoxi He\"},{\"authorId\":\"47230183\",\"name\":\"Zimu Zhou\"},{\"authorId\":\"134897335\",\"name\":\"Yongxin Tong\"},{\"authorId\":\"1818226223\",\"name\":\"Ke Xu\"},{\"authorId\":\"143980067\",\"name\":\"L. Thiele\"}],\"doi\":\"10.1145/3394486.3403058\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a9dd1e7fd1741d66d109232feccd22982f10def5\",\"title\":\"Rethinking Pruning for Accelerating Deep Inference At the Edge\",\"url\":\"https://www.semanticscholar.org/paper/a9dd1e7fd1741d66d109232feccd22982f10def5\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"26900125\",\"name\":\"Jinghao Lin\"},{\"authorId\":\"2440041\",\"name\":\"X. Jiang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3123266.3123364\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2411270f111a160c9289d56132651c896a5738f6\",\"title\":\"Video Question Answering via Hierarchical Dual-Level Attention Network Learning\",\"url\":\"https://www.semanticscholar.org/paper/2411270f111a160c9289d56132651c896a5738f6\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1908.00943\",\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"49745735\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"199c2a410cf4430841907e27d5b7026efd95a6ec\",\"title\":\"Prediction and Description of Near-Future Activities in Video.\",\"url\":\"https://www.semanticscholar.org/paper/199c2a410cf4430841907e27d5b7026efd95a6ec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1643954281\",\"name\":\"Noopur Ballal\"},{\"authorId\":\"39734694\",\"name\":\"S. K. Saritha\"}],\"doi\":\"10.1007/978-981-15-2071-6_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"35ddd3b2d7115dde8b3c514348329ebf157fe6bc\",\"title\":\"A Study of Deep Learning in Text Analytics\",\"url\":\"https://www.semanticscholar.org/paper/35ddd3b2d7115dde8b3c514348329ebf157fe6bc\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1803.07950\",\"authors\":[{\"authorId\":\"4322411\",\"name\":\"L. Li\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/WACV.2019.00042\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"title\":\"End-to-End Video Captioning With Multitask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50218076\",\"name\":\"Z. Wang\"},{\"authorId\":\"51300955\",\"name\":\"Taiping Yao\"},{\"authorId\":\"48056150\",\"name\":\"Huawei Wei\"},{\"authorId\":\"51303943\",\"name\":\"Shanyan Guan\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"}],\"doi\":\"10.1007/978-3-030-00767-6_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf52f10c6f23f99cda01b7c33541f4dab1f7667d\",\"title\":\"Multi-person/Group Interactive Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/bf52f10c6f23f99cda01b7c33541f4dab1f7667d\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":\"1704.02163\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"},{\"authorId\":\"87972149\",\"name\":\"Sergi Soler\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1016/j.jvcir.2017.11.022\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"title\":\"Egocentric video description based on temporally-linked sequences\",\"url\":\"https://www.semanticscholar.org/paper/a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"2003.00636\",\"authors\":[{\"authorId\":\"1471706812\",\"name\":\"Fang Xu\"},{\"authorId\":\"50149990\",\"name\":\"Shijie Lin\"},{\"authorId\":\"49230398\",\"name\":\"Wen Yang\"},{\"authorId\":\"49296869\",\"name\":\"L. Yu\"},{\"authorId\":\"1778526\",\"name\":\"Dengxin Dai\"},{\"authorId\":\"51280933\",\"name\":\"G. Xia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"109a2888e7eed19aafeaa359d56556e72041816d\",\"title\":\"Matching Neuromorphic Events and Color Images via Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/109a2888e7eed19aafeaa359d56556e72041816d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1576151335\",\"name\":\"Trung Ky Nguyen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"36a8af51e773b19dead1ff121acb849532423c62\",\"title\":\"Story Generation from Smart Phone Data : A script approach. (G\\u00e9n\\u00e9ration d'histoires \\u00e0 partir de donn\\u00e9es de t\\u00e9l\\u00e9phone intelligentes : une approche de script)\",\"url\":\"https://www.semanticscholar.org/paper/36a8af51e773b19dead1ff121acb849532423c62\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"83039268\",\"name\":\"Soichiro Oura\"},{\"authorId\":\"2816822\",\"name\":\"T. Matsukawa\"},{\"authorId\":\"1690503\",\"name\":\"E. Suzuki\"}],\"doi\":\"10.1109/IJCNN.2018.8489668\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9e564f2ed4796e32fab8f9b90a52be8d6481a7fa\",\"title\":\"Multimodal Deep Neural Network with Image Sequence Features for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9e564f2ed4796e32fab8f9b90a52be8d6481a7fa\",\"venue\":\"2018 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26928334\",\"name\":\"Shivam Duggal\"},{\"authorId\":\"37059411\",\"name\":\"S. Manik\"},{\"authorId\":\"27059378\",\"name\":\"Mohan Ghai\"}],\"doi\":\"10.1145/3163080.3163108\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f781277a40f14ef83485863ba24e937d76d9bb04\",\"title\":\"Amalgamation of Video Description and Multiple Object Localization using single Deep Learning Model\",\"url\":\"https://www.semanticscholar.org/paper/f781277a40f14ef83485863ba24e937d76d9bb04\",\"venue\":\"ICSPS 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"8058716\",\"name\":\"Noriki Nishida\"},{\"authorId\":\"7412686\",\"name\":\"Raphael Shu\"},{\"authorId\":\"35257737\",\"name\":\"Yo Ehara\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf05e710dae791f82cc639a09dbe5ec66fed2008\",\"title\":\"Generating Video Description using Sequence-to-sequence Model with Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/bf05e710dae791f82cc639a09dbe5ec66fed2008\",\"venue\":\"COLING\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2005957344\",\"name\":\"Soohan Kang\"},{\"authorId\":\"47180565\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/ACCESS.2020.3040405\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ba7d1d521d8a7be0ec9073db31ab305902f7aaa\",\"title\":\"New RNN Activation Technique for Deeper Networks: LSTCM Cells\",\"url\":\"https://www.semanticscholar.org/paper/3ba7d1d521d8a7be0ec9073db31ab305902f7aaa\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152281902\",\"name\":\"S. Wu\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"1700911\",\"name\":\"Zhiqi Shen\"},{\"authorId\":\"145977143\",\"name\":\"Mohan S. Kankanhalli\"},{\"authorId\":\"1699730\",\"name\":\"Anthony K. H. Tung\"}],\"doi\":\"10.1145/3394171.3413589\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"35ba6ed07ef68db187674498e684de7f3e160716\",\"title\":\"Who You Are Decides How You Tell\",\"url\":\"https://www.semanticscholar.org/paper/35ba6ed07ef68db187674498e684de7f3e160716\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/ICCV.2017.20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"62e6b749ed5825739aa906021c5e613803d5cbe2\",\"title\":\"Temporal Tessellation: A Unified Approach for Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/62e6b749ed5825739aa906021c5e613803d5cbe2\",\"venue\":\"ICCV\",\"year\":2017},{\"arxivId\":\"2010.11757\",\"authors\":[{\"authorId\":\"48239920\",\"name\":\"Chun-Fu Chen\"},{\"authorId\":\"1819152\",\"name\":\"R. Panda\"},{\"authorId\":\"40544169\",\"name\":\"K. Ramakrishnan\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"38060482\",\"name\":\"J. M. Cohn\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"},{\"authorId\":\"33421444\",\"name\":\"Quanfu Fan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5c2623a44660ad9fe6cd46710fec6e812a3375a\",\"title\":\"Deep Analysis of CNN-based Spatio-temporal Representations for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f5c2623a44660ad9fe6cd46710fec6e812a3375a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1511.04590\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.5244/C.30.141\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"16aac81ae033f7295d82e5b679400d105170a3e1\",\"title\":\"Oracle Performance for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16aac81ae033f7295d82e5b679400d105170a3e1\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40231554\",\"name\":\"W. Wang\"},{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1007/978-3-319-54184-6_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f6600fa4862a1aeebab707a171c380d095c312a\",\"title\":\"Recurrent Convolutional Face Alignment\",\"url\":\"https://www.semanticscholar.org/paper/5f6600fa4862a1aeebab707a171c380d095c312a\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102523405\",\"name\":\"J. Lee\"},{\"authorId\":\"1390565629\",\"name\":\"Yekang Lee\"},{\"authorId\":\"1911697\",\"name\":\"Sihyeon Seong\"},{\"authorId\":\"97531942\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"153275028\",\"name\":\"Sungjin Kim\"},{\"authorId\":\"1769295\",\"name\":\"Junmo Kim\"}],\"doi\":\"10.1109/ICIP.2019.8803143\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"67a85632e96bbeb0748100d9a570d06e75b0e99b\",\"title\":\"Capturing Long-Range Dependencies in Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/67a85632e96bbeb0748100d9a570d06e75b0e99b\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66534613\",\"name\":\"Sukanya Kudi\"},{\"authorId\":\"3185334\",\"name\":\"A. Namboodiri\"}],\"doi\":\"10.1109/ACPR.2017.141\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d9887fb200cd15bfa8374dca44caf0e0b3e823a3\",\"title\":\"Words Speak for Actions: Using Text to Find Video Highlights\",\"url\":\"https://www.semanticscholar.org/paper/d9887fb200cd15bfa8374dca44caf0e0b3e823a3\",\"venue\":\"2017 4th IAPR Asian Conference on Pattern Recognition (ACPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49969948\",\"name\":\"Zhihui Li\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"2082966\",\"name\":\"L. Yao\"},{\"authorId\":\"2585415\",\"name\":\"Shirui Pan\"},{\"authorId\":\"144062687\",\"name\":\"Zongyuan Ge\"},{\"authorId\":\"46702510\",\"name\":\"Hua-Xiang Zhang\"}],\"doi\":\"10.1145/3394486.3403072\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c2dbeb0edfd63241c5c902f5bded78e42d6d1ef\",\"title\":\"Grounding Visual Concepts for Zero-Shot Event Detection and Event Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c2dbeb0edfd63241c5c902f5bded78e42d6d1ef\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1879112302\",\"name\":\"Xinlong Xiao\"},{\"authorId\":\"3067458\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"51304315\",\"name\":\"Rui Feng\"},{\"authorId\":\"103245682\",\"name\":\"T. Zhang\"},{\"authorId\":\"48869251\",\"name\":\"Shang Gao\"},{\"authorId\":\"1878750882\",\"name\":\"Weiguo Fan\"}],\"doi\":\"10.1109/ICME46284.2020.9102967\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34e98526fc9d4ca177483eb7c858ea0ee8a65a04\",\"title\":\"Video Captioning With Temporal And Region Graph Convolution Network\",\"url\":\"https://www.semanticscholar.org/paper/34e98526fc9d4ca177483eb7c858ea0ee8a65a04\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"145307652\",\"name\":\"Li Dong\"},{\"authorId\":\"3110003\",\"name\":\"Shaohan Huang\"},{\"authorId\":\"145389711\",\"name\":\"K. Xu\"}],\"doi\":\"10.18653/V1/E17-1059\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93f967d32669a34c93fe13b904e572b285e18f05\",\"title\":\"Learning to Generate Product Reviews from Attributes\",\"url\":\"https://www.semanticscholar.org/paper/93f967d32669a34c93fe13b904e572b285e18f05\",\"venue\":\"EACL\",\"year\":2017},{\"arxivId\":\"1505.05914\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"title\":\"A Multi-scale Multiple Instance Video Description Network\",\"url\":\"https://www.semanticscholar.org/paper/08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5781871\",\"name\":\"Jiaqi Su\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"title\":\"Study of Video Captioning Problem\",\"url\":\"https://www.semanticscholar.org/paper/511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3358971\",\"name\":\"Ashwin K. Vijayakumar\"},{\"authorId\":\"37824829\",\"name\":\"M. Cogswell\"},{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"144207643\",\"name\":\"Q. Sun\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1b1e3f7218f1c0f0db56bf2bd9475521454693a1\",\"title\":\"Diverse Beam Search for Improved Description of Complex Scenes\",\"url\":\"https://www.semanticscholar.org/paper/1b1e3f7218f1c0f0db56bf2bd9475521454693a1\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123284890\",\"name\":\"Thomas\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"},{\"authorId\":null,\"name\":\"Dr. Raymond Ptucha\"},{\"authorId\":null,\"name\":\". Andres Kwasinski\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4749597cb3138932fd3e08798b491a5356b755be\",\"title\":\"The Emotional Impact of Audio-Visual Stimuli By , Titus Pallithottathu\",\"url\":\"https://www.semanticscholar.org/paper/4749597cb3138932fd3e08798b491a5356b755be\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38179026\",\"name\":\"Y. Shi\"},{\"authorId\":\"40161651\",\"name\":\"Yonghong Tian\"},{\"authorId\":\"34097174\",\"name\":\"Tiejun Huang\"},{\"authorId\":\"5765799\",\"name\":\"Yaowei Wang\"}],\"doi\":\"10.1109/ICME.2018.8486452\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4d4f52f922b3f28251ae03abcca8c0a369694fd\",\"title\":\"Temporal Attentive Network for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a4d4f52f922b3f28251ae03abcca8c0a369694fd\",\"venue\":\"2018 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"title\":\"Story Understanding through Semantic Analysis and Automatic Alignment of Text and Video\",\"url\":\"https://www.semanticscholar.org/paper/39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1b47776ecc194616d5ae789357ac69b1298e47ae\",\"title\":\"Frames CNN Low-level Encoder ( Bi-LSTM ) High-level Encoder ( LSTM ) Worker Manager Internal Critic Environment segment signal goal state reward action HRL Agent context context\",\"url\":\"https://www.semanticscholar.org/paper/1b47776ecc194616d5ae789357ac69b1298e47ae\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1607.06215\",\"authors\":[{\"authorId\":\"2500189\",\"name\":\"K. Wang\"},{\"authorId\":\"2397961\",\"name\":\"Qiyue Yin\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":\"50425438\",\"name\":\"S. Wu\"},{\"authorId\":null,\"name\":\"Liang Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6721e26e65c6b72dafae74fd1e7a6f2e6023a312\",\"title\":\"A Comprehensive Survey on Cross-modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6721e26e65c6b72dafae74fd1e7a6f2e6023a312\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145800141\",\"name\":\"A. S. Lin\"},{\"authorId\":\"8687492\",\"name\":\"L. Wu\"},{\"authorId\":\"143972253\",\"name\":\"R. Corona\"},{\"authorId\":\"1626005033\",\"name\":\"K. Tai\"},{\"authorId\":\"151485038\",\"name\":\"Qixing Huang\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c112ed7dbfa654a2ca566fa6599dc2d81b5f84b6\",\"title\":\"Generating Animated Videos of Human Activities from Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/c112ed7dbfa654a2ca566fa6599dc2d81b5f84b6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31212599\",\"name\":\"Sneh Rathore\"},{\"authorId\":\"48704050\",\"name\":\"S. Sharma\"},{\"authorId\":\"9107333\",\"name\":\"Lisha Singh\"}],\"doi\":\"10.1007/978-981-13-6772-4_50\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e123f71e1dcc4806c8adafce956c6c0384452a93\",\"title\":\"Drishti\\u2014Artificial Vision\",\"url\":\"https://www.semanticscholar.org/paper/e123f71e1dcc4806c8adafce956c6c0384452a93\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743075\",\"name\":\"I. M\\u0103ndoiu\"},{\"authorId\":\"145538991\",\"name\":\"G. Narasimhan\"},{\"authorId\":\"1753666476\",\"name\":\"Pavel Skums\"},{\"authorId\":\"46909929\",\"name\":\"Xuan Guo\"}],\"doi\":\"10.1007/978-3-030-57821-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a731e4b418a37390a738437df2e01cb0acc7e947\",\"title\":\"Bioinformatics Research and Applications: 16th International Symposium, ISBRA 2020, Moscow, Russia, December 1\\u20134, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/a731e4b418a37390a738437df2e01cb0acc7e947\",\"venue\":\"ISBRA\",\"year\":2020},{\"arxivId\":\"2003.08897\",\"authors\":[{\"authorId\":\"26982950\",\"name\":\"Longteng Guo\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"48386951\",\"name\":\"Xinxin Zhu\"},{\"authorId\":\"49051904\",\"name\":\"P. Yao\"},{\"authorId\":\"116829059\",\"name\":\"Shi-chen Lu\"},{\"authorId\":\"1699900\",\"name\":\"H. Lu\"}],\"doi\":\"10.1109/cvpr42600.2020.01034\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"833560cd68a3e3d1be1bc650756dd6c679798551\",\"title\":\"Normalized and Geometry-Aware Self-Attention Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/833560cd68a3e3d1be1bc650756dd6c679798551\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"9764377\",\"name\":\"Xuanhan Wang\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"46399266\",\"name\":\"Yang Liu\"}],\"doi\":\"10.1016/J.NEUCOM.2018.06.096\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"31b28e96a337dfcf2dbfde104a1ec46f4e755844\",\"title\":\"Fused GRU with semantic-temporal attention for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/31b28e96a337dfcf2dbfde104a1ec46f4e755844\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/978-3-319-68548-9_36\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a\",\"title\":\"Towards Video Captioning with Naming: A Novel Dataset and a Multi-modal Approach\",\"url\":\"https://www.semanticscholar.org/paper/ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a\",\"venue\":\"ICIAP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89041447\",\"name\":\"Xiang Chen\"},{\"authorId\":\"34646933\",\"name\":\"T. V. Nguyen\"},{\"authorId\":\"144506018\",\"name\":\"Z. Shen\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1145/3343031.3350888\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8ef271b4c1018f9135395f70fb6c8a5b3c606305\",\"title\":\"LiveSense: Contextual Advertising in Live Streaming Videos\",\"url\":\"https://www.semanticscholar.org/paper/8ef271b4c1018f9135395f70fb6c8a5b3c606305\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26952440\",\"name\":\"Gil Keren\"},{\"authorId\":\"2169682\",\"name\":\"A. Mousa\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"}],\"doi\":\"10.1145/3107990.3107996\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"736688ae77b4489efdd2d5584a50d967a7f30fdb\",\"title\":\"Deep learning for multisensorial and multimodal interaction\",\"url\":\"https://www.semanticscholar.org/paper/736688ae77b4489efdd2d5584a50d967a7f30fdb\",\"venue\":\"The Handbook of Multimodal-Multisensor Interfaces, Volume 2\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39970828\",\"name\":\"J. Jacob\"},{\"authorId\":\"2457110\",\"name\":\"M. S. Elayidom\"},{\"authorId\":\"3109670\",\"name\":\"V. P. Devassia\"}],\"doi\":\"10.1007/978-3-030-51859-2_52\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be8c2dd6b91217e4eb5ac1e388c951bda8a343f6\",\"title\":\"An Innovative Approach for Aerial Video Surveillance Using Video Content Analysis and Indexing\",\"url\":\"https://www.semanticscholar.org/paper/be8c2dd6b91217e4eb5ac1e388c951bda8a343f6\",\"venue\":\"ICIP 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390818869\",\"name\":\"Jinlei Xu\"},{\"authorId\":\"144546140\",\"name\":\"T. Xu\"},{\"authorId\":\"123432231\",\"name\":\"Xin Tian\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144911521\",\"name\":\"Y. Ji\"}],\"doi\":\"10.1109/IJCNN.2019.8851897\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf7b38dd24c20223e006066be4202d1da700af37\",\"title\":\"Context Gating with Short Temporal Information for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf7b38dd24c20223e006066be4202d1da700af37\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"1710.00421\",\"authors\":[{\"authorId\":\"2664705\",\"name\":\"Y. Li\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"144752689\",\"name\":\"David Edwin Carlson\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a2bbf1f895a1850da3bb6d92b4ffbe61b68145d\",\"title\":\"Video Generation From Text\",\"url\":\"https://www.semanticscholar.org/paper/3a2bbf1f895a1850da3bb6d92b4ffbe61b68145d\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491675970\",\"name\":\"Zeyuan Wang\"},{\"authorId\":\"48422087\",\"name\":\"Josiah Poon\"},{\"authorId\":\"153281721\",\"name\":\"S. Poon\"}],\"doi\":\"10.1109/BIBM47256.2019.8983384\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e9e542f00353995feec9549a281e6683cc78593\",\"title\":\"TCM Translator: A Sequence Generation Approach for Prescribing Herbal Medicines\",\"url\":\"https://www.semanticscholar.org/paper/0e9e542f00353995feec9549a281e6683cc78593\",\"venue\":\"2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)\",\"year\":2019},{\"arxivId\":\"1611.09967\",\"authors\":[{\"authorId\":\"48513733\",\"name\":\"Y. Li\"},{\"authorId\":\"2604251\",\"name\":\"Guosheng Lin\"},{\"authorId\":\"3194022\",\"name\":\"Bohan Zhuang\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2017.600\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3d24b386d003bee176a942c26336dbe8f427aadd\",\"title\":\"Sequential Person Recognition in Photo Albums with a Recurrent Network\",\"url\":\"https://www.semanticscholar.org/paper/3d24b386d003bee176a942c26336dbe8f427aadd\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1907.13487\",\"authors\":[{\"authorId\":\"40457423\",\"name\":\"Y. Liu\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"},{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"title\":\"Use What You Have: Video retrieval using representations from collaborative experts\",\"url\":\"https://www.semanticscholar.org/paper/b16eeb1e975e8e6ea9450c78fd12da05cfd1375f\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2023948950\",\"name\":\"Y. Losieva\"}],\"doi\":\"10.17721/1812-5409.2019/2.10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f57baa5a026fd6934f539de940240231bcc89427\",\"title\":\"Representation of Words in Natural Language Processing: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/f57baa5a026fd6934f539de940240231bcc89427\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3077136.3084144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7cdf9b822b07199415f0e25aa0517c82b1bd499a\",\"title\":\"Seeing Bot\",\"url\":\"https://www.semanticscholar.org/paper/7cdf9b822b07199415f0e25aa0517c82b1bd499a\",\"venue\":\"SIGIR\",\"year\":2017},{\"arxivId\":\"2010.13588\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2e4ca3d95ffb83870661dd66deee143e782f0706\",\"title\":\"Curious Case of Language Generation Evaluation Metrics: A Cautionary Tale\",\"url\":\"https://www.semanticscholar.org/paper/2e4ca3d95ffb83870661dd66deee143e782f0706\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"1506.01911\",\"authors\":[{\"authorId\":\"2660640\",\"name\":\"Lionel Pigou\"},{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"10182287\",\"name\":\"M. V. Herreweghe\"},{\"authorId\":\"2309489\",\"name\":\"J. Dambre\"}],\"doi\":\"10.1007/s11263-016-0957-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eae099ca54c9ee12a5763f6347b91f77df2c7bf4\",\"title\":\"Beyond Temporal Pooling: Recurrence and Temporal Convolutions for Gesture Recognition in Video\",\"url\":\"https://www.semanticscholar.org/paper/eae099ca54c9ee12a5763f6347b91f77df2c7bf4\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"},{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"}],\"doi\":\"10.21437/Interspeech.2016-380\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"title\":\"Generating Natural Video Descriptions via Multimodal Processing\",\"url\":\"https://www.semanticscholar.org/paper/2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"venue\":\"INTERSPEECH\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255213\",\"name\":\"Z. Zhang\"},{\"authorId\":\"38188040\",\"name\":\"Dong Xu\"},{\"authorId\":\"47337540\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"2597292\",\"name\":\"Chuanqi Tan\"}],\"doi\":\"10.1109/TCSVT.2019.2936526\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"title\":\"Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization\",\"url\":\"https://www.semanticscholar.org/paper/b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46758254\",\"name\":\"Virg\\u00ednia P. Campos\"},{\"authorId\":\"152123212\",\"name\":\"Tiago M. U. de Ara\\u00fajo\"},{\"authorId\":\"1521983402\",\"name\":\"Guido L. de Souza Filho\"},{\"authorId\":\"1703957\",\"name\":\"L. Gon\\u00e7alves\"}],\"doi\":\"10.1007/s10209-018-0634-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97b0e43ae18f32378d41c2730268aa7f0f6ba462\",\"title\":\"CineAD: a system for automated audio description script generation for the visually impaired\",\"url\":\"https://www.semanticscholar.org/paper/97b0e43ae18f32378d41c2730268aa7f0f6ba462\",\"venue\":\"Universal Access in the Information Society\",\"year\":2018},{\"arxivId\":\"1611.05592\",\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":null,\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c687986256ce206c93fb78303565bacffb09efe\",\"title\":\"Multimodal Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0c687986256ce206c93fb78303565bacffb09efe\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1703.04096\",\"authors\":[{\"authorId\":\"3431029\",\"name\":\"Y. Dong\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"145254043\",\"name\":\"J. Zhu\"},{\"authorId\":\"49846744\",\"name\":\"Bo Zhang\"}],\"doi\":\"10.1109/CVPR.2017.110\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ebfca6a48dde396e2274caa9f15389fdbf08fd12\",\"title\":\"Improving Interpretability of Deep Neural Networks with Semantic Information\",\"url\":\"https://www.semanticscholar.org/paper/ebfca6a48dde396e2274caa9f15389fdbf08fd12\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1900432330\",\"name\":\"Ke Ning\"},{\"authorId\":\"1811534\",\"name\":\"M. Cai\"},{\"authorId\":\"145982709\",\"name\":\"Di Xie\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TMM.2019.2957854\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f9f2837b150eab8a1aa2e74a1a528700e1e6d8b8\",\"title\":\"An Attentive Sequence to Sequence Translator for Localizing Video Clips by Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/f9f2837b150eab8a1aa2e74a1a528700e1e6d8b8\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ICIP.2019.8803785\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"da0cce8d791ca90b01696c6ef0de96c7904dd8cf\",\"title\":\"A Novel Attribute Selection Mechanism for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/da0cce8d791ca90b01696c6ef0de96c7904dd8cf\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"83979477\",\"name\":\"Suman Kalyan Adari\"},{\"authorId\":\"47238599\",\"name\":\"W. Garcia\"},{\"authorId\":\"1784947\",\"name\":\"K. Butler\"}],\"doi\":\"10.1109/DSN-W.2019.00012\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5ebc08cbe3daf87afc0cd11a762f6b7c05f9b20b\",\"title\":\"Adversarial Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5ebc08cbe3daf87afc0cd11a762f6b7c05f9b20b\",\"venue\":\"2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40909443\",\"name\":\"Y. Hu\"},{\"authorId\":\"50655168\",\"name\":\"MingQi Lu\"},{\"authorId\":\"47415615\",\"name\":\"Chao Xie\"},{\"authorId\":\"153010660\",\"name\":\"Xiao-Bo Lu\"}],\"doi\":\"10.1109/TCSVT.2019.2958188\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fed50b6e4411a15bdec1dc85680fc0ddb60e107b\",\"title\":\"Driver Drowsiness Recognition via 3D Conditional GAN and Two-Level Attention Bi-LSTM\",\"url\":\"https://www.semanticscholar.org/paper/fed50b6e4411a15bdec1dc85680fc0ddb60e107b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741830\",\"name\":\"Haibo Li\"},{\"authorId\":\"48204604\",\"name\":\"Liqing Gao\"},{\"authorId\":\"81631713\",\"name\":\"Ruize Han\"},{\"authorId\":\"39124212\",\"name\":\"L. Wan\"},{\"authorId\":\"1409949029\",\"name\":\"W. Feng\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054316\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1cd0738c83bdd4f3b489b30b099d2b85bb9c02d2\",\"title\":\"Key Action and Joint CTC-Attention based Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1cd0738c83bdd4f3b489b30b099d2b85bb9c02d2\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1961291164\",\"name\":\"Marco Monforte\"},{\"authorId\":\"2132505\",\"name\":\"Ander Arriandiaga\"},{\"authorId\":\"2763635\",\"name\":\"A. Glover\"},{\"authorId\":\"1897771\",\"name\":\"C. Bartolozzi\"}],\"doi\":\"10.1109/ICRA40945.2020.9197373\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd26ab0d3cc323c0995a80a921a7fd092be074d0\",\"title\":\"Where and When: Event-Based Spatiotemporal Trajectory Prediction from the iCub\\u2019s Point-Of-View\",\"url\":\"https://www.semanticscholar.org/paper/bd26ab0d3cc323c0995a80a921a7fd092be074d0\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150337817\",\"name\":\"Weike Jin\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"67144160\",\"name\":\"Mao Gu\"},{\"authorId\":\"9919436\",\"name\":\"J. Yu\"},{\"authorId\":\"145974111\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3331184.3331240\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8954b3f74373e953b4449efca71e5ec6be46fb6\",\"title\":\"Video Dialog via Multi-Grained Convolutional Self-Attention Context Networks\",\"url\":\"https://www.semanticscholar.org/paper/f8954b3f74373e953b4449efca71e5ec6be46fb6\",\"venue\":\"SIGIR\",\"year\":2019},{\"arxivId\":\"1807.03658\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"}],\"doi\":\"10.1016/j.neucom.2020.08.035\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"title\":\"Video Captioning with Boundary-aware Hierarchical Language Decoding and Joint Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1809.10267\",\"authors\":[{\"authorId\":\"48935207\",\"name\":\"C. Zhang\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"38916638\",\"name\":\"Dheeraj Peri\"},{\"authorId\":\"34679323\",\"name\":\"A. Loui\"},{\"authorId\":\"2879097\",\"name\":\"C. Salvaggio\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/GlobalSIP.2017.8309051\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"080ea4c468a982c73a7741abe59da5255c7d2c38\",\"title\":\"Semantic sentence embeddings for paraphrasing and text summarization\",\"url\":\"https://www.semanticscholar.org/paper/080ea4c468a982c73a7741abe59da5255c7d2c38\",\"venue\":\"2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)\",\"year\":2017},{\"arxivId\":\"1810.06327\",\"authors\":[{\"authorId\":\"2368143\",\"name\":\"Jinsong Zhang\"},{\"authorId\":\"1689681\",\"name\":\"Rodrigo Verschae\"},{\"authorId\":\"1778111\",\"name\":\"S. Nobuhara\"},{\"authorId\":\"144430305\",\"name\":\"Jean-Fran\\u00e7ois Lalonde\"}],\"doi\":\"10.1016/J.SOLENER.2018.10.024\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1188c925d90e93a205c5fc15d11fb2ae02660f2e\",\"title\":\"Deep Photovoltaic Nowcasting\",\"url\":\"https://www.semanticscholar.org/paper/1188c925d90e93a205c5fc15d11fb2ae02660f2e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"},{\"authorId\":\"116409536\",\"name\":\"A. Gray\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1401154472\",\"name\":\"Emily Tucker Prud'hommeaux\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/WACV.2017.115\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c9aea006e80cf215b72693860c3234b61006c911\",\"title\":\"Semantic Text Summarization of Long Videos\",\"url\":\"https://www.semanticscholar.org/paper/c9aea006e80cf215b72693860c3234b61006c911\",\"venue\":\"2017 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2017},{\"arxivId\":\"2012.02639\",\"authors\":[{\"authorId\":\"2031911039\",\"name\":\"Edward Fish\"},{\"authorId\":\"2014512338\",\"name\":\"Andrew Gilbert\"},{\"authorId\":\"47371758\",\"name\":\"Jon Weinbren\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ffe1f5c2d579cc63a785023f4df11207504fbc1c\",\"title\":\"Rethinking movie genre classification with fine-grained semantic clustering\",\"url\":\"https://www.semanticscholar.org/paper/ffe1f5c2d579cc63a785023f4df11207504fbc1c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.01067\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"title\":\"Video Captioning Using Weak Annotation\",\"url\":\"https://www.semanticscholar.org/paper/aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1604.06573\",\"authors\":[{\"authorId\":\"2322150\",\"name\":\"Christoph Feichtenhofer\"},{\"authorId\":\"1718587\",\"name\":\"A. Pinz\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.1109/CVPR.2016.213\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d9aced120e530484609164c836da64548693484\",\"title\":\"Convolutional Two-Stream Network Fusion for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9d9aced120e530484609164c836da64548693484\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1604.03390\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1007/978-3-319-44781-0_1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"799271daced99bd88b3a3e15921d5e31d9ea8323\",\"title\":\"Video Description Using Bidirectional Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/799271daced99bd88b3a3e15921d5e31d9ea8323\",\"venue\":\"ICANN\",\"year\":2016},{\"arxivId\":\"1611.02261\",\"authors\":[{\"authorId\":\"2915023\",\"name\":\"Rasool Fakoor\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1738740\",\"name\":\"S. B. Kang\"},{\"authorId\":\"143967473\",\"name\":\"Pushmeet Kohli\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"title\":\"Memory-augmented Attention Modelling for Videos\",\"url\":\"https://www.semanticscholar.org/paper/249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1708.09666\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"}],\"doi\":\"10.1145/3078971.3079000\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"title\":\"Generating Video Descriptions with Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50024168\",\"name\":\"Yitong Li\"},{\"authorId\":\"5477477\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":null,\"name\":\"Dinghan Shen\"},{\"authorId\":null,\"name\":\"David Carlson\"},{\"authorId\":\"145006560\",\"name\":\"Lawrence Carin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13cdaa567cee45a83bd47cad047c591e04336d0c\",\"title\":\"\\u223c N ( 0 , 1 ) Video Generator Video Discriminator Real ? Fake ?\",\"url\":\"https://www.semanticscholar.org/paper/13cdaa567cee45a83bd47cad047c591e04336d0c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46964024\",\"name\":\"Adita Kulkarni\"},{\"authorId\":\"49986654\",\"name\":\"A. Seetharam\"},{\"authorId\":\"152459641\",\"name\":\"Arti Ramesh\"},{\"authorId\":\"145021872\",\"name\":\"J. Herath\"}],\"doi\":\"10.1109/TVT.2019.2949954\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70b51dfb91c65343b42fd6b1e0d104ebf0d943ef\",\"title\":\"DeepChannel: Wireless Channel Quality Prediction Using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/70b51dfb91c65343b42fd6b1e0d104ebf0d943ef\",\"venue\":\"IEEE Transactions on Vehicular Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47666014\",\"name\":\"Hongjun Chen\"},{\"authorId\":\"13032169\",\"name\":\"H. Li\"},{\"authorId\":\"9512265\",\"name\":\"Xueqin Wu\"}],\"doi\":\"10.1145/3380625.3380669\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a3b4b243afc2616eb2c3e24ec0abad76c2a80f7\",\"title\":\"Research on Feature Extraction and Multimodal Fusion of Video Caption Based on Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1a3b4b243afc2616eb2c3e24ec0abad76c2a80f7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TCSVT.2018.2867286\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"title\":\"Dual-Stream Recurrent Neural Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":\"1610.01206\",\"authors\":[{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"2909406\",\"name\":\"Ming Yang\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"}],\"doi\":\"10.1109/TKDE.2018.2872063\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fa53bb7432cad104622f45f5ede17a2cb4642b0e\",\"title\":\"Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods\",\"url\":\"https://www.semanticscholar.org/paper/fa53bb7432cad104622f45f5ede17a2cb4642b0e\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1778215\",\"name\":\"Swagath Venkataramani\"},{\"authorId\":\"49315336\",\"name\":\"Ashish Ranjan\"},{\"authorId\":\"17821157\",\"name\":\"Subarno Banerjee\"},{\"authorId\":\"38858451\",\"name\":\"D. Das\"},{\"authorId\":\"1777558\",\"name\":\"Sasikanth Avancha\"},{\"authorId\":\"1688089\",\"name\":\"A. Jagannathan\"},{\"authorId\":\"17854422\",\"name\":\"Ajaya Durg\"},{\"authorId\":\"91565690\",\"name\":\"D. Nagaraj\"},{\"authorId\":\"40279588\",\"name\":\"B. Kaul\"},{\"authorId\":\"49380397\",\"name\":\"P. Dubey\"},{\"authorId\":\"145291370\",\"name\":\"A. Raghunathan\"}],\"doi\":\"10.1145/3079856.3080244\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dbef80d8c7784618159a2e281c6eacdb16716dca\",\"title\":\"SCALEDEEP: A scalable compute architecture for learning and evaluating deep networks\",\"url\":\"https://www.semanticscholar.org/paper/dbef80d8c7784618159a2e281c6eacdb16716dca\",\"venue\":\"2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"12257085\",\"name\":\"Pang-Chi Huang\"},{\"authorId\":\"3596543\",\"name\":\"Chi-Yu Yang\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ASRU.2017.8268961\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"title\":\"Seeing and hearing too: Audio representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2569534\",\"name\":\"J. Kim\"},{\"authorId\":\"144887521\",\"name\":\"Suhong Moon\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":\"10.1109/cvpr42600.2020.00968\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b7e34296ea9a74193b906b37c5bb6d939ba68017\",\"title\":\"Advisable Learning for Self-Driving Vehicles by Internalizing Observation-to-Action Rules\",\"url\":\"https://www.semanticscholar.org/paper/b7e34296ea9a74193b906b37c5bb6d939ba68017\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1908.08990\",\"authors\":[{\"authorId\":\"1720818\",\"name\":\"Sebastian Agethen\"},{\"authorId\":\"1716836\",\"name\":\"W. Hsu\"}],\"doi\":\"10.1109/TMM.2019.2932564\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1f4e2bdd9bf848e967b4e68203a914475d306a58\",\"title\":\"Deep Multi-Kernel Convolutional LSTM Networks and an Attention-Based Mechanism for Videos\",\"url\":\"https://www.semanticscholar.org/paper/1f4e2bdd9bf848e967b4e68203a914475d306a58\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2633680\",\"name\":\"Chaochun Liu\"},{\"authorId\":\"11121990\",\"name\":\"Huan Sun\"},{\"authorId\":\"145585757\",\"name\":\"Nan Du\"},{\"authorId\":\"144734908\",\"name\":\"Shulong Tan\"},{\"authorId\":\"2940124\",\"name\":\"Hongliang Fei\"},{\"authorId\":\"144934701\",\"name\":\"W. Fan\"},{\"authorId\":\"49876187\",\"name\":\"T. Yang\"},{\"authorId\":\"46477167\",\"name\":\"H. Wu\"},{\"authorId\":\"2694924\",\"name\":\"Y. Li\"},{\"authorId\":\"2418496\",\"name\":\"Chenwei Zhang\"}],\"doi\":\"10.1109/ICDM.2016.0036\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d081aa9683edbc4d79e4022b11c22cfa7de39666\",\"title\":\"Augmented LSTM Framework to Construct Medical Self-Diagnosis Android\",\"url\":\"https://www.semanticscholar.org/paper/d081aa9683edbc4d79e4022b11c22cfa7de39666\",\"venue\":\"2016 IEEE 16th International Conference on Data Mining (ICDM)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/LRA.2018.2793345\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"title\":\"Full-GRU Natural Language Video Description for Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701451\",\"name\":\"Ramesh Nallapati\"},{\"authorId\":\"144028698\",\"name\":\"B. Xiang\"},{\"authorId\":\"145218984\",\"name\":\"Bowen Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"033bc4febf590f6e011e9b0f497cadfe6a4c292d\",\"title\":\"S EQUENCE-TO-SEQUENCE RNN S FOR T EXT S UMMARIZATION\",\"url\":\"https://www.semanticscholar.org/paper/033bc4febf590f6e011e9b0f497cadfe6a4c292d\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34876449\",\"name\":\"E. Aksoy\"},{\"authorId\":\"145942066\",\"name\":\"E. Ovchinnikova\"},{\"authorId\":\"9738883\",\"name\":\"Adil Orhan\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1722677\",\"name\":\"T. Asfour\"}],\"doi\":\"10.1109/LRA.2017.2669363\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"36894bf4d64c247baa96b81f0b83975e246c07e2\",\"title\":\"Unsupervised Linking of Visual Features to Textual Descriptions in Long Manipulation Activities\",\"url\":\"https://www.semanticscholar.org/paper/36894bf4d64c247baa96b81f0b83975e246c07e2\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2017},{\"arxivId\":\"1810.03989\",\"authors\":[{\"authorId\":\"3213196\",\"name\":\"Zhongwei Xie\"},{\"authorId\":\"47681623\",\"name\":\"Lin Li\"},{\"authorId\":\"144905578\",\"name\":\"Xian Zhong\"},{\"authorId\":\"50442192\",\"name\":\"L. Zhong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7d840cf207802918e5b09af0a650b8f38f824a8\",\"title\":\"Image-to-Video Person Re-Identification by Reusing Cross-modal Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/e7d840cf207802918e5b09af0a650b8f38f824a8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48056150\",\"name\":\"Huawei Wei\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"3460423\",\"name\":\"Yichao Yan\"},{\"authorId\":\"41030694\",\"name\":\"Huanyu Yu\"},{\"authorId\":\"1795291\",\"name\":\"X. Yang\"},{\"authorId\":\"145486463\",\"name\":\"C. Yao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6ec09bad57cc81a71ef7596f57e94ee13b380ae3\",\"title\":\"Video Summarization via Semantic Attended Networks\",\"url\":\"https://www.semanticscholar.org/paper/6ec09bad57cc81a71ef7596f57e94ee13b380ae3\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40685026\",\"name\":\"Duona Zhang\"},{\"authorId\":\"2911928\",\"name\":\"Wenrui Ding\"},{\"authorId\":\"1740430\",\"name\":\"Baochang Zhang\"},{\"authorId\":\"3471524\",\"name\":\"Chunyu Xie\"},{\"authorId\":\"49046516\",\"name\":\"Chunhui Liu\"},{\"authorId\":\"1783847\",\"name\":\"Jungong Han\"},{\"authorId\":\"46381969\",\"name\":\"Hongguang Li\"}],\"doi\":\"10.20944/PREPRINTS201801.0097.V1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c67f2377783352fd0152b52ea39d86997d6b9572\",\"title\":\"Heterogeneous Deep Model Fusion for Automatic Modulation Classification\",\"url\":\"https://www.semanticscholar.org/paper/c67f2377783352fd0152b52ea39d86997d6b9572\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1903.09761\",\"authors\":[{\"authorId\":\"145062693\",\"name\":\"Anh Nguyen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"af4df89ad28580d98113fa6a816195137f7d1a1d\",\"title\":\"Scene Understanding for Autonomous Manipulation with Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/af4df89ad28580d98113fa6a816195137f7d1a1d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49299019\",\"name\":\"Junnan Li\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47521917\",\"name\":\"Q. Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e93c977025e2829f852fc8c1e8f9547c3588dbf0\",\"title\":\"vv 1 camping tent food fire Residual BRNN Input Video Visual Encoder ( CNN ) Video Encoder Sentence Encoder Word 2 Vecs Sentence Semantic Embedding vv 2 vv 3 vvNN \\u2212 1 vvNN vv Video Semantic Embedding xx\",\"url\":\"https://www.semanticscholar.org/paper/e93c977025e2829f852fc8c1e8f9547c3588dbf0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TMM.2019.2896515\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"title\":\"Generating Video Descriptions With Latent Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32860700\",\"name\":\"P. Nguyen\"},{\"authorId\":\"145138436\",\"name\":\"Qing Li\"},{\"authorId\":\"3493929\",\"name\":\"Zhi-Qi Cheng\"},{\"authorId\":\"40501192\",\"name\":\"Yi-Jie Lu\"},{\"authorId\":\"71777847\",\"name\":\"H. Zhang\"},{\"authorId\":\"1772198\",\"name\":\"X. Wu\"},{\"authorId\":\"152650698\",\"name\":\"Chong-Wah Ngo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bf2e9704e49b9ca57794bb54e29997f3bebbe6a\",\"title\":\"VIREO @ TRECVID 2017: Video-to-Text, Ad-hoc Video Search, and Video hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/3bf2e9704e49b9ca57794bb54e29997f3bebbe6a\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46725125\",\"name\":\"Marc ten Bosch\"},{\"authorId\":\"51247661\",\"name\":\"Christopher S. Gifford\"},{\"authorId\":\"29961709\",\"name\":\"Agata Ciesielski\"},{\"authorId\":\"71918324\",\"name\":\"Scott Almes\"},{\"authorId\":\"152601933\",\"name\":\"Rachel Ellison\"},{\"authorId\":\"50005563\",\"name\":\"Gordon Christie\"}],\"doi\":\"10.1117/12.2518163\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03413947c70d65de0f5352f9f7911c2d01409f81\",\"title\":\"Captioning of full motion video from unmanned aerial platforms\",\"url\":\"https://www.semanticscholar.org/paper/03413947c70d65de0f5352f9f7911c2d01409f81\",\"venue\":\"Defense + Commercial Sensing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1380729056\",\"name\":\"Anuj Rathore\"},{\"authorId\":\"9361887\",\"name\":\"Pravin Nagar\"},{\"authorId\":\"38772597\",\"name\":\"C. Arora\"},{\"authorId\":\"1380374163\",\"name\":\"C.V. Jawahar\"}],\"doi\":\"10.1145/3343031.3350880\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b844e3eb550758d907c44729279760e4f981a6a\",\"title\":\"Generating 1 Minute Summaries of Day Long Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/4b844e3eb550758d907c44729279760e4f981a6a\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1801.10111\",\"authors\":[{\"authorId\":\"47513257\",\"name\":\"Jie Huang\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"46324995\",\"name\":\"Q. Zhang\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"1683483\",\"name\":\"W. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"81a1660d57738347a04b22920571bc394dd97a9e\",\"title\":\"Video-based Sign Language Recognition without Temporal Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/81a1660d57738347a04b22920571bc394dd97a9e\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1811.02745\",\"authors\":[{\"authorId\":\"2904100\",\"name\":\"Z. Han\"},{\"authorId\":\"41066029\",\"name\":\"Mingyang Shang\"},{\"authorId\":\"48631765\",\"name\":\"Xiyang Wang\"},{\"authorId\":\"46399275\",\"name\":\"Yu-Shen Liu\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"}],\"doi\":\"10.1609/aaai.v33i01.3301126\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f3b03bc2f5a049fbcd4c7eab74066b562a6ab70\",\"title\":\"Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by Joint Reconstruction and Prediction of View and Word Sequences\",\"url\":\"https://www.semanticscholar.org/paper/1f3b03bc2f5a049fbcd4c7eab74066b562a6ab70\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1657600534\",\"name\":\"Adhiraj Chattopadhyay\"},{\"authorId\":\"36709228\",\"name\":\"M. Dey\"}],\"doi\":\"10.1007/978-981-15-2188-1_23\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c95868877af69a1981214d7e58890ff3925cef95\",\"title\":\"Design and Implementation of an Automatic Summarizer Using Extractive and Abstractive Methods\",\"url\":\"https://www.semanticscholar.org/paper/c95868877af69a1981214d7e58890ff3925cef95\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"88130532\",\"name\":\"Q. Liu\"},{\"authorId\":\"49292863\",\"name\":\"Zihe Liu\"},{\"authorId\":\"47362195\",\"name\":\"Hongming Zhu\"},{\"authorId\":\"33908381\",\"name\":\"Hongfei Fan\"},{\"authorId\":\"2525530\",\"name\":\"B. Du\"},{\"authorId\":\"50059673\",\"name\":\"Yu Qian\"}],\"doi\":\"10.1109/MSR.2019.00056\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03f2e9c9fd3f85cafdbdb2a4ef812a562caf6ef1\",\"title\":\"Generating Commit Messages from Diffs using Pointer-Generator Network\",\"url\":\"https://www.semanticscholar.org/paper/03f2e9c9fd3f85cafdbdb2a4ef812a562caf6ef1\",\"venue\":\"2019 IEEE/ACM 16th International Conference on Mining Software Repositories (MSR)\",\"year\":2019},{\"arxivId\":\"1809.10312\",\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"48935207\",\"name\":\"C. Zhang\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"143805478\",\"name\":\"Dheeraj Kumar Peri\"},{\"authorId\":\"29980978\",\"name\":\"Ameya Shringi\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/AIPR.2017.8457957\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3e4d3c54e40161e7f355d8bd24ab3f8556ec9e14\",\"title\":\"Vector Learning for Cross Domain Representations\",\"url\":\"https://www.semanticscholar.org/paper/3e4d3c54e40161e7f355d8bd24ab3f8556ec9e14\",\"venue\":\"2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143715692\",\"name\":\"X. Hao\"},{\"authorId\":\"46468475\",\"name\":\"F. Zhou\"},{\"authorId\":\"33899331\",\"name\":\"Xiaoyong Li\"}],\"doi\":\"10.1109/ITNEC48623.2020.9084781\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"653de101370307afc2eba27d4e4c574441eb06da\",\"title\":\"Scene-Edge GRU for Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/653de101370307afc2eba27d4e4c574441eb06da\",\"venue\":\"2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39925987\",\"name\":\"V. Upadhya\"},{\"authorId\":\"49926328\",\"name\":\"A. Chatterjee\"},{\"authorId\":\"2988091\",\"name\":\"A. Prathosh\"},{\"authorId\":\"40344652\",\"name\":\"P. Praveena\"}],\"doi\":\"10.1109/BIBE.2016.37\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"364b743942ed0e35b87c2376f3ca5a8a9b29c956\",\"title\":\"Respiration Monitoring through Thoraco-Abdominal Video with an LSTM\",\"url\":\"https://www.semanticscholar.org/paper/364b743942ed0e35b87c2376f3ca5a8a9b29c956\",\"venue\":\"2016 IEEE 16th International Conference on Bioinformatics and Bioengineering (BIBE)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2849794\",\"name\":\"P. Klosowski\"}],\"doi\":\"10.23919/SPA.2019.8936782\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"65243e9fdeec7c3f8cc81c602a2eee8dea8b50ae\",\"title\":\"Polish Language Modelling Based on Deep Learning Methods and Techniques\",\"url\":\"https://www.semanticscholar.org/paper/65243e9fdeec7c3f8cc81c602a2eee8dea8b50ae\",\"venue\":\"2019 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)\",\"year\":2019},{\"arxivId\":\"1808.04450\",\"authors\":[{\"authorId\":\"30123167\",\"name\":\"Yecheng Lyu\"},{\"authorId\":\"8858861\",\"name\":\"X. Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f9cacb5fc126f87dbf53dd547a9fb9f58ded557\",\"title\":\"RoadNet-v2: A 10 ms Road Segmentation Using Spatial Sequence Layer\",\"url\":\"https://www.semanticscholar.org/paper/7f9cacb5fc126f87dbf53dd547a9fb9f58ded557\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1992928171\",\"name\":\"Mingxing Wang\"}],\"doi\":\"10.1109/CCET50901.2020.9213129\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"52673af36ec05e0e70c708258a984889d09f8f4b\",\"title\":\"Video Description with GAN\",\"url\":\"https://www.semanticscholar.org/paper/52673af36ec05e0e70c708258a984889d09f8f4b\",\"venue\":\"2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81842808\",\"name\":\"Tomer Golany\"},{\"authorId\":\"2357385\",\"name\":\"Kira Radinsky\"}],\"doi\":\"10.1609/AAAI.V33I01.3301557\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"02ffc0a4ffa53c754ed8ddadb66a07e7a80ff246\",\"title\":\"PGANs: Personalized Generative Adversarial Networks for ECG Synthesis to Improve Patient-Specific Deep ECG Classification\",\"url\":\"https://www.semanticscholar.org/paper/02ffc0a4ffa53c754ed8ddadb66a07e7a80ff246\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1612.06950\",\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aba537944ace733bbab2290cf2814cf7f4e4e275\",\"title\":\"Temporal Tessellation for Video Annotation and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/aba537944ace733bbab2290cf2814cf7f4e4e275\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1906.07689\",\"authors\":[{\"authorId\":\"47300698\",\"name\":\"Hao Tan\"},{\"authorId\":\"2462276\",\"name\":\"Franck Dernoncourt\"},{\"authorId\":\"145527705\",\"name\":\"Zhe Lin\"},{\"authorId\":\"145262461\",\"name\":\"Trung Bui\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P19-1182\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157\",\"title\":\"Expressing Visual Relationships via Language\",\"url\":\"https://www.semanticscholar.org/paper/4eb2e2b9c22cb1da8561044ca0dc8fc0b13e3157\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"1890615\",\"name\":\"Y. Huo\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":\"10.1145/2964284.2984064\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f28cd3ad13fe0b3e94d7c49886648fb164601f20\",\"title\":\"Early Embedding and Late Reranking for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f28cd3ad13fe0b3e94d7c49886648fb164601f20\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1418234543\",\"name\":\"Antonio Hern\\u00e1ndez-Blanco\"},{\"authorId\":\"1418204640\",\"name\":\"Boris Herrera-Flores\"},{\"authorId\":\"120157495\",\"name\":\"David Tom\\u00e1s\"},{\"authorId\":\"1399088129\",\"name\":\"Borja Navarro-Colorado\"}],\"doi\":\"10.1155/2019/1306039\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"86e9183f68268ca4223255e523c63efdf1bdb65c\",\"title\":\"A Systematic Review of Deep Learning Approaches to Educational Data Mining\",\"url\":\"https://www.semanticscholar.org/paper/86e9183f68268ca4223255e523c63efdf1bdb65c\",\"venue\":\"Complex.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740650\",\"name\":\"Jian Jhen Chen\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"2838253\",\"name\":\"C. He\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3132847.3132922\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3085671f6232aac4492ad861d09334b8f3a7e2a7\",\"title\":\"Movie Fill in the Blank with Adaptive Temporal Attention and Description Update\",\"url\":\"https://www.semanticscholar.org/paper/3085671f6232aac4492ad861d09334b8f3a7e2a7\",\"venue\":\"CIKM\",\"year\":2017},{\"arxivId\":\"1603.08486\",\"authors\":[{\"authorId\":\"1797022\",\"name\":\"Hoo-Chang Shin\"},{\"authorId\":\"1742509\",\"name\":\"Kirk Roberts\"},{\"authorId\":\"50706692\",\"name\":\"Le Lu\"},{\"authorId\":\"1398175407\",\"name\":\"Dina Demner-Fushman\"},{\"authorId\":\"150167064\",\"name\":\"Jianhua Yao\"},{\"authorId\":\"144838131\",\"name\":\"R. Summers\"}],\"doi\":\"10.1109/CVPR.2016.274\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0460d3497490fa8332c5ff2ecdab88fb7dff4755\",\"title\":\"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/0460d3497490fa8332c5ff2ecdab88fb7dff4755\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47860328\",\"name\":\"Shagun Uppal\"},{\"authorId\":\"73368394\",\"name\":\"Sarthak Bhagat\"},{\"authorId\":\"8223433\",\"name\":\"Devamanyu Hazarika\"},{\"authorId\":\"1999177921\",\"name\":\"Navonil Majumdar\"},{\"authorId\":\"1746416\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"153015119\",\"name\":\"R. Zimmermann\"},{\"authorId\":\"144802290\",\"name\":\"Amir Zadeh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"title\":\"Emerging Trends of Multimodal Research in Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/0f23b548ff7b3517f028164a30c8bf186f11a1a6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.06775\",\"authors\":[{\"authorId\":\"3083814\",\"name\":\"J. Mohan\"},{\"authorId\":\"3078275\",\"name\":\"Amar Phanishayee\"},{\"authorId\":\"2123949\",\"name\":\"A. Raniwala\"},{\"authorId\":\"2002462\",\"name\":\"Vijay Chidambaram\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6ec443d94a48eb4d0d3cb41b0dc1b13aa4e3a2b7\",\"title\":\"Analyzing and Mitigating Data Stalls in DNN Training\",\"url\":\"https://www.semanticscholar.org/paper/6ec443d94a48eb4d0d3cb41b0dc1b13aa4e3a2b7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47056886\",\"name\":\"Xiangpeng Li\"},{\"authorId\":\"30076791\",\"name\":\"Zhilong Zhou\"},{\"authorId\":\"35153304\",\"name\":\"Lijiang Chen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0531-z\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"title\":\"Residual attention-based LSTM for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":\"1710.00290\",\"authors\":[{\"authorId\":\"145062693\",\"name\":\"Anh Nguyen\"},{\"authorId\":\"1704541\",\"name\":\"Dimitrios Kanoulas\"},{\"authorId\":\"48421415\",\"name\":\"L. Muratore\"},{\"authorId\":\"1745158\",\"name\":\"D. Caldwell\"},{\"authorId\":\"145887349\",\"name\":\"N. Tsagarakis\"}],\"doi\":\"10.1109/ICRA.2018.8460857\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f12742105fd7e678d8543510248883ffbf89a631\",\"title\":\"Translating Videos to Commands for Robotic Manipulation with Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f12742105fd7e678d8543510248883ffbf89a631\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49251978\",\"name\":\"J. Chen\"},{\"authorId\":\"41079034\",\"name\":\"Hong-Yang Chao\"}],\"doi\":\"10.1145/3394171.3416291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"598ad06c164043c45c952dbde37e0c75991e66aa\",\"title\":\"VideoTRM: Pre-training for Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/598ad06c164043c45c952dbde37e0c75991e66aa\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2006.03315\",\"authors\":[{\"authorId\":\"1387548078\",\"name\":\"K. Lin\"},{\"authorId\":\"1738276592\",\"name\":\"Zhuoxin Gan\"},{\"authorId\":\"46659782\",\"name\":\"L. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aea0c2fdcb618d3b0fdea17a5fc5b068b80a0ec3\",\"title\":\"Multi-modal Feature Fusion with Feature Attention for VATEX Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/aea0c2fdcb618d3b0fdea17a5fc5b068b80a0ec3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.02649\",\"authors\":[{\"authorId\":\"9202187\",\"name\":\"Chaoqun Duan\"},{\"authorId\":\"145500846\",\"name\":\"Lei Cui\"},{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2675365\",\"name\":\"Conghui Zhu\"},{\"authorId\":\"1856039\",\"name\":\"T. Zhao\"}],\"doi\":\"10.3233/FAIA200320\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"7bf0bf048b88c2bb5a7d53e34ec912907295be81\",\"title\":\"Multimodal Matching Transformer for Live Commenting\",\"url\":\"https://www.semanticscholar.org/paper/7bf0bf048b88c2bb5a7d53e34ec912907295be81\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3456884\",\"name\":\"T. Thomas\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"43c5be1f64e0135fb3d6e43a9c33caaaa58f7213\",\"title\":\"The Emotional Impact of Audio - Visual Stimuli\",\"url\":\"https://www.semanticscholar.org/paper/43c5be1f64e0135fb3d6e43a9c33caaaa58f7213\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1911.09989\",\"authors\":[{\"authorId\":\"1429191721\",\"name\":\"Menatallh Hammad\"},{\"authorId\":\"1429191719\",\"name\":\"May Hammad\"},{\"authorId\":\"31358369\",\"name\":\"M. ElShenawy\"}],\"doi\":\"10.1007/978-3-030-59830-3_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"title\":\"Characterizing the impact of using features extracted from pre-trained models on the quality of video captioning sequence-to-sequence models\",\"url\":\"https://www.semanticscholar.org/paper/86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"venue\":\"ICPRAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"144118452\",\"name\":\"Pengfei Liu\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3468740e4a9fc72a269f4f0ca8470ccd60925f92\",\"title\":\"2019 Formatting Instructions for Authors Using LaTeX\",\"url\":\"https://www.semanticscholar.org/paper/3468740e4a9fc72a269f4f0ca8470ccd60925f92\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31468750\",\"name\":\"J. Ye\"},{\"authorId\":\"145834008\",\"name\":\"Le Dong\"},{\"authorId\":\"49191636\",\"name\":\"Wenpu Dong\"},{\"authorId\":\"145901246\",\"name\":\"Ning Feng\"},{\"authorId\":\"145002061\",\"name\":\"N. Zhang\"}],\"doi\":\"10.1145/3321408.3322623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ef5424c6cb47e17b9aeba447289af8aa1705c339\",\"title\":\"Policy multi-region integration for video description\",\"url\":\"https://www.semanticscholar.org/paper/ef5424c6cb47e17b9aeba447289af8aa1705c339\",\"venue\":\"ACM TUR-C\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"LIU Hao-Yu\"},{\"authorId\":null,\"name\":\"HE Shi-Bo\"},{\"authorId\":null,\"name\":\"CHEN Ji-Ming\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f5a33c39fdb10706d85a3a8108c5179477fe8bb3\",\"title\":\"Data-driven Adaptive Adjustment Strategy for Strong Wind Alarm in High-speed Railway\",\"url\":\"https://www.semanticscholar.org/paper/f5a33c39fdb10706d85a3a8108c5179477fe8bb3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1604.06838\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ca366bc08a738a92e2c7e2c142ec853dbea3b82b\",\"title\":\"Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ca366bc08a738a92e2c7e2c142ec853dbea3b82b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1606.02960\",\"authors\":[{\"authorId\":\"2844243\",\"name\":\"Sam Wiseman\"},{\"authorId\":\"2531268\",\"name\":\"Alexander M. Rush\"}],\"doi\":\"10.18653/v1/D16-1137\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5507dc32b368c8afd3b9507e9b5888da7bd7d7cd\",\"title\":\"Sequence-to-Sequence Learning as Beam-Search Optimization\",\"url\":\"https://www.semanticscholar.org/paper/5507dc32b368c8afd3b9507e9b5888da7bd7d7cd\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1608.07068\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-46475-6_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"title\":\"Title Generation for User Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1807.11605\",\"authors\":[{\"authorId\":\"49042662\",\"name\":\"H. Arslan\"},{\"authorId\":\"2032659\",\"name\":\"M. Fishel\"},{\"authorId\":\"3087532\",\"name\":\"G. Anbarjafari\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"98eec958fad32c98db12efbcc2585e4b0c0d181b\",\"title\":\"Doubly Attentive Transformer Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/98eec958fad32c98db12efbcc2585e4b0c0d181b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"title\":\"Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1804.09066\",\"authors\":[{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"145264990\",\"name\":\"K. Singh\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1007/978-3-030-01216-8_43\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa63893b34f523973d0692dc74ff22512daac322\",\"title\":\"ECO: Efficient Convolutional Network for Online Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/aa63893b34f523973d0692dc74ff22512daac322\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1808.04091\",\"authors\":[{\"authorId\":\"10780897\",\"name\":\"Damai Dai\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"771f66019981b451c29d152c199e9e21a00ef29e\",\"title\":\"Live Video Comment Generation Based on Surrounding Frames and Live Comments\",\"url\":\"https://www.semanticscholar.org/paper/771f66019981b451c29d152c199e9e21a00ef29e\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1907.05597\",\"authors\":[{\"authorId\":\"145014299\",\"name\":\"Alireza Ghods\"},{\"authorId\":\"144758070\",\"name\":\"D. Cook\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"84ea0968bc6988d1036e2f6c12d52064c825d06a\",\"title\":\"Activity2Vec: Learning ADL Embeddings from Sensor Data with a Sequence-to-Sequence Model\",\"url\":\"https://www.semanticscholar.org/paper/84ea0968bc6988d1036e2f6c12d52064c825d06a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2653622\",\"name\":\"Junfu Pu\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"413a354acbc42a0455ce1bbbd5d5c75cc38bc53d\",\"title\":\"Iterative Alignment Network for Continuous Sign Language\",\"url\":\"https://www.semanticscholar.org/paper/413a354acbc42a0455ce1bbbd5d5c75cc38bc53d\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e4efbcd8f52446b276129e1272512b916ddf093\",\"title\":\"NVIDIA DGX-1 With Tesla V 100 System Architecture The Fastest Platform for Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/2e4efbcd8f52446b276129e1272512b916ddf093\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1912.11872\",\"authors\":[{\"authorId\":\"153040576\",\"name\":\"T. Mei\"},{\"authorId\":\"101586660\",\"name\":\"W. Zhang\"},{\"authorId\":\"48577275\",\"name\":\"Ting Yao\"}],\"doi\":\"10.1017/ATSIP.2020.10\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"title\":\"Vision and Language: from Visual Perception to Content Creation\",\"url\":\"https://www.semanticscholar.org/paper/3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1380993111\",\"name\":\"Ljubisa Sehovac\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9de17998461ccba78ab7c236f22ff2bd5a48aa86\",\"title\":\"Forecasting Energy Consumption using Sequence to Sequence Attention models\",\"url\":\"https://www.semanticscholar.org/paper/9de17998461ccba78ab7c236f22ff2bd5a48aa86\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3218631\",\"name\":\"Hideaki Misawa\"},{\"authorId\":\"21165828\",\"name\":\"Kazuhito Takenaka\"},{\"authorId\":\"40805291\",\"name\":\"Tomoya Sugihara\"},{\"authorId\":\"48447075\",\"name\":\"H. Liu\"},{\"authorId\":\"1684099\",\"name\":\"T. Taniguchi\"},{\"authorId\":\"35280692\",\"name\":\"T. Bando\"}],\"doi\":\"10.1109/ITSC.2017.8317789\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b737f8604972e6b41fb83b16a648924014d39cec\",\"title\":\"Prediction of driving behavior based on sequence to sequence model with parametric bias\",\"url\":\"https://www.semanticscholar.org/paper/b737f8604972e6b41fb83b16a648924014d39cec\",\"venue\":\"2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)\",\"year\":2017},{\"arxivId\":\"1805.07935\",\"authors\":[{\"authorId\":\"145591949\",\"name\":\"Yuan Cheng\"},{\"authorId\":\"1792137\",\"name\":\"Guangya Li\"},{\"authorId\":\"2793468\",\"name\":\"Hai-Bao Chen\"},{\"authorId\":\"1733737\",\"name\":\"Sheldon X.-D. Tan\"},{\"authorId\":\"40355833\",\"name\":\"Hao Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b04e96e1fc56c11c43a6fc3e84ba4ca95905e2de\",\"title\":\"DEEPEYE: A Compact and Accurate Video Comprehension at Terminal Devices Compressed with Quantization and Tensorization\",\"url\":\"https://www.semanticscholar.org/paper/b04e96e1fc56c11c43a6fc3e84ba4ca95905e2de\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/ICPR.2016.7900081\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5d8eb0ccd00d66b649c6a4c06edb0e34093a2357\",\"title\":\"Automatic video description generation via LSTM with joint two-stream encoding\",\"url\":\"https://www.semanticscholar.org/paper/5d8eb0ccd00d66b649c6a4c06edb0e34093a2357\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":\"1807.10957\",\"authors\":[{\"authorId\":\"3377097\",\"name\":\"A. Sharghi\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"},{\"authorId\":\"7430758\",\"name\":\"C. Li\"},{\"authorId\":\"40381920\",\"name\":\"Tianbao Yang\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1007/978-3-030-01219-9_32\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45785f61445527acef55b9cf269c1562fce938fa\",\"title\":\"Improving Sequential Determinantal Point Processes for Supervised Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/45785f61445527acef55b9cf269c1562fce938fa\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"},{\"authorId\":\"48126134\",\"name\":\"Raman Arora\"},{\"authorId\":\"2934336\",\"name\":\"Lo\\u00efc Barrault\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"144404308\",\"name\":\"Amanda Duarte\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"51261844\",\"name\":\"N. Holzenberger\"},{\"authorId\":\"1908331\",\"name\":\"Chiraag Lala\"},{\"authorId\":\"2513954\",\"name\":\"S. Lee\"},{\"authorId\":\"3448602\",\"name\":\"Jind\\u0159ich Libovick\\u00fd\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"78476552\",\"name\":\"K. Mulligan\"},{\"authorId\":\"1752987954\",\"name\":\"Alissa Ostapenka\"},{\"authorId\":\"26400211\",\"name\":\"Shruti Palaskar\"},{\"authorId\":\"40489004\",\"name\":\"R. Sanabria\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"}],\"doi\":\"10.1109/JSTSP.2020.2998415\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"14aadd24040edaa9ce9978b53b00aeede015f859\",\"title\":\"Grounded Sequence to Sequence Transduction\",\"url\":\"https://www.semanticscholar.org/paper/14aadd24040edaa9ce9978b53b00aeede015f859\",\"venue\":\"IEEE Journal of Selected Topics in Signal Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"153173208\",\"name\":\"J. Xu\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2019.11.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"title\":\"Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into LSTM-based model\",\"url\":\"https://www.semanticscholar.org/paper/2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51240443\",\"name\":\"Jacob A. Allison\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"},{\"authorId\":\"1678800\",\"name\":\"S. Lyshevski\"}],\"doi\":\"10.1109/ICUAS.2018.8453309\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9575f9b22d4ef2d3898cb1b2ab65105ee0f5183a\",\"title\":\"Resilient Communication, Object Classification and Data Fusion in Unmanned Aerial Systems\",\"url\":\"https://www.semanticscholar.org/paper/9575f9b22d4ef2d3898cb1b2ab65105ee0f5183a\",\"venue\":\"2018 International Conference on Unmanned Aircraft Systems (ICUAS)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"39491387\",\"name\":\"J. Zhou\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"}],\"doi\":\"10.1109/TIP.2018.2855422\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fd3d94fac6a282414406716040b10c1746634ecd\",\"title\":\"Video Captioning by Adversarial LSTM\",\"url\":\"https://www.semanticscholar.org/paper/fd3d94fac6a282414406716040b10c1746634ecd\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1511.04670\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9ed7d774684a1770445c1c53e276011a8364b9e2\",\"title\":\"Uncovering Temporal Context for Video Question and Answering\",\"url\":\"https://www.semanticscholar.org/paper/9ed7d774684a1770445c1c53e276011a8364b9e2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8060096\",\"name\":\"Sheng-hung Hu\"},{\"authorId\":\"2180892\",\"name\":\"Yikang Li\"},{\"authorId\":\"2913552\",\"name\":\"Baoxin Li\"}],\"doi\":\"10.1109/ICPR.2016.7899735\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f6ab4543cc38f23d0339e3037a952df7bcf696b\",\"title\":\"Video2vec: Learning semantic spatio-temporal embeddings for video representation\",\"url\":\"https://www.semanticscholar.org/paper/5f6ab4543cc38f23d0339e3037a952df7bcf696b\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":\"2008.11089\",\"authors\":[{\"authorId\":\"153533625\",\"name\":\"Yinghua Zhang\"},{\"authorId\":\"95882703\",\"name\":\"Y. Song\"},{\"authorId\":\"50685621\",\"name\":\"J. Liang\"},{\"authorId\":\"50139529\",\"name\":\"Kun Bai\"},{\"authorId\":\"153096468\",\"name\":\"Q. Yang\"}],\"doi\":\"10.1145/3394486.3403349\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d4081f199ea5746e1202b96c31ee7042a090150\",\"title\":\"Two Sides of the Same Coin: White-box and Black-box Attacks for Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/4d4081f199ea5746e1202b96c31ee7042a090150\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"1809.00241\",\"authors\":[{\"authorId\":\"47287745\",\"name\":\"Ankit Shah\"},{\"authorId\":\"51434736\",\"name\":\"Harini Kesavamoorthy\"},{\"authorId\":\"51441023\",\"name\":\"Poorva Rane\"},{\"authorId\":\"1989712\",\"name\":\"Pramati Kalwad\"},{\"authorId\":\"7661726\",\"name\":\"Alexander G. Hauptmann\"},{\"authorId\":\"1740721\",\"name\":\"Florian Metze\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72f4c415b5f3ecf63380b6985c95c5af2ba72632\",\"title\":\"Activity Recognition on a Large Scale in Short Videos - Moments in Time Dataset\",\"url\":\"https://www.semanticscholar.org/paper/72f4c415b5f3ecf63380b6985c95c5af2ba72632\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2808158\",\"name\":\"Necati Cihan Camg\\u00f6z\"},{\"authorId\":\"2417546\",\"name\":\"S. Hadfield\"},{\"authorId\":\"145398628\",\"name\":\"R. Bowden\"}],\"doi\":\"10.1109/ICCVW.2017.364\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"908667ca31085ff89a56f00f30aa3c6592a223e7\",\"title\":\"Particle Filter Based Probabilistic Forced Alignment for Continuous Gesture Recognition\",\"url\":\"https://www.semanticscholar.org/paper/908667ca31085ff89a56f00f30aa3c6592a223e7\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730952\",\"name\":\"Ravi Kiran Sarvadevabhatla\"},{\"authorId\":\"3436918\",\"name\":\"S. Surya\"},{\"authorId\":\"9214782\",\"name\":\"Trisha Mittal\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1109/TPAMI.2018.2877996\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"71807a11819431cce1b29b9c64dbd876f1da39c7\",\"title\":\"Pictionary-Style Word Guessing on Hand-Drawn Object Sketches: Dataset, Analysis and Deep Network Models\",\"url\":\"https://www.semanticscholar.org/paper/71807a11819431cce1b29b9c64dbd876f1da39c7\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2061766\",\"name\":\"C. Chuan\"},{\"authorId\":\"3320845\",\"name\":\"Dorien Herremans\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5c8449b37e26d1fc9f29c0daf6a4a65dec243c0\",\"title\":\"Modeling Temporal Tonal Relations in Polyphonic Music Through Deep Networks With a Novel Image-Based Representation\",\"url\":\"https://www.semanticscholar.org/paper/a5c8449b37e26d1fc9f29c0daf6a4a65dec243c0\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1908.01341\",\"authors\":[{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"113515522\",\"name\":\"Zhenmei Shi\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd712a873ae4eefb6c623c8e605e42c5a0173e3e\",\"title\":\"SF-Net: Structured Feature Network for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bd712a873ae4eefb6c623c8e605e42c5a0173e3e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3459901\",\"name\":\"Aaron Harlap\"}],\"doi\":\"10.1184/R1/8175407.V1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c074125441b2a741409d3c943356c096f7869d12\",\"title\":\"Improving ML Applications in Shared Computing Environments\",\"url\":\"https://www.semanticscholar.org/paper/c074125441b2a741409d3c943356c096f7869d12\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143935964\",\"name\":\"B. Lang\"},{\"authorId\":\"37305127\",\"name\":\"Daniel M. Lavery\"},{\"authorId\":\"51177290\",\"name\":\"Kelly Roman\"},{\"authorId\":\"1564294108\",\"name\":\"Erford Porter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c108524a0789db6456d64d1a5200d58a1c16cf36\",\"title\":\"TRECVID 2017: Video to Text Notebook Pages\",\"url\":\"https://www.semanticscholar.org/paper/c108524a0789db6456d64d1a5200d58a1c16cf36\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49035023\",\"name\":\"T. Nguyen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"64eac8b653194e2d479c2bf28d8f2bd2bfb9f53c\",\"title\":\"Automatic Video Captioning using Deep Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/64eac8b653194e2d479c2bf28d8f2bd2bfb9f53c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"47539491\",\"name\":\"J. Zhang\"},{\"authorId\":\"47747947\",\"name\":\"Qiang Guo\"},{\"authorId\":\"47570083\",\"name\":\"Jun Lei\"},{\"authorId\":\"3143729\",\"name\":\"D. Tu\"}],\"doi\":\"10.1109/ICIVC.2016.7571276\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4451d8e34e92abe8dceb425b3e41cb5fe948739\",\"title\":\"Generating video description with Long-Short Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/b4451d8e34e92abe8dceb425b3e41cb5fe948739\",\"venue\":\"2016 International Conference on Image, Vision and Computing (ICIVC)\",\"year\":2016},{\"arxivId\":\"1701.06521\",\"authors\":[{\"authorId\":\"3068677\",\"name\":\"Iacer Calixto\"},{\"authorId\":\"1688015\",\"name\":\"Qun Liu\"}],\"doi\":\"10.18653/v1/D17-1105\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"380fb2b79a53f884e7d81057fd519237550e988f\",\"title\":\"Incorporating Global Visual Features into Attention-based Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/380fb2b79a53f884e7d81057fd519237550e988f\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":\"1708.02478\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TNNLS.2018.2851077\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d78c47093fbf3d85225fd502674aba4a29b3987\",\"title\":\"From Deterministic to Generative: Multimodal Stochastic RNNs for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7d78c47093fbf3d85225fd502674aba4a29b3987\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2019},{\"arxivId\":\"2006.14262\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"title\":\"SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.02182\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/TPAMI.2019.2921539\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"95138f276b34cc84695b64ee5fc00c1e27091497\",\"title\":\"Two-Stream Region Convolutional 3D Network for Temporal Activity Detection\",\"url\":\"https://www.semanticscholar.org/paper/95138f276b34cc84695b64ee5fc00c1e27091497\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2184355\",\"name\":\"Karl Pichotta\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"168ce3386bc2a531694cd700e483bb207f1b8bb5\",\"title\":\"Statistical Script Learning with Recurrent Neural Nets\",\"url\":\"https://www.semanticscholar.org/paper/168ce3386bc2a531694cd700e483bb207f1b8bb5\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2739186\",\"name\":\"Michael Maynord\"},{\"authorId\":\"49530191\",\"name\":\"S. Bhattacharya\"},{\"authorId\":\"1969847\",\"name\":\"D. Aha\"}],\"doi\":\"10.1109/WACVW.2016.7470119\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73f0aeca87dded8ecbe782e3e0dc09fb3bd4f761\",\"title\":\"Image Surveillance Assistant\",\"url\":\"https://www.semanticscholar.org/paper/73f0aeca87dded8ecbe782e3e0dc09fb3bd4f761\",\"venue\":\"2016 IEEE Winter Applications of Computer Vision Workshops (WACVW)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67144160\",\"name\":\"Mao Gu\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"150337817\",\"name\":\"Weike Jin\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TCSVT.2019.2957309\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c0acaec480993efb5f882cea44879545dd5687c\",\"title\":\"Video Dialog via Multi-Grained Convolutional Self-Attention Context Multi-Modal Networks\",\"url\":\"https://www.semanticscholar.org/paper/1c0acaec480993efb5f882cea44879545dd5687c\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":\"2009.07335\",\"authors\":[{\"authorId\":\"16286752\",\"name\":\"Md. Mushfiqur Rahman\"},{\"authorId\":\"1945520590\",\"name\":\"Thasin Abedin\"},{\"authorId\":\"1945917684\",\"name\":\"Khondokar S. S. Prottoy\"},{\"authorId\":\"1945917586\",\"name\":\"Ayana Moshruba\"},{\"authorId\":\"32826273\",\"name\":\"F. H. Siddiqui\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e2a0c5a1d5ae87ae734da2ef9c5eac6b2146536b\",\"title\":\"Semantically Sensible Video Captioning (SSVC)\",\"url\":\"https://www.semanticscholar.org/paper/e2a0c5a1d5ae87ae734da2ef9c5eac6b2146536b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46964024\",\"name\":\"Adita Kulkarni\"},{\"authorId\":\"49986654\",\"name\":\"A. Seetharam\"},{\"authorId\":\"152459641\",\"name\":\"Arti Ramesh\"}],\"doi\":\"10.1145/3360774.3360803\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"839015130f30a152a5c5fdd235ad91c1fe8489c2\",\"title\":\"DeepFit: deep learning based fitness center equipment use modeling and prediction\",\"url\":\"https://www.semanticscholar.org/paper/839015130f30a152a5c5fdd235ad91c1fe8489c2\",\"venue\":\"MobiQuitous\",\"year\":2019},{\"arxivId\":\"1803.10827\",\"authors\":[{\"authorId\":\"2883417\",\"name\":\"Kiana Ehsani\"},{\"authorId\":\"2456400\",\"name\":\"Hessam Bagherinezhad\"},{\"authorId\":\"40497777\",\"name\":\"Joseph Redmon\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2018.00426\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a63b8429ebeef316a65a94b021ef9a214c705f83\",\"title\":\"Who Let the Dogs Out? Modeling Dog Behavior from Visual Data\",\"url\":\"https://www.semanticscholar.org/paper/a63b8429ebeef316a65a94b021ef9a214c705f83\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de228875bc33e9db85123469ef80fc0071a92386\",\"title\":\"Word2VisualVec: Image and Video to Sentence Matching by Visual Feature Prediction\",\"url\":\"https://www.semanticscholar.org/paper/de228875bc33e9db85123469ef80fc0071a92386\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1807.11546\",\"authors\":[{\"authorId\":\"2569534\",\"name\":\"J. Kim\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":\"10.1007/978-3-030-01216-8_35\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d0907770cd4619aa6a36139a859e8f09bc9f0ef\",\"title\":\"Textual Explanations for Self-Driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/9d0907770cd4619aa6a36139a859e8f09bc9f0ef\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32324177\",\"name\":\"C. Wu\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"15862607\",\"name\":\"Xiaoliang Chu\"},{\"authorId\":\"2037988\",\"name\":\"Weichen Sun\"},{\"authorId\":\"144310030\",\"name\":\"F. Su\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"}],\"doi\":\"10.1016/j.neucom.2018.07.029\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fc4a590d1859ba43c1303927c86c64b34e43287\",\"title\":\"Hierarchical attention-based multimodal fusion for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4fc4a590d1859ba43c1303927c86c64b34e43287\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3449299\",\"name\":\"Atsushi Ushiku\"},{\"authorId\":\"144921213\",\"name\":\"H. Hashimoto\"},{\"authorId\":\"50594656\",\"name\":\"A. Hashimoto\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abd4152773ebb97b90163b9a6bbdf2075e825481\",\"title\":\"Procedural Text Generation from an Execution Video\",\"url\":\"https://www.semanticscholar.org/paper/abd4152773ebb97b90163b9a6bbdf2075e825481\",\"venue\":\"IJCNLP\",\"year\":2017},{\"arxivId\":\"1911.01497\",\"authors\":[{\"authorId\":\"24025563\",\"name\":\"Vikas Raunak\"},{\"authorId\":\"48021693\",\"name\":\"V. Kumar\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"},{\"authorId\":\"1400997347\",\"name\":\"Jaimie Callan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fac57bce1076d1578a92f1c6baf6d48cf05119b8\",\"title\":\"On Compositionality in Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/fac57bce1076d1578a92f1c6baf6d48cf05119b8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.10322\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1109/CVPR.2019.01277\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"20888a7aebaf77a306c0886f165bd0d468db806d\",\"title\":\"Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20888a7aebaf77a306c0886f165bd0d468db806d\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1413471182\",\"name\":\"Mahdi Moghadasi\"},{\"authorId\":\"122011647\",\"name\":\"Yu Zhuang\"},{\"authorId\":\"2038104183\",\"name\":\"Hashim Gellban\"}],\"doi\":\"10.1145/3421515.3421525\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"53409e0575eef4e5640da3a3fdcf9c3623fd85c9\",\"title\":\"Robo : A Counselor Chatbot for Opioid Addicted Patients\",\"url\":\"https://www.semanticscholar.org/paper/53409e0575eef4e5640da3a3fdcf9c3623fd85c9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32015441\",\"name\":\"Yingfan Huang\"},{\"authorId\":\"3441309\",\"name\":\"Huikun Bi\"},{\"authorId\":\"46947468\",\"name\":\"Zhaoxin Li\"},{\"authorId\":\"122214464\",\"name\":\"Tianlu Mao\"},{\"authorId\":\"1752178\",\"name\":\"Zhaoqi Wang\"}],\"doi\":\"10.1109/ICCV.2019.00637\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f2bd59f9ca31e941f63b900510109308d8ec9acf\",\"title\":\"STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction\",\"url\":\"https://www.semanticscholar.org/paper/f2bd59f9ca31e941f63b900510109308d8ec9acf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1807.08089\",\"authors\":[{\"authorId\":\"49070426\",\"name\":\"Yi-Chen Chen\"},{\"authorId\":\"40898493\",\"name\":\"Sung-Feng Huang\"},{\"authorId\":\"3386316\",\"name\":\"C. Shen\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"},{\"authorId\":\"145535692\",\"name\":\"L. Lee\"}],\"doi\":\"10.1109/SLT.2018.8639553\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f5683537b3945666ce442f0977d1cd893c1e588\",\"title\":\"Phonetic-and-Semantic Embedding of Spoken words with Applications in Spoken Content Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/8f5683537b3945666ce442f0977d1cd893c1e588\",\"venue\":\"2018 IEEE Spoken Language Technology Workshop (SLT)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49163024\",\"name\":\"Shuang Bai\"},{\"authorId\":\"3380543\",\"name\":\"S. An\"}],\"doi\":\"10.1016/j.neucom.2018.05.080\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe66fc9e3d9a52497119bab89ac54fc8b5f8859a\",\"title\":\"A survey on automatic image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/fe66fc9e3d9a52497119bab89ac54fc8b5f8859a\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143702938\",\"name\":\"C. Yang\"},{\"authorId\":\"1748404\",\"name\":\"Zhongwen Guo\"},{\"authorId\":\"1381477895\",\"name\":\"Lintao Xian\"}],\"doi\":\"10.1088/1757-899x/692/1/012047\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"113bca4a2ccf86086142fbfded665ff93d2052ff\",\"title\":\"Time Series Data Prediction Based on Sequence to Sequence Model\",\"url\":\"https://www.semanticscholar.org/paper/113bca4a2ccf86086142fbfded665ff93d2052ff\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2007.13913\",\"authors\":[{\"authorId\":\"144568152\",\"name\":\"David M. Chan\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"144711958\",\"name\":\"D. Ross\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8ac8179dbdd256e514700b076673b6d5b9083251\",\"title\":\"Active Learning for Video Description With Cluster-Regularized Ensemble Ranking\",\"url\":\"https://www.semanticscholar.org/paper/8ac8179dbdd256e514700b076673b6d5b9083251\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"3145905\",\"name\":\"Jingqiu Zhang\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0530-0\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"title\":\"Exploiting long-term temporal dynamics for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":\"2009.02568\",\"authors\":[{\"authorId\":\"117232498\",\"name\":\"Anelise Newman\"},{\"authorId\":\"1482544048\",\"name\":\"Camilo Fosco\"},{\"authorId\":\"24026083\",\"name\":\"Vincent Casser\"},{\"authorId\":\"123872529\",\"name\":\"Allen Lee\"},{\"authorId\":\"47276980\",\"name\":\"Barry\"},{\"authorId\":\"46498555\",\"name\":\"Mcnamara\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"}],\"doi\":\"10.1007/978-3-030-58517-4_14\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"203bfa5e488a9c3100e3d9b9af5ea34537068612\",\"title\":\"Multimodal Memorability: Modeling Effects of Semantics and Decay on Video Memorability\",\"url\":\"https://www.semanticscholar.org/paper/203bfa5e488a9c3100e3d9b9af5ea34537068612\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1808.03766\",\"authors\":[{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"8983218\",\"name\":\"S. Buch\"},{\"authorId\":\"3409955\",\"name\":\"C. D. Dao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5468c96e3846da23c26b59c28c313506bffbf7ce\",\"title\":\"The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary\",\"url\":\"https://www.semanticscholar.org/paper/5468c96e3846da23c26b59c28c313506bffbf7ce\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938051940\",\"name\":\"Dylan Flaute\"},{\"authorId\":\"2405109\",\"name\":\"B. Narayanan\"}],\"doi\":\"10.1117/12.2568016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"title\":\"Video captioning using weakly supervised convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17437794\",\"name\":\"Pin-Chu Yang\"},{\"authorId\":\"3340390\",\"name\":\"Nishanth Koganti\"},{\"authorId\":\"2110421\",\"name\":\"Gustavo Alfonso Garcia Ricardez\"},{\"authorId\":\"145937975\",\"name\":\"M. Yamamoto\"},{\"authorId\":\"21825576\",\"name\":\"J. Takamatsu\"},{\"authorId\":\"35246508\",\"name\":\"T. Ogasawara\"}],\"doi\":\"10.1109/RO-MAN47096.2020.9223341\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2fb858ce6255e441651927a9e54b4d7a715816db\",\"title\":\"Context Dependent Trajectory Generation using Sequence-to-Sequence Models for Robotic Toilet Cleaning\",\"url\":\"https://www.semanticscholar.org/paper/2fb858ce6255e441651927a9e54b4d7a715816db\",\"venue\":\"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"69901495\",\"name\":\"H. Sadr\"},{\"authorId\":\"72476885\",\"name\":\"Mir Mohsen Pedram\"},{\"authorId\":\"1709359\",\"name\":\"M. Teshnehlab\"}],\"doi\":\"10.1109/ACCESS.2020.2992063\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4464e0581393c4d96762a1cb73e9be206aa1b712\",\"title\":\"Multi-View Deep Network: A Deep Model Based on Learning Features From Heterogeneous Neural Networks for Sentiment Analysis\",\"url\":\"https://www.semanticscholar.org/paper/4464e0581393c4d96762a1cb73e9be206aa1b712\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1611.07837\",\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2d692d14b4277e6ad00b9030ad3b68141b3bbc21\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Language\",\"url\":\"https://www.semanticscholar.org/paper/2d692d14b4277e6ad00b9030ad3b68141b3bbc21\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"title\":\"Trainable performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"2007.02036\",\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"103278467\",\"name\":\"Minuk Ma\"},{\"authorId\":\"48920375\",\"name\":\"T. Pham\"},{\"authorId\":\"97531942\",\"name\":\"Kyungsu Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/cvpr42600.2020.01012\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7fa5fdaacc87165a0281babd96949e8ec74d3a41\",\"title\":\"Modality Shifting Attention Network for Multi-Modal Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/7fa5fdaacc87165a0281babd96949e8ec74d3a41\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1806.09278\",\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"50980046\",\"name\":\"Moyini Yao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b87e33101a5564cbd3d212246aa48e2b6123227\",\"title\":\"Best Vision Technologies Submission to ActivityNet Challenge 2018-Task: Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8b87e33101a5564cbd3d212246aa48e2b6123227\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1511.05526\",\"authors\":[{\"authorId\":\"1900013\",\"name\":\"Zhengyang Wu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1733702\",\"name\":\"Matthew R. Walter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a9453721f35f364e176a5aaa7bdb622f72fbcaec\",\"title\":\"Learning Articulated Motion Models from Visual and Lingual Signals\",\"url\":\"https://www.semanticscholar.org/paper/a9453721f35f364e176a5aaa7bdb622f72fbcaec\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1812.02872\",\"authors\":[{\"authorId\":\"34777509\",\"name\":\"Yapeng Tian\"},{\"authorId\":\"2149345\",\"name\":\"Chenxiao Guan\"},{\"authorId\":\"48616329\",\"name\":\"J. Goodman\"},{\"authorId\":\"50583301\",\"name\":\"Marc Moore\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5328a7024f820fafdab4165777807c2ecb855fe4\",\"title\":\"An Attempt towards Interpretable Audio-Visual Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5328a7024f820fafdab4165777807c2ecb855fe4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2849794\",\"name\":\"P. Klosowski\"}],\"doi\":\"10.23919/SPA.2018.8563389\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f73647c3290ed7d857cc103971e343f654b44685\",\"title\":\"Deep Learning for Natural Language Processing and Language Modelling\",\"url\":\"https://www.semanticscholar.org/paper/f73647c3290ed7d857cc103971e343f654b44685\",\"venue\":\"2018 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)\",\"year\":2018},{\"arxivId\":\"1708.09667\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3123420\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6199348281e14a5a127b539f5cdb92fcddbac17\",\"title\":\"Video Captioning with Guidance of Multimodal Latent Topics\",\"url\":\"https://www.semanticscholar.org/paper/a6199348281e14a5a127b539f5cdb92fcddbac17\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1812.08407\",\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"51011510\",\"name\":\"Juan Jose Alvarado Leanos\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"title\":\"Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2898850\",\"name\":\"R\\u00e9mi Ronfard\"},{\"authorId\":\"2898850\",\"name\":\"R\\u00e9mi Ronfard\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf3ca7fa12d224416480b303c3024531b0122a30\",\"title\":\"Five Challenges for Intelligent Cinematography and Editing R\\u00e9mi Ronfard\",\"url\":\"https://www.semanticscholar.org/paper/cf3ca7fa12d224416480b303c3024531b0122a30\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145113946\",\"name\":\"J. Tang\"},{\"authorId\":\"144005516\",\"name\":\"Jing Wang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3291925\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a437bb550d1df02188e4b145e01675551da36336\",\"title\":\"Show, Reward, and Tell\",\"url\":\"https://www.semanticscholar.org/paper/a437bb550d1df02188e4b145e01675551da36336\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"104510214\",\"name\":\"K. Li\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TETCI.2019.2892755\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"title\":\"Visual to Text: Survey of Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152998639\",\"name\":\"Y. Li\"},{\"authorId\":\"2604251\",\"name\":\"Guosheng Lin\"},{\"authorId\":\"3194022\",\"name\":\"Bohan Zhuang\"},{\"authorId\":\"47968029\",\"name\":\"Lingqiao Liu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2f90b744cf3a4959689935416b9d40169b646e00\",\"title\":\"CNN joint embedding LSTM LSTM CNN joint embedding LSTM CNN joint embedding LSTM CNN joint embedding LSTM CNN joint embedding\",\"url\":\"https://www.semanticscholar.org/paper/2f90b744cf3a4959689935416b9d40169b646e00\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82599235\",\"name\":\"S. R. Ammar\"},{\"authorId\":\"152629075\",\"name\":\"M. R. Anjum\"},{\"authorId\":\"145614509\",\"name\":\"Md. Touhidul Islam Islam\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"24bb9e9785c714a8f2470366395f33a2cc339398\",\"title\":\"Using deep learning algorithms to detect violent activities\",\"url\":\"https://www.semanticscholar.org/paper/24bb9e9785c714a8f2470366395f33a2cc339398\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"},{\"authorId\":\"51211765\",\"name\":\"A. Bhat\"},{\"authorId\":\"119713376\",\"name\":\"Aravindh Kuppusamy\"},{\"authorId\":\"1678800\",\"name\":\"S. Lyshevski\"}],\"doi\":\"10.1117/12.2519300\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d76c6d6f23680d596dc7d3e5c33c193ca5dc10e\",\"title\":\"Object recognition, identification and classification for intelligent surveillance and reconnaissance platforms\",\"url\":\"https://www.semanticscholar.org/paper/2d76c6d6f23680d596dc7d3e5c33c193ca5dc10e\",\"venue\":\"Defense + Commercial Sensing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66957238\",\"name\":\"Youngsaeng Jin\"},{\"authorId\":\"1561609794\",\"name\":\"Junggi Kwak\"},{\"authorId\":\"18016679\",\"name\":\"Younglo Lee\"},{\"authorId\":\"70346423\",\"name\":\"Jeongseop Yun\"},{\"authorId\":\"81950948\",\"name\":\"Hanseok Ko\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"85113ab90b59b7365d6b85ba94f907c0f5de3b5b\",\"title\":\"KU-ISPL TRECVID 2018 VTT Model\",\"url\":\"https://www.semanticscholar.org/paper/85113ab90b59b7365d6b85ba94f907c0f5de3b5b\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":\"1806.03377\",\"authors\":[{\"authorId\":\"3459901\",\"name\":\"Aaron Harlap\"},{\"authorId\":\"22252150\",\"name\":\"D. Narayanan\"},{\"authorId\":\"3078275\",\"name\":\"Amar Phanishayee\"},{\"authorId\":\"1720084\",\"name\":\"Vivek Seshadri\"},{\"authorId\":\"7692691\",\"name\":\"Nikhil R. Devanur\"},{\"authorId\":\"1707164\",\"name\":\"G. Ganger\"},{\"authorId\":\"1974678\",\"name\":\"Phillip B. Gibbons\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd\",\"title\":\"PipeDream: Fast and Efficient Pipeline Parallel DNN Training\",\"url\":\"https://www.semanticscholar.org/paper/c314d97d75b4a988b106ddcec8a40a4d3bcdb8bd\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46709826\",\"name\":\"Pengcheng Yang\"},{\"authorId\":\"72871419\",\"name\":\"Zhihan Zhang\"},{\"authorId\":\"17866105\",\"name\":\"Fuli Luo\"},{\"authorId\":\"49192881\",\"name\":\"Lei Li\"},{\"authorId\":\"50418711\",\"name\":\"Chengyang Huang\"},{\"authorId\":\"48305273\",\"name\":\"Xu Sun\"}],\"doi\":\"10.18653/v1/P19-1257\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbf4d7e9274ba37f040aeeaad451abe98c569b88\",\"title\":\"Cross-Modal Commentator: Automatic Machine Commenting Based on Cross-Modal Information\",\"url\":\"https://www.semanticscholar.org/paper/cbf4d7e9274ba37f040aeeaad451abe98c569b88\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":\"1911.01857\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2020.03.001\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"583222b6a573ad698207a0ebabb06685c4517558\",\"title\":\"Video Captioning with Text-based Dynamic Attention and Step-by-Step Learning\",\"url\":\"https://www.semanticscholar.org/paper/583222b6a573ad698207a0ebabb06685c4517558\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48116016\",\"name\":\"J. Ren\"},{\"authorId\":\"50550351\",\"name\":\"W. Zhang\"}],\"doi\":\"10.1007/S11760-019-01449-W\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1581301ddb2b9b76c10b31eef101733ffebc46f8\",\"title\":\"CLOSE: Coupled content\\u2013semantic embedding\",\"url\":\"https://www.semanticscholar.org/paper/1581301ddb2b9b76c10b31eef101733ffebc46f8\",\"venue\":\"Signal Image Video Process.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143691596\",\"name\":\"A. E. Vela\"},{\"authorId\":\"2026998314\",\"name\":\"Yousef Oleyaei-Motlagh\"}],\"doi\":\"10.1109/DASC50938.2020.9256761\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"24fde45b378dd4b6a0284b9b460b61666ffb5c31\",\"title\":\"Ground Level Aviation Noise Prediction: A Sequence to Sequence Modeling Approach Using LSTM Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/24fde45b378dd4b6a0284b9b460b61666ffb5c31\",\"venue\":\"2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"79685414\",\"name\":\"I-Feng Kao\"},{\"authorId\":\"10665554\",\"name\":\"Yanlai Zhou\"},{\"authorId\":\"2783711\",\"name\":\"L. Chang\"},{\"authorId\":\"3070618\",\"name\":\"Fi-John Chang\"}],\"doi\":\"10.1016/j.jhydrol.2020.124631\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b637666b323fc72e2e0bc7a02e9430803320e86\",\"title\":\"Exploring a Long Short-Term Memory based Encoder-Decoder framework for multi-step-ahead flood forecasting\",\"url\":\"https://www.semanticscholar.org/paper/0b637666b323fc72e2e0bc7a02e9430803320e86\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40296536\",\"name\":\"Z. Wang\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"2582320\",\"name\":\"J. Ma\"},{\"authorId\":\"1492115784\",\"name\":\"Jing-jing Li\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"46173234\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1016/J.IPM.2019.102130\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0eaa75b2c7ef5ccbe19a1b88b7f4de3e5c52713\",\"title\":\"Discovering attractive segments in the user-generated video streams\",\"url\":\"https://www.semanticscholar.org/paper/c0eaa75b2c7ef5ccbe19a1b88b7f4de3e5c52713\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"1612.07360\",\"authors\":[{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"1701293\",\"name\":\"J. Zhang\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2017.334\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"76f83380fe193ae8475e660c1c6b12b60521a29f\",\"title\":\"Top-Down Visual Saliency Guided by Captions\",\"url\":\"https://www.semanticscholar.org/paper/76f83380fe193ae8475e660c1c6b12b60521a29f\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"47775167\",\"name\":\"Teng Li\"},{\"authorId\":\"47906413\",\"name\":\"Y. Wang\"},{\"authorId\":\"1730308\",\"name\":\"X. Wu\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1016/j.cviu.2017.06.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"94a86a758ae2608c00e9690e9951e805755bb1a1\",\"title\":\"Learning explicit video attributes from mid-level representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/94a86a758ae2608c00e9690e9951e805755bb1a1\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"49292319\",\"name\":\"Bo Wang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/3123266.3127904\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7a736b7347fc5ea93c196ddfe0630ecddc17d324\",\"title\":\"Multirate Multimodal Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a736b7347fc5ea93c196ddfe0630ecddc17d324\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1904.12004\",\"authors\":[{\"authorId\":\"98243944\",\"name\":\"Chenglong Wang\"},{\"authorId\":\"3407947\",\"name\":\"R. Bunel\"},{\"authorId\":\"1729912\",\"name\":\"Krishnamurthy Dvijotham\"},{\"authorId\":\"2421691\",\"name\":\"Po-Sen Huang\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"}],\"doi\":\"10.1109/CVPR.2019.01254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59b439bde73d80dccf367d414e209d08d312c059\",\"title\":\"Knowing When to Stop: Evaluation and Verification of Conformity to Output-Size Specifications\",\"url\":\"https://www.semanticscholar.org/paper/59b439bde73d80dccf367d414e209d08d312c059\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886528\",\"name\":\"Guolong Wang\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"},{\"authorId\":\"2168639\",\"name\":\"Kaiping Xu\"},{\"authorId\":\"145489794\",\"name\":\"K. Huang\"},{\"authorId\":\"19204816\",\"name\":\"Shuxiong Ye\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"title\":\"Bridge Video and Text with Cascade Syntactic Structure\",\"url\":\"https://www.semanticscholar.org/paper/b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":\"1810.11954\",\"authors\":[{\"authorId\":\"144992211\",\"name\":\"Shubham Agarwal\"},{\"authorId\":\"2544049\",\"name\":\"Ondrej Dusek\"},{\"authorId\":\"2621022\",\"name\":\"Ioannis Konstas\"},{\"authorId\":\"1681799\",\"name\":\"Verena Rieser\"}],\"doi\":\"10.18653/v1/W18-5709\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2b41790d33770ba43c45ed2da89c644840debf0\",\"title\":\"A Knowledge-Grounded Multimodal Search-Based Conversational Agent\",\"url\":\"https://www.semanticscholar.org/paper/b2b41790d33770ba43c45ed2da89c644840debf0\",\"venue\":\"SCAI@EMNLP\",\"year\":2018},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1705.00930\",\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2017.64\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"title\":\"Show, Adapt and Tell: Adversarial Training of Cross-Domain Image Captioner\",\"url\":\"https://www.semanticscholar.org/paper/828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1611.04021\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1da2431a799f68888b7e035fe49fe47a4735b71b\",\"title\":\"Leveraging Video Descriptions to Learn Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1da2431a799f68888b7e035fe49fe47a4735b71b\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1607.08584\",\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1007/978-3-319-46493-0_9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"18994755100686833b92c34af8ec1b35934e23d6\",\"title\":\"Connectionist Temporal Modeling for Weakly Supervised Action Labeling\",\"url\":\"https://www.semanticscholar.org/paper/18994755100686833b92c34af8ec1b35934e23d6\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1803.08976\",\"authors\":[{\"authorId\":\"2815804\",\"name\":\"Yu-An Chung\"},{\"authorId\":\"145898106\",\"name\":\"James R. Glass\"}],\"doi\":\"10.21437/Interspeech.2018-2341\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"083ec74cc10f96fbb64322ba23450666fd4df6cd\",\"title\":\"Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech\",\"url\":\"https://www.semanticscholar.org/paper/083ec74cc10f96fbb64322ba23450666fd4df6cd\",\"venue\":\"INTERSPEECH\",\"year\":2018},{\"arxivId\":\"1901.00097\",\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"2031845\",\"name\":\"Junbo Guo\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"title\":\"Not All Words Are Equal: Video-specific Information Loss for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"49730189\",\"name\":\"Boyang Li\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACVW.2019.00011\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"32b3e8f7a673801d6bcfb482a72c52c78e96b006\",\"title\":\"Joint Event Detection and Description in Continuous Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/32b3e8f7a673801d6bcfb482a72c52c78e96b006\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"66622154\",\"name\":\"Ray Ptucha\"}],\"doi\":\"10.1007/s10044-018-00770-3\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3ddfe22a67bdd1cc2b8f1a2e6663044690226933\",\"title\":\"Understanding temporal structure for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/3ddfe22a67bdd1cc2b8f1a2e6663044690226933\",\"venue\":\"Pattern Analysis and Applications\",\"year\":2019},{\"arxivId\":\"1808.10592\",\"authors\":[{\"authorId\":\"40223399\",\"name\":\"Renjie Zheng\"},{\"authorId\":\"46285635\",\"name\":\"Yilin Yang\"},{\"authorId\":\"1847848\",\"name\":\"M. Ma\"},{\"authorId\":\"144768480\",\"name\":\"Liang Huang\"}],\"doi\":\"10.18653/v1/W18-6443\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"546ad62ac456d4fa65e6f256fd1798e7f164b012\",\"title\":\"Ensemble Sequence Level Training for Multimodal MT: OSU-Baidu WMT18 Multimodal Machine Translation System Report\",\"url\":\"https://www.semanticscholar.org/paper/546ad62ac456d4fa65e6f256fd1798e7f164b012\",\"venue\":\"WMT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"2558130\",\"name\":\"Z. Shi\"}],\"doi\":\"10.1145/2964284.2967298\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0c014c19b68a781ccd6e26fcc7c47ba9b1cf020f\",\"title\":\"Boosting Video Description Generation by Explicitly Translating from Frame-Level Captions\",\"url\":\"https://www.semanticscholar.org/paper/0c014c19b68a781ccd6e26fcc7c47ba9b1cf020f\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1804.08274\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2018.00782\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"title\":\"Jointly Localizing and Describing Events for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40685026\",\"name\":\"Duona Zhang\"},{\"authorId\":\"2911928\",\"name\":\"Wenrui Ding\"},{\"authorId\":\"1740430\",\"name\":\"B. Zhang\"},{\"authorId\":\"3471524\",\"name\":\"C. Xie\"},{\"authorId\":\"46381969\",\"name\":\"H. Li\"},{\"authorId\":\"49046516\",\"name\":\"C. Liu\"},{\"authorId\":\"1783847\",\"name\":\"J. Han\"}],\"doi\":\"10.3390/s18030924\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"af8709c837e324bc875ced6fe6b2527cb7b1f1ea\",\"title\":\"Automatic Modulation Classification Based on Deep Learning for Unmanned Aerial Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/af8709c837e324bc875ced6fe6b2527cb7b1f1ea\",\"venue\":\"Sensors\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2151048\",\"name\":\"Taiki Miyanishi\"},{\"authorId\":\"8425523\",\"name\":\"J. Hirayama\"},{\"authorId\":\"34772057\",\"name\":\"Takuya Maekawa\"},{\"authorId\":\"1716788\",\"name\":\"M. Kawanabe\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"686b14ce7f02516730de03f459cadb223a03765f\",\"title\":\"Generating an Event Timeline About Daily Activities From a Semantic Concept Stream\",\"url\":\"https://www.semanticscholar.org/paper/686b14ce7f02516730de03f459cadb223a03765f\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"144404414\",\"name\":\"S. Satoh\"}],\"doi\":\"10.1145/3123266.3127898\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b2e0e08e4d4c722d0f54f5a124ca28a67d74ce3e\",\"title\":\"MANet: A Modal Attention Network for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/b2e0e08e4d4c722d0f54f5a124ca28a67d74ce3e\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1905.01641\",\"authors\":[{\"authorId\":\"31698884\",\"name\":\"M. Hua\"},{\"authorId\":\"3424956\",\"name\":\"Fuyuan Shi\"},{\"authorId\":\"11802634\",\"name\":\"Yibing Nan\"},{\"authorId\":\"37833805\",\"name\":\"Kai Wang\"},{\"authorId\":null,\"name\":\"Hao Chen\"},{\"authorId\":\"143763658\",\"name\":\"Shiguo Lian\"}],\"doi\":\"10.1109/IROS40897.2019.8968038\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d3626fef4932ffa879ec16a8367d913010ce22aa\",\"title\":\"Towards More Realistic Human-Robot Conversation: A Seq2Seq-based Body Gesture Interaction System\",\"url\":\"https://www.semanticscholar.org/paper/d3626fef4932ffa879ec16a8367d913010ce22aa\",\"venue\":\"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98419684\",\"name\":\"Phil Kinghorn\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9549e8e59f5ce436cc4d81ddb859b3295d131c5b\",\"title\":\"Deep learning-based regional image caption generation with refined descriptions\",\"url\":\"https://www.semanticscholar.org/paper/9549e8e59f5ce436cc4d81ddb859b3295d131c5b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"ReLU\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32fcbb6a9f1fe87b03ae798d657d6bb7cf5b25de\",\"title\":\"CSVideoNet : A Real-time End-to-end Learning Framework for High-framerate Video Compressive Sensing\",\"url\":\"https://www.semanticscholar.org/paper/32fcbb6a9f1fe87b03ae798d657d6bb7cf5b25de\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"Jinglun Shi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Describing Video with Multiple Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150282005\",\"name\":\"Rizard Renanda Adhi Pramono\"},{\"authorId\":\"73365425\",\"name\":\"Y. Chen\"},{\"authorId\":\"122315920\",\"name\":\"Wen-Hsien Fang\"}],\"doi\":\"10.1109/ICCV.2019.00015\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b76da17a060f1edb80b26f489f4b6256d785c57\",\"title\":\"Hierarchical Self-Attention Network for Action Localization in Videos\",\"url\":\"https://www.semanticscholar.org/paper/3b76da17a060f1edb80b26f489f4b6256d785c57\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1912.05730\",\"authors\":[{\"authorId\":\"1456146130\",\"name\":\"Rushi J. Babariya\"},{\"authorId\":\"134811419\",\"name\":\"Toru Tamaki\"}],\"doi\":\"10.1007/978-3-030-41299-9_37\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4874d0e044b44ee75d3125b0b8a002596918eeae\",\"title\":\"Meaning guided video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4874d0e044b44ee75d3125b0b8a002596918eeae\",\"venue\":\"ACPR\",\"year\":2019},{\"arxivId\":\"1902.06468\",\"authors\":[{\"authorId\":\"46312037\",\"name\":\"Youngeun Kwon\"},{\"authorId\":\"1998820\",\"name\":\"Minsoo Rhu\"}],\"doi\":\"10.1109/MICRO.2018.00021\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c841fcd728e7fe12af5f02c898e99859432f42ff\",\"title\":\"Beyond the Memory Wall: A Case for Memory-Centric HPC System for Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/c841fcd728e7fe12af5f02c898e99859432f42ff\",\"venue\":\"2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3374519\",\"name\":\"H. Lee\"},{\"authorId\":\"144248119\",\"name\":\"Minju Jung\"},{\"authorId\":\"1780524\",\"name\":\"J. Tani\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a468ac6399a82ccad9bbfa7133264dae9f14584\",\"title\":\"Characteristics of Visual Categorization of Long-Concatenated and Object-Directed Human Actions by a Multiple Spatio-Temporal Scales Recurrent Neural Network Model\",\"url\":\"https://www.semanticscholar.org/paper/2a468ac6399a82ccad9bbfa7133264dae9f14584\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"2909406\",\"name\":\"Ming Yang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b9127cbb1180c228475eefa8ca28be9eab1db2e9\",\"title\":\"A Survey of MultiView Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/b9127cbb1180c228475eefa8ca28be9eab1db2e9\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1711.08097\",\"authors\":[{\"authorId\":\"8598253\",\"name\":\"Wang-Li Hao\"},{\"authorId\":\"145274329\",\"name\":\"Zhaoxiang Zhang\"},{\"authorId\":\"32561502\",\"name\":\"H. Guan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"dde65325dc7600d02983a76bd54693f0050946a4\",\"title\":\"Integrating both Visual and Audio Cues for Enhanced Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/dde65325dc7600d02983a76bd54693f0050946a4\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"2011.00597\",\"authors\":[{\"authorId\":\"2007582232\",\"name\":\"Simon Ging\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"1835025\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"80089ad641bae28b0e57771afef181b60011069e\",\"title\":\"COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/80089ad641bae28b0e57771afef181b60011069e\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123791608\",\"name\":\"H. P. Desai\"},{\"authorId\":\"72090061\",\"name\":\"Anuja P. Parameshwaran\"},{\"authorId\":\"1756672\",\"name\":\"R. Sunderraman\"},{\"authorId\":\"144539409\",\"name\":\"M. Weeks\"}],\"doi\":\"10.1007/978-3-030-57821-3_25\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"63b51c417ebf59bd896aabfa1c47cadee3ab12f1\",\"title\":\"Deep Ensemble Models for 16S Ribosomal Gene Classification\",\"url\":\"https://www.semanticscholar.org/paper/63b51c417ebf59bd896aabfa1c47cadee3ab12f1\",\"venue\":\"ISBRA\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31149748\",\"name\":\"P. Farajiparvar\"},{\"authorId\":\"145225976\",\"name\":\"H. Ying\"},{\"authorId\":\"7640290\",\"name\":\"A. Pandya\"}],\"doi\":\"10.3389/frobt.2020.578805\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c771fb29ac8a1db59da19690a5d23de0372d6625\",\"title\":\"A Brief Survey of Telerobotic Time Delay Mitigation\",\"url\":\"https://www.semanticscholar.org/paper/c771fb29ac8a1db59da19690a5d23de0372d6625\",\"venue\":\"Frontiers in Robotics and AI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"1974929\",\"name\":\"Jiewu Xia\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1007/s11042-020-09674-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"title\":\"Double-channel language feature mining based model for video description\",\"url\":\"https://www.semanticscholar.org/paper/0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"venue\":\"Multim. Tools Appl.\",\"year\":2020},{\"arxivId\":\"2007.12402\",\"authors\":[{\"authorId\":\"1832280277\",\"name\":\"Ka Leong Cheng\"},{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"1559427865\",\"name\":\"Qifeng Chen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1007/978-3-030-58586-0_41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"723f008092e1fb25e73bc1e4f7e1084a95727e89\",\"title\":\"Fully Convolutional Networks for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/723f008092e1fb25e73bc1e4f7e1084a95727e89\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2002.11701\",\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":\"10.1145/3366423.3380137\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a842fe8c25348627764462a57f0cd43d8cef103b\",\"title\":\"CLARA: Clinical Report Auto-completion\",\"url\":\"https://www.semanticscholar.org/paper/a842fe8c25348627764462a57f0cd43d8cef103b\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":\"2012.04329\",\"authors\":[{\"authorId\":\"51238351\",\"name\":\"Andr\\u00e9s Mafla\"},{\"authorId\":\"147961332\",\"name\":\"R. S. Rezende\"},{\"authorId\":\"51231577\",\"name\":\"Llu\\u00eds G\\u00f3mez\"},{\"authorId\":\"2295553\",\"name\":\"Diane Larlus\"},{\"authorId\":\"1694974\",\"name\":\"Dimosthenis Karatzas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23f74eff7453a31f75cd34abc04167c4b1ac9acb\",\"title\":\"StacMR: Scene-Text Aware Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/23f74eff7453a31f75cd34abc04167c4b1ac9acb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46651496\",\"name\":\"Chengxi Li\"},{\"authorId\":\"153194886\",\"name\":\"Sagar Gandhi\"},{\"authorId\":\"35066258\",\"name\":\"B. Harrison\"}],\"doi\":\"10.1145/3337722.3341870\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"79b31c51375fdfe87d854d238d2bfb4696fd71cf\",\"title\":\"End-to-end let's play commentary generation using multi-modal video representations\",\"url\":\"https://www.semanticscholar.org/paper/79b31c51375fdfe87d854d238d2bfb4696fd71cf\",\"venue\":\"FDG\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48419180\",\"name\":\"Tao Ren\"},{\"authorId\":\"37305311\",\"name\":\"X. Liu\"},{\"authorId\":\"143929163\",\"name\":\"J. Niu\"},{\"authorId\":\"9358765\",\"name\":\"X. Lei\"},{\"authorId\":\"51092685\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1016/j.jhydrol.2020.124783\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bec75c8488402ab1e5bac0c83386471f43087e2c\",\"title\":\"Real-time water level prediction of cascaded channels based on multilayer perception and recurrent neural network\",\"url\":\"https://www.semanticscholar.org/paper/bec75c8488402ab1e5bac0c83386471f43087e2c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410469725\",\"name\":\"Mohamed Abou-Hussein\"},{\"authorId\":\"1410276723\",\"name\":\"Stefan H. M\\u00fcller-Weinfurtner\"},{\"authorId\":\"145581493\",\"name\":\"J. Boedecker\"}],\"doi\":\"10.1109/ICRA.2019.8794410\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b672101a4c760ec3be0dbb69e6c6c038cc14289e\",\"title\":\"Multimodal Spatio-Temporal Information in End-to-End Networks for Automotive Steering Prediction\",\"url\":\"https://www.semanticscholar.org/paper/b672101a4c760ec3be0dbb69e6c6c038cc14289e\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1007/s11263-019-01189-x\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1e1bd132613866c176a8fc780cb1b9f9aa43feeb\",\"title\":\"Video Question Answering with Spatio-Temporal Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/1e1bd132613866c176a8fc780cb1b9f9aa43feeb\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"2003.04865\",\"authors\":[{\"authorId\":\"3087214\",\"name\":\"Yutaro Shigeto\"},{\"authorId\":\"31678456\",\"name\":\"Y. Yoshikawa\"},{\"authorId\":\"2996464\",\"name\":\"Jiaqing Lin\"},{\"authorId\":\"39702069\",\"name\":\"A. Takeuchi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"title\":\"Video Caption Dataset for Describing Human Actions in Japanese\",\"url\":\"https://www.semanticscholar.org/paper/46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47751104\",\"name\":\"Dali Yang\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"}],\"doi\":\"10.1109/ICIP.2018.8451740\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"758d1c17569eea2a698cac31b2d9d2a772c84322\",\"title\":\"Hierarchical Context Encoding for Events Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/758d1c17569eea2a698cac31b2d9d2a772c84322\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8071088\",\"name\":\"R. Oruganti\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"00e19d93780ecf8f807c510a1105749d5bb1a2f3\",\"title\":\"Image Description using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/00e19d93780ecf8f807c510a1105749d5bb1a2f3\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1909.00121\",\"authors\":[{\"authorId\":\"49178142\",\"name\":\"H. Chen\"},{\"authorId\":\"145468578\",\"name\":\"Ke Lin\"},{\"authorId\":\"1772128\",\"name\":\"A. Maye\"},{\"authorId\":\"47786863\",\"name\":\"J. Li\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.3389/frobt.2020.475767\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"304f94dbe2ed228309e86298766ad24d9b6c6747\",\"title\":\"A Semantics-Assisted Video Captioning Model Trained With Scheduled Sampling\",\"url\":\"https://www.semanticscholar.org/paper/304f94dbe2ed228309e86298766ad24d9b6c6747\",\"venue\":\"Frontiers in Robotics and AI\",\"year\":2020},{\"arxivId\":\"1711.01515\",\"authors\":[{\"authorId\":\"2815804\",\"name\":\"Yu-An Chung\"},{\"authorId\":\"145898106\",\"name\":\"James R. Glass\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18833f315d7985337abddc2fc52ee24ab592a300\",\"title\":\"Learning Word Embeddings from Speech\",\"url\":\"https://www.semanticscholar.org/paper/18833f315d7985337abddc2fc52ee24ab592a300\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2119906\",\"name\":\"Ryosuke Shigenaka\"},{\"authorId\":\"35081710\",\"name\":\"Yan-Ying Chen\"},{\"authorId\":\"27375808\",\"name\":\"Francine Chen\"},{\"authorId\":\"5113463\",\"name\":\"D. Joshi\"},{\"authorId\":\"3184895\",\"name\":\"Y. Tsuboshita\"}],\"doi\":\"10.1109/ICME.2017.8019330\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"081c59d88a49c32bb4abc604d0e0e297a38a7082\",\"title\":\"Image-based user profiling of frequent and regular venue categories\",\"url\":\"https://www.semanticscholar.org/paper/081c59d88a49c32bb4abc604d0e0e297a38a7082\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1109/TMM.2018.2807588\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d0aec1410f7e403b7ef8dc99243a9addf07daa5\",\"title\":\"Text2Video: An End-to-end Learning Framework for Expressing Text With Videos\",\"url\":\"https://www.semanticscholar.org/paper/3d0aec1410f7e403b7ef8dc99243a9addf07daa5\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20632291\",\"name\":\"Shiliang Sun\"},{\"authorId\":\"145307050\",\"name\":\"Liang Mao\"},{\"authorId\":\"66550933\",\"name\":\"Ziang Dong\"},{\"authorId\":\"4382815\",\"name\":\"Lidan Wu\"}],\"doi\":\"10.1007/978-981-13-3029-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b74af2609e83796ad07069ff76d34fd2223efc0\",\"title\":\"Multiview Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/0b74af2609e83796ad07069ff76d34fd2223efc0\",\"venue\":\"Springer Singapore\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5434752\",\"name\":\"Y. Jiang\"}],\"doi\":\"10.5120/IJCA2019918660\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"43c22ebb5ff264a5ea996c163464cf761035a405\",\"title\":\"Multi-Feature Fusion for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/43c22ebb5ff264a5ea996c163464cf761035a405\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3038583\",\"name\":\"T. Etchegoyhen\"},{\"authorId\":\"1715983\",\"name\":\"Oier Lopez de Lacalle\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97acd96d73061bb32d13448ef8514c8af8a81995\",\"title\":\"Neural Natural Language Generation with Unstructured Contextual Information\",\"url\":\"https://www.semanticscholar.org/paper/97acd96d73061bb32d13448ef8514c8af8a81995\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46179701\",\"name\":\"Yicun Liu\"},{\"authorId\":\"1719062\",\"name\":\"J. R. Kender\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"823e726283792ddb457c82c84c236be59f06aab4\",\"title\":\"Visual Feature Extractor ( Inception-ResNet ) Text Feature Extractor ( fastText ) p : I , Tc c c Image-Text Sequence of culture c Speech Recognition Frame Decomposition fcv fcl\",\"url\":\"https://www.semanticscholar.org/paper/823e726283792ddb457c82c84c236be59f06aab4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2924631\",\"name\":\"Chung-Ming Own\"},{\"authorId\":\"1411529183\",\"name\":\"Feiyu Sha\"},{\"authorId\":\"2206122\",\"name\":\"Wenyuan Tao\"}],\"doi\":\"10.1109/ACCESS.2019.2951780\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e6e270acfd490890f58344a9aa05413ef975287\",\"title\":\"Triplet Decoders Neural Network Ensemble System and T-Conversion for Traffic Speed Sequence Prediction\",\"url\":\"https://www.semanticscholar.org/paper/0e6e270acfd490890f58344a9aa05413ef975287\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1710.07477\",\"authors\":[{\"authorId\":\"27555915\",\"name\":\"Tz-Ying Wu\"},{\"authorId\":\"16261770\",\"name\":\"Ting-An Chien\"},{\"authorId\":\"36549981\",\"name\":\"Cheng-Sheng Chan\"},{\"authorId\":\"27538483\",\"name\":\"Chan-Wei Hu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2017.15\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"604575bf821ad655e195a78d53badb0a636ffa0f\",\"title\":\"Anticipating Daily Intention Using On-wrist Motion Triggered Sensing\",\"url\":\"https://www.semanticscholar.org/paper/604575bf821ad655e195a78d53badb0a636ffa0f\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1380993111\",\"name\":\"Ljubisa Sehovac\"},{\"authorId\":\"2222599\",\"name\":\"Katarina Grolinger\"}],\"doi\":\"10.1109/ACCESS.2020.2975738\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e3c897df9bbdc2fd32f99e5d9fa2ffadff0820e8\",\"title\":\"Deep Learning for Load Forecasting: Sequence to Sequence Recurrent Neural Networks With Attention\",\"url\":\"https://www.semanticscholar.org/paper/e3c897df9bbdc2fd32f99e5d9fa2ffadff0820e8\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"title\":\"Beyond Labels and Captions: Contextualizing Grounded Semantics for Explainable Visual Interpretation\",\"url\":\"https://www.semanticscholar.org/paper/267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"146022214\",\"name\":\"Amel Ben\\u00a0Mahjoub\"},{\"authorId\":\"40406356\",\"name\":\"Mohamed Atri\"}],\"doi\":\"10.1007/S10470-018-1306-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96feefcb2d6aa7f91da50c255f0a5c4792e27d66\",\"title\":\"An efficient end-to-end deep learning architecture for activity classification\",\"url\":\"https://www.semanticscholar.org/paper/96feefcb2d6aa7f91da50c255f0a5c4792e27d66\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1980311586\",\"name\":\"Yiming Han\"},{\"authorId\":\"34743161\",\"name\":\"Ramakrishnan Sundaram\"}],\"doi\":\"10.1117/12.2567520\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"74a302691aa65c487a2c89c24fc0f1dae7ddac45\",\"title\":\"Video analysis methods to track motion across frames\",\"url\":\"https://www.semanticscholar.org/paper/74a302691aa65c487a2c89c24fc0f1dae7ddac45\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1876419\",\"name\":\"X. Li\"},{\"authorId\":\"26950695\",\"name\":\"Chensi Mao\"},{\"authorId\":\"2287648\",\"name\":\"S. Huang\"},{\"authorId\":\"2066033\",\"name\":\"Z. Ye\"}],\"doi\":\"10.1007/978-3-319-69923-3_77\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a8369d829dc7dc30426611e37476162e190a09e\",\"title\":\"Chinese Sign Language Recognition Based on SHS Descriptor and Encoder-Decoder LSTM Model\",\"url\":\"https://www.semanticscholar.org/paper/7a8369d829dc7dc30426611e37476162e190a09e\",\"venue\":\"CCBR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"}],\"doi\":\"10.1145/2903513.2903517\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"968ab65077c4be1c1071120052b2e4b4f3d3c59a\",\"title\":\"\\\"Seeing is believing: the quest for multimodal knowledge\\\" by Gerard de Melo and Niket Tandon, with Martin Vesely as coordinator\",\"url\":\"https://www.semanticscholar.org/paper/968ab65077c4be1c1071120052b2e4b4f3d3c59a\",\"venue\":\"LINK\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51312029\",\"name\":\"Yu-Sheng Chou\"},{\"authorId\":\"2042119\",\"name\":\"Pai-Heng Hsiao\"},{\"authorId\":\"2818798\",\"name\":\"S. Lin\"},{\"authorId\":\"1704678\",\"name\":\"H. Liao\"}],\"doi\":\"10.1109/ICASSP.2018.8461899\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ceb67253b0ba134dd1c8b87a6be4e4bf507d35b\",\"title\":\"How Sampling Rate Affects Cross-Domain Transfer Learning for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/4ceb67253b0ba134dd1c8b87a6be4e4bf507d35b\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"},{\"authorId\":\"2000709\",\"name\":\"Y. Graham\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"144620851\",\"name\":\"S. Quinn\"},{\"authorId\":\"40063957\",\"name\":\"Eric Arazo Sanchez\"}],\"doi\":\"10.1007/978-3-030-05710-7_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f6066f9ab66c10b038f2afffb621b9db8042a4ed\",\"title\":\"Exploring the Impact of Training Data Bias on Automatic Generation of Video Captions\",\"url\":\"https://www.semanticscholar.org/paper/f6066f9ab66c10b038f2afffb621b9db8042a4ed\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151487400\",\"name\":\"Chu-yi Li\"},{\"authorId\":\"9319341\",\"name\":\"Wei-yu Yu\"}],\"doi\":\"10.1117/12.2514651\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ddbc1542476237b6ace7b871e34269e790d35bad\",\"title\":\"Spatial-temporal attention in Bi-LSTM networks based on multiple features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ddbc1542476237b6ace7b871e34269e790d35bad\",\"venue\":\"Other Conferences\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"1752427\",\"name\":\"Jiansheng Chen\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"47718901\",\"name\":\"Weitao Wan\"},{\"authorId\":\"153447481\",\"name\":\"Youze Xue\"}],\"doi\":\"10.1109/TIP.2020.2969330\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff5a1ec81f327b711a5433e4cd40467215a13f39\",\"title\":\"Image Captioning With End-to-End Attribute Detection and Subsequent Attributes Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ff5a1ec81f327b711a5433e4cd40467215a13f39\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/s11263-017-1033-7\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"828ac57f755db989e2886042a85278ae4823297c\",\"title\":\"Uncovering the Temporal Context for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/828ac57f755db989e2886042a85278ae4823297c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1803.11438\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"37378985\",\"name\":\"Wei Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2018.00795\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"title\":\"Reconstruction Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1778215\",\"name\":\"Swagath Venkataramani\"},{\"authorId\":\"2912041\",\"name\":\"X. Sun\"},{\"authorId\":\"1862823\",\"name\":\"Naigang Wang\"},{\"authorId\":\"48240124\",\"name\":\"Chia-Yu Chen\"},{\"authorId\":\"2027658626\",\"name\":\"Jungwook Choi\"},{\"authorId\":\"34875156\",\"name\":\"Mingu Kang\"},{\"authorId\":\"2027665918\",\"name\":\"Ankur Agarwal\"},{\"authorId\":\"3216445\",\"name\":\"Jinwook Oh\"},{\"authorId\":\"2027665802\",\"name\":\"Shubham Jain\"},{\"authorId\":\"2340794\",\"name\":\"Tina Babinsky\"},{\"authorId\":\"3283870\",\"name\":\"N. Cao\"},{\"authorId\":\"22228241\",\"name\":\"T. Fox\"},{\"authorId\":\"48180281\",\"name\":\"B. Fleischer\"},{\"authorId\":\"3005391\",\"name\":\"G. Gristede\"},{\"authorId\":\"145890078\",\"name\":\"M. Guillorn\"},{\"authorId\":\"51145561\",\"name\":\"Howard Haynie\"},{\"authorId\":\"2379649\",\"name\":\"H. Inoue\"},{\"authorId\":\"34806932\",\"name\":\"K. Ishizaki\"},{\"authorId\":\"46365024\",\"name\":\"Michael J. Klaiber\"},{\"authorId\":\"1853576\",\"name\":\"S. Lo\"},{\"authorId\":\"2314010\",\"name\":\"G. Maier\"},{\"authorId\":\"34634490\",\"name\":\"Silvia M. M\\u00fcller\"},{\"authorId\":\"46743319\",\"name\":\"M. Scheuermann\"},{\"authorId\":\"120814756\",\"name\":\"Eri Ogawa\"},{\"authorId\":\"3004738\",\"name\":\"M. Schaal\"},{\"authorId\":\"144768294\",\"name\":\"M. Serrano\"},{\"authorId\":\"32525064\",\"name\":\"J. Silberman\"},{\"authorId\":\"3029075\",\"name\":\"Christos Vezyrtzis\"},{\"authorId\":\"46314444\",\"name\":\"W. Wang\"},{\"authorId\":\"48781052\",\"name\":\"F. Yee\"},{\"authorId\":\"2028035406\",\"name\":\"Jintao Zhang\"},{\"authorId\":\"31690896\",\"name\":\"Matthew M. Ziegler\"},{\"authorId\":\"3342189\",\"name\":\"Ching Zhou\"},{\"authorId\":\"145140637\",\"name\":\"M. Ohara\"},{\"authorId\":\"1975663\",\"name\":\"Pong-Fei Lu\"},{\"authorId\":\"145985672\",\"name\":\"B. Curran\"},{\"authorId\":\"144162226\",\"name\":\"S. Shukla\"},{\"authorId\":\"145941003\",\"name\":\"V. Srinivasan\"},{\"authorId\":\"39613278\",\"name\":\"L. Chang\"},{\"authorId\":\"33678523\",\"name\":\"K. Gopalakrishnan\"}],\"doi\":\"10.1109/JPROC.2020.3029453\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"725f4cea24d7014117916cefbe20452119f1eac5\",\"title\":\"Efficient AI System Design With Cross-Layer Approximate Computing\",\"url\":\"https://www.semanticscholar.org/paper/725f4cea24d7014117916cefbe20452119f1eac5\",\"venue\":\"Proceedings of the IEEE\",\"year\":2020},{\"arxivId\":\"1805.08191\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"38504661\",\"name\":\"J. Wang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":\"10.1609/aaai.v33i01.33018465\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b02822cfbc50d17ec5220a19556be9d601c132\",\"title\":\"Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation\",\"url\":\"https://www.semanticscholar.org/paper/c2b02822cfbc50d17ec5220a19556be9d601c132\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1805.09461\",\"authors\":[{\"authorId\":\"2180949\",\"name\":\"Yaser Keneshloo\"},{\"authorId\":\"145531789\",\"name\":\"Tian Shi\"},{\"authorId\":\"1755938\",\"name\":\"Naren Ramakrishnan\"},{\"authorId\":\"144417522\",\"name\":\"C. Reddy\"}],\"doi\":\"10.1109/TNNLS.2019.2929141\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"15a06d8601539b5eb6df5baf6bc4c3bdefb34855\",\"title\":\"Deep Reinforcement Learning for Sequence-to-Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/15a06d8601539b5eb6df5baf6bc4c3bdefb34855\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":\"1711.06354\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"title\":\"Grounded Objects and Interactions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97667801\",\"name\":\"Matthew John Marter\"}],\"doi\":\"10.15126/THESIS.00850052\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a0f45a806a38bf38511a9f2fd774d7a18d0fc3b3\",\"title\":\"Learning to recognise visual content from textual annotation\",\"url\":\"https://www.semanticscholar.org/paper/a0f45a806a38bf38511a9f2fd774d7a18d0fc3b3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2006.16423\",\"authors\":[{\"authorId\":\"2220760\",\"name\":\"J. Tarnawski\"},{\"authorId\":\"3078275\",\"name\":\"Amar Phanishayee\"},{\"authorId\":\"7692691\",\"name\":\"Nikhil R. Devanur\"},{\"authorId\":\"33278013\",\"name\":\"Divya Mahajan\"},{\"authorId\":\"2380663\",\"name\":\"Fanny Nina Paravecino\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7208279034eba55f9d4aaab9525f5484b434d019\",\"title\":\"Efficient Algorithms for Device Placement of DNN Graph Operators\",\"url\":\"https://www.semanticscholar.org/paper/7208279034eba55f9d4aaab9525f5484b434d019\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1612.06152\",\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1109/CVPR.2017.569\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"91a59243c248478bc1f0f6542284eaa3c19f1ece\",\"title\":\"Few-Shot Object Recognition from Machine-Labeled Web Images\",\"url\":\"https://www.semanticscholar.org/paper/91a59243c248478bc1f0f6542284eaa3c19f1ece\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1704.04224\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1109/ICCV.2017.440\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2c00deb3774bcadf8f5e64ab48f483f06be56bc1\",\"title\":\"Spatial Memory for Context Reasoning in Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/2c00deb3774bcadf8f5e64ab48f483f06be56bc1\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1702.05658\",\"authors\":[{\"authorId\":null,\"name\":\"Chang Liu\"},{\"authorId\":\"143823065\",\"name\":\"F. Sun\"},{\"authorId\":\"1697065\",\"name\":\"C. Wang\"},{\"authorId\":\"39369840\",\"name\":\"Feng Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.24963/ijcai.2017/563\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2498124e6466ccde28c95477c923e7cd5843f4c0\",\"title\":\"MAT: A Multimodal Attentive Translator for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2498124e6466ccde28c95477c923e7cd5843f4c0\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"1611.08669\",\"authors\":[{\"authorId\":\"145497716\",\"name\":\"A. Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"39855500\",\"name\":\"K. Gupta\"},{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"},{\"authorId\":\"24508084\",\"name\":\"Deshraj Yadav\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/CVPR.2017.121\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2231f44be9a8472a46d8e8a628b4e52b9a8f44e0\",\"title\":\"Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/2231f44be9a8472a46d8e8a628b4e52b9a8f44e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3068677\",\"name\":\"Iacer Calixto\"},{\"authorId\":\"1688015\",\"name\":\"Qun Liu\"}],\"doi\":\"10.1007/s10590-019-09226-9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d62c17e052905fbb149cc7ddf14b1d420ea4bfa3\",\"title\":\"An error analysis for image-based multi-modal neural machine translation\",\"url\":\"https://www.semanticscholar.org/paper/d62c17e052905fbb149cc7ddf14b1d420ea4bfa3\",\"venue\":\"Machine Translation\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29724362\",\"name\":\"Longlong Jing\"},{\"authorId\":\"38101706\",\"name\":\"Xiaodong Yang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1016/j.jvcir.2018.01.016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e4b43fd34e14a476ac135bbfd5532a2d0cc50f3\",\"title\":\"Video you only look once: Overall temporal convolutions for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/1e4b43fd34e14a476ac135bbfd5532a2d0cc50f3\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2017.2746267\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1b94c49c119c7490d2df6a2dd093e5ddd8bfba14\",\"title\":\"Unifying the Video and Question Attentions for Open-Ended Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1b94c49c119c7490d2df6a2dd093e5ddd8bfba14\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"09926ed62511c340f4540b5bc53cf2480e8063f8\",\"title\":\"Tubelet Detector for Spatio-Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/09926ed62511c340f4540b5bc53cf2480e8063f8\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"title\":\"Empirical performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"Li Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"117aae1dc5b3aee679a690f7dab84e9a23add930\",\"title\":\"AGE AND VIDEO CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/117aae1dc5b3aee679a690f7dab84e9a23add930\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"2248826\",\"name\":\"R. Hong\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TIP.2018.2846664\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7fe2ab9f54242ef8609ef9bf988f008c7d42407c\",\"title\":\"Sequential Video VLAD: Training the Aggregation Locally and Temporally\",\"url\":\"https://www.semanticscholar.org/paper/7fe2ab9f54242ef8609ef9bf988f008c7d42407c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1808.09012\",\"authors\":[{\"authorId\":\"22168594\",\"name\":\"Hareesh Bahuleyan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0923e5650189836a815b03ada6a20338609bce9b\",\"title\":\"Natural Language Generation with Neural Variational Models\",\"url\":\"https://www.semanticscholar.org/paper/0923e5650189836a815b03ada6a20338609bce9b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2001.01248\",\"authors\":[{\"authorId\":\"144934401\",\"name\":\"M. Monforte\"},{\"authorId\":\"2132505\",\"name\":\"Ander Arriandiaga\"},{\"authorId\":\"2763635\",\"name\":\"A. Glover\"},{\"authorId\":\"1897771\",\"name\":\"C. Bartolozzi\"}],\"doi\":\"10.1109/AICAS48895.2020.9073855\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e15a78c9c7a3ac2f21260e33fef29a112e1e5e63\",\"title\":\"Exploiting Event Cameras for Spatio-Temporal Prediction of Fast-Changing Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/e15a78c9c7a3ac2f21260e33fef29a112e1e5e63\",\"venue\":\"2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26405229\",\"name\":\"Garrett Wilson\"},{\"authorId\":\"144758070\",\"name\":\"D. Cook\"}],\"doi\":\"10.1017/9781139061773.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a5f636fb827219ee8f0d49db0d2810db7a8a362\",\"title\":\"Adversarial Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/1a5f636fb827219ee8f0d49db0d2810db7a8a362\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145742542\",\"name\":\"W. Li\"},{\"authorId\":\"144536247\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/ACCESS.2018.2863943\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd22e6532211f679ba6057d15a801ba448b9915c\",\"title\":\"Human Action Recognition Based on Selected Spatio-Temporal Features via Bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/cd22e6532211f679ba6057d15a801ba448b9915c\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35664579\",\"name\":\"Sahil Chelaramani\"},{\"authorId\":\"35391990\",\"name\":\"Vamsidhar Muthireddy\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1109/ICCVW.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6259824432274c8c01ad837b74354a5415e0c00f\",\"title\":\"An Interactive Tour Guide for a Heritage Site\",\"url\":\"https://www.semanticscholar.org/paper/6259824432274c8c01ad837b74354a5415e0c00f\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"9515493\",\"name\":\"Liangfu Cao\"},{\"authorId\":\"48669907\",\"name\":\"X. Xu\"},{\"authorId\":\"145496508\",\"name\":\"J. Shao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1016/J.NEUCOM.2018.11.102\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be63949be4151ed73503b3eb218aa9175233661b\",\"title\":\"Question-Led object attention for visual question answering\",\"url\":\"https://www.semanticscholar.org/paper/be63949be4151ed73503b3eb218aa9175233661b\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"2002.03187\",\"authors\":[{\"authorId\":null,\"name\":\"Hao Zhou\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"24520518\",\"name\":\"Y. Zhou\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"}],\"doi\":\"10.1609/AAAI.V34I07.7001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a1e2665ac39dcb389e12f3f993004b4b4651826d\",\"title\":\"Spatial-Temporal Multi-Cue Network for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a1e2665ac39dcb389e12f3f993004b4b4651826d\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144620591\",\"name\":\"X. Wu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"2523380\",\"name\":\"Qingge Ji\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/CVPR.2018.00714\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"title\":\"Interpretable Video Captioning via Trajectory Structured Localization\",\"url\":\"https://www.semanticscholar.org/paper/f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1910.07482\",\"authors\":[{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"151480727\",\"name\":\"Zixiu Wu\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"2635321\",\"name\":\"Josiah Wang\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"37f371fe04f3dc38df7f27f43277ba15d3637890\",\"title\":\"Imperial College London Submission to VATEX Video Captioning Task\",\"url\":\"https://www.semanticscholar.org/paper/37f371fe04f3dc38df7f27f43277ba15d3637890\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"1693997\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1145/3240508.3240538\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"72f9116a04e584081635500e9f0789fa26e4d15f\",\"title\":\"Hierarchical Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72f9116a04e584081635500e9f0789fa26e4d15f\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115606612\",\"name\":\"Tianyang Lin\"},{\"authorId\":\"39870808\",\"name\":\"Q. Li\"},{\"authorId\":\"8010931\",\"name\":\"Yangli-ao Geng\"},{\"authorId\":\"98582406\",\"name\":\"Lei Jiang\"},{\"authorId\":\"102220923\",\"name\":\"Liangtao Xu\"},{\"authorId\":\"145299101\",\"name\":\"Dong Zheng\"},{\"authorId\":\"49726963\",\"name\":\"Wenbing Yao\"},{\"authorId\":\"52238338\",\"name\":\"W. Lyu\"},{\"authorId\":\"39293150\",\"name\":\"Yijun Zhang\"}],\"doi\":\"10.1109/ACCESS.2019.2950328\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f82c0b2ca9d3ce40ad867bb5b0a3e93c7517b82\",\"title\":\"Attention-Based Dual-Source Spatiotemporal Neural Network for Lightning Forecast\",\"url\":\"https://www.semanticscholar.org/paper/3f82c0b2ca9d3ce40ad867bb5b0a3e93c7517b82\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74437294\",\"name\":\"Sk. Arif Ahmed\"},{\"authorId\":\"3320759\",\"name\":\"D. P. Dogra\"},{\"authorId\":\"32614479\",\"name\":\"S. Kar\"},{\"authorId\":\"40813600\",\"name\":\"P. Roy\"}],\"doi\":\"10.1007/978-981-10-7590-2_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"522b13ea02d6d62e54180bd13595eb0e40333d48\",\"title\":\"Natural Language Description of Surveillance Events\",\"url\":\"https://www.semanticscholar.org/paper/522b13ea02d6d62e54180bd13595eb0e40333d48\",\"venue\":\"ICITAM\",\"year\":2017},{\"arxivId\":\"1911.01258\",\"authors\":[{\"authorId\":\"151197449\",\"name\":\"R. Yazdani\"},{\"authorId\":\"2537545\",\"name\":\"Olatunji Ruwase\"},{\"authorId\":\"67016465\",\"name\":\"Minjia Zhang\"},{\"authorId\":\"1772774\",\"name\":\"Yuxiong He\"},{\"authorId\":\"3084705\",\"name\":\"J. Arnau\"},{\"authorId\":\"1384125624\",\"name\":\"A. Gonz\\u00e1lez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"092d0ba602a8275254c0ac5718bf176526b12eaa\",\"title\":\"LSTM-Sharp: An Adaptable, Energy-Efficient Hardware Accelerator for Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/092d0ba602a8275254c0ac5718bf176526b12eaa\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38958673\",\"name\":\"Shu-Mei Tseng\"},{\"authorId\":\"145925450\",\"name\":\"Bogdan Nicolae\"},{\"authorId\":\"2556809\",\"name\":\"G. Bosilca\"},{\"authorId\":\"1795494\",\"name\":\"E. Jeannot\"},{\"authorId\":\"3054091\",\"name\":\"Aparna Chandramowlishwaran\"},{\"authorId\":\"1721552\",\"name\":\"F. Cappello\"}],\"doi\":\"10.1007/978-3-030-29400-7_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"668ac3b4af52cc122e2a3e4242293211f27fc831\",\"title\":\"Towards Portable Online Prediction of Network Utilization Using MPI-Level Monitoring\",\"url\":\"https://www.semanticscholar.org/paper/668ac3b4af52cc122e2a3e4242293211f27fc831\",\"venue\":\"Euro-Par\",\"year\":2019},{\"arxivId\":\"1908.10072\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"67074535\",\"name\":\"W. Zhang\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"46641690\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/ICCV.2019.00273\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"title\":\"Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"},{\"authorId\":\"50763020\",\"name\":\"Jingwen Chen\"},{\"authorId\":\"41079034\",\"name\":\"Hong-Yang Chao\"},{\"authorId\":\"2458059\",\"name\":\"Y. Huang\"},{\"authorId\":\"1394465427\",\"name\":\"Qiuyu Cai\"}],\"doi\":\"10.1145/3394171.3416290\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93aed487e9b9f51bf05803ef69c92599001358ac\",\"title\":\"XlanV Model with Adaptively Multi-Modality Feature Fusing for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/93aed487e9b9f51bf05803ef69c92599001358ac\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2301765\",\"name\":\"Tsung-Wei Ke\"},{\"authorId\":\"3172276\",\"name\":\"Che-Wei Lin\"},{\"authorId\":\"1805102\",\"name\":\"Tyng-Luh Liu\"},{\"authorId\":\"14489533\",\"name\":\"D. Geiger\"}],\"doi\":\"10.1007/978-3-319-54190-7_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1df74a5047e953766fa07dec356bba285c605a1\",\"title\":\"Variational Convolutional Networks for Human-Centric Annotations\",\"url\":\"https://www.semanticscholar.org/paper/d1df74a5047e953766fa07dec356bba285c605a1\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.1145/3347450.3357660\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fdab9c73255869d9c681e41aa3963991159d946d\",\"title\":\"Connecting Language and Vision: From Captioning towards Embodied Learning\",\"url\":\"https://www.semanticscholar.org/paper/fdab9c73255869d9c681e41aa3963991159d946d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.00120\",\"authors\":[{\"authorId\":\"2904100\",\"name\":\"Z. Han\"},{\"authorId\":\"102574232\",\"name\":\"C. Chen\"},{\"authorId\":\"46399275\",\"name\":\"Yu-Shen Liu\"},{\"authorId\":\"1796846\",\"name\":\"Matthias Zwicker\"}],\"doi\":\"10.1145/3394171.3413889\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66e87d0adf7df03fdd74e6dfea81f21aa03afab0\",\"title\":\"ShapeCaptioner: Generative Caption Network for 3D Shapes by Learning a Mapping from Parts Detected in Multiple Views to Sentences\",\"url\":\"https://www.semanticscholar.org/paper/66e87d0adf7df03fdd74e6dfea81f21aa03afab0\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.01816\",\"authors\":[{\"authorId\":null,\"name\":\"Yizhou Wang\"},{\"authorId\":\"49165505\",\"name\":\"Zhongyu Jiang\"},{\"authorId\":\"49779773\",\"name\":\"Xiangyu Gao\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"3232358\",\"name\":\"Guanbin Xing\"},{\"authorId\":\"72217143\",\"name\":\"H. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c84e990a5b1b63408039e47e696a66eb4991e29\",\"title\":\"RODNet: Object Detection under Severe Conditions Using Vision-Radio Cross-Modal Supervision\",\"url\":\"https://www.semanticscholar.org/paper/7c84e990a5b1b63408039e47e696a66eb4991e29\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49019533\",\"name\":\"Ying Wei\"}],\"doi\":\"10.14711/thesis-991012554867303412\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"46d55d5f6d501d2b7600e5973e90ace9b38bf6da\",\"title\":\"Heterogeneous transfer learning\",\"url\":\"https://www.semanticscholar.org/paper/46d55d5f6d501d2b7600e5973e90ace9b38bf6da\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e0dedb6fc4d370f4399bf7d67e234dc44deb4333\",\"title\":\"Material : MultiTask Video Captioning with Video and Entailment Generation\",\"url\":\"https://www.semanticscholar.org/paper/e0dedb6fc4d370f4399bf7d67e234dc44deb4333\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50178601\",\"name\":\"Shiliang Huang\"},{\"authorId\":\"26950695\",\"name\":\"Chensi Mao\"},{\"authorId\":\"2768286\",\"name\":\"Jinxu Tao\"},{\"authorId\":\"2066033\",\"name\":\"Z. Ye\"}],\"doi\":\"10.1109/LSP.2018.2797228\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"755188a20f6bf67b8250aed35636ae7938674031\",\"title\":\"A Novel Chinese Sign Language Recognition Method Based on Keyframe-Centered Clips\",\"url\":\"https://www.semanticscholar.org/paper/755188a20f6bf67b8250aed35636ae7938674031\",\"venue\":\"IEEE Signal Processing Letters\",\"year\":2018},{\"arxivId\":\"1811.07631\",\"authors\":[{\"authorId\":\"48028532\",\"name\":\"Lili Yao\"},{\"authorId\":\"31834306\",\"name\":\"Ruijian Xu\"},{\"authorId\":\"46651287\",\"name\":\"C. Li\"},{\"authorId\":\"144060462\",\"name\":\"Dongyan Zhao\"},{\"authorId\":\"144539156\",\"name\":\"R. Yan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"415cba771d466f959161b793c3cd0e7a1db62ae2\",\"title\":\"Chat More If You Like: Dynamic Cue Words Planning to Flow Longer Conversations\",\"url\":\"https://www.semanticscholar.org/paper/415cba771d466f959161b793c3cd0e7a1db62ae2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49551061\",\"name\":\"Shubham Singh\"},{\"authorId\":\"40339392\",\"name\":\"R. Kaushal\"},{\"authorId\":\"3170352\",\"name\":\"A. Buduru\"},{\"authorId\":\"1734731\",\"name\":\"P. Kumaraguru\"}],\"doi\":\"10.1145/3297280.3297487\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6a9bfb8ea06d011900d7facc93ee77e5733b8870\",\"title\":\"KidsGUARD: fine grained approach for child unsafe video representation and detection\",\"url\":\"https://www.semanticscholar.org/paper/6a9bfb8ea06d011900d7facc93ee77e5733b8870\",\"venue\":\"SAC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70043725\",\"name\":\"\\u00deorgeir Au\\u00f0unn Karlsson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a66ff6cb7e0ae6dcf79f22ad1916141159a18720\",\"title\":\"Epidemiological surveillance through cellphone metadata\",\"url\":\"https://www.semanticscholar.org/paper/a66ff6cb7e0ae6dcf79f22ad1916141159a18720\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46179701\",\"name\":\"Yicun Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cfe78c14c82d69d3d2888b41767e513ce21e02a9\",\"title\":\"Understanding Cultural Differences in News Videos\",\"url\":\"https://www.semanticscholar.org/paper/cfe78c14c82d69d3d2888b41767e513ce21e02a9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1912.05911\",\"authors\":[{\"authorId\":\"1456064291\",\"name\":\"Robin M. Schmidt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c8a6ab02a048de36aa11bd03ca80203e34de359\",\"title\":\"Recurrent Neural Networks (RNNs): A gentle Introduction and Overview\",\"url\":\"https://www.semanticscholar.org/paper/9c8a6ab02a048de36aa11bd03ca80203e34de359\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46179701\",\"name\":\"Yicun Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d44b837b8f9e1f03d6075059e1c07381c61576d\",\"title\":\"Tagging and Browsing Videos According to the Preferences of Differing Affinity Groups\",\"url\":\"https://www.semanticscholar.org/paper/7d44b837b8f9e1f03d6075059e1c07381c61576d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46419391\",\"name\":\"M. Elfeki\"},{\"authorId\":\"3377097\",\"name\":\"A. Sharghi\"},{\"authorId\":\"1976152\",\"name\":\"Srikrishna Karanam\"},{\"authorId\":\"3311781\",\"name\":\"Z. Wu\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c436d547d1656c938aabfd69f6c34aaeccd6aeb0\",\"title\":\"Multi-Stream Dynamic Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/c436d547d1656c938aabfd69f6c34aaeccd6aeb0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144196376\",\"name\":\"Z. Fang\"},{\"authorId\":\"2505157\",\"name\":\"Dezhi Hong\"},{\"authorId\":\"145170139\",\"name\":\"R. Gupta\"}],\"doi\":\"10.1145/3304109.3306221\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"01fa5eb436db99529af2ab69722416afcb03d03e\",\"title\":\"Serving deep neural networks at the cloud edge for vision applications on mobile platforms\",\"url\":\"https://www.semanticscholar.org/paper/01fa5eb436db99529af2ab69722416afcb03d03e\",\"venue\":\"MMSys\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"51043468\",\"name\":\"Maksim Bolonkin\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7ba0bf9323c2d79300f1a433ff8b4fe0a00ad889\",\"title\":\"VideoMCC: a New Benchmark for Video Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/7ba0bf9323c2d79300f1a433ff8b4fe0a00ad889\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chang Liu\"},{\"authorId\":\"3045152\",\"name\":\"Mark Last\"},{\"authorId\":\"2887672\",\"name\":\"A. Shmilovici\"}],\"doi\":\"10.1016/J.ESWA.2019.01.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d360eb1162e4fc4013dc9d06c665544f164cdb5b\",\"title\":\"Identifying turning points in animated cartoons\",\"url\":\"https://www.semanticscholar.org/paper/d360eb1162e4fc4013dc9d06c665544f164cdb5b\",\"venue\":\"Expert Syst. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102227052\",\"name\":\"J. A. Le\"},{\"authorId\":\"1411510561\",\"name\":\"H. el-Askary\"},{\"authorId\":\"1411510561\",\"name\":\"H. el-Askary\"},{\"authorId\":\"46503536\",\"name\":\"M. Allali\"},{\"authorId\":\"1940439\",\"name\":\"D. Struppa\"}],\"doi\":\"10.1016/J.ATMOSRES.2017.01.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f93d80115b79d9ba1e4644d4e003c41245eacde8\",\"title\":\"Application of Recurrent Neural Networks for Drought Projections in California\",\"url\":\"https://www.semanticscholar.org/paper/f93d80115b79d9ba1e4644d4e003c41245eacde8\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118150711\",\"name\":\"J. Liang\"},{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"2319973\",\"name\":\"Po-Yao Huang\"},{\"authorId\":\"2314980\",\"name\":\"X. Li\"},{\"authorId\":\"1481734737\",\"name\":\"Lu Jiang\"},{\"authorId\":\"34692532\",\"name\":\"Zhenzhong Lan\"},{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"51299154\",\"name\":\"Jiande Sun\"},{\"authorId\":\"145906066\",\"name\":\"Yang Chen\"},{\"authorId\":\"79327094\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b9ce2f554881a921557f8e9f47e1249c07027c0\",\"title\":\"Informedia @ TRECVID 2016\",\"url\":\"https://www.semanticscholar.org/paper/4b9ce2f554881a921557f8e9f47e1249c07027c0\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":\"1912.12655\",\"authors\":[{\"authorId\":\"3459502\",\"name\":\"Washington L. S. Ramos\"},{\"authorId\":\"3461833\",\"name\":\"M. M. Silva\"},{\"authorId\":\"51929166\",\"name\":\"Edson Roteia Araujo Junior\"},{\"authorId\":\"3360914\",\"name\":\"Alan C. Neves\"},{\"authorId\":\"2310152\",\"name\":\"Erickson R. Nascimento\"}],\"doi\":\"10.1109/WACV45572.2020.9093330\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0155df608af9c0024c6d557b16bdc15af78c5133\",\"title\":\"Personalizing Fast-Forward Videos Based on Visual and Textual Features from Social Network\",\"url\":\"https://www.semanticscholar.org/paper/0155df608af9c0024c6d557b16bdc15af78c5133\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1608.04959\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1145/2964284.2984062\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41eec260e0980f8fd0af46f0dfc043139087a928\",\"title\":\"Frame- and Segment-Level Features and Candidate Pool Evaluation for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/41eec260e0980f8fd0af46f0dfc043139087a928\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2269558\",\"name\":\"Q. Li\"},{\"authorId\":\"144854843\",\"name\":\"X. Zhao\"},{\"authorId\":\"143712929\",\"name\":\"R. He\"},{\"authorId\":\"2887871\",\"name\":\"K. Huang\"}],\"doi\":\"10.1109/TCSVT.2019.2923444\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ff61d933f95bab4d0390b0372b0e6f1829251c6\",\"title\":\"Recurrent Prediction With Spatio-Temporal Attention for Crowd Attribute Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9ff61d933f95bab4d0390b0372b0e6f1829251c6\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1702.01287\",\"authors\":[{\"authorId\":\"3068677\",\"name\":\"Iacer Calixto\"},{\"authorId\":\"1688015\",\"name\":\"Qun Liu\"},{\"authorId\":\"143731137\",\"name\":\"N. Campbell\"}],\"doi\":\"10.18653/v1/P17-1175\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b3f6fab90f9592662d2dc21cb650226b3ca3166\",\"title\":\"Doubly-Attentive Decoder for Multi-modal Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/2b3f6fab90f9592662d2dc21cb650226b3ca3166\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":null,\"name\":\"Jing Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1016/j.jvcir.2018.12.027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"58323b889d916403c6674eb3112370a13bd63fe9\",\"title\":\"Scene graph captioner: Image captioning based on structural visual representation\",\"url\":\"https://www.semanticscholar.org/paper/58323b889d916403c6674eb3112370a13bd63fe9\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9422489\",\"name\":\"Shuyan Li\"},{\"authorId\":\"1723853\",\"name\":\"Z. Chen\"},{\"authorId\":\"121856633\",\"name\":\"Xiu Li\"},{\"authorId\":\"100475213\",\"name\":\"Jiwen Lu\"},{\"authorId\":\"144535460\",\"name\":\"J. Zhou\"}],\"doi\":\"10.1109/TMM.2019.2946096\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c7aa18a724edb175952c7820fd4838751b75f48a\",\"title\":\"Unsupervised Variational Video Hashing With 1D-CNN-LSTM Networks\",\"url\":\"https://www.semanticscholar.org/paper/c7aa18a724edb175952c7820fd4838751b75f48a\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50846763\",\"name\":\"W. Zhang\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"1993631009\",\"name\":\"Haizhou Shi\"},{\"authorId\":\"1490931889\",\"name\":\"Haochen Shi\"},{\"authorId\":\"145974114\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"49336516\",\"name\":\"W. Wang\"}],\"doi\":\"10.1145/3394171.3413746\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fa8f78e665c037da28a7e35a98c4d2521e3f12e\",\"title\":\"Relational Graph Learning for Grounded Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/9fa8f78e665c037da28a7e35a98c4d2521e3f12e\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1908.06327\",\"authors\":[{\"authorId\":\"38727845\",\"name\":\"A. Burns\"},{\"authorId\":\"73441526\",\"name\":\"R. Tan\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"}],\"doi\":\"10.1109/ICCV.2019.00757\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"71634edd80ce000b3d1e462137fcfa8c2b377943\",\"title\":\"Language Features Matter: Effective Language Representations for Vision-Language Tasks\",\"url\":\"https://www.semanticscholar.org/paper/71634edd80ce000b3d1e462137fcfa8c2b377943\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1602.01921\",\"authors\":[{\"authorId\":\"3374519\",\"name\":\"H. Lee\"},{\"authorId\":\"144248119\",\"name\":\"Minju Jung\"},{\"authorId\":\"1780524\",\"name\":\"J. Tani\"}],\"doi\":\"10.1109/tcds.2017.2768422\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2002456c4789e763c2c37f1dd159025e7253f9d7\",\"title\":\"Recognition of Visually Perceived Compositional Human Actions by Multiple Spatio-Temporal Scales Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/2002456c4789e763c2c37f1dd159025e7253f9d7\",\"venue\":\"IEEE Transactions on Cognitive and Developmental Systems\",\"year\":2018},{\"arxivId\":\"1805.08545\",\"authors\":[{\"authorId\":\"3116195\",\"name\":\"Arturo Marb\\u00e1n\"},{\"authorId\":\"4365900\",\"name\":\"V. Srinivasan\"},{\"authorId\":\"1699054\",\"name\":\"W. Samek\"},{\"authorId\":\"122748666\",\"name\":\"J. Fern\\u00e1ndez\"},{\"authorId\":\"144921705\",\"name\":\"A. Casals\"}],\"doi\":\"10.1016/j.bspc.2019.01.011\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf27b16f28496d66df75edafc0fb3fa6da87742b\",\"title\":\"A Recurrent Convolutional Neural Network Approach for Sensorless Force Estimation in Robotic Surgery\",\"url\":\"https://www.semanticscholar.org/paper/bf27b16f28496d66df75edafc0fb3fa6da87742b\",\"venue\":\"Biomed. Signal Process. Control.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3239379\",\"name\":\"J. Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1007/978-3-030-05710-7_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"title\":\"Hierarchical Vision-Language Alignment for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740039\",\"name\":\"Jie Chen\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"2838253\",\"name\":\"C. He\"}],\"doi\":\"10.1016/J.PATREC.2018.06.030\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c93fcf73554f2cb9e0bc21da82f9611ae631f55\",\"title\":\"Movie fill in the blank by joint learning from video and text with adaptive temporal attention\",\"url\":\"https://www.semanticscholar.org/paper/3c93fcf73554f2cb9e0bc21da82f9611ae631f55\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1724431\",\"name\":\"D. Amanatidis\"},{\"authorId\":\"2283824\",\"name\":\"Michael F. Dossis\"},{\"authorId\":\"2139316\",\"name\":\"I. Mylona\"}],\"doi\":\"10.1109/SEEDA-CECNSM.2019.8908466\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4bc184acb290aaf43dc93b81570d9cbf6ba4481\",\"title\":\"A Convolutional Neural Network for Sentiment Analysis of TripAdvisor reviews\",\"url\":\"https://www.semanticscholar.org/paper/c4bc184acb290aaf43dc93b81570d9cbf6ba4481\",\"venue\":\"2019 4th South-East Europe Design Automation, Computer Engineering, Computer Networks and Social Media Conference (SEEDA-CECNSM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145910244\",\"name\":\"Bin Tong\"},{\"authorId\":\"2411436\",\"name\":\"Martin Klinkigt\"},{\"authorId\":\"2144702\",\"name\":\"M. Iwayama\"},{\"authorId\":\"144149950\",\"name\":\"T. Yanase\"},{\"authorId\":\"10249589\",\"name\":\"Y. Kobayashi\"},{\"authorId\":\"2532559\",\"name\":\"A. Sahu\"},{\"authorId\":\"2774676\",\"name\":\"Ravigopal Vennelakanti\"}],\"doi\":\"10.1145/3097983.3098132\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9879da626ea07cc27e588943e5f0d37898812adb\",\"title\":\"Learning to Generate Rock Descriptions from Multivariate Well Logs with Hierarchical Attention\",\"url\":\"https://www.semanticscholar.org/paper/9879da626ea07cc27e588943e5f0d37898812adb\",\"venue\":\"KDD\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"50219447\",\"name\":\"Zheng Wang\"}],\"doi\":\"10.1145/3123266.3123327\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"title\":\"Catching the Temporal Regions-of-Interest for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1704.01518\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"39578749\",\"name\":\"Siyu Tang\"},{\"authorId\":\"2390510\",\"name\":\"Seong Joon Oh\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2017.447\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db2fecc8b1bd175d39687eb471360707a5fddb03\",\"title\":\"Generating Descriptions with Grounded and Co-referenced People\",\"url\":\"https://www.semanticscholar.org/paper/db2fecc8b1bd175d39687eb471360707a5fddb03\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1804.00100\",\"authors\":[{\"authorId\":null,\"name\":\"Jingwen Wang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"}],\"doi\":\"10.1109/CVPR.2018.00751\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"title\":\"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"2012.02128\",\"authors\":[{\"authorId\":\"1768759403\",\"name\":\"Jing Su\"},{\"authorId\":\"49892954\",\"name\":\"Qingyun Dai\"},{\"authorId\":\"8139616\",\"name\":\"F. Guerin\"},{\"authorId\":\"1510708415\",\"name\":\"Mian Zhou\"}],\"doi\":\"10.1016/j.csl.2020.101169\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c1a435ffd06c26f9f4273b11efed10bce2138d08\",\"title\":\"BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/c1a435ffd06c26f9f4273b11efed10bce2138d08\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.12968\",\"authors\":[{\"authorId\":\"1380222019\",\"name\":\"Zijian Kuang\"},{\"authorId\":\"2003585139\",\"name\":\"Xinran Tie\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5cedb91cbafc80af3c6e04a426f2d6350122df0b\",\"title\":\"Video Understanding based on Human Action and Group Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/5cedb91cbafc80af3c6e04a426f2d6350122df0b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39796129\",\"name\":\"Hayden Faulkner\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":\"10.1109/DICTA.2017.8227494\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"title\":\"TenniSet: A Dataset for Dense Fine-Grained Event Recognition, Localisation and Description\",\"url\":\"https://www.semanticscholar.org/paper/4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"venue\":\"2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8194130\",\"name\":\"Qinyu Li\"}],\"doi\":\"10.1145/3303083\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"91aa0eb38446643cd622b060a76043b0ca2d7991\",\"title\":\"Rich Visual and Language Representation with Complementary Semantics for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/91aa0eb38446643cd622b060a76043b0ca2d7991\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50754085\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"49479171\",\"name\":\"Chen Fang\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"2047181\",\"name\":\"Seokhwan Kim\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"144015161\",\"name\":\"Y. Fu\"}],\"doi\":\"10.1109/cvpr42600.2020.01254\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1a780c6219996c8481c117056efcf071cbfbd15\",\"title\":\"Screencast Tutorial Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b1a780c6219996c8481c117056efcf071cbfbd15\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40652109\",\"name\":\"Sirish Kumar Pasupuleti\"},{\"authorId\":\"3444608\",\"name\":\"R. Gadde\"},{\"authorId\":\"49590244\",\"name\":\"V. Rajagopal\"},{\"authorId\":\"51267398\",\"name\":\"A. Vishnoi\"},{\"authorId\":\"47262465\",\"name\":\"N. C. Sekhar\"},{\"authorId\":\"49844991\",\"name\":\"R. Kumar\"},{\"authorId\":\"1692444\",\"name\":\"Narasinga Rao Miniskar\"}],\"doi\":\"10.1109/ISCAS.2019.8702528\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed56fc28df4cc5dca812da5ab355818f42af60fd\",\"title\":\"Low Complex & High Accuracy Computation Approximations to Enable On-Device RNN Applications\",\"url\":\"https://www.semanticscholar.org/paper/ed56fc28df4cc5dca812da5ab355818f42af60fd\",\"venue\":\"2019 IEEE International Symposium on Circuits and Systems (ISCAS)\",\"year\":2019},{\"arxivId\":\"1611.09312\",\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"153925540\",\"name\":\"C. Grana\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2017.339\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"title\":\"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2017.219\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"151b87de997e55db892b122c211f9c749f4293de\",\"title\":\"Joint Learning of Object and Action Detectors\",\"url\":\"https://www.semanticscholar.org/paper/151b87de997e55db892b122c211f9c749f4293de\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b2c543e0c47454c4512569175094e6cb6ae02a9\",\"title\":\"The VizWiz Grand Challenge : A Large Visual Question Answering Dataset from Blind People Anonymous CVPR submission\",\"url\":\"https://www.semanticscholar.org/paper/0b2c543e0c47454c4512569175094e6cb6ae02a9\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29724362\",\"name\":\"Longlong Jing\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3eb9f4ca21bd104b1d9963a5a74e0ad48a1a1bdf\",\"title\":\"Self-supervised Spatiotemporal Feature Learning by Video Geometric Transformations\",\"url\":\"https://www.semanticscholar.org/paper/3eb9f4ca21bd104b1d9963a5a74e0ad48a1a1bdf\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144841441\",\"name\":\"J. Xu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3123266.3123448\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"title\":\"Learning Multimodal Attention LSTM Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1812.02501\",\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"144031869\",\"name\":\"A. Yao\"}],\"doi\":\"10.1109/ICCV.2019.00095\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"15a36f9639f608c4567302de65355543fdcee910\",\"title\":\"Zero-Shot Anticipation for Instructional Activities\",\"url\":\"https://www.semanticscholar.org/paper/15a36f9639f608c4567302de65355543fdcee910\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1145/3343031.3351072\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"db6035229a71a6c93d4f15c4a4280eb644228da4\",\"title\":\"Hierarchical Global-Local Temporal Modeling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/db6035229a71a6c93d4f15c4a4280eb644228da4\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1581863540\",\"name\":\"Aidean Sharghi Karganroodi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"550b08b659d3b8e7f45bdc09602af2184791d082\",\"title\":\"Visual-Textual Video Synopsis Generation\",\"url\":\"https://www.semanticscholar.org/paper/550b08b659d3b8e7f45bdc09602af2184791d082\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153197501\",\"name\":\"Yuan Yuan\"},{\"authorId\":\"152644265\",\"name\":\"Lei Lin\"},{\"authorId\":\"3224666\",\"name\":\"Lian-Zhi Huo\"},{\"authorId\":\"9683264\",\"name\":\"Yun-long Kong\"},{\"authorId\":\"2392254\",\"name\":\"Zengguang Zhou\"},{\"authorId\":\"1699037\",\"name\":\"Bin Wu\"},{\"authorId\":\"145095641\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/JSTARS.2020.2988324\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b591b008c953358d981fa0b1d6e550e2ef1a4cad\",\"title\":\"Using An Attention-Based LSTM Encoder\\u2013Decoder Network for Near Real-Time Disturbance Detection\",\"url\":\"https://www.semanticscholar.org/paper/b591b008c953358d981fa0b1d6e550e2ef1a4cad\",\"venue\":\"IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing\",\"year\":2020},{\"arxivId\":\"1801.09356\",\"authors\":[{\"authorId\":\"1730952\",\"name\":\"Ravi Kiran Sarvadevabhatla\"},{\"authorId\":\"3436918\",\"name\":\"S. Surya\"},{\"authorId\":\"9214782\",\"name\":\"Trisha Mittal\"},{\"authorId\":\"7757884\",\"name\":\"V. Radhakrishnan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b081d11a8d19048c65b34c520357a82efcc91856\",\"title\":\"Game of Sketches: Deep Recurrent Models of Pictionary-style Word Guessing\",\"url\":\"https://www.semanticscholar.org/paper/b081d11a8d19048c65b34c520357a82efcc91856\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1710.11601\",\"authors\":[{\"authorId\":\"2875615\",\"name\":\"Lea Frermann\"},{\"authorId\":\"40146204\",\"name\":\"Shay B. Cohen\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":\"10.1162/tacl_a_00001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f6dd15b7e674755b01fe605efb42d03d920ebcde\",\"title\":\"Whodunnit? Crime Drama as a Case for Natural Language Understanding\",\"url\":\"https://www.semanticscholar.org/paper/f6dd15b7e674755b01fe605efb42d03d920ebcde\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52201852\",\"name\":\"Eleftherios Daskalakis\"},{\"authorId\":\"2817419\",\"name\":\"Maria Tzelepi\"},{\"authorId\":\"1737071\",\"name\":\"A. Tefas\"}],\"doi\":\"10.1016/j.patrec.2018.09.022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fc1332023c370dc55fabb1b6c895af1a5f48f889\",\"title\":\"Learning deep spatiotemporal features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/fc1332023c370dc55fabb1b6c895af1a5f48f889\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50111883\",\"name\":\"S. Lee\"},{\"authorId\":\"48084799\",\"name\":\"In-Cheol Kim\"}],\"doi\":\"10.1155/2018/3125879\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c919568836e236da738282f9015f58c455d26f7\",\"title\":\"Multimodal Feature Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c919568836e236da738282f9015f58c455d26f7\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1804.05448\",\"authors\":[{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.18653/v1/N18-2125\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"title\":\"Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018191\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"title\":\"Motion Guided Spatial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1804.00819\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"34872128\",\"name\":\"Yingbo Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.1109/CVPR.2018.00911\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35ed258aede3df17ee20a6635364cb5fd2461049\",\"title\":\"End-to-End Dense Video Captioning with Masked Transformer\",\"url\":\"https://www.semanticscholar.org/paper/35ed258aede3df17ee20a6635364cb5fd2461049\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"145211780\",\"name\":\"Bin Fan\"},{\"authorId\":\"1380311632\",\"name\":\"Shinming Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.18653/v1/D19-1213\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"title\":\"Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag\",\"url\":\"https://www.semanticscholar.org/paper/ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9734988\",\"name\":\"Yuecong Xu\"},{\"authorId\":\"2562263\",\"name\":\"Jianfei Yang\"},{\"authorId\":\"144067957\",\"name\":\"K. Mao\"}],\"doi\":\"10.1016/J.NEUCOM.2019.05.027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd9a6ff5f908a6e8e785eb0a1432a5c9da2c2192\",\"title\":\"Semantic-filtered Soft-Split-Aware video captioning with audio-augmented feature\",\"url\":\"https://www.semanticscholar.org/paper/fd9a6ff5f908a6e8e785eb0a1432a5c9da2c2192\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143986279\",\"name\":\"X. Wu\"},{\"authorId\":\"145375313\",\"name\":\"Haotian Chang\"},{\"authorId\":\"40116904\",\"name\":\"J. Li\"},{\"authorId\":\"50316717\",\"name\":\"Zhenyuan Zhang\"},{\"authorId\":\"144171278\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/ISGT-Asia.2019.8881168\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"92ef1a33beb0043e3d3b3b8e764a09c40b2b27b4\",\"title\":\"Electricity Consumption and Weather Reflect Macro-Economic Status\",\"url\":\"https://www.semanticscholar.org/paper/92ef1a33beb0043e3d3b3b8e764a09c40b2b27b4\",\"venue\":\"2019 IEEE Innovative Smart Grid Technologies - Asia (ISGT Asia)\",\"year\":2019},{\"arxivId\":\"1809.04938\",\"authors\":[{\"authorId\":\"8093340\",\"name\":\"Shuming Ma\"},{\"authorId\":\"145500855\",\"name\":\"Lei Cui\"},{\"authorId\":\"10780897\",\"name\":\"Damai Dai\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.1609/AAAI.V33I01.33016810\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"09e39f4334417f92bddc20072f43efade9bd5b60\",\"title\":\"LiveBot: Generating Live Video Comments Based on Visual and Textual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/09e39f4334417f92bddc20072f43efade9bd5b60\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"143852685\",\"name\":\"H. Tavakoli\"},{\"authorId\":\"1708642\",\"name\":\"Jorma T. Laaksonen\"}],\"doi\":\"10.1109/MMUL.2018.112135923\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"da8f9211ea60755bb40811bb92de76be389566c6\",\"title\":\"Image and Video Captioning with Augmented Neural Architectures\",\"url\":\"https://www.semanticscholar.org/paper/da8f9211ea60755bb40811bb92de76be389566c6\",\"venue\":\"IEEE MultiMedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"47295036\",\"name\":\"Zehua Zhang\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":\"10.1016/j.jvcir.2018.05.008\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"26eff1aa014ed0f19fbda0ac6b554f8ba9881f25\",\"title\":\"Deepdiary: Lifelogging image captioning and summarization\",\"url\":\"https://www.semanticscholar.org/paper/26eff1aa014ed0f19fbda0ac6b554f8ba9881f25\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":\"1705.01253\",\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1016/j.neucom.2018.06.069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"783e48629dfbb44697b15a3bc0cb2aa3eea490eb\",\"title\":\"The Forgettable-Watcher Model for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/783e48629dfbb44697b15a3bc0cb2aa3eea490eb\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/3122865\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"title\":\"Frontiers of Multimedia Research\",\"url\":\"https://www.semanticscholar.org/paper/8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2184355\",\"name\":\"Karl Pichotta\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":\"10.18653/v1/W16-6003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0ca1fbe1f429d2ebe1a0220641e0dad6449884f\",\"title\":\"Statistical Script Learning with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/a0ca1fbe1f429d2ebe1a0220641e0dad6449884f\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"144478646\",\"name\":\"Z. Guo\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TMM.2017.2729019\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"51b2c1e750b1d3b893072829d012f2352d6bd373\",\"title\":\"Video Captioning With Attention-Based LSTM and Semantic Consistency\",\"url\":\"https://www.semanticscholar.org/paper/51b2c1e750b1d3b893072829d012f2352d6bd373\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153907743\",\"name\":\"J. Camps\"},{\"authorId\":\"47201833\",\"name\":\"Albert Sam\\u00e0\"},{\"authorId\":\"94958894\",\"name\":\"M. Mart\\u00edn\"},{\"authorId\":\"30235321\",\"name\":\"D. Mart\\u00edn\"},{\"authorId\":\"1402153683\",\"name\":\"C. P\\u00e9rez-L\\u00f3pez\"},{\"authorId\":\"34659960\",\"name\":\"Sheila Alcaine\"},{\"authorId\":\"1405183803\",\"name\":\"Berta Mestre\"},{\"authorId\":\"39203352\",\"name\":\"A. Prats\"},{\"authorId\":\"40542911\",\"name\":\"M. C. Crespo\"},{\"authorId\":\"1744371\",\"name\":\"J. Cabestany\"},{\"authorId\":\"46337344\",\"name\":\"\\u00c0. Bay\\u00e9s\"},{\"authorId\":\"145928942\",\"name\":\"A. Catal\\u00e0\"}],\"doi\":\"10.1007/978-3-319-59147-6_30\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b9a2f788461aa28c96bfe73d7f3c24f988c0ae85\",\"title\":\"Deep Learning for Detecting Freezing of Gait Episodes in Parkinson's Disease Based on Accelerometers\",\"url\":\"https://www.semanticscholar.org/paper/b9a2f788461aa28c96bfe73d7f3c24f988c0ae85\",\"venue\":\"IWANN\",\"year\":2017},{\"arxivId\":\"1604.03249\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1007/978-3-319-50077-5_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"title\":\"Attributes as Semantic Units between Natural Language and Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720849132\",\"name\":\"Chen Jun\"},{\"authorId\":\"20910883\",\"name\":\"Yang Su-hua\"},{\"authorId\":\"1391317038\",\"name\":\"Jiang Shao-feng\"}],\"doi\":\"10.1109/ICEMI46757.2019.9101847\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"40738929212cd70d2701a21b38499bfe038880a0\",\"title\":\"Automatic classification and recognition of complex documents based on Faster RCNN\",\"url\":\"https://www.semanticscholar.org/paper/40738929212cd70d2701a21b38499bfe038880a0\",\"venue\":\"2019 14th IEEE International Conference on Electronic Measurement & Instruments (ICEMI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":\"10.1145/3347449.3357484\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"989730c00381805543baa470a2d6490cc5354a13\",\"title\":\"L-STAP: Learned Spatio-Temporal Adaptive Pooling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/989730c00381805543baa470a2d6490cc5354a13\",\"venue\":\"AI4TV@MM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1825675711\",\"name\":\"Arvind Narayanan\"},{\"authorId\":\"2003543359\",\"name\":\"Eman Ramadan\"},{\"authorId\":\"1557364548\",\"name\":\"Rishabh Mehta\"},{\"authorId\":\"50050002\",\"name\":\"Xinyue Hu\"},{\"authorId\":\"2000470624\",\"name\":\"Qingxu Liu\"},{\"authorId\":\"2002987108\",\"name\":\"Rostand A. K. Fezeu\"},{\"authorId\":\"2003541362\",\"name\":\"Udhaya Kumar Dayalan\"},{\"authorId\":\"47383867\",\"name\":\"Saurabh Verma\"},{\"authorId\":\"2000493167\",\"name\":\"Peiqi Ji\"},{\"authorId\":\"1876824505\",\"name\":\"Tao Li\"},{\"authorId\":\"47073602\",\"name\":\"F. Qian\"},{\"authorId\":\"47295098\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1145/3419394.3423629\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15c8e0f531016441439d0204cd28b26fb2ecddd2\",\"title\":\"Lumos5G: Mapping and Predicting Commercial mmWave 5G Throughput\",\"url\":\"https://www.semanticscholar.org/paper/15c8e0f531016441439d0204cd28b26fb2ecddd2\",\"venue\":\"Internet Measurement Conference\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4b5c35e70954a05ec4b836f166882982f459eefa\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/4b5c35e70954a05ec4b836f166882982f459eefa\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1607.03250\",\"authors\":[{\"authorId\":\"3437504\",\"name\":\"H. Hu\"},{\"authorId\":\"145122505\",\"name\":\"R. Peng\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"},{\"authorId\":\"2088295\",\"name\":\"C. Tang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60ae4f18cb53efff0174e3fea7064049737e1e67\",\"title\":\"Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures\",\"url\":\"https://www.semanticscholar.org/paper/60ae4f18cb53efff0174e3fea7064049737e1e67\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1707.00803\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"47119743\",\"name\":\"X. Wang\"},{\"authorId\":\"7868449\",\"name\":\"Yongyi Tang\"},{\"authorId\":\"3179887\",\"name\":\"Xinpeng Chen\"},{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ef70b917ef25646d7eb919752448cfbc028d861a\",\"title\":\"Aggregating Frame-level Features for Large-Scale Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/ef70b917ef25646d7eb919752448cfbc028d861a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"50678151\",\"name\":\"Bingtao Liu\"},{\"authorId\":\"7590116\",\"name\":\"C. Yan\"}],\"doi\":\"10.1145/3123266.3123354\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"title\":\"Video Description with Spatial-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1904.12251\",\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1145/3123266.3123328\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"454e65c2a9b019a00790a1d6029dc5539edad35d\",\"title\":\"Hierarchical Recurrent Neural Network for Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/454e65c2a9b019a00790a1d6029dc5539edad35d\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1810.07212\",\"authors\":[{\"authorId\":\"3047890\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"}],\"doi\":\"10.1007/978-3-030-01261-8_23\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea133d0067740902bc26a082c842d9e7ba48ecf6\",\"title\":\"Cross-Modal and Hierarchical Modeling of Video and Text\",\"url\":\"https://www.semanticscholar.org/paper/ea133d0067740902bc26a082c842d9e7ba48ecf6\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/ACCESS.2018.2879642\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"title\":\"A Fine-Grained Spatial-Temporal Attention Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33970300\",\"name\":\"Bor-Chun Chen\"},{\"authorId\":\"35081710\",\"name\":\"Yan-Ying Chen\"},{\"authorId\":\"27375808\",\"name\":\"Francine Chen\"}],\"doi\":\"10.5244/C.31.118\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"fb3bce3a6221eb65451584efa898ecbe211bdab6\",\"title\":\"Video to Text Summary: Joint Video Summarization and Captioning with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/fb3bce3a6221eb65451584efa898ecbe211bdab6\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"65767906\",\"name\":\"Silvio Olivastri\"},{\"authorId\":\"1931660\",\"name\":\"Gurkirt Singh\"},{\"authorId\":\"1754181\",\"name\":\"Fabio Cuzzolin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20f3d85b99b4b595b1c92f60a9b9a126f7384e15\",\"title\":\"An End-to-End Baseline for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20f3d85b99b4b595b1c92f60a9b9a126f7384e15\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1904.09722\",\"authors\":[{\"authorId\":\"1878452\",\"name\":\"Hsin-I Chen\"},{\"authorId\":\"1720818\",\"name\":\"Sebastian Agethen\"},{\"authorId\":\"36224312\",\"name\":\"Chia-Min Wu\"},{\"authorId\":\"1716836\",\"name\":\"Winston H. Hsu\"},{\"authorId\":\"1733344\",\"name\":\"Bing-Yu Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c31af583284bcbd713c9e1af61c097ff5c1a310\",\"title\":\"FishNet: A Camera Localizer using Deep Recurrent Networks\",\"url\":\"https://www.semanticscholar.org/paper/5c31af583284bcbd713c9e1af61c097ff5c1a310\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3039477\",\"name\":\"Alvaro Ulloa\"},{\"authorId\":\"5276826\",\"name\":\"Linyuan Jing\"},{\"authorId\":\"7913725\",\"name\":\"C. W. Good\"},{\"authorId\":\"6337685\",\"name\":\"D. vanMaanen\"},{\"authorId\":\"1382542531\",\"name\":\"Sushravya Raghunath\"},{\"authorId\":\"2275724\",\"name\":\"Jonathan D. Suever\"},{\"authorId\":\"3586989\",\"name\":\"Christopher D. Nevius\"},{\"authorId\":\"3895478\",\"name\":\"Gregory J. Wehner\"},{\"authorId\":\"3466971\",\"name\":\"D. Hartzel\"},{\"authorId\":\"47648047\",\"name\":\"Joseph B. Leader\"},{\"authorId\":\"13999508\",\"name\":\"A. Alsaid\"},{\"authorId\":\"50841642\",\"name\":\"A. Patel\"},{\"authorId\":\"145601639\",\"name\":\"H. L. Kirchner\"},{\"authorId\":\"47293698\",\"name\":\"M. Pattichis\"},{\"authorId\":\"2149632\",\"name\":\"C. Haggerty\"},{\"authorId\":\"5732263\",\"name\":\"B. Fornwalt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5ccc4119f22fdbb88c5eb8c349e5329938a14f1\",\"title\":\"A deep neural network to enhance prediction of 1-year mortality using echocardiographic videos of the heart\",\"url\":\"https://www.semanticscholar.org/paper/d5ccc4119f22fdbb88c5eb8c349e5329938a14f1\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"49991208\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c631bb439284f6a5a90608b715fa631d5c5807e4\",\"title\":\"EEGtoText: Learning to Write Medical Reports from EEG Recordings\",\"url\":\"https://www.semanticscholar.org/paper/c631bb439284f6a5a90608b715fa631d5c5807e4\",\"venue\":\"MLHC\",\"year\":2019},{\"arxivId\":\"2003.01163\",\"authors\":[{\"authorId\":\"103393695\",\"name\":\"C. Jiang\"},{\"authorId\":\"153597007\",\"name\":\"Masood Dehghan\"},{\"authorId\":\"3160299\",\"name\":\"Martin J\\u00e4gersand\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d3ed110990a5245a7f787fe3d1d1e3f1ac01573\",\"title\":\"Understanding Contexts Inside Robot and Human Manipulation Tasks through a Vision-Language Model and Ontology System in a Video Stream\",\"url\":\"https://www.semanticscholar.org/paper/4d3ed110990a5245a7f787fe3d1d1e3f1ac01573\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1382474435\",\"name\":\"Ivona Milanova\"},{\"authorId\":\"1382474463\",\"name\":\"Ksenija Sarvanoska\"},{\"authorId\":\"1382474412\",\"name\":\"Viktor Srbinoski\"},{\"authorId\":\"2438687\",\"name\":\"Hristijan Gjoreski\"}],\"doi\":\"10.1007/978-3-030-33110-8_1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3933dcc0b0641aca597ea80ff603bad299fa416\",\"title\":\"Automatic Text Generation in Macedonian Using Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/a3933dcc0b0641aca597ea80ff603bad299fa416\",\"venue\":\"ICT Innovations\",\"year\":2019},{\"arxivId\":\"2006.04058\",\"authors\":[{\"authorId\":\"48775710\",\"name\":\"Alok Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"098317f445d4753188658ebd3a72c272d10132cd\",\"title\":\"NITS-VC System for VATEX Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/098317f445d4753188658ebd3a72c272d10132cd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"145950948\",\"name\":\"Xue Li\"},{\"authorId\":\"2558130\",\"name\":\"Z. Shi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e8ce74a73bb0b3197d4194fcb638710d76970654\",\"title\":\"Video Captioning with Listwise Supervision\",\"url\":\"https://www.semanticscholar.org/paper/e8ce74a73bb0b3197d4194fcb638710d76970654\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2964097\",\"name\":\"A. Ghosh\"},{\"authorId\":\"144549904\",\"name\":\"Yash Patel\"},{\"authorId\":\"3026786\",\"name\":\"M. Sukhwani\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1007/978-3-319-46604-0_59\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd45cfc3751fa93d25766bddf66fc1d9c7f4f6d2\",\"title\":\"Dynamic Narratives for Heritage Tour\",\"url\":\"https://www.semanticscholar.org/paper/cd45cfc3751fa93d25766bddf66fc1d9c7f4f6d2\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":\"1810.06682\",\"authors\":[{\"authorId\":\"35836381\",\"name\":\"Shaojie Bai\"},{\"authorId\":\"145116464\",\"name\":\"J. Z. Kolter\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a14af711aaa3ae83eb64d1f517b024b8c3094a8a\",\"title\":\"Trellis Networks for Sequence Modeling\",\"url\":\"https://www.semanticscholar.org/paper/a14af711aaa3ae83eb64d1f517b024b8c3094a8a\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1704.01502\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"47008023\",\"name\":\"Z. Su\"},{\"authorId\":\"3700393\",\"name\":\"Minjun Li\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/CVPR.2017.548\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"title\":\"Weakly Supervised Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1903.01038\",\"authors\":[{\"authorId\":\"3530540\",\"name\":\"Yunbo Wang\"},{\"authorId\":\"35776445\",\"name\":\"Mingsheng Long\"},{\"authorId\":\"1751179\",\"name\":\"J. Wang\"},{\"authorId\":\"144019071\",\"name\":\"Philip S. Yu\"}],\"doi\":\"10.1109/CVPR.2017.226\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f1f66207713871da193af24e19a5d8855bfe7f5a\",\"title\":\"Spatiotemporal Pyramid Network for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f1f66207713871da193af24e19a5d8855bfe7f5a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/2962719\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"title\":\"Semantic Feature Mining for Video Event Understanding\",\"url\":\"https://www.semanticscholar.org/paper/99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"venue\":\"TOMM\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2822290\",\"name\":\"Suraj Srinivas\"},{\"authorId\":\"1730952\",\"name\":\"Ravi Kiran Sarvadevabhatla\"},{\"authorId\":\"2217000\",\"name\":\"Konda Reddy Mopuri\"},{\"authorId\":\"40390520\",\"name\":\"N. Prabhu\"},{\"authorId\":\"1784761\",\"name\":\"S. Kruthiventi\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.1016/B978-0-12-810408-8.00003-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"783cd91797aad234b1558099689e794b48694092\",\"title\":\"An Introduction to Deep Convolutional Neural Nets for Computer Vision\",\"url\":\"https://www.semanticscholar.org/paper/783cd91797aad234b1558099689e794b48694092\",\"venue\":\"Deep Learning for Medical Image Analysis\",\"year\":2017},{\"arxivId\":\"1604.04279\",\"authors\":[{\"authorId\":\"34280810\",\"name\":\"Gunnar A. Sigurdsson\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1007/978-3-319-46454-1_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3583f39138f3e70fab0754ed4e5578a91fb08a20\",\"title\":\"Learning Visual Storylines with Skipping Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/3583f39138f3e70fab0754ed4e5578a91fb08a20\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114766222\",\"name\":\"P. Laffitte\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d4096a58126d27ea740f06c7d16b43ce4b16f6ae\",\"title\":\"Automatic detection of screams and shouts in the metro\",\"url\":\"https://www.semanticscholar.org/paper/d4096a58126d27ea740f06c7d16b43ce4b16f6ae\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49111436\",\"name\":\"Jonathan Chung\"},{\"authorId\":\"5470607\",\"name\":\"Sarah A. Chau\"},{\"authorId\":\"144528209\",\"name\":\"N. Herrmann\"},{\"authorId\":\"4797664\",\"name\":\"K. Lanct\\u00f4t\"},{\"authorId\":\"2331840\",\"name\":\"M. Eizenman\"}],\"doi\":\"10.1145/3219819.3219908\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a9957afa56db59f80b09b4f0a8b677e7d65c71a\",\"title\":\"Detection of Apathy in Alzheimer Patients by Analysing Visual Scanning Behaviour with RNNs\",\"url\":\"https://www.semanticscholar.org/paper/0a9957afa56db59f80b09b4f0a8b677e7d65c71a\",\"venue\":\"KDD\",\"year\":2018},{\"arxivId\":\"1904.03885\",\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":null,\"name\":\"Abhinav Shrivastava\"},{\"authorId\":\"2852035\",\"name\":\"V. Morariu\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.18653/v1/W19-1802\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e164e75632e23a7fba6a46d2ee2dc328720601af\",\"title\":\"Referring to Objects in Videos using Spatio-Temporal Identifying Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e164e75632e23a7fba6a46d2ee2dc328720601af\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1612.00234\",\"authors\":[{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"}],\"doi\":\"10.1162/tacl_a_00013\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5a732016c3f74dc7d78899bf33cf25df03ef46b4\",\"title\":\"Video Captioning with Multi-Faceted Attention\",\"url\":\"https://www.semanticscholar.org/paper/5a732016c3f74dc7d78899bf33cf25df03ef46b4\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2018},{\"arxivId\":\"2007.08814\",\"authors\":[{\"authorId\":\"72066761\",\"name\":\"Jun-Bin Xiao\"},{\"authorId\":\"2444704\",\"name\":\"Xindi Shang\"},{\"authorId\":\"72347323\",\"name\":\"X. Yang\"},{\"authorId\":\"144044848\",\"name\":\"Sheng Tang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1007/978-3-030-58539-6_27\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13ee363f71e07112210ac2ff27d46625f6f8edab\",\"title\":\"Visual Relation Grounding in Videos\",\"url\":\"https://www.semanticscholar.org/paper/13ee363f71e07112210ac2ff27d46625f6f8edab\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.07735\",\"authors\":[{\"authorId\":\"40016108\",\"name\":\"Aman Chadha\"},{\"authorId\":\"2025073690\",\"name\":\"Gurneet Arora\"},{\"authorId\":\"2025065763\",\"name\":\"Navpreet Kaloty\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"title\":\"iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"17045108\",\"name\":\"Neziha Jaouedi\"},{\"authorId\":\"1797154\",\"name\":\"Noureddine Boujnah\"},{\"authorId\":\"33786813\",\"name\":\"M. Bouhlel\"}],\"doi\":\"10.3844/JCSSP.2019.1040.1049\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e192f32cbe2657bf9b46b77d3de4c6ce1c372b9\",\"title\":\"Deep Learning Approach for Human Action Recognition Using Gated Recurrent Unit Neural Networks and Motion Analysis\",\"url\":\"https://www.semanticscholar.org/paper/8e192f32cbe2657bf9b46b77d3de4c6ce1c372b9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1711.11135\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/CVPR.2018.00443\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"74b284a66e75b65f5970d05bac000fe91243ee49\",\"title\":\"Video Captioning via Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/74b284a66e75b65f5970d05bac000fe91243ee49\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409222519\",\"name\":\"U. A. Khan\"},{\"authorId\":\"150244890\",\"name\":\"N. Ejaz\"},{\"authorId\":\"1401947265\",\"name\":\"M. A. Mart\\u00ednez-del-Amor\"},{\"authorId\":\"2366822\",\"name\":\"H. Sparenberg\"}],\"doi\":\"10.1109/AVSS.2017.8078459\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b8f805e18c205916285c4a8ca5f233cb8952cc8\",\"title\":\"Movies tags extraction using deep learning\",\"url\":\"https://www.semanticscholar.org/paper/4b8f805e18c205916285c4a8ca5f233cb8952cc8\",\"venue\":\"2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)\",\"year\":2017},{\"arxivId\":\"1611.05594\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1406012647\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"104757141\",\"name\":\"Jian Shao\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/CVPR.2017.667\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"88513e738a95840de05a62f0e43d30a67b3c542e\",\"title\":\"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/88513e738a95840de05a62f0e43d30a67b3c542e\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1906.01290\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"097981245eb3c66cc10a3164275d0bd52f5ae22a\",\"title\":\"Relational Reasoning using Prior Knowledge for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/097981245eb3c66cc10a3164275d0bd52f5ae22a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yu Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"31c8f1f728df2cfea5d0a9dda67a27de82f5a879\",\"title\":\"Let Your Photos Talk: Generating Narrative Paragraph for Photo Stream via Bidirectional Attention Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/31c8f1f728df2cfea5d0a9dda67a27de82f5a879\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1905.02963\",\"authors\":[{\"authorId\":\"145114776\",\"name\":\"L. Sun\"},{\"authorId\":\"143721383\",\"name\":\"Bing Li\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":\"10.1109/ICME.2019.00226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"title\":\"Multimodal Semantic Attention Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144730169\",\"name\":\"B. Tan\"},{\"authorId\":\"1809614\",\"name\":\"Y. Song\"},{\"authorId\":\"1687583\",\"name\":\"Erheng Zhong\"},{\"authorId\":\"152290618\",\"name\":\"Qiang Yang\"}],\"doi\":\"10.1145/2783258.2783295\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"edffd0c62233c51ae99c8c72f8eb286aa9c950b5\",\"title\":\"Transitive Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/edffd0c62233c51ae99c8c72f8eb286aa9c950b5\",\"venue\":\"KDD\",\"year\":2015},{\"arxivId\":\"1506.00019\",\"authors\":[{\"authorId\":\"32219137\",\"name\":\"Zachary Chase Lipton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883\",\"title\":\"A Critical Review of Recurrent Neural Networks for Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736727\",\"name\":\"Kai Yu\"},{\"authorId\":\"47122584\",\"name\":\"Z. Zhao\"},{\"authorId\":\"8768708\",\"name\":\"X. Wu\"},{\"authorId\":\"49955789\",\"name\":\"H. Lin\"},{\"authorId\":\"26966619\",\"name\":\"Xuan Liu\"}],\"doi\":\"10.1109/TASLP.2018.2819941\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2100e99aa0af489add2c47552a0f2be8288636b2\",\"title\":\"Rich Short Text Conversation Using Semantic-Key-Controlled Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/2100e99aa0af489add2c47552a0f2be8288636b2\",\"venue\":\"IEEE/ACM Transactions on Audio, Speech, and Language Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46875241\",\"name\":\"Rui Shi Liang\"},{\"authorId\":\"1734697\",\"name\":\"Qingxin Zhu\"}],\"doi\":\"10.2991/ICMEIT-16.2016.74\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72c3bc0e664ddefbd1e67380c23e6199c78cd426\",\"title\":\"Multi Semantic Feature Fusion Framework for Video Segmentation and Description\",\"url\":\"https://www.semanticscholar.org/paper/72c3bc0e664ddefbd1e67380c23e6199c78cd426\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144934447\",\"name\":\"M. Dom\\u00ednguez\"},{\"authorId\":\"41053064\",\"name\":\"Rohan Dhamdhere\"},{\"authorId\":\"41051406\",\"name\":\"Atir Petkar\"},{\"authorId\":\"143954368\",\"name\":\"S. Jain\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/WACV.2018.00218\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a803ac657f396fca67760af073f659b0e69f7afc\",\"title\":\"General-Purpose Deep Point Cloud Feature Extractor\",\"url\":\"https://www.semanticscholar.org/paper/a803ac657f396fca67760af073f659b0e69f7afc\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2110665\",\"name\":\"A. Padmakumar\"},{\"authorId\":\"2346355\",\"name\":\"Ashish Bora\"},{\"authorId\":\"3647010\",\"name\":\"A. Gholaminejad\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de95fa1dd69a2d0d2b76539357062062f8b1e7b8\",\"title\":\"Face to Age\",\"url\":\"https://www.semanticscholar.org/paper/de95fa1dd69a2d0d2b76539357062062f8b1e7b8\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"101574342\",\"name\":\"Laokulrat Natsuda\"},{\"authorId\":\"72456985\",\"name\":\"Okazaki Naoaki\"},{\"authorId\":\"72028078\",\"name\":\"Nakayama Hideki\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5dc7c33475b545271d1de726fd88bb68dfb7e11b\",\"title\":\"Generating Video Description using RNN with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/5dc7c33475b545271d1de726fd88bb68dfb7e11b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2898850\",\"name\":\"R\\u00e9mi Ronfard\"}],\"doi\":\"10.2312/wiced.20171069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d5c43f7a1c4e2fec310474bb78a2e7aca5aa609\",\"title\":\"Five Challenges for Intelligent Cinematography and Editing\",\"url\":\"https://www.semanticscholar.org/paper/6d5c43f7a1c4e2fec310474bb78a2e7aca5aa609\",\"venue\":\"WICED@Eurographics\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"144245551\",\"name\":\"M. Tang\"},{\"authorId\":\"50561313\",\"name\":\"J. Zhang\"}],\"doi\":\"10.1117/1.JEI.27.2.023027\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"title\":\"Deep hierarchical attention network for video description\",\"url\":\"https://www.semanticscholar.org/paper/8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1ec568db83e0921e7d4bf73bd6b9f3dc86e09c38\",\"title\":\"Human Activity Analysis using Multi-modalities and Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ec568db83e0921e7d4bf73bd6b9f3dc86e09c38\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"84147453\",\"name\":\"Steven Charles Hauser\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"21ca59b7cfa20051303709d6fecda18741f3f4b9\",\"title\":\"Real-World Considerations for Deep Learning in Spectrum Sensing\",\"url\":\"https://www.semanticscholar.org/paper/21ca59b7cfa20051303709d6fecda18741f3f4b9\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150965117\",\"name\":\"Daniel Alejandro Castro Chin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b24c218e8545bdbe58d3a614aee6c9063dd40d62\",\"title\":\"Understanding The Motion of A Human State In Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/b24c218e8545bdbe58d3a614aee6c9063dd40d62\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1926536\",\"name\":\"Chengcheng Wei\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"2653622\",\"name\":\"Junfu Pu\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"}],\"doi\":\"10.1109/BigMM.2019.00027\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be5b602bfbc6488f8ef633db1292f70138b5751b\",\"title\":\"Deep Grammatical Multi-classifier for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/be5b602bfbc6488f8ef633db1292f70138b5751b\",\"venue\":\"2019 IEEE Fifth International Conference on Multimedia Big Data (BigMM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":null,\"name\":\"Shuo Wang\"},{\"authorId\":\"144876834\",\"name\":\"Q. Tian\"},{\"authorId\":\"73160450\",\"name\":\"Meng Wang\"}],\"doi\":\"10.24963/ijcai.2019/105\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f07bef10500f55d4d34bac96bbe93f1120a6ca8d\",\"title\":\"Dense Temporal Convolution Network for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/f07bef10500f55d4d34bac96bbe93f1120a6ca8d\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150173695\",\"name\":\"Korrawe Karunratanakul\"},{\"authorId\":\"49687474\",\"name\":\"Hsin-Yao Tang\"},{\"authorId\":\"1967068\",\"name\":\"D. Speicher\"},{\"authorId\":\"1819128\",\"name\":\"Ekapol Chuangsuwanich\"},{\"authorId\":\"2512208\",\"name\":\"Sira Sriswasdi\"}],\"doi\":\"10.1101/667527\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"996e7e94d5f5a8e3be706f4beead88f45b1ea488\",\"title\":\"Uncovering thousands of new HLA antigens and phosphopeptides with deep learning-based sequence-mask-search de novo peptide sequencing framework\",\"url\":\"https://www.semanticscholar.org/paper/996e7e94d5f5a8e3be706f4beead88f45b1ea488\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1703.09902\",\"authors\":[{\"authorId\":\"1700894\",\"name\":\"Albert Gatt\"},{\"authorId\":\"145210073\",\"name\":\"E. Krahmer\"}],\"doi\":\"10.1613/jair.5477\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d13bb317e87f3f6da10da11059ebf4350b754814\",\"title\":\"Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation\",\"url\":\"https://www.semanticscholar.org/paper/d13bb317e87f3f6da10da11059ebf4350b754814\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82564091\",\"name\":\"Ey\\u00fcp \\u00d6zer\"},{\"authorId\":\"1658966195\",\"name\":\"\\u0130lteber Nur Karap\\u0131nar\"},{\"authorId\":\"1658997114\",\"name\":\"Sena Ba\\u015fbu\\u011f\"},{\"authorId\":\"1657299979\",\"name\":\"S\\u00fcmeyye Turan\"},{\"authorId\":\"52219742\",\"name\":\"An\\u0131l Utku\"},{\"authorId\":\"153780569\",\"name\":\"M. Akcayol\"}],\"doi\":\"10.14569/ijacsa.2020.0110365\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f682a618e680e6c488f620a65cf5cf657fc6986b\",\"title\":\"Deep Learning based, a New Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f682a618e680e6c488f620a65cf5cf657fc6986b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49046603\",\"name\":\"C. Liu\"},{\"authorId\":\"2887672\",\"name\":\"A. Shmilovici\"}],\"doi\":\"10.1007/978-3-030-47124-8_39\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f78ec67e18413fe70e37eee132e7527ffee5ec62\",\"title\":\"Towards Automatic Textual Summarization of Movies\",\"url\":\"https://www.semanticscholar.org/paper/f78ec67e18413fe70e37eee132e7527ffee5ec62\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"1804.07351\",\"authors\":[{\"authorId\":\"3367790\",\"name\":\"Seong Jae Hwang\"},{\"authorId\":\"7689277\",\"name\":\"Ronak Mehta\"},{\"authorId\":\"144711711\",\"name\":\"V. Singh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"887d2b031b9d303bcad7d473ff6858a60f219258\",\"title\":\"Sampling-free Uncertainty Estimation in Gated Recurrent Units with Exponential Families\",\"url\":\"https://www.semanticscholar.org/paper/887d2b031b9d303bcad7d473ff6858a60f219258\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1806.01954\",\"authors\":[{\"authorId\":\"51011850\",\"name\":\"Iulia Duta\"},{\"authorId\":\"50986865\",\"name\":\"A. Nicolicioiu\"},{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c42f427b54ab12a1d89827ee4c6951efae733b55\",\"title\":\"Mining for meaning: from vision to language through multiple networks consensus\",\"url\":\"https://www.semanticscholar.org/paper/c42f427b54ab12a1d89827ee4c6951efae733b55\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1820762548\",\"name\":\"Jaimon Jacob\"},{\"authorId\":\"23548701\",\"name\":\"M. Elayidom\"},{\"authorId\":\"3109670\",\"name\":\"V. P. Devassia\"}],\"doi\":\"10.1109/ICCES48766.2020.9138076\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2f5943e7041dac273428e732f964ff994c157d7a\",\"title\":\"An innovative Method of Accessing Digital Video Archives through Video Indexing\",\"url\":\"https://www.semanticscholar.org/paper/2f5943e7041dac273428e732f964ff994c157d7a\",\"venue\":\"2020 5th International Conference on Communication and Electronics Systems (ICCES)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"144016781\",\"name\":\"Heng Ji\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"},{\"authorId\":\"1817166\",\"name\":\"Clare R. Voss\"}],\"doi\":\"10.18653/v1/D18-1433\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c7de1e95e7f130fcbab0dea763869ff2244523e8\",\"title\":\"Incorporating Background Knowledge into Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/c7de1e95e7f130fcbab0dea763869ff2244523e8\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46276803\",\"name\":\"J. Li\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1016/j.jvcir.2020.102875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"title\":\"Translating video into language by enhancing visual and language representations\",\"url\":\"https://www.semanticscholar.org/paper/32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"145723090\",\"name\":\"Z. Wang\"},{\"authorId\":\"143640801\",\"name\":\"S. Chen\"},{\"authorId\":\"143760554\",\"name\":\"Q. Wei\"}],\"doi\":\"10.1007/978-3-030-26075-0_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac82935ce0768ae6c541405569be891663a3b27c\",\"title\":\"Discovering Attractive Segments in the User Generated Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/ac82935ce0768ae6c541405569be891663a3b27c\",\"venue\":\"APWeb/WAIM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394741222\",\"name\":\"Yuling Gui\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"97522088\",\"name\":\"Ye Zhao\"}],\"doi\":\"10.1145/3347319.3356839\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"title\":\"Semantic Enhanced Encoder-Decoder Network (SEN) for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1903.01489\",\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/s11042-018-7040-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1344317f255a9d338fb80f276126951b9644f7e3\",\"title\":\"M-VAD names: a dataset for video captioning with naming\",\"url\":\"https://www.semanticscholar.org/paper/1344317f255a9d338fb80f276126951b9644f7e3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451596\",\"name\":\"Adriana Baltaretu\"},{\"authorId\":\"145210073\",\"name\":\"E. Krahmer\"},{\"authorId\":\"145900027\",\"name\":\"A. Maes\"}],\"doi\":\"10.1080/13875868.2016.1212863\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eae42e3d05645c11e35c85fc434412b469816980\",\"title\":\"Landmarks on the move: Producing and understanding references to moving landmarks\",\"url\":\"https://www.semanticscholar.org/paper/eae42e3d05645c11e35c85fc434412b469816980\",\"venue\":\"Spatial Cogn. Comput.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145307652\",\"name\":\"Li Dong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9e98709ea1a13a7264071d6beb927c0d2a8f10e7\",\"title\":\"Learning natural language interfaces with neural models\",\"url\":\"https://www.semanticscholar.org/paper/9e98709ea1a13a7264071d6beb927c0d2a8f10e7\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144548932\",\"name\":\"Feiyang Liu\"},{\"authorId\":\"153697270\",\"name\":\"G. Cao\"},{\"authorId\":\"67325011\",\"name\":\"Daiqin Yang\"},{\"authorId\":\"71877589\",\"name\":\"Yiyong Zha\"},{\"authorId\":\"48380493\",\"name\":\"Yunfei Zhang\"},{\"authorId\":\"46522256\",\"name\":\"Xiaoxia Liu\"}],\"doi\":\"10.1145/3338533.3366630\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"197ce1ae2307bb8f05d050ecbe2ccc9f5785d0bf\",\"title\":\"An LSTM based Rate and Distortion Prediction Method for Low-delay Video Coding\",\"url\":\"https://www.semanticscholar.org/paper/197ce1ae2307bb8f05d050ecbe2ccc9f5785d0bf\",\"venue\":\"MMAsia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7025059\",\"name\":\"C. Orozco\"},{\"authorId\":\"36507903\",\"name\":\"M. E. Buemi\"},{\"authorId\":\"1405801592\",\"name\":\"J. Jacobo-Berlles\"}],\"doi\":\"10.1109/SCCC.2018.8705254\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"00b0e22ff30f68e6163bfe34beebbf23e3a85813\",\"title\":\"Video to Text Study using an Encoder-Decoder Networks Approach\",\"url\":\"https://www.semanticscholar.org/paper/00b0e22ff30f68e6163bfe34beebbf23e3a85813\",\"venue\":\"2018 37th International Conference of the Chilean Computer Science Society (SCCC)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"93992321\",\"name\":\"S. Li\"},{\"authorId\":\"153090156\",\"name\":\"Charles Patrick Martin\"}],\"doi\":\"10.1007/978-3-030-64984-5_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5b92adb3efe86fcab1b85d0ead5814bed8ae8042\",\"title\":\"Comparing Three Data Representations for Music with a Sequence-to-Sequence Model\",\"url\":\"https://www.semanticscholar.org/paper/5b92adb3efe86fcab1b85d0ead5814bed8ae8042\",\"venue\":\"Australasian Conference on Artificial Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"1790251284\",\"name\":\"Lin Li\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"9594118\",\"name\":\"C. Gu\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/s11063-020-10352-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"title\":\"Adaptively Converting Auxiliary Attributes and Textual Embedding for Video Captioning Based on BiLSTM\",\"url\":\"https://www.semanticscholar.org/paper/3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"venue\":\"Neural Process. Lett.\",\"year\":2020},{\"arxivId\":\"1910.12019\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Diverse Video Captioning Through Latent Variable Expansion with Conditional GAN\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"18103812\",\"name\":\"Shengdong Du\"},{\"authorId\":\"66782928\",\"name\":\"Tianrui Li\"},{\"authorId\":\"50017440\",\"name\":\"Yan Yang\"},{\"authorId\":\"72419379\",\"name\":\"S. Horng\"}],\"doi\":\"10.1016/j.neucom.2019.12.118\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ca9bacd6c211e91ad0f92ff9d499c0f17ce19f4\",\"title\":\"Multivariate time series forecasting via attention-based encoder-decoder framework\",\"url\":\"https://www.semanticscholar.org/paper/5ca9bacd6c211e91ad0f92ff9d499c0f17ce19f4\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1807.09418\",\"authors\":[{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TMM.2019.2930041\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"92e02bd58b99ac17b475081611f091f4b0776482\",\"title\":\"Video Storytelling: Textual Summaries for Events\",\"url\":\"https://www.semanticscholar.org/paper/92e02bd58b99ac17b475081611f091f4b0776482\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"1909.13162\",\"authors\":[{\"authorId\":\"50784905\",\"name\":\"A. Roy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"382a890f3be6b4718e72e7fa88b09b3e9ea54659\",\"title\":\"Translation, Sentiment and Voices: A Computational Model to Translate and Analyse Voices from Real-Time Video Calling\",\"url\":\"https://www.semanticscholar.org/paper/382a890f3be6b4718e72e7fa88b09b3e9ea54659\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153446767\",\"name\":\"Qinkun Xiao\"},{\"authorId\":\"143742875\",\"name\":\"Xin Chang\"},{\"authorId\":\"97644075\",\"name\":\"X. Zhang\"},{\"authorId\":\"97713335\",\"name\":\"X. Liu\"}],\"doi\":\"10.1109/ACCESS.2020.3039539\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d120180a56558b530848cdc6ead3c914e0e3a35\",\"title\":\"Multi-Information Spatial\\u2013Temporal LSTM Fusion Continuous Sign Language Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/7d120180a56558b530848cdc6ead3c914e0e3a35\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1809.04560\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D18-1012\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"667a6eea4c3039d4d1bde2ebf4f2fe8bcfa4af23\",\"title\":\"Game-Based Video-Context Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/667a6eea4c3039d4d1bde2ebf4f2fe8bcfa4af23\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145078521\",\"name\":\"Kang Li\"},{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"2457612\",\"name\":\"Sangmin Oh\"},{\"authorId\":\"46956675\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TIP.2017.2678800\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fa785c9db83b1be92672f0f77edb66a328108b80\",\"title\":\"Videography-Based Unconstrained Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/fa785c9db83b1be92672f0f77edb66a328108b80\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":\"1604.01753\",\"authors\":[{\"authorId\":\"34280810\",\"name\":\"Gunnar A. Sigurdsson\"},{\"authorId\":\"2668759\",\"name\":\"G. Varol\"},{\"authorId\":\"39849136\",\"name\":\"X. Wang\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"}],\"doi\":\"10.1007/978-3-319-46448-0_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21334d1aac5422da88780f8e24e181bfa15ef0e1\",\"title\":\"Hollywood in Homes: Crowdsourcing Data Collection for Activity Understanding\",\"url\":\"https://www.semanticscholar.org/paper/21334d1aac5422da88780f8e24e181bfa15ef0e1\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":\"50500629\",\"name\":\"Z. Shao\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1007/s11042-017-5532-x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6795a5a06790ee4fbe5b873b563b34b0be6ad6b3\",\"title\":\"LSTM-based multi-label video event detection\",\"url\":\"https://www.semanticscholar.org/paper/6795a5a06790ee4fbe5b873b563b34b0be6ad6b3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1691390\",\"name\":\"Wen-Li Wei\"},{\"authorId\":\"66191041\",\"name\":\"Jen-Chun Lin\"},{\"authorId\":\"1805102\",\"name\":\"Tyng-Luh Liu\"},{\"authorId\":\"1689230\",\"name\":\"Y. Yang\"},{\"authorId\":\"1710199\",\"name\":\"H. Wang\"},{\"authorId\":\"2188516\",\"name\":\"H. Tyan\"},{\"authorId\":\"1704678\",\"name\":\"H. Liao\"}],\"doi\":\"10.1109/ICME.2018.8486496\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"660523533bcef6de5d3a4366c49c0e193fa1f5bd\",\"title\":\"Seethevoice: Learning from Music to Visual Storytelling of Shots\",\"url\":\"https://www.semanticscholar.org/paper/660523533bcef6de5d3a4366c49c0e193fa1f5bd\",\"venue\":\"2018 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2018},{\"arxivId\":\"1606.00625\",\"authors\":[{\"authorId\":null,\"name\":\"Yu Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0c56a5c466fd012739dfa71ae36f6558e4c817ed\",\"title\":\"Storytelling of Photo Stream with Bidirectional Multi-thread Recurrent Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/0c56a5c466fd012739dfa71ae36f6558e4c817ed\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1610.04997\",\"authors\":[{\"authorId\":\"1975564\",\"name\":\"M. Zanfir\"},{\"authorId\":\"2045166\",\"name\":\"Elisabeta Marinoiu\"},{\"authorId\":\"1781120\",\"name\":\"C. Sminchisescu\"}],\"doi\":\"10.1007/978-3-319-54190-7_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ed613b6f0427d3ec4cad6c51dcc451786812959\",\"title\":\"Spatio-Temporal Attention Models for Grounded Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ed613b6f0427d3ec4cad6c51dcc451786812959\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52327816\",\"name\":\"Jinkyu Kim\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9d462a10299ce908574b1a9bbe37d7369bf2b445\",\"title\":\"Explainable and Advisable Learning for Self-driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/9d462a10299ce908574b1a9bbe37d7369bf2b445\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1808.08803\",\"authors\":[{\"authorId\":\"1819984\",\"name\":\"K. Ning\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"47118295\",\"name\":\"Ming Cai\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":\"145982709\",\"name\":\"Di Xie\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"20da5315cfe5eab69d99bbda270e73ab488a49ba\",\"title\":\"Attentive Sequence to Sequence Translation for Localizing Clips of Interest by Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/20da5315cfe5eab69d99bbda270e73ab488a49ba\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1804.05164\",\"authors\":[{\"authorId\":\"30123167\",\"name\":\"Yecheng Lyu\"},{\"authorId\":\"8858861\",\"name\":\"X. Huang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97e38c2e8fda9aec9bc24e407af0d4e5d4d7ab94\",\"title\":\"Road Segmentation Using CNN with GRU\",\"url\":\"https://www.semanticscholar.org/paper/97e38c2e8fda9aec9bc24e407af0d4e5d4d7ab94\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"41c853eebc563cc6dde2df32fbaca816ce0bab3e\",\"title\":\"Perceptual Prediction Error Detection Learning Signal Learning Signal\",\"url\":\"https://www.semanticscholar.org/paper/41c853eebc563cc6dde2df32fbaca816ce0bab3e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c8ad1c0d40b416ac52522d3cf110f574b71c0db6\",\"title\":\"DSTC 7-AVSD : Scene-Aware Video-Dialogue Systems with Dual Attention\",\"url\":\"https://www.semanticscholar.org/paper/c8ad1c0d40b416ac52522d3cf110f574b71c0db6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1906.04375\",\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/CVPR.2019.00852\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"title\":\"Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1604.04953\",\"authors\":[{\"authorId\":\"2002678\",\"name\":\"Zecheng Xie\"},{\"authorId\":\"46554940\",\"name\":\"Zenghui Sun\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"},{\"authorId\":\"2790939\",\"name\":\"Ziyong Feng\"},{\"authorId\":\"2911939\",\"name\":\"Shuye Zhang\"}],\"doi\":\"10.1109/ICPR.2016.7900261\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"584c44968bce99d105099867812a3654cf271fcd\",\"title\":\"Fully convolutional recurrent network for handwritten Chinese text recognition\",\"url\":\"https://www.semanticscholar.org/paper/584c44968bce99d105099867812a3654cf271fcd\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"49051008\",\"name\":\"J. Zhang\"},{\"authorId\":\"145618006\",\"name\":\"Q. Guo\"},{\"authorId\":\"47570083\",\"name\":\"Jun Lei\"},{\"authorId\":\"3143729\",\"name\":\"D. Tu\"}],\"doi\":\"10.1049/iet-cvi.2015.0473\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"259a5666615944ee27e53feaf1eaa7bf256ff581\",\"title\":\"Generating image descriptions with multidirectional 2D long short-term memory\",\"url\":\"https://www.semanticscholar.org/paper/259a5666615944ee27e53feaf1eaa7bf256ff581\",\"venue\":\"IET Comput. Vis.\",\"year\":2017},{\"arxivId\":\"1711.07480\",\"authors\":[{\"authorId\":\"30008313\",\"name\":\"Franyell Silfa\"},{\"authorId\":\"2912680\",\"name\":\"Gem Dot\"},{\"authorId\":\"3084705\",\"name\":\"J. Arnau\"},{\"authorId\":\"1747103\",\"name\":\"A. Gonz\\u00e1lez\"}],\"doi\":\"10.1145/3243176.3243184\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c84f4b8451c8382b1fba95a45c5cda9fcabfe60c\",\"title\":\"E-PUR: an energy-efficient processing unit for recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/c84f4b8451c8382b1fba95a45c5cda9fcabfe60c\",\"venue\":\"PACT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5442167\",\"name\":\"Yiran Chen\"},{\"authorId\":\"31959733\",\"name\":\"H. Li\"},{\"authorId\":\"3207491\",\"name\":\"Chunpeng Wu\"},{\"authorId\":\"143774361\",\"name\":\"Chang Song\"},{\"authorId\":\"3285742\",\"name\":\"Sicheng Li\"},{\"authorId\":\"3446580\",\"name\":\"Chuhan Min\"},{\"authorId\":\"3461946\",\"name\":\"Hsin-Pai Cheng\"},{\"authorId\":\"145225262\",\"name\":\"Wei Wen\"},{\"authorId\":\"49543226\",\"name\":\"Xiaoxiao Liu\"}],\"doi\":\"10.1016/j.vlsi.2017.11.001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bdc397cfeccf356015a50f20abd5654ae3f66c86\",\"title\":\"Neuromorphic computing's yesterday, today, and tomorrow - an evolutional view\",\"url\":\"https://www.semanticscholar.org/paper/bdc397cfeccf356015a50f20abd5654ae3f66c86\",\"venue\":\"Integr.\",\"year\":2018},{\"arxivId\":\"1507.06120\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2837527\",\"name\":\"Mariella Dimiccoli\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1109/THMS.2016.2616296\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d4c2dd3996cb3d87da6c35d72572637d3175ea5\",\"title\":\"Toward Storytelling From Visual Lifelogging: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/1d4c2dd3996cb3d87da6c35d72572637d3175ea5\",\"venue\":\"IEEE Transactions on Human-Machine Systems\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1016/j.cviu.2017.04.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"title\":\"Hierarchical & multimodal video captioning: Discovering and transferring multimodal knowledge for vision to language\",\"url\":\"https://www.semanticscholar.org/paper/96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":\"10.1007/978-3-030-31756-0_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"title\":\"The Encoder-Decoder Framework and Its Applications\",\"url\":\"https://www.semanticscholar.org/paper/0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1903.00110\",\"authors\":[{\"authorId\":\"46419391\",\"name\":\"M. Elfeki\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":\"10.1109/WACV.2019.00085\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"be447eedf6c50096cc6a85b47ae7afa203c511b6\",\"title\":\"Video Summarization Via Actionness Ranking\",\"url\":\"https://www.semanticscholar.org/paper/be447eedf6c50096cc6a85b47ae7afa203c511b6\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398693138\",\"name\":\"I. Gonz\\u00e1lez-D\\u00edaz\"},{\"authorId\":\"1381345946\",\"name\":\"J. Benois-Pineau\"},{\"authorId\":\"1746076\",\"name\":\"Jean-Philippe Domenger\"},{\"authorId\":\"8258896\",\"name\":\"D. Cattaert\"},{\"authorId\":\"7162505\",\"name\":\"A. Rugy\"}],\"doi\":\"10.1016/j.patcog.2018.11.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"81548cfb716a085ea6b2e56ee2dc8494fc6d19e4\",\"title\":\"Perceptually-guided deep neural networks for ego-action prediction: Object grasping\",\"url\":\"https://www.semanticscholar.org/paper/81548cfb716a085ea6b2e56ee2dc8494fc6d19e4\",\"venue\":\"Pattern Recognit.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77537913\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/s11042-019-08011-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2623c1b4aee3f043da30d05f3e2c0f62fca5d5e\",\"title\":\"Deep multimodal embedding for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/c2623c1b4aee3f043da30d05f3e2c0f62fca5d5e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8804979\",\"name\":\"Siddhesh Khandelwal\"}],\"doi\":\"10.14288/1.0384602\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0217b35a52d1e6b29ab734fe8b637ddad66cbe89\",\"title\":\"Enforcing structure in visual attention\",\"url\":\"https://www.semanticscholar.org/paper/0217b35a52d1e6b29ab734fe8b637ddad66cbe89\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40231554\",\"name\":\"W. Wang\"},{\"authorId\":\"144801562\",\"name\":\"Zhen Cui\"},{\"authorId\":\"47971190\",\"name\":\"Yan Yan\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"},{\"authorId\":\"2287686\",\"name\":\"Xiangbo Shu\"},{\"authorId\":\"1703601\",\"name\":\"N. Sebe\"}],\"doi\":\"10.1109/CVPR.2016.261\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"adab8450f5a3a9cba4c6679bb6fae5cbea18c612\",\"title\":\"Recurrent Face Aging\",\"url\":\"https://www.semanticscholar.org/paper/adab8450f5a3a9cba4c6679bb6fae5cbea18c612\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145446752\",\"name\":\"Marie-Francine Moens\"},{\"authorId\":\"2861393\",\"name\":\"K. Pastra\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1704728\",\"name\":\"T. Tuytelaars\"}],\"doi\":\"10.1109/MMUL.2018.023121160\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"37bc08f554333225c85d00261123aec934cbd682\",\"title\":\"Vision and Language Integration Meets Multimedia Fusion\",\"url\":\"https://www.semanticscholar.org/paper/37bc08f554333225c85d00261123aec934cbd682\",\"venue\":\"IEEE Multim.\",\"year\":2018},{\"arxivId\":\"1812.00344\",\"authors\":[{\"authorId\":\"1769749\",\"name\":\"S. Wang\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"16132631\",\"name\":\"Z. Kou\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7d1c2d4b4ca2bf3109757f181bc2cf15240fcc5\",\"title\":\"How to Make a BLT Sandwich? Learning to Reason towards Understanding Web Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/e7d1c2d4b4ca2bf3109757f181bc2cf15240fcc5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1917234\",\"name\":\"S. Sun\"},{\"authorId\":\"2018393\",\"name\":\"Hyeonwoo Noh\"},{\"authorId\":\"144136097\",\"name\":\"S. Somasundaram\"},{\"authorId\":\"35198686\",\"name\":\"Joseph J. Lim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c35f8f48f748c040c33bbe1f21c794e209341987\",\"title\":\"Neural Program Synthesis from Diverse Demonstration Videos\",\"url\":\"https://www.semanticscholar.org/paper/c35f8f48f748c040c33bbe1f21c794e209341987\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46867157\",\"name\":\"Y. Zhang\"},{\"authorId\":\"47364599\",\"name\":\"Mingli Ding\"},{\"authorId\":\"2860057\",\"name\":\"Yancheng Bai\"},{\"authorId\":\"48928816\",\"name\":\"Dandan Liu\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1016/j.patrec.2019.10.005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d277ff82da3cb6bdfe9987f1be12998cccbf3c37\",\"title\":\"Learning a strong detector for action localization in videos\",\"url\":\"https://www.semanticscholar.org/paper/d277ff82da3cb6bdfe9987f1be12998cccbf3c37\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50997773\",\"name\":\"Xuguang Duan\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"1591133899\",\"name\":\"Yiwei Zhang\"},{\"authorId\":\"123175679\",\"name\":\"W. Huang\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"145583986\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3343031.3351094\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c0b12c784965baac88b6597890303fa834fa9eea\",\"title\":\"Watch, Reason and Code: Learning to Represent Videos Using Program\",\"url\":\"https://www.semanticscholar.org/paper/c0b12c784965baac88b6597890303fa834fa9eea\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"48093314\",\"name\":\"Jing-Wen Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3394171.3413908\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"title\":\"Controllable Video Captioning with an Exemplar Sentence\",\"url\":\"https://www.semanticscholar.org/paper/40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1912.04608\",\"authors\":[{\"authorId\":\"1383481973\",\"name\":\"Yan Bin Ng\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"}],\"doi\":\"10.1109/TIP.2020.3021497\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08bf24e179aff7971ba95aed185c8e13da9f8ce4\",\"title\":\"Forecasting Future Action Sequences With Attention: A New Approach to Weakly Supervised Action Forecasting\",\"url\":\"https://www.semanticscholar.org/paper/08bf24e179aff7971ba95aed185c8e13da9f8ce4\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21148831\",\"name\":\"Zhong Ji\"},{\"authorId\":\"48947242\",\"name\":\"Fang Jiao\"},{\"authorId\":\"48278149\",\"name\":\"Y. Pang\"},{\"authorId\":\"40799321\",\"name\":\"Ling Shao\"}],\"doi\":\"10.1016/j.neucom.2020.04.132\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e824b1cdd3e227933f18e7ebf54c4fa3cbcc2534\",\"title\":\"Deep attentive and semantic preserving video summarization\",\"url\":\"https://www.semanticscholar.org/paper/e824b1cdd3e227933f18e7ebf54c4fa3cbcc2534\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1906.01452\",\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474871\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/TPAMI.2019.2920899\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"83a3fe38887880bccc15daa740d8d5041f826d91\",\"title\":\"Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/83a3fe38887880bccc15daa740d8d5041f826d91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144669461\",\"name\":\"Kuncheng Fang\"},{\"authorId\":\"144913277\",\"name\":\"Lian Zhou\"},{\"authorId\":\"145020731\",\"name\":\"Cheng Jin\"},{\"authorId\":\"7550713\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"35632219\",\"name\":\"Kangnian Weng\"},{\"authorId\":\"1689115\",\"name\":\"Tao Zhang\"},{\"authorId\":\"145631869\",\"name\":\"W. Fan\"}],\"doi\":\"10.1609/AAAI.V33I01.33018271\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"title\":\"Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention\",\"url\":\"https://www.semanticscholar.org/paper/506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1704.03615\",\"authors\":[{\"authorId\":\"2298523\",\"name\":\"Achal Dave\"},{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"}],\"doi\":\"10.1109/CVPR.2017.223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"08ed84f2cb8f4961dea6ba60c09cc36959921287\",\"title\":\"Predictive-Corrective Networks for Action Detection\",\"url\":\"https://www.semanticscholar.org/paper/08ed84f2cb8f4961dea6ba60c09cc36959921287\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2739186\",\"name\":\"Michael Maynord\"},{\"authorId\":\"1969847\",\"name\":\"D. Aha\"},{\"authorId\":\"49530191\",\"name\":\"S. Bhattacharya\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f33a5fcc5db4625c66972f0e6f06540b64d4f1e\",\"title\":\"Image Surveillance Assistant Architecture : Status and Planned Extensions\",\"url\":\"https://www.semanticscholar.org/paper/7f33a5fcc5db4625c66972f0e6f06540b64d4f1e\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1505.01121\",\"authors\":[{\"authorId\":\"145478807\",\"name\":\"Mateusz Malinowski\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"}],\"doi\":\"10.1109/ICCV.2015.9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd7bd1d2945a58cdcc1797ba9698b8810fe68f60\",\"title\":\"Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images\",\"url\":\"https://www.semanticscholar.org/paper/bd7bd1d2945a58cdcc1797ba9698b8810fe68f60\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1900013\",\"name\":\"Zhengyang Wu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1733702\",\"name\":\"Matthew R. Walter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"177c48590469c62d430cf74fee7b5bd28bfbbc1d\",\"title\":\"Articulated Motion Learning via Visual and Lingual Signals\",\"url\":\"https://www.semanticscholar.org/paper/177c48590469c62d430cf74fee7b5bd28bfbbc1d\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1605.08110\",\"authors\":[{\"authorId\":\"47968942\",\"name\":\"K. Zhang\"},{\"authorId\":\"38784892\",\"name\":\"Wei-Lun Chao\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-319-46478-7_47\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1dbc12e54ceb70f2022f956aa0a46e2706e99962\",\"title\":\"Video Summarization with Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/1dbc12e54ceb70f2022f956aa0a46e2706e99962\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50980387\",\"name\":\"Y. Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b2e65f5dc2889845706555bc6b66aa42f60346a9\",\"title\":\"Talk : Generating Narrative Paragraph for Photo Stream via Bidirectional Attention Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b2e65f5dc2889845706555bc6b66aa42f60346a9\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40387883\",\"name\":\"Florian Patzelt\"},{\"authorId\":\"2112168\",\"name\":\"Robert Haschke\"},{\"authorId\":\"30258243\",\"name\":\"Helge J. Ritter\"}],\"doi\":\"10.1007/978-3-319-44781-0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf9ce9a9114a29b69e48acb1193d21dc6267f0d3\",\"title\":\"Artificial Neural Networks and Machine Learning \\u2013 ICANN 2016\",\"url\":\"https://www.semanticscholar.org/paper/bf9ce9a9114a29b69e48acb1193d21dc6267f0d3\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144713153\",\"name\":\"Dan Guo\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d44c20c48e764a546d00b9155a56b171b0dc04bc\",\"title\":\"Hierarchical LSTM for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/d44c20c48e764a546d00b9155a56b171b0dc04bc\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"title\":\"Image Input OR Video Hierarchical LSTMs with Adaptive Attention ( hLSTMat ) Feature Extraction Generated Captions Losses\",\"url\":\"https://www.semanticscholar.org/paper/e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9382626\",\"name\":\"M. Amaresh\"},{\"authorId\":\"144902122\",\"name\":\"S. Chitrakala\"}],\"doi\":\"10.1109/ICCSP.2019.8698097\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aad4525b28b18fde9c793ab387ac327802ef71d2\",\"title\":\"Video Captioning using Deep Learning: An Overview of Methods, Datasets and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/aad4525b28b18fde9c793ab387ac327802ef71d2\",\"venue\":\"2019 International Conference on Communication and Signal Processing (ICCSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51436077\",\"name\":\"Xinyuan Chen\"},{\"authorId\":\"144157740\",\"name\":\"Li Song\"},{\"authorId\":\"1795291\",\"name\":\"X. Yang\"}],\"doi\":\"10.1117/12.2239260\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"131f30f98b4cb719a5f6ff53f84e36335df264bd\",\"title\":\"Deep RNNs for video denoising\",\"url\":\"https://www.semanticscholar.org/paper/131f30f98b4cb719a5f6ff53f84e36335df264bd\",\"venue\":\"Optical Engineering + Applications\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5387396\",\"name\":\"Huidong Li\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"},{\"authorId\":\"3000498\",\"name\":\"Lejian Liao\"},{\"authorId\":\"151479762\",\"name\":\"Cuimei Peng\"}],\"doi\":\"10.1109/ICME.2019.00228\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1567fff9f411af320f678c66b812d2c963151678\",\"title\":\"REVnet: Bring Reviewing Into Video Captioning for a Better Description\",\"url\":\"https://www.semanticscholar.org/paper/1567fff9f411af320f678c66b812d2c963151678\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30075807\",\"name\":\"L. Zaman\"},{\"authorId\":\"9377338\",\"name\":\"S. Sumpeno\"},{\"authorId\":\"2517400\",\"name\":\"M. Hariadi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"27d20aaf0d5a85a3b44b0cccb5c6daacd62bda60\",\"title\":\"GRU sebagai Model Generatif untuk Tari Remo\",\"url\":\"https://www.semanticscholar.org/paper/27d20aaf0d5a85a3b44b0cccb5c6daacd62bda60\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33975342\",\"name\":\"Jiajun Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f31e2d67a479c44a9942b1cd6e4905b6b1972d37\",\"title\":\"Video Understanding : From Video Classification to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f31e2d67a479c44a9942b1cd6e4905b6b1972d37\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"title\":\"Video representation learning with deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"title\":\"Incorporating Semantic Attention in Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"venue\":\"LREC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3469333\",\"name\":\"Sameer Bansal\"}],\"doi\":\"10.7488/ERA/86\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1aacddf2619f704416427473a0f26d353cb97a0f\",\"title\":\"Low-resource speech translation\",\"url\":\"https://www.semanticscholar.org/paper/1aacddf2619f704416427473a0f26d353cb97a0f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46958420\",\"name\":\"Tianyi Wang\"},{\"authorId\":\"47539594\",\"name\":\"Jiang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1007/978-3-030-00776-8_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c50c91875767ec7c6391d99d30838d90275a0f1b\",\"title\":\"Collaborative Detection and Caption Network\",\"url\":\"https://www.semanticscholar.org/paper/c50c91875767ec7c6391d99d30838d90275a0f1b\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47243105\",\"name\":\"P. Zhang\"},{\"authorId\":\"9308544\",\"name\":\"Chunmiao Yuan\"},{\"authorId\":\"3386073\",\"name\":\"Kunliang Liu\"},{\"authorId\":\"46676502\",\"name\":\"Y. Sun\"},{\"authorId\":\"2607232\",\"name\":\"Jiayu Liang\"},{\"authorId\":\"151470674\",\"name\":\"Guanghao Jin\"},{\"authorId\":\"46584793\",\"name\":\"J. Wang\"}],\"doi\":\"10.1007/978-3-030-35231-8_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"801a3ada495732f95b34bce187de9dd41a40d10d\",\"title\":\"Fast Video Clip Retrieval Method via Language Query\",\"url\":\"https://www.semanticscholar.org/paper/801a3ada495732f95b34bce187de9dd41a40d10d\",\"venue\":\"ADMA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47776625\",\"name\":\"GuoYu Zhang\"},{\"authorId\":\"50561547\",\"name\":\"J. Zhang\"},{\"authorId\":\"143901629\",\"name\":\"J. Shang\"}],\"doi\":\"10.2151/SOLA.2018-023\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf181374af96bb24fc419222243da8177a170a73\",\"title\":\"Contrail Recognition with Convolutional Neural Network and Contrail Parameterizations Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/cf181374af96bb24fc419222243da8177a170a73\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"46276092\",\"name\":\"Jiaming Li\"},{\"authorId\":\"144488755\",\"name\":\"H. Cheng\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1007/978-3-319-54526-4_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"44e68bb3b0652fb51beab1db90c3762e8733f386\",\"title\":\"Multi-cue Information Fusion for Two-Layer Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/44e68bb3b0652fb51beab1db90c3762e8733f386\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":\"1811.04869\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/CVPR.2019.00129\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"063ba2f8f6f1624f42de4e9bee0ac5ae6ce06032\",\"title\":\"A Perceptual Prediction Framework for Self Supervised Event Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/063ba2f8f6f1624f42de4e9bee0ac5ae6ce06032\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1711.06315\",\"authors\":[{\"authorId\":\"144933637\",\"name\":\"S. Sen\"},{\"authorId\":\"145342039\",\"name\":\"Shubham Jain\"},{\"authorId\":\"1778215\",\"name\":\"Swagath Venkataramani\"},{\"authorId\":\"145291370\",\"name\":\"A. Raghunathan\"}],\"doi\":\"10.1109/tc.2018.2879434\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"adb2b08ca47813983f54dc71ee82e224e667b156\",\"title\":\"SparCE: Sparsity Aware General-Purpose Core Extensions to Accelerate Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/adb2b08ca47813983f54dc71ee82e224e667b156\",\"venue\":\"IEEE Transactions on Computers\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66783923\",\"name\":\"Summra Saleem\"},{\"authorId\":\"40893753\",\"name\":\"A. Dilawari\"},{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"1816309\",\"name\":\"Razi Iqbal\"},{\"authorId\":\"31197849\",\"name\":\"Shaohua Wan\"},{\"authorId\":\"1858727\",\"name\":\"Tariq Umer\"}],\"doi\":\"10.1016/J.COMPELECENG.2019.07.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a3cdb9ba6eb846fcb2b37df8557a8d10057d238\",\"title\":\"Stateful human-centered visual captioning system to aid video surveillance\",\"url\":\"https://www.semanticscholar.org/paper/5a3cdb9ba6eb846fcb2b37df8557a8d10057d238\",\"venue\":\"Comput. Electr. Eng.\",\"year\":2019},{\"arxivId\":\"1808.04108\",\"authors\":[{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"144300094\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ICASSP.2019.8682383\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"908c342d66137f5e70544e8204951f28cb02deb0\",\"title\":\"Towards Audio to Scene Image Synthesis Using Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/908c342d66137f5e70544e8204951f28cb02deb0\",\"venue\":\"ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48677815\",\"name\":\"Xinyan Yu\"},{\"authorId\":\"49890039\",\"name\":\"Y. Zhang\"},{\"authorId\":\"143758471\",\"name\":\"Rui Zhang\"}],\"doi\":\"10.1007/978-3-030-30671-7_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"62afe2605541f485bbd2bfe89b41161ce08a5977\",\"title\":\"Cross-Modality Video Segment Retrieval with Ensemble Learning\",\"url\":\"https://www.semanticscholar.org/paper/62afe2605541f485bbd2bfe89b41161ce08a5977\",\"venue\":\"Domain Adaptation for Visual Understanding\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"2069818\",\"name\":\"D. Zhang\"},{\"authorId\":\"1706774\",\"name\":\"J. Li\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/CVPR.2017.662\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3b0b706fc94b35a1eddd830685e07870315b9565\",\"title\":\"Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description\",\"url\":\"https://www.semanticscholar.org/paper/3b0b706fc94b35a1eddd830685e07870315b9565\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"2007.14682\",\"authors\":[{\"authorId\":\"1840585237\",\"name\":\"Philipp Rimle\"},{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"143720818\",\"name\":\"M. Gro\\u00df\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"title\":\"Enriching Video Captions With Contextual Text\",\"url\":\"https://www.semanticscholar.org/paper/f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"70435288\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"2674678\",\"name\":\"Xiaoxun Zhang\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1609/AAAI.V34I07.6731\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4df184d6a74f1ffd84b644735c9afb5060552770\",\"title\":\"Joint Commonsense and Relation Reasoning for Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4df184d6a74f1ffd84b644735c9afb5060552770\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41030694\",\"name\":\"Huanyu Yu\"},{\"authorId\":\"3392007\",\"name\":\"Shuo Cheng\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"7272302\",\"name\":\"Minsi Wang\"},{\"authorId\":\"40430880\",\"name\":\"J. Zhang\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"}],\"doi\":\"10.1109/CVPR.2018.00629\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f5876f67129a80a1ee753f715efcd2e2109bf432\",\"title\":\"Fine-Grained Video Captioning for Sports Narrative\",\"url\":\"https://www.semanticscholar.org/paper/f5876f67129a80a1ee753f715efcd2e2109bf432\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/3240508.3240677\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5abe63d687f927a0ac61e9ad62f88b355d89caf\",\"title\":\"Spotting and Aggregating Salient Regions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e5abe63d687f927a0ac61e9ad62f88b355d89caf\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"47381330\",\"name\":\"A. Dimou\"},{\"authorId\":\"33961149\",\"name\":\"G. Papadopoulos\"},{\"authorId\":\"1747572\",\"name\":\"P. Daras\"}],\"doi\":\"10.1109/ICE.2019.8792602\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"title\":\"Incorporating Textual Similarity in Video Captioning Schemes\",\"url\":\"https://www.semanticscholar.org/paper/2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"venue\":\"2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)\",\"year\":2019},{\"arxivId\":\"1608.03819\",\"authors\":[{\"authorId\":\"2047692\",\"name\":\"Chenyou Fan\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9bdc406ad9e9fc0ce356e6d0e53780534f418849\",\"title\":\"DeepDiary: Automatic Caption Generation for Lifelogging Image Streams\",\"url\":\"https://www.semanticscholar.org/paper/9bdc406ad9e9fc0ce356e6d0e53780534f418849\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2636941\",\"name\":\"D. Belanger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c03a8a2cbdb9d209040d76e4450ef1135cf1c793\",\"title\":\"Deep Energy-Based Models for Structured Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c03a8a2cbdb9d209040d76e4450ef1135cf1c793\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3123266.3123391\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"39836fbbcd2a664edb31119e88870c38b83df352\",\"title\":\"Adaptively Attending to Visual Attributes and Linguistic Knowledge for Captioning\",\"url\":\"https://www.semanticscholar.org/paper/39836fbbcd2a664edb31119e88870c38b83df352\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20632291\",\"name\":\"Shiliang Sun\"},{\"authorId\":\"145307050\",\"name\":\"Liang Mao\"},{\"authorId\":\"66550933\",\"name\":\"Ziang Dong\"},{\"authorId\":\"4382815\",\"name\":\"Lidan Wu\"}],\"doi\":\"10.1007/978-981-13-3029-2_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"173a286cfb01a605641c8a7c1c110d648b404459\",\"title\":\"Multiview Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/173a286cfb01a605641c8a7c1c110d648b404459\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1610.02616\",\"authors\":[{\"authorId\":\"2002678\",\"name\":\"Zecheng Xie\"},{\"authorId\":\"46554940\",\"name\":\"Zenghui Sun\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"},{\"authorId\":\"143946730\",\"name\":\"H. Ni\"},{\"authorId\":\"144749402\",\"name\":\"Terry Lyons\"}],\"doi\":\"10.1109/TPAMI.2017.2732978\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"20014e7cf20e3513eb3cd70dda1dd3e15e6e22f1\",\"title\":\"Learning Spatial-Semantic Context with Fully Convolutional Recurrent Network for Online Handwritten Chinese Text Recognition\",\"url\":\"https://www.semanticscholar.org/paper/20014e7cf20e3513eb3cd70dda1dd3e15e6e22f1\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"2011.09046\",\"authors\":[{\"authorId\":\"1490732820\",\"name\":\"Bowen Zhang\"},{\"authorId\":\"2804000\",\"name\":\"Hexiang Hu\"},{\"authorId\":\"2119006\",\"name\":\"Joonseok Lee\"},{\"authorId\":\"145327825\",\"name\":\"Mingde Zhao\"},{\"authorId\":\"40600020\",\"name\":\"Sheide Chammas\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"39543696\",\"name\":\"Fei Sha\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe21d30afa60f8ab72da309ca0a80eee1ac07a66\",\"title\":\"A Hierarchical Multi-Modal Encoder for Moment Localization in Video Corpus\",\"url\":\"https://www.semanticscholar.org/paper/fe21d30afa60f8ab72da309ca0a80eee1ac07a66\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"}],\"doi\":\"10.1007/978-3-030-03243-2_861-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3e1fef773258fa4591565394a2308a9cc4a1c748\",\"title\":\"Few-Shot Learning\",\"url\":\"https://www.semanticscholar.org/paper/3e1fef773258fa4591565394a2308a9cc4a1c748\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000421075\",\"name\":\"Jingxu Lin\"},{\"authorId\":\"108624584\",\"name\":\"Sheng-hua Zhong\"}],\"doi\":\"10.1109/ICTAI50040.2020.00176\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7f0cbd289ee9f17aad546d78c14830c99e3d37d\",\"title\":\"Bi-Directional Self-Attention with Relative Positional Encoding for Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/f7f0cbd289ee9f17aad546d78c14830c99e3d37d\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":\"1912.04608\",\"authors\":[{\"authorId\":\"1383481973\",\"name\":\"Yan Bin Ng\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b99ef13af9802bc77fb1b51dc68538ef7a01a5e1\",\"title\":\"Forecasting Future Sequence of Actions to Complete an Activity\",\"url\":\"https://www.semanticscholar.org/paper/b99ef13af9802bc77fb1b51dc68538ef7a01a5e1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1702.05538\",\"authors\":[{\"authorId\":\"32097919\",\"name\":\"Terrance Devries\"},{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82070bef06578a24e63b2b739ec86d4d31bb576a\",\"title\":\"Dataset Augmentation in Feature Space\",\"url\":\"https://www.semanticscholar.org/paper/82070bef06578a24e63b2b739ec86d4d31bb576a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1947015\",\"name\":\"Zhaowei Qu\"},{\"authorId\":\"1845790885\",\"name\":\"Luhan Zhang\"},{\"authorId\":\"38435706\",\"name\":\"X. Wang\"},{\"authorId\":\"2148442\",\"name\":\"Bingyu Cao\"},{\"authorId\":\"93648239\",\"name\":\"Yueli Li\"},{\"authorId\":\"145987554\",\"name\":\"F. Li\"}],\"doi\":\"10.1109/IWCMC48107.2020.9148294\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e7c1b1ec7fd51c8ee9e2b45eee9c095834b0b56\",\"title\":\"KSF-ST: Video Captioning Based on Key Semantic Frames Extraction and Spatio-Temporal Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/8e7c1b1ec7fd51c8ee9e2b45eee9c095834b0b56\",\"venue\":\"2020 International Wireless Communications and Mobile Computing (IWCMC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"11445222\",\"name\":\"H. Wang\"}],\"doi\":\"10.1609/AAAI.V34I07.7012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d718baf9187fa5851a9389e5e250d47ba1210e0e\",\"title\":\"FASTER Recurrent Networks for Efficient Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/d718baf9187fa5851a9389e5e250d47ba1210e0e\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1965909970\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"1755773\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930562\",\"name\":\"G. Chen\"},{\"authorId\":\"153016830\",\"name\":\"J. Guo\"}],\"doi\":\"10.1109/ACCESS.2020.3021857\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"title\":\"Understanding Objects in Video: Object-Oriented Video Captioning via Structured Trajectory and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143860910\",\"name\":\"N. Zhao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"},{\"authorId\":\"47446553\",\"name\":\"M. Wang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/TMM.2017.2722687\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e9b97cff4bf7ca4029cbe6b3919f796c6668e1a1\",\"title\":\"VideoWhisper: Toward Discriminative Unsupervised Video Feature Learning With Attention-Based Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/e9b97cff4bf7ca4029cbe6b3919f796c6668e1a1\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":\"1708.02977\",\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.18653/v1/D17-1101\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0e404fd1adaa0fa7b1ef12b4b828db3d497ab1c\",\"title\":\"Hierarchically-Attentive RNN for Album Summarization and Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/b0e404fd1adaa0fa7b1ef12b4b828db3d497ab1c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":null,\"name\":\"Juncheng Li\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"},{\"authorId\":\"2968713\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1007/s13735-018-00166-3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6305115f393d96df92f9044b8951969e28aa7114\",\"title\":\"Joint embeddings with multimodal cues for video-text retrieval\",\"url\":\"https://www.semanticscholar.org/paper/6305115f393d96df92f9044b8951969e28aa7114\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2018},{\"arxivId\":\"1808.08732\",\"authors\":[{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"7896029\",\"name\":\"Yuanxin Liu\"},{\"authorId\":\"1781885\",\"name\":\"Houfeng Wang\"},{\"authorId\":\"2511091\",\"name\":\"X. Sun\"}],\"doi\":\"10.18653/v1/D18-1013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8968072ad12bcb96c513ae1c01abf6abdae810df\",\"title\":\"simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions\",\"url\":\"https://www.semanticscholar.org/paper/8968072ad12bcb96c513ae1c01abf6abdae810df\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1704.04497\",\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.149\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"title\":\"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1606.04631\",\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"83672162\",\"name\":\"Zi Huang\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967258\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"title\":\"Bidirectional Long-Short Term Memory for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1705.09021\",\"authors\":[{\"authorId\":\"3112203\",\"name\":\"Yongqiang Huang\"},{\"authorId\":\"144825829\",\"name\":\"Y. Sun\"}],\"doi\":\"10.1109/IROS.2017.8206626\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02b0552c980e75a84df44876f602a2d3d2ee8794\",\"title\":\"Learning to pour\",\"url\":\"https://www.semanticscholar.org/paper/02b0552c980e75a84df44876f602a2d3d2ee8794\",\"venue\":\"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2017},{\"arxivId\":\"1906.04226\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"},{\"authorId\":\"143907244\",\"name\":\"Yi Yang\"},{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a71b7cf982cf1b54cda4f0746d4ba7c7ea3f4e4\",\"title\":\"FASTER Recurrent Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/8a71b7cf982cf1b54cda4f0746d4ba7c7ea3f4e4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"1791344388\",\"name\":\"Lei Ji\"},{\"authorId\":\"1783553\",\"name\":\"Zhen-dong Niu\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/3394171.3413498\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"title\":\"Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning\",\"url\":\"https://www.semanticscholar.org/paper/de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1609.08124\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2068f66a10254d457cdb5fab74b0128b24bfdb65\",\"title\":\"Learning Language-Visual Embedding for Movie Understanding with Natural-Language\",\"url\":\"https://www.semanticscholar.org/paper/2068f66a10254d457cdb5fab74b0128b24bfdb65\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1611.09502\",\"authors\":[{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.435\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ab4c97575b17b9ce9b7523743ce7cae4fa9572e5\",\"title\":\"Deep Quantization: Encoding Convolutional Activations with Deep Generative Model\",\"url\":\"https://www.semanticscholar.org/paper/ab4c97575b17b9ce9b7523743ce7cae4fa9572e5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1709.06531\",\"authors\":[{\"authorId\":\"1756362\",\"name\":\"Swathikiran Sudhakaran\"},{\"authorId\":\"1717522\",\"name\":\"O. Lanz\"}],\"doi\":\"10.1109/AVSS.2017.8078468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"945f5af596ec2e6bad6df09bef92eae9374c4338\",\"title\":\"Learning to detect violent videos using convolutional long short-term memory\",\"url\":\"https://www.semanticscholar.org/paper/945f5af596ec2e6bad6df09bef92eae9374c4338\",\"venue\":\"2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49813626\",\"name\":\"Y. Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eecfaf49500434d91970b24831081d5d2c68697e\",\"title\":\"Sequence to Sequence Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/eecfaf49500434d91970b24831081d5d2c68697e\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3127901\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"title\":\"Knowing Yourself: Improving Video Caption via In-depth Recap\",\"url\":\"https://www.semanticscholar.org/paper/3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5813a4a0cca115b05e03d8d8c1ac8bf07176e96\",\"title\":\"Supplementary Material : Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/d5813a4a0cca115b05e03d8d8c1ac8bf07176e96\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2474496\",\"name\":\"Haithem Afli\"},{\"authorId\":\"3330863\",\"name\":\"F. Hu\"},{\"authorId\":\"39785875\",\"name\":\"Jinhua Du\"},{\"authorId\":\"36673232\",\"name\":\"D. Cosgrove\"},{\"authorId\":\"145470864\",\"name\":\"Kevin McGuinness\"},{\"authorId\":\"98536322\",\"name\":\"N. O'Connor\"},{\"authorId\":\"40063957\",\"name\":\"Eric Arazo Sanchez\"},{\"authorId\":\"49640190\",\"name\":\"Jiang Zhou\"},{\"authorId\":\"72191633\",\"name\":\"A. F. Smeaton\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9d1e975e5cb7996a49785153cae31802e0b2b895\",\"title\":\"Dublin City University Participation in the VTT Track at TRECVid 2017\",\"url\":\"https://www.semanticscholar.org/paper/9d1e975e5cb7996a49785153cae31802e0b2b895\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chi Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ad9e18e919b4452bd6f021fd7c52c0e361f62d4\",\"title\":\"Evolution of A Common Vector Space Approach to Multi-Modal Problems\",\"url\":\"https://www.semanticscholar.org/paper/5ad9e18e919b4452bd6f021fd7c52c0e361f62d4\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"46697303\",\"name\":\"Dawei Xu\"},{\"authorId\":\"2481620\",\"name\":\"Shimin Feng\"},{\"authorId\":\"3222657\",\"name\":\"X. Liu\"}],\"doi\":\"10.1109/GCWkshps45667.2019.9024615\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d4b6ca6991dd154362791fa068cb8405b7eba09d\",\"title\":\"Unsafe Action Recognition of Miners Based on Video Description\",\"url\":\"https://www.semanticscholar.org/paper/d4b6ca6991dd154362791fa068cb8405b7eba09d\",\"venue\":\"2019 IEEE Globecom Workshops (GC Wkshps)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"af5b71c042e2bf39e5085ad5b5f1215e129989df\",\"title\":\"Deep Learning for Semantic Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/af5b71c042e2bf39e5085ad5b5f1215e129989df\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"title\":\"LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/CRV.2017.51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6718f2feea2d16b894b738551c38871c8afee11b\",\"title\":\"Towards a Knowledge-Based Approach for Generating Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/6718f2feea2d16b894b738551c38871c8afee11b\",\"venue\":\"2017 14th Conference on Computer and Robot Vision (CRV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004502909\",\"name\":\"K. JeevithaV\"},{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"}],\"doi\":\"10.1109/incet49848.2020.9154103\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a1504b2eafe0d02f7777804b8d6c9631cfbe2e30\",\"title\":\"Natural Language Description for Videos Using NetVLAD and Attentional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/a1504b2eafe0d02f7777804b8d6c9631cfbe2e30\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77537913\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/s11042-019-07948-9\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fea09a15a91558db9090c4cb4b11184b91310839\",\"title\":\"Deep learning ensemble with data augmentation using a transcoder in visual description\",\"url\":\"https://www.semanticscholar.org/paper/fea09a15a91558db9090c4cb4b11184b91310839\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"14618116\",\"name\":\"Chongyang Gao\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1016/J.PATREC.2018.07.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab27d39857f613af36eff3fa3796904f474f8cbd\",\"title\":\"Sequence in sequence for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ab27d39857f613af36eff3fa3796904f474f8cbd\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145098331\",\"name\":\"S. Han\"},{\"authorId\":\"92134816\",\"name\":\"Bo-Won Go\"},{\"authorId\":\"145530103\",\"name\":\"H. Choi\"}],\"doi\":\"10.1109/BIGCOMP.2019.8679213\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eadde672d900b7cc3949a854d2b7b850ab2e9c5c\",\"title\":\"Multiple Videos Captioning Model for Video Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/eadde672d900b7cc3949a854d2b7b850ab2e9c5c\",\"venue\":\"2019 IEEE International Conference on Big Data and Smart Computing (BigComp)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2018/164\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2f4821a615f08fdad69957a19366c79d939bfd5f\",\"title\":\"Video Captioning with Tube Features\",\"url\":\"https://www.semanticscholar.org/paper/2f4821a615f08fdad69957a19366c79d939bfd5f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1824669111\",\"name\":\"Jiangbin Zheng\"},{\"authorId\":\"152254348\",\"name\":\"Z. Zhao\"},{\"authorId\":\"50133268\",\"name\":\"M. Chen\"},{\"authorId\":\"49251781\",\"name\":\"J. Chen\"},{\"authorId\":\"46740253\",\"name\":\"C. Wu\"},{\"authorId\":\"50579754\",\"name\":\"Y. Chen\"},{\"authorId\":\"1755321\",\"name\":\"X. Shi\"},{\"authorId\":\"2007715582\",\"name\":\"Yiqi Tong\"}],\"doi\":\"10.1155/2020/8816125\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c2b91d5074f848c8543e4858565b60e2e755139a\",\"title\":\"An Improved Sign Language Translation Model with Explainable Adaptations for Processing Long Sign Sentences\",\"url\":\"https://www.semanticscholar.org/paper/c2b91d5074f848c8543e4858565b60e2e755139a\",\"venue\":\"Computational intelligence and neuroscience\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1016/j.neucom.2019.08.042\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"00b350e4211dd5ed4791744920e664880cd3fd3a\",\"title\":\"Recurrent convolutional video captioning with global and local attention\",\"url\":\"https://www.semanticscholar.org/paper/00b350e4211dd5ed4791744920e664880cd3fd3a\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2582104\",\"name\":\"Bojan Kolosnjaji\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"758a6bcf56a3213a3cc18fc9a080d0822cb9905f\",\"title\":\"Machine Learning for Anomaly Detection under Constraints\",\"url\":\"https://www.semanticscholar.org/paper/758a6bcf56a3213a3cc18fc9a080d0822cb9905f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48490580\",\"name\":\"J. Park\"},{\"authorId\":\"35409051\",\"name\":\"Chibon Song\"},{\"authorId\":\"47180565\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/ICIIBMS.2017.8279760\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"title\":\"A study of evaluation metrics and datasets for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"venue\":\"2017 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26950695\",\"name\":\"Chensi Mao\"},{\"authorId\":\"2287648\",\"name\":\"S. Huang\"},{\"authorId\":\"1876419\",\"name\":\"X. Li\"},{\"authorId\":\"2066033\",\"name\":\"Z. Ye\"}],\"doi\":\"10.1007/978-981-10-7299-4_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd85c61a3f9bb6e7a7ff262133947f7b9f770644\",\"title\":\"Chinese Sign Language Recognition with Sequence to Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/dd85c61a3f9bb6e7a7ff262133947f7b9f770644\",\"venue\":\"CCCV\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122263328\",\"name\":\"C. Anantharaj\"},{\"authorId\":\"14399385\",\"name\":\"V. Arutchelvan\"},{\"authorId\":\"31680942\",\"name\":\"N. A. Kumar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f070494c43852cf7198f9ecbe669e849ad6bd7ab\",\"title\":\"International Journal of Innovative Technology and Exploring Engineering (IJITEE)\",\"url\":\"https://www.semanticscholar.org/paper/f070494c43852cf7198f9ecbe669e849ad6bd7ab\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1802.02305\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"}],\"doi\":\"10.1109/TIP.2018.2814344\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f97e9818a8055668f9db7967b076dd036d25c417\",\"title\":\"Self-Supervised Video Hashing With Hierarchical Binary Auto-Encoder\",\"url\":\"https://www.semanticscholar.org/paper/f97e9818a8055668f9db7967b076dd036d25c417\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2210067\",\"name\":\"Asma Belhadi\"},{\"authorId\":\"1685655461\",\"name\":\"Youcef Djenouri\"},{\"authorId\":\"50554965\",\"name\":\"Chun-Wei Lin\"},{\"authorId\":\"143947197\",\"name\":\"A. Cano\"}],\"doi\":\"10.1145/3399631\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20dc4f4647bf492b46594efb61a2f0087bdbfaeb\",\"title\":\"Trajectory Outlier Detection\",\"url\":\"https://www.semanticscholar.org/paper/20dc4f4647bf492b46594efb61a2f0087bdbfaeb\",\"venue\":\"ACM Trans. Manag. Inf. Syst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2015.7298878\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5838af587938e74b5758414c384dcf16dd6e1d1e\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/5838af587938e74b5758414c384dcf16dd6e1d1e\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387548078\",\"name\":\"K. Lin\"},{\"authorId\":\"1738276592\",\"name\":\"Zhuoxin Gan\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.98\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3263b941d0a77bbd2040612ec774ef063ef64c48\",\"title\":\"Semi-Supervised Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3263b941d0a77bbd2040612ec774ef063ef64c48\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1506971452\",\"name\":\"Yu-min Yao\"},{\"authorId\":\"1996992\",\"name\":\"L. Song\"},{\"authorId\":\"145473770\",\"name\":\"J. Ye\"}],\"doi\":\"10.3390/s20041134\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3df6d227800b142911c2fbaae8727120f1cf132\",\"title\":\"Motion-To-BMI: Using Motion Sensors to Predict the Body Mass Index of Smartphone Users\",\"url\":\"https://www.semanticscholar.org/paper/c3df6d227800b142911c2fbaae8727120f1cf132\",\"venue\":\"Sensors\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yu-Siang Wang\"},{\"authorId\":\"71309591\",\"name\":\"Hung-Ting Su\"},{\"authorId\":\"150053992\",\"name\":\"Chen-Hsi Chang\"},{\"authorId\":\"46270526\",\"name\":\"Z. Liu\"},{\"authorId\":\"1716836\",\"name\":\"W. Hsu\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053476\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9dd133c6baed0b080687da0954754ea23abc8ba1\",\"title\":\"Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning\",\"url\":\"https://www.semanticscholar.org/paper/9dd133c6baed0b080687da0954754ea23abc8ba1\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49308278\",\"name\":\"Yizhong Yang\"},{\"authorId\":\"50615625\",\"name\":\"Tao Zhang\"},{\"authorId\":\"151484716\",\"name\":\"J. Hu\"},{\"authorId\":\"145223020\",\"name\":\"D. Xu\"},{\"authorId\":\"2205456\",\"name\":\"G. Xie\"}],\"doi\":\"10.1109/ACCESS.2019.2930319\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17cc53381d5f8cee9df3c9d0eff510d62b9bc7b1\",\"title\":\"End-to-End Background Subtraction via a Multi-Scale Spatio-Temporal Model\",\"url\":\"https://www.semanticscholar.org/paper/17cc53381d5f8cee9df3c9d0eff510d62b9bc7b1\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1703.06585\",\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"51283515\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/ICCV.2017.321\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c18df1edc0a45891806d44896a8f666944e93d01\",\"title\":\"Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c18df1edc0a45891806d44896a8f666944e93d01\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"96066534\",\"name\":\"P. Joshi\"},{\"authorId\":\"51497543\",\"name\":\"Chitwan Saharia\"},{\"authorId\":\"1557378944\",\"name\":\"V. Singh\"},{\"authorId\":\"51267359\",\"name\":\"Digvijaysingh Gautam\"},{\"authorId\":\"145799547\",\"name\":\"Ganesh Ramakrishnan\"},{\"authorId\":\"1557645545\",\"name\":\"P. Jyothi\"}],\"doi\":\"10.1109/ICCVW.2019.00459\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"title\":\"A Tale of Two Modalities for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49685502\",\"name\":\"J. Lee\"},{\"authorId\":\"1769295\",\"name\":\"Junmo Kim\"}],\"doi\":\"10.1109/ICCE-ASIA.2018.8552140\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22409d9471b426e0bcac3f850aa16ad158b355a7\",\"title\":\"Improving Video Captioning with Non-Local Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/22409d9471b426e0bcac3f850aa16ad158b355a7\",\"venue\":\"2018 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500654996\",\"name\":\"Aske Plaat\"}],\"doi\":\"10.1007/978-3-030-59238-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"26b6477ef683ef55c453d65fbcbf4e51e0d77b07\",\"title\":\"Learning to Play: Reinforcement Learning and Games\",\"url\":\"https://www.semanticscholar.org/paper/26b6477ef683ef55c453d65fbcbf4e51e0d77b07\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145672842\",\"name\":\"J. Ball\"},{\"authorId\":\"152900419\",\"name\":\"D. Anderson\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"}],\"doi\":\"10.1117/1.JRS.11.042609\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9bfc3bd38cdaec93540b3f147d0475563ea09cb8\",\"title\":\"Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community\",\"url\":\"https://www.semanticscholar.org/paper/9bfc3bd38cdaec93540b3f147d0475563ea09cb8\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"2909406\",\"name\":\"Ming Yang\"},{\"authorId\":\"48805287\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1109/TKDE.2018.2872063\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"63787b1fcf5add4b5e25376f7bee155a39e958c3\",\"title\":\"A Survey of Multi-View Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/63787b1fcf5add4b5e25376f7bee155a39e958c3\",\"venue\":\"IEEE Transactions on Knowledge and Data Engineering\",\"year\":2019},{\"arxivId\":\"1606.07373\",\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"1732879\",\"name\":\"Lorenzo Torresani\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c69040777e2b0e1d443e22f86e45e527381d79e7\",\"title\":\"ViCom: Benchmark and Methods for Video Comprehension\",\"url\":\"https://www.semanticscholar.org/paper/c69040777e2b0e1d443e22f86e45e527381d79e7\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"144934447\",\"name\":\"M. Dom\\u00ednguez\"},{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/CVPRW.2017.274\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"477d58ad32e0e54c40da135fb8db28b23ad0ffd0\",\"title\":\"Temporally Steered Gaussian Attention for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/477d58ad32e0e54c40da135fb8db28b23ad0ffd0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"31ea3186aa7072a9e25218efe229f5ee3cca3316\",\"title\":\"A ug 2 01 7 Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/31ea3186aa7072a9e25218efe229f5ee3cca3316\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1709.00308\",\"authors\":[{\"authorId\":\"145672842\",\"name\":\"J. Ball\"},{\"authorId\":\"144319140\",\"name\":\"D. Anderson\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"}],\"doi\":\"10.1117/1.JRS.11.042609.\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d819d0949692635e73368f8879a3b2ab0462974\",\"title\":\"A Comprehensive Survey of Deep Learning in Remote Sensing: Theories, Tools and Challenges for the Community\",\"url\":\"https://www.semanticscholar.org/paper/5d819d0949692635e73368f8879a3b2ab0462974\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27575517\",\"name\":\"Dian Shao\"},{\"authorId\":\"145984817\",\"name\":\"Yu Xiong\"},{\"authorId\":\"47827957\",\"name\":\"Y. Zhao\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01240-3_13\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9925eb898e7484ae289b675785313ddb47bb20bd\",\"title\":\"Find and Focus: Retrieve and Localize Video Events with Natural Language Queries\",\"url\":\"https://www.semanticscholar.org/paper/9925eb898e7484ae289b675785313ddb47bb20bd\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921001\",\"name\":\"Spandana Gella\"},{\"authorId\":\"35084211\",\"name\":\"M. Lewis\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.18653/v1/D18-1117\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"title\":\"A Dataset for Telling the Stories of Social Media Videos\",\"url\":\"https://www.semanticscholar.org/paper/b5647cbbfdc7d1ee91a8ec264b200b66afd7b8b2\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"37498905\",\"name\":\"L. Li\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/BigMM.2018.8499257\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ae5f10acd306a7842a16542b6b236e0a964de10\",\"title\":\"Saliency-Based Spatiotemporal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7ae5f10acd306a7842a16542b6b236e0a964de10\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143868463\",\"name\":\"Hao Zhou\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"}],\"doi\":\"10.1109/ICME.2019.00223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4a391a41b386612a86f6296a7a86b0e71f1381e\",\"title\":\"Dynamic Pseudo Label Decoding for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f4a391a41b386612a86f6296a7a86b0e71f1381e\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"title\":\"Natural Language Description of Images and Videos\",\"url\":\"https://www.semanticscholar.org/paper/d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144324729\",\"name\":\"Jun Han\"},{\"authorId\":\"40505818\",\"name\":\"Chaoli Wang\"}],\"doi\":\"10.1109/TVCG.2019.2934255\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"107e9ea4a3ee66cfb85ec84dbe53e6831f793a8b\",\"title\":\"TSR-TVD: Temporal Super-Resolution for Time-Varying Data Analysis and Visualization\",\"url\":\"https://www.semanticscholar.org/paper/107e9ea4a3ee66cfb85ec84dbe53e6831f793a8b\",\"venue\":\"IEEE Transactions on Visualization and Computer Graphics\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12211701\",\"name\":\"Jianhui Han\"},{\"authorId\":\"2038787\",\"name\":\"H. Liu\"},{\"authorId\":\"152808709\",\"name\":\"M. Wang\"},{\"authorId\":\"1801439\",\"name\":\"Zhaolin Li\"},{\"authorId\":\"7549706\",\"name\":\"Youhui Zhang\"}],\"doi\":\"10.1109/TPDS.2019.2962806\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b3818789b3d64e31a16f55e83f3a00e38e127d59\",\"title\":\"ERA-LSTM: An Efficient ReRAM-Based Architecture for Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/b3818789b3d64e31a16f55e83f3a00e38e127d59\",\"venue\":\"IEEE Transactions on Parallel and Distributed Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/TIP.2020.2988435\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"title\":\"Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50111883\",\"name\":\"S. Lee\"},{\"authorId\":\"1784897\",\"name\":\"Incheol Kim\"}],\"doi\":\"10.3745/JIPS.02.0098\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc1d373ca1b7abc470bf6a6a639436ea12461378\",\"title\":\"Video Captioning with Visual and Semantic Features\",\"url\":\"https://www.semanticscholar.org/paper/fc1d373ca1b7abc470bf6a6a639436ea12461378\",\"venue\":\"J. Inf. Process. Syst.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40095300\",\"name\":\"J. Stamford\"},{\"authorId\":\"51890841\",\"name\":\"B. Peach\"}],\"doi\":\"10.1049/IC.2016.0062\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7aac942c6c0db9219635f5b27cdb74318a7a4a38\",\"title\":\"Scene Detection using Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/7aac942c6c0db9219635f5b27cdb74318a7a4a38\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1710.02254\",\"authors\":[{\"authorId\":\"118242121\",\"name\":\"Chaitanya Ahuja\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f0cd4de3cdf547dcdcc6995dca9ab3f65955b324\",\"title\":\"Lattice Recurrent Unit: Improving Convergence and Statistical Efficiency for Sequence Modeling\",\"url\":\"https://www.semanticscholar.org/paper/f0cd4de3cdf547dcdcc6995dca9ab3f65955b324\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/MSP.2017.2741510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5b803c2fee9bbf6d9132f633de70332b5e80a4d\",\"title\":\"Deep Learning for Image-to-Text Generation: A Technical Overview\",\"url\":\"https://www.semanticscholar.org/paper/c5b803c2fee9bbf6d9132f633de70332b5e80a4d\",\"venue\":\"IEEE Signal Processing Magazine\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3200630\",\"name\":\"N. Passalis\"},{\"authorId\":\"1737071\",\"name\":\"A. Tefas\"}],\"doi\":\"10.1007/978-3-319-94030-4_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b0e7e4cf96082fa87b8e423c612c7e21e06fe73d\",\"title\":\"Deep Learning Analytics\",\"url\":\"https://www.semanticscholar.org/paper/b0e7e4cf96082fa87b8e423c612c7e21e06fe73d\",\"venue\":\"Machine Learning Paradigms\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1813915\",\"name\":\"S. Liu\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3240508.3240667\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"849642b4701ac11c035326069f707f23a51a6f1a\",\"title\":\"SibNet: Sibling Convolutional Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/849642b4701ac11c035326069f707f23a51a6f1a\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145303654\",\"name\":\"Ravi Bansal\"},{\"authorId\":\"1708109\",\"name\":\"Sandip Chakraborty\"}],\"doi\":\"10.1145/3297280.3297303\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"09536a08a0e9319df478a986334314bef935dc2a\",\"title\":\"Visual content based video retrieval on natural language queries\",\"url\":\"https://www.semanticscholar.org/paper/09536a08a0e9319df478a986334314bef935dc2a\",\"venue\":\"SAC\",\"year\":2019},{\"arxivId\":\"1806.07380\",\"authors\":[{\"authorId\":\"2192306\",\"name\":\"Binbing Liao\"},{\"authorId\":\"47540100\",\"name\":\"Jingqing Zhang\"},{\"authorId\":\"49762880\",\"name\":\"Chao Wu\"},{\"authorId\":\"35379213\",\"name\":\"D. McIlwraith\"},{\"authorId\":\"48129306\",\"name\":\"T. Chen\"},{\"authorId\":\"37923938\",\"name\":\"Shengwen Yang\"},{\"authorId\":\"3194920\",\"name\":\"Y. Guo\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3219819.3219895\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e23d99764eef23b2240150c96768db8e1668a8c\",\"title\":\"Deep Sequence Learning with Auxiliary Information for Traffic Prediction\",\"url\":\"https://www.semanticscholar.org/paper/8e23d99764eef23b2240150c96768db8e1668a8c\",\"venue\":\"KDD\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49162764\",\"name\":\"T. Young\"},{\"authorId\":\"1574901544\",\"name\":\"Vlad Pandelea\"},{\"authorId\":\"1382302885\",\"name\":\"Soujanya Poria\"},{\"authorId\":\"49943757\",\"name\":\"E. Cambria\"}],\"doi\":\"10.1016/j.neucom.2019.12.126\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9673def390b874f2fe7b2fe2e72a7259989a1773\",\"title\":\"Dialogue systems with audio context\",\"url\":\"https://www.semanticscholar.org/paper/9673def390b874f2fe7b2fe2e72a7259989a1773\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1911.02739\",\"authors\":[{\"authorId\":\"72871419\",\"name\":\"Zhihan Zhang\"},{\"authorId\":\"1770874\",\"name\":\"Zhiyi Yin\"},{\"authorId\":\"1906099\",\"name\":\"Shuhuai Ren\"},{\"authorId\":\"78145275\",\"name\":\"Xinhang Li\"},{\"authorId\":\"50341802\",\"name\":\"S. Li\"}],\"doi\":\"10.1007/978-3-030-60457-8_1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"422f098e1e5b6fa34e73ad83557ba793ca3c0403\",\"title\":\"DCA: Diversified Co-Attention towards Informative Live Video Commenting\",\"url\":\"https://www.semanticscholar.org/paper/422f098e1e5b6fa34e73ad83557ba793ca3c0403\",\"venue\":\"NLPCC\",\"year\":2020},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"2006.14150\",\"authors\":[{\"authorId\":\"145749362\",\"name\":\"Jing Shi\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"47871517\",\"name\":\"Pengcheng Guo\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"144307323\",\"name\":\"Y. Fujita\"},{\"authorId\":\"46372563\",\"name\":\"Jiaming Xu\"},{\"authorId\":\"46798766\",\"name\":\"Bo Xu\"},{\"authorId\":\"144206960\",\"name\":\"Lei Xie\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1\",\"title\":\"Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals\",\"url\":\"https://www.semanticscholar.org/paper/e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2003.11618\",\"authors\":[{\"authorId\":\"48211835\",\"name\":\"J. Liu\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"5524736\",\"name\":\"Y. Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"35729970\",\"name\":\"Yiming Yang\"},{\"authorId\":\"46700348\",\"name\":\"Jing-jing Liu\"}],\"doi\":\"10.1109/cvpr42600.2020.01091\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"704ec27b8399df574a96da338c428a923509385e\",\"title\":\"Violin: A Large-Scale Dataset for Video-and-Language Inference\",\"url\":\"https://www.semanticscholar.org/paper/704ec27b8399df574a96da338c428a923509385e\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1998961496\",\"name\":\"Qiang Yang\"},{\"authorId\":\"153533704\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1752769\",\"name\":\"W. Dai\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"}],\"doi\":\"10.1017/9781139061773.022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96d278fa72d8a0d8c5d07402e90af6a79159d038\",\"title\":\"Transfer Learning in Recommender Systems\",\"url\":\"https://www.semanticscholar.org/paper/96d278fa72d8a0d8c5d07402e90af6a79159d038\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38397846\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"50857536\",\"name\":\"Z. Wang\"},{\"authorId\":\"123865558\",\"name\":\"Liang Wang\"},{\"authorId\":\"11675891\",\"name\":\"D. Feng\"},{\"authorId\":\"145808910\",\"name\":\"Tieniu Tan\"}],\"doi\":\"10.1145/3343031.3350992\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a402ff486216270c38ee72f793e49e6f4b861e1\",\"title\":\"Stacked Memory Network for Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/5a402ff486216270c38ee72f793e49e6f4b861e1\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9260404\",\"name\":\"Xiaotong Du\"},{\"authorId\":\"46685438\",\"name\":\"J. Yuan\"},{\"authorId\":\"47652550\",\"name\":\"L. Hu\"},{\"authorId\":\"122907636\",\"name\":\"Yuke Dai\"}],\"doi\":\"10.1007/s00371-018-1591-x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4170882122b559fc39ab3eafd66babe2429ba858\",\"title\":\"Description generation of open-domain videos incorporating multimodal features and bidirectional encoder\",\"url\":\"https://www.semanticscholar.org/paper/4170882122b559fc39ab3eafd66babe2429ba858\",\"venue\":\"The Visual Computer\",\"year\":2018},{\"arxivId\":\"1904.08831\",\"authors\":[{\"authorId\":\"3469366\",\"name\":\"Matthew A. Wright\"},{\"authorId\":\"103172520\",\"name\":\"Simon F. G. Ehlers\"},{\"authorId\":\"1740258\",\"name\":\"R. Horowitz\"}],\"doi\":\"10.1109/ITSC.2019.8917174\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3aa1aa31b07a7c858c751391e90cbeb526e08dfe\",\"title\":\"Neural-Attention-Based Deep Learning Architectures for Modeling Traffic Dynamics on Lane Graphs\",\"url\":\"https://www.semanticscholar.org/paper/3aa1aa31b07a7c858c751391e90cbeb526e08dfe\",\"venue\":\"2019 IEEE Intelligent Transportation Systems Conference (ITSC)\",\"year\":2019},{\"arxivId\":\"1905.09400\",\"authors\":[{\"authorId\":\"8804979\",\"name\":\"Siddhesh Khandelwal\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/ICCV.2019.00352\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f3805f0058ac3b320016ca518a98de358aca9823\",\"title\":\"AttentionRNN: A Structured Spatial Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/f3805f0058ac3b320016ca518a98de358aca9823\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"101687783\",\"name\":\"O. Friberg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba018189673d883920111184040d307153346267\",\"title\":\"Recognizing Semantics in Human Actions with Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/ba018189673d883920111184040d307153346267\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32400505\",\"name\":\"Chun-chuan Liu\"},{\"authorId\":\"145327672\",\"name\":\"Min Gao\"},{\"authorId\":\"49605996\",\"name\":\"J. Wang\"},{\"authorId\":\"2652806\",\"name\":\"Liming Zou\"},{\"authorId\":\"40584648\",\"name\":\"E. Yu\"},{\"authorId\":\"51299154\",\"name\":\"Jiande Sun\"},{\"authorId\":\"24072150\",\"name\":\"Lingchen Gu\"},{\"authorId\":\"49543928\",\"name\":\"Xiaoxi Liu\"},{\"authorId\":null,\"name\":\"Yu Lu\"},{\"authorId\":\"46867976\",\"name\":\"Y. Zhang\"},{\"authorId\":\"9549142\",\"name\":\"X. Zhao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2f6103bf2668cbe9f042d4ce2b5e8a1962069e5b\",\"title\":\"Shandong Normal University in the VTT Tasks at TRECVID 2018\",\"url\":\"https://www.semanticscholar.org/paper/2f6103bf2668cbe9f042d4ce2b5e8a1962069e5b\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":\"1608.00525\",\"authors\":[{\"authorId\":\"3081378\",\"name\":\"Varun K. Nagaraja\"},{\"authorId\":\"2852035\",\"name\":\"V. Morariu\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1007/978-3-319-46493-0_48\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"86eef3a1dff2bd2808847358cdb7f5ba2b7e0214\",\"title\":\"Modeling Context Between Objects for Referring Expression Understanding\",\"url\":\"https://www.semanticscholar.org/paper/86eef3a1dff2bd2808847358cdb7f5ba2b7e0214\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"145974111\",\"name\":\"Jun Xiao\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3123266.3123427\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"057b80e235b10799d03876ad25465208a4c64caf\",\"title\":\"Video Question Answering via Gradually Refined Attention over Appearance and Motion\",\"url\":\"https://www.semanticscholar.org/paper/057b80e235b10799d03876ad25465208a4c64caf\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2006.07972\",\"authors\":[{\"authorId\":\"47287674\",\"name\":\"Sijie He\"},{\"authorId\":\"50080204\",\"name\":\"Xinyan Li\"},{\"authorId\":\"9024969\",\"name\":\"T. DelSole\"},{\"authorId\":\"145969795\",\"name\":\"P. Ravikumar\"},{\"authorId\":\"47997468\",\"name\":\"A. Banerjee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cac3eda036b01ff0c0c6babab25fb77dd8633afb\",\"title\":\"Sub-Seasonal Climate Forecasting via Machine Learning: Challenges, Analysis, and Advances\",\"url\":\"https://www.semanticscholar.org/paper/cac3eda036b01ff0c0c6babab25fb77dd8633afb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29972240\",\"name\":\"Z. Hu\"},{\"authorId\":\"2000928\",\"name\":\"Turki Turki\"},{\"authorId\":\"11032760\",\"name\":\"Nhathai Phan\"},{\"authorId\":\"152924797\",\"name\":\"J. T. Wang\"}],\"doi\":\"10.1109/ACCESS.2018.2861223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e4554d2fba23d1c06bde7431a407319021e52de\",\"title\":\"A 3D Atrous Convolutional Long Short-Term Memory Network for Background Subtraction\",\"url\":\"https://www.semanticscholar.org/paper/2e4554d2fba23d1c06bde7431a407319021e52de\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38774604\",\"name\":\"Prasoon Goyal\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a3ae022c8552019cf15afbb0509188aa71090872\",\"title\":\"Summer 2015 Internship Report: Generating Natural Language Description for Videos\",\"url\":\"https://www.semanticscholar.org/paper/a3ae022c8552019cf15afbb0509188aa71090872\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"26559284\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"title\":\"Captioning Near-Future Activity Sequences\",\"url\":\"https://www.semanticscholar.org/paper/dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.02602\",\"authors\":[{\"authorId\":\"1383481973\",\"name\":\"Yan Bin Ng\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"42b7b4bb2f96b21c33541a83606917be5eb6abbb\",\"title\":\"Human Action Sequence Classification\",\"url\":\"https://www.semanticscholar.org/paper/42b7b4bb2f96b21c33541a83606917be5eb6abbb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"title\":\"Large-scale video analysis and understanding\",\"url\":\"https://www.semanticscholar.org/paper/6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3456962\",\"name\":\"Amar Shrestha\"},{\"authorId\":\"26474734\",\"name\":\"K. Ahmed\"},{\"authorId\":\"46393431\",\"name\":\"Yanzhi Wang\"},{\"authorId\":\"1849633\",\"name\":\"D. P. Widemann\"},{\"authorId\":\"8554041\",\"name\":\"Adam T. Moody\"},{\"authorId\":\"11149452\",\"name\":\"Brian C. Van Essen\"},{\"authorId\":\"1862322\",\"name\":\"Q. Qiu\"}],\"doi\":\"10.1109/JETCAS.2018.2856117\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"334454dcff4b5474abd457f7513da76883528ca7\",\"title\":\"Modular Spiking Neural Circuits for Mapping Long Short-Term Memory on a Neurosynaptic Processor\",\"url\":\"https://www.semanticscholar.org/paper/334454dcff4b5474abd457f7513da76883528ca7\",\"venue\":\"IEEE Journal on Emerging and Selected Topics in Circuits and Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102696161\",\"name\":\"Rui Wang\"},{\"authorId\":\"152876495\",\"name\":\"W. Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b3bd6d1e507f85b2915c03b654c39a02eb5cc2cd\",\"title\":\"A Semantic and Local Correlation Method\",\"url\":\"https://www.semanticscholar.org/paper/b3bd6d1e507f85b2915c03b654c39a02eb5cc2cd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1706.01231\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"153757316\",\"name\":\"Zhao Guo\"},{\"authorId\":\"144973314\",\"name\":\"Wu Liu\"},{\"authorId\":\"2712862\",\"name\":\"D. Zhang\"},{\"authorId\":\"152555512\",\"name\":\"Heng Tao Shen\"}],\"doi\":\"10.24963/ijcai.2017/381\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"616c2b2c8bb35b0da1feb9d869131edd5b53642a\",\"title\":\"Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/616c2b2c8bb35b0da1feb9d869131edd5b53642a\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"2011768695\",\"name\":\"Junjun Guo\"},{\"authorId\":\"2409659\",\"name\":\"Shengxiang Gao\"},{\"authorId\":\"121854326\",\"name\":\"Zhengtao Yu\"}],\"doi\":\"10.1016/j.patcog.2020.107702\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6686fadf7f7ef2283cc9286095db281f8520ec04\",\"title\":\"Enhancing the alignment between target words and corresponding frames for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/6686fadf7f7ef2283cc9286095db281f8520ec04\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70097297\",\"name\":\"Anji Liu\"},{\"authorId\":\"144400083\",\"name\":\"Y. Laili\"}],\"doi\":\"10.1016/j.neucom.2018.08.075\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e08c4f7364aa11aa61e2878f8eb773bf9ab5fa7f\",\"title\":\"Balance gate controlled deep neural network\",\"url\":\"https://www.semanticscholar.org/paper/e08c4f7364aa11aa61e2878f8eb773bf9ab5fa7f\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1605.02697\",\"authors\":[{\"authorId\":\"145478807\",\"name\":\"Mateusz Malinowski\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"}],\"doi\":\"10.1007/s11263-017-1038-2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7ddf708442dad7ed2978658b101c797c7c10220\",\"title\":\"Ask Your Neurons: A Deep Learning Approach to Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f7ddf708442dad7ed2978658b101c797c7c10220\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"2010.05379\",\"authors\":[{\"authorId\":\"27866536\",\"name\":\"Q. Wang\"},{\"authorId\":\"1725438390\",\"name\":\"H. Tan\"},{\"authorId\":\"2191455\",\"name\":\"Sheng Shen\"},{\"authorId\":\"1717098\",\"name\":\"M. W. Mahoney\"},{\"authorId\":\"9088433\",\"name\":\"Zhewei Yao\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.159\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f68429e4cb119d3ac6cfbc9be7476609878fd2fa\",\"title\":\"MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding\",\"url\":\"https://www.semanticscholar.org/paper/f68429e4cb119d3ac6cfbc9be7476609878fd2fa\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2002.11566\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":null,\"name\":\"Bing Li\"},{\"authorId\":\"39397292\",\"name\":\"Peijin Wang\"},{\"authorId\":\"48594951\",\"name\":\"Weiming Hu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1109/cvpr42600.2020.01329\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1dd557a8839733a5ee06d19989a265e61f603c1\",\"title\":\"Object Relational Graph With Teacher-Recommended Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1dd557a8839733a5ee06d19989a265e61f603c1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2005.00596\",\"authors\":[{\"authorId\":\"34145947\",\"name\":\"Zhuolin Jiang\"},{\"authorId\":\"3330139\",\"name\":\"J. Silovsk\\u00fd\"},{\"authorId\":\"143882614\",\"name\":\"M. Siu\"},{\"authorId\":\"144339076\",\"name\":\"W. Hartmann\"},{\"authorId\":\"1793645\",\"name\":\"H. Gish\"},{\"authorId\":\"32484187\",\"name\":\"S. Adali\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d4ead18fc29e22a5f8b4d42e2f6041861b65b58\",\"title\":\"Learning from Noisy Labels with Noise Modeling Network\",\"url\":\"https://www.semanticscholar.org/paper/0d4ead18fc29e22a5f8b4d42e2f6041861b65b58\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22252150\",\"name\":\"D. Narayanan\"},{\"authorId\":\"3459901\",\"name\":\"Aaron Harlap\"},{\"authorId\":\"3078275\",\"name\":\"Amar Phanishayee\"},{\"authorId\":\"1720084\",\"name\":\"Vivek Seshadri\"},{\"authorId\":\"7692691\",\"name\":\"Nikhil R. Devanur\"},{\"authorId\":\"1707164\",\"name\":\"G. Ganger\"},{\"authorId\":\"1974678\",\"name\":\"Phillip B. Gibbons\"},{\"authorId\":\"143834867\",\"name\":\"M. Zaharia\"}],\"doi\":\"10.1145/3341301.3359646\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3fd7c9ba742dd2b435afa75217847e5087e2f2a8\",\"title\":\"PipeDream: generalized pipeline parallelism for DNN training\",\"url\":\"https://www.semanticscholar.org/paper/3fd7c9ba742dd2b435afa75217847e5087e2f2a8\",\"venue\":\"SOSP\",\"year\":2019},{\"arxivId\":\"1711.06330\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":\"10.1109/CVPR.2018.00710\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66aebb3af16aaa78579344784212ae10f60ec27e\",\"title\":\"Attend and Interact: Higher-Order Object Interactions for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/66aebb3af16aaa78579344784212ae10f60ec27e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1908.00275\",\"authors\":[{\"authorId\":\"31698884\",\"name\":\"M. Hua\"},{\"authorId\":\"11802634\",\"name\":\"Yibing Nan\"},{\"authorId\":\"143763659\",\"name\":\"S. Lian\"}],\"doi\":\"10.1109/ICCVW.2019.00158\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"90b11c6848e2b99674edb7ed6b16e8ba68b64036\",\"title\":\"Falls Prediction Based on Body Keypoints and Seq2Seq Architecture\",\"url\":\"https://www.semanticscholar.org/paper/90b11c6848e2b99674edb7ed6b16e8ba68b64036\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3466783\",\"name\":\"Gwenaelle Cunha Sergio\"},{\"authorId\":\"91899029\",\"name\":\"Minho Lee\"}],\"doi\":\"10.1007/s11042-020-09636-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8c83cea1314107698ca3c873f49823b2d352bf4\",\"title\":\"Scene2Wav: a deep convolutional sequence-to-conditional SampleRNN for emotional scene musicalization\",\"url\":\"https://www.semanticscholar.org/paper/c8c83cea1314107698ca3c873f49823b2d352bf4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2008034295\",\"name\":\"Nayu Liu\"},{\"authorId\":\"2946890\",\"name\":\"Xian Sun\"},{\"authorId\":\"150129463\",\"name\":\"Hongfeng Yu\"},{\"authorId\":\"1485232293\",\"name\":\"Wenkai Zhang\"},{\"authorId\":\"3287827\",\"name\":\"Guangluan Xu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.144\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e993f052fc1a860011e34e7043af9b3b1a2c8897\",\"title\":\"Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos\",\"url\":\"https://www.semanticscholar.org/paper/e993f052fc1a860011e34e7043af9b3b1a2c8897\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3322871\",\"name\":\"Zhuosheng Zhang\"},{\"authorId\":\"2849740\",\"name\":\"Kehai Chen\"},{\"authorId\":\"108085542\",\"name\":\"Rui Wang\"},{\"authorId\":\"1802277\",\"name\":\"M. Utiyama\"},{\"authorId\":\"1698363\",\"name\":\"Eiichiro Sumita\"},{\"authorId\":\"30658665\",\"name\":\"Z. Li\"},{\"authorId\":\"47941144\",\"name\":\"Hai Zhao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"834e987e107e251441970a6e5058446f1dd8a005\",\"title\":\"Neural Machine Translation with Universal Visual Representation\",\"url\":\"https://www.semanticscholar.org/paper/834e987e107e251441970a6e5058446f1dd8a005\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"},{\"authorId\":\"40286484\",\"name\":\"Xi Li\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"7181955\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/TIP.2018.2889265\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e3ec088d483f2302aa8b67069c82e8b59deb16d\",\"title\":\"User-Ranking Video Summarization With Multi-Stage Spatio\\u2013Temporal Representation\",\"url\":\"https://www.semanticscholar.org/paper/8e3ec088d483f2302aa8b67069c82e8b59deb16d\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"2007.03848\",\"authors\":[{\"authorId\":\"1947101\",\"name\":\"Shijie Geng\"},{\"authorId\":\"144740494\",\"name\":\"Peng Gao\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"9332945\",\"name\":\"Jonathan Le Roux\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03598364626c419d3a2578b5c22403f0dd246e99\",\"title\":\"Spatio-Temporal Scene Graphs for Video Dialog\",\"url\":\"https://www.semanticscholar.org/paper/03598364626c419d3a2578b5c22403f0dd246e99\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yizhou Wang\"},{\"authorId\":\"49165505\",\"name\":\"Zhongyu Jiang\"},{\"authorId\":\"49779773\",\"name\":\"Xiangyu Gao\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"3232358\",\"name\":\"Guanbin Xing\"},{\"authorId\":null,\"name\":\"Hui Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b858058c04da3871848b81e8d8304e24c1282d5\",\"title\":\"RODNet: Radar Object Detection Using Cross-Modal Supervision\",\"url\":\"https://www.semanticscholar.org/paper/3b858058c04da3871848b81e8d8304e24c1282d5\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40893753\",\"name\":\"A. Dilawari\"},{\"authorId\":\"35528948\",\"name\":\"M. A. Khan\"}],\"doi\":\"10.1109/ACCESS.2019.2902507\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1fe47bbcd536bb9ce80661b17dae92f41328e91f\",\"title\":\"ASoVS: Abstractive Summarization of Video Sequences\",\"url\":\"https://www.semanticscholar.org/paper/1fe47bbcd536bb9ce80661b17dae92f41328e91f\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"47659605\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/TPAMI.2017.2700381\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17d4fee6b21c9277375d6cf0c9087828595009b6\",\"title\":\"Retrieval of Sentence Sequences for an Image Stream via Coherence Recurrent Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/17d4fee6b21c9277375d6cf0c9087828595009b6\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"1708.09522\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c51069d03974bd28dd821142a852ec24ce7546a\",\"title\":\"Action Classification and Highlighting in Videos\",\"url\":\"https://www.semanticscholar.org/paper/2c51069d03974bd28dd821142a852ec24ce7546a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2579920\",\"name\":\"Haojie Li\"},{\"authorId\":\"66939932\",\"name\":\"Sihang Wu\"},{\"authorId\":\"1860840\",\"name\":\"Shuangping Huang\"},{\"authorId\":\"144847940\",\"name\":\"K. Lam\"},{\"authorId\":\"67023635\",\"name\":\"Xiao-Fen Xing\"}],\"doi\":\"10.1109/ACCESS.2019.2958405\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"529ce7b768a1cd88d3bafe93a581ccfd0c441402\",\"title\":\"Deep Motion-Appearance Convolutions for Robust Visual Tracking\",\"url\":\"https://www.semanticscholar.org/paper/529ce7b768a1cd88d3bafe93a581ccfd0c441402\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2736335\",\"name\":\"Q. Abbas\"},{\"authorId\":\"32041482\",\"name\":\"M. Ibrahim\"},{\"authorId\":\"144889214\",\"name\":\"M. Jaffar\"}],\"doi\":\"10.1007/s11042-017-5438-7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c250c1b8f160e953c2ac0253860abd6a27b8f7f7\",\"title\":\"Video scene analysis: an overview and challenges on deep learning algorithms\",\"url\":\"https://www.semanticscholar.org/paper/c250c1b8f160e953c2ac0253860abd6a27b8f7f7\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2017},{\"arxivId\":\"1807.09986\",\"authors\":[{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1007/978-3-030-01216-8_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"title\":\"Recurrent Fusion Network for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/04cd9168dcf1a0d2cf01db95a1af53d0900bc346\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2313517\",\"name\":\"Abhishek Das\"},{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"39855500\",\"name\":\"K. Gupta\"},{\"authorId\":\"1899992\",\"name\":\"Avi Singh\"},{\"authorId\":\"24508084\",\"name\":\"Deshraj Yadav\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"1746610\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1109/TPAMI.2018.2828437\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c16428a0bae507d2a1785860f07168a807d8e59\",\"title\":\"Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/4c16428a0bae507d2a1785860f07168a807d8e59\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":\"10.1016/j.csl.2020.101102\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a00a2b6eb505a172a36de81dc803a9d45597bc8a\",\"title\":\"Investigating topics, audio representations and attention for multimodal scene-aware dialog\",\"url\":\"https://www.semanticscholar.org/paper/a00a2b6eb505a172a36de81dc803a9d45597bc8a\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":\"2005.03804\",\"authors\":[{\"authorId\":\"3377097\",\"name\":\"A. Sharghi\"},{\"authorId\":\"1700665\",\"name\":\"N. Lobo\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7cd871b42efb42f507444386e4317efd7dfc10c\",\"title\":\"Text Synopsis Generation for Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/d7cd871b42efb42f507444386e4317efd7dfc10c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1804.05113\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1609/AAAI.V33I01.33019062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"title\":\"Multilevel Language and Vision Integration for Text-to-Clip Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150326205\",\"name\":\"T. Yuan\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"2199129\",\"name\":\"Tejaswini Ananthanarayana\"},{\"authorId\":\"48935225\",\"name\":\"C. Zhang\"},{\"authorId\":\"153757820\",\"name\":\"Aneesh Bhat\"},{\"authorId\":\"150296005\",\"name\":\"Sahaj Gandhi\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/FG.2019.8756506\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"420b1d8b55ecf715db55d829a392d06b5507caeb\",\"title\":\"Large Scale Sign Language Interpretation\",\"url\":\"https://www.semanticscholar.org/paper/420b1d8b55ecf715db55d829a392d06b5507caeb\",\"venue\":\"2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"153489843\",\"name\":\"T. Chen\"},{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"145909567\",\"name\":\"Wanli Yu\"}],\"doi\":\"10.1109/ACCESS.2020.3010872\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"title\":\"Attention-Based Convolutional LSTM for Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1007/978-3-319-48881-3_11\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d2cda0dbb8b2e83ce3e70d818f78d2add803c661\",\"title\":\"Automatic Video Captioning via Multi-channel Sequential Encoding\",\"url\":\"https://www.semanticscholar.org/paper/d2cda0dbb8b2e83ce3e70d818f78d2add803c661\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"3422202\",\"name\":\"Dong Huk Park\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1145/2964284.2984066\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"61d409b92860480a9188d23ba59b822ddc6331f9\",\"title\":\"Multimodal Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61d409b92860480a9188d23ba59b822ddc6331f9\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"144016781\",\"name\":\"Heng Ji\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"9546964\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"1817166\",\"name\":\"Clare R. Voss\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1691ea87ae353949331dd3e004391162fe52e071\",\"title\":\"Event Extraction Entity Extraction and Linking Document Retrieval Entities Types coup detained Attack Arrest-Jail Events Types KaVD\",\"url\":\"https://www.semanticscholar.org/paper/1691ea87ae353949331dd3e004391162fe52e071\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"5477477\",\"name\":\"Martin Renqiang Min\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"title\":\"Recent work often develops a probabilistic model of the caption , conditioned on a video\",\"url\":\"https://www.semanticscholar.org/paper/dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":null,\"name\":\"Ning Xie\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TCYB.2018.2831447\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"af6d6271f317a1a5a30908fdeac0fc054cd0493b\",\"title\":\"Describing Video With Attention-Based Bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/af6d6271f317a1a5a30908fdeac0fc054cd0493b\",\"venue\":\"IEEE Transactions on Cybernetics\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492114969\",\"name\":\"Jason Li\"},{\"authorId\":null,\"name\":\"Helen Qiu jasonkli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3e2bbf3cdc651c1531a9bafbda59a6807417d578\",\"title\":\"Comparing Attention-based Neural Architectures for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3e2bbf3cdc651c1531a9bafbda59a6807417d578\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1811.10553\",\"authors\":[{\"authorId\":\"3039477\",\"name\":\"Alvaro Ulloa\"},{\"authorId\":\"5276826\",\"name\":\"Linyuan Jing\"},{\"authorId\":\"7913725\",\"name\":\"C. W. Good\"},{\"authorId\":\"6337685\",\"name\":\"D. vanMaanen\"},{\"authorId\":\"144997052\",\"name\":\"Sushravya Raghunath\"},{\"authorId\":\"2275724\",\"name\":\"Jonathan D. Suever\"},{\"authorId\":\"3586989\",\"name\":\"Christopher D. Nevius\"},{\"authorId\":\"3895478\",\"name\":\"Gregory J. Wehner\"},{\"authorId\":\"3466971\",\"name\":\"D. Hartzel\"},{\"authorId\":\"3466202\",\"name\":\"Joseph B. Leader\"},{\"authorId\":\"13999508\",\"name\":\"A. Alsaid\"},{\"authorId\":\"50841642\",\"name\":\"A. Patel\"},{\"authorId\":\"145601639\",\"name\":\"H. L. Kirchner\"},{\"authorId\":\"2149632\",\"name\":\"C. Haggerty\"},{\"authorId\":\"5732263\",\"name\":\"B. Fornwalt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a51a467c42632f1074c514274378cbbcede00918\",\"title\":\"A deep neural network predicts survival after heart imaging better than cardiologists\",\"url\":\"https://www.semanticscholar.org/paper/a51a467c42632f1074c514274378cbbcede00918\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3367041\",\"name\":\"Himanshu Buckchash\"},{\"authorId\":\"1796442\",\"name\":\"B. Raman\"}],\"doi\":\"10.1109/ACCESS.2020.2985318\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a9e20bb1ecf7140dfe826e124f1d1a3282d24f7\",\"title\":\"Variational Conditioning of Deep Recurrent Networks for Modeling Complex Motion Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/0a9e20bb1ecf7140dfe826e124f1d1a3282d24f7\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2002.03740\",\"authors\":[{\"authorId\":\"51055350\",\"name\":\"Shuwen Xiao\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"48805561\",\"name\":\"Zijian Zhang\"},{\"authorId\":\"1749272\",\"name\":\"Ziyu Guan\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2020.2985868\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd93a358c7e441855ba7fd46872099da6dc23b5a\",\"title\":\"Query-Biased Self-Attentive Network for Query-Focused Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/dd93a358c7e441855ba7fd46872099da6dc23b5a\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47523598\",\"name\":\"T. Nguyen\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/WNYIPW.2017.8356255\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3417673c59544fcd33820a0a583a7543c70ac595\",\"title\":\"Multistream hierarchical boundary network for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/3417673c59544fcd33820a0a583a7543c70ac595\",\"venue\":\"2017 IEEE Western New York Image and Signal Processing Workshop (WNYISPW)\",\"year\":2017},{\"arxivId\":\"1708.04923\",\"authors\":[{\"authorId\":\"34065547\",\"name\":\"Naveen Panwar\"},{\"authorId\":\"3064586\",\"name\":\"Shreya Khare\"},{\"authorId\":\"3350685\",\"name\":\"Neelamadhav Gantayat\"},{\"authorId\":\"19509693\",\"name\":\"Rahul Aralikatte\"},{\"authorId\":\"40147882\",\"name\":\"Senthil Mani\"},{\"authorId\":\"2473935\",\"name\":\"A. Sankaran\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"236dd39aa92c93b7114befcfc16c146ce809d52c\",\"title\":\"mAnI: Movie Amalgamation using Neural Imitation\",\"url\":\"https://www.semanticscholar.org/paper/236dd39aa92c93b7114befcfc16c146ce809d52c\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Valeo. ai\"},{\"authorId\":null,\"name\":\"Valeo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9c8707ffb59b37029942baf4606b415b462de8eb\",\"title\":\"Driving Behavior Explanation with Multi-level Fusion\",\"url\":\"https://www.semanticscholar.org/paper/9c8707ffb59b37029942baf4606b415b462de8eb\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40061480\",\"name\":\"Z. Dong\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"2000237078\",\"name\":\"Qi Cui\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/978-3-030-55187-2_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78a1094e0968cf4e2b61c83100d971031597ae4b\",\"title\":\"Adaptive Attention Mechanism Based Semantic Compositional Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/78a1094e0968cf4e2b61c83100d971031597ae4b\",\"venue\":\"IntelliSys\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"20332986\",\"name\":\"Q. Hu\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TCSVT.2019.2956593\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"230a8581672b3147238eaab2cf686c70fe4f672b\",\"title\":\"Convolutional Reconstruction-to-Sequence for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/230a8581672b3147238eaab2cf686c70fe4f672b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1905.03966\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"49050519\",\"name\":\"Jiyuan Zhang\"},{\"authorId\":\"47119038\",\"name\":\"X. Wang\"},{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/CVPR.2019.00854\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"title\":\"Memory-Attended Recurrent Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1804.04087\",\"authors\":[{\"authorId\":\"3428634\",\"name\":\"Marco Lippi\"},{\"authorId\":\"2245726\",\"name\":\"M. Montemurro\"},{\"authorId\":\"32349764\",\"name\":\"M. Degli Esposti\"},{\"authorId\":\"2483707\",\"name\":\"G. Cristadoro\"}],\"doi\":\"10.1109/TNNLS.2019.2890970\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ae307bfefb75495331e66be955e7f64a0aba1430\",\"title\":\"Natural Language Statistical Features of LSTM-Generated Texts\",\"url\":\"https://www.semanticscholar.org/paper/ae307bfefb75495331e66be955e7f64a0aba1430\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40939264\",\"name\":\"Jiaxu Leng\"},{\"authorId\":\"46399320\",\"name\":\"Y. Liu\"},{\"authorId\":\"9416328\",\"name\":\"Tianlin Zhang\"},{\"authorId\":\"46234628\",\"name\":\"Pei Quan\"}],\"doi\":\"10.1109/ICDMW.2018.00103\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b80a7bbde2986a0b3474258ec2fad0a75813d89f\",\"title\":\"Context Learning Network for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/b80a7bbde2986a0b3474258ec2fad0a75813d89f\",\"venue\":\"2018 IEEE International Conference on Data Mining Workshops (ICDMW)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51001584\",\"name\":\"Hyung-min Lee\"},{\"authorId\":\"153481384\",\"name\":\"Il-Koo Kim\"}],\"doi\":\"10.1109/IJCNN.2019.8851892\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"title\":\"Generating Natural Video Descriptions using Semantic Gate\",\"url\":\"https://www.semanticscholar.org/paper/b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1907.12905\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":\"10.1145/3343031.3351060\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"title\":\"Watch It Twice: Video Captioning with a Refocused Video Encoder\",\"url\":\"https://www.semanticscholar.org/paper/5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1708.09545\",\"authors\":[{\"authorId\":\"143780023\",\"name\":\"Zhong Ji\"},{\"authorId\":\"50033817\",\"name\":\"Kailin Xiong\"},{\"authorId\":\"145134722\",\"name\":\"Y. Pang\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TCSVT.2019.2904996\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88a8baa1be5292e62622f1cb8e627fbf759bf741\",\"title\":\"Video Summarization With Attention-Based Encoder\\u2013Decoder Networks\",\"url\":\"https://www.semanticscholar.org/paper/88a8baa1be5292e62622f1cb8e627fbf759bf741\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"2003.05162\",\"authors\":[{\"authorId\":\"1500408667\",\"name\":\"Zhiyuan Fang\"},{\"authorId\":\"120838645\",\"name\":\"Tejas Gokhale\"},{\"authorId\":\"120722271\",\"name\":\"Pratyay Banerjee\"},{\"authorId\":\"1760291\",\"name\":\"Chitta Baral\"},{\"authorId\":\"1784500\",\"name\":\"Yezhou Yang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.61\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1794bc353c94e8d708476132eb326fe3af51c2e6\",\"title\":\"Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1794bc353c94e8d708476132eb326fe3af51c2e6\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49868702\",\"name\":\"Ran Wei\"},{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1016/j.jvcir.2020.102751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"title\":\"Exploiting the local temporal information for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"36251013\",\"name\":\"Wei Li\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1007/s11063-017-9591-9\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"37eaf94fa6619ee857019937677cb055a2a51bf3\",\"title\":\"Capturing Temporal Structures for Video Captioning by Spatio-temporal Contexts and Channel Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/37eaf94fa6619ee857019937677cb055a2a51bf3\",\"venue\":\"Neural Processing Letters\",\"year\":2017},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.08445\",\"authors\":[{\"authorId\":\"3705643\",\"name\":\"H. Wang\"},{\"authorId\":\"50762305\",\"name\":\"Jingrong Chen\"},{\"authorId\":\"1870061344\",\"name\":\"Xinchen Wan\"},{\"authorId\":\"46985321\",\"name\":\"H. Tian\"},{\"authorId\":\"23231639\",\"name\":\"Jiacheng Xia\"},{\"authorId\":\"65999879\",\"name\":\"Gaoxiong Zeng\"},{\"authorId\":\"2601624\",\"name\":\"Weiyan Wang\"},{\"authorId\":\"48543239\",\"name\":\"K. Chen\"},{\"authorId\":\"39853418\",\"name\":\"Wei Bai\"},{\"authorId\":\"1727978\",\"name\":\"J. Jiang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b34ec60146f823f7ee970765bd45335b93200b9\",\"title\":\"Domain-specific Communication Optimization for Distributed DNN Training\",\"url\":\"https://www.semanticscholar.org/paper/9b34ec60146f823f7ee970765bd45335b93200b9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150291092\",\"name\":\"Stelios Andreadis\"},{\"authorId\":\"2559834\",\"name\":\"A. Moumtzidou\"},{\"authorId\":\"2261494\",\"name\":\"K. Apostolidis\"},{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"3234471\",\"name\":\"D. Galanopoulos\"},{\"authorId\":\"2675084\",\"name\":\"E. Michail\"},{\"authorId\":\"1988554\",\"name\":\"Ilias Gialampoukidis\"},{\"authorId\":\"1381295446\",\"name\":\"S. Vrochidis\"},{\"authorId\":\"1737436\",\"name\":\"V. Mezaris\"},{\"authorId\":\"1715604\",\"name\":\"Y. Kompatsiaris\"}],\"doi\":\"10.1007/978-3-030-37734-2_69\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4c64613c8d2bb192c4ff177e5f3c767fd5b80d2\",\"title\":\"VERGE in VBS 2020\",\"url\":\"https://www.semanticscholar.org/paper/b4c64613c8d2bb192c4ff177e5f3c767fd5b80d2\",\"venue\":\"MMM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f6dbe07a311cbc756eb6d38528d70eb66663311\",\"title\":\"Audio Visual Scene-Aware Dialog Track in DSTC8\",\"url\":\"https://www.semanticscholar.org/paper/8f6dbe07a311cbc756eb6d38528d70eb66663311\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1510.04709\",\"authors\":[{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"37922370\",\"name\":\"S. Frank\"},{\"authorId\":\"2955842\",\"name\":\"E. Hasler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5af227e3e3158158163fb0715ae3971be2e1df4\",\"title\":\"Multilingual Image Description with Neural Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/a5af227e3e3158158163fb0715ae3971be2e1df4\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8071088\",\"name\":\"R. Oruganti\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"8262287\",\"name\":\"Suhas Pillai\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/ICIP.2016.7533033\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fe220d668a694cddf005958e2e58f568570276b\",\"title\":\"Image description through fusion based recurrent multi-modal learning\",\"url\":\"https://www.semanticscholar.org/paper/5fe220d668a694cddf005958e2e58f568570276b\",\"venue\":\"2016 IEEE International Conference on Image Processing (ICIP)\",\"year\":2016},{\"arxivId\":\"1607.03240\",\"authors\":[{\"authorId\":\"144208571\",\"name\":\"Sohil Shah\"},{\"authorId\":\"40222634\",\"name\":\"K. Kulkarni\"},{\"authorId\":\"47606073\",\"name\":\"A. Biswas\"},{\"authorId\":\"144883800\",\"name\":\"A. Gandhi\"},{\"authorId\":\"2116262\",\"name\":\"O. Deshmukh\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1007/978-3-319-46466-4_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b7180854e2f2911ac4878c91b451ad07f989725\",\"title\":\"Weakly Supervised Learning of Heterogeneous Concepts in Videos\",\"url\":\"https://www.semanticscholar.org/paper/7b7180854e2f2911ac4878c91b451ad07f989725\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7644453\",\"name\":\"Hanqi Wang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"46867455\",\"name\":\"Yin Sheng Zhang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"}],\"doi\":\"10.1145/3126686.3126715\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70f7bcfe2ce3789e62846c73e98feeaa319135e5\",\"title\":\"Learning Deep Contextual Attention Network for Narrative Photo Stream Captioning\",\"url\":\"https://www.semanticscholar.org/paper/70f7bcfe2ce3789e62846c73e98feeaa319135e5\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50664617\",\"name\":\"K. Adam\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c88430b26776d3df3c665ec301e66569ec8a0d7\",\"title\":\"LSTM neural network implementation using memristive crossbar circuits and its various topologies\",\"url\":\"https://www.semanticscholar.org/paper/4c88430b26776d3df3c665ec301e66569ec8a0d7\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"3493516\",\"name\":\"Yifan Xiong\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/2964284.2984065\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7492cac0babe8d514995bcde6456ae00c17325a3\",\"title\":\"Describing Videos using Multi-modal Fusion\",\"url\":\"https://www.semanticscholar.org/paper/7492cac0babe8d514995bcde6456ae00c17325a3\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"50080046\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/TIP.2019.2916757\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"title\":\"CAM-RNN: Co-Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":\"1909.02701\",\"authors\":[{\"authorId\":\"49243413\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"2410227\",\"name\":\"Yulun Zhang\"},{\"authorId\":\"94451829\",\"name\":\"K. Li\"},{\"authorId\":\"47003439\",\"name\":\"Yuanyuan Li\"},{\"authorId\":\"46956675\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/ICCV.2019.00475\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26500af6bb97bf2dab1cc14dfb3b8b08fef67940\",\"title\":\"Visual Semantic Reasoning for Image-Text Matching\",\"url\":\"https://www.semanticscholar.org/paper/26500af6bb97bf2dab1cc14dfb3b8b08fef67940\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74437294\",\"name\":\"Sk. Arif Ahmed\"},{\"authorId\":\"3320759\",\"name\":\"D. P. Dogra\"},{\"authorId\":\"46817269\",\"name\":\"Samarjit Kar\"},{\"authorId\":\"40813600\",\"name\":\"P. Roy\"}],\"doi\":\"10.1109/TCSVT.2018.2857489\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a609736cb7d92a668dbafde515b7ef9be19bf08\",\"title\":\"Trajectory-Based Surveillance Analysis: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/7a609736cb7d92a668dbafde515b7ef9be19bf08\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1611.05216\",\"authors\":[{\"authorId\":\"38179026\",\"name\":\"Y. Shi\"},{\"authorId\":\"40161651\",\"name\":\"Yonghong Tian\"},{\"authorId\":\"5765799\",\"name\":\"Yaowei Wang\"},{\"authorId\":\"144424248\",\"name\":\"Wei Zeng\"},{\"authorId\":\"34097174\",\"name\":\"Tiejun Huang\"}],\"doi\":\"10.1109/ICCV.2017.84\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d145edb79ec035d6cf3f50714429f51fb18f0a2f\",\"title\":\"Learning Long-Term Dependencies for Action Recognition with a Biologically-Inspired Deep Network\",\"url\":\"https://www.semanticscholar.org/paper/d145edb79ec035d6cf3f50714429f51fb18f0a2f\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32524586\",\"name\":\"Elias Lundeqvist\"},{\"authorId\":\"144082496\",\"name\":\"M. Svensson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"203464f5f6343d9874519899947c2b6426484a80\",\"title\":\"profiling : A machine learning approach towards detecting gender , age and native language of users in social media\",\"url\":\"https://www.semanticscholar.org/paper/203464f5f6343d9874519899947c2b6426484a80\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"}],\"doi\":\"10.1007/s11063-019-10030-y\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed3215857d14557d0afe517b4d28b0e98b384f4b\",\"title\":\"Refocused Attention: Long Short-Term Rewards Guided Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ed3215857d14557d0afe517b4d28b0e98b384f4b\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"title\":\"Multi-Modal Deep Learning to Understand Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50695255\",\"name\":\"S. Wang\"},{\"authorId\":\"144713153\",\"name\":\"Dan Guo\"},{\"authorId\":\"2779625\",\"name\":\"Wen-gang Zhou\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":\"10.1145/3240508.3240671\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"33e309f993023a0384221733dd884e2b891c8311\",\"title\":\"Connectionist Temporal Fusion for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/33e309f993023a0384221733dd884e2b891c8311\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66073991\",\"name\":\"Edoardo Alati\"},{\"authorId\":\"65770249\",\"name\":\"L. Mauro\"},{\"authorId\":\"2368860\",\"name\":\"Valsamis Ntouskos\"},{\"authorId\":\"1695016\",\"name\":\"F. Pirri\"}],\"doi\":\"10.1007/978-3-030-29516-5_60\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"422910cd883a39a42e40d6630997c95cb1864d44\",\"title\":\"Anticipating Next Goal for Robot Plan Prediction\",\"url\":\"https://www.semanticscholar.org/paper/422910cd883a39a42e40d6630997c95cb1864d44\",\"venue\":\"IntelliSys\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51446297\",\"name\":\"Shivansh Mundra\"},{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"52145960\",\"name\":\"Sayan Sinha\"}],\"doi\":\"10.1007/978-3-030-25614-2_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"42ab409898faf4050b8893a2f69077d53d32e817\",\"title\":\"Video DeCaptioning Using U-Net with Stacked Dilated Convolutional Layers\",\"url\":\"https://www.semanticscholar.org/paper/42ab409898faf4050b8893a2f69077d53d32e817\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1904.03282\",\"authors\":[{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"49616225\",\"name\":\"Sujoy Paul\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1109/CVPR.2019.01186\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca4d965ab8fd07fd236a2ec5b5c7a520077a3085\",\"title\":\"Weakly Supervised Video Moment Retrieval From Text Queries\",\"url\":\"https://www.semanticscholar.org/paper/ca4d965ab8fd07fd236a2ec5b5c7a520077a3085\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1906.02792\",\"authors\":[{\"authorId\":\"74480447\",\"name\":\"Manjot Bilkhu\"},{\"authorId\":\"14506569\",\"name\":\"S. Wang\"},{\"authorId\":\"70060571\",\"name\":\"Tushar Dobhal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b2cb203f6b09a3bf734c705c999da706b7a7c031\",\"title\":\"Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers\",\"url\":\"https://www.semanticscholar.org/paper/b2cb203f6b09a3bf734c705c999da706b7a7c031\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1611.08002\",\"authors\":[{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"143690259\",\"name\":\"K. Tran\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1109/CVPR.2017.127\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"778ce81457383bd5e3fdb11b145ded202ebb4970\",\"title\":\"Semantic Compositional Networks for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/778ce81457383bd5e3fdb11b145ded202ebb4970\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1997924\",\"name\":\"Samira Pouyanfar\"},{\"authorId\":\"2864059\",\"name\":\"Y. Yang\"},{\"authorId\":\"145758552\",\"name\":\"S. Chen\"},{\"authorId\":\"144987531\",\"name\":\"M. Shyu\"},{\"authorId\":\"153093860\",\"name\":\"S. S. Iyengar\"}],\"doi\":\"10.1145/3150226\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f257e3ac714cd8fcd3b22d7d27ac6fab2db34097\",\"title\":\"Multimedia Big Data Analytics\",\"url\":\"https://www.semanticscholar.org/paper/f257e3ac714cd8fcd3b22d7d27ac6fab2db34097\",\"venue\":\"ACM Comput. Surv.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1418117493\",\"name\":\"Ramgoapl Segu\"},{\"authorId\":\"145642542\",\"name\":\"K. Suresh\"}],\"doi\":\"10.5013/ijssst.a.20.01.07\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5f52bb7c60f036cd94dab4384b2da0688ea0a8f\",\"title\":\"Computer Vision Approach for Detection and Extraction of Text from Video Frames Using Color Continuity, Color Variations and HV Projections Methods\",\"url\":\"https://www.semanticscholar.org/paper/f5f52bb7c60f036cd94dab4384b2da0688ea0a8f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1609.06782\",\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3122865.3122867\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"title\":\"Deep Learning for Video Classification and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"venue\":\"Frontiers of Multimedia Research\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722627\",\"name\":\"Xiaodong He\"},{\"authorId\":\"144718788\",\"name\":\"L. Deng\"}],\"doi\":\"10.1007/978-981-10-5209-5_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"77991dca4fdc99b6622c55f86ca87429a5b8b308\",\"title\":\"Deep Learning in Natural Language Generation from Images\",\"url\":\"https://www.semanticscholar.org/paper/77991dca4fdc99b6622c55f86ca87429a5b8b308\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhenyu Liao\"},{\"authorId\":\"2885287\",\"name\":\"Yikun Xian\"},{\"authorId\":\"48520620\",\"name\":\"X. Yang\"},{\"authorId\":\"1729695\",\"name\":\"Qinpei Zhao\"},{\"authorId\":\"50445730\",\"name\":\"C. Zhang\"},{\"authorId\":\"1780516\",\"name\":\"J. Li\"}],\"doi\":\"10.1145/3172944.3172966\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ebe7eda4fd10caa326d57b93e8a1e2a13e09a403\",\"title\":\"TSCSet: A Crowdsourced Time-Sync Comment Dataset for Exploration of User Experience Improvement\",\"url\":\"https://www.semanticscholar.org/paper/ebe7eda4fd10caa326d57b93e8a1e2a13e09a403\",\"venue\":\"IUI\",\"year\":2018},{\"arxivId\":\"1707.05357\",\"authors\":[{\"authorId\":\"35223379\",\"name\":\"Sumit Shekhar\"},{\"authorId\":\"22234092\",\"name\":\"Dhruv Singal\"},{\"authorId\":\"20400898\",\"name\":\"Harvineet Singh\"},{\"authorId\":\"3419748\",\"name\":\"Manav Kedia\"},{\"authorId\":\"37722215\",\"name\":\"Akhil Shetty\"}],\"doi\":\"10.1109/ICCVW.2017.321\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"21efd287c95b045d929761cbd7c35d331398df21\",\"title\":\"Show and Recall: Learning What Makes Videos Memorable\",\"url\":\"https://www.semanticscholar.org/paper/21efd287c95b045d929761cbd7c35d331398df21\",\"venue\":\"2017 IEEE International Conference on Computer Vision Workshops (ICCVW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47968942\",\"name\":\"K. Zhang\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"}],\"doi\":\"10.1007/978-3-030-01237-3_24\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a965d76c1a7414edcf7258a011b4237cfdf0ac7\",\"title\":\"Retrospective Encoders for Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/3a965d76c1a7414edcf7258a011b4237cfdf0ac7\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143860910\",\"name\":\"N. Zhao\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"145179162\",\"name\":\"Mingxing Zhang\"},{\"authorId\":\"48043335\",\"name\":\"R. Hong\"},{\"authorId\":\"47446553\",\"name\":\"M. Wang\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/ICME.2017.8019344\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf7eee93909266babb79017c4acb8f4b22b955c8\",\"title\":\"VIDEOWHISPER: Towards unsupervised learning of discriminative features of videos with RNN\",\"url\":\"https://www.semanticscholar.org/paper/bf7eee93909266babb79017c4acb8f4b22b955c8\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1809527\",\"name\":\"J. Ying\"},{\"authorId\":\"2229131\",\"name\":\"Po-Yu Huang\"},{\"authorId\":\"38546923\",\"name\":\"Chih-Kai Chang\"},{\"authorId\":\"1720296\",\"name\":\"D. Yang\"}],\"doi\":\"10.1109/BigData.2017.8258131\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"99e26d6a29d53868ed462274f4b3af4a14b967ea\",\"title\":\"A preliminary study on deep learning for predicting social insurance payment behavior\",\"url\":\"https://www.semanticscholar.org/paper/99e26d6a29d53868ed462274f4b3af4a14b967ea\",\"venue\":\"2017 IEEE International Conference on Big Data (Big Data)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103178505\",\"name\":\"Ulloa Cerna\"},{\"authorId\":\"84023704\",\"name\":\"Alvaro Emilio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3265edca921ad3a70ea6bdd44fec05812101db1b\",\"title\":\"Large Scale Electronic Health Record Data and Echocardiography Video Analysis for Mortality Risk Prediction\",\"url\":\"https://www.semanticscholar.org/paper/3265edca921ad3a70ea6bdd44fec05812101db1b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"50272388\",\"name\":\"Ioana Croitoru\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"title\":\"A hierarchical approach to vision-based language generation: from simple sentences to complex natural language\",\"url\":\"https://www.semanticscholar.org/paper/9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9623845\",\"name\":\"Junki Park\"},{\"authorId\":\"35518250\",\"name\":\"Wooseok Yi\"},{\"authorId\":\"51139214\",\"name\":\"Daehyun Ahn\"},{\"authorId\":\"2484938\",\"name\":\"J. Kung\"},{\"authorId\":\"1939164\",\"name\":\"J. Kim\"}],\"doi\":\"10.1109/TCAD.2019.2926482\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"80d7fdd4161d3d3d27e0cfb12882871eb32002d7\",\"title\":\"Balancing Computation Loads and Optimizing Input Vector Loading in LSTM Accelerators\",\"url\":\"https://www.semanticscholar.org/paper/80d7fdd4161d3d3d27e0cfb12882871eb32002d7\",\"venue\":\"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ACCESS.2019.2942000\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"801827592d18c4e6170d88f8345465de4a8db7ca\",\"title\":\"Video Captioning With Adaptive Attention and Mixed Loss Optimization\",\"url\":\"https://www.semanticscholar.org/paper/801827592d18c4e6170d88f8345465de4a8db7ca\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26409184\",\"name\":\"M. Z. Khan\"},{\"authorId\":\"40589170\",\"name\":\"M. A. Hassan\"},{\"authorId\":\"123354310\",\"name\":\"Saleet Ul Hassan\"},{\"authorId\":\"65752088\",\"name\":\"Muhammad Usman Ghanni Khan\"}],\"doi\":\"10.1109/ICET.2018.8603653\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6061d7697c331e7da3f18f002e5bc30fbe26ded0\",\"title\":\"Semantic Analysis of News Based on the Deep Convolution Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/6061d7697c331e7da3f18f002e5bc30fbe26ded0\",\"venue\":\"2018 14th International Conference on Emerging Technologies (ICET)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"153576781\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/JIOT.2017.2779865\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"title\":\"Attention-in-Attention Networks for Surveillance Video Understanding in Internet of Things\",\"url\":\"https://www.semanticscholar.org/paper/d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"venue\":\"IEEE Internet of Things Journal\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144478646\",\"name\":\"Z. Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967242\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"90c4a6c6f790dbcef9a29c9a755458be09e319b6\",\"title\":\"Attention-based LSTM with Semantic Consistency for Videos Captioning\",\"url\":\"https://www.semanticscholar.org/paper/90c4a6c6f790dbcef9a29c9a755458be09e319b6\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1909.02218\",\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"2061528\",\"name\":\"Wenqing Chu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2018.2859820\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96f0908cc138aceb2d5e0180c440e5adc711d855\",\"title\":\"A Better Way to Attend: Attention With Trees for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/96f0908cc138aceb2d5e0180c440e5adc711d855\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1905.01077\",\"authors\":[{\"authorId\":\"50763020\",\"name\":\"Jingwen Chen\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1609/aaai.v33i01.33018167\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d40892479541c2d173c836534e6fb2acb597de49\",\"title\":\"Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d40892479541c2d173c836534e6fb2acb597de49\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2008.09791\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-58589-1_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"title\":\"Identity-Aware Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/ICCV.2017.83\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"title\":\"Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1710.08315\",\"authors\":[{\"authorId\":\"144448374\",\"name\":\"Jinhua Tao\"},{\"authorId\":\"1678776\",\"name\":\"Zidong Du\"},{\"authorId\":\"145461470\",\"name\":\"Q. Guo\"},{\"authorId\":\"153791429\",\"name\":\"Hui-Ying Lan\"},{\"authorId\":\"50081487\",\"name\":\"L. Zhang\"},{\"authorId\":\"7523063\",\"name\":\"Shengyuan Zhou\"},{\"authorId\":\"47775733\",\"name\":\"Lingjie Xu\"},{\"authorId\":\"145589202\",\"name\":\"C. Liu\"},{\"authorId\":\"48447405\",\"name\":\"Haifeng Liu\"},{\"authorId\":\"145709801\",\"name\":\"Shan Tang\"},{\"authorId\":\"38253244\",\"name\":\"A. Rush\"},{\"authorId\":\"14288086\",\"name\":\"W. Chen\"},{\"authorId\":\"39419985\",\"name\":\"Shaoli Liu\"},{\"authorId\":\"81369877\",\"name\":\"Yunji Chen\"},{\"authorId\":\"144049725\",\"name\":\"Tianshi Chen\"}],\"doi\":\"10.1007/s11390-018-1805-8\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"72f629bfad2e49eb8cb75e0b746a5afcd703f2ab\",\"title\":\"BenchIP: Benchmarking Intelligence Processors\",\"url\":\"https://www.semanticscholar.org/paper/72f629bfad2e49eb8cb75e0b746a5afcd703f2ab\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":\"10.1145/3009906\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6e60536c847ac25dba4c1c071e0355e5537fe061\",\"title\":\"Computer Vision and Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/6e60536c847ac25dba4c1c071e0355e5537fe061\",\"venue\":\"ACM Comput. Surv.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1007/978-3-030-14657-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a9bae357bc63540cfbd630f4aaab4088edf2e0\",\"title\":\"Video Captioning Using Hierarchical LSTM and Text-Based Sliding Window\",\"url\":\"https://www.semanticscholar.org/paper/08a9bae357bc63540cfbd630f4aaab4088edf2e0\",\"venue\":\"IoTaaS\",\"year\":2018},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28951424\",\"name\":\"Jegyung Son\"},{\"authorId\":\"1682370\",\"name\":\"G. Jang\"},{\"authorId\":\"1734091\",\"name\":\"M. Lee\"}],\"doi\":\"10.1007/978-3-319-70096-0_44\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"412f12879dc8713a9b2d47a3a50d0279f61fa922\",\"title\":\"Temporal Attention Neural Network for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/412f12879dc8713a9b2d47a3a50d0279f61fa922\",\"venue\":\"ICONIP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1007/s00530-018-0598-5\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"title\":\"Multi-guiding long short-term memory for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"venue\":\"Multimedia Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47435551\",\"name\":\"Xu Shen\"},{\"authorId\":\"40434674\",\"name\":\"X. Tian\"},{\"authorId\":\"144521811\",\"name\":\"J. Xing\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c6ea379d48526bc43bec5ae3731deff67c6aa6ea\",\"title\":\"Sequence-to-Sequence Learning via Shared Latent Representation\",\"url\":\"https://www.semanticscholar.org/paper/c6ea379d48526bc43bec5ae3731deff67c6aa6ea\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2569534\",\"name\":\"Jinkyu Kim\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1729041\",\"name\":\"John F. Canny\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f5536ecbce6fbe991b76c799d3d9c2b8b757892\",\"title\":\"UvA-DARE ( Digital Academic Repository ) Textual Explanations for Self-Driving Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/0f5536ecbce6fbe991b76c799d3d9c2b8b757892\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"150898549\",\"name\":\"Alessandro Devo\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/TMM.2019.2924598\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"61a10a61e95ede1064567be38196a2348af3fb6c\",\"title\":\"The Role of the Input in Natural Language Video Description\",\"url\":\"https://www.semanticscholar.org/paper/61a10a61e95ede1064567be38196a2348af3fb6c\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"104452015\",\"name\":\"\\u00d6zkan Inik\"},{\"authorId\":\"2406427\",\"name\":\"E. \\u00dclker\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bbf2ac448ec144e3de85e59b5540d45a370b7755\",\"title\":\"Derin \\u00d6\\u011frenme ve G\\u00f6r\\u00fcnt\\u00fc Analizinde Kullan\\u0131lan Derin \\u00d6\\u011frenme Modelleri\",\"url\":\"https://www.semanticscholar.org/paper/bbf2ac448ec144e3de85e59b5540d45a370b7755\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2653622\",\"name\":\"Junfu Pu\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"}],\"doi\":\"10.1109/CVPR.2019.00429\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"714df3e97817ec56b8dbc7217155adadf2a0487f\",\"title\":\"Iterative Alignment Network for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/714df3e97817ec56b8dbc7217155adadf2a0487f\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701451\",\"name\":\"Ramesh Nallapati\"},{\"authorId\":\"144028698\",\"name\":\"B. Xiang\"},{\"authorId\":\"145218984\",\"name\":\"Bowen Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"221ef0a2f185036c06f9fb089109ded5c888c4c6\",\"title\":\"Sequence-to-Sequence RNNs for Text Summarization\",\"url\":\"https://www.semanticscholar.org/paper/221ef0a2f185036c06f9fb089109ded5c888c4c6\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1610.02424\",\"authors\":[{\"authorId\":\"3358971\",\"name\":\"Ashwin K. Vijayakumar\"},{\"authorId\":\"144354133\",\"name\":\"Michael Cogswell\"},{\"authorId\":\"35100058\",\"name\":\"R. R. Selvaraju\"},{\"authorId\":\"144207643\",\"name\":\"Q. Sun\"},{\"authorId\":\"2297229\",\"name\":\"Stefan Lee\"},{\"authorId\":\"2821130\",\"name\":\"David J. Crandall\"},{\"authorId\":\"145054147\",\"name\":\"Dhruv Batra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4dd95c4341ec7d14317a3d97022773a0822906c\",\"title\":\"Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/e4dd95c4341ec7d14317a3d97022773a0822906c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1904.03366\",\"authors\":[{\"authorId\":\"93872817\",\"name\":\"Yatri Modi\"},{\"authorId\":\"2326758\",\"name\":\"Natalie Parde\"}],\"doi\":\"10.18653/v1/W19-1805\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a1269edf443cce55ad8de22822ec65a88a9db49\",\"title\":\"The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models\",\"url\":\"https://www.semanticscholar.org/paper/8a1269edf443cce55ad8de22822ec65a88a9db49\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151430731\",\"name\":\"Adarsh Jamadandi\"},{\"authorId\":\"1584997238\",\"name\":\"Sunidhi Kotturshettar\"},{\"authorId\":\"2938896\",\"name\":\"U. Mudenagudi\"}],\"doi\":\"10.1145/3293353.3293354\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"24bd4101e52e0b8c8c47c864152eabd78646c03d\",\"title\":\"PredGAN: a deep multi-scale video prediction framework for detecting anomalies in videos\",\"url\":\"https://www.semanticscholar.org/paper/24bd4101e52e0b8c8c47c864152eabd78646c03d\",\"venue\":\"ICVGIP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.1145/3123266.3127895\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"title\":\"Richer Semantic Visual and Language Representation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3046293\",\"name\":\"D. Lee\"},{\"authorId\":\"38726140\",\"name\":\"Jongmin Lee\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"}],\"doi\":\"10.1007/978-3-319-54427-4_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"25a83fd401a528e4979afaf4554a3e941dd7016d\",\"title\":\"Multi-view Automatic Lip-Reading Using Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/25a83fd401a528e4979afaf4554a3e941dd7016d\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d1959ba4637739dcc6cc6995e10fd41fd6604713\",\"title\":\"Deep Learning for Semantic Video Understanding by Sourabh Kulhare\",\"url\":\"https://www.semanticscholar.org/paper/d1959ba4637739dcc6cc6995e10fd41fd6604713\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38179026\",\"name\":\"Y. Shi\"},{\"authorId\":\"40161651\",\"name\":\"Yonghong Tian\"},{\"authorId\":\"5765799\",\"name\":\"Yaowei Wang\"},{\"authorId\":\"34097174\",\"name\":\"Tiejun Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c829be73584966e3162f7ccae72d9284a2ebf358\",\"title\":\"shuttleNet: A biologically-inspired RNN with loop connection and parameter sharing\",\"url\":\"https://www.semanticscholar.org/paper/c829be73584966e3162f7ccae72d9284a2ebf358\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"Jianfeng Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"2896042\",\"name\":\"Weiyu Lan\"},{\"authorId\":\"1890615\",\"name\":\"Yujia Huo\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd14c733736d8d977588d450521a9c18bb65818b\",\"title\":\"UvA-DARE ( Digital Academic Repository ) Early Embedding and Late Reranking for Video\",\"url\":\"https://www.semanticscholar.org/paper/fd14c733736d8d977588d450521a9c18bb65818b\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74480447\",\"name\":\"Manjot Bilkhu\"},{\"authorId\":\"1734076\",\"name\":\"Siyang Wang\"},{\"authorId\":\"70060571\",\"name\":\"Tushar Dobhal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc5328e765cf001503d0909313238a4152fc01a1\",\"title\":\"VIDEO SUMMARIZATION USING TRANSFORMERS\",\"url\":\"https://www.semanticscholar.org/paper/fc5328e765cf001503d0909313238a4152fc01a1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"2272096\",\"name\":\"Guo-Jun Qi\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/WACV.2019.00034\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"449b9189e3058d33f871dfd3b07cc75a717038f7\",\"title\":\"Improving Diversity of Image Captioning Through Variational Autoencoders and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/449b9189e3058d33f871dfd3b07cc75a717038f7\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":\"2003.12633\",\"authors\":[{\"authorId\":\"24339915\",\"name\":\"Davis Gilton\"},{\"authorId\":\"3212867\",\"name\":\"R. Luo\"},{\"authorId\":\"145952380\",\"name\":\"R. Willett\"},{\"authorId\":\"46208708\",\"name\":\"G. Shakhnarovich\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"title\":\"Detection and Description of Change in Visual Streams\",\"url\":\"https://www.semanticscholar.org/paper/dea7e4fdaa5c56a8e1df800149b8d3e8e9950990\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31747675\",\"name\":\"Sue Han Lee\"},{\"authorId\":\"2863960\",\"name\":\"Chee Seng Chan\"},{\"authorId\":\"1711669\",\"name\":\"Paolo Remagnino\"}],\"doi\":\"10.1109/TIP.2018.2836321\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"976647b32fd7e1a5c8ee4a11792155903bb34e43\",\"title\":\"Multi-Organ Plant Classification Based on Convolutional and Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/976647b32fd7e1a5c8ee4a11792155903bb34e43\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1902.00669\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"47190894\",\"name\":\"F. Zhang\"}],\"doi\":\"10.1609/aaai.v33i01.33018909\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe33597affd4e99a5dc979ef4ed99ee6311fdc2b\",\"title\":\"Hierarchical Photo-Scene Encoder for Album Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/fe33597affd4e99a5dc979ef4ed99ee6311fdc2b\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145813731\",\"name\":\"X. Xu\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"49958046\",\"name\":\"HaiBin Liu\"},{\"authorId\":null,\"name\":\"Ji Yi\"},{\"authorId\":\"50218964\",\"name\":\"Zhaohui Wang\"}],\"doi\":\"10.2991/ITIM-17.2017.34\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dd1ee9e1d24ac60e722a8e51518e44669b052557\",\"title\":\"Video Description Using Learning Multiple Features\",\"url\":\"https://www.semanticscholar.org/paper/dd1ee9e1d24ac60e722a8e51518e44669b052557\",\"venue\":\"ICIT 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2884561\",\"name\":\"Sebastian Ruder\"},{\"authorId\":\"39139825\",\"name\":\"Matthew E. Peters\"},{\"authorId\":\"2705113\",\"name\":\"Swabha Swayamdipta\"},{\"authorId\":\"145662441\",\"name\":\"Thomas Wolf\"}],\"doi\":\"10.18653/v1/N19-5004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3\",\"title\":\"Transfer Learning in Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-54407-6_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"title\":\"Video Captioning via Sentence Augmentation and Spatio-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":\"1808.09564\",\"authors\":[{\"authorId\":\"40223399\",\"name\":\"Renjie Zheng\"},{\"authorId\":\"1847848\",\"name\":\"M. Ma\"},{\"authorId\":\"144768480\",\"name\":\"Liang Huang\"}],\"doi\":\"10.18653/v1/D18-1357\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9a716bf565445740f47243bfde06398ded224cb4\",\"title\":\"Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/9a716bf565445740f47243bfde06398ded224cb4\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47968201\",\"name\":\"Lixin Liu\"},{\"authorId\":\"145078589\",\"name\":\"Xiaojun Wan\"},{\"authorId\":\"35310979\",\"name\":\"Zongming Guo\"}],\"doi\":\"10.1145/3240508.3241910\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c064c00fb5aecfb181ef33d430dfc3053f1dc646\",\"title\":\"Images2Poem: Generating Chinese Poetry from Image Streams\",\"url\":\"https://www.semanticscholar.org/paper/c064c00fb5aecfb181ef33d430dfc3053f1dc646\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50506129\",\"name\":\"E. Barati\"},{\"authorId\":\"2410994\",\"name\":\"Xue-wen Chen\"}],\"doi\":\"10.1145/3343031.3351037\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf8a3f260fbe4ee104380437cd576a556dccd290\",\"title\":\"Critic-based Attention Network for Event-based Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cf8a3f260fbe4ee104380437cd576a556dccd290\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1145/3239576.3239580\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"title\":\"Video Captioning using Hierarchical Multi-Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"venue\":\"ICAIP '18\",\"year\":2018},{\"arxivId\":\"1909.04312\",\"authors\":[{\"authorId\":\"1692609\",\"name\":\"S. Yang\"},{\"authorId\":\"143715293\",\"name\":\"W. Zhang\"},{\"authorId\":\"2882531\",\"name\":\"Weizhi Lu\"},{\"authorId\":\"2970970\",\"name\":\"Hesheng Wang\"},{\"authorId\":\"9348561\",\"name\":\"Yibin Li\"}],\"doi\":\"10.1109/IROS40897.2019.8968278\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"07c33f1d771ed265f9c6fe40a43323417d0951a6\",\"title\":\"Learning Actions from Human Demonstration Video for Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/07c33f1d771ed265f9c6fe40a43323417d0951a6\",\"venue\":\"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144585402\",\"name\":\"Peng Zhang\"},{\"authorId\":\"2448188\",\"name\":\"Li Su\"},{\"authorId\":\"73596205\",\"name\":\"L. Li\"},{\"authorId\":\"8180253\",\"name\":\"Bing-Kun Bao\"},{\"authorId\":\"1763529\",\"name\":\"P. Cosman\"},{\"authorId\":\"46439250\",\"name\":\"Guorong Li\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"}],\"doi\":\"10.1145/3343031.3351089\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73ac14e3793342ca621ca815b23ea3ffc57bda5\",\"title\":\"Training Efficient Saliency Prediction Models with Knowledge Distillation\",\"url\":\"https://www.semanticscholar.org/paper/e73ac14e3793342ca621ca815b23ea3ffc57bda5\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2011.07635\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"48843025\",\"name\":\"Han Guo\"},{\"authorId\":\"143977266\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.625\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1fb985c6624bede346e05f9ef16aa4505731c330\",\"title\":\"DORB: Dynamically Optimizing Multiple Rewards with Bandits\",\"url\":\"https://www.semanticscholar.org/paper/1fb985c6624bede346e05f9ef16aa4505731c330\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"2003.12185\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1007/978-3-030-58568-6_18\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"61b166040abff8309e23d804551fc3d3acc833f6\",\"title\":\"Action Localization through Continual Predictive Learning\",\"url\":\"https://www.semanticscholar.org/paper/61b166040abff8309e23d804551fc3d3acc833f6\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"}],\"doi\":\"10.1145/2911996.2912043\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"title\":\"Video Description Generation using Audio and Visual Cues\",\"url\":\"https://www.semanticscholar.org/paper/54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"venue\":\"ICMR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"8799308\",\"name\":\"U. Nadeem\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"1980026040\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1007/978-3-030-49724-8_2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"title\":\"Vision to Language: Methods, Metrics and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/8885ae1cbdac1d27443167689ed872dfe46c9e3f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2101.00359\",\"authors\":[{\"authorId\":null,\"name\":\"Mingjian Zhu\"},{\"authorId\":null,\"name\":\"Chenrui Duan\"},{\"authorId\":\"1409820051\",\"name\":\"Changbin Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"title\":\"Video Captioning in Compressed Video\",\"url\":\"https://www.semanticscholar.org/paper/93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39970828\",\"name\":\"J. Jacob\"},{\"authorId\":\"1413613449\",\"name\":\"Sudeep Ilayidom\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78248c957222b8bb2d2935367885acdd4216bfd8\",\"title\":\"An Innovative Video Searching Approach using Video Indexing\",\"url\":\"https://www.semanticscholar.org/paper/78248c957222b8bb2d2935367885acdd4216bfd8\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3213196\",\"name\":\"Zhongwei Xie\"},{\"authorId\":\"38690169\",\"name\":\"Lin Li\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"50442192\",\"name\":\"L. Zhong\"},{\"authorId\":\"50746914\",\"name\":\"Jianwen Xiang\"}],\"doi\":\"10.1016/J.PATREC.2019.03.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5eb36c74797aca8d68b1a30783c9a61b7d7603d4\",\"title\":\"Image-to-video person re-identification with cross-modal embeddings\",\"url\":\"https://www.semanticscholar.org/paper/5eb36c74797aca8d68b1a30783c9a61b7d7603d4\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48637710\",\"name\":\"Yongqing Zhu\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1145/3343031.3350932\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"title\":\"Attention-based Densely Connected LSTM for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"2003.13942\",\"authors\":[{\"authorId\":\"52170427\",\"name\":\"Boxiao Pan\"},{\"authorId\":\"30017683\",\"name\":\"Haoye Cai\"},{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"144015229\",\"name\":\"Kuan-Hui Lee\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"46408185\",\"name\":\"E. Adeli\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/cvpr42600.2020.01088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a444d32f30d38fb0cb811fa1a9b601511244fb5b\",\"title\":\"Spatio-Temporal Graph for Video Captioning With Knowledge Distillation\",\"url\":\"https://www.semanticscholar.org/paper/a444d32f30d38fb0cb811fa1a9b601511244fb5b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1801.07388\",\"authors\":[{\"authorId\":\"118242229\",\"name\":\"D. Castro\"},{\"authorId\":\"2935619\",\"name\":\"Steven Hickson\"},{\"authorId\":\"3430745\",\"name\":\"Patsorn Sangkloy\"},{\"authorId\":\"145389008\",\"name\":\"Bhavishya Mittal\"},{\"authorId\":\"35459529\",\"name\":\"Sean Dai\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"21472040\",\"name\":\"Irfan Essa\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6e25cab2251a8ded43c44b28a87f4c62e3a548a\",\"title\":\"Let's Dance: Learning From Online Dance Videos\",\"url\":\"https://www.semanticscholar.org/paper/a6e25cab2251a8ded43c44b28a87f4c62e3a548a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"51133784\",\"name\":\"F. Pollastri\"},{\"authorId\":\"1705203\",\"name\":\"C. Grana\"}],\"doi\":\"10.1109/IPAS.2018.8708893\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9184fe68648d1e7fcd1d9d153e3b888096f355b1\",\"title\":\"A Hierarchical Quasi-Recurrent approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9184fe68648d1e7fcd1d9d153e3b888096f355b1\",\"venue\":\"2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS)\",\"year\":2018},{\"arxivId\":\"1809.07257\",\"authors\":[{\"authorId\":\"2662002\",\"name\":\"Oliver Nina\"},{\"authorId\":\"47238599\",\"name\":\"W. Garcia\"},{\"authorId\":\"47637016\",\"name\":\"Scott Clouse\"},{\"authorId\":\"1858702\",\"name\":\"A. Yilmaz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"title\":\"MTLE: A Multitask Learning Encoder of Visual Feature Representations for Video and Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1705.01861\",\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2017.472\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8dea2081609d838dc5b5a929f2ac19ce0c41c9a0\",\"title\":\"Action Tubelet Detector for Spatio-Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/8dea2081609d838dc5b5a929f2ac19ce0c41c9a0\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1602.06023\",\"authors\":[{\"authorId\":\"1701451\",\"name\":\"Ramesh Nallapati\"},{\"authorId\":\"145218984\",\"name\":\"Bowen Zhou\"},{\"authorId\":\"1790831\",\"name\":\"C. D. Santos\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"144028698\",\"name\":\"B. Xiang\"}],\"doi\":\"10.18653/v1/K16-1028\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f37076f426023241f19cdc2fb0a0fd733a6fa7fa\",\"title\":\"Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/f37076f426023241f19cdc2fb0a0fd733a6fa7fa\",\"venue\":\"CoNLL\",\"year\":2016},{\"arxivId\":\"1611.09053\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1109/CVPR.2017.147\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"533d14e539ae5cdca0ece392487a2b19106d468a\",\"title\":\"Bidirectional Multirate Reconstruction for Temporal Modeling in Videos\",\"url\":\"https://www.semanticscholar.org/paper/533d14e539ae5cdca0ece392487a2b19106d468a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1707.04045\",\"authors\":[{\"authorId\":\"3223082\",\"name\":\"Jae Hyeon Yoo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a03500376200c0ded58eab702ddc678d4e1cca82\",\"title\":\"Large-scale Video Classification guided by Batch Normalized LSTM Translator\",\"url\":\"https://www.semanticscholar.org/paper/a03500376200c0ded58eab702ddc678d4e1cca82\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3446334\",\"name\":\"Hehe Fan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"7590116\",\"name\":\"C. Yan\"},{\"authorId\":\"38263913\",\"name\":\"J. Ge\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.24963/ijcai.2018/98\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"3da5de9c29e007ff2bca0cc9152bcf4dd83fe7a0\",\"title\":\"Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/3da5de9c29e007ff2bca0cc9152bcf4dd83fe7a0\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2815804\",\"name\":\"Yu-An Chung\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7086b32013c38fa2f5250dfdc116a5259aca1c63\",\"title\":\"Unsupervised learning of cross-modal mappings between speech and text\",\"url\":\"https://www.semanticscholar.org/paper/7086b32013c38fa2f5250dfdc116a5259aca1c63\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145307652\",\"name\":\"Li Dong\"},{\"authorId\":\"3110003\",\"name\":\"Shaohan Huang\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"145389714\",\"name\":\"K. Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"21e31a5d15e2294bd7028307f88c1aac5d25c295\",\"title\":\"Explorer Learning to Generate Product Reviews from Attributes\",\"url\":\"https://www.semanticscholar.org/paper/21e31a5d15e2294bd7028307f88c1aac5d25c295\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797022\",\"name\":\"Hoo-Chang Shin\"},{\"authorId\":\"1742509\",\"name\":\"Kirk Roberts\"},{\"authorId\":\"50706692\",\"name\":\"Le Lu\"},{\"authorId\":\"1398175407\",\"name\":\"Dina Demner-Fushman\"},{\"authorId\":\"1722252\",\"name\":\"J. Yao\"},{\"authorId\":\"67216634\",\"name\":\"Ronald M. Summer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"71afbb37a81a44299872bee9ae888f4129962e18\",\"title\":\"Recurrent Neural Feedback Model for Automated Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/71afbb37a81a44299872bee9ae888f4129962e18\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"48624462\",\"name\":\"W. Li\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1109/TMM.2018.2839534\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"602412f61e8902052e6489e84a6f24ccc7407814\",\"title\":\"Fully Convolutional Network for Multiscale Temporal Action Proposals\",\"url\":\"https://www.semanticscholar.org/paper/602412f61e8902052e6489e84a6f24ccc7407814\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"153252274\",\"name\":\"A. Li\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/TIP.2019.2941267\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a97b13151ee3b3ddfc6f17c3cc04eaf827f00341\",\"title\":\"Hierarchical Recurrent Deep Fusion Using Adaptive Clip Summarization for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/a97b13151ee3b3ddfc6f17c3cc04eaf827f00341\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49890994\",\"name\":\"Y. Zhang\"},{\"authorId\":\"48109119\",\"name\":\"Dan Li\"},{\"authorId\":\"49418575\",\"name\":\"Y. Wang\"},{\"authorId\":\"2033454\",\"name\":\"Yang Fang\"},{\"authorId\":\"144343548\",\"name\":\"W. Xiao\"}],\"doi\":\"10.3390/app9081665\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2d1b5b7e7337a2622e6dc32d9d2478a1c734fbb3\",\"title\":\"Abstract Text Summarization with a Convolutional Seq2seq Model\",\"url\":\"https://www.semanticscholar.org/paper/2d1b5b7e7337a2622e6dc32d9d2478a1c734fbb3\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1802.10250\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACV.2019.00048\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"99cdb10443a0543be3466c9231ff922bcc996843\",\"title\":\"Joint Event Detection and Description in Continuous Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/99cdb10443a0543be3466c9231ff922bcc996843\",\"venue\":\"2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)\",\"year\":2019},{\"arxivId\":\"1704.07489\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P17-1117\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9b08d3201af644a638e291755a5e51f6b17a51f3\",\"title\":\"Multi-Task Video Captioning with Video and Entailment Generation\",\"url\":\"https://www.semanticscholar.org/paper/9b08d3201af644a638e291755a5e51f6b17a51f3\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Akshay Khattera\"},{\"authorId\":null,\"name\":\"Keshav Issarc\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a45e9865a7bfbfc2769f09451403b766d863f93\",\"title\":\"Video To Text Analysis : Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/8a45e9865a7bfbfc2769f09451403b766d863f93\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39970828\",\"name\":\"J. Jacob\"},{\"authorId\":\"2457110\",\"name\":\"M. S. Elayidom\"},{\"authorId\":\"3109670\",\"name\":\"V. P. Devassia\"}],\"doi\":\"10.11591/IJECE.V10I6.PP6019-6025\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de96c55acc706c4a02307a5e572753ffab6853d2\",\"title\":\"Video content analysis and retrieval system using video storytelling and indexing techniques\",\"url\":\"https://www.semanticscholar.org/paper/de96c55acc706c4a02307a5e572753ffab6853d2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.13608\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"50144812\",\"name\":\"Z. Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394486.3403325\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d271e93c7566b231e560c48b4cc4942077d762f9\",\"title\":\"Comprehensive Information Integration Modeling Framework for Video Titling\",\"url\":\"https://www.semanticscholar.org/paper/d271e93c7566b231e560c48b4cc4942077d762f9\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"1511.05284\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPR.2016.8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"title\":\"Deep Compositional Captioning: Describing Novel Object Categories without Paired Training Data\",\"url\":\"https://www.semanticscholar.org/paper/e516d22697bad6d0f7956b0e8bfa93d6eb0b2f17\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1708.02300\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D17-1103\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"title\":\"Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46868527\",\"name\":\"Yueying Zhang\"},{\"authorId\":\"31078650\",\"name\":\"Xu Zhi-jie\"},{\"authorId\":\"3833112\",\"name\":\"J. Zhang\"}],\"doi\":\"10.3233/jcm-204466\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d3a44b5381263529e972df32e316cd9eeb2d94ce\",\"title\":\"Traffic index prediction based on sequence to sequence learning\",\"url\":\"https://www.semanticscholar.org/paper/d3a44b5381263529e972df32e316cd9eeb2d94ce\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1808.07272\",\"authors\":[{\"authorId\":\"2527741\",\"name\":\"Sibo Song\"},{\"authorId\":\"143770929\",\"name\":\"N. Cheung\"},{\"authorId\":\"1802086\",\"name\":\"V. Chandrasekhar\"},{\"authorId\":\"1709001\",\"name\":\"B. Mandal\"}],\"doi\":\"10.1145/3240508.3240713\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d09d663055b3b6d588bf4de2f386bb144d09aea8\",\"title\":\"Deep Adaptive Temporal Pooling for Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d09d663055b3b6d588bf4de2f386bb144d09aea8\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1812.05634\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00676\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"title\":\"Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004857242\",\"name\":\"Ye Zhao\"},{\"authorId\":\"2005410031\",\"name\":\"Xiaobin Hu\"},{\"authorId\":\"1390610633\",\"name\":\"Xueliang Liu\"},{\"authorId\":\"10701403\",\"name\":\"Chunxiao Fan\"}],\"doi\":\"10.1007/978-981-15-7670-6_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"86c23cf445c2b11148efa74f8b8019a5bf7c0dae\",\"title\":\"Learning Unsupervised Video Summarization with Semantic-Consistent Network\",\"url\":\"https://www.semanticscholar.org/paper/86c23cf445c2b11148efa74f8b8019a5bf7c0dae\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145859238\",\"name\":\"Liping Xie\"},{\"authorId\":\"1930070\",\"name\":\"Junsheng Zhao\"},{\"authorId\":\"3140817\",\"name\":\"Haikun Wei\"},{\"authorId\":\"145208719\",\"name\":\"Z. Fan\"},{\"authorId\":\"40542981\",\"name\":\"Guochen Pang\"}],\"doi\":\"10.1109/ACCESS.2019.2925916\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"52fbb7ee98835f099c1021c939b0088859c7a2c3\",\"title\":\"Efficient Early Event Detector for Streaming Sequence\",\"url\":\"https://www.semanticscholar.org/paper/52fbb7ee98835f099c1021c939b0088859c7a2c3\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1810.11735\",\"authors\":[{\"authorId\":\"32251567\",\"name\":\"Shikib Mehri\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a82034bd78ee09117baa35ab23b9d600a7509167\",\"title\":\"Middle-Out Decoding\",\"url\":\"https://www.semanticscholar.org/paper/a82034bd78ee09117baa35ab23b9d600a7509167\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22616164\",\"name\":\"Poo-Hee Chang\"},{\"authorId\":\"144362750\",\"name\":\"A. Tan\"}],\"doi\":\"10.1007/978-3-030-03014-8_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a39e9376e0795bbbebdb2ed771636046ded52bc9\",\"title\":\"Learning Generalized Video Memory for Automatic Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a39e9376e0795bbbebdb2ed771636046ded52bc9\",\"venue\":\"MIWAI\",\"year\":2018},{\"arxivId\":\"1803.00057\",\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":\"10.1109/CVPR.2018.00912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"title\":\"A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1611.06492\",\"authors\":[{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"34762956\",\"name\":\"Abhinav Agarwalla\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"144240262\",\"name\":\"P. Mitra\"}],\"doi\":\"10.1109/CVPRW.2017.273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"title\":\"Recurrent Memory Addressing for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47733442\",\"name\":\"Niluthpol Chowdhury Mithun\"},{\"authorId\":\"3428237\",\"name\":\"Juncheng Billy Li\"},{\"authorId\":\"2048745\",\"name\":\"F. Metze\"},{\"authorId\":\"2968713\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1145/3206025.3206064\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9dbca9da6a72ba3739813288b677888a6cf76272\",\"title\":\"Learning Joint Embedding with Multimodal Cues for Cross-Modal Video-Text Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/9dbca9da6a72ba3739813288b677888a6cf76272\",\"venue\":\"ICMR\",\"year\":2018},{\"arxivId\":\"1707.06029\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"4945045\",\"name\":\"Yeonhwa Kim\"},{\"authorId\":\"143912065\",\"name\":\"Kyung Yoo\"},{\"authorId\":\"2135453\",\"name\":\"S. Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.648\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"title\":\"Supervising Neural Attention Models for Video Captioning by Human Gaze Data\",\"url\":\"https://www.semanticscholar.org/paper/1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"48682997\",\"name\":\"P. Vajda\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"}],\"doi\":\"10.1007/978-3-030-58523-5_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f692df9ca116884860d580902fa642370bd1be5d\",\"title\":\"Learning to Generate Grounded Visual Captions Without Localization Supervision\",\"url\":\"https://www.semanticscholar.org/paper/f692df9ca116884860d580902fa642370bd1be5d\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1804.03803\",\"authors\":[{\"authorId\":\"1887625\",\"name\":\"Yuehua Wu\"},{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"39978626\",\"name\":\"Lu Jiang\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1145/3240508.3240640\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d64f52b94977b71976327eeb3db702b246ee39ce\",\"title\":\"Decoupled Novel Object Captioner\",\"url\":\"https://www.semanticscholar.org/paper/d64f52b94977b71976327eeb3db702b246ee39ce\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1910.01442\",\"authors\":[{\"authorId\":\"40879119\",\"name\":\"Kexin Yi\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"3422021\",\"name\":\"Yunzhu Li\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"3045089\",\"name\":\"Jiajun Wu\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f3ecbe546efed8ba42812f977354c16590bad77\",\"title\":\"CLEVRER: CoLlision Events for Video REpresentation and Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/7f3ecbe546efed8ba42812f977354c16590bad77\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mehrdad Hosseinzadeh\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"title\":\"Video Captioning of Future Frames\",\"url\":\"https://www.semanticscholar.org/paper/da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"2001.04608\",\"authors\":[{\"authorId\":\"1527103472\",\"name\":\"Yixuan Li\"},{\"authorId\":\"50218816\",\"name\":\"Zixu Wang\"},{\"authorId\":\"48170350\",\"name\":\"Limin Wang\"},{\"authorId\":\"39914710\",\"name\":\"Gangshan Wu\"}],\"doi\":\"10.1007/978-3-030-58517-4_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6e3034cd8855616533d091dc1d70e969c20a42b\",\"title\":\"Actions as Moving Points\",\"url\":\"https://www.semanticscholar.org/paper/e6e3034cd8855616533d091dc1d70e969c20a42b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1601.06615\",\"authors\":[{\"authorId\":\"2822290\",\"name\":\"Suraj Srinivas\"},{\"authorId\":\"1730952\",\"name\":\"Ravi Kiran Sarvadevabhatla\"},{\"authorId\":\"2217000\",\"name\":\"Konda Reddy Mopuri\"},{\"authorId\":\"40390520\",\"name\":\"N. Prabhu\"},{\"authorId\":\"1784761\",\"name\":\"S. Kruthiventi\"},{\"authorId\":\"144682140\",\"name\":\"R. Venkatesh Babu\"}],\"doi\":\"10.3389/frobt.2015.00036\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"412b3ef02c85087e5f1721176114672c722b17a4\",\"title\":\"A Taxonomy of Deep Convolutional Neural Nets for Computer Vision\",\"url\":\"https://www.semanticscholar.org/paper/412b3ef02c85087e5f1721176114672c722b17a4\",\"venue\":\"Front. Robot. AI\",\"year\":2016},{\"arxivId\":\"2012.04983\",\"authors\":[{\"authorId\":\"1405301761\",\"name\":\"Hedi Ben-younes\"},{\"authorId\":null,\"name\":\"Eloi Zablocki\"},{\"authorId\":\"1398301486\",\"name\":\"P. P\\u00e9rez\"},{\"authorId\":\"51021910\",\"name\":\"M. Cord\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44194713255b6ee890ce1a2d2b727d30e0b71cdb\",\"title\":\"Driving Behavior Explanation with Multi-level Fusion\",\"url\":\"https://www.semanticscholar.org/paper/44194713255b6ee890ce1a2d2b727d30e0b71cdb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.00200\",\"authors\":[{\"authorId\":\"50703697\",\"name\":\"Linjie Li\"},{\"authorId\":\"2378902\",\"name\":\"Yen-Chun Chen\"},{\"authorId\":\"1664725279\",\"name\":\"Yu Cheng\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"1520007550\",\"name\":\"Jingjing Liu\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"title\":\"HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/6961065a16f6c3db4879cfad5875d11ce75e6b2f\",\"venue\":\"EMNLP\",\"year\":2020}],\"corpusId\":4228546,\"doi\":\"10.1109/ICCV.2015.515\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":169,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"references\":[{\"arxivId\":\"1409.4842\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"46641766\",\"name\":\"W. Liu\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"48840704\",\"name\":\"Scott Reed\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"}],\"doi\":\"10.1109/CVPR.2015.7298594\",\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"title\":\"Going deeper with convolutions\",\"url\":\"https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C Szegedy\"},{\"authorId\":null,\"name\":\"W Liu\"},{\"authorId\":null,\"name\":\"Y Jia\"},{\"authorId\":null,\"name\":\"P Sermanet\"},{\"authorId\":null,\"name\":\"S Reed\"},{\"authorId\":null,\"name\":\"D Anguelov\"},{\"authorId\":null,\"name\":\"D Erhan\"},{\"authorId\":null,\"name\":\"V Vanhoucke\"},{\"authorId\":null,\"name\":\"A Rabinovich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Going deeper with convolutions. CVPR\",\"url\":\"\",\"venue\":\"Going deeper with convolutions. CVPR\",\"year\":2015},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Russakovsky\"},{\"authorId\":null,\"name\":\"J. Deng\"},{\"authorId\":null,\"name\":\"H. Su\"},{\"authorId\":null,\"name\":\"J. Krause\"},{\"authorId\":null,\"name\":\"S. Satheesh\"},{\"authorId\":null,\"name\":\"S. Ma\"},{\"authorId\":null,\"name\":\"Z. Huang\"},{\"authorId\":null,\"name\":\"A. Karpathy\"},{\"authorId\":null,\"name\":\"A. Khosla\"},{\"authorId\":null,\"name\":\"M. Bernstein\"},{\"authorId\":null,\"name\":\"A. C. Berg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and L\",\"url\":\"\",\"venue\":\"Fei-Fei. ILSVRC\",\"year\":2014},{\"arxivId\":\"1405.0312\",\"authors\":[{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"145854440\",\"name\":\"M. Maire\"},{\"authorId\":\"50172592\",\"name\":\"Serge J. Belongie\"},{\"authorId\":\"48966748\",\"name\":\"James Hays\"},{\"authorId\":\"1690922\",\"name\":\"P. Perona\"},{\"authorId\":\"1770537\",\"name\":\"D. Ramanan\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":\"10.1007/978-3-319-10602-1_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"71b7178df5d2b112d07e45038cb5637208659ff7\",\"title\":\"Microsoft COCO: Common Objects in Context\",\"url\":\"https://www.semanticscholar.org/paper/71b7178df5d2b112d07e45038cb5637208659ff7\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":\"1503.08909\",\"authors\":[{\"authorId\":\"2340579\",\"name\":\"J. Ng\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"2259154\",\"name\":\"Sudheendra Vijayanarasimhan\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"}],\"doi\":\"10.1109/CVPR.2015.7299101\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"5418b2a482720e013d487a385c26fae0f017c6a6\",\"title\":\"Beyond short snippets: Deep networks for video classification\",\"url\":\"https://www.semanticscholar.org/paper/5418b2a482720e013d487a385c26fae0f017c6a6\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Darpa\"},{\"authorId\":null,\"name\":\"Afrl Dod\"},{\"authorId\":null,\"name\":\"Muri\"},{\"authorId\":null,\"name\":\"Deft Program\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AFRL grant FA8750-13- 2-0026), NSF awards IIS-1427425, IIS-1451244, and IIS- 1212798, and Berkeley Vision and Learning Center\",\"url\":\"\",\"venue\":\"Raymond and Kate acknowledge support from Google. Marcus was supported by the FITweltweit-Program of the German Academic Exchange Service (DAAD)\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C Szegedy\"},{\"authorId\":null,\"name\":\"W Liu\"},{\"authorId\":null,\"name\":\"Y Jia\"},{\"authorId\":null,\"name\":\"P Sermanet\"},{\"authorId\":null,\"name\":\"S Reed\"},{\"authorId\":null,\"name\":\"D Anguelov\"},{\"authorId\":null,\"name\":\"D Erhan\"},{\"authorId\":null,\"name\":\"V Vanhoucke\"},{\"authorId\":null,\"name\":\"A Rabinovich\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Going deeper with convolutions. CoRR, abs/1409\",\"url\":\"\",\"venue\":\"Going deeper with convolutions. CoRR, abs/1409\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M-Vad Venugopalan\"},{\"authorId\":null,\"name\":\"M Rohrbach\"},{\"authorId\":null,\"name\":\"J Donahue\"},{\"authorId\":null,\"name\":\"R Mooney\"},{\"authorId\":null,\"name\":\"T Darrell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"https://youtu.be/pER0mjzSYaM Code and more examples http://vsubhashini.github.io/s2vt.html Sequence to Sequence -Video to Text (S2VT) S\",\"url\":\"\",\"venue\":\"https://youtu.be/pER0mjzSYaM Code and more examples http://vsubhashini.github.io/s2vt.html Sequence to Sequence -Video to Text (S2VT) S\",\"year\":null},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145592791\",\"name\":\"P. Kuznetsova\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.1162/tacl_a_00188\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59927ded86ab4f7253fc32efb351e5a13e746ead\",\"title\":\"TreeTalk: Composition and Compression of Trees for Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/59927ded86ab4f7253fc32efb351e5a13e746ead\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b9f8101c61b415f946625b69f69fc9e3d0d6fc4\",\"title\":\"Generating Natural-Language Video Descriptions Using Text-Mined Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/3b9f8101c61b415f946625b69f69fc9e3d0d6fc4\",\"venue\":\"AAAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4746730\",\"name\":\"J. Lackie\"}],\"doi\":\"10.1016/b978-0-12-384931-1.00016-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db750f063bc099e6d1d2d0a92a34d7c213a893e3\",\"title\":\"P\",\"url\":\"https://www.semanticscholar.org/paper/db750f063bc099e6d1d2d0a92a34d7c213a893e3\",\"venue\":\"The Dictionary of Cell & Molecular Biology\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Movie Clips\"},{\"authorId\":null,\"name\":\"Mpii-Md & M-Vads2vt ) S Venugopalan\"},{\"authorId\":null,\"name\":\"M Rohrbach\"},{\"authorId\":null,\"name\":\"J Donahue\"},{\"authorId\":null,\"name\":\"R Mooney\"},{\"authorId\":null,\"name\":\"T Darrell\"},{\"authorId\":null,\"name\":\"K Saenko Mpii-Md\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Sequence to Sequence -Video to Text\",\"url\":\"\",\"venue\":\"Sequence to Sequence -Video to Text\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Darpa\"},{\"authorId\":null,\"name\":\"Afrl Dod\"},{\"authorId\":null,\"name\":\"Muri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"DEFT program (AFRL grant FA8750- 13-2-0026), NSF awards IIS-1427425, IIS-1451244, and IIS-1212798, and BVLC\",\"url\":\"\",\"venue\":\"Raymond and Kate acknowledge support from Google. Marcus was supported by the FITweltweit-Program of the German Academic Exchange Service (DAAD)\",\"year\":null},{\"arxivId\":\"1410.4615\",\"authors\":[{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a\",\"title\":\"Learning to Execute\",\"url\":\"https://www.semanticscholar.org/paper/0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3111912\",\"name\":\"Navdeep Jaitly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f\",\"title\":\"Towards End-To-End Speech Recognition with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0fa553cfa0cf3cbdf7a913aa2ae789a757dfb32f\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"144031005\",\"name\":\"A. Bruhn\"},{\"authorId\":\"2188270\",\"name\":\"N. Papenberg\"},{\"authorId\":\"7789445\",\"name\":\"J. Weickert\"}],\"doi\":\"10.1007/978-3-540-24673-2_3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"91228e00fe33ed6072cfe849ab9e98160461549d\",\"title\":\"High Accuracy Optical Flow Estimation Based on a Theory for Warping\",\"url\":\"https://www.semanticscholar.org/paper/91228e00fe33ed6072cfe849ab9e98160461549d\",\"venue\":\"ECCV\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143645506\",\"name\":\"P. Over\"},{\"authorId\":\"145876242\",\"name\":\"G. Awad\"},{\"authorId\":\"3241934\",\"name\":\"J. Fiscus\"},{\"authorId\":\"2974372\",\"name\":\"Brian Antonishek\"},{\"authorId\":\"33765735\",\"name\":\"M. Michel\"},{\"authorId\":\"1740640\",\"name\":\"Wessel Kraaij\"},{\"authorId\":\"1680223\",\"name\":\"A. Smeaton\"},{\"authorId\":\"1693391\",\"name\":\"G. Qu\\u00e9not\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"760f81db6dabbbd0744595e9fc3d55138e5cb88e\",\"title\":\"TRECVID 2015 - An Overview of the Goals, Tasks, Data, Evaluation Mechanisms and Metrics\",\"url\":\"https://www.semanticscholar.org/paper/760f81db6dabbbd0744595e9fc3d55138e5cb88e\",\"venue\":\"TRECVID\",\"year\":2010},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"145297531\",\"name\":\"A. Lai\"},{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1162/tacl_a_00166\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"44040913380206991b1991daf1192942e038fe31\",\"title\":\"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions\",\"url\":\"https://www.semanticscholar.org/paper/44040913380206991b1991daf1192942e038fe31\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3141511\",\"name\":\"S. Banerjee\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"title\":\"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments\",\"url\":\"https://www.semanticscholar.org/paper/0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7\",\"venue\":\"IEEvaluation@ACL\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Mao\"},{\"authorId\":null,\"name\":\"W Xu\"},{\"authorId\":null,\"name\":\"Y Yang\"},{\"authorId\":null,\"name\":\"J Wang\"},{\"authorId\":null,\"name\":\"A L Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep captioning with multimodal recurrent neural networks (mrnn ). arXiv:1412\",\"url\":\"\",\"venue\":\"Deep captioning with multimodal recurrent neural networks (mrnn ). arXiv:1412\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"42.9% of the predictions are identical to some training sentence, and another 38.3% can be obtained by inserting, deleting or substituting one word from some sentence in the training corpus\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Sequence to Sequence Video to Text -Subhashini Venugopalan\",\"url\":\"\",\"venue\":\"Sequence to Sequence Video to Text -Subhashini Venugopalan\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"title\":\"Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3312922\",\"name\":\"H. Aradhye\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"1842163\",\"name\":\"J. Yagnik\"}],\"doi\":\"10.1109/ICDMW.2009.79\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0cc75d0939454a42f81a0ed692bf9fd1cef11f8f\",\"title\":\"Video2Text: Learning to Annotate Video Content\",\"url\":\"https://www.semanticscholar.org/paper/0cc75d0939454a42f81a0ed692bf9fd1cef11f8f\",\"venue\":\"2009 IEEE International Conference on Data Mining Workshops\",\"year\":2009},{\"arxivId\":\"1502.04681\",\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"title\":\"Unsupervised Learning of Video Representations using LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46730712\",\"name\":\"S. Wei\"},{\"authorId\":\"38161033\",\"name\":\"Y. Zhao\"},{\"authorId\":\"1749780\",\"name\":\"Zhenfeng Zhu\"},{\"authorId\":\"144387494\",\"name\":\"N. Liu\"}],\"doi\":\"10.1109/TKDE.2009.145\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ede815b17c091bdbe55d86842059027317d66b7\",\"title\":\"Multimodal Fusion for Video Search Reranking\",\"url\":\"https://www.semanticscholar.org/paper/1ede815b17c091bdbe55d86842059027317d66b7\",\"venue\":\"IEEE Transactions on Knowledge and Data Engineering\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Over\"},{\"authorId\":null,\"name\":\"G. Awad\"},{\"authorId\":null,\"name\":\"M. Michel\"},{\"authorId\":null,\"name\":\"J. Fiscus\"},{\"authorId\":null,\"name\":\"G. Sanders\"},{\"authorId\":null,\"name\":\"B. Shaw\"},{\"authorId\":null,\"name\":\"A. F. Smeaton\"},{\"authorId\":null,\"name\":\"G. Qu\\u00e9enot\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"TRECVID 2012 \\u2013 an overview of the goals\",\"url\":\"\",\"venue\":\"tasks, data, evaluation mechanisms and metrics. In Proceedings of TRECVID 2012\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3338852\",\"name\":\"Haiqi Huang\"},{\"authorId\":\"1792278\",\"name\":\"Yueming Lu\"},{\"authorId\":\"2098026\",\"name\":\"Fangwei Zhang\"},{\"authorId\":\"1770819\",\"name\":\"Songlin Sun\"}],\"doi\":\"10.1007/978-3-642-35795-4_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a89f5aa9fa1a316c9a3054cba6f9ef02cb299475\",\"title\":\"A Multi-modal Clustering Method for Web Videos\",\"url\":\"https://www.semanticscholar.org/paper/a89f5aa9fa1a316c9a3054cba6f9ef02cb299475\",\"venue\":\"ISCTCS\",\"year\":2012},{\"arxivId\":\"1411.5654\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f4af49a1ead3c81cc5d023878cb67c5646dd8a04\",\"title\":\"Learning a Recurrent Visual Representation for Image Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/f4af49a1ead3c81cc5d023878cb67c5646dd8a04\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1412.6632\",\"authors\":[{\"authorId\":\"36010601\",\"name\":\"Junhua Mao\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"title\":\"Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)\",\"url\":\"https://www.semanticscholar.org/paper/54b2b6f35f1b5704dddfaa3a137a2f4ad3dfe745\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.441\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d721f4d64b8e722222c876f0a0f226ed49476347\",\"title\":\"Action Recognition with Improved Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/d721f4d64b8e722222c876f0a0f226ed49476347\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"title\":\"Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"venue\":\"COLING\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1648075979\",\"name\":\"B. A. R. Kernfach\"},{\"authorId\":\"1648268257\",\"name\":\"Nur IM Sommersemester\"},{\"authorId\":\"1648268256\",\"name\":\"\\u00dcbersicht Franz\\u00f6sisch\"},{\"authorId\":\"1648134774\",\"name\":\"\\u00dcbersicht Italienisch\"}],\"doi\":\"10.1515/9783111697888-004\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fff1b293b45d06c8462021aa6c90c81e743e131b\",\"title\":\"B\",\"url\":\"https://www.semanticscholar.org/paper/fff1b293b45d06c8462021aa6c90c81e743e131b\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1408.5093\",\"authors\":[{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"3049736\",\"name\":\"S. Karayev\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1145/2647868.2654889\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bdb186ec4726e00a8051119636d4df3b94043b5\",\"title\":\"Caffe: Convolutional Architecture for Fast Feature Embedding\",\"url\":\"https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1409.1259\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/W14-4012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1eb09fecd75eb27825dce4f964b97f4f5cc399d7\",\"title\":\"On the Properties of Neural Machine Translation: Encoder-Decoder Approaches\",\"url\":\"https://www.semanticscholar.org/paper/1eb09fecd75eb27825dce4f964b97f4f5cc399d7\",\"venue\":\"SSST@EMNLP\",\"year\":2014},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1411.6031\",\"authors\":[{\"authorId\":\"2082991\",\"name\":\"Georgia Gkioxari\"},{\"authorId\":\"143751119\",\"name\":\"Jitendra Malik\"}],\"doi\":\"10.1109/CVPR.2015.7298676\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"295a1c000c5e3fd5959bb250e8376a06efa405b1\",\"title\":\"Finding action tubes\",\"url\":\"https://www.semanticscholar.org/paper/295a1c000c5e3fd5959bb250e8376a06efa405b1\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015}],\"title\":\"Sequence to Sequence -- Video to Text\",\"topics\":[{\"topic\":\"Recurrent neural network\",\"topicId\":\"16115\",\"url\":\"https://www.semanticscholar.org/topic/16115\"},{\"topic\":\"Complex dynamics\",\"topicId\":\"112741\",\"url\":\"https://www.semanticscholar.org/topic/112741\"},{\"topic\":\"Input/output\",\"topicId\":\"8197\",\"url\":\"https://www.semanticscholar.org/topic/8197\"},{\"topic\":\"Language model\",\"topicId\":\"26812\",\"url\":\"https://www.semanticscholar.org/topic/26812\"},{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Video clip\",\"topicId\":\"30493\",\"url\":\"https://www.semanticscholar.org/topic/30493\"},{\"topic\":\"Voice activity detection\",\"topicId\":\"191934\",\"url\":\"https://www.semanticscholar.org/topic/191934\"},{\"topic\":\"Long short-term memory\",\"topicId\":\"117199\",\"url\":\"https://www.semanticscholar.org/topic/117199\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"}],\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015}\n"