"{\"abstract\":\"Recent progress in using recurrent neural networks (RNNs) for image description has motivated the exploration of their application for video description. However, while images are static, working with videos requires modeling their dynamic temporal structure and then properly integrating that information into a natural language description model. In this context, we propose an approach that successfully takes into account both the local and global temporal structure of videos to produce descriptions. First, our approach incorporates a spatial temporal 3-D convolutional neural network (3-D CNN) representation of the short temporal dynamics. The 3-D CNN representation is trained on video action recognition tasks, so as to produce a representation that is tuned to human motion and behavior. Second we propose a temporal attention mechanism that allows to go beyond local temporal modeling and learns to automatically select the most relevant temporal segments given the text-generating RNN. Our approach exceeds the current state-of-art for both BLEU and METEOR metrics on the Youtube2Text dataset. We also present results on a new, larger and more challenging dataset of paired video and natural language descriptions.\",\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\",\"url\":\"https://www.semanticscholar.org/author/145095579\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\",\"url\":\"https://www.semanticscholar.org/author/1730844\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\",\"url\":\"https://www.semanticscholar.org/author/1979489\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\",\"url\":\"https://www.semanticscholar.org/author/2482072\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\",\"url\":\"https://www.semanticscholar.org/author/1972076\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\",\"url\":\"https://www.semanticscholar.org/author/1777528\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\",\"url\":\"https://www.semanticscholar.org/author/1760871\"}],\"citationVelocity\":155,\"citations\":[{\"arxivId\":\"1703.10025\",\"authors\":[{\"authorId\":\"2578924\",\"name\":\"X. Zhu\"},{\"authorId\":null,\"name\":\"Yujie Wang\"},{\"authorId\":\"3304536\",\"name\":\"Jifeng Dai\"},{\"authorId\":\"145347147\",\"name\":\"Lu Yuan\"},{\"authorId\":\"1732264\",\"name\":\"Y. Wei\"}],\"doi\":\"10.1109/ICCV.2017.52\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0246f6754c38324a837c0ebd1b51976f413f80ad\",\"title\":\"Flow-Guided Feature Aggregation for Video Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/0246f6754c38324a837c0ebd1b51976f413f80ad\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1801.06066\",\"authors\":[{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"145779951\",\"name\":\"X. Wang\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1007/s11263-018-1095-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac5d0705a9ddba29151fd539c668ba2c0d16deb6\",\"title\":\"RED-Net: A Recurrent Encoder\\u2013Decoder Network for Video-Based Face Alignment\",\"url\":\"https://www.semanticscholar.org/paper/ac5d0705a9ddba29151fd539c668ba2c0d16deb6\",\"venue\":\"International Journal of Computer Vision\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40940512\",\"name\":\"Jun Liu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"},{\"authorId\":\"1693461\",\"name\":\"Ping Hu\"},{\"authorId\":\"7667912\",\"name\":\"L. Duan\"},{\"authorId\":\"1711097\",\"name\":\"A. Kot\"}],\"doi\":\"10.1109/CVPR.2017.391\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2630bf9c818406ee951d5737f532a27aa21ad9f5\",\"title\":\"Global Context-Aware Attention LSTM Networks for 3D Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2630bf9c818406ee951d5737f532a27aa21ad9f5\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1611.04899\",\"authors\":[{\"authorId\":\"49992767\",\"name\":\"Yilin Song\"},{\"authorId\":\"6220115\",\"name\":\"Jonathan Viventi\"},{\"authorId\":\"1711589\",\"name\":\"Yao Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9b90c8dcc4fa53d472312fd9dc558259443de49d\",\"title\":\"Diversity encouraged learning of unsupervised LSTM ensemble for neural activity video prediction\",\"url\":\"https://www.semanticscholar.org/paper/9b90c8dcc4fa53d472312fd9dc558259443de49d\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1708.03725\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bc7a3573a464bca2cdca71f6f32e798464b85ee6\",\"title\":\"Exploiting Semantic Contextualization for Interpretation of Human Activity in Videos\",\"url\":\"https://www.semanticscholar.org/paper/bc7a3573a464bca2cdca71f6f32e798464b85ee6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157339\",\"name\":\"Samuele Martelli\"},{\"authorId\":\"2773787\",\"name\":\"L. Mazzei\"},{\"authorId\":\"32842734\",\"name\":\"C. Canali\"},{\"authorId\":\"47800024\",\"name\":\"Paolo Guardiani\"},{\"authorId\":\"49169568\",\"name\":\"S. Giunta\"},{\"authorId\":\"40866763\",\"name\":\"Alberto Ghiazza\"},{\"authorId\":\"13559034\",\"name\":\"I. Mondino\"},{\"authorId\":\"2578223\",\"name\":\"F. Cannella\"},{\"authorId\":\"1727204\",\"name\":\"Vittorio Murino\"},{\"authorId\":\"8955013\",\"name\":\"A. D. Bue\"}],\"doi\":\"10.1109/TII.2018.2807797\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"89bcfab917c073371548c4ebc2fdedcb974d94c6\",\"title\":\"Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry\",\"url\":\"https://www.semanticscholar.org/paper/89bcfab917c073371548c4ebc2fdedcb974d94c6\",\"venue\":\"IEEE Transactions on Industrial Informatics\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"Jinglun Shi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Describing Video with Multiple Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"C. Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145254492\",\"name\":\"Z. Wen\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"0ed94607d6f1506f0cfb1bf0896a41300db52a1a\",\"title\":\"Nested-Wasserstein Distance for Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/0ed94607d6f1506f0cfb1bf0896a41300db52a1a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"title\":\"CLARA: Dynamic Doctor Representation Learning for Clinical Trial Recruitment\",\"url\":\"https://www.semanticscholar.org/paper/e7cc8e0430de6b9f1a2970e7f4f7596be0a0716e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1601.03896\",\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.24963/ijcai.2017/704\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/3240508.3240677\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5abe63d687f927a0ac61e9ad62f88b355d89caf\",\"title\":\"Spotting and Aggregating Salient Regions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e5abe63d687f927a0ac61e9ad62f88b355d89caf\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46395893\",\"name\":\"Y. Wang\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"39527132\",\"name\":\"Xiaojie Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c53127df6a87a2842f6b64440382890c397e4daf\",\"title\":\"Video description with integrated visual and textual information\",\"url\":\"https://www.semanticscholar.org/paper/c53127df6a87a2842f6b64440382890c397e4daf\",\"venue\":\"China Communications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3459761\",\"name\":\"Kyle Caudle\"},{\"authorId\":\"1819409\",\"name\":\"R. Hoover\"},{\"authorId\":\"120139070\",\"name\":\"Aaron Alphonsus\"},{\"authorId\":\"1500529573\",\"name\":\"S. Shradha\"}],\"doi\":\"10.1109/ICMLA.2019.00091\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3d4d26e991d7eecb82d06699afd7c7a7f84e0c3b\",\"title\":\"Advanced Decision Making and Interpretability through Neural Shrubs\",\"url\":\"https://www.semanticscholar.org/paper/3d4d26e991d7eecb82d06699afd7c7a7f84e0c3b\",\"venue\":\"2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48490580\",\"name\":\"J. Park\"},{\"authorId\":\"35409051\",\"name\":\"Chibon Song\"},{\"authorId\":\"47180565\",\"name\":\"J. Han\"}],\"doi\":\"10.1109/ICIIBMS.2017.8279760\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"title\":\"A study of evaluation metrics and datasets for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/2e8d9299af393da5e7534f0a8cce5a270c0b7775\",\"venue\":\"2017 International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":\"10.1145/3347449.3357484\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"989730c00381805543baa470a2d6490cc5354a13\",\"title\":\"L-STAP: Learned Spatio-Temporal Adaptive Pooling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/989730c00381805543baa470a2d6490cc5354a13\",\"venue\":\"AI4TV@MM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23278631\",\"name\":\"Guangle Yao\"},{\"authorId\":\"144673774\",\"name\":\"T. Lei\"},{\"authorId\":\"23227806\",\"name\":\"Jiandan Zhong\"}],\"doi\":\"10.1016/j.patrec.2018.05.018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2c12495bfb2f47881191ce0cb672f0372c6a31e2\",\"title\":\"A review of Convolutional-Neural-Network-based action recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c12495bfb2f47881191ce0cb672f0372c6a31e2\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":\"1710.01559\",\"authors\":[{\"authorId\":\"27079423\",\"name\":\"H. Hajj\"},{\"authorId\":\"1727789\",\"name\":\"M. Lamard\"},{\"authorId\":\"2742744\",\"name\":\"P. Conze\"},{\"authorId\":\"1974436\",\"name\":\"B. Cochener\"},{\"authorId\":\"1684052\",\"name\":\"G. Quellec\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fc56e02563776c3d5be7b74012940779da6a789d\",\"title\":\"Monitoring tool usage in cataract surgery videos using boosted convolutional and recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/fc56e02563776c3d5be7b74012940779da6a789d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46276803\",\"name\":\"J. Li\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1016/j.jvcir.2020.102875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"title\":\"Translating video into language by enhancing visual and language representations\",\"url\":\"https://www.semanticscholar.org/paper/32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"1603.08486\",\"authors\":[{\"authorId\":\"1797022\",\"name\":\"Hoo-Chang Shin\"},{\"authorId\":\"1742509\",\"name\":\"Kirk Roberts\"},{\"authorId\":\"50706692\",\"name\":\"Le Lu\"},{\"authorId\":\"1398175407\",\"name\":\"Dina Demner-Fushman\"},{\"authorId\":\"150167064\",\"name\":\"Jianhua Yao\"},{\"authorId\":\"144838131\",\"name\":\"R. Summers\"}],\"doi\":\"10.1109/CVPR.2016.274\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0460d3497490fa8332c5ff2ecdab88fb7dff4755\",\"title\":\"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation\",\"url\":\"https://www.semanticscholar.org/paper/0460d3497490fa8332c5ff2ecdab88fb7dff4755\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"2006.14150\",\"authors\":[{\"authorId\":\"145749362\",\"name\":\"Jing Shi\"},{\"authorId\":\"8776560\",\"name\":\"Xuankai Chang\"},{\"authorId\":\"47871517\",\"name\":\"Pengcheng Guo\"},{\"authorId\":\"1746678\",\"name\":\"Shinji Watanabe\"},{\"authorId\":\"144307323\",\"name\":\"Y. Fujita\"},{\"authorId\":\"46372563\",\"name\":\"Jiaming Xu\"},{\"authorId\":\"46798766\",\"name\":\"Bo Xu\"},{\"authorId\":\"144206960\",\"name\":\"Lei Xie\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1\",\"title\":\"Sequence to Multi-Sequence Learning via Conditional Chain Mapping for Mixture Signals\",\"url\":\"https://www.semanticscholar.org/paper/e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":\"10.1016/j.csl.2020.101102\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a00a2b6eb505a172a36de81dc803a9d45597bc8a\",\"title\":\"Investigating topics, audio representations and attention for multimodal scene-aware dialog\",\"url\":\"https://www.semanticscholar.org/paper/a00a2b6eb505a172a36de81dc803a9d45597bc8a\",\"venue\":\"Comput. Speech Lang.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46740175\",\"name\":\"Gobinath Loganathan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee87cd4570a1d20f0ef6e40ad8e9ac81252ca26d\",\"title\":\"Real-time Intrusion Detection using Multidimensional Sequence-to-Sequence Machine Learning and Adaptive Stream Processing\",\"url\":\"https://www.semanticscholar.org/paper/ee87cd4570a1d20f0ef6e40ad8e9ac81252ca26d\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150337817\",\"name\":\"Weike Jin\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"48515305\",\"name\":\"Yimeng Li\"},{\"authorId\":\"49298718\",\"name\":\"Jie Li\"},{\"authorId\":\"145974112\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3321505\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99b722fc4e168eddea69521f26c77e31e56fc9f4\",\"title\":\"Video Question Answering via Knowledge-based Progressive Spatial-Temporal Attention Network\",\"url\":\"https://www.semanticscholar.org/paper/99b722fc4e168eddea69521f26c77e31e56fc9f4\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73312190\",\"name\":\"W. Zhang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"3245182\",\"name\":\"Yanpeng Cao\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/TMM.2019.2935678\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8559c89a4df40ec6250cc8a04d1a5d5f3a84623e\",\"title\":\"Frame Augmented Alternating Attention Network for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8559c89a4df40ec6250cc8a04d1a5d5f3a84623e\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144620591\",\"name\":\"X. Wu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"2523380\",\"name\":\"Qingge Ji\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/CVPR.2018.00714\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"title\":\"Interpretable Video Captioning via Trajectory Structured Localization\",\"url\":\"https://www.semanticscholar.org/paper/f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1706.01231\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"153757316\",\"name\":\"Zhao Guo\"},{\"authorId\":\"144973314\",\"name\":\"Wu Liu\"},{\"authorId\":\"2712862\",\"name\":\"D. Zhang\"},{\"authorId\":\"152555512\",\"name\":\"Heng Tao Shen\"}],\"doi\":\"10.24963/ijcai.2017/381\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"616c2b2c8bb35b0da1feb9d869131edd5b53642a\",\"title\":\"Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/616c2b2c8bb35b0da1feb9d869131edd5b53642a\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"2007.14164\",\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"1654091065\",\"name\":\"Wei Liu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1007/978-3-030-58548-8_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fb52197928290d3020b2256ccab22d5bf93c366\",\"title\":\"Learning Modality Interaction for Temporal Sentence Localization and Event Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/5fb52197928290d3020b2256ccab22d5bf93c366\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40617978\",\"name\":\"X. Wang\"},{\"authorId\":\"49986692\",\"name\":\"Yanan Gu\"},{\"authorId\":\"49779966\",\"name\":\"Xinbo Gao\"},{\"authorId\":\"36893758\",\"name\":\"Zheng Hui\"}],\"doi\":\"10.1016/J.NEUCOM.2019.06.078\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e6ec2b7af6431da2bee5e2397705c863c1ee7bcf\",\"title\":\"Dual residual attention module network for single image super resolution\",\"url\":\"https://www.semanticscholar.org/paper/e6ec2b7af6431da2bee5e2397705c863c1ee7bcf\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"1908.03451\",\"authors\":[{\"authorId\":\"32412901\",\"name\":\"W. Yang\"},{\"authorId\":\"49166656\",\"name\":\"Weijia Jia\"},{\"authorId\":\"49896524\",\"name\":\"W. Gao\"},{\"authorId\":\"50177674\",\"name\":\"X. Zhou\"},{\"authorId\":\"2950191\",\"name\":\"Yutao Luo\"}],\"doi\":\"10.1145/3357384.3357872\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5db8d349238d918eafecf61641c81a2fe6f21c3\",\"title\":\"Interactive Variance Attention based Online Spoiler Detection for Time-Sync Comments\",\"url\":\"https://www.semanticscholar.org/paper/c5db8d349238d918eafecf61641c81a2fe6f21c3\",\"venue\":\"CIKM\",\"year\":2019},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1708.02300\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D17-1103\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"title\":\"Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3123266.3123391\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"39836fbbcd2a664edb31119e88870c38b83df352\",\"title\":\"Adaptively Attending to Visual Attributes and Linguistic Knowledge for Captioning\",\"url\":\"https://www.semanticscholar.org/paper/39836fbbcd2a664edb31119e88870c38b83df352\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37467623\",\"name\":\"Y. Miao\"},{\"authorId\":\"1740721\",\"name\":\"F. Metze\"}],\"doi\":\"10.1007/978-3-319-64680-0_13\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8ef8e52a04132d2cbb789fa793c628fac22935b\",\"title\":\"End-to-End Architectures for Speech Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d8ef8e52a04132d2cbb789fa793c628fac22935b\",\"venue\":\"New Era for Robust Speech Recognition, Exploiting Deep Learning\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"96066534\",\"name\":\"P. Joshi\"},{\"authorId\":\"51497543\",\"name\":\"Chitwan Saharia\"},{\"authorId\":\"1557378944\",\"name\":\"V. Singh\"},{\"authorId\":\"51267359\",\"name\":\"Digvijaysingh Gautam\"},{\"authorId\":\"145799547\",\"name\":\"Ganesh Ramakrishnan\"},{\"authorId\":\"1557645545\",\"name\":\"P. Jyothi\"}],\"doi\":\"10.1109/ICCVW.2019.00459\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"title\":\"A Tale of Two Modalities for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/523e8f226cc75a8fa5597aeb410e9236efc02f5d\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49340052\",\"name\":\"L. Belhadj\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5dfcee6042427dfaf445bb3452eccae4cfc4b42\",\"title\":\"Reconnaissance des actions humaines : m\\u00e9thode bas\\u00e9e sur la r\\u00e9duction de dimensionnalit\\u00e9 par MDS spatio-temporelle\",\"url\":\"https://www.semanticscholar.org/paper/a5dfcee6042427dfaf445bb3452eccae4cfc4b42\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2203994\",\"name\":\"Tengfei Xing\"},{\"authorId\":\"50218964\",\"name\":\"Zhaohui Wang\"},{\"authorId\":\"7788280\",\"name\":\"J. Yang\"},{\"authorId\":\"144602988\",\"name\":\"Yi Ji\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"}],\"doi\":\"10.1007/978-3-030-00764-5_68\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2147e0ef8507e1a4880a916e46c27e15c11d65f4\",\"title\":\"Text to Region: Visual-Word Guided Saliency Detection\",\"url\":\"https://www.semanticscholar.org/paper/2147e0ef8507e1a4880a916e46c27e15c11d65f4\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35548557\",\"name\":\"Konstantinos Gkountakos\"},{\"authorId\":\"47381330\",\"name\":\"A. Dimou\"},{\"authorId\":\"33961149\",\"name\":\"G. Papadopoulos\"},{\"authorId\":\"1747572\",\"name\":\"P. Daras\"}],\"doi\":\"10.1109/ICE.2019.8792602\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"title\":\"Incorporating Textual Similarity in Video Captioning Schemes\",\"url\":\"https://www.semanticscholar.org/paper/2e3bdb38138a5adbb7b24257780bc3dc6d3a3f3f\",\"venue\":\"2019 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)\",\"year\":2019},{\"arxivId\":\"2012.10930\",\"authors\":[{\"authorId\":\"51174755\",\"name\":\"X. Zhang\"},{\"authorId\":\"47535646\",\"name\":\"C. Liu\"},{\"authorId\":\"32617816\",\"name\":\"Faliang Chang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"abd8dd9da90b5b08a8ea67d937b8688101fa0f86\",\"title\":\"Guidance Module Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abd8dd9da90b5b08a8ea67d937b8688101fa0f86\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8071088\",\"name\":\"R. Oruganti\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"00e19d93780ecf8f807c510a1105749d5bb1a2f3\",\"title\":\"Image Description using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/00e19d93780ecf8f807c510a1105749d5bb1a2f3\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"2069818\",\"name\":\"D. Zhang\"},{\"authorId\":\"1706774\",\"name\":\"J. Li\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/CVPR.2017.662\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3b0b706fc94b35a1eddd830685e07870315b9565\",\"title\":\"Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description\",\"url\":\"https://www.semanticscholar.org/paper/3b0b706fc94b35a1eddd830685e07870315b9565\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390538450\",\"name\":\"Xun Yang\"},{\"authorId\":\"49544292\",\"name\":\"Xueliang Liu\"},{\"authorId\":\"2980051\",\"name\":\"Meng Jian\"},{\"authorId\":\"1993659018\",\"name\":\"Xinjian Gao\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1145/3394171.3413610\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b839e6b1f65672d154feb6f6668d64a2333c71ee\",\"title\":\"Weakly-Supervised Video Object Grounding by Exploring Spatio-Temporal Contexts\",\"url\":\"https://www.semanticscholar.org/paper/b839e6b1f65672d154feb6f6668d64a2333c71ee\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"11445222\",\"name\":\"H. Wang\"}],\"doi\":\"10.1609/AAAI.V34I07.7012\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d718baf9187fa5851a9389e5e250d47ba1210e0e\",\"title\":\"FASTER Recurrent Networks for Efficient Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/d718baf9187fa5851a9389e5e250d47ba1210e0e\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"af5b71c042e2bf39e5085ad5b5f1215e129989df\",\"title\":\"Deep Learning for Semantic Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/af5b71c042e2bf39e5085ad5b5f1215e129989df\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1606.00625\",\"authors\":[{\"authorId\":null,\"name\":\"Yu Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"0c56a5c466fd012739dfa71ae36f6558e4c817ed\",\"title\":\"Storytelling of Photo Stream with Bidirectional Multi-thread Recurrent Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/0c56a5c466fd012739dfa71ae36f6558e4c817ed\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1612.06704\",\"authors\":[{\"authorId\":\"2283756\",\"name\":\"Donggeun Yoo\"},{\"authorId\":\"2377298\",\"name\":\"Sunggyun Park\"},{\"authorId\":\"2848176\",\"name\":\"Kyunghyun Paeng\"},{\"authorId\":\"1926578\",\"name\":\"Joon-Young Lee\"},{\"authorId\":\"2398271\",\"name\":\"In-So Kweon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa32f5b0a866b04a89f75cda32e0975a541864ff\",\"title\":\"Action-Driven Object Detection with Top-Down Visual Attentions\",\"url\":\"https://www.semanticscholar.org/paper/aa32f5b0a866b04a89f75cda32e0975a541864ff\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"144542135\",\"name\":\"D. Mahajan\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3afd90a0675936e2f747171a1063d8171d987656\",\"title\":\"l 1 l 2 l 3 l 4 l 5 ( a ) Class-Agnostic Temporal\",\"url\":\"https://www.semanticscholar.org/paper/3afd90a0675936e2f747171a1063d8171d987656\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5781871\",\"name\":\"Jiaqi Su\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"title\":\"Study of Video Captioning Problem\",\"url\":\"https://www.semanticscholar.org/paper/511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27575517\",\"name\":\"Dian Shao\"},{\"authorId\":\"145984817\",\"name\":\"Yu Xiong\"},{\"authorId\":\"47827957\",\"name\":\"Y. Zhao\"},{\"authorId\":\"39360892\",\"name\":\"Q. Huang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01240-3_13\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9925eb898e7484ae289b675785313ddb47bb20bd\",\"title\":\"Find and Focus: Retrieve and Localize Video Events with Natural Language Queries\",\"url\":\"https://www.semanticscholar.org/paper/9925eb898e7484ae289b675785313ddb47bb20bd\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1812.10786\",\"authors\":[{\"authorId\":\"29899438\",\"name\":\"Talha Ahmad Siddiqui\"},{\"authorId\":\"34173298\",\"name\":\"Samarth Bharadwaj\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac0f6e63cefbc5c9a3fba3f3166d6b3af83cfd03\",\"title\":\"Future semantic segmentation of time-lapsed videos with large temporal displacement\",\"url\":\"https://www.semanticscholar.org/paper/ac0f6e63cefbc5c9a3fba3f3166d6b3af83cfd03\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"505a777fbf3760c16a80a3e893017d91d2c35b08\",\"title\":\"Supplementary Material to : Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/505a777fbf3760c16a80a3e893017d91d2c35b08\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"51305348\",\"name\":\"Zhu Zhang\"},{\"authorId\":\"51055350\",\"name\":\"Shuwen Xiao\"},{\"authorId\":\"123034558\",\"name\":\"Z. Xiao\"},{\"authorId\":\"145477645\",\"name\":\"X. Yan\"},{\"authorId\":\"50812077\",\"name\":\"J. Yu\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TIP.2019.2922062\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80c67ece4198e3dd1ef88e6ddb81eb71bee5f3fa\",\"title\":\"Long-Form Video Question Answering via Dynamic Hierarchical Reinforced Networks\",\"url\":\"https://www.semanticscholar.org/paper/80c67ece4198e3dd1ef88e6ddb81eb71bee5f3fa\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":null,\"name\":\"Ning Xu\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"title\":\"Tianjin University and National University of Singapore at TRECVID 2017: Video to Text Description\",\"url\":\"https://www.semanticscholar.org/paper/c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"64f2a36c4cb34d1e7f47a68a36a12a2d9e8f4897\",\"title\":\"IJCAI 2019 Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5)\",\"url\":\"https://www.semanticscholar.org/paper/64f2a36c4cb34d1e7f47a68a36a12a2d9e8f4897\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"65790980\",\"name\":\"Haixin Peng\"},{\"authorId\":\"12843106\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1002/ima.22430\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"398d38813c7b2dced75415a814eee0da3ebf55c9\",\"title\":\"Spatio\\u2010temporal context based recurrent visual attention model for lymph node detection\",\"url\":\"https://www.semanticscholar.org/paper/398d38813c7b2dced75415a814eee0da3ebf55c9\",\"venue\":\"Int. J. Imaging Syst. Technol.\",\"year\":2020},{\"arxivId\":\"2003.03749\",\"authors\":[{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1109/CVPR42600.2020.01090\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96485bda4f4118da249cc8a898230281ac8040a7\",\"title\":\"Better Captioning With Sequence-Level Exploration\",\"url\":\"https://www.semanticscholar.org/paper/96485bda4f4118da249cc8a898230281ac8040a7\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7025059\",\"name\":\"C. Orozco\"},{\"authorId\":\"36507903\",\"name\":\"M. E. Buemi\"},{\"authorId\":\"1405801592\",\"name\":\"J. Jacobo-Berlles\"}],\"doi\":\"10.1109/SCCC.2018.8705254\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"00b0e22ff30f68e6163bfe34beebbf23e3a85813\",\"title\":\"Video to Text Study using an Encoder-Decoder Networks Approach\",\"url\":\"https://www.semanticscholar.org/paper/00b0e22ff30f68e6163bfe34beebbf23e3a85813\",\"venue\":\"2018 37th International Conference of the Chilean Computer Science Society (SCCC)\",\"year\":2018},{\"arxivId\":\"1903.01489\",\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/s11042-018-7040-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1344317f255a9d338fb80f276126951b9644f7e3\",\"title\":\"M-VAD names: a dataset for video captioning with naming\",\"url\":\"https://www.semanticscholar.org/paper/1344317f255a9d338fb80f276126951b9644f7e3\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":\"2008.12432\",\"authors\":[{\"authorId\":\"3349165\",\"name\":\"Pallabi Ghosh\"},{\"authorId\":\"19173161\",\"name\":\"Nirat Saini\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"},{\"authorId\":\"51453757\",\"name\":\"Abhinav Shrivastava\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"117595877c1fca610f94c8d07009105092939ecc\",\"title\":\"All About Knowledge Graphs for Actions\",\"url\":\"https://www.semanticscholar.org/paper/117595877c1fca610f94c8d07009105092939ecc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.01447\",\"authors\":[{\"authorId\":\"145908975\",\"name\":\"X. Shen\"},{\"authorId\":\"40434674\",\"name\":\"X. Tian\"},{\"authorId\":\"3493947\",\"name\":\"Anfeng He\"},{\"authorId\":\"3141359\",\"name\":\"Shaoyan Sun\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1145/2964284.2964316\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"346e044735c9c862bf9687e36bab466d290f91ab\",\"title\":\"Transform-Invariant Convolutional Neural Networks for Image Classification and Search\",\"url\":\"https://www.semanticscholar.org/paper/346e044735c9c862bf9687e36bab466d290f91ab\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"2008.06880\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"144644708\",\"name\":\"Jin Yu\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":null,\"name\":\"Jie Liu\"},{\"authorId\":\"1726030259\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"1712223662\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394171.3413880\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"title\":\"Poet: Product-oriented Video Captioner for E-commerce\",\"url\":\"https://www.semanticscholar.org/paper/72ca5f49b67f0e57e1f213323ff5d884e91ee824\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98831710\",\"name\":\"B. Yang\"},{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"86010b6cb557103eda7e28fa2b497c9a9697fa8d\",\"title\":\"Non-Autoregressive Video Captioning with Iterative Refinement\",\"url\":\"https://www.semanticscholar.org/paper/86010b6cb557103eda7e28fa2b497c9a9697fa8d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72547325\",\"name\":\"Bassel S. Chawky\"},{\"authorId\":\"2805974\",\"name\":\"A. S. Elons\"},{\"authorId\":\"114287783\",\"name\":\"A. Ali\"},{\"authorId\":\"2382767\",\"name\":\"H. A. Shedeed\"}],\"doi\":\"10.1007/978-3-319-63754-9_19\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88d70d8cf4557e032bc359d2c2a15b06686612ea\",\"title\":\"A Study of Action Recognition Problems: Dataset and Architectures Perspectives\",\"url\":\"https://www.semanticscholar.org/paper/88d70d8cf4557e032bc359d2c2a15b06686612ea\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"49418159\",\"name\":\"Y. Wang\"},{\"authorId\":\"145546921\",\"name\":\"J. Qin\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1109/TCSVT.2019.2894161\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c75881e226b5d57e1c5570bf8e51a93bdada9e8\",\"title\":\"stagNet: An Attentive Semantic RNN for Group Activity and Individual Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6c75881e226b5d57e1c5570bf8e51a93bdada9e8\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1807.09418\",\"authors\":[{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"49033321\",\"name\":\"Qi Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TMM.2019.2930041\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"92e02bd58b99ac17b475081611f091f4b0776482\",\"title\":\"Video Storytelling: Textual Summaries for Events\",\"url\":\"https://www.semanticscholar.org/paper/92e02bd58b99ac17b475081611f091f4b0776482\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":\"2101.00359\",\"authors\":[{\"authorId\":null,\"name\":\"Mingjian Zhu\"},{\"authorId\":null,\"name\":\"Chenrui Duan\"},{\"authorId\":\"1409820051\",\"name\":\"Changbin Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"title\":\"Video Captioning in Compressed Video\",\"url\":\"https://www.semanticscholar.org/paper/93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"145950948\",\"name\":\"Xue Li\"},{\"authorId\":\"2558130\",\"name\":\"Z. Shi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e8ce74a73bb0b3197d4194fcb638710d76970654\",\"title\":\"Video Captioning with Listwise Supervision\",\"url\":\"https://www.semanticscholar.org/paper/e8ce74a73bb0b3197d4194fcb638710d76970654\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40dd2b9aace337467c6e1e269d0cb813442313d7\",\"title\":\"Localizing spatially and temporally objects and actions in videos. (Localiser spatio-temporallement des objets et des actions dans des vid\\u00e9os)\",\"url\":\"https://www.semanticscholar.org/paper/40dd2b9aace337467c6e1e269d0cb813442313d7\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1109/ICPR.2016.7900081\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5d8eb0ccd00d66b649c6a4c06edb0e34093a2357\",\"title\":\"Automatic video description generation via LSTM with joint two-stream encoding\",\"url\":\"https://www.semanticscholar.org/paper/5d8eb0ccd00d66b649c6a4c06edb0e34093a2357\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32860700\",\"name\":\"P. Nguyen\"},{\"authorId\":\"145138436\",\"name\":\"Qing Li\"},{\"authorId\":\"3493929\",\"name\":\"Zhi-Qi Cheng\"},{\"authorId\":\"40501192\",\"name\":\"Yi-Jie Lu\"},{\"authorId\":\"71777847\",\"name\":\"H. Zhang\"},{\"authorId\":\"1772198\",\"name\":\"X. Wu\"},{\"authorId\":\"152650698\",\"name\":\"Chong-Wah Ngo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bf2e9704e49b9ca57794bb54e29997f3bebbe6a\",\"title\":\"VIREO @ TRECVID 2017: Video-to-Text, Ad-hoc Video Search, and Video hyperlinking\",\"url\":\"https://www.semanticscholar.org/paper/3bf2e9704e49b9ca57794bb54e29997f3bebbe6a\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ning Xu\"},{\"authorId\":\"49299019\",\"name\":\"Junnan Li\"},{\"authorId\":null,\"name\":\"Yang Li\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"153576783\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b4cab3708793b39bad96ea4af7e3c7cc45ef7e2\",\"title\":\"MSR Video to Language Challenge\",\"url\":\"https://www.semanticscholar.org/paper/5b4cab3708793b39bad96ea4af7e3c7cc45ef7e2\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2706269\",\"name\":\"Chengxiang Yin\"},{\"authorId\":\"152949437\",\"name\":\"Jian Tang\"},{\"authorId\":\"48559420\",\"name\":\"Zhiyuan Xu\"},{\"authorId\":null,\"name\":\"Yanzhi Wang\"}],\"doi\":\"10.1109/TNNLS.2019.2938015\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d796ff29f8798c418d5374a6632231f02233dbba\",\"title\":\"Memory Augmented Deep Recurrent Neural Network for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/d796ff29f8798c418d5374a6632231f02233dbba\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145583985\",\"name\":\"Wenwu Zhu\"},{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"48385803\",\"name\":\"W. Gao\"}],\"doi\":\"10.1109/TMM.2020.2969791\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a94ac484b3cc19cbf03bdbac0c727579a09fc3b9\",\"title\":\"Multimedia Intelligence: When Multimedia Meets Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/a94ac484b3cc19cbf03bdbac0c727579a09fc3b9\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"14618116\",\"name\":\"Chongyang Gao\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1016/J.PATREC.2018.07.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab27d39857f613af36eff3fa3796904f474f8cbd\",\"title\":\"Sequence in sequence for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ab27d39857f613af36eff3fa3796904f474f8cbd\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92538707\",\"name\":\"Qi Zheng\"},{\"authorId\":\"1409848027\",\"name\":\"Chaoyue Wang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":\"10.1109/CVPR42600.2020.01311\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"title\":\"Syntax-Aware Action Targeting for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/59cca2242fb20a6070369b5c1f172e5ee1d71785\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1808.09284\",\"authors\":[{\"authorId\":\"1765674\",\"name\":\"Tianshui Chen\"},{\"authorId\":\"51055456\",\"name\":\"R. Chen\"},{\"authorId\":\"48443459\",\"name\":\"Lin Nie\"},{\"authorId\":\"144361019\",\"name\":\"Xiaonan Luo\"},{\"authorId\":\"144799773\",\"name\":\"Xiaobai Liu\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/TMM.2018.2870062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9de38f78b3859e3155a7d7fdc3eee362152b4e61\",\"title\":\"Neural Task Planning With AND\\u2013OR Graph Representations\",\"url\":\"https://www.semanticscholar.org/paper/9de38f78b3859e3155a7d7fdc3eee362152b4e61\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29072828\",\"name\":\"Seungwhan Moon\"},{\"authorId\":\"143712374\",\"name\":\"J. Carbonell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fdfabec6da5006096bb7370a26c293f35104712f\",\"title\":\"Learn to Active Learn: Dynamic Active Learning with Attention-based Strategies Selection\",\"url\":\"https://www.semanticscholar.org/paper/fdfabec6da5006096bb7370a26c293f35104712f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"39491387\",\"name\":\"J. Zhou\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"}],\"doi\":\"10.1109/TIP.2018.2855422\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fd3d94fac6a282414406716040b10c1746634ecd\",\"title\":\"Video Captioning by Adversarial LSTM\",\"url\":\"https://www.semanticscholar.org/paper/fd3d94fac6a282414406716040b10c1746634ecd\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":\"1711.08097\",\"authors\":[{\"authorId\":\"8598253\",\"name\":\"Wang-Li Hao\"},{\"authorId\":\"145274329\",\"name\":\"Zhaoxiang Zhang\"},{\"authorId\":\"32561502\",\"name\":\"H. Guan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dde65325dc7600d02983a76bd54693f0050946a4\",\"title\":\"Integrating both Visual and Audio Cues for Enhanced Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/dde65325dc7600d02983a76bd54693f0050946a4\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26928334\",\"name\":\"Shivam Duggal\"},{\"authorId\":\"37059411\",\"name\":\"S. Manik\"},{\"authorId\":\"27059378\",\"name\":\"Mohan Ghai\"}],\"doi\":\"10.1145/3163080.3163108\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f781277a40f14ef83485863ba24e937d76d9bb04\",\"title\":\"Amalgamation of Video Description and Multiple Object Localization using single Deep Learning Model\",\"url\":\"https://www.semanticscholar.org/paper/f781277a40f14ef83485863ba24e937d76d9bb04\",\"venue\":\"ICSPS 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"1693997\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1145/3240508.3240538\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"72f9116a04e584081635500e9f0789fa26e4d15f\",\"title\":\"Hierarchical Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72f9116a04e584081635500e9f0789fa26e4d15f\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2035969\",\"name\":\"S. Pini\"},{\"authorId\":\"3468983\",\"name\":\"M. Cornia\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1007/978-3-319-68548-9_36\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a\",\"title\":\"Towards Video Captioning with Naming: A Novel Dataset and a Multi-modal Approach\",\"url\":\"https://www.semanticscholar.org/paper/ff44d8938c52cfdca48c80f8e1618bbcbf91cb2a\",\"venue\":\"ICIAP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38949713\",\"name\":\"J. Deshmukh\"},{\"authorId\":\"1818074\",\"name\":\"K. M. Annervaz\"},{\"authorId\":\"35120578\",\"name\":\"Shubhashis Sengupta\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"09e0abd2b609e7d7dd37c6dafd10312710bf43d4\",\"title\":\"A Sequence Modeling Approach for Structured Data Extraction from Unstructured Text\",\"url\":\"https://www.semanticscholar.org/paper/09e0abd2b609e7d7dd37c6dafd10312710bf43d4\",\"venue\":\"SemDeep@IJCAI\",\"year\":2019},{\"arxivId\":\"1804.05448\",\"authors\":[{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.18653/v1/N18-2125\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"title\":\"Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2714a3932b9d096b7bb285f6ec415cb047eafe09\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51312029\",\"name\":\"Yu-Sheng Chou\"},{\"authorId\":\"2042119\",\"name\":\"Pai-Heng Hsiao\"},{\"authorId\":\"2818798\",\"name\":\"S. Lin\"},{\"authorId\":\"1704678\",\"name\":\"H. Liao\"}],\"doi\":\"10.1109/ICASSP.2018.8461899\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ceb67253b0ba134dd1c8b87a6be4e4bf507d35b\",\"title\":\"How Sampling Rate Affects Cross-Domain Transfer Learning for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/4ceb67253b0ba134dd1c8b87a6be4e4bf507d35b\",\"venue\":\"2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26409184\",\"name\":\"M. Z. Khan\"},{\"authorId\":\"40589170\",\"name\":\"M. A. Hassan\"},{\"authorId\":\"123354310\",\"name\":\"Saleet Ul Hassan\"},{\"authorId\":\"65752088\",\"name\":\"Muhammad Usman Ghanni Khan\"}],\"doi\":\"10.1109/ICET.2018.8603653\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6061d7697c331e7da3f18f002e5bc30fbe26ded0\",\"title\":\"Semantic Analysis of News Based on the Deep Convolution Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/6061d7697c331e7da3f18f002e5bc30fbe26ded0\",\"venue\":\"2018 14th International Conference on Emerging Technologies (ICET)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/CRV.2017.51\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6718f2feea2d16b894b738551c38871c8afee11b\",\"title\":\"Towards a Knowledge-Based Approach for Generating Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/6718f2feea2d16b894b738551c38871c8afee11b\",\"venue\":\"2017 14th Conference on Computer and Robot Vision (CRV)\",\"year\":2017},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1711.06330\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":\"10.1109/CVPR.2018.00710\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66aebb3af16aaa78579344784212ae10f60ec27e\",\"title\":\"Attend and Interact: Higher-Order Object Interactions for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/66aebb3af16aaa78579344784212ae10f60ec27e\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1711.06354\",\"authors\":[{\"authorId\":\"7437104\",\"name\":\"Chih-Yao Ma\"},{\"authorId\":\"2293919\",\"name\":\"Asim Kadav\"},{\"authorId\":\"50162780\",\"name\":\"I. Melvin\"},{\"authorId\":\"145276578\",\"name\":\"Z. Kira\"},{\"authorId\":\"9202076\",\"name\":\"G. Al-Regib\"},{\"authorId\":\"1775043\",\"name\":\"H. Graf\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"title\":\"Grounded Objects and Interactions for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/28ee8af25582c9c3a04fa0f0809367d7ee936dca\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1090/QAM/1530\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3e0b6b6921e93ee2dbc279c4a63c630156e5d1e9\",\"title\":\"Generating open world descriptions of video using common sense knowledge in a pattern theory framework\",\"url\":\"https://www.semanticscholar.org/paper/3e0b6b6921e93ee2dbc279c4a63c630156e5d1e9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32471241\",\"name\":\"Panuwat Assawinjaipetch\"},{\"authorId\":\"2173767\",\"name\":\"K. Shirai\"},{\"authorId\":\"1779078\",\"name\":\"Virach Sornlertlamvanich\"},{\"authorId\":\"31395575\",\"name\":\"S. Marukata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3ebe6114afb2c9f345cb2e6e5702a32b4f49ebcc\",\"title\":\"Recurrent Neural Network with Word Embedding for Complaint Classification\",\"url\":\"https://www.semanticscholar.org/paper/3ebe6114afb2c9f345cb2e6e5702a32b4f49ebcc\",\"venue\":\"WLSI/OIAF4HLT@COLING\",\"year\":2016},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1801.01078\",\"authors\":[{\"authorId\":\"145331822\",\"name\":\"H. Salehinejad\"},{\"authorId\":\"6440332\",\"name\":\"Julianne K Baarb\\u00e9\"},{\"authorId\":\"48588799\",\"name\":\"Sharan Sankar\"},{\"authorId\":\"4775678\",\"name\":\"J. Barfett\"},{\"authorId\":\"6164044\",\"name\":\"Errol Colak\"},{\"authorId\":\"1742726\",\"name\":\"S. Valaee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ca781fb293c2521d0737899d252d47a97eca0d58\",\"title\":\"Recent Advances in Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/ca781fb293c2521d0737899d252d47a97eca0d58\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1705.09882\",\"authors\":[{\"authorId\":\"2474219\",\"name\":\"Nikolaos Karianakis\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"},{\"authorId\":\"2249952\",\"name\":\"Y. Chen\"},{\"authorId\":\"1715959\",\"name\":\"Stefano Soatto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7237b27dac6dfe5c07a2c6c36ad848e6bcc7ac77\",\"title\":\"Person Depth ReID: Robust Person Re-identification with Commodity Depth Sensors\",\"url\":\"https://www.semanticscholar.org/paper/7237b27dac6dfe5c07a2c6c36ad848e6bcc7ac77\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9920529\",\"name\":\"Yihang Lou\"},{\"authorId\":\"145079208\",\"name\":\"Y. Bai\"},{\"authorId\":\"1705047\",\"name\":\"S. Wang\"},{\"authorId\":\"7667912\",\"name\":\"L. Duan\"}],\"doi\":\"10.1145/3240508.3240602\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d255c9e87d701078116665428f4d229a78c5de07\",\"title\":\"Multi-Scale Context Attention Network for Image Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d255c9e87d701078116665428f4d229a78c5de07\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9734988\",\"name\":\"Yuecong Xu\"},{\"authorId\":\"2562263\",\"name\":\"Jianfei Yang\"},{\"authorId\":\"144067957\",\"name\":\"K. Mao\"}],\"doi\":\"10.1016/J.NEUCOM.2019.05.027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fd9a6ff5f908a6e8e785eb0a1432a5c9da2c2192\",\"title\":\"Semantic-filtered Soft-Split-Aware video captioning with audio-augmented feature\",\"url\":\"https://www.semanticscholar.org/paper/fd9a6ff5f908a6e8e785eb0a1432a5c9da2c2192\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144455842\",\"name\":\"Y. Pan\"}],\"doi\":\"10.1016/J.ENG.2016.04.018\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"668bfea0aaa0565f58aa76d44d700e8a3a576f52\",\"title\":\"Heading toward Artificial Intelligence 2.0\",\"url\":\"https://www.semanticscholar.org/paper/668bfea0aaa0565f58aa76d44d700e8a3a576f52\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1511.05526\",\"authors\":[{\"authorId\":\"1900013\",\"name\":\"Zhengyang Wu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1733702\",\"name\":\"Matthew R. Walter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a9453721f35f364e176a5aaa7bdb622f72fbcaec\",\"title\":\"Learning Articulated Motion Models from Visual and Lingual Signals\",\"url\":\"https://www.semanticscholar.org/paper/a9453721f35f364e176a5aaa7bdb622f72fbcaec\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"47539491\",\"name\":\"J. Zhang\"},{\"authorId\":\"47747947\",\"name\":\"Qiang Guo\"},{\"authorId\":\"47570083\",\"name\":\"Jun Lei\"},{\"authorId\":\"3143729\",\"name\":\"D. Tu\"}],\"doi\":\"10.1109/ICIVC.2016.7571276\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4451d8e34e92abe8dceb425b3e41cb5fe948739\",\"title\":\"Generating video description with Long-Short Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/b4451d8e34e92abe8dceb425b3e41cb5fe948739\",\"venue\":\"2016 International Conference on Image, Vision and Computing (ICIVC)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2786437\",\"name\":\"Linghui Li\"},{\"authorId\":\"144044848\",\"name\":\"Sheng Tang\"},{\"authorId\":\"4303531\",\"name\":\"Lixi Deng\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d1a7dae43b630d61d19d6cf139824380f2cf42f\",\"title\":\"Image Caption with Global-Local Attention\",\"url\":\"https://www.semanticscholar.org/paper/7d1a7dae43b630d61d19d6cf139824380f2cf42f\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"37498905\",\"name\":\"L. Li\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/BigMM.2018.8499257\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7ae5f10acd306a7842a16542b6b236e0a964de10\",\"title\":\"Saliency-Based Spatiotemporal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7ae5f10acd306a7842a16542b6b236e0a964de10\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"title\":\"Video representation learning with deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1611.06607\",\"authors\":[{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"153365679\",\"name\":\"J. Johnson\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2017.356\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a7011346ce939e3251915e92ae2f252e4c7f777\",\"title\":\"A Hierarchical Approach for Generating Descriptive Image Paragraphs\",\"url\":\"https://www.semanticscholar.org/paper/3a7011346ce939e3251915e92ae2f252e4c7f777\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49969948\",\"name\":\"Zhihui Li\"},{\"authorId\":\"144950946\",\"name\":\"Xiaojun Chang\"},{\"authorId\":\"2082966\",\"name\":\"L. Yao\"},{\"authorId\":\"2585415\",\"name\":\"Shirui Pan\"},{\"authorId\":\"144062687\",\"name\":\"Zongyuan Ge\"},{\"authorId\":\"46702510\",\"name\":\"Hua-Xiang Zhang\"}],\"doi\":\"10.1145/3394486.3403072\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c2dbeb0edfd63241c5c902f5bded78e42d6d1ef\",\"title\":\"Grounding Visual Concepts for Zero-Shot Event Detection and Event Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c2dbeb0edfd63241c5c902f5bded78e42d6d1ef\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"2001.06944\",\"authors\":[{\"authorId\":\"1390533012\",\"name\":\"Ruiyi Zhang\"},{\"authorId\":\"1752041\",\"name\":\"Changyou Chen\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145254492\",\"name\":\"Z. Wen\"},{\"authorId\":\"49337256\",\"name\":\"W. Wang\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"86724befacdd9fd8748607f5b025aa59fb7ef010\",\"title\":\"Nested-Wasserstein Self-Imitation Learning for Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/86724befacdd9fd8748607f5b025aa59fb7ef010\",\"venue\":\"AISTATS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47285696\",\"name\":\"Oscar Koller\"},{\"authorId\":\"10040378\",\"name\":\"Sepehr Zargaran\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.1109/CVPR.2017.364\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28b85543e8f12c3d2d2227dcc9f5e87c685535ea\",\"title\":\"Re-Sign: Re-Aligned End-to-End Sequence Modelling with Deep Recurrent CNN-HMMs\",\"url\":\"https://www.semanticscholar.org/paper/28b85543e8f12c3d2d2227dcc9f5e87c685535ea\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"9390267\",\"name\":\"Qifan Yang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.24963/ijcai.2017/492\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a05e84f77e1dacaa1c59ba0d92919bdcfe4debbb\",\"title\":\"Video Question Answering via Hierarchical Spatio-Temporal Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/a05e84f77e1dacaa1c59ba0d92919bdcfe4debbb\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70147929\",\"name\":\"H. Yang\"},{\"authorId\":\"18997752\",\"name\":\"Jun Zhang\"},{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"3249639\",\"name\":\"Tingjin Luo\"}],\"doi\":\"10.3233/JIFS-18209\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6d8c084b395804b68ffbec2724a91b1cf8e269b\",\"title\":\"Bi-direction hierarchical LSTM with spatial-temporal attention for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/c6d8c084b395804b68ffbec2724a91b1cf8e269b\",\"venue\":\"J. Intell. Fuzzy Syst.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143704786\",\"name\":\"David Balderas\"},{\"authorId\":\"144302764\",\"name\":\"P. Ponce\"},{\"authorId\":\"144394807\",\"name\":\"A. Molina\"}],\"doi\":\"10.1016/J.ESWA.2018.12.055\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dca5599927a69bc8b8c315197cbd1b877e8af556\",\"title\":\"Convolutional long short term memory deep neural networks for image sequence prediction\",\"url\":\"https://www.semanticscholar.org/paper/dca5599927a69bc8b8c315197cbd1b877e8af556\",\"venue\":\"Expert Syst. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"47775167\",\"name\":\"Teng Li\"},{\"authorId\":\"47906413\",\"name\":\"Y. Wang\"},{\"authorId\":\"1730308\",\"name\":\"X. Wu\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1016/j.cviu.2017.06.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"94a86a758ae2608c00e9690e9951e805755bb1a1\",\"title\":\"Learning explicit video attributes from mid-level representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/94a86a758ae2608c00e9690e9951e805755bb1a1\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1511.03745\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2874347\",\"name\":\"Ronghang Hu\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-46448-0_49\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14c2321851fb5ae580a19726dd2753a525d6ad76\",\"title\":\"Grounding of Textual Phrases in Images by Reconstruction\",\"url\":\"https://www.semanticscholar.org/paper/14c2321851fb5ae580a19726dd2753a525d6ad76\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1704.04497\",\"authors\":[{\"authorId\":\"2338742\",\"name\":\"Y. Jang\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"49170458\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.149\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"title\":\"TGIF-QA: Toward Spatio-Temporal Reasoning in Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b2f521c02c6ed3080c5fe123e938cdf4555e6fd2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"48519520\",\"name\":\"L. Wolf\"}],\"doi\":\"10.1109/ICCV.2017.20\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"62e6b749ed5825739aa906021c5e613803d5cbe2\",\"title\":\"Temporal Tessellation: A Unified Approach for Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/62e6b749ed5825739aa906021c5e613803d5cbe2\",\"venue\":\"ICCV\",\"year\":2017},{\"arxivId\":\"1810.11735\",\"authors\":[{\"authorId\":\"32251567\",\"name\":\"Shikib Mehri\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a82034bd78ee09117baa35ab23b9d600a7509167\",\"title\":\"Middle-Out Decoding\",\"url\":\"https://www.semanticscholar.org/paper/a82034bd78ee09117baa35ab23b9d600a7509167\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66916694\",\"name\":\"X. Xiao\"},{\"authorId\":\"46660013\",\"name\":\"L. Wang\"},{\"authorId\":\"145211780\",\"name\":\"Bin Fan\"},{\"authorId\":\"1380311632\",\"name\":\"Shinming Xiang\"},{\"authorId\":\"144809241\",\"name\":\"C. Pan\"}],\"doi\":\"10.18653/v1/D19-1213\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"title\":\"Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag\",\"url\":\"https://www.semanticscholar.org/paper/ed8cf8a585e3506778ba0584cdff1ac7d9db75b4\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":\"1705.01253\",\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1016/j.neucom.2018.06.069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"783e48629dfbb44697b15a3bc0cb2aa3eea490eb\",\"title\":\"The Forgettable-Watcher Model for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/783e48629dfbb44697b15a3bc0cb2aa3eea490eb\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1705.09406\",\"authors\":[{\"authorId\":\"11138090\",\"name\":\"Tadas Baltru\\u0161aitis\"},{\"authorId\":\"118242121\",\"name\":\"Chaitanya Ahuja\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/TPAMI.2018.2798607\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91\",\"title\":\"Multimodal Machine Learning: A Survey and Taxonomy\",\"url\":\"https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1609.06782\",\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3122865.3122867\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"title\":\"Deep Learning for Video Classification and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"venue\":\"Frontiers of Multimedia Research\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"144631588\",\"name\":\"A. Chandar\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.1162/neco_a_01060\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe1077f6b79e14457db77d7477a477f40f87e7e6\",\"title\":\"Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes\",\"url\":\"https://www.semanticscholar.org/paper/fe1077f6b79e14457db77d7477a477f40f87e7e6\",\"venue\":\"Neural Computation\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46583706\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"40476140\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1109/CVPR.2018.00784\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b910a6f687a4e56062dc326786cee297bd60e8c1\",\"title\":\"M3: Multimodal Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b910a6f687a4e56062dc326786cee297bd60e8c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.24963/ijcai.2018/143\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e2e5cef45c60c52fb0d0415cca6cbf35beab3873\",\"title\":\"Multi-modal Circulant Fusion for Video-to-Language and Backward\",\"url\":\"https://www.semanticscholar.org/paper/e2e5cef45c60c52fb0d0415cca6cbf35beab3873\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1611.05216\",\"authors\":[{\"authorId\":\"38179026\",\"name\":\"Y. Shi\"},{\"authorId\":\"40161651\",\"name\":\"Yonghong Tian\"},{\"authorId\":\"5765799\",\"name\":\"Yaowei Wang\"},{\"authorId\":\"144424248\",\"name\":\"Wei Zeng\"},{\"authorId\":\"34097174\",\"name\":\"Tiejun Huang\"}],\"doi\":\"10.1109/ICCV.2017.84\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d145edb79ec035d6cf3f50714429f51fb18f0a2f\",\"title\":\"Learning Long-Term Dependencies for Action Recognition with a Biologically-Inspired Deep Network\",\"url\":\"https://www.semanticscholar.org/paper/d145edb79ec035d6cf3f50714429f51fb18f0a2f\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1577678641\",\"name\":\"Ganesh Samarth\"},{\"authorId\":\"1974345797\",\"name\":\"Sheetal Ojha\"},{\"authorId\":\"96566998\",\"name\":\"N. Pareek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5624963548a28f00a3aef20dca3bdbfe3d394d9d\",\"title\":\"Knowledge Fusion Transformers for Video Action Classification\",\"url\":\"https://www.semanticscholar.org/paper/5624963548a28f00a3aef20dca3bdbfe3d394d9d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34751361\",\"name\":\"Arjun Sharma\"},{\"authorId\":\"47606073\",\"name\":\"A. Biswas\"},{\"authorId\":\"144883800\",\"name\":\"A. Gandhi\"},{\"authorId\":\"145844088\",\"name\":\"Sonal Patil\"},{\"authorId\":\"2116262\",\"name\":\"O. Deshmukh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dac50ff2168c1c5228be9ad0d854bfaf13a5388\",\"title\":\"LIVELINET: A Multimodal Deep Recurrent Neural Network to Predict Liveliness in Educational Videos\",\"url\":\"https://www.semanticscholar.org/paper/3dac50ff2168c1c5228be9ad0d854bfaf13a5388\",\"venue\":\"EDM\",\"year\":2016},{\"arxivId\":\"1612.00385\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"1756344\",\"name\":\"T. Baltrusaitis\"},{\"authorId\":\"2743835\",\"name\":\"D. Tax\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1109/CVPR.2017.94\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"04556d5f283d7f90e24d43371d3f51faff8c0423\",\"title\":\"Temporal Attention-Gated Model for Robust Sequence Classification\",\"url\":\"https://www.semanticscholar.org/paper/04556d5f283d7f90e24d43371d3f51faff8c0423\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1704.01194\",\"authors\":[{\"authorId\":\"9284940\",\"name\":\"Harshala Gammulle\"},{\"authorId\":\"1980700\",\"name\":\"Simon Denman\"},{\"authorId\":\"1729760\",\"name\":\"S. Sridharan\"},{\"authorId\":\"3140440\",\"name\":\"C. Fookes\"}],\"doi\":\"10.1109/WACV.2017.27\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"cdd828917bfafd1b7d876f48abeae094d2ba3bcf\",\"title\":\"Two Stream LSTM: A Deep Fusion Framework for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/cdd828917bfafd1b7d876f48abeae094d2ba3bcf\",\"venue\":\"2017 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2017},{\"arxivId\":\"1707.04045\",\"authors\":[{\"authorId\":\"3223082\",\"name\":\"Jae Hyeon Yoo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a03500376200c0ded58eab702ddc678d4e1cca82\",\"title\":\"Large-scale Video Classification guided by Batch Normalized LSTM Translator\",\"url\":\"https://www.semanticscholar.org/paper/a03500376200c0ded58eab702ddc678d4e1cca82\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1812.08407\",\"authors\":[{\"authorId\":\"32208823\",\"name\":\"S. H. Kumar\"},{\"authorId\":\"3442103\",\"name\":\"Eda Okur\"},{\"authorId\":\"38531701\",\"name\":\"S. Sahay\"},{\"authorId\":\"51011510\",\"name\":\"Juan Jose Alvarado Leanos\"},{\"authorId\":\"1808244\",\"name\":\"J. Huang\"},{\"authorId\":\"1896095\",\"name\":\"L. Nachman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"title\":\"Context, Attention and Audio Feature Explorations for Audio Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/c672dbd03c6b9d2be7c7bb92ef0a5d2f827fcf65\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1906.01885\",\"authors\":[{\"authorId\":\"134470377\",\"name\":\"Mohammad Ibrahim Sarker\"},{\"authorId\":\"2958010\",\"name\":\"HyeongSik Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bbcb74c27ab878010d6edb57765cfefbf1b984bf\",\"title\":\"Farm land weed detection with region-based deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/bbcb74c27ab878010d6edb57765cfefbf1b984bf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492114969\",\"name\":\"Jason Li\"},{\"authorId\":null,\"name\":\"Helen Qiu jasonkli\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3e2bbf3cdc651c1531a9bafbda59a6807417d578\",\"title\":\"Comparing Attention-based Neural Architectures for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3e2bbf3cdc651c1531a9bafbda59a6807417d578\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2759569\",\"name\":\"N. Neverova\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fc2ba7caaf440ab28c5a223f5093d3faa62a2413\",\"title\":\"Deep learning for human motion analysis\",\"url\":\"https://www.semanticscholar.org/paper/fc2ba7caaf440ab28c5a223f5093d3faa62a2413\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255213\",\"name\":\"Z. Zhang\"},{\"authorId\":\"38188040\",\"name\":\"Dong Xu\"},{\"authorId\":\"47337540\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"2597292\",\"name\":\"Chuanqi Tan\"}],\"doi\":\"10.1109/TCSVT.2019.2936526\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"title\":\"Show, Tell and Summarize: Dense Video Captioning Using Visual Cue Aided Sentence Summarization\",\"url\":\"https://www.semanticscholar.org/paper/b1fed611b13bd5c463a340b375f382e48d45d1dc\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40061480\",\"name\":\"Z. Dong\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"2000237078\",\"name\":\"Qi Cui\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/978-3-030-55187-2_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78a1094e0968cf4e2b61c83100d971031597ae4b\",\"title\":\"Adaptive Attention Mechanism Based Semantic Compositional Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/78a1094e0968cf4e2b61c83100d971031597ae4b\",\"venue\":\"IntelliSys\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145813731\",\"name\":\"X. Xu\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"49958046\",\"name\":\"HaiBin Liu\"},{\"authorId\":null,\"name\":\"Ji Yi\"},{\"authorId\":\"50218964\",\"name\":\"Zhaohui Wang\"}],\"doi\":\"10.2991/ITIM-17.2017.34\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd1ee9e1d24ac60e722a8e51518e44669b052557\",\"title\":\"Video Description Using Learning Multiple Features\",\"url\":\"https://www.semanticscholar.org/paper/dd1ee9e1d24ac60e722a8e51518e44669b052557\",\"venue\":\"ICIT 2017\",\"year\":2017},{\"arxivId\":\"1903.06879\",\"authors\":[{\"authorId\":\"49279229\",\"name\":\"Lifang Wu\"},{\"authorId\":\"144037701\",\"name\":\"Z. Yang\"},{\"authorId\":\"2842106\",\"name\":\"Jiaoyu He\"},{\"authorId\":\"2980051\",\"name\":\"Meng Jian\"},{\"authorId\":\"3471034\",\"name\":\"Y. Xu\"},{\"authorId\":\"144983032\",\"name\":\"D. Xu\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":\"10.1109/TCSVT.2019.2912529\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"30e4e817459b0d032d11ae6759e717bc4213836b\",\"title\":\"Ontology-Based Global and Collective Motion Patterns for Event Classification in Basketball Videos\",\"url\":\"https://www.semanticscholar.org/paper/30e4e817459b0d032d11ae6759e717bc4213836b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1604.03249\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1007/978-3-319-50077-5_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"title\":\"Attributes as Semantic Units between Natural Language and Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1606.04631\",\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"83672162\",\"name\":\"Zi Huang\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967258\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"title\":\"Bidirectional Long-Short Term Memory for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/b193b01b4d15959ac85c3bd9d98af1f82159bd1f\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"1706.03121\",\"authors\":[{\"authorId\":\"21496852\",\"name\":\"R. Panda\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1109/TMM.2017.2708981\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7b3cf7f4888fef2d827be9d3e6047054e8fa0940\",\"title\":\"Multi-View Surveillance Video Summarization via Joint Embedding and Sparse Optimization\",\"url\":\"https://www.semanticscholar.org/paper/7b3cf7f4888fef2d827be9d3e6047054e8fa0940\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145179162\",\"name\":\"Mingxing Zhang\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"145833207\",\"name\":\"Ning Xie\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"}],\"doi\":\"10.1016/j.sigpro.2017.12.008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ae9f14872520bb2926cfef2b670a5e9bc3870a5\",\"title\":\"Recurrent attention network using spatial-temporal relations for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/2ae9f14872520bb2926cfef2b670a5e9bc3870a5\",\"venue\":\"Signal Process.\",\"year\":2018},{\"arxivId\":\"1803.00057\",\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":\"10.1109/CVPR.2018.00912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"title\":\"A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/f8027791ca64f4270cd86e2deb830a3a7383dcff\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ACCESS.2019.2942000\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"801827592d18c4e6170d88f8345465de4a8db7ca\",\"title\":\"Video Captioning With Adaptive Attention and Mixed Loss Optimization\",\"url\":\"https://www.semanticscholar.org/paper/801827592d18c4e6170d88f8345465de4a8db7ca\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31320085\",\"name\":\"Li Liang-hua\"},{\"authorId\":\"31190693\",\"name\":\"Wang Yong-xiong\"}],\"doi\":\"10.12086/OEE.2020.190139\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"676fcbc0e65f6440bef432085b28cae1d8684d31\",\"title\":\"Efficient 3D dense residual network and its application in human action recognition\",\"url\":\"https://www.semanticscholar.org/paper/676fcbc0e65f6440bef432085b28cae1d8684d31\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.04865\",\"authors\":[{\"authorId\":\"3087214\",\"name\":\"Yutaro Shigeto\"},{\"authorId\":\"31678456\",\"name\":\"Y. Yoshikawa\"},{\"authorId\":\"2996464\",\"name\":\"Jiaqing Lin\"},{\"authorId\":\"39702069\",\"name\":\"A. Takeuchi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"title\":\"Video Caption Dataset for Describing Human Actions in Japanese\",\"url\":\"https://www.semanticscholar.org/paper/46cff2f107f0f9c84aa0d70c64a6d1acc5e766fe\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27079423\",\"name\":\"H. Hajj\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da07c4617624a0bb992e3981b80b2a7a263141db\",\"title\":\"Video analysis for augmented cataract surgery\",\"url\":\"https://www.semanticscholar.org/paper/da07c4617624a0bb992e3981b80b2a7a263141db\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145478041\",\"name\":\"Shikhar Sharma\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b47402a9a68f23b548ae6e0349700ea651b7a373\",\"title\":\"Action Recognition and Video Description using Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/b47402a9a68f23b548ae6e0349700ea651b7a373\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1708.09666\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"}],\"doi\":\"10.1145/3078971.3079000\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"title\":\"Generating Video Descriptions with Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":\"1806.08612\",\"authors\":[{\"authorId\":\"2164604\",\"name\":\"Shervin Minaee\"},{\"authorId\":\"2469923\",\"name\":\"Imed Bouazizi\"},{\"authorId\":\"2429905\",\"name\":\"P. Kolan\"},{\"authorId\":\"2158224\",\"name\":\"H. Najafzadeh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4d9fbca21ea595f6d8294e54c1f2410ab67c267\",\"title\":\"Ad-Net: Audio-Visual Convolutional Neural Network for Advertisement Detection In Videos\",\"url\":\"https://www.semanticscholar.org/paper/e4d9fbca21ea595f6d8294e54c1f2410ab67c267\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49370397\",\"name\":\"D. Wang\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"}],\"doi\":\"10.1109/ICBK.2017.26\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d22c000c12aaedadcf075736dfc998dea932f06\",\"title\":\"Video Captioning with Semantic Information from the Knowledge Base\",\"url\":\"https://www.semanticscholar.org/paper/4d22c000c12aaedadcf075736dfc998dea932f06\",\"venue\":\"2017 IEEE International Conference on Big Knowledge (ICBK)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35384916\",\"name\":\"Y. Jiao\"},{\"authorId\":\"3208023\",\"name\":\"Zhetao Li\"},{\"authorId\":\"50178505\",\"name\":\"Shucheng Huang\"},{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"49166856\",\"name\":\"B. Liu\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"}],\"doi\":\"10.1109/TMM.2018.2815998\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a9fc8efd1aa3d58f89c0f53f0cb112725b5bda10\",\"title\":\"Three-Dimensional Attention-Based Deep Ranking Model for Video Highlight Detection\",\"url\":\"https://www.semanticscholar.org/paper/a9fc8efd1aa3d58f89c0f53f0cb112725b5bda10\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144713153\",\"name\":\"Dan Guo\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d44c20c48e764a546d00b9155a56b171b0dc04bc\",\"title\":\"Hierarchical LSTM for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/d44c20c48e764a546d00b9155a56b171b0dc04bc\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81693661\",\"name\":\"Tryambak Gangopadhyay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"13c4772cfa01defbae2073746bdbde9214037c5d\",\"title\":\"Deep learning for monitoring cyber-physical systems\",\"url\":\"https://www.semanticscholar.org/paper/13c4772cfa01defbae2073746bdbde9214037c5d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1907.11117\",\"authors\":[{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b73ff5846772da8575262925aa7709b5e64079a0\",\"title\":\"Learning Visual Actions Using Multiple Verb-Only Labels\",\"url\":\"https://www.semanticscholar.org/paper/b73ff5846772da8575262925aa7709b5e64079a0\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":\"1812.02501\",\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"144031869\",\"name\":\"A. Yao\"}],\"doi\":\"10.1109/ICCV.2019.00095\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"15a36f9639f608c4567302de65355543fdcee910\",\"title\":\"Zero-Shot Anticipation for Instructional Activities\",\"url\":\"https://www.semanticscholar.org/paper/15a36f9639f608c4567302de65355543fdcee910\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2011.07735\",\"authors\":[{\"authorId\":\"40016108\",\"name\":\"Aman Chadha\"},{\"authorId\":\"2025073690\",\"name\":\"Gurneet Arora\"},{\"authorId\":\"2025065763\",\"name\":\"Navpreet Kaloty\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"title\":\"iPerceive: Applying Common-Sense Reasoning to Multi-Modal Dense Video Captioning and Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/bcffc406b4cc5b179ed973cd7f974c656e129c4f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1812.01969\",\"authors\":[{\"authorId\":\"48018722\",\"name\":\"Jiri Fajtl\"},{\"authorId\":\"2022687\",\"name\":\"Hajar Sadeghi Sokeh\"},{\"authorId\":\"1689047\",\"name\":\"V. Argyriou\"},{\"authorId\":\"7158544\",\"name\":\"Dorothy Monekosso\"},{\"authorId\":\"1711669\",\"name\":\"Paolo Remagnino\"}],\"doi\":\"10.1007/978-3-030-21074-8_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb48b7b39d86cd5156cda58636088c702be10c50\",\"title\":\"Summarizing Videos with Attention\",\"url\":\"https://www.semanticscholar.org/paper/bb48b7b39d86cd5156cda58636088c702be10c50\",\"venue\":\"ACCV Workshops\",\"year\":2018},{\"arxivId\":\"1711.04161\",\"authors\":[{\"authorId\":\"1696573\",\"name\":\"Jiagang Zhu\"},{\"authorId\":\"9276071\",\"name\":\"Wei Zou\"},{\"authorId\":\"40031201\",\"name\":\"Z. Zhu\"},{\"authorId\":\"12791587\",\"name\":\"Lin Li\"}],\"doi\":\"10.1109/ICPR.2018.8545710\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c501a89092252a9f62f76a6f439916efe626251\",\"title\":\"End-to-end Video-level Representation Learning for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/8c501a89092252a9f62f76a6f439916efe626251\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"145723090\",\"name\":\"Z. Wang\"},{\"authorId\":\"143640801\",\"name\":\"S. Chen\"},{\"authorId\":\"143760554\",\"name\":\"Q. Wei\"}],\"doi\":\"10.1007/978-3-030-26075-0_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ac82935ce0768ae6c541405569be891663a3b27c\",\"title\":\"Discovering Attractive Segments in the User Generated Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/ac82935ce0768ae6c541405569be891663a3b27c\",\"venue\":\"APWeb/WAIM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jing Wang\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b8218640e95bb2d925a617b1c3012eed7d209351\",\"title\":\"Show, Reward and Tell: Automatic Generation of Narrative Paragraph From Photo Stream by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/b8218640e95bb2d925a617b1c3012eed7d209351\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1707.05740\",\"authors\":[{\"authorId\":\"40940512\",\"name\":\"Jun Liu\"},{\"authorId\":\"2096527\",\"name\":\"G. Wang\"},{\"authorId\":\"7667912\",\"name\":\"L. Duan\"},{\"authorId\":\"51283631\",\"name\":\"Kamila Abdiyeva\"},{\"authorId\":\"1711097\",\"name\":\"A. Kot\"}],\"doi\":\"10.1109/TIP.2017.2785279\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5202b026976b9057300eb30ee1804f8ae30a4c42\",\"title\":\"Skeleton-Based Human Action Recognition With Global Context-Aware Attention LSTM Networks\",\"url\":\"https://www.semanticscholar.org/paper/5202b026976b9057300eb30ee1804f8ae30a4c42\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66367478\",\"name\":\"D. Micha\\u00ebl\"}],\"doi\":\"10.6084/M9.FIGSHARE.4491686.V1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a7525add5a413e27563c2d4e137fdedaffbd354\",\"title\":\"Deep Learning on Graphs\",\"url\":\"https://www.semanticscholar.org/paper/8a7525add5a413e27563c2d4e137fdedaffbd354\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"1790251284\",\"name\":\"Lin Li\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"9594118\",\"name\":\"C. Gu\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/s11063-020-10352-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"title\":\"Adaptively Converting Auxiliary Attributes and Textual Embedding for Video Captioning Based on BiLSTM\",\"url\":\"https://www.semanticscholar.org/paper/3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"venue\":\"Neural Process. Lett.\",\"year\":2020},{\"arxivId\":\"1908.06616\",\"authors\":[{\"authorId\":\"71134752\",\"name\":\"H. Emami\"},{\"authorId\":\"94080087\",\"name\":\"M. Aliabadi\"},{\"authorId\":\"50330286\",\"name\":\"Ming Dong\"},{\"authorId\":\"2327639\",\"name\":\"R. Chinnam\"}],\"doi\":\"10.1109/TMM.2020.2975961\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e90dfb06f1bdffbe465f231d087a60027d79da05\",\"title\":\"SPA-GAN: Spatial Attention GAN for Image-to-Image Translation\",\"url\":\"https://www.semanticscholar.org/paper/e90dfb06f1bdffbe465f231d087a60027d79da05\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2021},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"8194130\",\"name\":\"Qinyu Li\"}],\"doi\":\"10.1145/3303083\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"91aa0eb38446643cd622b060a76043b0ca2d7991\",\"title\":\"Rich Visual and Language Representation with Complementary Semantics for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/91aa0eb38446643cd622b060a76043b0ca2d7991\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390818869\",\"name\":\"Jinlei Xu\"},{\"authorId\":\"144546140\",\"name\":\"T. Xu\"},{\"authorId\":\"123432231\",\"name\":\"Xin Tian\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144911521\",\"name\":\"Y. Ji\"}],\"doi\":\"10.1109/IJCNN.2019.8851897\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf7b38dd24c20223e006066be4202d1da700af37\",\"title\":\"Context Gating with Short Temporal Information for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf7b38dd24c20223e006066be4202d1da700af37\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32324177\",\"name\":\"C. Wu\"},{\"authorId\":\"19261873\",\"name\":\"Yiwei Wei\"},{\"authorId\":\"15862607\",\"name\":\"Xiaoliang Chu\"},{\"authorId\":\"2037988\",\"name\":\"Weichen Sun\"},{\"authorId\":\"144310030\",\"name\":\"F. Su\"},{\"authorId\":\"2250564\",\"name\":\"Leiquan Wang\"}],\"doi\":\"10.1016/j.neucom.2018.07.029\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fc4a590d1859ba43c1303927c86c64b34e43287\",\"title\":\"Hierarchical attention-based multimodal fusion for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4fc4a590d1859ba43c1303927c86c64b34e43287\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"}],\"doi\":\"10.1007/978-3-030-00764-5_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2036b394a5dff537df48d6db62a0d61491c92046\",\"title\":\"iMakeup: Makeup Instructional Video Dataset for Fine-Grained Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2036b394a5dff537df48d6db62a0d61491c92046\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74437294\",\"name\":\"Sk. Arif Ahmed\"},{\"authorId\":\"3320759\",\"name\":\"D. P. Dogra\"},{\"authorId\":\"32614479\",\"name\":\"S. Kar\"},{\"authorId\":\"40813600\",\"name\":\"P. Roy\"}],\"doi\":\"10.1007/978-981-10-7590-2_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"522b13ea02d6d62e54180bd13595eb0e40333d48\",\"title\":\"Natural Language Description of Surveillance Events\",\"url\":\"https://www.semanticscholar.org/paper/522b13ea02d6d62e54180bd13595eb0e40333d48\",\"venue\":\"ICITAM\",\"year\":2017},{\"arxivId\":\"1612.06950\",\"authors\":[{\"authorId\":\"48842639\",\"name\":\"Dotan Kaufman\"},{\"authorId\":\"36813724\",\"name\":\"Gil Levi\"},{\"authorId\":\"1756099\",\"name\":\"Tal Hassner\"},{\"authorId\":\"145128145\",\"name\":\"Lior Wolf\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aba537944ace733bbab2290cf2814cf7f4e4e275\",\"title\":\"Temporal Tessellation for Video Annotation and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/aba537944ace733bbab2290cf2814cf7f4e4e275\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2002.11886\",\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5dff9ae50c0aadbd99ca59ff70425f63213243e\",\"title\":\"Hierarchical Memory Decoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a5dff9ae50c0aadbd99ca59ff70425f63213243e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"153173208\",\"name\":\"J. Xu\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2019.11.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"title\":\"Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into LSTM-based model\",\"url\":\"https://www.semanticscholar.org/paper/2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"93762952\",\"name\":\"W. Li\"},{\"authorId\":\"1423419337\",\"name\":\"Jianhui Sun\"},{\"authorId\":\"48574046\",\"name\":\"Ge Liu\"},{\"authorId\":\"1657444300\",\"name\":\"Linglan Zhao\"},{\"authorId\":\"35680253\",\"name\":\"Xiangzhong Fang\"}],\"doi\":\"10.1016/j.patrec.2020.02.031\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"471e503f93c86b9c99e34d0f175b69f1db77f395\",\"title\":\"Visual question answering with attention transfer and a cross-modal gating mechanism\",\"url\":\"https://www.semanticscholar.org/paper/471e503f93c86b9c99e34d0f175b69f1db77f395\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145583891\",\"name\":\"Tuan-Hung Vu\"},{\"authorId\":\"17132791\",\"name\":\"W. Choi\"},{\"authorId\":\"1790643\",\"name\":\"S. Schulter\"},{\"authorId\":\"2099305\",\"name\":\"Manmohan Chandraker\"}],\"doi\":\"10.1109/WACV.2019.00128\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db70e64fb69c64f174fd97ab86566373504fd702\",\"title\":\"Memory Warps for Long-Term Online Video Representations and Anticipation\",\"url\":\"https://www.semanticscholar.org/paper/db70e64fb69c64f174fd97ab86566373504fd702\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"3076466\",\"name\":\"X. Liu\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":\"10.1007/978-3-030-00776-8_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"title\":\"Video Captioning Based on the Spatial-Temporal Saliency Tracing\",\"url\":\"https://www.semanticscholar.org/paper/ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"},{\"authorId\":\"116409536\",\"name\":\"A. Gray\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1401154472\",\"name\":\"Emily Tucker Prud'hommeaux\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/WACV.2017.115\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c9aea006e80cf215b72693860c3234b61006c911\",\"title\":\"Semantic Text Summarization of Long Videos\",\"url\":\"https://www.semanticscholar.org/paper/c9aea006e80cf215b72693860c3234b61006c911\",\"venue\":\"2017 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89779095\",\"name\":\"Bimal Bhattarai\"},{\"authorId\":\"144570483\",\"name\":\"Rohan Kumar Yadav\"},{\"authorId\":\"2773317\",\"name\":\"Hui-Seon Gang\"},{\"authorId\":\"1739676\",\"name\":\"J. Pyun\"}],\"doi\":\"10.1109/ACCESS.2019.2902573\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"40cf6d7213cc1d03b78f27924f1f9970442f97c9\",\"title\":\"Geomagnetic Field Based Indoor Landmark Classification Using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/40cf6d7213cc1d03b78f27924f1f9970442f97c9\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1605.04129\",\"authors\":[{\"authorId\":\"2084534\",\"name\":\"Maedeh Aghaei\"},{\"authorId\":\"2837527\",\"name\":\"Mariella Dimiccoli\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1109/ICPR.2016.7900087\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"1564bf0a268662df752b68bee5addc4b08868739\",\"title\":\"With whom do I interact? Detecting social interactions in egocentric photo-streams\",\"url\":\"https://www.semanticscholar.org/paper/1564bf0a268662df752b68bee5addc4b08868739\",\"venue\":\"2016 23rd International Conference on Pattern Recognition (ICPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145676753\",\"name\":\"Z. Zhou\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":null,\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1109/CVPR.2017.717\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f1df5f74477658df45814743f2e605656e8ac16e\",\"title\":\"See the Forest for the Trees: Joint Spatial and Temporal Recurrent Neural Networks for Video-Based Person Re-identification\",\"url\":\"https://www.semanticscholar.org/paper/f1df5f74477658df45814743f2e605656e8ac16e\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1904.03870\",\"authors\":[{\"authorId\":\"8511875\",\"name\":\"Jonghwan Mun\"},{\"authorId\":\"2889075\",\"name\":\"L. Yang\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"40030651\",\"name\":\"B. Han\"}],\"doi\":\"10.1109/CVPR.2019.00675\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c5a757427132fda0c66e18a0d059eca8e2472d13\",\"title\":\"Streamlined Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c5a757427132fda0c66e18a0d059eca8e2472d13\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1807.10018\",\"authors\":[{\"authorId\":\"51152390\",\"name\":\"Yilei Xiong\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1007/978-3-030-01252-6_29\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"title\":\"Move Forward and Tell: A Progressive Generator of Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b74a094b6e35fab07e1a4694afd12cad9696f1c1\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"48624462\",\"name\":\"W. Li\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1109/TMM.2018.2839534\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"602412f61e8902052e6489e84a6f24ccc7407814\",\"title\":\"Fully Convolutional Network for Multiscale Temporal Action Proposals\",\"url\":\"https://www.semanticscholar.org/paper/602412f61e8902052e6489e84a6f24ccc7407814\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5434752\",\"name\":\"Y. Jiang\"}],\"doi\":\"10.5120/IJCA2019918660\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"43c22ebb5ff264a5ea996c163464cf761035a405\",\"title\":\"Multi-Feature Fusion for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/43c22ebb5ff264a5ea996c163464cf761035a405\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2721485\",\"name\":\"Shuohao Li\"},{\"authorId\":\"144245551\",\"name\":\"M. Tang\"},{\"authorId\":\"50561313\",\"name\":\"J. Zhang\"}],\"doi\":\"10.1117/1.JEI.27.2.023027\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"title\":\"Deep hierarchical attention network for video description\",\"url\":\"https://www.semanticscholar.org/paper/8caa685c2f78ef4a5694b9ca1992c0cebcc52447\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"}],\"doi\":\"10.21437/interspeech.2019-3143\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f78c136471778771c29fb385d3a8c1a1def28de1\",\"title\":\"Joint Student-Teacher Learning for Audio-Visual Scene-Aware Dialog\",\"url\":\"https://www.semanticscholar.org/paper/f78c136471778771c29fb385d3a8c1a1def28de1\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"144478646\",\"name\":\"Z. Guo\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TMM.2017.2729019\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"51b2c1e750b1d3b893072829d012f2352d6bd373\",\"title\":\"Video Captioning With Attention-Based LSTM and Semantic Consistency\",\"url\":\"https://www.semanticscholar.org/paper/51b2c1e750b1d3b893072829d012f2352d6bd373\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":\"1507.01053\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.1109/TMM.2015.2477044\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"654a3e53fb41d8168798ee0ee61dfab73739b1ed\",\"title\":\"Describing Multimedia Content Using Attention-Based Encoder-Decoder Networks\",\"url\":\"https://www.semanticscholar.org/paper/654a3e53fb41d8168798ee0ee61dfab73739b1ed\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144478646\",\"name\":\"Z. Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"47158869\",\"name\":\"Xing Xu\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/2964284.2967242\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"90c4a6c6f790dbcef9a29c9a755458be09e319b6\",\"title\":\"Attention-based LSTM with Semantic Consistency for Videos Captioning\",\"url\":\"https://www.semanticscholar.org/paper/90c4a6c6f790dbcef9a29c9a755458be09e319b6\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":\"2012.06567\",\"authors\":[{\"authorId\":\"1805946041\",\"name\":\"Yi Zhu\"},{\"authorId\":\"21657733\",\"name\":\"X. Li\"},{\"authorId\":\"49046944\",\"name\":\"C. Liu\"},{\"authorId\":\"2890820\",\"name\":\"Mohammadreza Zolfaghari\"},{\"authorId\":\"3331521\",\"name\":\"Yuanjun Xiong\"},{\"authorId\":\"22539483\",\"name\":\"Chongruo Wu\"},{\"authorId\":\"152781163\",\"name\":\"Z. Zhang\"},{\"authorId\":\"2397422\",\"name\":\"Joseph Tighe\"},{\"authorId\":\"1758550\",\"name\":\"R. Manmatha\"},{\"authorId\":\"49140510\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"title\":\"A Comprehensive Study of Deep Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ee12f02830a5e1f7cadd3623dfed495cf099bc82\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1901.05635\",\"authors\":[{\"authorId\":\"145071472\",\"name\":\"X. Tu\"},{\"authorId\":\"74182593\",\"name\":\"Hengsheng Zhang\"},{\"authorId\":\"144917416\",\"name\":\"M. Xie\"},{\"authorId\":\"144861057\",\"name\":\"Y. Luo\"},{\"authorId\":\"46867875\",\"name\":\"Yuefei Zhang\"},{\"authorId\":\"80398011\",\"name\":\"Z. Ma\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cf480433046f698987e612e5a532a39f9ad0e64\",\"title\":\"Enhance the Motion Cues for Face Anti-Spoofing using CNN-LSTM Architecture\",\"url\":\"https://www.semanticscholar.org/paper/0cf480433046f698987e612e5a532a39f9ad0e64\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1601.07754\",\"authors\":[{\"authorId\":\"50124502\",\"name\":\"Anna Podlesnaya\"},{\"authorId\":\"1698825\",\"name\":\"Sergey Podlesnyy\"}],\"doi\":\"10.1007/978-3-319-56991-8_27\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a93c2265dacf3afaa02b9e0aefe00d5f0d9e2c05\",\"title\":\"Deep Learning Based Semantic Video Indexing and Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/a93c2265dacf3afaa02b9e0aefe00d5f0d9e2c05\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1611.08240\",\"authors\":[{\"authorId\":\"24899770\",\"name\":\"Amlan Kar\"},{\"authorId\":\"145193060\",\"name\":\"N. Rai\"},{\"authorId\":\"39707211\",\"name\":\"Karan Sikka\"},{\"authorId\":\"144054467\",\"name\":\"G. Sharma\"}],\"doi\":\"10.1109/CVPR.2017.604\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"574ad7ef015995efb7338829a021776bf9daaa08\",\"title\":\"AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for Human Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/574ad7ef015995efb7338829a021776bf9daaa08\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1710.10501\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"27395113\",\"name\":\"Eric Poblenz\"},{\"authorId\":\"69415676\",\"name\":\"Dmitry Dagunts\"},{\"authorId\":\"40044677\",\"name\":\"Ben Covington\"},{\"authorId\":\"153204619\",\"name\":\"Devon Bernard\"},{\"authorId\":\"40503044\",\"name\":\"Kevin Lyman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5c5e022d7b14f3690992d48514fdb4cb19825ad\",\"title\":\"Learning to diagnose from scratch by exploiting dependencies among labels\",\"url\":\"https://www.semanticscholar.org/paper/e5c5e022d7b14f3690992d48514fdb4cb19825ad\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1805.05622\",\"authors\":[{\"authorId\":\"115003962\",\"name\":\"Marko Smilevski\"},{\"authorId\":\"117355926\",\"name\":\"Ilija Lalkovski\"},{\"authorId\":\"145798745\",\"name\":\"Gjorgji Madjarov\"}],\"doi\":\"10.1007/978-3-030-00825-3_13\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"07edbcc0acd09abd81d0353cdb48d137ce498f24\",\"title\":\"Stories for Images-in-Sequence by using Visual and Narrative Components\",\"url\":\"https://www.semanticscholar.org/paper/07edbcc0acd09abd81d0353cdb48d137ce498f24\",\"venue\":\"ICT Innovations\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"41c853eebc563cc6dde2df32fbaca816ce0bab3e\",\"title\":\"Perceptual Prediction Error Detection Learning Signal Learning Signal\",\"url\":\"https://www.semanticscholar.org/paper/41c853eebc563cc6dde2df32fbaca816ce0bab3e\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"104510214\",\"name\":\"K. Li\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TETCI.2019.2892755\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"title\":\"Visual to Text: Survey of Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49299019\",\"name\":\"Junnan Li\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47521917\",\"name\":\"Q. Zhao\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e93c977025e2829f852fc8c1e8f9547c3588dbf0\",\"title\":\"vv 1 camping tent food fire Residual BRNN Input Video Visual Encoder ( CNN ) Video Encoder Sentence Encoder Word 2 Vecs Sentence Semantic Embedding vv 2 vv 3 vvNN \\u2212 1 vvNN vv Video Semantic Embedding xx\",\"url\":\"https://www.semanticscholar.org/paper/e93c977025e2829f852fc8c1e8f9547c3588dbf0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/WACV.2019.00026\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"183bf77d4f9b4eb227ba1d5a26eff5b6ab3d889d\",\"title\":\"Going Deeper With Semantics: Video Activity Interpretation Using Semantic Contextualization\",\"url\":\"https://www.semanticscholar.org/paper/183bf77d4f9b4eb227ba1d5a26eff5b6ab3d889d\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144118452\",\"name\":\"Pengfei Liu\"},{\"authorId\":\"1704421\",\"name\":\"Hongjian Li\"},{\"authorId\":\"145734991\",\"name\":\"Shuai Li\"},{\"authorId\":\"144446033\",\"name\":\"Kwong-Sak Leung\"}],\"doi\":\"10.1186/s12859-019-2910-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38721dc91fde4dce0d4d606f6bb9e4145bf56eff\",\"title\":\"Improving prediction of phenotypic drug response on cancer cell lines using deep convolutional network\",\"url\":\"https://www.semanticscholar.org/paper/38721dc91fde4dce0d4d606f6bb9e4145bf56eff\",\"venue\":\"BMC Bioinformatics\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9173291\",\"name\":\"L. Wang\"},{\"authorId\":\"14800230\",\"name\":\"J. Zang\"},{\"authorId\":\"46324995\",\"name\":\"Q. Zhang\"},{\"authorId\":\"1786361\",\"name\":\"Zhenxing Niu\"},{\"authorId\":\"144988571\",\"name\":\"Gang Hua\"},{\"authorId\":\"145608731\",\"name\":\"N. Zheng\"}],\"doi\":\"10.3390/s18071979\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2588acc7a730d864f84d4e1a050070ff873b03d5\",\"title\":\"Action Recognition by an Attention-Aware Temporal Weighted Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/2588acc7a730d864f84d4e1a050070ff873b03d5\",\"venue\":\"Sensors\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"}],\"doi\":\"10.1007/s11063-019-10030-y\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed3215857d14557d0afe517b4d28b0e98b384f4b\",\"title\":\"Refocused Attention: Long Short-Term Rewards Guided Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ed3215857d14557d0afe517b4d28b0e98b384f4b\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de228875bc33e9db85123469ef80fc0071a92386\",\"title\":\"Word2VisualVec: Image and Video to Sentence Matching by Visual Feature Prediction\",\"url\":\"https://www.semanticscholar.org/paper/de228875bc33e9db85123469ef80fc0071a92386\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"1791344388\",\"name\":\"Lei Ji\"},{\"authorId\":\"1783553\",\"name\":\"Zhen-dong Niu\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1710220\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/3394171.3413498\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"title\":\"Learning Semantic Concepts and Temporal Alignment for Narrated Video Procedural Captioning\",\"url\":\"https://www.semanticscholar.org/paper/de4eabc5a672e5c3c1b3acbfa724cd8c85169c8c\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1611.05592\",\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":null,\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0c687986256ce206c93fb78303565bacffb09efe\",\"title\":\"Multimodal Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0c687986256ce206c93fb78303565bacffb09efe\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"2004.01398\",\"authors\":[{\"authorId\":\"48513712\",\"name\":\"Y. Li\"},{\"authorId\":\"102880425\",\"name\":\"Bin Ji\"},{\"authorId\":\"48203223\",\"name\":\"Xintian Shi\"},{\"authorId\":\"98697812\",\"name\":\"J. Zhang\"},{\"authorId\":\"48418655\",\"name\":\"Bin Kang\"},{\"authorId\":\"48170350\",\"name\":\"Limin Wang\"}],\"doi\":\"10.1109/cvpr42600.2020.00099\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"1ebeb84e2b8e1182a2b4821c906200ecc49ae187\",\"title\":\"TEA: Temporal Excitation and Aggregation for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/1ebeb84e2b8e1182a2b4821c906200ecc49ae187\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1511.05234\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1007/978-3-319-46478-7_28\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1cf6bc0866226c1f8e282463adc8b75d92fba9bb\",\"title\":\"Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1cf6bc0866226c1f8e282463adc8b75d92fba9bb\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41030694\",\"name\":\"Huanyu Yu\"},{\"authorId\":\"3392007\",\"name\":\"Shuo Cheng\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"7272302\",\"name\":\"Minsi Wang\"},{\"authorId\":\"40430880\",\"name\":\"J. Zhang\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"}],\"doi\":\"10.1109/CVPR.2018.00629\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f5876f67129a80a1ee753f715efcd2e2109bf432\",\"title\":\"Fine-Grained Video Captioning for Sports Narrative\",\"url\":\"https://www.semanticscholar.org/paper/f5876f67129a80a1ee753f715efcd2e2109bf432\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145098331\",\"name\":\"S. Han\"},{\"authorId\":\"92134816\",\"name\":\"Bo-Won Go\"},{\"authorId\":\"145530103\",\"name\":\"H. Choi\"}],\"doi\":\"10.1109/BIGCOMP.2019.8679213\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"eadde672d900b7cc3949a854d2b7b850ab2e9c5c\",\"title\":\"Multiple Videos Captioning Model for Video Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/eadde672d900b7cc3949a854d2b7b850ab2e9c5c\",\"venue\":\"2019 IEEE International Conference on Big Data and Smart Computing (BigComp)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143623051\",\"name\":\"K. Ahmad\"},{\"authorId\":\"3058987\",\"name\":\"N. Conci\"}],\"doi\":\"10.1145/3306240\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e231537cb0680136b9f4c2dd3124f83ce0792780\",\"title\":\"How Deep Features Have Improved Event Recognition in Multimedia\",\"url\":\"https://www.semanticscholar.org/paper/e231537cb0680136b9f4c2dd3124f83ce0792780\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"title\":\"Incorporating Semantic Attention in Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/8972dc1f5e59042c6ce111dc7591e8b5eed9737d\",\"venue\":\"LREC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"3271933\",\"name\":\"M. Douze\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"},{\"authorId\":\"1681054\",\"name\":\"H. J\\u00e9gou\"}],\"doi\":\"10.1109/CVPR.2018.00814\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f11acabdc1aa9fb8917431268f85746b88d88c32\",\"title\":\"LAMV: Learning to Align and Match Videos with Kernelized Temporal Layers\",\"url\":\"https://www.semanticscholar.org/paper/f11acabdc1aa9fb8917431268f85746b88d88c32\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48631492\",\"name\":\"X. Wang\"},{\"authorId\":\"2505291\",\"name\":\"Siavash Hosseinyalamdary\"}],\"doi\":\"10.5194/ISPRS-ARCHIVES-XLII-2-W13-127-2019\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f23249af5289b0aa299f98cd48bf86582e8d486a\",\"title\":\"HUMAN DETECTION BASED ON A SEQUENCE OF THERMAL IMAGES USING DEEP LEARNING\",\"url\":\"https://www.semanticscholar.org/paper/f23249af5289b0aa299f98cd48bf86582e8d486a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"144016781\",\"name\":\"Heng Ji\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"},{\"authorId\":\"1817166\",\"name\":\"Clare R. Voss\"}],\"doi\":\"10.18653/v1/D18-1433\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c7de1e95e7f130fcbab0dea763869ff2244523e8\",\"title\":\"Incorporating Background Knowledge into Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/c7de1e95e7f130fcbab0dea763869ff2244523e8\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1905.05142\",\"authors\":[{\"authorId\":\"5228508\",\"name\":\"Yujing Chen\"},{\"authorId\":\"152781698\",\"name\":\"Y. Ning\"},{\"authorId\":\"49835358\",\"name\":\"Z. Chai\"},{\"authorId\":\"145344187\",\"name\":\"H. Rangwala\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed4365e559b1ccb6d8bc9372623e9840433b0629\",\"title\":\"Federated Multi-task Hierarchical Attention Model for Sensor Analytics\",\"url\":\"https://www.semanticscholar.org/paper/ed4365e559b1ccb6d8bc9372623e9840433b0629\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1903.00110\",\"authors\":[{\"authorId\":\"46419391\",\"name\":\"M. Elfeki\"},{\"authorId\":\"3177797\",\"name\":\"A. Borji\"}],\"doi\":\"10.1109/WACV.2019.00085\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be447eedf6c50096cc6a85b47ae7afa203c511b6\",\"title\":\"Video Summarization Via Actionness Ranking\",\"url\":\"https://www.semanticscholar.org/paper/be447eedf6c50096cc6a85b47ae7afa203c511b6\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"12257085\",\"name\":\"Pang-Chi Huang\"},{\"authorId\":\"3596543\",\"name\":\"Chi-Yu Yang\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ASRU.2017.8268961\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"title\":\"Seeing and hearing too: Audio representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"36251013\",\"name\":\"Wei Li\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1007/s11063-017-9591-9\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"37eaf94fa6619ee857019937677cb055a2a51bf3\",\"title\":\"Capturing Temporal Structures for Video Captioning by Spatio-temporal Contexts and Channel Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/37eaf94fa6619ee857019937677cb055a2a51bf3\",\"venue\":\"Neural Processing Letters\",\"year\":2017},{\"arxivId\":\"2008.10966\",\"authors\":[{\"authorId\":\"2548303\",\"name\":\"Lijie Fan\"},{\"authorId\":\"47268124\",\"name\":\"T. Li\"},{\"authorId\":\"46499812\",\"name\":\"Yuan Yuan\"},{\"authorId\":\"1785714\",\"name\":\"D. Katabi\"}],\"doi\":\"10.1007/978-3-030-58536-5_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4\",\"title\":\"In-Home Daily-Life Captioning Using Radio Signals\",\"url\":\"https://www.semanticscholar.org/paper/fadd6e5a8e877884dccb7ca5c8167f32f65ec5c4\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1603.00391\",\"authors\":[{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3009779\",\"name\":\"M. Moczulski\"},{\"authorId\":\"1715051\",\"name\":\"Misha Denil\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"568374ac9433e29b812008b2a01f81e657bdbd34\",\"title\":\"Noisy Activation Functions\",\"url\":\"https://www.semanticscholar.org/paper/568374ac9433e29b812008b2a01f81e657bdbd34\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1016/j.cviu.2017.04.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"title\":\"Hierarchical & multimodal video captioning: Discovering and transferring multimodal knowledge for vision to language\",\"url\":\"https://www.semanticscholar.org/paper/96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1709.03946\",\"authors\":[{\"authorId\":\"19304470\",\"name\":\"Nikhita Vedula\"},{\"authorId\":\"49935089\",\"name\":\"W. Sun\"},{\"authorId\":\"8858199\",\"name\":\"H. Lee\"},{\"authorId\":\"47442671\",\"name\":\"H. Gupta\"},{\"authorId\":\"144669577\",\"name\":\"M. Ogihara\"},{\"authorId\":\"145026873\",\"name\":\"Joseph Johnson\"},{\"authorId\":\"144718269\",\"name\":\"Gang Ren\"},{\"authorId\":\"145022640\",\"name\":\"S. Parthasarathy\"}],\"doi\":\"10.1109/ICDM.2017.149\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c3ecab62a6e3b372618d7e231561621fda8c6d1\",\"title\":\"Multimodal Content Analysis for Effective Advertisements on YouTube\",\"url\":\"https://www.semanticscholar.org/paper/6c3ecab62a6e3b372618d7e231561621fda8c6d1\",\"venue\":\"2017 IEEE International Conference on Data Mining (ICDM)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3471544\",\"name\":\"Feixiang He\"},{\"authorId\":\"2254178\",\"name\":\"Fayao Liu\"},{\"authorId\":\"145786594\",\"name\":\"Rui Yao\"},{\"authorId\":\"2604251\",\"name\":\"Guosheng Lin\"}],\"doi\":\"10.1016/J.IMAVIS.2018.12.002\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"17eaac80a260bc5d665e1035d7291293b056d3b6\",\"title\":\"Local fusion networks with chained residual pooling for video action recognition\",\"url\":\"https://www.semanticscholar.org/paper/17eaac80a260bc5d665e1035d7291293b056d3b6\",\"venue\":\"Image Vis. Comput.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1813915\",\"name\":\"S. Liu\"},{\"authorId\":\"145888238\",\"name\":\"Zhou Ren\"},{\"authorId\":\"34316743\",\"name\":\"Junsong Yuan\"}],\"doi\":\"10.1145/3240508.3240667\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"849642b4701ac11c035326069f707f23a51a6f1a\",\"title\":\"SibNet: Sibling Convolutional Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/849642b4701ac11c035326069f707f23a51a6f1a\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2474219\",\"name\":\"Nikolaos Karianakis\"},{\"authorId\":\"1691128\",\"name\":\"Zicheng Liu\"},{\"authorId\":\"2249952\",\"name\":\"Y. Chen\"},{\"authorId\":\"1715959\",\"name\":\"Stefano Soatto\"}],\"doi\":\"10.1007/978-3-030-01228-1_44\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"536210d839dd439b14637354e23aabd7689afae0\",\"title\":\"Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-identification\",\"url\":\"https://www.semanticscholar.org/paper/536210d839dd439b14637354e23aabd7689afae0\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1612.02095\",\"authors\":[{\"authorId\":\"3159503\",\"name\":\"Evan Racah\"},{\"authorId\":\"12757989\",\"name\":\"C. Beckham\"},{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"1764912\",\"name\":\"Prabhat\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3faebe9d5c47fc90998811c4ac768706283d605c\",\"title\":\"Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets\",\"url\":\"https://www.semanticscholar.org/paper/3faebe9d5c47fc90998811c4ac768706283d605c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51033208\",\"name\":\"B. Liu\"},{\"authorId\":\"34149749\",\"name\":\"Serena Yeung\"},{\"authorId\":\"34613203\",\"name\":\"Edward Chou\"},{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1007/978-3-030-01219-9_34\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73d1b35cd28befe845fcb60a3fed67c9fb7793ad\",\"title\":\"Temporal Modular Networks for Retrieving Complex Compositional Activities in Videos\",\"url\":\"https://www.semanticscholar.org/paper/73d1b35cd28befe845fcb60a3fed67c9fb7793ad\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":\"1809.07257\",\"authors\":[{\"authorId\":\"2662002\",\"name\":\"Oliver Nina\"},{\"authorId\":\"47238599\",\"name\":\"W. Garcia\"},{\"authorId\":\"47637016\",\"name\":\"Scott Clouse\"},{\"authorId\":\"1858702\",\"name\":\"A. Yilmaz\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"title\":\"MTLE: A Multitask Learning Encoder of Visual Feature Representations for Video and Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/81e31899aa9f0f54db069f0f4c2a29ed9587fe89\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2411436\",\"name\":\"Martin Klinkigt\"},{\"authorId\":\"1802416\",\"name\":\"D. Le\"},{\"authorId\":\"2687076\",\"name\":\"A. Hiroike\"},{\"authorId\":\"3417350\",\"name\":\"Hung Quoc Vo\"},{\"authorId\":\"1564010116\",\"name\":\"Mohit Chabra\"},{\"authorId\":\"1471395364\",\"name\":\"Vu-Minh-Hieu Dang\"},{\"authorId\":\"1557386872\",\"name\":\"Quan Kong\"},{\"authorId\":\"34453615\",\"name\":\"V. Nguyen\"},{\"authorId\":\"2668511\",\"name\":\"T. Murakami\"},{\"authorId\":\"1561042963\",\"name\":\"Tien-Van Do\"},{\"authorId\":\"32710145\",\"name\":\"Tomoaki Yoshinaga\"},{\"authorId\":\"1560660589\",\"name\":\"Duy-Nhat Nguyen\"},{\"authorId\":\"1564000905\",\"name\":\"Sinha Saptarshi\"},{\"authorId\":\"3080041\",\"name\":\"Thanh Duc Ngo\"},{\"authorId\":\"1563939623\",\"name\":\"Charles Limasanches\"},{\"authorId\":\"48447688\",\"name\":\"Tushar Agrawal\"},{\"authorId\":\"66535001\",\"name\":\"J. Vora\"},{\"authorId\":\"147577950\",\"name\":\"Manikandan Ravikiran\"},{\"authorId\":null,\"name\":\"Zheng Wang\"},{\"authorId\":\"144404414\",\"name\":\"S. Satoh\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b452ac0c1f6f189cbdeaac48d1d89621a3560f63\",\"title\":\"NII-HITACHI-UIT at TRECVID 2017\",\"url\":\"https://www.semanticscholar.org/paper/b452ac0c1f6f189cbdeaac48d1d89621a3560f63\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":\"1705.01861\",\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2017.472\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8dea2081609d838dc5b5a929f2ac19ce0c41c9a0\",\"title\":\"Action Tubelet Detector for Spatio-Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/8dea2081609d838dc5b5a929f2ac19ce0c41c9a0\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/LRA.2018.2793345\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"title\":\"Full-GRU Natural Language Video Description for Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46740175\",\"name\":\"G. Loganathan\"},{\"authorId\":\"1804589\",\"name\":\"J. Samarabandu\"},{\"authorId\":\"1723265\",\"name\":\"X. Wang\"}],\"doi\":\"10.1109/CCECE.2018.8447597\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"00823d787be96b077e14730d610294e9851fbb61\",\"title\":\"Sequence to Sequence Pattern Learning Algorithm for Real-Time Anomaly Detection in Network Traffic\",\"url\":\"https://www.semanticscholar.org/paper/00823d787be96b077e14730d610294e9851fbb61\",\"venue\":\"2018 IEEE Canadian Conference on Electrical & Computer Engineering (CCECE)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102320256\",\"name\":\"Hui Yang\"},{\"authorId\":\"145223726\",\"name\":\"Penghai Wu\"},{\"authorId\":\"5407696\",\"name\":\"Xuedong Yao\"},{\"authorId\":\"30980329\",\"name\":\"Yanlan Wu\"},{\"authorId\":\"39467013\",\"name\":\"B. Wang\"},{\"authorId\":\"1837647\",\"name\":\"Yongyang Xu\"}],\"doi\":\"10.3390/rs10111768\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ec0e33ef969164f6f3a81aa65aa0e78c82447a8\",\"title\":\"Building Extraction in Very High Resolution Imagery by Dense-Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/2ec0e33ef969164f6f3a81aa65aa0e78c82447a8\",\"venue\":\"Remote. Sens.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46725125\",\"name\":\"Marc ten Bosch\"},{\"authorId\":\"51247661\",\"name\":\"Christopher S. Gifford\"},{\"authorId\":\"29961709\",\"name\":\"Agata Ciesielski\"},{\"authorId\":\"71918324\",\"name\":\"Scott Almes\"},{\"authorId\":\"152601933\",\"name\":\"Rachel Ellison\"},{\"authorId\":\"50005563\",\"name\":\"Gordon Christie\"}],\"doi\":\"10.1117/12.2518163\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03413947c70d65de0f5352f9f7911c2d01409f81\",\"title\":\"Captioning of full motion video from unmanned aerial platforms\",\"url\":\"https://www.semanticscholar.org/paper/03413947c70d65de0f5352f9f7911c2d01409f81\",\"venue\":\"Defense + Commercial Sensing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"49991208\",\"name\":\"Jimeng Sun\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c631bb439284f6a5a90608b715fa631d5c5807e4\",\"title\":\"EEGtoText: Learning to Write Medical Reports from EEG Recordings\",\"url\":\"https://www.semanticscholar.org/paper/c631bb439284f6a5a90608b715fa631d5c5807e4\",\"venue\":\"MLHC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"151469538\",\"name\":\"Shengeng Tang\"},{\"authorId\":\"73160450\",\"name\":\"Meng Wang\"}],\"doi\":\"10.24963/ijcai.2019/106\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dba3e83d294e9981d4b1fa086e1ff98403ff2704\",\"title\":\"Connectionist Temporal Modeling of Video and Language: a Joint Model for Translation and Sign Labeling\",\"url\":\"https://www.semanticscholar.org/paper/dba3e83d294e9981d4b1fa086e1ff98403ff2704\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1804.05113\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"2856622\",\"name\":\"Bryan A. Plummer\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1609/AAAI.V33I01.33019062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"title\":\"Multilevel Language and Vision Integration for Text-to-Clip Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/83b2a55aecd5f917dbedbc0c5ef3ff3b61013958\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47739808\",\"name\":\"Junkun Chen\"},{\"authorId\":\"1767521\",\"name\":\"Xipeng Qiu\"},{\"authorId\":\"144118452\",\"name\":\"Pengfei Liu\"},{\"authorId\":\"144052385\",\"name\":\"X. Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3468740e4a9fc72a269f4f0ca8470ccd60925f92\",\"title\":\"2019 Formatting Instructions for Authors Using LaTeX\",\"url\":\"https://www.semanticscholar.org/paper/3468740e4a9fc72a269f4f0ca8470ccd60925f92\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50846763\",\"name\":\"W. Zhang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"48696362\",\"name\":\"Yanpeng Cao\"},{\"authorId\":\"145974114\",\"name\":\"Jun Xiao\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"144894845\",\"name\":\"Fei Wu\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"}],\"doi\":\"10.1145/3394171.3413745\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a95d2cad9fd439831d1d0c05d6bf7d1731dcefe8\",\"title\":\"Photo Stream Question Answer\",\"url\":\"https://www.semanticscholar.org/paper/a95d2cad9fd439831d1d0c05d6bf7d1731dcefe8\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50610439\",\"name\":\"Songtao Ding\"},{\"authorId\":\"2641875\",\"name\":\"Shiru Qu\"},{\"authorId\":\"6962569\",\"name\":\"Yuling Xi\"},{\"authorId\":\"49725227\",\"name\":\"Shaohua Wan\"}],\"doi\":\"10.1016/J.NEUCOM.2019.04.095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"088d11c03ac72c6d2a85dea758b283a09d4e519f\",\"title\":\"Stimulus-driven and concept-driven analysis for image caption generation\",\"url\":\"https://www.semanticscholar.org/paper/088d11c03ac72c6d2a85dea758b283a09d4e519f\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15287636\",\"name\":\"K. Sun\"},{\"authorId\":\"50703807\",\"name\":\"L. Li\"},{\"authorId\":\"12791664\",\"name\":\"Lianqiang Li\"},{\"authorId\":\"51511262\",\"name\":\"Ningyu He\"},{\"authorId\":\"50820964\",\"name\":\"Jie Zhu\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054641\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df8b5676d62e0114ae2a804894c33b8e3d8ab3bf\",\"title\":\"Spatial Attentional Bilinear 3D Convolutional Network for Video-Based Autism Spectrum Disorder Detection\",\"url\":\"https://www.semanticscholar.org/paper/df8b5676d62e0114ae2a804894c33b8e3d8ab3bf\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134649559\",\"name\":\"R. Kiziltepe\"},{\"authorId\":\"3000774\",\"name\":\"J. Gan\"}],\"doi\":\"10.1007/978-3-030-62362-3_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e543b9710b3dba5d9f77b8f863067bd26422ab6\",\"title\":\"Simple Effective Methods for Decision-Level Fusion in Two-Stream Convolutional Neural Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/1e543b9710b3dba5d9f77b8f863067bd26422ab6\",\"venue\":\"IDEAL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153446767\",\"name\":\"Qinkun Xiao\"},{\"authorId\":\"143742875\",\"name\":\"Xin Chang\"},{\"authorId\":\"97644075\",\"name\":\"X. Zhang\"},{\"authorId\":\"97713335\",\"name\":\"X. Liu\"}],\"doi\":\"10.1109/ACCESS.2020.3039539\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7d120180a56558b530848cdc6ead3c914e0e3a35\",\"title\":\"Multi-Information Spatial\\u2013Temporal LSTM Fusion Continuous Sign Language Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/7d120180a56558b530848cdc6ead3c914e0e3a35\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145223726\",\"name\":\"Penghai Wu\"},{\"authorId\":\"1701087\",\"name\":\"Zhixiang Yin\"},{\"authorId\":\"102320256\",\"name\":\"Hui Yang\"},{\"authorId\":\"30980329\",\"name\":\"Yanlan Wu\"},{\"authorId\":\"50088200\",\"name\":\"Xiaoshuang Ma\"}],\"doi\":\"10.3390/rs11030300\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bac6d11f5a54b66e086ed3842a2cc5b7b8b6ff03\",\"title\":\"Reconstructing Geostationary Satellite Land Surface Temperature Imagery Based on a Multiscale Feature Connected Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/bac6d11f5a54b66e086ed3842a2cc5b7b8b6ff03\",\"venue\":\"Remote. Sens.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mehrdad Hosseinzadeh\"},{\"authorId\":null,\"name\":\"Yang Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"title\":\"Video Captioning of Future Frames\",\"url\":\"https://www.semanticscholar.org/paper/da8a8a0ce5a15d072c85e1bace61e28701547c12\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1905.01077\",\"authors\":[{\"authorId\":\"50763020\",\"name\":\"Jingwen Chen\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1609/aaai.v33i01.33018167\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d40892479541c2d173c836534e6fb2acb597de49\",\"title\":\"Temporal Deformable Convolutional Encoder-Decoder Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d40892479541c2d173c836534e6fb2acb597de49\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1811.02765\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"145979995\",\"name\":\"D. Zhang\"},{\"authorId\":\"1758652\",\"name\":\"Yu Su\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33018965\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"title\":\"Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/aeb1fe15261f0ee10a27d1753fb301b7a044933a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144558300\",\"name\":\"J. Tian\"},{\"authorId\":\"49672743\",\"name\":\"C. Li\"},{\"authorId\":\"2558130\",\"name\":\"Z. Shi\"},{\"authorId\":\"2724114\",\"name\":\"F. Xu\"}],\"doi\":\"10.1007/978-3-030-00934-2_78\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2b8a479c1234c8957dadbc6c02588cc5551f79db\",\"title\":\"A Diagnostic Report Generator from CT Volumes on Liver Tumor with Semi-supervised Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/2b8a479c1234c8957dadbc6c02588cc5551f79db\",\"venue\":\"MICCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48404632\",\"name\":\"Felix Stahlberg\"}],\"doi\":\"10.1613/jair.1.12007\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d08dcd2cc1e9691defe664a10f021424a896a1e\",\"title\":\"Neural Machine Translation: A Review\",\"url\":\"https://www.semanticscholar.org/paper/4d08dcd2cc1e9691defe664a10f021424a896a1e\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.24963/ijcai.2019/877\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"title\":\"Deep Learning for Video Captioning: A Review\",\"url\":\"https://www.semanticscholar.org/paper/eeca19117a8a733aae6fb4a91c51d1c1dc03eb7f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21496852\",\"name\":\"R. Panda\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1109/ICASSP.2017.7952384\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c425c057a26accd8f2aadde5c37cf6bf385a59d7\",\"title\":\"Sparse modeling for topic-oriented video summarization\",\"url\":\"https://www.semanticscholar.org/paper/c425c057a26accd8f2aadde5c37cf6bf385a59d7\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1900013\",\"name\":\"Zhengyang Wu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1733702\",\"name\":\"Matthew R. Walter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"177c48590469c62d430cf74fee7b5bd28bfbbc1d\",\"title\":\"Articulated Motion Learning via Visual and Lingual Signals\",\"url\":\"https://www.semanticscholar.org/paper/177c48590469c62d430cf74fee7b5bd28bfbbc1d\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1710.07477\",\"authors\":[{\"authorId\":\"27555915\",\"name\":\"Tz-Ying Wu\"},{\"authorId\":\"16261770\",\"name\":\"Ting-An Chien\"},{\"authorId\":\"36549981\",\"name\":\"Cheng-Sheng Chan\"},{\"authorId\":\"27538483\",\"name\":\"Chan-Wei Hu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2017.15\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"604575bf821ad655e195a78d53badb0a636ffa0f\",\"title\":\"Anticipating Daily Intention Using On-wrist Motion Triggered Sensing\",\"url\":\"https://www.semanticscholar.org/paper/604575bf821ad655e195a78d53badb0a636ffa0f\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46812189\",\"name\":\"K. Gao\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1007/978-3-319-77383-4_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"23de896c1b3487803ec6989b1ec1c3bd0c0f8136\",\"title\":\"Spatio-Temporal Context Networks for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/23de896c1b3487803ec6989b1ec1c3bd0c0f8136\",\"venue\":\"PCM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"145905328\",\"name\":\"Kun He\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"title\":\"Text-to-Clip Video Retrieval with Early Fusion and Re-Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a50d2245d46ce0595ddbf25ae9acb8513aa70067\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48707795\",\"name\":\"Ziwei Wang\"},{\"authorId\":\"7988538\",\"name\":\"Yadan Luo\"},{\"authorId\":null,\"name\":\"Yang Li\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"2416851\",\"name\":\"Hongzhi Yin\"}],\"doi\":\"10.1145/3240508.3240583\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f9853c76065b72a8a25d984f7aa4f2c65a2df623\",\"title\":\"Look Deeper See Richer: Depth-aware Image Paragraph Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f9853c76065b72a8a25d984f7aa4f2c65a2df623\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10376365\",\"name\":\"Sourabh Kulhare\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d1959ba4637739dcc6cc6995e10fd41fd6604713\",\"title\":\"Deep Learning for Semantic Video Understanding by Sourabh Kulhare\",\"url\":\"https://www.semanticscholar.org/paper/d1959ba4637739dcc6cc6995e10fd41fd6604713\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47435551\",\"name\":\"Xu Shen\"},{\"authorId\":\"40434674\",\"name\":\"X. Tian\"},{\"authorId\":\"144521811\",\"name\":\"J. Xing\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c6ea379d48526bc43bec5ae3731deff67c6aa6ea\",\"title\":\"Sequence-to-Sequence Learning via Shared Latent Representation\",\"url\":\"https://www.semanticscholar.org/paper/c6ea379d48526bc43bec5ae3731deff67c6aa6ea\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"Li Yao\"},{\"authorId\":\"144350546\",\"name\":\"Ying Qian\"}],\"doi\":\"10.1109/WACVW.2019.00009\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f9a9031fdff486aac00815f83cf5a9cc5d7a392\",\"title\":\"Novel Activities Detection Algorithm in Extended Videos\",\"url\":\"https://www.semanticscholar.org/paper/9f9a9031fdff486aac00815f83cf5a9cc5d7a392\",\"venue\":\"2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)\",\"year\":2019},{\"arxivId\":\"1903.09761\",\"authors\":[{\"authorId\":\"145062693\",\"name\":\"Anh Nguyen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"af4df89ad28580d98113fa6a816195137f7d1a1d\",\"title\":\"Scene Understanding for Autonomous Manipulation with Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/af4df89ad28580d98113fa6a816195137f7d1a1d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1ec568db83e0921e7d4bf73bd6b9f3dc86e09c38\",\"title\":\"Human Activity Analysis using Multi-modalities and Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ec568db83e0921e7d4bf73bd6b9f3dc86e09c38\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11932073\",\"name\":\"Luchao Han\"},{\"authorId\":\"2112821\",\"name\":\"Y. Sheng\"},{\"authorId\":\"2764348\",\"name\":\"Xuewen Zeng\"}],\"doi\":\"10.1109/ACCESS.2019.2924492\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d39e01f3469c3a574712fdc359a358f93ed06a03\",\"title\":\"A Packet-Length-Adjustable Attention Model Based on Bytes Embedding Using Flow-WGAN for Smart Cybersecurity\",\"url\":\"https://www.semanticscholar.org/paper/d39e01f3469c3a574712fdc359a358f93ed06a03\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46580562\",\"name\":\"Stefan Braun\"},{\"authorId\":\"145243593\",\"name\":\"D. Neil\"},{\"authorId\":\"24033690\",\"name\":\"Jithendar Anumula\"},{\"authorId\":\"9314936\",\"name\":\"Enea Ceolini\"},{\"authorId\":\"1704961\",\"name\":\"Shih-Chii Liu\"}],\"doi\":\"10.1109/IJCNN.2019.8852396\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f7706104e1d3e1c5c18874350de6d35334fd8fed\",\"title\":\"Attention-driven Multi-sensor Selection\",\"url\":\"https://www.semanticscholar.org/paper/f7706104e1d3e1c5c18874350de6d35334fd8fed\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"title\":\"Large-scale video analysis and understanding\",\"url\":\"https://www.semanticscholar.org/paper/6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1471729603\",\"name\":\"Hao Ge\"},{\"authorId\":\"7892302\",\"name\":\"Xiaoguang Tu\"},{\"authorId\":\"153002732\",\"name\":\"W. Ai\"},{\"authorId\":\"1596821842\",\"name\":\"Yao Luo\"},{\"authorId\":\"1730232\",\"name\":\"Zheng Ma\"},{\"authorId\":\"144917416\",\"name\":\"M. Xie\"}],\"doi\":\"10.1109/CTISC49998.2020.00025\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9eab782d1a530c66150bf2754a266cfb87ab618c\",\"title\":\"Face Anti-Spoofing by the Enhancement of Temporal Motion\",\"url\":\"https://www.semanticscholar.org/paper/9eab782d1a530c66150bf2754a266cfb87ab618c\",\"venue\":\"2020 2nd International Conference on Advances in Computer Technology, Information Science and Communications (CTISC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2648498\",\"name\":\"Wenlong Xie\"},{\"authorId\":\"1720100\",\"name\":\"H. Yao\"},{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"9162163\",\"name\":\"T. Han\"},{\"authorId\":\"1755487\",\"name\":\"S. Zhao\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/TMM.2018.2879749\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c1d06998c57d06a792df88a48f4e52b59f730ff\",\"title\":\"Discovering Latent Discriminative Patterns for Multi-Mode Event Representation\",\"url\":\"https://www.semanticscholar.org/paper/2c1d06998c57d06a792df88a48f4e52b59f730ff\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51237531\",\"name\":\"Shaoning Xiao\"},{\"authorId\":\"48515305\",\"name\":\"Yimeng Li\"},{\"authorId\":\"22228139\",\"name\":\"Yunan Ye\"},{\"authorId\":null,\"name\":\"Long Chen\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"2549731\",\"name\":\"Jian Shao\"},{\"authorId\":\"145974112\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.1007/s11063-019-10003-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8bf0b37ed005285b6cbef70a78434978ca065120\",\"title\":\"Hierarchical Temporal Fusion of Multi-grained Attention Features for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/8bf0b37ed005285b6cbef70a78434978ca065120\",\"venue\":\"Neural Processing Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143715692\",\"name\":\"X. Hao\"},{\"authorId\":\"46468475\",\"name\":\"F. Zhou\"},{\"authorId\":\"33899331\",\"name\":\"Xiaoyong Li\"}],\"doi\":\"10.1109/ITNEC48623.2020.9084781\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"653de101370307afc2eba27d4e4c574441eb06da\",\"title\":\"Scene-Edge GRU for Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/653de101370307afc2eba27d4e4c574441eb06da\",\"venue\":\"2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yu Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"31c8f1f728df2cfea5d0a9dda67a27de82f5a879\",\"title\":\"Let Your Photos Talk: Generating Narrative Paragraph for Photo Stream via Bidirectional Attention Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/31c8f1f728df2cfea5d0a9dda67a27de82f5a879\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1812.06587\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"1944225\",\"name\":\"Yannis Kalantidis\"},{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00674\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"171a027fc6c7f4194569170accc48187c8bb5aaa\",\"title\":\"Grounded Video Description\",\"url\":\"https://www.semanticscholar.org/paper/171a027fc6c7f4194569170accc48187c8bb5aaa\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51001584\",\"name\":\"Hyung-min Lee\"},{\"authorId\":\"153481384\",\"name\":\"Il-Koo Kim\"}],\"doi\":\"10.1109/IJCNN.2019.8851892\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"title\":\"Generating Natural Video Descriptions using Semantic Gate\",\"url\":\"https://www.semanticscholar.org/paper/b89d030332f7ff66ef270160dfc93e6b3122f34b\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740039\",\"name\":\"Jie Chen\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"2838253\",\"name\":\"C. He\"}],\"doi\":\"10.1016/J.PATREC.2018.06.030\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c93fcf73554f2cb9e0bc21da82f9611ae631f55\",\"title\":\"Movie fill in the blank by joint learning from video and text with adaptive temporal attention\",\"url\":\"https://www.semanticscholar.org/paper/3c93fcf73554f2cb9e0bc21da82f9611ae631f55\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":\"1905.02963\",\"authors\":[{\"authorId\":\"145114776\",\"name\":\"L. Sun\"},{\"authorId\":\"143721383\",\"name\":\"Bing Li\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"}],\"doi\":\"10.1109/ICME.2019.00226\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"title\":\"Multimodal Semantic Attention Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ea5bcfce4ee889346c08efb2db3cb2e97250029\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":\"1511.06432\",\"authors\":[{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"title\":\"Delving Deeper into Convolutional Networks for Learning Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/ed95c6bcdc16fb1f68b20d5bcd15c4aca4d0abde\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"23604671\",\"name\":\"H. Wang\"},{\"authorId\":\"49292319\",\"name\":\"Bo Wang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/3123266.3127904\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7a736b7347fc5ea93c196ddfe0630ecddc17d324\",\"title\":\"Multirate Multimodal Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7a736b7347fc5ea93c196ddfe0630ecddc17d324\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1909.02218\",\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"2061528\",\"name\":\"Wenqing Chu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2018.2859820\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"96f0908cc138aceb2d5e0180c440e5adc711d855\",\"title\":\"A Better Way to Attend: Attention With Trees for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/96f0908cc138aceb2d5e0180c440e5adc711d855\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40201308\",\"name\":\"C\\u00e9sar Laurent\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145467703\",\"name\":\"P. Vincent\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"65fb87b62dfbaa8df24d1d115f534fc8bd0144df\",\"title\":\"Recurrent Normalization Propagation\",\"url\":\"https://www.semanticscholar.org/paper/65fb87b62dfbaa8df24d1d115f534fc8bd0144df\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1511.06674\",\"authors\":[{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"1749627\",\"name\":\"Marius Leordeanu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8120a64a73b89294990b3c1e4567b503869b8979\",\"title\":\"Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/8120a64a73b89294990b3c1e4567b503869b8979\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8071088\",\"name\":\"R. Oruganti\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"8262287\",\"name\":\"Suhas Pillai\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/ICIP.2016.7533033\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fe220d668a694cddf005958e2e58f568570276b\",\"title\":\"Image description through fusion based recurrent multi-modal learning\",\"url\":\"https://www.semanticscholar.org/paper/5fe220d668a694cddf005958e2e58f568570276b\",\"venue\":\"2016 IEEE International Conference on Image Processing (ICIP)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7762524\",\"name\":\"Xiao-Yu Du\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"46554639\",\"name\":\"Liu Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"7477697\",\"name\":\"Zhiguang Qin\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"}],\"doi\":\"10.1007/s11390-017-1738-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69a9cf9bc8e585782824666fa3fb5ce5cf07cef2\",\"title\":\"Captioning Videos Using Large-Scale Image Corpus\",\"url\":\"https://www.semanticscholar.org/paper/69a9cf9bc8e585782824666fa3fb5ce5cf07cef2\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144179801\",\"name\":\"R. Bhatt\"},{\"authorId\":\"66790247\",\"name\":\"M. G. Uzunbas\"},{\"authorId\":\"152527483\",\"name\":\"Thai Hoang\"},{\"authorId\":\"1394268433\",\"name\":\"Ozge C. Whiting\"}],\"doi\":\"10.1109/CVPRW.2019.00113\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"969b04f58a2bf4a1658de0af544de590cd99ead2\",\"title\":\"Segmentation of Low-Level Temporal Plume Patterns From IR Video\",\"url\":\"https://www.semanticscholar.org/paper/969b04f58a2bf4a1658de0af544de590cd99ead2\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47523598\",\"name\":\"T. Nguyen\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/WNYIPW.2017.8356255\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3417673c59544fcd33820a0a583a7543c70ac595\",\"title\":\"Multistream hierarchical boundary network for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/3417673c59544fcd33820a0a583a7543c70ac595\",\"venue\":\"2017 IEEE Western New York Image and Signal Processing Workshop (WNYISPW)\",\"year\":2017},{\"arxivId\":\"1711.08690\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"1723272\",\"name\":\"H. Dibeklioglu\"},{\"authorId\":\"1756344\",\"name\":\"T. Baltrusaitis\"},{\"authorId\":\"2743835\",\"name\":\"D. Tax\"}],\"doi\":\"10.1109/TIP.2019.2948288\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fff43f341ae9985b5ecaa5a30f9ba7d9be1f9aa7\",\"title\":\"Attended End-to-End Architecture for Age Estimation From Facial Expression Videos\",\"url\":\"https://www.semanticscholar.org/paper/fff43f341ae9985b5ecaa5a30f9ba7d9be1f9aa7\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21496852\",\"name\":\"R. Panda\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"3311781\",\"name\":\"Z. Wu\"},{\"authorId\":\"39497207\",\"name\":\"J. Ernst\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1109/ICCV.2017.395\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97b5800e144a8df48f1f7e91383b0f37bc37cf60\",\"title\":\"Weakly Supervised Summarization of Web Videos\",\"url\":\"https://www.semanticscholar.org/paper/97b5800e144a8df48f1f7e91383b0f37bc37cf60\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89753104\",\"name\":\"Misbah Munir\"},{\"authorId\":\"1754181\",\"name\":\"Fabio Cuzzolin\"},{\"authorId\":null,\"name\":\"Suman Saha\"},{\"authorId\":\"48477977\",\"name\":\"R. Vagner\"},{\"authorId\":\"47790942\",\"name\":\"F. Mitchell\"},{\"authorId\":\"144719498\",\"name\":\"F. Saleh\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"2407f8cc69171362c35da5ba0435d87c87c8942c\",\"title\":\"Video Classification using Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/2407f8cc69171362c35da5ba0435d87c87c8942c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144180678\",\"name\":\"M. Lu\"},{\"authorId\":\"1689656\",\"name\":\"Z. Li\"},{\"authorId\":\"7135663\",\"name\":\"Y. Wang\"},{\"authorId\":\"144563871\",\"name\":\"Gang Pan\"}],\"doi\":\"10.1109/TIP.2019.2901707\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"355f769cc896ff3ea302423587c9a1b7c2301c4e\",\"title\":\"Deep Attention Network for Egocentric Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/355f769cc896ff3ea302423587c9a1b7c2301c4e\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1109/TMM.2019.2896515\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"title\":\"Generating Video Descriptions With Latent Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/e5f79ee6c9b3e5951e4267d4624d2d7669a72cb3\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":null,\"name\":\"Shuo Wang\"},{\"authorId\":\"144876834\",\"name\":\"Q. Tian\"},{\"authorId\":\"73160450\",\"name\":\"Meng Wang\"}],\"doi\":\"10.24963/ijcai.2019/105\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f07bef10500f55d4d34bac96bbe93f1120a6ca8d\",\"title\":\"Dense Temporal Convolution Network for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/f07bef10500f55d4d34bac96bbe93f1120a6ca8d\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1911.09345\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"1747500\",\"name\":\"A. Mian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"title\":\"Empirical Autopsy of Deep Video Captioning Frameworks\",\"url\":\"https://www.semanticscholar.org/paper/62fadf3cd3ba64cd148600f2582e2cfa6859fad7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50695255\",\"name\":\"S. Wang\"},{\"authorId\":\"144713153\",\"name\":\"Dan Guo\"},{\"authorId\":\"2779625\",\"name\":\"Wen-gang Zhou\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":\"10.1145/3240508.3240671\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"33e309f993023a0384221733dd884e2b891c8311\",\"title\":\"Connectionist Temporal Fusion for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/33e309f993023a0384221733dd884e2b891c8311\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77537913\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/s11042-019-08011-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c2623c1b4aee3f043da30d05f3e2c0f62fca5d5e\",\"title\":\"Deep multimodal embedding for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/c2623c1b4aee3f043da30d05f3e2c0f62fca5d5e\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"2011768695\",\"name\":\"Junjun Guo\"},{\"authorId\":\"2409659\",\"name\":\"Shengxiang Gao\"},{\"authorId\":\"121854326\",\"name\":\"Zhengtao Yu\"}],\"doi\":\"10.1016/j.patcog.2020.107702\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6686fadf7f7ef2283cc9286095db281f8520ec04\",\"title\":\"Enhancing the alignment between target words and corresponding frames for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/6686fadf7f7ef2283cc9286095db281f8520ec04\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":\"2005.00596\",\"authors\":[{\"authorId\":\"34145947\",\"name\":\"Zhuolin Jiang\"},{\"authorId\":\"3330139\",\"name\":\"J. Silovsk\\u00fd\"},{\"authorId\":\"143882614\",\"name\":\"M. Siu\"},{\"authorId\":\"144339076\",\"name\":\"W. Hartmann\"},{\"authorId\":\"1793645\",\"name\":\"H. Gish\"},{\"authorId\":\"32484187\",\"name\":\"S. Adali\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d4ead18fc29e22a5f8b4d42e2f6041861b65b58\",\"title\":\"Learning from Noisy Labels with Noise Modeling Network\",\"url\":\"https://www.semanticscholar.org/paper/0d4ead18fc29e22a5f8b4d42e2f6041861b65b58\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103887800\",\"name\":\"Rose Rustowicz\"},{\"authorId\":\"113983041\",\"name\":\"Robin Cheong\"},{\"authorId\":\"51471745\",\"name\":\"Lijing Wang\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"},{\"authorId\":\"49240687\",\"name\":\"M. Burke\"},{\"authorId\":\"2465182\",\"name\":\"D. Lobell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"87f0cffad2d7527273a25a945736c1e813c08312\",\"title\":\"Semantic Segmentation of Crop Type in Africa: A Novel Dataset and Analysis of Deep Learning Methods\",\"url\":\"https://www.semanticscholar.org/paper/87f0cffad2d7527273a25a945736c1e813c08312\",\"venue\":\"CVPR Workshops\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1917800589\",\"name\":\"Xuewei Ding\"},{\"authorId\":\"1918733632\",\"name\":\"Yehao Li\"},{\"authorId\":\"51018452\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"50190972\",\"name\":\"Dan Zeng\"},{\"authorId\":\"1917497790\",\"name\":\"Ting Yao\"}],\"doi\":\"10.1109/MIPR49039.2020.00065\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf64558dad6729720abd47dd2dde281765baa079\",\"title\":\"Exploring Depth Information for Spatial Relation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bf64558dad6729720abd47dd2dde281765baa079\",\"venue\":\"2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)\",\"year\":2020},{\"arxivId\":\"1901.00097\",\"authors\":[{\"authorId\":\"51288954\",\"name\":\"Jiarong Dong\"},{\"authorId\":\"144947766\",\"name\":\"Ke Gao\"},{\"authorId\":\"3162023\",\"name\":\"Xiaokai Chen\"},{\"authorId\":\"2031845\",\"name\":\"Junbo Guo\"},{\"authorId\":\"144089410\",\"name\":\"J. Cao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"title\":\"Not All Words Are Equal: Video-specific Information Loss for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3fc5d77e3238a3a9d17698b35fb425cc227263be\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1994707\",\"name\":\"X. Gao\"},{\"authorId\":\"1695158\",\"name\":\"Tingting Mu\"},{\"authorId\":\"1723263\",\"name\":\"J. Y. Goulermas\"},{\"authorId\":\"47446553\",\"name\":\"M. Wang\"}],\"doi\":\"10.1016/j.ins.2017.08.026\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"40a2215a06506fc4068cd70d5cdd045bf8950678\",\"title\":\"Attention driven multi-modal similarity learning\",\"url\":\"https://www.semanticscholar.org/paper/40a2215a06506fc4068cd70d5cdd045bf8950678\",\"venue\":\"Inf. Sci.\",\"year\":2018},{\"arxivId\":\"1904.13003\",\"authors\":[{\"authorId\":\"143714141\",\"name\":\"H. Chen\"},{\"authorId\":\"1778107\",\"name\":\"G. Chirikjian\"}],\"doi\":\"10.1109/CVPRW50498.2020.00437\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3f91bdd76a8f50241fba9341c0eb67ed5a03b71f\",\"title\":\"Curvature: A signature for Action Recognition in Video Sequences\",\"url\":\"https://www.semanticscholar.org/paper/3f91bdd76a8f50241fba9341c0eb67ed5a03b71f\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902168\",\"name\":\"Zhen Wei\"}],\"doi\":\"10.25560/76530\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"35f011014990a4642d75e7f845b60e606e3476a2\",\"title\":\"Machine learning applications in finance: some case studies\",\"url\":\"https://www.semanticscholar.org/paper/35f011014990a4642d75e7f845b60e606e3476a2\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1612.01033\",\"authors\":[{\"authorId\":\"3048367\",\"name\":\"M. Pedersoli\"},{\"authorId\":\"144982160\",\"name\":\"T. Lucas\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"34602236\",\"name\":\"Jakob Verbeek\"}],\"doi\":\"10.1109/ICCV.2017.140\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d86f0e22fed5e065ecf54b273d540b2430f014d\",\"title\":\"Areas of Attention for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6d86f0e22fed5e065ecf54b273d540b2430f014d\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1612.07360\",\"authors\":[{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"40521893\",\"name\":\"Abir Das\"},{\"authorId\":\"1701293\",\"name\":\"J. Zhang\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2017.334\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"76f83380fe193ae8475e660c1c6b12b60521a29f\",\"title\":\"Top-Down Visual Saliency Guided by Captions\",\"url\":\"https://www.semanticscholar.org/paper/76f83380fe193ae8475e660c1c6b12b60521a29f\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1007/978-3-030-14657-3_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a9bae357bc63540cfbd630f4aaab4088edf2e0\",\"title\":\"Video Captioning Using Hierarchical LSTM and Text-Based Sliding Window\",\"url\":\"https://www.semanticscholar.org/paper/08a9bae357bc63540cfbd630f4aaab4088edf2e0\",\"venue\":\"IoTaaS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3159503\",\"name\":\"Evan Racah\"},{\"authorId\":\"12757989\",\"name\":\"C. Beckham\"},{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"3127597\",\"name\":\"S. Kahou\"},{\"authorId\":\"1764912\",\"name\":\"Prabhat\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fd0a1a2ecf69a6c1a6efcb18b8f23e4d5402f601\",\"title\":\"ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events\",\"url\":\"https://www.semanticscholar.org/paper/fd0a1a2ecf69a6c1a6efcb18b8f23e4d5402f601\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1910.12019\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"title\":\"Diverse Video Captioning Through Latent Variable Expansion with Conditional GAN\",\"url\":\"https://www.semanticscholar.org/paper/a17542ca3c7a39470bdbe70a2209c195be6d63df\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1603.09025\",\"authors\":[{\"authorId\":\"2348758\",\"name\":\"Tim Cooijmans\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"40201308\",\"name\":\"C\\u00e9sar Laurent\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"952454718139dba3aafc6b3b67c4f514ac3964af\",\"title\":\"Recurrent Batch Normalization\",\"url\":\"https://www.semanticscholar.org/paper/952454718139dba3aafc6b3b67c4f514ac3964af\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1606.02382\",\"authors\":[{\"authorId\":\"3422124\",\"name\":\"Petteri Teikari\"},{\"authorId\":\"2221517\",\"name\":\"M. Santos\"},{\"authorId\":\"11211939\",\"name\":\"Charissa Poon\"},{\"authorId\":\"143815485\",\"name\":\"K. Hynynen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"76d1b969679db8b2392a08cf699e4ed4d868e7e9\",\"title\":\"Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/76d1b969679db8b2392a08cf699e4ed4d868e7e9\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1910.09920\",\"authors\":[{\"authorId\":\"10007321\",\"name\":\"Farnoosh Heidarivincheh\"},{\"authorId\":\"1728108\",\"name\":\"M. Mirmehdi\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":\"10.1109/ICCVW.2019.00150\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"85accdf27ef03155d41f9740fed6044afb5dd5f6\",\"title\":\"Weakly-Supervised Completion Moment Detection using Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/85accdf27ef03155d41f9740fed6044afb5dd5f6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2662002\",\"name\":\"Oliver Nina\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c43cd58f79a56538c8990693d900617b9bd940e5\",\"title\":\"A Multitask Learning Encoder-Decoders Framework for Generating Movie and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c43cd58f79a56538c8990693d900617b9bd940e5\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1803.06798\",\"authors\":[{\"authorId\":\"46772448\",\"name\":\"X. Chen\"},{\"authorId\":\"145371957\",\"name\":\"Chang Xu\"},{\"authorId\":\"1795291\",\"name\":\"X. Yang\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1007/978-3-030-01216-8_11\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5470c1d487c5d86648aa4292e1cdbd852ce6947\",\"title\":\"Attention-GAN for Object Transfiguration in Wild Images\",\"url\":\"https://www.semanticscholar.org/paper/d5470c1d487c5d86648aa4292e1cdbd852ce6947\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35882697\",\"name\":\"A. Dhillon\"},{\"authorId\":\"35077572\",\"name\":\"G. Verma\"}],\"doi\":\"10.1007/s13748-019-00203-0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ca5bb12c0d46f9e074e74bd7e08845ebfebf3d5\",\"title\":\"Convolutional neural network: a review of models, methodologies and applications to object detection\",\"url\":\"https://www.semanticscholar.org/paper/1ca5bb12c0d46f9e074e74bd7e08845ebfebf3d5\",\"venue\":\"Progress in Artificial Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2097029\",\"name\":\"J. Zhang\"},{\"authorId\":\"122700784\",\"name\":\"Zhaohui Tang\"},{\"authorId\":\"49291167\",\"name\":\"Yongfang Xie\"},{\"authorId\":\"46247410\",\"name\":\"M. Ai\"},{\"authorId\":\"145202887\",\"name\":\"W. Gui\"}],\"doi\":\"10.1016/j.mineng.2020.106332\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c40be90ac3125d7fcd8594bae9e3d8150c71d417\",\"title\":\"Convolutional memory network-based flotation performance monitoring\",\"url\":\"https://www.semanticscholar.org/paper/c40be90ac3125d7fcd8594bae9e3d8150c71d417\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"48093314\",\"name\":\"Jing-Wen Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3394171.3413908\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"title\":\"Controllable Video Captioning with an Exemplar Sentence\",\"url\":\"https://www.semanticscholar.org/paper/40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1907.12905\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"50490213\",\"name\":\"Jianfei Cai\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"}],\"doi\":\"10.1145/3343031.3351060\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"title\":\"Watch It Twice: Video Captioning with a Refocused Video Encoder\",\"url\":\"https://www.semanticscholar.org/paper/5ea12e7ccefa226593a6918ae3100bfcd4b2d284\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1706.04508\",\"authors\":[{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"8053308\",\"name\":\"J. Tang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/TMM.2018.2823900\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"36ab143da8b6f6d49811afaaa7bcbf81c22a210e\",\"title\":\"Modeling Multimodal Clues in a Hybrid Deep Learning Framework for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/36ab143da8b6f6d49811afaaa7bcbf81c22a210e\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46812189\",\"name\":\"K. Gao\"},{\"authorId\":\"15318113\",\"name\":\"Xianglei Zhu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1007/978-981-10-8530-7_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"333ff6c4544ea083581476e4dc70548aff7fb365\",\"title\":\"Initialized Frame Attention Networks for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/333ff6c4544ea083581476e4dc70548aff7fb365\",\"venue\":\"ICIMCS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145179162\",\"name\":\"Mingxing Zhang\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"50006507\",\"name\":\"Yanli Ji\"},{\"authorId\":\"145833207\",\"name\":\"Ning Xie\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1007/978-3-319-68155-9_20\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b3041b13dc0b862e71a67b7d7841ad01e01f7f0a\",\"title\":\"Deep Semantic Indexing Using Convolutional Localization Network with Region-Based Visual Attention for Image Database\",\"url\":\"https://www.semanticscholar.org/paper/b3041b13dc0b862e71a67b7d7841ad01e01f7f0a\",\"venue\":\"ADC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50633239\",\"name\":\"I. Abramovich\"},{\"authorId\":\"1411460100\",\"name\":\"Tomer Ben-Yehuda\"},{\"authorId\":\"2221482\",\"name\":\"Rami Cohen\"}],\"doi\":\"10.1109/ICSEE.2018.8646076\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b0bb2de5b9bd7b350cba97adea5826226329ab5a\",\"title\":\"Low-Complexity Video Classification using Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b0bb2de5b9bd7b350cba97adea5826226329ab5a\",\"venue\":\"2018 IEEE International Conference on the Science of Electrical Engineering in Israel (ICSEE)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"50272388\",\"name\":\"Ioana Croitoru\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"title\":\"A hierarchical approach to vision-based language generation: from simple sentences to complex natural language\",\"url\":\"https://www.semanticscholar.org/paper/9348890ecbbfd7bb75667fa2014ebe6f4a5558b1\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118423971\",\"name\":\"K. Y. Wong\"},{\"authorId\":\"2877050\",\"name\":\"R. Wong\"}],\"doi\":\"10.1109/DSAA49011.2020.00119\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"043abeffb4d46dc5ff3e0b11d63327229f240d2f\",\"title\":\"Big Data Quality Prediction on Banking Applications: Extended Abstract\",\"url\":\"https://www.semanticscholar.org/paper/043abeffb4d46dc5ff3e0b11d63327229f240d2f\",\"venue\":\"2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151487400\",\"name\":\"Chu-yi Li\"},{\"authorId\":\"9319341\",\"name\":\"Wei-yu Yu\"}],\"doi\":\"10.1117/12.2514651\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ddbc1542476237b6ace7b871e34269e790d35bad\",\"title\":\"Spatial-temporal attention in Bi-LSTM networks based on multiple features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ddbc1542476237b6ace7b871e34269e790d35bad\",\"venue\":\"Other Conferences\",\"year\":2018},{\"arxivId\":\"1611.04021\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1da2431a799f68888b7e035fe49fe47a4735b71b\",\"title\":\"Leveraging Video Descriptions to Learn Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1da2431a799f68888b7e035fe49fe47a4735b71b\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"2002.12886\",\"authors\":[{\"authorId\":\"1965933798\",\"name\":\"Alban Main De Boissiere\"},{\"authorId\":\"2479033\",\"name\":\"Rita Noumeir\"}],\"doi\":\"10.1109/ACCESS.2020.3023599\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f67eca7b8511a3253d58fd12b0768c9caa12610d\",\"title\":\"Infrared and 3D Skeleton Feature Fusion for RGB-D Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/f67eca7b8511a3253d58fd12b0768c9caa12610d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2012.02128\",\"authors\":[{\"authorId\":\"1768759403\",\"name\":\"Jing Su\"},{\"authorId\":\"49892954\",\"name\":\"Qingyun Dai\"},{\"authorId\":\"8139616\",\"name\":\"F. Guerin\"},{\"authorId\":\"1510708415\",\"name\":\"Mian Zhou\"}],\"doi\":\"10.1016/j.csl.2020.101169\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c1a435ffd06c26f9f4273b11efed10bce2138d08\",\"title\":\"BERT-hLSTMs: BERT and Hierarchical LSTMs for Visual Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/c1a435ffd06c26f9f4273b11efed10bce2138d08\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387548078\",\"name\":\"K. Lin\"},{\"authorId\":\"1738276592\",\"name\":\"Zhuoxin Gan\"},{\"authorId\":\"39060743\",\"name\":\"Liwei Wang\"}],\"doi\":\"10.18653/v1/2020.findings-emnlp.98\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3263b941d0a77bbd2040612ec774ef063ef64c48\",\"title\":\"Semi-Supervised Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3263b941d0a77bbd2040612ec774ef063ef64c48\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":\"1812.11178\",\"authors\":[{\"authorId\":\"144118452\",\"name\":\"Pengfei Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"482377754f5a371efd51d4948087eca3e0cd955b\",\"title\":\"Drug cell line interaction prediction\",\"url\":\"https://www.semanticscholar.org/paper/482377754f5a371efd51d4948087eca3e0cd955b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49035023\",\"name\":\"T. Nguyen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"64eac8b653194e2d479c2bf28d8f2bd2bfb9f53c\",\"title\":\"Automatic Video Captioning using Deep Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/64eac8b653194e2d479c2bf28d8f2bd2bfb9f53c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1608.02367\",\"authors\":[{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"},{\"authorId\":\"3111194\",\"name\":\"J. Heikkil\\u00e4\"},{\"authorId\":\"1771769\",\"name\":\"N. Yokoya\"}],\"doi\":\"10.1007/978-3-319-46604-0_46\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"aa76f655c2ad655080593a191c4b479ab9f18117\",\"title\":\"Learning Joint Representations of Videos and Sentences with Web Image Search\",\"url\":\"https://www.semanticscholar.org/paper/aa76f655c2ad655080593a191c4b479ab9f18117\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742445396\",\"name\":\"Lin Wang\"},{\"authorId\":\"8417088\",\"name\":\"Xingfu Wang\"},{\"authorId\":\"3132704\",\"name\":\"Ammar Hawbani\"},{\"authorId\":\"38131633\",\"name\":\"Y. Xiong\"},{\"authorId\":\"89013155\",\"name\":\"Xu Zhang\"}],\"doi\":\"10.1109/ACCESS.2020.3012154\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54d296dab7a1fd42ad30ed52d266c9faf1c63fab\",\"title\":\"Convolution Encoders for End-to-End Action Tracking With Space-Time Cubic Kernels\",\"url\":\"https://www.semanticscholar.org/paper/54d296dab7a1fd42ad30ed52d266c9faf1c63fab\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1391032149\",\"name\":\"S. Zheng\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"}],\"doi\":\"10.1145/3343031.3350962\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72d3c465ada9bda805e5df1d2e62f9da76cd0dba\",\"title\":\"Visual Relation Detection with Multi-Level Attention\",\"url\":\"https://www.semanticscholar.org/paper/72d3c465ada9bda805e5df1d2e62f9da76cd0dba\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"09926ed62511c340f4540b5bc53cf2480e8063f8\",\"title\":\"Tubelet Detector for Spatio-Temporal Action Localization\",\"url\":\"https://www.semanticscholar.org/paper/09926ed62511c340f4540b5bc53cf2480e8063f8\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1610.04062\",\"authors\":[{\"authorId\":\"144839067\",\"name\":\"Amir Mazaheri\"},{\"authorId\":\"119745921\",\"name\":\"Dong Zhang\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fb31d0375552500bd494af20ab0c3109c9be3d2\",\"title\":\"Video Fill in the Blank with Merging LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/9fb31d0375552500bd494af20ab0c3109c9be3d2\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1805.07935\",\"authors\":[{\"authorId\":\"145591949\",\"name\":\"Yuan Cheng\"},{\"authorId\":\"1792137\",\"name\":\"Guangya Li\"},{\"authorId\":\"2793468\",\"name\":\"Hai-Bao Chen\"},{\"authorId\":\"1733737\",\"name\":\"Sheldon X.-D. Tan\"},{\"authorId\":\"40355833\",\"name\":\"Hao Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b04e96e1fc56c11c43a6fc3e84ba4ca95905e2de\",\"title\":\"DEEPEYE: A Compact and Accurate Video Comprehension at Terminal Devices Compressed with Quantization and Tensorization\",\"url\":\"https://www.semanticscholar.org/paper/b04e96e1fc56c11c43a6fc3e84ba4ca95905e2de\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1007/s00530-018-0598-5\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"title\":\"Multi-guiding long short-term memory for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f386d25bfbb0399fd6c8116add5faa66ffcfa467\",\"venue\":\"Multimedia Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47672151\",\"name\":\"Ning Xu\"},{\"authorId\":\"143602033\",\"name\":\"Anan Liu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144536249\",\"name\":\"W. Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/TCSVT.2018.2867286\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"title\":\"Dual-Stream Recurrent Neural Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbb5b0a9ccb8a1f70b49524285b7bc3cbcc2d91b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144180678\",\"name\":\"Minlong Lu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"972b1d8842df8ad06c50a33782af1cfdc9ffd411\",\"title\":\"Action analysis and control strategy for rat robot automatic navigation\",\"url\":\"https://www.semanticscholar.org/paper/972b1d8842df8ad06c50a33782af1cfdc9ffd411\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"144350546\",\"name\":\"Y. Qian\"}],\"doi\":\"10.1007/978-3-030-00776-8_57\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f094bd9360b59afc2880c20ad3a37ee640c6c6c9\",\"title\":\"DT-3DResNet-LSTM: An Architecture for Temporal Activity Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/f094bd9360b59afc2880c20ad3a37ee640c6c6c9\",\"venue\":\"TRECVID\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32062468\",\"name\":\"H. Ou\"},{\"authorId\":\"2994709\",\"name\":\"J. Sun\"}],\"doi\":\"10.1117/1.JEI.28.2.023009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fbb35c26df06eb8e0afe82d1213b8561af4af7a5\",\"title\":\"Spatiotemporal information deep fusion network with frame attention mechanism for video action recognition\",\"url\":\"https://www.semanticscholar.org/paper/fbb35c26df06eb8e0afe82d1213b8561af4af7a5\",\"venue\":\"J. Electronic Imaging\",\"year\":2019},{\"arxivId\":\"1505.00393\",\"authors\":[{\"authorId\":\"2077146\",\"name\":\"Francesco Visin\"},{\"authorId\":\"2182706\",\"name\":\"Kyle Kastner\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"145927530\",\"name\":\"M. Matteucci\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e463877264e70d53c844cf4b1bf3b15baec8cfb\",\"title\":\"ReNet: A Recurrent Neural Network Based Alternative to Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/7e463877264e70d53c844cf4b1bf3b15baec8cfb\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1511.06425\",\"authors\":[{\"authorId\":\"2316359\",\"name\":\"Q. Gan\"},{\"authorId\":\"3187768\",\"name\":\"Qipeng Guo\"},{\"authorId\":null,\"name\":\"Zheng Zhang\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9ad8c207d66553d0fa7a7cb57c5e1be12896d1d9\",\"title\":\"First Step toward Model-Free, Anonymous Object Tracking with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/9ad8c207d66553d0fa7a7cb57c5e1be12896d1d9\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1612.00234\",\"authors\":[{\"authorId\":\"144858226\",\"name\":\"Xiang Long\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"}],\"doi\":\"10.1162/tacl_a_00013\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5a732016c3f74dc7d78899bf33cf25df03ef46b4\",\"title\":\"Video Captioning with Multi-Faceted Attention\",\"url\":\"https://www.semanticscholar.org/paper/5a732016c3f74dc7d78899bf33cf25df03ef46b4\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2018},{\"arxivId\":\"1911.12682\",\"authors\":[{\"authorId\":\"47435551\",\"name\":\"Xu Shen\"},{\"authorId\":\"40434674\",\"name\":\"X. Tian\"},{\"authorId\":\"3141359\",\"name\":\"Shaoyan Sun\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"04583eacb6b18d297f680a3a5cda5cbf653efeda\",\"title\":\"Patch Reordering: a Novel Way to Achieve Rotation and Translation Invariance in Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/04583eacb6b18d297f680a3a5cda5cbf653efeda\",\"venue\":\"AAAI 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40296536\",\"name\":\"Z. Wang\"},{\"authorId\":null,\"name\":\"Jie Zhou\"},{\"authorId\":\"2582320\",\"name\":\"J. Ma\"},{\"authorId\":\"1492115784\",\"name\":\"Jing-jing Li\"},{\"authorId\":\"21576252\",\"name\":\"Jiangbo Ai\"},{\"authorId\":\"46173234\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1016/J.IPM.2019.102130\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0eaa75b2c7ef5ccbe19a1b88b7f4de3e5c52713\",\"title\":\"Discovering attractive segments in the user-generated video streams\",\"url\":\"https://www.semanticscholar.org/paper/c0eaa75b2c7ef5ccbe19a1b88b7f4de3e5c52713\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":\"1808.07272\",\"authors\":[{\"authorId\":\"2527741\",\"name\":\"Sibo Song\"},{\"authorId\":\"143770929\",\"name\":\"N. Cheung\"},{\"authorId\":\"1802086\",\"name\":\"V. Chandrasekhar\"},{\"authorId\":\"1709001\",\"name\":\"B. Mandal\"}],\"doi\":\"10.1145/3240508.3240713\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d09d663055b3b6d588bf4de2f386bb144d09aea8\",\"title\":\"Deep Adaptive Temporal Pooling for Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d09d663055b3b6d588bf4de2f386bb144d09aea8\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":\"1902.10322\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1109/CVPR.2019.01277\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20888a7aebaf77a306c0886f165bd0d468db806d\",\"title\":\"Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20888a7aebaf77a306c0886f165bd0d468db806d\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1507.06120\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2837527\",\"name\":\"Mariella Dimiccoli\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1109/THMS.2016.2616296\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d4c2dd3996cb3d87da6c35d72572637d3175ea5\",\"title\":\"Toward Storytelling From Visual Lifelogging: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/1d4c2dd3996cb3d87da6c35d72572637d3175ea5\",\"venue\":\"IEEE Transactions on Human-Machine Systems\",\"year\":2017},{\"arxivId\":\"1805.07888\",\"authors\":[{\"authorId\":\"31868671\",\"name\":\"W. Chen\"},{\"authorId\":\"1801452\",\"name\":\"D. McDuff\"}],\"doi\":\"10.1007/978-3-030-01216-8_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f220beedd8d00627aedb7e4f0bb4ce45c75c65c\",\"title\":\"DeepPhys: Video-Based Physiological Measurement Using Convolutional Attention Networks\",\"url\":\"https://www.semanticscholar.org/paper/1f220beedd8d00627aedb7e4f0bb4ce45c75c65c\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897806\",\"name\":\"Yusuke Goutsu\"},{\"authorId\":\"1832291\",\"name\":\"W. Takano\"},{\"authorId\":\"49123707\",\"name\":\"Y. Nakamura\"}],\"doi\":\"10.1007/s11263-017-1053-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fad2fb939aa77fda344ad1e74b2676182c28cf2c\",\"title\":\"Classification of Multi-class Daily Human Motion using Discriminative Body Parts and Sentence Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/fad2fb939aa77fda344ad1e74b2676182c28cf2c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1506.01144\",\"authors\":[{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2016.29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"title\":\"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?\",\"url\":\"https://www.semanticscholar.org/paper/00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1803.10827\",\"authors\":[{\"authorId\":\"2883417\",\"name\":\"Kiana Ehsani\"},{\"authorId\":\"2456400\",\"name\":\"Hessam Bagherinezhad\"},{\"authorId\":\"40497777\",\"name\":\"Joseph Redmon\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/CVPR.2018.00426\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a63b8429ebeef316a65a94b021ef9a214c705f83\",\"title\":\"Who Let the Dogs Out? Modeling Dog Behavior from Visual Data\",\"url\":\"https://www.semanticscholar.org/paper/a63b8429ebeef316a65a94b021ef9a214c705f83\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119700639\",\"name\":\"Botian Shi\"},{\"authorId\":\"50688017\",\"name\":\"L. Ji\"},{\"authorId\":\"3887469\",\"name\":\"Yaobo Liang\"},{\"authorId\":\"46429989\",\"name\":\"N. Duan\"},{\"authorId\":\"46915168\",\"name\":\"P. Chen\"},{\"authorId\":\"46764518\",\"name\":\"Zhendong Niu\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"}],\"doi\":\"10.18653/v1/P19-1641\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea57ffa3e13400cad53dc061887a6fbbd45e7f12\",\"title\":\"Dense Procedure Captioning in Narrated Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/ea57ffa3e13400cad53dc061887a6fbbd45e7f12\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1145/3239576.3239580\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"title\":\"Video Captioning using Hierarchical Multi-Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"venue\":\"ICAIP '18\",\"year\":2018},{\"arxivId\":\"1610.06370\",\"authors\":[{\"authorId\":\"3130583\",\"name\":\"Georgios P. Spithourakis\"},{\"authorId\":\"144236487\",\"name\":\"S. Petersen\"},{\"authorId\":\"145941664\",\"name\":\"S. Riedel\"}],\"doi\":\"10.18653/v1/W16-6102\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"909d1474ef8779adc9fb1e7209a7057bd502f8e6\",\"title\":\"Clinical Text Prediction with Numerically Grounded Conditional Language Models\",\"url\":\"https://www.semanticscholar.org/paper/909d1474ef8779adc9fb1e7209a7057bd502f8e6\",\"venue\":\"Louhi@EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144514329\",\"name\":\"Tao Zeng\"},{\"authorId\":\"144022388\",\"name\":\"B. Wu\"},{\"authorId\":\"145487992\",\"name\":\"Jiayu Zhou\"},{\"authorId\":\"143763341\",\"name\":\"I. Davidson\"},{\"authorId\":\"1743600\",\"name\":\"S. Ji\"}],\"doi\":\"10.1109/ICDM.2017.156\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e114bd414b44b3174ea0a4339e0155a2c58af642\",\"title\":\"Recurrent Encoder-Decoder Networks for Time-Varying Dense Prediction\",\"url\":\"https://www.semanticscholar.org/paper/e114bd414b44b3174ea0a4339e0155a2c58af642\",\"venue\":\"2017 IEEE International Conference on Data Mining (ICDM)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004341822\",\"name\":\"Noorhan K. Fawzy\"},{\"authorId\":\"37370786\",\"name\":\"M. Marey\"},{\"authorId\":\"143987203\",\"name\":\"Mostafa Aref\"}],\"doi\":\"10.1007/978-3-030-58669-0_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18bacea52db19f69bc4933c6ad82eb5d22da281b\",\"title\":\"Video Captioning Using Attention Based Visual Fusion with Bi-temporal Context and Bi-modal Semantic Feature Learning\",\"url\":\"https://www.semanticscholar.org/paper/18bacea52db19f69bc4933c6ad82eb5d22da281b\",\"venue\":\"AISI\",\"year\":2020},{\"arxivId\":\"1511.03339\",\"authors\":[{\"authorId\":\"34192119\",\"name\":\"Liang-Chieh Chen\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"152924487\",\"name\":\"Jiang Wang\"},{\"authorId\":\"1399905857\",\"name\":\"Wei Xu\"},{\"authorId\":\"145081362\",\"name\":\"A. Yuille\"}],\"doi\":\"10.1109/CVPR.2016.396\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f48616039cb21903132528c0be5348b3019db50\",\"title\":\"Attention to Scale: Scale-Aware Semantic Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/9f48616039cb21903132528c0be5348b3019db50\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3301207\",\"name\":\"H. Cheng\"},{\"authorId\":\"143690781\",\"name\":\"S. Chung\"}],\"doi\":\"10.1007/s11042-017-4711-0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"978113287575e753cdea683f3b26097b8d315fb8\",\"title\":\"Action recognition from point cloud patches using discrete orthogonal moments\",\"url\":\"https://www.semanticscholar.org/paper/978113287575e753cdea683f3b26097b8d315fb8\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2017},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1938051940\",\"name\":\"Dylan Flaute\"},{\"authorId\":\"2405109\",\"name\":\"B. Narayanan\"}],\"doi\":\"10.1117/12.2568016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"title\":\"Video captioning using weakly supervised convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/724776b0c788c6801e48b2ba6f0b8984d9ac7a67\",\"venue\":\"Optical Engineering + Applications\",\"year\":2020},{\"arxivId\":\"1909.09602\",\"authors\":[{\"authorId\":\"1388016741\",\"name\":\"Chris Careaga\"},{\"authorId\":\"144156036\",\"name\":\"Brian Hutchinson\"},{\"authorId\":\"47312946\",\"name\":\"Nathan Hodas\"},{\"authorId\":\"21785345\",\"name\":\"L. Phillips\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de3c3e7f2a9d6b48e01f02ec452458e9f37bb6bc\",\"title\":\"Metric-Based Few-Shot Learning for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/de3c3e7f2a9d6b48e01f02ec452458e9f37bb6bc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"145546921\",\"name\":\"J. Qin\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"}],\"doi\":\"10.1007/978-3-030-01249-6_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"603bedfb454acfb5d0fdc5e91620e9be12a0d559\",\"title\":\"stagNet: An Attentive Semantic RNN for Group Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/603bedfb454acfb5d0fdc5e91620e9be12a0d559\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52423203\",\"name\":\"L. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3985dbf7616c7d2c6178eaa262389815f95290e6\",\"title\":\"Cross-view learning\",\"url\":\"https://www.semanticscholar.org/paper/3985dbf7616c7d2c6178eaa262389815f95290e6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2964097\",\"name\":\"A. Ghosh\"},{\"authorId\":\"144549904\",\"name\":\"Yash Patel\"},{\"authorId\":\"3026786\",\"name\":\"M. Sukhwani\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1007/978-3-319-46604-0_59\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd45cfc3751fa93d25766bddf66fc1d9c7f4f6d2\",\"title\":\"Dynamic Narratives for Heritage Tour\",\"url\":\"https://www.semanticscholar.org/paper/cd45cfc3751fa93d25766bddf66fc1d9c7f4f6d2\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":\"1701.03126\",\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1747615\",\"name\":\"Teng-Yok Lee\"},{\"authorId\":\"7969330\",\"name\":\"Ziming Zhang\"},{\"authorId\":\"145222187\",\"name\":\"B. Harsham\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"145441213\",\"name\":\"K. Sumi\"}],\"doi\":\"10.1109/ICCV.2017.450\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"08903ceeee6420992d30ff3f3b8b4830118af4d9\",\"title\":\"Attention-Based Multimodal Fusion for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/08903ceeee6420992d30ff3f3b8b4830118af4d9\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40497459\",\"name\":\"M. Riemer\"},{\"authorId\":\"2513113\",\"name\":\"Aditya Vempaty\"},{\"authorId\":\"144717568\",\"name\":\"F. Calmon\"},{\"authorId\":\"2462598\",\"name\":\"Fenno Terry Heath\"},{\"authorId\":\"144837668\",\"name\":\"R. Hull\"},{\"authorId\":\"1840408\",\"name\":\"Elham Khabiri\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ffa7461d32864d3bf8c73d3a45708776ef7b9c21\",\"title\":\"Correcting Forecasts with Multifactor Neural Attention\",\"url\":\"https://www.semanticscholar.org/paper/ffa7461d32864d3bf8c73d3a45708776ef7b9c21\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1608.08128\",\"authors\":[{\"authorId\":\"145346209\",\"name\":\"A. Montes\"},{\"authorId\":\"31571033\",\"name\":\"A. Salvador\"},{\"authorId\":\"1398090762\",\"name\":\"Xavier Gir\\u00f3-i-Nieto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1927afcfc5aa0d830cd867065ccb07f890b6d261\",\"title\":\"Temporal Activity Detection in Untrimmed Videos with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/1927afcfc5aa0d830cd867065ccb07f890b6d261\",\"venue\":\"NIPS 2016\",\"year\":2016},{\"arxivId\":\"1809.01348\",\"authors\":[{\"authorId\":\"3250505\",\"name\":\"Avisek Lahiri\"},{\"authorId\":\"2189227\",\"name\":\"V. Jain\"},{\"authorId\":\"46745991\",\"name\":\"Arnab Mondal\"},{\"authorId\":\"1758797\",\"name\":\"P. Biswas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"83d1830143a01343ec6e515ba9c72b8d4f35d758\",\"title\":\"Retinal Vessel Segmentation under Extreme Low Annotation: A Generative Adversarial Network Approach\",\"url\":\"https://www.semanticscholar.org/paper/83d1830143a01343ec6e515ba9c72b8d4f35d758\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50980387\",\"name\":\"Y. Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"1735257\",\"name\":\"C. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2e65f5dc2889845706555bc6b66aa42f60346a9\",\"title\":\"Talk : Generating Narrative Paragraph for Photo Stream via Bidirectional Attention Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b2e65f5dc2889845706555bc6b66aa42f60346a9\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33970300\",\"name\":\"Bor-Chun Chen\"},{\"authorId\":\"35081710\",\"name\":\"Yan-Ying Chen\"},{\"authorId\":\"27375808\",\"name\":\"Francine Chen\"}],\"doi\":\"10.5244/C.31.118\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fb3bce3a6221eb65451584efa898ecbe211bdab6\",\"title\":\"Video to Text Summary: Joint Video Summarization and Captioning with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/fb3bce3a6221eb65451584efa898ecbe211bdab6\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40685026\",\"name\":\"Duona Zhang\"},{\"authorId\":\"2911928\",\"name\":\"Wenrui Ding\"},{\"authorId\":\"1740430\",\"name\":\"Baochang Zhang\"},{\"authorId\":\"3471524\",\"name\":\"Chunyu Xie\"},{\"authorId\":\"49046516\",\"name\":\"Chunhui Liu\"},{\"authorId\":\"1783847\",\"name\":\"Jungong Han\"},{\"authorId\":\"46381969\",\"name\":\"Hongguang Li\"}],\"doi\":\"10.20944/PREPRINTS201801.0097.V1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c67f2377783352fd0152b52ea39d86997d6b9572\",\"title\":\"Heterogeneous Deep Model Fusion for Automatic Modulation Classification\",\"url\":\"https://www.semanticscholar.org/paper/c67f2377783352fd0152b52ea39d86997d6b9572\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1908.01341\",\"authors\":[{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"113515522\",\"name\":\"Zhenmei Shi\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd712a873ae4eefb6c623c8e605e42c5a0173e3e\",\"title\":\"SF-Net: Structured Feature Network for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bd712a873ae4eefb6c623c8e605e42c5a0173e3e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"153252274\",\"name\":\"A. Li\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/TIP.2019.2941267\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a97b13151ee3b3ddfc6f17c3cc04eaf827f00341\",\"title\":\"Hierarchical Recurrent Deep Fusion Using Adaptive Clip Summarization for Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/a97b13151ee3b3ddfc6f17c3cc04eaf827f00341\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"title\":\"Multi-Modal Deep Learning to Understand Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/950bb107027681a2a4b60b5c0439c3209c05a0ee\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1702.05552\",\"authors\":[{\"authorId\":\"34735743\",\"name\":\"T. Fernando\"},{\"authorId\":\"1980700\",\"name\":\"Simon Denman\"},{\"authorId\":\"1729760\",\"name\":\"S. Sridharan\"},{\"authorId\":\"3140440\",\"name\":\"C. Fookes\"}],\"doi\":\"10.1016/j.neunet.2018.09.002\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1939447e7ee0140d0e3580a27bd9d71782bc2e9c\",\"title\":\"Soft + Hardwired Attention: An LSTM Framework for Human Trajectory Prediction and Abnormal Event Detection\",\"url\":\"https://www.semanticscholar.org/paper/1939447e7ee0140d0e3580a27bd9d71782bc2e9c\",\"venue\":\"Neural Networks\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3048822\",\"name\":\"Jinyin Chen\"},{\"authorId\":\"1796810\",\"name\":\"H. Zheng\"},{\"authorId\":\"4315088\",\"name\":\"R. Chen\"},{\"authorId\":\"151491226\",\"name\":\"Hui Xiong\"}],\"doi\":\"10.1016/j.cose.2020.101916\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc3dcd83c4446d3bdae1ab053a731fdd98793642\",\"title\":\"RCA-SOC: A novel adversarial defense by refocusing on critical areas and strengthening object contours\",\"url\":\"https://www.semanticscholar.org/paper/cc3dcd83c4446d3bdae1ab053a731fdd98793642\",\"venue\":\"Comput. Secur.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"50219447\",\"name\":\"Zheng Wang\"}],\"doi\":\"10.1145/3123266.3123327\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"title\":\"Catching the Temporal Regions-of-Interest for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2002.07793\",\"authors\":[{\"authorId\":\"51451501\",\"name\":\"Zihang Lai\"},{\"authorId\":\"31803091\",\"name\":\"Erika Lu\"},{\"authorId\":\"10096695\",\"name\":\"Weidi Xie\"}],\"doi\":\"10.1109/cvpr42600.2020.00651\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"17d9185640e46a49b260a1e1e533c36a71bffdc8\",\"title\":\"MAST: A Memory-Augmented Self-Supervised Tracker\",\"url\":\"https://www.semanticscholar.org/paper/17d9185640e46a49b260a1e1e533c36a71bffdc8\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.02047\",\"authors\":[{\"authorId\":\"48404632\",\"name\":\"Felix Stahlberg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3a55804b085cb33cbe9602c0a1f3c82c100fbf25\",\"title\":\"Neural Machine Translation: A Review and Survey\",\"url\":\"https://www.semanticscholar.org/paper/3a55804b085cb33cbe9602c0a1f3c82c100fbf25\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3239379\",\"name\":\"J. Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1007/978-3-030-05710-7_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"title\":\"Hierarchical Vision-Language Alignment for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":\"1805.08191\",\"authors\":[{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144953174\",\"name\":\"Dapeng Wu\"},{\"authorId\":\"38504661\",\"name\":\"J. Wang\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"}],\"doi\":\"10.1609/aaai.v33i01.33018465\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c2b02822cfbc50d17ec5220a19556be9d601c132\",\"title\":\"Hierarchically Structured Reinforcement Learning for Topically Coherent Visual Story Generation\",\"url\":\"https://www.semanticscholar.org/paper/c2b02822cfbc50d17ec5220a19556be9d601c132\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1902.11074\",\"authors\":[{\"authorId\":\"1780829\",\"name\":\"Ning Gui\"},{\"authorId\":\"145625841\",\"name\":\"Danni Ge\"},{\"authorId\":\"5388266\",\"name\":\"Ziyin Hu\"}],\"doi\":\"10.1609/AAAI.V33I01.33013705\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"59712d0a0cc255e545ea27b2f4dc733ea709756a\",\"title\":\"AFS: An Attention-based mechanism for Supervised Feature Selection\",\"url\":\"https://www.semanticscholar.org/paper/59712d0a0cc255e545ea27b2f4dc733ea709756a\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1609/AAAI.V33I01.33018191\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"title\":\"Motion Guided Spatial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/75719b4df1cd244fe5bda0d01b9eb7e0c053857d\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48116016\",\"name\":\"J. Ren\"},{\"authorId\":\"50550351\",\"name\":\"W. Zhang\"}],\"doi\":\"10.1007/S11760-019-01449-W\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1581301ddb2b9b76c10b31eef101733ffebc46f8\",\"title\":\"CLOSE: Coupled content\\u2013semantic embedding\",\"url\":\"https://www.semanticscholar.org/paper/1581301ddb2b9b76c10b31eef101733ffebc46f8\",\"venue\":\"Signal Image Video Process.\",\"year\":2019},{\"arxivId\":\"1911.09989\",\"authors\":[{\"authorId\":\"1429191721\",\"name\":\"Menatallh Hammad\"},{\"authorId\":\"1429191719\",\"name\":\"May Hammad\"},{\"authorId\":\"31358369\",\"name\":\"M. ElShenawy\"}],\"doi\":\"10.1007/978-3-030-59830-3_21\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"title\":\"Characterizing the impact of using features extracted from pre-trained models on the quality of video captioning sequence-to-sequence models\",\"url\":\"https://www.semanticscholar.org/paper/86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"venue\":\"ICPRAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50846763\",\"name\":\"W. Zhang\"},{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"1993631009\",\"name\":\"Haizhou Shi\"},{\"authorId\":\"1490931889\",\"name\":\"Haochen Shi\"},{\"authorId\":\"145974114\",\"name\":\"Jun Xiao\"},{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"49336516\",\"name\":\"W. Wang\"}],\"doi\":\"10.1145/3394171.3413746\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fa8f78e665c037da28a7e35a98c4d2521e3f12e\",\"title\":\"Relational Graph Learning for Grounded Video Description Generation\",\"url\":\"https://www.semanticscholar.org/paper/9fa8f78e665c037da28a7e35a98c4d2521e3f12e\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1611.06492\",\"authors\":[{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"34762956\",\"name\":\"Abhinav Agarwalla\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"144240262\",\"name\":\"P. Mitra\"}],\"doi\":\"10.1109/CVPRW.2017.273\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"title\":\"Recurrent Memory Addressing for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1704.07489\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/P17-1117\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"9b08d3201af644a638e291755a5e51f6b17a51f3\",\"title\":\"Multi-Task Video Captioning with Video and Entailment Generation\",\"url\":\"https://www.semanticscholar.org/paper/9b08d3201af644a638e291755a5e51f6b17a51f3\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3206025.3206067\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"47e363b85e90760c6bb2916b47dbaaee11b62083\",\"title\":\"Class-aware Self-Attention for Audio Event Recognition\",\"url\":\"https://www.semanticscholar.org/paper/47e363b85e90760c6bb2916b47dbaaee11b62083\",\"venue\":\"ICMR\",\"year\":2018},{\"arxivId\":\"1704.01502\",\"authors\":[{\"authorId\":\"145314568\",\"name\":\"Zhiqiang Shen\"},{\"authorId\":\"46277052\",\"name\":\"J. Li\"},{\"authorId\":\"47008023\",\"name\":\"Z. Su\"},{\"authorId\":\"3700393\",\"name\":\"Minjun Li\"},{\"authorId\":\"6060281\",\"name\":\"Y. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/CVPR.2017.548\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"title\":\"Weakly Supervised Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6979be4e3acbb6a5455946dc332565ccb10cf8de\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2296971\",\"name\":\"Iftekhar Naim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c6608fdd919f2bc4f8d7412bab287527dcbcf505\",\"title\":\"Unsupervised Alignment of Natural Language with Video\",\"url\":\"https://www.semanticscholar.org/paper/c6608fdd919f2bc4f8d7412bab287527dcbcf505\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46247410\",\"name\":\"M. Ai\"},{\"authorId\":\"7767746\",\"name\":\"Yongfang Xie\"},{\"authorId\":\"104210160\",\"name\":\"Z. Tang\"},{\"authorId\":\"2097029\",\"name\":\"J. Zhang\"},{\"authorId\":\"145202887\",\"name\":\"W. Gui\"}],\"doi\":\"10.1109/TIM.2020.3026456\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d539e354ae9ec4adec19c807a6cebe6017f34c2\",\"title\":\"Two-Stream Deep Feature-Based Froth Flotation Monitoring Using Visual Attention Clues\",\"url\":\"https://www.semanticscholar.org/paper/5d539e354ae9ec4adec19c807a6cebe6017f34c2\",\"venue\":\"IEEE Transactions on Instrumentation and Measurement\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40893753\",\"name\":\"A. Dilawari\"},{\"authorId\":\"35528948\",\"name\":\"M. A. Khan\"}],\"doi\":\"10.1109/ACCESS.2019.2902507\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1fe47bbcd536bb9ce80661b17dae92f41328e91f\",\"title\":\"ASoVS: Abstractive Summarization of Video Sequences\",\"url\":\"https://www.semanticscholar.org/paper/1fe47bbcd536bb9ce80661b17dae92f41328e91f\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1704.01518\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"39578749\",\"name\":\"Siyu Tang\"},{\"authorId\":\"2390510\",\"name\":\"Seong Joon Oh\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2017.447\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db2fecc8b1bd175d39687eb471360707a5fddb03\",\"title\":\"Generating Descriptions with Grounded and Co-referenced People\",\"url\":\"https://www.semanticscholar.org/paper/db2fecc8b1bd175d39687eb471360707a5fddb03\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145910244\",\"name\":\"Bin Tong\"},{\"authorId\":\"2411436\",\"name\":\"Martin Klinkigt\"},{\"authorId\":\"2144702\",\"name\":\"M. Iwayama\"},{\"authorId\":\"144149950\",\"name\":\"T. Yanase\"},{\"authorId\":\"10249589\",\"name\":\"Y. Kobayashi\"},{\"authorId\":\"2532559\",\"name\":\"A. Sahu\"},{\"authorId\":\"2774676\",\"name\":\"Ravigopal Vennelakanti\"}],\"doi\":\"10.1145/3097983.3098132\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9879da626ea07cc27e588943e5f0d37898812adb\",\"title\":\"Learning to Generate Rock Descriptions from Multivariate Well Logs with Hierarchical Attention\",\"url\":\"https://www.semanticscholar.org/paper/9879da626ea07cc27e588943e5f0d37898812adb\",\"venue\":\"KDD\",\"year\":2017},{\"arxivId\":\"1803.11438\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"37378985\",\"name\":\"Wei Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2018.00795\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"title\":\"Reconstruction Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1706.03114\",\"authors\":[{\"authorId\":\"21496852\",\"name\":\"R. Panda\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":\"10.1109/CVPR.2017.455\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a5220d70148a900ae12864b4d028ea80fc9e094\",\"title\":\"Collaborative Summarization of Topic-Related Videos\",\"url\":\"https://www.semanticscholar.org/paper/8a5220d70148a900ae12864b4d028ea80fc9e094\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1603.06430\",\"authors\":[{\"authorId\":\"3387623\",\"name\":\"Seonwoo Min\"},{\"authorId\":\"2821668\",\"name\":\"Byunghan Lee\"},{\"authorId\":\"2999019\",\"name\":\"S. Yoon\"}],\"doi\":\"10.1093/bib/bbw068\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc384cff27d8a609f89f9e915c26bf31c39749a1\",\"title\":\"Deep learning in bioinformatics\",\"url\":\"https://www.semanticscholar.org/paper/cc384cff27d8a609f89f9e915c26bf31c39749a1\",\"venue\":\"Briefings Bioinform.\",\"year\":2017},{\"arxivId\":\"1603.08199\",\"authors\":[{\"authorId\":\"1809420\",\"name\":\"Loris Bazzani\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f3158ce4e1c9e908e7d06533d711d84205c973b9\",\"title\":\"Recurrent Mixture Density Network for Spatiotemporal Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/f3158ce4e1c9e908e7d06533d711d84205c973b9\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"26900125\",\"name\":\"Jinghao Lin\"},{\"authorId\":\"2440041\",\"name\":\"X. Jiang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"3945955\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3123266.3123364\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2411270f111a160c9289d56132651c896a5738f6\",\"title\":\"Video Question Answering via Hierarchical Dual-Level Attention Network Learning\",\"url\":\"https://www.semanticscholar.org/paper/2411270f111a160c9289d56132651c896a5738f6\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71615308\",\"name\":\"Sudatta Mohanty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0aa5e98e6d8c61e47d0477d0acd1548bb917aff5\",\"title\":\"A Deep Learning, Model-Predictive Approach to Neighborhood Congestion Prediction and Control\",\"url\":\"https://www.semanticscholar.org/paper/0aa5e98e6d8c61e47d0477d0acd1548bb917aff5\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"144613610\",\"name\":\"Miguel Domingo\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1016/j.csl.2016.12.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"166af97bb03f26cf9df6999047b0da87017dc077\",\"title\":\"Interactive neural machine translation\",\"url\":\"https://www.semanticscholar.org/paper/166af97bb03f26cf9df6999047b0da87017dc077\",\"venue\":\"Comput. Speech Lang.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2961531\",\"name\":\"M. Hosseini\"},{\"authorId\":\"145226394\",\"name\":\"F. Ghaderi\"}],\"doi\":\"10.5829/ije.2020.33.05b.29\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ec274b911cf19d45b7cb2e92bcc7e42bd70a156\",\"title\":\"A Hybrid Deep Learning Architecture Using 3D CNNs and GRUs for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2ec274b911cf19d45b7cb2e92bcc7e42bd70a156\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.13782\",\"authors\":[{\"authorId\":\"1577678641\",\"name\":\"Ganesh Samarth\"},{\"authorId\":\"1974345797\",\"name\":\"Sheetal Ojha\"},{\"authorId\":\"96566998\",\"name\":\"N. Pareek\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29df95ddda4e52ada0564458fe6f438f462b5651\",\"title\":\"Knowledge Fusion Transformers for Video Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/29df95ddda4e52ada0564458fe6f438f462b5651\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.01067\",\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"},{\"authorId\":\"2125709\",\"name\":\"Xinxiao Wu\"},{\"authorId\":\"66547993\",\"name\":\"Yayun Qi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"title\":\"Video Captioning Using Weak Annotation\",\"url\":\"https://www.semanticscholar.org/paper/aa17f11df851372402a0fa1481fdbc6af36ba2b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1604.03390\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1007/978-3-319-44781-0_1\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"799271daced99bd88b3a3e15921d5e31d9ea8323\",\"title\":\"Video Description Using Bidirectional Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/799271daced99bd88b3a3e15921d5e31d9ea8323\",\"venue\":\"ICANN\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"144934447\",\"name\":\"M. Dom\\u00ednguez\"},{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/CVPRW.2017.274\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"477d58ad32e0e54c40da135fb8db28b23ad0ffd0\",\"title\":\"Temporally Steered Gaussian Attention for Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/477d58ad32e0e54c40da135fb8db28b23ad0ffd0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153188991\",\"name\":\"Spencer Whitehead\"},{\"authorId\":\"144016781\",\"name\":\"Heng Ji\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"9546964\",\"name\":\"Shih-Fu Chang\"},{\"authorId\":\"1817166\",\"name\":\"Clare R. Voss\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1691ea87ae353949331dd3e004391162fe52e071\",\"title\":\"Event Extraction Entity Extraction and Linking Document Retrieval Entities Types coup detained Attack Arrest-Jail Events Types KaVD\",\"url\":\"https://www.semanticscholar.org/paper/1691ea87ae353949331dd3e004391162fe52e071\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"},{\"authorId\":\"2387467\",\"name\":\"J. Hershey\"}],\"doi\":\"10.1109/ASRU.2017.8268968\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a8bed0a96d9ad3e5c7ecbaaf2a8967e034e72cb3\",\"title\":\"Early and late integration of audio features for automatic video description\",\"url\":\"https://www.semanticscholar.org/paper/a8bed0a96d9ad3e5c7ecbaaf2a8967e034e72cb3\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38179026\",\"name\":\"Y. Shi\"},{\"authorId\":\"40161651\",\"name\":\"Yonghong Tian\"},{\"authorId\":\"5765799\",\"name\":\"Yaowei Wang\"},{\"authorId\":\"34097174\",\"name\":\"Tiejun Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c829be73584966e3162f7ccae72d9284a2ebf358\",\"title\":\"shuttleNet: A biologically-inspired RNN with loop connection and parameter sharing\",\"url\":\"https://www.semanticscholar.org/paper/c829be73584966e3162f7ccae72d9284a2ebf358\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"title\":\"Story Understanding through Semantic Analysis and Automatic Alignment of Text and Video\",\"url\":\"https://www.semanticscholar.org/paper/39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50244843\",\"name\":\"E. Barsoum\"}],\"doi\":\"10.7916/d8-sq89-mm29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d8d9ab14c06bbbe084232517c8a67581d16d4ef0\",\"title\":\"Human Motion Anticipation and Recognition from RGB-D\",\"url\":\"https://www.semanticscholar.org/paper/d8d9ab14c06bbbe084232517c8a67581d16d4ef0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152325076\",\"name\":\"H. Ge\"},{\"authorId\":\"79403975\",\"name\":\"Zehang Yan\"},{\"authorId\":null,\"name\":\"Wenhao Yu\"},{\"authorId\":\"144526347\",\"name\":\"L. Sun\"}],\"doi\":\"10.1007/s11042-019-7404-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9804bbb0724c333e9b900d849e492db00b20dbde\",\"title\":\"An attention mechanism based convolutional LSTM network for video action recognition\",\"url\":\"https://www.semanticscholar.org/paper/9804bbb0724c333e9b900d849e492db00b20dbde\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47382681\",\"name\":\"Peipei Song\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"26906175\",\"name\":\"H. Xin\"},{\"authorId\":\"152808609\",\"name\":\"M. Wang\"}],\"doi\":\"10.1109/ICIP.2019.8803123\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4334155929aa5903abf4464a41898d759ef78c29\",\"title\":\"Parallel Temporal Encoder For Sign Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/4334155929aa5903abf4464a41898d759ef78c29\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50111883\",\"name\":\"S. Lee\"},{\"authorId\":\"1784897\",\"name\":\"Incheol Kim\"}],\"doi\":\"10.3745/JIPS.02.0098\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc1d373ca1b7abc470bf6a6a639436ea12461378\",\"title\":\"Video Captioning with Visual and Semantic Features\",\"url\":\"https://www.semanticscholar.org/paper/fc1d373ca1b7abc470bf6a6a639436ea12461378\",\"venue\":\"J. Inf. Process. Syst.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145271111\",\"name\":\"N. Henderson\"},{\"authorId\":\"3867127\",\"name\":\"R. Aygun\"}],\"doi\":\"10.1109/ISM.2017.22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cba090a5bfae7dd8a60a973259f0870ed68c4dd3\",\"title\":\"Human Action Classification Using Temporal Slicing for Deep Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cba090a5bfae7dd8a60a973259f0870ed68c4dd3\",\"venue\":\"2017 IEEE International Symposium on Multimedia (ISM)\",\"year\":2017},{\"arxivId\":\"1908.10072\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"67074535\",\"name\":\"W. Zhang\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"46641690\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/ICCV.2019.00273\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"title\":\"Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46867157\",\"name\":\"Y. Zhang\"},{\"authorId\":\"47364599\",\"name\":\"Mingli Ding\"},{\"authorId\":\"2860057\",\"name\":\"Yancheng Bai\"},{\"authorId\":\"48928816\",\"name\":\"Dandan Liu\"},{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"}],\"doi\":\"10.1016/j.patrec.2019.10.005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d277ff82da3cb6bdfe9987f1be12998cccbf3c37\",\"title\":\"Learning a strong detector for action localization in videos\",\"url\":\"https://www.semanticscholar.org/paper/d277ff82da3cb6bdfe9987f1be12998cccbf3c37\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":\"1811.04869\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/CVPR.2019.00129\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"063ba2f8f6f1624f42de4e9bee0ac5ae6ce06032\",\"title\":\"A Perceptual Prediction Framework for Self Supervised Event Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/063ba2f8f6f1624f42de4e9bee0ac5ae6ce06032\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1511.02917\",\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"4240351\",\"name\":\"Jonathan Huang\"},{\"authorId\":\"1389570466\",\"name\":\"Sami Abu-El-Haija\"},{\"authorId\":\"116230588\",\"name\":\"A. N. Gorban\"},{\"authorId\":\"145601650\",\"name\":\"K. Murphy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.332\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"195df1106f4d7aff0e9cb609358abbf80f54a716\",\"title\":\"Detecting Events and Key Actors in Multi-person Videos\",\"url\":\"https://www.semanticscholar.org/paper/195df1106f4d7aff0e9cb609358abbf80f54a716\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394741222\",\"name\":\"Yuling Gui\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"97522088\",\"name\":\"Ye Zhao\"}],\"doi\":\"10.1145/3347319.3356839\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"title\":\"Semantic Enhanced Encoder-Decoder Network (SEN) for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1511.04670\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9ed7d774684a1770445c1c53e276011a8364b9e2\",\"title\":\"Uncovering Temporal Context for Video Question and Answering\",\"url\":\"https://www.semanticscholar.org/paper/9ed7d774684a1770445c1c53e276011a8364b9e2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"},{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"}],\"doi\":\"10.21437/Interspeech.2016-380\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"title\":\"Generating Natural Video Descriptions via Multimodal Processing\",\"url\":\"https://www.semanticscholar.org/paper/2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"venue\":\"INTERSPEECH\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"1914578421\",\"name\":\"Sabarish Gopalakrishnan\"},{\"authorId\":\"1404315481\",\"name\":\"Raymond Ptucha\"}],\"doi\":\"10.1117/1.JEI.29.2.023013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"360d84f0649d80d3b96846c1cd958b6c54332835\",\"title\":\"Aligned attention for common multimodal embeddings\",\"url\":\"https://www.semanticscholar.org/paper/360d84f0649d80d3b96846c1cd958b6c54332835\",\"venue\":\"J. Electronic Imaging\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145302989\",\"name\":\"P. Novais\"},{\"authorId\":\"143869061\",\"name\":\"David Camacho\"},{\"authorId\":\"1709042\",\"name\":\"H. Yin\"}],\"doi\":\"10.1007/978-3-030-62362-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ef549fbd0aa3c6b0cd3cdbc01320e53e72ded9f\",\"title\":\"Intelligent Data Engineering and Automated Learning \\u2013 IDEAL 2020: 21st International Conference, Guimaraes, Portugal, November 4\\u20136, 2020, Proceedings, Part I\",\"url\":\"https://www.semanticscholar.org/paper/2ef549fbd0aa3c6b0cd3cdbc01320e53e72ded9f\",\"venue\":\"IDEAL\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"3231991\",\"name\":\"Byeongchang Kim\"},{\"authorId\":\"2841633\",\"name\":\"Hyunmin Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.18653/v1/N19-1011\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c4798919e74411d87f7745840e45b8bcf61128ff\",\"title\":\"AudioCaps: Generating Captions for Audios in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/c4798919e74411d87f7745840e45b8bcf61128ff\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3387623\",\"name\":\"Seonwoo Min\"},{\"authorId\":\"22813551\",\"name\":\"Byung-Han Lee\"},{\"authorId\":\"2999019\",\"name\":\"S. Yoon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb18eca2be1badfd0b53e9f72285903823c6fee1\",\"title\":\"Deep learning : a brief overview\",\"url\":\"https://www.semanticscholar.org/paper/eb18eca2be1badfd0b53e9f72285903823c6fee1\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144364295\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"1720488\",\"name\":\"Zhongfei Zhang\"},{\"authorId\":\"48669017\",\"name\":\"Siyu Huang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"title\":\"TVT: Two-View Transformer Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/2c3c72fffcbbf66cbb649b64aa51199722140ad1\",\"venue\":\"ACML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"153489843\",\"name\":\"T. Chen\"},{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"145909567\",\"name\":\"Wanli Yu\"}],\"doi\":\"10.1109/ACCESS.2020.3010872\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"title\":\"Attention-Based Convolutional LSTM for Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2077146\",\"name\":\"Francesco Visin\"},{\"authorId\":\"2182706\",\"name\":\"Kyle Kastner\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"145927530\",\"name\":\"M. Matteucci\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7654ed26f0c3ce38953d5c595105d3380bd09ccb\",\"title\":\"ReSeg: A Recurrent Neural Network for Object Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/7654ed26f0c3ce38953d5c595105d3380bd09ccb\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1705.09436\",\"authors\":[{\"authorId\":\"15629035\",\"name\":\"Daksh Varshneya\"},{\"authorId\":\"2228446\",\"name\":\"G. Srinivasaraghavan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"426c39358d592be673b72d26dad4560b06ec5d89\",\"title\":\"Human Trajectory Prediction using Spatially aware Deep Attention Models\",\"url\":\"https://www.semanticscholar.org/paper/426c39358d592be673b72d26dad4560b06ec5d89\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1707.05501\",\"authors\":[{\"authorId\":\"2719746\",\"name\":\"Parag Jain\"},{\"authorId\":\"7421228\",\"name\":\"Priyanka Agrawal\"},{\"authorId\":\"1746093\",\"name\":\"A. Mishra\"},{\"authorId\":\"3026786\",\"name\":\"M. Sukhwani\"},{\"authorId\":\"2039596\",\"name\":\"Anirban Laha\"},{\"authorId\":\"145590185\",\"name\":\"K. Sankaranarayanan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3740df4c6e7144e2c1bc441e18fa4a996c9d57b9\",\"title\":\"Story Generation from Sequence of Independent Short Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/3740df4c6e7144e2c1bc441e18fa4a996c9d57b9\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1708.02478\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TNNLS.2018.2851077\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d78c47093fbf3d85225fd502674aba4a29b3987\",\"title\":\"From Deterministic to Generative: Multimodal Stochastic RNNs for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7d78c47093fbf3d85225fd502674aba4a29b3987\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2511637\",\"name\":\"Hongyang Xue\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1109/TIP.2017.2746267\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1b94c49c119c7490d2df6a2dd093e5ddd8bfba14\",\"title\":\"Unifying the Video and Question Attentions for Open-Ended Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/1b94c49c119c7490d2df6a2dd093e5ddd8bfba14\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2786437\",\"name\":\"Linghui Li\"},{\"authorId\":\"46321465\",\"name\":\"Sheng Tang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"4303531\",\"name\":\"Lixi Deng\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TMM.2017.2751140\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2e0f1c89c4e099b14c4d77bd406be9f7b78d6f6d\",\"title\":\"GLA: Global\\u2013Local Attention for Image Description\",\"url\":\"https://www.semanticscholar.org/paper/2e0f1c89c4e099b14c4d77bd406be9f7b78d6f6d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":\"1803.07179\",\"authors\":[{\"authorId\":\"14800230\",\"name\":\"J. Zang\"},{\"authorId\":\"46659696\",\"name\":\"L. Wang\"},{\"authorId\":\"9071789\",\"name\":\"Z. Liu\"},{\"authorId\":\"46324995\",\"name\":\"Q. Zhang\"},{\"authorId\":\"1786361\",\"name\":\"Zhenxing Niu\"},{\"authorId\":\"144988570\",\"name\":\"Gang Hua\"},{\"authorId\":\"1715389\",\"name\":\"Nanning Zheng\"}],\"doi\":\"10.1007/978-3-319-92007-8_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd613000f7b2b6161548b1c1044ab46c7327a901\",\"title\":\"Attention-based Temporal Weighted Convolutional Neural Network for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/dd613000f7b2b6161548b1c1044ab46c7327a901\",\"venue\":\"AIAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49813626\",\"name\":\"Y. Guo\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"eecfaf49500434d91970b24831081d5d2c68697e\",\"title\":\"Sequence to Sequence Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/eecfaf49500434d91970b24831081d5d2c68697e\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"31ea3186aa7072a9e25218efe229f5ee3cca3316\",\"title\":\"A ug 2 01 7 Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/31ea3186aa7072a9e25218efe229f5ee3cca3316\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"101361574\",\"name\":\"Markus Polleryd\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8646eb13471008b07b0c762768fe558ca495bf89\",\"title\":\"Convoluted Events Neutron Reconstruction using Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/8646eb13471008b07b0c762768fe558ca495bf89\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48632022\",\"name\":\"Xin Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1b47776ecc194616d5ae789357ac69b1298e47ae\",\"title\":\"Frames CNN Low-level Encoder ( Bi-LSTM ) High-level Encoder ( LSTM ) Worker Manager Internal Critic Environment segment signal goal state reward action HRL Agent context context\",\"url\":\"https://www.semanticscholar.org/paper/1b47776ecc194616d5ae789357ac69b1298e47ae\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5387396\",\"name\":\"Huidong Li\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"},{\"authorId\":\"3000498\",\"name\":\"Lejian Liao\"},{\"authorId\":\"151479762\",\"name\":\"Cuimei Peng\"}],\"doi\":\"10.1109/ICME.2019.00228\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1567fff9f411af320f678c66b812d2c963151678\",\"title\":\"REVnet: Bring Reviewing Into Video Captioning for a Better Description\",\"url\":\"https://www.semanticscholar.org/paper/1567fff9f411af320f678c66b812d2c963151678\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"title\":\"Beyond Labels and Captions: Contextualizing Grounded Semantics for Explainable Visual Interpretation\",\"url\":\"https://www.semanticscholar.org/paper/267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48404632\",\"name\":\"Felix Stahlberg\"}],\"doi\":\"10.17863/CAM.49422\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"38b40ae531ddca434de07015637b78413370d15a\",\"title\":\"The Roles of Language Models and Hierarchical Models in Neural Sequence-to-Sequence Prediction\",\"url\":\"https://www.semanticscholar.org/paper/38b40ae531ddca434de07015637b78413370d15a\",\"venue\":\"EAMT\",\"year\":2020},{\"arxivId\":\"1708.07590\",\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"33830793\",\"name\":\"J. Smith\"},{\"authorId\":\"40178769\",\"name\":\"Wenjin Lu\"},{\"authorId\":\"1782912\",\"name\":\"B. Zhang\"}],\"doi\":\"10.1016/j.image.2017.11.005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1d6cd361b70a30a0ed8bd9c4bc65685e296647da\",\"title\":\"Hierarchical Multi-scale Attention Networks for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/1d6cd361b70a30a0ed8bd9c4bc65685e296647da\",\"venue\":\"Signal Process. Image Commun.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ICIP.2019.8803785\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"da0cce8d791ca90b01696c6ef0de96c7904dd8cf\",\"title\":\"A Novel Attribute Selection Mechanism for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/da0cce8d791ca90b01696c6ef0de96c7904dd8cf\",\"venue\":\"2019 IEEE International Conference on Image Processing (ICIP)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52201852\",\"name\":\"Eleftherios Daskalakis\"},{\"authorId\":\"2817419\",\"name\":\"Maria Tzelepi\"},{\"authorId\":\"1737071\",\"name\":\"A. Tefas\"}],\"doi\":\"10.1016/j.patrec.2018.09.022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fc1332023c370dc55fabb1b6c895af1a5f48f889\",\"title\":\"Learning deep spatiotemporal features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/fc1332023c370dc55fabb1b6c895af1a5f48f889\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1410507241\",\"name\":\"Joseph Cahill-Lane\"},{\"authorId\":\"1754999\",\"name\":\"S. Mills\"}],\"doi\":\"10.1109/IVCNZ.2017.8402453\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43cdec7e1a0f3dab0e70a80aacb4a89085fab3f4\",\"title\":\"Of mice, men, and machines: Real and artificial deep networks for vision\",\"url\":\"https://www.semanticscholar.org/paper/43cdec7e1a0f3dab0e70a80aacb4a89085fab3f4\",\"venue\":\"2017 International Conference on Image and Vision Computing New Zealand (IVCNZ)\",\"year\":2017},{\"arxivId\":\"1906.03683\",\"authors\":[{\"authorId\":\"144015229\",\"name\":\"Kuan-Hui Lee\"},{\"authorId\":\"1914418\",\"name\":\"Takaaki Tagawa\"},{\"authorId\":\"147128583\",\"name\":\"Jia-En M. Pan\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"2674833\",\"name\":\"B. Douillard\"}],\"doi\":\"10.1109/IVS.2019.8814278\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c7437cce0417848d1492f980daa8742e71131cdf\",\"title\":\"An Attention-based Recurrent Convolutional Network for Vehicle Taillight Recognition\",\"url\":\"https://www.semanticscholar.org/paper/c7437cce0417848d1492f980daa8742e71131cdf\",\"venue\":\"2019 IEEE Intelligent Vehicles Symposium (IV)\",\"year\":2019},{\"arxivId\":\"1809.10267\",\"authors\":[{\"authorId\":\"48935207\",\"name\":\"C. Zhang\"},{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"38916638\",\"name\":\"Dheeraj Peri\"},{\"authorId\":\"34679323\",\"name\":\"A. Loui\"},{\"authorId\":\"2879097\",\"name\":\"C. Salvaggio\"},{\"authorId\":\"1749833\",\"name\":\"R. Ptucha\"}],\"doi\":\"10.1109/GlobalSIP.2017.8309051\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"080ea4c468a982c73a7741abe59da5255c7d2c38\",\"title\":\"Semantic sentence embeddings for paraphrasing and text summarization\",\"url\":\"https://www.semanticscholar.org/paper/080ea4c468a982c73a7741abe59da5255c7d2c38\",\"venue\":\"2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)\",\"year\":2017},{\"arxivId\":\"1705.00754\",\"authors\":[{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"1382195702\",\"name\":\"Kenji Hata\"},{\"authorId\":\"3260219\",\"name\":\"F. Ren\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/ICCV.2017.83\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"title\":\"Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/96dd1fc39a368d23291816d57763bc6eb4f7b8d6\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740650\",\"name\":\"Jian Jhen Chen\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"2838253\",\"name\":\"C. He\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3132847.3132922\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3085671f6232aac4492ad861d09334b8f3a7e2a7\",\"title\":\"Movie Fill in the Blank with Adaptive Temporal Attention and Description Update\",\"url\":\"https://www.semanticscholar.org/paper/3085671f6232aac4492ad861d09334b8f3a7e2a7\",\"venue\":\"CIKM\",\"year\":2017},{\"arxivId\":\"1812.05634\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1109/CVPR.2019.00676\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"title\":\"Adversarial Inference for Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/6aa6932c22b9bd407e615ec2bfffc20cd88a9069\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28909797\",\"name\":\"Xiyao Fu\"},{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":null,\"name\":\"Zheng Wang\"},{\"authorId\":\"143760554\",\"name\":\"Q. Wei\"},{\"authorId\":\"47336231\",\"name\":\"S. Chen\"}],\"doi\":\"10.1007/978-3-030-26075-0_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"42c6c030ea35449ee824cbab20031add67d1f999\",\"title\":\"Supervised Hashing with Recurrent Scaling\",\"url\":\"https://www.semanticscholar.org/paper/42c6c030ea35449ee824cbab20031add67d1f999\",\"venue\":\"APWeb/WAIM\",\"year\":2019},{\"arxivId\":\"1804.00100\",\"authors\":[{\"authorId\":null,\"name\":\"Jingwen Wang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"}],\"doi\":\"10.1109/CVPR.2018.00751\",\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"title\":\"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1811.12043\",\"authors\":[{\"authorId\":\"3098768\",\"name\":\"Junhyuk Kim\"},{\"authorId\":\"1704860\",\"name\":\"J. Choi\"},{\"authorId\":\"35997085\",\"name\":\"M. Cheon\"},{\"authorId\":\"49685399\",\"name\":\"J. Lee\"}],\"doi\":\"10.1016/j.neucom.2020.03.069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fa0c50368a6709f764ea87228c092a9f06cf1e0b\",\"title\":\"MAMNet: Multi-path adaptive modulation network for image super-resolution\",\"url\":\"https://www.semanticscholar.org/paper/fa0c50368a6709f764ea87228c092a9f06cf1e0b\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1921534\",\"name\":\"Philip Kinghorn\"},{\"authorId\":\"41204462\",\"name\":\"L. Zhang\"},{\"authorId\":\"144082425\",\"name\":\"L. Shao\"}],\"doi\":\"10.1109/IJCNN.2017.7965950\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"abe4c1d6b964c4f5443b0334a44f0b03dd1909f4\",\"title\":\"Deep learning based image description generation\",\"url\":\"https://www.semanticscholar.org/paper/abe4c1d6b964c4f5443b0334a44f0b03dd1909f4\",\"venue\":\"2017 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2017},{\"arxivId\":\"1703.04096\",\"authors\":[{\"authorId\":\"3431029\",\"name\":\"Y. Dong\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"145254043\",\"name\":\"J. Zhu\"},{\"authorId\":\"49846744\",\"name\":\"Bo Zhang\"}],\"doi\":\"10.1109/CVPR.2017.110\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ebfca6a48dde396e2274caa9f15389fdbf08fd12\",\"title\":\"Improving Interpretability of Deep Neural Networks with Semantic Information\",\"url\":\"https://www.semanticscholar.org/paper/ebfca6a48dde396e2274caa9f15389fdbf08fd12\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"James Thewlis\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4fc560a78e48bd927c2f6253f5fc489927bb15f\",\"title\":\"Detecting objects in internet videos AIMS Project Report\",\"url\":\"https://www.semanticscholar.org/paper/e4fc560a78e48bd927c2f6253f5fc489927bb15f\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"77537913\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/s11042-019-07948-9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fea09a15a91558db9090c4cb4b11184b91310839\",\"title\":\"Deep learning ensemble with data augmentation using a transcoder in visual description\",\"url\":\"https://www.semanticscholar.org/paper/fea09a15a91558db9090c4cb4b11184b91310839\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":\"1911.11306\",\"authors\":[{\"authorId\":\"2537286\",\"name\":\"H. Eun\"},{\"authorId\":\"50112704\",\"name\":\"Sumin Lee\"},{\"authorId\":\"3035146\",\"name\":\"Jinyoung Moon\"},{\"authorId\":\"2800227\",\"name\":\"Jongyoul Park\"},{\"authorId\":\"2024203\",\"name\":\"Chanho Jung\"},{\"authorId\":\"145568138\",\"name\":\"Changick Kim\"}],\"doi\":\"10.1109/TCSVT.2019.2953187\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"13aa627f35de78af64d1861fceb97c834a769b05\",\"title\":\"SRG: Snippet Relatedness-Based Temporal Action Proposal Generator\",\"url\":\"https://www.semanticscholar.org/paper/13aa627f35de78af64d1861fceb97c834a769b05\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1806.01954\",\"authors\":[{\"authorId\":\"51011850\",\"name\":\"Iulia Duta\"},{\"authorId\":\"50986865\",\"name\":\"A. Nicolicioiu\"},{\"authorId\":\"9947219\",\"name\":\"Simion-Vlad Bogolin\"},{\"authorId\":\"1749627\",\"name\":\"M. Leordeanu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c42f427b54ab12a1d89827ee4c6951efae733b55\",\"title\":\"Mining for meaning: from vision to language through multiple networks consensus\",\"url\":\"https://www.semanticscholar.org/paper/c42f427b54ab12a1d89827ee4c6951efae733b55\",\"venue\":\"BMVC\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886528\",\"name\":\"Guolong Wang\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"},{\"authorId\":\"2168639\",\"name\":\"Kaiping Xu\"},{\"authorId\":\"145489794\",\"name\":\"K. Huang\"},{\"authorId\":\"19204816\",\"name\":\"Shuxiong Ye\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"title\":\"Bridge Video and Text with Cascade Syntactic Structure\",\"url\":\"https://www.semanticscholar.org/paper/b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144861485\",\"name\":\"Z. Ma\"},{\"authorId\":\"3261741\",\"name\":\"Z. Sun\"}],\"doi\":\"10.1007/s11042-018-6260-6\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8613fef2738325a5697e253276b160099100528d\",\"title\":\"Time-varying LSTM networks for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/8613fef2738325a5697e253276b160099100528d\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31635336\",\"name\":\"E. Matsuo\"},{\"authorId\":\"3236658\",\"name\":\"I. Kobayashi\"},{\"authorId\":\"37572653\",\"name\":\"S. Nishimoto\"},{\"authorId\":\"32224191\",\"name\":\"Satoshi Nishida\"},{\"authorId\":\"7142317\",\"name\":\"Hideki Asoh\"}],\"doi\":\"10.18653/v1/P16-3004\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d783b56a3d17f95a9a0b2bf76f3ebf285e16ca8\",\"title\":\"Generating Natural Language Descriptions for Semantic Representations of Human Brain Activity\",\"url\":\"https://www.semanticscholar.org/paper/8d783b56a3d17f95a9a0b2bf76f3ebf285e16ca8\",\"venue\":\"ACL\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2443605\",\"name\":\"Deqiang Ouyang\"},{\"authorId\":\"48380035\",\"name\":\"Y. Zhang\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"}],\"doi\":\"10.1016/j.patrec.2018.05.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4963c9d2b56eafaa9c84dccabb97f0db28f19473\",\"title\":\"Video-based person re-identification via spatio-temporal attentional and two-stream fusion convolutional networks\",\"url\":\"https://www.semanticscholar.org/paper/4963c9d2b56eafaa9c84dccabb97f0db28f19473\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47908694\",\"name\":\"Y. Liu\"},{\"authorId\":\"50535300\",\"name\":\"Jianqiang Huang\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"},{\"authorId\":\"143863243\",\"name\":\"X. Hua\"}],\"doi\":\"10.1145/3126686.3126705\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ce11b2d7905d2955c4282db5b68482edb846f29f\",\"title\":\"Spatiotemporal Multi-Task Network for Human Activity Understanding\",\"url\":\"https://www.semanticscholar.org/paper/ce11b2d7905d2955c4282db5b68482edb846f29f\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"2006.14262\",\"authors\":[{\"authorId\":\"2973730\",\"name\":\"Chiranjib Sur\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"title\":\"SACT: Self-Aware Multi-Space Feature Composition Transformer for Multinomial Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/55c4cf3ed07f594a1826e604a875d7a2713a35e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152498392\",\"name\":\"Eldar \\u0160abanovi\\u010d\"}],\"doi\":\"10.20334/2019-051-m\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"45adc0e955daeccf8bf4b6da45e7c272ccaf27b6\",\"title\":\"S\\u0105s\\u016bkos dirbtini\\u0173 neuron\\u0173 tinkl\\u0173 \\u012fgyvendinimas spartinan\\u010diuosiuose \\u012frenginiuose vaizdams analizuoti realiuoju laiku\",\"url\":\"https://www.semanticscholar.org/paper/45adc0e955daeccf8bf4b6da45e7c272ccaf27b6\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1805.12098\",\"authors\":[{\"authorId\":\"46192189\",\"name\":\"Man-Chin Sun\"},{\"authorId\":\"2240375\",\"name\":\"Shih-Huan Hsu\"},{\"authorId\":\"2013424\",\"name\":\"Min-Chun Yang\"},{\"authorId\":\"2296300\",\"name\":\"Jen-Hsien Chien\"}],\"doi\":\"10.1109/ACIIASIA.2018.8470372\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df4e31f00c6c27e1a76df1cc539696faff384fb6\",\"title\":\"Context-aware Cascade Attention-based RNN for Video Emotion Recognition\",\"url\":\"https://www.semanticscholar.org/paper/df4e31f00c6c27e1a76df1cc539696faff384fb6\",\"venue\":\"2018 First Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47952527\",\"name\":\"G. Tsagkatakis\"},{\"authorId\":\"40165419\",\"name\":\"M. Jaber\"},{\"authorId\":\"1694755\",\"name\":\"P. Tsakalides\"}],\"doi\":\"10.2352/ISSN.2470-1173.2017.16.CVAS-344\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"30b515cc19bd8fed04bea6ea152f284525671f60\",\"title\":\"Goal!! Event detection in sports video\",\"url\":\"https://www.semanticscholar.org/paper/30b515cc19bd8fed04bea6ea152f284525671f60\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"3493516\",\"name\":\"Yifan Xiong\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/2964284.2984065\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7492cac0babe8d514995bcde6456ae00c17325a3\",\"title\":\"Describing Videos using Multi-modal Fusion\",\"url\":\"https://www.semanticscholar.org/paper/7492cac0babe8d514995bcde6456ae00c17325a3\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30050286\",\"name\":\"Abdulmajid Murad\"},{\"authorId\":\"1739676\",\"name\":\"J. Pyun\"}],\"doi\":\"10.3390/s17112556\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"899db02ba28ef2479b5bea3e51627685be5b3865\",\"title\":\"Deep Recurrent Neural Networks for Human Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/899db02ba28ef2479b5bea3e51627685be5b3865\",\"venue\":\"Sensors\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"65767906\",\"name\":\"Silvio Olivastri\"},{\"authorId\":\"1931660\",\"name\":\"Gurkirt Singh\"},{\"authorId\":\"1754181\",\"name\":\"Fabio Cuzzolin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"20f3d85b99b4b595b1c92f60a9b9a126f7384e15\",\"title\":\"An End-to-End Baseline for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20f3d85b99b4b595b1c92f60a9b9a126f7384e15\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98419684\",\"name\":\"Phil Kinghorn\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9549e8e59f5ce436cc4d81ddb859b3295d131c5b\",\"title\":\"Deep learning-based regional image caption generation with refined descriptions\",\"url\":\"https://www.semanticscholar.org/paper/9549e8e59f5ce436cc4d81ddb859b3295d131c5b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151178129\",\"name\":\"Quan Kong\"},{\"authorId\":\"151480716\",\"name\":\"Ziming Wu\"},{\"authorId\":\"90812043\",\"name\":\"Z. Deng\"},{\"authorId\":\"2411436\",\"name\":\"Martin Klinkigt\"},{\"authorId\":\"145910244\",\"name\":\"Bin Tong\"},{\"authorId\":\"2668511\",\"name\":\"T. Murakami\"}],\"doi\":\"10.1109/ICCV.2019.00875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6d5fd1c40604ebbda2de58b29fbdaa97745ce7d6\",\"title\":\"MMAct: A Large-Scale Dataset for Cross Modal Human Action Understanding\",\"url\":\"https://www.semanticscholar.org/paper/6d5fd1c40604ebbda2de58b29fbdaa97745ce7d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/TIP.2020.2988435\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"title\":\"Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"1804.00819\",\"authors\":[{\"authorId\":\"2677364\",\"name\":\"Luowei Zhou\"},{\"authorId\":\"34872128\",\"name\":\"Yingbo Zhou\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"}],\"doi\":\"10.1109/CVPR.2018.00911\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"35ed258aede3df17ee20a6635364cb5fd2461049\",\"title\":\"End-to-End Dense Video Captioning with Masked Transformer\",\"url\":\"https://www.semanticscholar.org/paper/35ed258aede3df17ee20a6635364cb5fd2461049\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2857402\",\"name\":\"H. Park\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/ICASSP.2017.7952660\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7488e76dd3a5c7bf78768a7b434a657ec7374920\",\"title\":\"Melody extraction and detection through LSTM-RNN with harmonic sum loss\",\"url\":\"https://www.semanticscholar.org/paper/7488e76dd3a5c7bf78768a7b434a657ec7374920\",\"venue\":\"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2017},{\"arxivId\":\"1905.08181\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.18653/v1/P19-3014\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0997497ee345083379e320233415f1f388f6aa25\",\"title\":\"A Neural, Interactive-predictive System for Multimodal Sequence to Sequence Tasks\",\"url\":\"https://www.semanticscholar.org/paper/0997497ee345083379e320233415f1f388f6aa25\",\"venue\":\"ACL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46958420\",\"name\":\"Tianyi Wang\"},{\"authorId\":\"47539594\",\"name\":\"Jiang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1007/978-3-030-00776-8_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c50c91875767ec7c6391d99d30838d90275a0f1b\",\"title\":\"Collaborative Detection and Caption Network\",\"url\":\"https://www.semanticscholar.org/paper/c50c91875767ec7c6391d99d30838d90275a0f1b\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47569376\",\"name\":\"Shijie Yang\"},{\"authorId\":\"73596205\",\"name\":\"L. Li\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"49356099\",\"name\":\"Dechao Meng\"},{\"authorId\":\"153159021\",\"name\":\"Qingming Huang\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1145/3343031.3350859\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"title\":\"Structured Stochastic Recurrent Network for Linguistic Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/952a74f839536bd7668acb8d65086b4f4e3a4dee\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.cviu.2019.102840\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b4c920c72a32196c1dd4aa18984f7b3c0b77861\",\"title\":\"Video captioning using boosted and parallel Long Short-Term Memory networks\",\"url\":\"https://www.semanticscholar.org/paper/1b4c920c72a32196c1dd4aa18984f7b3c0b77861\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152281902\",\"name\":\"S. Wu\"},{\"authorId\":\"2087470\",\"name\":\"S. Fan\"},{\"authorId\":\"1700911\",\"name\":\"Zhiqi Shen\"},{\"authorId\":\"145977143\",\"name\":\"Mohan S. Kankanhalli\"},{\"authorId\":\"1699730\",\"name\":\"Anthony K. H. Tung\"}],\"doi\":\"10.1145/3394171.3413589\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"35ba6ed07ef68db187674498e684de7f3e160716\",\"title\":\"Who You Are Decides How You Tell\",\"url\":\"https://www.semanticscholar.org/paper/35ba6ed07ef68db187674498e684de7f3e160716\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2006.13608\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"50144812\",\"name\":\"Z. Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394486.3403325\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d271e93c7566b231e560c48b4cc4942077d762f9\",\"title\":\"Comprehensive Information Integration Modeling Framework for Video Titling\",\"url\":\"https://www.semanticscholar.org/paper/d271e93c7566b231e560c48b4cc4942077d762f9\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1515829486\",\"name\":\"Aditya Raj\"},{\"authorId\":\"1481712080\",\"name\":\"Pooja Consul\"},{\"authorId\":\"2000301595\",\"name\":\"Sakar K. Pal\"}],\"doi\":\"10.1007/978-3-030-55180-3_34\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"482cf0c7ffdbb13eb5a62cacafca2a253064ac75\",\"title\":\"Fast Neural Accumulator (NAC) Based Badminton Video Action Classification\",\"url\":\"https://www.semanticscholar.org/paper/482cf0c7ffdbb13eb5a62cacafca2a253064ac75\",\"venue\":\"IntelliSys\",\"year\":2020},{\"arxivId\":\"1807.03658\",\"authors\":[{\"authorId\":\"8668622\",\"name\":\"Xiangxi Shi\"},{\"authorId\":\"1688642\",\"name\":\"J. Cai\"},{\"authorId\":\"2174964\",\"name\":\"Jiuxiang Gu\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"}],\"doi\":\"10.1016/j.neucom.2020.08.035\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"title\":\"Video Captioning with Boundary-aware Hierarchical Language Decoding and Joint Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/c5eda56ee3714e9cd0a8c0fb043341d1ddc1604d\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1708.04301\",\"authors\":[{\"authorId\":\"144909180\",\"name\":\"H. Hosseini\"},{\"authorId\":\"2797515\",\"name\":\"Baicen Xiao\"},{\"authorId\":\"145280190\",\"name\":\"A. Clark\"},{\"authorId\":\"144786412\",\"name\":\"R. Poovendran\"}],\"doi\":\"10.1145/3137616.3137618\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a1832c646a98e4794e8408cb23593e8e049d9351\",\"title\":\"Attacking Automatic Video Analysis Algorithms: A Case Study of Google Cloud Video Intelligence API\",\"url\":\"https://www.semanticscholar.org/paper/a1832c646a98e4794e8408cb23593e8e049d9351\",\"venue\":\"MPS@CCS\",\"year\":2017},{\"arxivId\":\"1708.02977\",\"authors\":[{\"authorId\":\"1714982\",\"name\":\"Licheng Yu\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.18653/v1/D17-1101\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b0e404fd1adaa0fa7b1ef12b4b828db3d497ab1c\",\"title\":\"Hierarchically-Attentive RNN for Album Summarization and Storytelling\",\"url\":\"https://www.semanticscholar.org/paper/b0e404fd1adaa0fa7b1ef12b4b828db3d497ab1c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47897687\",\"name\":\"H. Tan\"},{\"authorId\":\"47297550\",\"name\":\"Hongyuan Zhu\"},{\"authorId\":\"6516914\",\"name\":\"J. Lim\"},{\"authorId\":\"1694051\",\"name\":\"Cheston Tan\"}],\"doi\":\"10.1016/J.CVIU.2020.103107\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d79912ee3469c1c93f2c726f9a9b4baf7682ec5f\",\"title\":\"A comprehensive survey of procedural video datasets\",\"url\":\"https://www.semanticscholar.org/paper/d79912ee3469c1c93f2c726f9a9b4baf7682ec5f\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741435593\",\"name\":\"Fei Fang\"},{\"authorId\":\"1808151\",\"name\":\"Fei Luo\"},{\"authorId\":\"1739347467\",\"name\":\"H. Zhang\"},{\"authorId\":\"1739176380\",\"name\":\"Hua-Jian Zhou\"},{\"authorId\":\"2832051\",\"name\":\"Alix L. H. Chow\"},{\"authorId\":\"2420700\",\"name\":\"Chunxia Xiao\"}],\"doi\":\"10.1007/s11390-020-0305-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d8ca36ad467ac24b1538ccafabd2daaadd2c2d9\",\"title\":\"A Comprehensive Pipeline for Complex Text-to-Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/5d8ca36ad467ac24b1538ccafabd2daaadd2c2d9\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2020},{\"arxivId\":\"2007.14682\",\"authors\":[{\"authorId\":\"1840585237\",\"name\":\"Philipp Rimle\"},{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"143720818\",\"name\":\"M. Gro\\u00df\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"title\":\"Enriching Video Captions With Contextual Text\",\"url\":\"https://www.semanticscholar.org/paper/f8cf7fdf3f9595f7841ea2e128f569e23f99468f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39796129\",\"name\":\"Hayden Faulkner\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":\"10.1109/DICTA.2017.8227494\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"title\":\"TenniSet: A Dataset for Dense Fine-Grained Event Recognition, Localisation and Description\",\"url\":\"https://www.semanticscholar.org/paper/4ee94572ae1d9c090fe81baa7236c7efbe1ca5b4\",\"venue\":\"2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2378843\",\"name\":\"L. Lu\"},{\"authorId\":\"1932096\",\"name\":\"Huijun Di\"},{\"authorId\":\"1803391\",\"name\":\"Y. Lu\"},{\"authorId\":\"47058844\",\"name\":\"Lin Zhang\"},{\"authorId\":\"9437193\",\"name\":\"Shunzhou Wang\"}],\"doi\":\"10.1016/j.neucom.2018.09.060\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ccb80471d2bb2236b7f2c37ff9159e9b0d082474\",\"title\":\"A two-level attention-based interaction model for multi-person activity recognition\",\"url\":\"https://www.semanticscholar.org/paper/ccb80471d2bb2236b7f2c37ff9159e9b0d082474\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":\"1705.00930\",\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"1826179\",\"name\":\"Yuan-Hong Liao\"},{\"authorId\":\"8551209\",\"name\":\"Ching-Yao Chuang\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1109/ICCV.2017.64\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"title\":\"Show, Adapt and Tell: Adversarial Training of Cross-Domain Image Captioner\",\"url\":\"https://www.semanticscholar.org/paper/828a7b3122ebd5b8b0c617902bc04ac5a6c60240\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1709.01471\",\"authors\":[{\"authorId\":\"34885007\",\"name\":\"Edward Raff\"},{\"authorId\":\"40214038\",\"name\":\"J. Sylvester\"},{\"authorId\":\"144440349\",\"name\":\"C. Nicholas\"}],\"doi\":\"10.1145/3128572.3140442\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a8c3e049b5969e0b64ca8e5def9777525e8e73bf\",\"title\":\"Learning the PE Header, Malware Detection with Minimal Domain Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/a8c3e049b5969e0b64ca8e5def9777525e8e73bf\",\"venue\":\"AISec@CCS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2862582\",\"name\":\"Peratham Wiriyathammabhum\"},{\"authorId\":\"1403432120\",\"name\":\"Douglas Summers-Stay\"},{\"authorId\":\"1759899\",\"name\":\"C. Ferm\\u00fcller\"},{\"authorId\":\"1697493\",\"name\":\"Y. Aloimonos\"}],\"doi\":\"10.1145/3009906\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e60536c847ac25dba4c1c071e0355e5537fe061\",\"title\":\"Computer Vision and Natural Language Processing\",\"url\":\"https://www.semanticscholar.org/paper/6e60536c847ac25dba4c1c071e0355e5537fe061\",\"venue\":\"ACM Comput. Surv.\",\"year\":2017},{\"arxivId\":\"1704.02163\",\"authors\":[{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"},{\"authorId\":\"87972149\",\"name\":\"Sergi Soler\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"}],\"doi\":\"10.1016/j.jvcir.2017.11.022\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"title\":\"Egocentric video description based on temporally-linked sequences\",\"url\":\"https://www.semanticscholar.org/paper/a3fadfae9e54b62401585473e5c1cf7a4a623f62\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29850862\",\"name\":\"He-Yen Hsieh\"},{\"authorId\":\"2785372\",\"name\":\"Ding-Jie Chen\"},{\"authorId\":\"1805102\",\"name\":\"Tyng-Luh Liu\"}],\"doi\":\"10.1109/ICIP40778.2020.9190975\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d0a1105f231ffc200d7b299fb962d8b673369be\",\"title\":\"Temporal Action Proposal Generation Via Deep Feature Enhancement\",\"url\":\"https://www.semanticscholar.org/paper/8d0a1105f231ffc200d7b299fb962d8b673369be\",\"venue\":\"2020 IEEE International Conference on Image Processing (ICIP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39763340\",\"name\":\"Donghui Lin\"},{\"authorId\":\"144882422\",\"name\":\"Nancy Ide\"},{\"authorId\":\"1707726\",\"name\":\"J. Pustejovsky\"},{\"authorId\":\"1719005\",\"name\":\"M. Adriani\"},{\"authorId\":\"3389490\",\"name\":\"Mairehaba Aili\"},{\"authorId\":\"1742458\",\"name\":\"N\\u00faria Bel\"},{\"authorId\":null,\"name\":\"Pompeu Universitat\"},{\"authorId\":\"48722239\",\"name\":\"S. Fabra\"},{\"authorId\":\"1723649\",\"name\":\"Kalina Bontcheva\"},{\"authorId\":\"1707466\",\"name\":\"N. Calzolari\"},{\"authorId\":\"1678451\",\"name\":\"K. Choukri\"},{\"authorId\":null,\"name\":\"France Elda\"},{\"authorId\":\"1912437\",\"name\":\"L. Dini\"},{\"authorId\":\"1799334\",\"name\":\"Jens Grivolla\"},{\"authorId\":null,\"name\":\"Universitat Pompeu Glicom\"},{\"authorId\":\"34803801\",\"name\":\"Yoshinobu Kano\"},{\"authorId\":\"1763416\",\"name\":\"M. Monachini\"},{\"authorId\":\"145025479\",\"name\":\"H. Vu\"},{\"authorId\":\"118887428\",\"name\":\"Qu\\u1eadn\"},{\"authorId\":\"1779078\",\"name\":\"Virach Sornlertlamvanich\"},{\"authorId\":null,\"name\":\"Thailand Siit\"},{\"authorId\":\"1918046\",\"name\":\"Andrejs Vasiljevs\"},{\"authorId\":null,\"name\":\"Latvia Tilde\"},{\"authorId\":\"24178944\",\"name\":\"S. Mohanty\"},{\"authorId\":\"1931512\",\"name\":\"Nehal J. Wani\"},{\"authorId\":\"48950875\",\"name\":\"M. Srivastava\"},{\"authorId\":\"2570172\",\"name\":\"Keith Suderman\"},{\"authorId\":\"144287919\",\"name\":\"Eric Nyberg\"},{\"authorId\":\"145539145\",\"name\":\"M. Verhagen\"},{\"authorId\":\"32471241\",\"name\":\"Panuwat Assawinjaipetch\"},{\"authorId\":\"2173767\",\"name\":\"K. Shirai\"},{\"authorId\":\"31395575\",\"name\":\"S. Marukata\"},{\"authorId\":\"12786608\",\"name\":\"M. Eli\"},{\"authorId\":\"31451779\",\"name\":\"Weinila Mushajiang\"},{\"authorId\":\"47861988\",\"name\":\"Tuergen Yibulayin\"},{\"authorId\":\"2308591\",\"name\":\"Kahaerjiang Abiderexiti\"},{\"authorId\":\"40013577\",\"name\":\"Y. Liu\"},{\"authorId\":\"14952902\",\"name\":\"Hieu-Thi Luong\"},{\"authorId\":\"32298039\",\"name\":\"H. Quan\"},{\"authorId\":\"39104718\",\"name\":\"Tingming Lu\"},{\"authorId\":\"39712394\",\"name\":\"M. Zhu\"},{\"authorId\":\"145063056\",\"name\":\"Z. Gao\"},{\"authorId\":\"2703128\",\"name\":\"Yaocheng Gui\"},{\"authorId\":\"153171021\",\"name\":\"D. Sharma\"},{\"authorId\":\"49492414\",\"name\":\"Hai-Quan Vu\"},{\"authorId\":\"8703425\",\"name\":\"Y. Murakami\"},{\"authorId\":\"2914379\",\"name\":\"T. Nakaguchi\"},{\"authorId\":\"143807934\",\"name\":\"T. Ishida\"},{\"authorId\":\"24178944\",\"name\":\"S. Mohanty\"},{\"authorId\":\"34839641\",\"name\":\"Dipti Misra\"},{\"authorId\":\"1409519377\",\"name\":\"Sharma\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"53c05bc02c8c487cba988c3d5d8a13e3a7fe2ff2\",\"title\":\"Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies Combining Human Inputters and Language Services to Provide Multi-language Support System for In- Ternational Symposiums Recurr\",\"url\":\"https://www.semanticscholar.org/paper/53c05bc02c8c487cba988c3d5d8a13e3a7fe2ff2\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"46700331\",\"name\":\"J. Liu\"},{\"authorId\":\"1680068\",\"name\":\"X. Wang\"}],\"doi\":\"10.1145/3126686.3126714\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b5bfe824fc49fe78b538ac15f21c4cd6a9d44347\",\"title\":\"Image Caption with Synchronous Cross-Attention\",\"url\":\"https://www.semanticscholar.org/paper/b5bfe824fc49fe78b538ac15f21c4cd6a9d44347\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1603.07415\",\"authors\":[{\"authorId\":\"46276098\",\"name\":\"Jianan Li\"},{\"authorId\":\"49020088\",\"name\":\"Yunchao Wei\"},{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"145550812\",\"name\":\"J. Dong\"},{\"authorId\":\"39001620\",\"name\":\"T. Xu\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":\"10.1109/TMM.2016.2642789\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d52463daf96825a8df3131dc1d88b062e2349b81\",\"title\":\"Attentive Contexts for Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/d52463daf96825a8df3131dc1d88b062e2349b81\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2017},{\"arxivId\":\"1808.03766\",\"authors\":[{\"authorId\":\"2931652\",\"name\":\"Bernard Ghanem\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"},{\"authorId\":\"3175258\",\"name\":\"Fabian Caba Heilbron\"},{\"authorId\":\"19198894\",\"name\":\"Humam Alwassel\"},{\"authorId\":\"144201025\",\"name\":\"Victor Escorcia\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"8983218\",\"name\":\"S. Buch\"},{\"authorId\":\"3409955\",\"name\":\"C. D. Dao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5468c96e3846da23c26b59c28c313506bffbf7ce\",\"title\":\"The ActivityNet Large-Scale Activity Recognition Challenge 2018 Summary\",\"url\":\"https://www.semanticscholar.org/paper/5468c96e3846da23c26b59c28c313506bffbf7ce\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1608.05477\",\"authors\":[{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"1723233\",\"name\":\"R. Feris\"},{\"authorId\":\"145779951\",\"name\":\"X. Wang\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.1007/978-3-319-46448-0_3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf0fb5813903f8afaba2bc52fd3c886fc672d203\",\"title\":\"A Recurrent Encoder-Decoder Network for Sequential Face Alignment\",\"url\":\"https://www.semanticscholar.org/paper/bf0fb5813903f8afaba2bc52fd3c886fc672d203\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":null,\"name\":\"Chao Li\"},{\"authorId\":\"39767248\",\"name\":\"Jiewei Cao\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":null,\"name\":\"Lei Zhu\"}],\"doi\":\"10.1109/ICCV.2017.394\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6ae53b5837f6e6ca975dcb7af53ef51ac820e3a7\",\"title\":\"Leveraging Weak Semantic Relevance for Complex Video Event Classification\",\"url\":\"https://www.semanticscholar.org/paper/6ae53b5837f6e6ca975dcb7af53ef51ac820e3a7\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"144404414\",\"name\":\"S. Satoh\"}],\"doi\":\"10.1145/3123266.3127898\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b2e0e08e4d4c722d0f54f5a124ca28a67d74ce3e\",\"title\":\"MANet: A Modal Attention Network for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/b2e0e08e4d4c722d0f54f5a124ca28a67d74ce3e\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3046293\",\"name\":\"D. Lee\"},{\"authorId\":\"38726140\",\"name\":\"Jongmin Lee\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"}],\"doi\":\"10.1007/978-3-319-54427-4_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"25a83fd401a528e4979afaf4554a3e941dd7016d\",\"title\":\"Multi-view Automatic Lip-Reading Using Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/25a83fd401a528e4979afaf4554a3e941dd7016d\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"145443186\",\"name\":\"T. Hori\"},{\"authorId\":\"1816785\",\"name\":\"G. Wichern\"},{\"authorId\":null,\"name\":\"Jue Wang\"},{\"authorId\":\"1747615\",\"name\":\"Teng-Yok Lee\"},{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e561b60c6aea937f9d98ee336dde01abd1ff651\",\"title\":\"Multimodal Attention for Fusion of Audio and Spatiotemporal Features for Video Description\",\"url\":\"https://www.semanticscholar.org/paper/8e561b60c6aea937f9d98ee336dde01abd1ff651\",\"venue\":\"CVPR Workshops\",\"year\":2018},{\"arxivId\":\"1811.05253\",\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"5145427\",\"name\":\"Fangyu Wu\"},{\"authorId\":\"33830793\",\"name\":\"Jeremy S. Smith\"},{\"authorId\":\"40178769\",\"name\":\"Wenjin Lu\"},{\"authorId\":\"1782912\",\"name\":\"Bailing Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7294dd0ec8ff8d3353286bce6b4ace60f957a08\",\"title\":\"Image Captioning Based on a Hierarchical Attention Mechanism and Policy Gradient Optimization\",\"url\":\"https://www.semanticscholar.org/paper/b7294dd0ec8ff8d3353286bce6b4ace60f957a08\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/ACCESS.2018.2879642\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"title\":\"A Fine-Grained Spatial-Temporal Attention Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1902.01117\",\"authors\":[{\"authorId\":\"35285736\",\"name\":\"E. Sibirtseva\"},{\"authorId\":\"1907802\",\"name\":\"A. Ghadirzadeh\"},{\"authorId\":\"39799707\",\"name\":\"Iolanda Leite\"},{\"authorId\":\"7347502\",\"name\":\"M\\u00e5rten Bj\\u00f6rkman\"},{\"authorId\":\"1731490\",\"name\":\"D. Kragic\"}],\"doi\":\"10.1007/978-3-030-21565-1_8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4e56fe01b092c588e29f02eff0f6c8694c7adb5f\",\"title\":\"Exploring Temporal Dependencies in Multimodal Referring Expressions with Mixed Reality\",\"url\":\"https://www.semanticscholar.org/paper/4e56fe01b092c588e29f02eff0f6c8694c7adb5f\",\"venue\":\"HCI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2786437\",\"name\":\"Linghui Li\"},{\"authorId\":\"144044848\",\"name\":\"Sheng Tang\"},{\"authorId\":\"2031845\",\"name\":\"Junbo Guo\"},{\"authorId\":null,\"name\":\"Rui Wang\"},{\"authorId\":\"144241178\",\"name\":\"B. Lyu\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"}],\"doi\":\"10.1109/BigMM.2018.8499066\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a830ef990527adef14fed8d05ef5df5b9c87d4c\",\"title\":\"Image Captioning Based on Adaptive Balancing Loss\",\"url\":\"https://www.semanticscholar.org/paper/9a830ef990527adef14fed8d05ef5df5b9c87d4c\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49685502\",\"name\":\"J. Lee\"},{\"authorId\":\"1769295\",\"name\":\"Junmo Kim\"}],\"doi\":\"10.1109/ICCE-ASIA.2018.8552140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"22409d9471b426e0bcac3f850aa16ad158b355a7\",\"title\":\"Improving Video Captioning with Non-Local Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/22409d9471b426e0bcac3f850aa16ad158b355a7\",\"venue\":\"2018 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50111883\",\"name\":\"S. Lee\"},{\"authorId\":\"48084799\",\"name\":\"In-Cheol Kim\"}],\"doi\":\"10.1155/2018/3125879\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c919568836e236da738282f9015f58c455d26f7\",\"title\":\"Multimodal Feature Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/3c919568836e236da738282f9015f58c455d26f7\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1806.08854\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"46970799\",\"name\":\"Y. Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"10713620\",\"name\":\"J. Qiu\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a9c92a56e388997adb513305a4259798506b7f5\",\"title\":\"RUC+CMU: System Report for Dense Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/9a9c92a56e388997adb513305a4259798506b7f5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"},{\"authorId\":\"20332986\",\"name\":\"Q. Hu\"},{\"authorId\":\"144894837\",\"name\":\"F. Wu\"}],\"doi\":\"10.1109/TCSVT.2019.2956593\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"230a8581672b3147238eaab2cf686c70fe4f672b\",\"title\":\"Convolutional Reconstruction-to-Sequence for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/230a8581672b3147238eaab2cf686c70fe4f672b\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"1905.03966\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"49050519\",\"name\":\"Jiyuan Zhang\"},{\"authorId\":\"47119038\",\"name\":\"X. Wang\"},{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/CVPR.2019.00854\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"title\":\"Memory-Attended Recurrent Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":\"2002.11701\",\"authors\":[{\"authorId\":\"3233864\",\"name\":\"S. Biswal\"},{\"authorId\":\"47343720\",\"name\":\"Cao Xiao\"},{\"authorId\":\"28331874\",\"name\":\"Lucas Glass\"},{\"authorId\":\"144293787\",\"name\":\"M. Westover\"},{\"authorId\":\"1738536\",\"name\":\"Jimeng Sun\"}],\"doi\":\"10.1145/3366423.3380137\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a842fe8c25348627764462a57f0cd43d8cef103b\",\"title\":\"CLARA: Clinical Report Auto-completion\",\"url\":\"https://www.semanticscholar.org/paper/a842fe8c25348627764462a57f0cd43d8cef103b\",\"venue\":\"WWW\",\"year\":2020},{\"arxivId\":\"1906.04226\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"1403581832\",\"name\":\"Laura Sevilla-Lara\"},{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"3429328\",\"name\":\"Matt Feiszli\"},{\"authorId\":\"143907244\",\"name\":\"Yi Yang\"},{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a71b7cf982cf1b54cda4f0746d4ba7c7ea3f4e4\",\"title\":\"FASTER Recurrent Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/8a71b7cf982cf1b54cda4f0746d4ba7c7ea3f4e4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1611.07212\",\"authors\":[{\"authorId\":\"39398377\",\"name\":\"A. Haque\"},{\"authorId\":\"3304525\",\"name\":\"Alexandre Alahi\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.138\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"868ae15b05c015fd9fdd93a2ca00c26ca4108699\",\"title\":\"Recurrent Attention Models for Depth-Based Person Identification\",\"url\":\"https://www.semanticscholar.org/paper/868ae15b05c015fd9fdd93a2ca00c26ca4108699\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1603.02814\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"121177698\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/TPAMI.2017.2708709\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":\"1606.03632\",\"authors\":[{\"authorId\":\"145478041\",\"name\":\"Shikhar Sharma\"},{\"authorId\":\"143852605\",\"name\":\"Jing He\"},{\"authorId\":\"2987426\",\"name\":\"Kaheer Suleman\"},{\"authorId\":\"1944614\",\"name\":\"Hannes Schulz\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8932c433b41c955ef87acefbdbfcbf81a00faeef\",\"title\":\"Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data\",\"url\":\"https://www.semanticscholar.org/paper/8932c433b41c955ef87acefbdbfcbf81a00faeef\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"9764377\",\"name\":\"Xuanhan Wang\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"46399266\",\"name\":\"Yang Liu\"}],\"doi\":\"10.1016/J.NEUCOM.2018.06.096\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"31b28e96a337dfcf2dbfde104a1ec46f4e755844\",\"title\":\"Fused GRU with semantic-temporal attention for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/31b28e96a337dfcf2dbfde104a1ec46f4e755844\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1879112302\",\"name\":\"Xinlong Xiao\"},{\"authorId\":\"3067458\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"51304315\",\"name\":\"Rui Feng\"},{\"authorId\":\"103245682\",\"name\":\"T. Zhang\"},{\"authorId\":\"48869251\",\"name\":\"Shang Gao\"},{\"authorId\":\"1878750882\",\"name\":\"Weiguo Fan\"}],\"doi\":\"10.1109/ICME46284.2020.9102967\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34e98526fc9d4ca177483eb7c858ea0ee8a65a04\",\"title\":\"Video Captioning With Temporal And Region Graph Convolution Network\",\"url\":\"https://www.semanticscholar.org/paper/34e98526fc9d4ca177483eb7c858ea0ee8a65a04\",\"venue\":\"2020 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1715610\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"3775903\",\"name\":\"J. Wang\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"55a7286f014cc6b51a3f50b1e6bc8acc8166f231\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/55a7286f014cc6b51a3f50b1e6bc8acc8166f231\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1906.02467\",\"authors\":[{\"authorId\":\"48567197\",\"name\":\"Zhou Yu\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1720236\",\"name\":\"J. Yu\"},{\"authorId\":\"144478231\",\"name\":\"T. Yu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"143719920\",\"name\":\"D. Tao\"}],\"doi\":\"10.1609/aaai.v33i01.33019127\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4f2c1af57c056102806a184517313804f66e7447\",\"title\":\"ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/4f2c1af57c056102806a184517313804f66e7447\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1801.10111\",\"authors\":[{\"authorId\":\"47513257\",\"name\":\"Jie Huang\"},{\"authorId\":\"38272296\",\"name\":\"W. Zhou\"},{\"authorId\":\"46324995\",\"name\":\"Q. Zhang\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"1683483\",\"name\":\"W. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"81a1660d57738347a04b22920571bc394dd97a9e\",\"title\":\"Video-based Sign Language Recognition without Temporal Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/81a1660d57738347a04b22920571bc394dd97a9e\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1812.09041\",\"authors\":[{\"authorId\":\"153198570\",\"name\":\"Guoyun Tu\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"11004839\",\"name\":\"Jiarui Gao\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1109/TMM.2019.2922129\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"41b76703b03ecb40dbcc00e9fbf6a73b0b808778\",\"title\":\"A Multi-Task Neural Approach for Emotion Attribution, Classification, and Summarization\",\"url\":\"https://www.semanticscholar.org/paper/41b76703b03ecb40dbcc00e9fbf6a73b0b808778\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738471\",\"name\":\"Chenyang Zhang\"},{\"authorId\":\"35484757\",\"name\":\"Yingli Tian\"}],\"doi\":\"10.1007/978-3-319-48881-3_11\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d2cda0dbb8b2e83ce3e70d818f78d2add803c661\",\"title\":\"Automatic Video Captioning via Multi-channel Sequential Encoding\",\"url\":\"https://www.semanticscholar.org/paper/d2cda0dbb8b2e83ce3e70d818f78d2add803c661\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":\"1609.05281\",\"authors\":[{\"authorId\":\"144883800\",\"name\":\"A. Gandhi\"},{\"authorId\":\"34751361\",\"name\":\"Arjun Sharma\"},{\"authorId\":\"47606073\",\"name\":\"A. Biswas\"},{\"authorId\":\"2116262\",\"name\":\"O. Deshmukh\"}],\"doi\":\"10.1007/978-3-319-48881-3_58\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"551fa37e8d6d03b89d195a5c00c74cc52ff1c67a\",\"title\":\"GeThR-Net: A Generalized Temporally Hybrid Recurrent Neural Network for Multimodal Information Fusion\",\"url\":\"https://www.semanticscholar.org/paper/551fa37e8d6d03b89d195a5c00c74cc52ff1c67a\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":\"1605.08110\",\"authors\":[{\"authorId\":\"47968942\",\"name\":\"K. Zhang\"},{\"authorId\":\"38784892\",\"name\":\"Wei-Lun Chao\"},{\"authorId\":\"145757665\",\"name\":\"F. Sha\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1007/978-3-319-46478-7_47\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1dbc12e54ceb70f2022f956aa0a46e2706e99962\",\"title\":\"Video Summarization with Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/1dbc12e54ceb70f2022f956aa0a46e2706e99962\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1612.01022\",\"authors\":[{\"authorId\":\"2283739\",\"name\":\"Y. Wu\"},{\"authorId\":\"1849628\",\"name\":\"Huachun Tan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c52b77c18700e9625c885a824a0c8b95c3e9cf21\",\"title\":\"Short-term traffic flow forecasting with spatial-temporal correlation in a hybrid deep learning framework\",\"url\":\"https://www.semanticscholar.org/paper/c52b77c18700e9625c885a824a0c8b95c3e9cf21\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144841441\",\"name\":\"J. Xu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3123266.3123448\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"title\":\"Learning Multimodal Attention LSTM Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1881509\",\"name\":\"Vicky Kalogeiton\"},{\"authorId\":\"2492127\",\"name\":\"Philippe Weinzaepfel\"},{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2017.219\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"151b87de997e55db892b122c211f9c749f4293de\",\"title\":\"Joint Learning of Object and Action Detectors\",\"url\":\"https://www.semanticscholar.org/paper/151b87de997e55db892b122c211f9c749f4293de\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1807.11110\",\"authors\":[{\"authorId\":\"48569266\",\"name\":\"X. Li\"},{\"authorId\":\"3295274\",\"name\":\"Zhisheng Hu\"},{\"authorId\":\"3389437\",\"name\":\"Yiwei Fu\"},{\"authorId\":\"144787615\",\"name\":\"Ping Chen\"},{\"authorId\":\"1771269\",\"name\":\"Minghui Zhu\"},{\"authorId\":\"39336958\",\"name\":\"Peng Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a99c49ffbb618d76dd58944d2ec0380084367ffb\",\"title\":\"ROPNN: Detection of ROP Payloads Using Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/a99c49ffbb618d76dd58944d2ec0380084367ffb\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3127901\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"title\":\"Knowing Yourself: Improving Video Caption via In-depth Recap\",\"url\":\"https://www.semanticscholar.org/paper/3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40387883\",\"name\":\"Florian Patzelt\"},{\"authorId\":\"2112168\",\"name\":\"Robert Haschke\"},{\"authorId\":\"30258243\",\"name\":\"Helge J. Ritter\"}],\"doi\":\"10.1007/978-3-319-44781-0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf9ce9a9114a29b69e48acb1193d21dc6267f0d3\",\"title\":\"Artificial Neural Networks and Machine Learning \\u2013 ICANN 2016\",\"url\":\"https://www.semanticscholar.org/paper/bf9ce9a9114a29b69e48acb1193d21dc6267f0d3\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2016},{\"arxivId\":\"1611.02261\",\"authors\":[{\"authorId\":\"2915023\",\"name\":\"Rasool Fakoor\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1738740\",\"name\":\"S. B. Kang\"},{\"authorId\":\"143967473\",\"name\":\"Pushmeet Kohli\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"title\":\"Memory-augmented Attention Modelling for Videos\",\"url\":\"https://www.semanticscholar.org/paper/249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1907.05092\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"46490565\",\"name\":\"Zhaoyang Zeng\"},{\"authorId\":\"50678073\",\"name\":\"Bei Liu\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b7bf64b2c7372aa82d32424aacc6f4a86215433\",\"title\":\"Activitynet 2019 Task 3: Exploring Contexts for Dense Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8b7bf64b2c7372aa82d32424aacc6f4a86215433\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22710849\",\"name\":\"Patrick Bordes\"},{\"authorId\":\"39541096\",\"name\":\"Eloi Zablocki\"},{\"authorId\":\"145159652\",\"name\":\"L. Soulier\"},{\"authorId\":\"1703777\",\"name\":\"Benjamin Piwowarski\"},{\"authorId\":\"1741426\",\"name\":\"P. Gallinari\"}],\"doi\":\"10.24348/coria.2019.CORIA_2019_paper_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"175ee94e33c3871edd198d6fca9d33669ab24d6d\",\"title\":\"Un mod\\u00e8le multimodal d'apprentissage de repr\\u00e9sentations de phrases qui pr\\u00e9serve la s\\u00e9mantique visuelle\",\"url\":\"https://www.semanticscholar.org/paper/175ee94e33c3871edd198d6fca9d33669ab24d6d\",\"venue\":\"CORIA\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a13e8fb74f437c1e4b267e1e341c0d195458df19\",\"title\":\"Where and When Counts: Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/a13e8fb74f437c1e4b267e1e341c0d195458df19\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2474219\",\"name\":\"Nikolaos Karianakis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"75b987f86af2bc7f68edc45be240dd30e1ef2699\",\"title\":\"Sampling Algorithms to Handle Nuisances in Large-Scale Recognition\",\"url\":\"https://www.semanticscholar.org/paper/75b987f86af2bc7f68edc45be240dd30e1ef2699\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2924101\",\"name\":\"Laura K. Allen\"},{\"authorId\":\"2379945\",\"name\":\"Matthew E. Jacovina\"},{\"authorId\":\"2912495\",\"name\":\"M. Dascalu\"},{\"authorId\":\"2122997\",\"name\":\"Rod D. Roscoe\"},{\"authorId\":\"145819845\",\"name\":\"Kevin Kent\"},{\"authorId\":\"3303728\",\"name\":\"Aaron D. Likens\"},{\"authorId\":\"1801516\",\"name\":\"D. McNamara\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0569d399ec23876b834777a0ace15c8eb82e29d6\",\"title\":\"{ENTER}ing the Time Series {SPACE}: Uncovering the Writing Process through Keystroke Analyses\",\"url\":\"https://www.semanticscholar.org/paper/0569d399ec23876b834777a0ace15c8eb82e29d6\",\"venue\":\"EDM\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71615308\",\"name\":\"Sudatta Mohanty\"},{\"authorId\":\"2621165\",\"name\":\"Alexey Pozdnukhov\"},{\"authorId\":\"40355715\",\"name\":\"M. Cassidy\"}],\"doi\":\"10.1016/j.trc.2020.102624\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"85ae78974cec29b7c6a0b1ebd2a9544371c5d211\",\"title\":\"Region-wide congestion prediction and control using deep learning\",\"url\":\"https://www.semanticscholar.org/paper/85ae78974cec29b7c6a0b1ebd2a9544371c5d211\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1507.05738\",\"authors\":[{\"authorId\":\"34149749\",\"name\":\"Serena Yeung\"},{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"47243342\",\"name\":\"N. Jin\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"10771328\",\"name\":\"G. Mori\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-017-1013-y\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"aa2d1dac4e27aebfb759d6e0b41b8a8ab1b01406\",\"title\":\"Every Moment Counts: Dense Detailed Labeling of Actions in Complex Videos\",\"url\":\"https://www.semanticscholar.org/paper/aa2d1dac4e27aebfb759d6e0b41b8a8ab1b01406\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1607.05910\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"2406263\",\"name\":\"Damien Teney\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1016/j.cviu.2017.05.001\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"88c307c51594c6d802080a0780d0d654e2e2891f\",\"title\":\"Visual question answering: A survey of methods and datasets\",\"url\":\"https://www.semanticscholar.org/paper/88c307c51594c6d802080a0780d0d654e2e2891f\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.1613/jair.4900\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c162d791b63682d928c09578bd38c3dd61f78c8c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures\",\"url\":\"https://www.semanticscholar.org/paper/c162d791b63682d928c09578bd38c3dd61f78c8c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2016},{\"arxivId\":\"1904.02628\",\"authors\":[{\"authorId\":\"65767906\",\"name\":\"Silvio Olivastri\"},{\"authorId\":\"1931660\",\"name\":\"Gurkirt Singh\"},{\"authorId\":\"1754181\",\"name\":\"Fabio Cuzzolin\"}],\"doi\":\"10.1109/ICCVW.2019.00185\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0c1ebaa635f68bb4a09fc59191642f30cfa894c9\",\"title\":\"End-to-End Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0c1ebaa635f68bb4a09fc59191642f30cfa894c9\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40566477\",\"name\":\"H. Yang\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":\"49730034\",\"name\":\"B. Li\"},{\"authorId\":\"144708945\",\"name\":\"Yang Du\"},{\"authorId\":\"1757173\",\"name\":\"Junliang Xing\"},{\"authorId\":\"40506509\",\"name\":\"W. Hu\"},{\"authorId\":\"144555237\",\"name\":\"S. Maybank\"}],\"doi\":\"10.1016/j.patcog.2018.07.028\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"756532d707209f13c44b96e6306ac0c96e6733a5\",\"title\":\"Asymmetric 3D Convolutional Neural Networks for action recognition\",\"url\":\"https://www.semanticscholar.org/paper/756532d707209f13c44b96e6306ac0c96e6733a5\",\"venue\":\"Pattern Recognit.\",\"year\":2019},{\"arxivId\":\"1511.04590\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.5244/C.30.141\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16aac81ae033f7295d82e5b679400d105170a3e1\",\"title\":\"Oracle Performance for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16aac81ae033f7295d82e5b679400d105170a3e1\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"title\":\"Natural Language Description of Images and Videos\",\"url\":\"https://www.semanticscholar.org/paper/d69f69e84c57d92914c03cf028ad8cf0cfe29140\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1560895396\",\"name\":\"Hao Xue\"},{\"authorId\":\"144199437\",\"name\":\"D. Huynh\"},{\"authorId\":\"40641996\",\"name\":\"Mark Reynolds\"}],\"doi\":\"10.1109/ACCESS.2020.2977747\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ffb5d43cbb544bbaf52f08681569a0a2a32f882d\",\"title\":\"A Location-Velocity-Temporal Attention LSTM Model for Pedestrian Trajectory Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ffb5d43cbb544bbaf52f08681569a0a2a32f882d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1576529647\",\"name\":\"Wenjing Su\"},{\"authorId\":\"145981582\",\"name\":\"Jiang Zhu\"},{\"authorId\":\"51401760\",\"name\":\"H. Liao\"},{\"authorId\":\"9450966\",\"name\":\"M. Tentzeris\"}],\"doi\":\"10.1109/ACCESS.2020.2982965\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"89c1e6f8fe12edd4ceb4565d16754c5a7d259580\",\"title\":\"Wearable Antennas for Cross-Body Communication and Human Activity Recognition\",\"url\":\"https://www.semanticscholar.org/paper/89c1e6f8fe12edd4ceb4565d16754c5a7d259580\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1807.03878\",\"authors\":[{\"authorId\":\"5968525\",\"name\":\"Arshdeep Sekhon\"},{\"authorId\":\"3407330\",\"name\":\"R. Singh\"},{\"authorId\":\"1791105\",\"name\":\"Y. Qi\"}],\"doi\":\"10.1093/bioinformatics/bty612\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e095892714ed0337584dcbd98aa9dfea6483ffef\",\"title\":\"DeepDiff: DEEP\\u2010learning for predicting DIFFerential gene expression from histone modifications\",\"url\":\"https://www.semanticscholar.org/paper/e095892714ed0337584dcbd98aa9dfea6483ffef\",\"venue\":\"Bioinform.\",\"year\":2018},{\"arxivId\":\"1609.08124\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2068f66a10254d457cdb5fab74b0128b24bfdb65\",\"title\":\"Learning Language-Visual Embedding for Movie Understanding with Natural-Language\",\"url\":\"https://www.semanticscholar.org/paper/2068f66a10254d457cdb5fab74b0128b24bfdb65\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"2558130\",\"name\":\"Z. Shi\"}],\"doi\":\"10.1145/2964284.2967298\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0c014c19b68a781ccd6e26fcc7c47ba9b1cf020f\",\"title\":\"Boosting Video Description Generation by Explicitly Translating from Frame-Level Captions\",\"url\":\"https://www.semanticscholar.org/paper/0c014c19b68a781ccd6e26fcc7c47ba9b1cf020f\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35031371\",\"name\":\"Wenbin Du\"},{\"authorId\":\"2799162\",\"name\":\"Y. Wang\"},{\"authorId\":null,\"name\":\"Yu Qiao\"}],\"doi\":\"10.1109/TIP.2017.2778563\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"57eeaceb14a01a2560d0b90d38205e512dcca691\",\"title\":\"Recurrent Spatial-Temporal Attention Network for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/57eeaceb14a01a2560d0b90d38205e512dcca691\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"26559284\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"title\":\"Captioning Near-Future Activity Sequences\",\"url\":\"https://www.semanticscholar.org/paper/dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1806.09278\",\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"50980046\",\"name\":\"Moyini Yao\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b87e33101a5564cbd3d212246aa48e2b6123227\",\"title\":\"Best Vision Technologies Submission to ActivityNet Challenge 2018-Task: Dense-Captioning Events in Videos\",\"url\":\"https://www.semanticscholar.org/paper/8b87e33101a5564cbd3d212246aa48e2b6123227\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"title\":\"Image Input OR Video Hierarchical LSTMs with Adaptive Attention ( hLSTMat ) Feature Extraction Generated Captions Losses\",\"url\":\"https://www.semanticscholar.org/paper/e060c24c57930007d9496edc6847ed78ef1b0ddd\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2105743\",\"name\":\"Y. Bin\"},{\"authorId\":\"144757965\",\"name\":\"Y. Yang\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":null,\"name\":\"Ning Xie\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"}],\"doi\":\"10.1109/TCYB.2018.2831447\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"af6d6271f317a1a5a30908fdeac0fc054cd0493b\",\"title\":\"Describing Video With Attention-Based Bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/af6d6271f317a1a5a30908fdeac0fc054cd0493b\",\"venue\":\"IEEE Transactions on Cybernetics\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yu-Siang Wang\"},{\"authorId\":\"71309591\",\"name\":\"Hung-Ting Su\"},{\"authorId\":\"150053992\",\"name\":\"Chen-Hsi Chang\"},{\"authorId\":\"46270526\",\"name\":\"Z. Liu\"},{\"authorId\":\"1716836\",\"name\":\"W. Hsu\"}],\"doi\":\"10.1109/ICASSP40776.2020.9053476\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9dd133c6baed0b080687da0954754ea23abc8ba1\",\"title\":\"Video Question Generation via Semantic Rich Cross-Modal Self-Attention Networks Learning\",\"url\":\"https://www.semanticscholar.org/paper/9dd133c6baed0b080687da0954754ea23abc8ba1\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1965909970\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"1755773\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930562\",\"name\":\"G. Chen\"},{\"authorId\":\"153016830\",\"name\":\"J. Guo\"}],\"doi\":\"10.1109/ACCESS.2020.3021857\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"title\":\"Understanding Objects in Video: Object-Oriented Video Captioning via Structured Trajectory and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"2002.11566\",\"authors\":[{\"authorId\":\"36811682\",\"name\":\"Z. Zhang\"},{\"authorId\":\"37198550\",\"name\":\"Yaya Shi\"},{\"authorId\":null,\"name\":\"Chunfeng Yuan\"},{\"authorId\":null,\"name\":\"Bing Li\"},{\"authorId\":\"39397292\",\"name\":\"Peijin Wang\"},{\"authorId\":\"48594951\",\"name\":\"Weiming Hu\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.1109/cvpr42600.2020.01329\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1dd557a8839733a5ee06d19989a265e61f603c1\",\"title\":\"Object Relational Graph With Teacher-Recommended Learning for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f1dd557a8839733a5ee06d19989a265e61f603c1\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49308892\",\"name\":\"Mahlagha Afrasiabi\"},{\"authorId\":\"2264128\",\"name\":\"H. Khotanlou\"},{\"authorId\":\"1620219267\",\"name\":\"Theo Gevers\"}],\"doi\":\"10.1007/s11042-020-08845-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"233fde55a797ac4fcd018bb2e7e967e51731c341\",\"title\":\"Spatial-temporal dual-actor CNN for human interaction prediction in video\",\"url\":\"https://www.semanticscholar.org/paper/233fde55a797ac4fcd018bb2e7e967e51731c341\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144907434\",\"name\":\"T. Kobayashi\"}],\"doi\":\"10.1109/ICCV.2017.600\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"703c9c8f20860a1b1be63e6df1622b2021b003ca\",\"title\":\"Flip-Invariant Motion Representation\",\"url\":\"https://www.semanticscholar.org/paper/703c9c8f20860a1b1be63e6df1622b2021b003ca\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2269558\",\"name\":\"Q. Li\"},{\"authorId\":\"144854843\",\"name\":\"X. Zhao\"},{\"authorId\":\"143712929\",\"name\":\"R. He\"},{\"authorId\":\"2887871\",\"name\":\"K. Huang\"}],\"doi\":\"10.1109/TCSVT.2019.2923444\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ff61d933f95bab4d0390b0372b0e6f1829251c6\",\"title\":\"Recurrent Prediction With Spatio-Temporal Attention for Crowd Attribute Recognition\",\"url\":\"https://www.semanticscholar.org/paper/9ff61d933f95bab4d0390b0372b0e6f1829251c6\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50506129\",\"name\":\"E. Barati\"},{\"authorId\":\"2410994\",\"name\":\"Xue-wen Chen\"}],\"doi\":\"10.1145/3343031.3351037\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf8a3f260fbe4ee104380437cd576a556dccd290\",\"title\":\"Critic-based Attention Network for Event-based Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/cf8a3f260fbe4ee104380437cd576a556dccd290\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"83979477\",\"name\":\"Suman Kalyan Adari\"},{\"authorId\":\"47238599\",\"name\":\"W. Garcia\"},{\"authorId\":\"1784947\",\"name\":\"K. Butler\"}],\"doi\":\"10.1109/DSN-W.2019.00012\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5ebc08cbe3daf87afc0cd11a762f6b7c05f9b20b\",\"title\":\"Adversarial Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5ebc08cbe3daf87afc0cd11a762f6b7c05f9b20b\",\"venue\":\"2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN-W)\",\"year\":2019},{\"arxivId\":\"1802.02210\",\"authors\":[{\"authorId\":\"31635336\",\"name\":\"E. Matsuo\"},{\"authorId\":\"3236658\",\"name\":\"I. Kobayashi\"},{\"authorId\":\"37572653\",\"name\":\"S. Nishimoto\"},{\"authorId\":\"32224191\",\"name\":\"Satoshi Nishida\"},{\"authorId\":\"7142317\",\"name\":\"Hideki Asoh\"}],\"doi\":\"10.1109/SMC.2018.00107\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6ef8407c9c8f9153f25d5339724c357191b47202\",\"title\":\"Describing Semantic Representations of Brain Activity Evoked by Visual Stimuli\",\"url\":\"https://www.semanticscholar.org/paper/6ef8407c9c8f9153f25d5339724c357191b47202\",\"venue\":\"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1708.09522\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2c51069d03974bd28dd821142a852ec24ce7546a\",\"title\":\"Action Classification and Highlighting in Videos\",\"url\":\"https://www.semanticscholar.org/paper/2c51069d03974bd28dd821142a852ec24ce7546a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9260404\",\"name\":\"Xiaotong Du\"},{\"authorId\":\"46685438\",\"name\":\"J. Yuan\"},{\"authorId\":\"47652550\",\"name\":\"L. Hu\"},{\"authorId\":\"122907636\",\"name\":\"Yuke Dai\"}],\"doi\":\"10.1007/s00371-018-1591-x\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4170882122b559fc39ab3eafd66babe2429ba858\",\"title\":\"Description generation of open-domain videos incorporating multimodal features and bidirectional encoder\",\"url\":\"https://www.semanticscholar.org/paper/4170882122b559fc39ab3eafd66babe2429ba858\",\"venue\":\"The Visual Computer\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2889541\",\"name\":\"Guangkai Ma\"},{\"authorId\":\"49415704\",\"name\":\"Y. Wang\"},{\"authorId\":\"1728174\",\"name\":\"L. Wu\"}],\"doi\":\"10.1016/j.neucom.2016.10.047\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"06880a9472b3d2f7bcf5666727d3542d17d280d9\",\"title\":\"Subspace ensemble learning via totally-corrective boosting for gait recognition\",\"url\":\"https://www.semanticscholar.org/paper/06880a9472b3d2f7bcf5666727d3542d17d280d9\",\"venue\":\"Neurocomputing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-54407-6_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"title\":\"Video Captioning via Sentence Augmentation and Spatio-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":\"1611.07810\",\"authors\":[{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":\"10.1109/CVPR.2017.778\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"title\":\"A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1703.07022\",\"authors\":[{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"1682058\",\"name\":\"H. Zhang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.1109/ICCV.2017.364\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"428818a9edfb547431be6d7ec165c6af576c83d5\",\"title\":\"Recurrent Topic-Transition GAN for Visual Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/428818a9edfb547431be6d7ec165c6af576c83d5\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":\"1511.06973\",\"authors\":[{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"48319305\",\"name\":\"P. Wang\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2016.500\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20dbdf02497aa84510970d0f5e8b599073bca1bc\",\"title\":\"Ask Me Anything: Free-Form Visual Question Answering Based on Knowledge from External Sources\",\"url\":\"https://www.semanticscholar.org/paper/20dbdf02497aa84510970d0f5e8b599073bca1bc\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1686715\",\"name\":\"S. Yu\"},{\"authorId\":\"49388033\",\"name\":\"Y. Cheng\"},{\"authorId\":\"8143876\",\"name\":\"Songzhi Su\"},{\"authorId\":\"1900810\",\"name\":\"Guo-Rong Cai\"},{\"authorId\":\"8086812\",\"name\":\"Shaozi Li\"}],\"doi\":\"10.1007/s11042-016-3768-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5d5cc27ca519d1300e77e3c1a535a089f52f646\",\"title\":\"Stratified pooling based deep convolutional neural networks for human action recognition\",\"url\":\"https://www.semanticscholar.org/paper/d5d5cc27ca519d1300e77e3c1a535a089f52f646\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"47119743\",\"name\":\"X. Wang\"},{\"authorId\":\"145222820\",\"name\":\"H. Ye\"},{\"authorId\":\"145905953\",\"name\":\"X. Xue\"}],\"doi\":\"10.1145/2964284.2964328\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"899be93e14d991017b1f8a4afdf907cbc03cf300\",\"title\":\"Multi-Stream Multi-Class Fusion of Deep Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/899be93e14d991017b1f8a4afdf907cbc03cf300\",\"venue\":\"ACM Multimedia\",\"year\":2016},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c92a7c8446ac9d23b1c6130486255d76eaca7a81\",\"title\":\"WLSI-OIAF4HLT 2016 Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies\",\"url\":\"https://www.semanticscholar.org/paper/c92a7c8446ac9d23b1c6130486255d76eaca7a81\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2012.11673\",\"authors\":[{\"authorId\":\"1900466056\",\"name\":\"Sirjan Kafle\"},{\"authorId\":\"1633418058\",\"name\":\"Aman Gupta\"},{\"authorId\":\"1410725735\",\"name\":\"Xue Xia\"},{\"authorId\":\"145630384\",\"name\":\"A. Sankar\"},{\"authorId\":\"46772427\",\"name\":\"X. Chen\"},{\"authorId\":\"144006576\",\"name\":\"D. Wen\"},{\"authorId\":\"2121454\",\"name\":\"Libao Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18d91abf683d10837f384c1b4c9a3f9f90cf3e87\",\"title\":\"Smoothed Gaussian Mixture Models for Video Classification and Recommendation\",\"url\":\"https://www.semanticscholar.org/paper/18d91abf683d10837f384c1b4c9a3f9f90cf3e87\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1906.01452\",\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474871\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/TPAMI.2019.2920899\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"83a3fe38887880bccc15daa740d8d5041f826d91\",\"title\":\"Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/83a3fe38887880bccc15daa740d8d5041f826d91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3491491\",\"name\":\"Shiyang Yan\"},{\"authorId\":\"1410066063\",\"name\":\"Yuan Xie\"},{\"authorId\":\"152345893\",\"name\":\"F. Wu\"},{\"authorId\":\"33830793\",\"name\":\"J. Smith\"},{\"authorId\":\"40178769\",\"name\":\"Wenjin Lu\"},{\"authorId\":\"46824190\",\"name\":\"B. Zhang\"}],\"doi\":\"10.1016/j.sigpro.2019.107329\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea53299a067694a24e5e9cf8e852e122d5918847\",\"title\":\"Image captioning via hierarchical attention mechanism and policy gradient optimization\",\"url\":\"https://www.semanticscholar.org/paper/ea53299a067694a24e5e9cf8e852e122d5918847\",\"venue\":\"Signal Process.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48195668\",\"name\":\"Jin Yuan\"},{\"authorId\":\"2094026\",\"name\":\"Chunna Tian\"},{\"authorId\":\"46448210\",\"name\":\"Xiangnan Zhang\"},{\"authorId\":\"47814961\",\"name\":\"Y. Ding\"},{\"authorId\":\"145673165\",\"name\":\"Wei Wei\"}],\"doi\":\"10.1109/BigMM.2018.8499357\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ad8d0432bdc1fcefbd7ebc8badea8aceec16fbdf\",\"title\":\"Video Captioning with Semantic Guiding\",\"url\":\"https://www.semanticscholar.org/paper/ad8d0432bdc1fcefbd7ebc8badea8aceec16fbdf\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2447631\",\"name\":\"Junyeong Kim\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":\"10.1109/ICIP.2017.8296918\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9199c2b8cdad80735651a6a8bb9115c83acae650\",\"title\":\"Deep partial person re-identification via attention model\",\"url\":\"https://www.semanticscholar.org/paper/9199c2b8cdad80735651a6a8bb9115c83acae650\",\"venue\":\"2017 IEEE International Conference on Image Processing (ICIP)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50754085\",\"name\":\"Kunpeng Li\"},{\"authorId\":\"49479171\",\"name\":\"Chen Fang\"},{\"authorId\":\"8056043\",\"name\":\"Zhaowen Wang\"},{\"authorId\":\"2047181\",\"name\":\"Seokhwan Kim\"},{\"authorId\":\"41151701\",\"name\":\"H. Jin\"},{\"authorId\":\"144015161\",\"name\":\"Y. Fu\"}],\"doi\":\"10.1109/cvpr42600.2020.01254\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1a780c6219996c8481c117056efcf071cbfbd15\",\"title\":\"Screencast Tutorial Video Understanding\",\"url\":\"https://www.semanticscholar.org/paper/b1a780c6219996c8481c117056efcf071cbfbd15\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2007.11888\",\"authors\":[{\"authorId\":\"48028411\",\"name\":\"T. Jin\"},{\"authorId\":\"1796216614\",\"name\":\"Siyu Huang\"},{\"authorId\":\"1796254\",\"name\":\"M. Chen\"},{\"authorId\":\"2367491\",\"name\":\"Y. Li\"},{\"authorId\":\"9338907\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.24963/ijcai.2020/88\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"884be34dd5d2ea78940da96d2813be7768933857\",\"title\":\"SBAT: Video Captioning with Sparse Boundary-Aware Transformer\",\"url\":\"https://www.semanticscholar.org/paper/884be34dd5d2ea78940da96d2813be7768933857\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47751104\",\"name\":\"Dali Yang\"},{\"authorId\":\"144204924\",\"name\":\"C. Yuan\"}],\"doi\":\"10.1109/ICIP.2018.8451740\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"758d1c17569eea2a698cac31b2d9d2a772c84322\",\"title\":\"Hierarchical Context Encoding for Events Captioning in Videos\",\"url\":\"https://www.semanticscholar.org/paper/758d1c17569eea2a698cac31b2d9d2a772c84322\",\"venue\":\"2018 25th IEEE International Conference on Image Processing (ICIP)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2601489\",\"name\":\"Fu-Hsiang Chan\"},{\"authorId\":\"1731362\",\"name\":\"Y. Chen\"},{\"authorId\":\"144863550\",\"name\":\"Y. Xiang\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-54190-7_9\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4a83d9d07cbac4a8a279073e3873d01f3215f2f8\",\"title\":\"Anticipating Accidents in Dashcam Videos\",\"url\":\"https://www.semanticscholar.org/paper/4a83d9d07cbac4a8a279073e3873d01f3215f2f8\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":\"1604.03517\",\"authors\":[{\"authorId\":\"2445131\",\"name\":\"Hyungtae Lee\"},{\"authorId\":\"1688527\",\"name\":\"H. Kwon\"},{\"authorId\":\"3372204\",\"name\":\"A. J. Bency\"},{\"authorId\":\"2092466\",\"name\":\"W. Nothwang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"947f2d465df60ec49f441f02733edbeb81dde2f2\",\"title\":\"Fast Object Localization Using a CNN Feature Map Based Multi-Scale Search\",\"url\":\"https://www.semanticscholar.org/paper/947f2d465df60ec49f441f02733edbeb81dde2f2\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1604.06838\",\"authors\":[{\"authorId\":\"40240283\",\"name\":\"J. Dong\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"},{\"authorId\":\"145404204\",\"name\":\"Cees G. M. Snoek\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca366bc08a738a92e2c7e2c142ec853dbea3b82b\",\"title\":\"Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction\",\"url\":\"https://www.semanticscholar.org/paper/ca366bc08a738a92e2c7e2c142ec853dbea3b82b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"title\":\"Video Captioning and Retrieval Models with Semantic Attention\",\"url\":\"https://www.semanticscholar.org/paper/1ecb6b37ed067b2f16dbb6f476d449f113fae534\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1610.00731\",\"authors\":[{\"authorId\":\"7590435\",\"name\":\"Siva Karthik Mustikovela\"},{\"authorId\":\"143672748\",\"name\":\"M. Yang\"},{\"authorId\":\"1756036\",\"name\":\"C. Rother\"}],\"doi\":\"10.1007/978-3-319-49409-8_66\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb527b04c760c0acd0e7c74fc36da9dc75461fa9\",\"title\":\"Can Ground Truth Label Propagation from Video Help Semantic Segmentation?\",\"url\":\"https://www.semanticscholar.org/paper/cb527b04c760c0acd0e7c74fc36da9dc75461fa9\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"22066021\",\"name\":\"Xishan Zhang\"},{\"authorId\":\"50678151\",\"name\":\"Bingtao Liu\"},{\"authorId\":\"7590116\",\"name\":\"C. Yan\"}],\"doi\":\"10.1145/3123266.3123354\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"title\":\"Video Description with Spatial-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/c2cf74ba6f107aa9508e7ef1bad93916d944cb4c\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1807.06779\",\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"5196783\",\"name\":\"Yuancheng Wang\"},{\"authorId\":\"144654041\",\"name\":\"N. Li\"},{\"authorId\":\"48684256\",\"name\":\"X. Cheng\"},{\"authorId\":\"2284679\",\"name\":\"Yifeng Zhang\"},{\"authorId\":\"1713582\",\"name\":\"Yongming Huang\"},{\"authorId\":\"1748411\",\"name\":\"G. Lu\"}],\"doi\":\"10.1109/ICPR.2018.8545760\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8b06c51ceacd08a8b2f9d3551e007e0dbb019fb2\",\"title\":\"An Attention-Based Approach for Single Image Super Resolution\",\"url\":\"https://www.semanticscholar.org/paper/8b06c51ceacd08a8b2f9d3551e007e0dbb019fb2\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40685026\",\"name\":\"Duona Zhang\"},{\"authorId\":\"2911928\",\"name\":\"Wenrui Ding\"},{\"authorId\":\"1740430\",\"name\":\"B. Zhang\"},{\"authorId\":\"3471524\",\"name\":\"C. Xie\"},{\"authorId\":\"46381969\",\"name\":\"H. Li\"},{\"authorId\":\"49046516\",\"name\":\"C. Liu\"},{\"authorId\":\"1783847\",\"name\":\"J. Han\"}],\"doi\":\"10.3390/s18030924\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"af8709c837e324bc875ced6fe6b2527cb7b1f1ea\",\"title\":\"Automatic Modulation Classification Based on Deep Learning for Unmanned Aerial Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/af8709c837e324bc875ced6fe6b2527cb7b1f1ea\",\"venue\":\"Sensors\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389940060\",\"name\":\"Felix Juefei-Xu\"},{\"authorId\":\"7476726\",\"name\":\"Eshan Verma\"},{\"authorId\":\"7747740\",\"name\":\"Parag Goel\"},{\"authorId\":\"1412708076\",\"name\":\"Anisha Cherodian\"},{\"authorId\":\"1794486\",\"name\":\"M. Savvides\"}],\"doi\":\"10.1109/CVPRW.2016.24\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5720c765f38e6a98d79b889f3f3534b7e6f4bfa\",\"title\":\"DeepGender: Occlusion and Low Resolution Robust Facial Gender Classification via Progressively Trained Convolutional Neural Networks with Attention\",\"url\":\"https://www.semanticscholar.org/paper/e5720c765f38e6a98d79b889f3f3534b7e6f4bfa\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2016},{\"arxivId\":\"1703.08338\",\"authors\":[{\"authorId\":\"145032628\",\"name\":\"Michael Wray\"},{\"authorId\":\"3420479\",\"name\":\"Davide Moltisanti\"},{\"authorId\":\"1398236231\",\"name\":\"Walterio W. Mayol-Cuevas\"},{\"authorId\":\"145089978\",\"name\":\"Dima Damen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b408a3ca6fb39b0fda4d77e6a9679003b2dc9ab\",\"title\":\"Improving Classification by Improving Labelling: Introducing Probabilistic Multi-Label Object Interaction Recognition\",\"url\":\"https://www.semanticscholar.org/paper/3b408a3ca6fb39b0fda4d77e6a9679003b2dc9ab\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145704540\",\"name\":\"Li Ren\"},{\"authorId\":\"2272096\",\"name\":\"Guo-Jun Qi\"},{\"authorId\":\"66719728\",\"name\":\"Kien A. Hua\"}],\"doi\":\"10.1109/WACV.2019.00034\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"449b9189e3058d33f871dfd3b07cc75a717038f7\",\"title\":\"Improving Diversity of Image Captioning Through Variational Autoencoders and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/449b9189e3058d33f871dfd3b07cc75a717038f7\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3233968\",\"name\":\"Mahsa Ghafarianzadeh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"53f4c27cc50001c56b9be1fb3f8023f40db95466\",\"title\":\"Towards Temporal Semantic Scene Understanding\",\"url\":\"https://www.semanticscholar.org/paper/53f4c27cc50001c56b9be1fb3f8023f40db95466\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1912.11872\",\"authors\":[{\"authorId\":\"153040576\",\"name\":\"T. Mei\"},{\"authorId\":\"101586660\",\"name\":\"W. Zhang\"},{\"authorId\":\"48577275\",\"name\":\"Ting Yao\"}],\"doi\":\"10.1017/ATSIP.2020.10\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"title\":\"Vision and Language: from Visual Perception to Content Creation\",\"url\":\"https://www.semanticscholar.org/paper/3ba3e7970fac892ed3079d570ef019fa0940fec2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500424473\",\"name\":\"Sadia Ilyas\"},{\"authorId\":\"1471459683\",\"name\":\"Hafeez Ur Rehman\"}],\"doi\":\"10.1109/ICET48972.2019.8994567\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db9b8afff5592b62829762879c35383337ebffba\",\"title\":\"A Deep Learning based Approach for Precise Video Tagging\",\"url\":\"https://www.semanticscholar.org/paper/db9b8afff5592b62829762879c35383337ebffba\",\"venue\":\"2019 15th International Conference on Emerging Technologies (ICET)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39650418\",\"name\":\"S. Chen\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39024a168ea1511821e5af17bdf838bf4afb3db8\",\"title\":\"FDU Participation in TRECVID 2019 VTT Task\",\"url\":\"https://www.semanticscholar.org/paper/39024a168ea1511821e5af17bdf838bf4afb3db8\",\"venue\":\"TRECVID\",\"year\":2019},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1912.06617\",\"authors\":[{\"authorId\":\"28798386\",\"name\":\"H. Doughty\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"1398236231\",\"name\":\"W. Mayol-Cuevas\"},{\"authorId\":\"145089978\",\"name\":\"D. Damen\"}],\"doi\":\"10.1109/CVPR42600.2020.00095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1bc01f26b29282855e7cc997a737aa72697a4cac\",\"title\":\"Action Modifiers: Learning From Adverbs in Instructional Videos\",\"url\":\"https://www.semanticscholar.org/paper/1bc01f26b29282855e7cc997a737aa72697a4cac\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"1903.00102\",\"authors\":[{\"authorId\":\"1414566008\",\"name\":\"Ghadeer Al-Bdour\"},{\"authorId\":\"1414508153\",\"name\":\"Raffi Al-Qurran\"},{\"authorId\":\"1398466553\",\"name\":\"Mahmoud Al-Ayyoub\"},{\"authorId\":\"1716400\",\"name\":\"A. Shatnawi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d92aca14cfb6257dfba45fcbc1d6dd9455f53163\",\"title\":\"A detailed comparative study of open source deep learning frameworks\",\"url\":\"https://www.semanticscholar.org/paper/d92aca14cfb6257dfba45fcbc1d6dd9455f53163\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1708.00339\",\"authors\":[{\"authorId\":\"3407330\",\"name\":\"R. Singh\"},{\"authorId\":\"3369052\",\"name\":\"Jack Lanchantin\"},{\"authorId\":\"5968525\",\"name\":\"Arshdeep Sekhon\"},{\"authorId\":\"1791105\",\"name\":\"Y. Qi\"}],\"doi\":\"10.1101/329334\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aff46129c5cb27fe3c000e0699c71dbdb7dc26a5\",\"title\":\"Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin\",\"url\":\"https://www.semanticscholar.org/paper/aff46129c5cb27fe3c000e0699c71dbdb7dc26a5\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27079423\",\"name\":\"H. Hajj\"},{\"authorId\":\"1727789\",\"name\":\"M. Lamard\"},{\"authorId\":\"2742744\",\"name\":\"P. Conze\"},{\"authorId\":\"2873874\",\"name\":\"Soumali Roychowdhury\"},{\"authorId\":\"48539252\",\"name\":\"Xiaowei Hu\"},{\"authorId\":\"16148311\",\"name\":\"Gabija Marsalkaite\"},{\"authorId\":\"1838348\",\"name\":\"Odysseas Zisimopoulos\"},{\"authorId\":\"51064939\",\"name\":\"Muneer Ahmad Dedmari\"},{\"authorId\":\"50686602\",\"name\":\"Fenqiang Zhao\"},{\"authorId\":\"46217866\",\"name\":\"Jonas Prellberg\"},{\"authorId\":\"47613401\",\"name\":\"Manish Sahu\"},{\"authorId\":\"2161526\",\"name\":\"Adrian Galdran\"},{\"authorId\":\"144675705\",\"name\":\"Teresa Ara\\u00fajo\"},{\"authorId\":\"35464334\",\"name\":\"Duc My Vo\"},{\"authorId\":\"9399135\",\"name\":\"C. Panda\"},{\"authorId\":\"4282694\",\"name\":\"N. Dahiya\"},{\"authorId\":\"47159394\",\"name\":\"S. Kondo\"},{\"authorId\":\"1882475\",\"name\":\"Zhengbing Bian\"},{\"authorId\":\"1684052\",\"name\":\"G. Quellec\"}],\"doi\":\"10.1016/j.media.2018.11.008\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"857cfe6b7f2c428692443021788aeaa1e0eb0c45\",\"title\":\"CATARACTS: Challenge on automatic tool annotation for cataRACT surgery\",\"url\":\"https://www.semanticscholar.org/paper/857cfe6b7f2c428692443021788aeaa1e0eb0c45\",\"venue\":\"Medical Image Anal.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/3122865\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"title\":\"Frontiers of Multimedia Research\",\"url\":\"https://www.semanticscholar.org/paper/8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2782248\",\"name\":\"M. Y. K. Tani\"},{\"authorId\":\"2512422\",\"name\":\"Abdelghani Ghomari\"},{\"authorId\":\"1975525\",\"name\":\"Adel Lablack\"},{\"authorId\":\"3036685\",\"name\":\"Ioan Marius Bilasco\"}],\"doi\":\"10.1007/s13735-017-0133-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa6536b25f6131ec0aba6595f0106aaafee721fe\",\"title\":\"OVIS: ontology video surveillance indexing and retrieval system\",\"url\":\"https://www.semanticscholar.org/paper/aa6536b25f6131ec0aba6595f0106aaafee721fe\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/s11263-017-1033-7\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"828ac57f755db989e2886042a85278ae4823297c\",\"title\":\"Uncovering the Temporal Context for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/828ac57f755db989e2886042a85278ae4823297c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1707.06029\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"4945045\",\"name\":\"Yeonhwa Kim\"},{\"authorId\":\"143912065\",\"name\":\"Kyung Yoo\"},{\"authorId\":\"2135453\",\"name\":\"S. Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.648\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"title\":\"Supervising Neural Attention Models for Video Captioning by Human Gaze Data\",\"url\":\"https://www.semanticscholar.org/paper/1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2018/164\",\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"2f4821a615f08fdad69957a19366c79d939bfd5f\",\"title\":\"Video Captioning with Tube Features\",\"url\":\"https://www.semanticscholar.org/paper/2f4821a615f08fdad69957a19366c79d939bfd5f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46651496\",\"name\":\"Chengxi Li\"},{\"authorId\":\"153194886\",\"name\":\"Sagar Gandhi\"},{\"authorId\":\"35066258\",\"name\":\"B. Harrison\"}],\"doi\":\"10.1145/3337722.3341870\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79b31c51375fdfe87d854d238d2bfb4696fd71cf\",\"title\":\"End-to-end let's play commentary generation using multi-modal video representations\",\"url\":\"https://www.semanticscholar.org/paper/79b31c51375fdfe87d854d238d2bfb4696fd71cf\",\"venue\":\"FDG\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92827207\",\"name\":\"J. Chen\"},{\"authorId\":\"118150711\",\"name\":\"J. Liang\"},{\"authorId\":\"1558681028\",\"name\":\"Jiang Liu\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"145196759\",\"name\":\"Chenqiang Gao\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c023a11e4b165672ae65acc3081c5b943163e9df\",\"title\":\"Informedia @ TRECVID 2017\",\"url\":\"https://www.semanticscholar.org/paper/c023a11e4b165672ae65acc3081c5b943163e9df\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145443283\",\"name\":\"A. Asadi\"},{\"authorId\":\"1682051\",\"name\":\"R. Safabakhsh\"}],\"doi\":\"10.1007/978-3-030-31756-0_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"title\":\"The Encoder-Decoder Framework and Its Applications\",\"url\":\"https://www.semanticscholar.org/paper/0e299ff8156d4c935f55edae12a1aa884de27e8a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1609.08758\",\"authors\":[{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"},{\"authorId\":\"3111194\",\"name\":\"J. Heikkil\\u00e4\"},{\"authorId\":\"1771769\",\"name\":\"N. Yokoya\"}],\"doi\":\"10.1007/978-3-319-54193-8_23\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"79f2bc7bf4c37e66bfc35b2de99d51c1f5e77a5f\",\"title\":\"Video Summarization Using Deep Semantic Features\",\"url\":\"https://www.semanticscholar.org/paper/79f2bc7bf4c37e66bfc35b2de99d51c1f5e77a5f\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2015.7298878\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5838af587938e74b5758414c384dcf16dd6e1d1e\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/5838af587938e74b5758414c384dcf16dd6e1d1e\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":\"2008.09791\",\"authors\":[{\"authorId\":\"46979645\",\"name\":\"J. Park\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"}],\"doi\":\"10.1007/978-3-030-58589-1_22\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"title\":\"Identity-Aware Multi-Sentence Video Description\",\"url\":\"https://www.semanticscholar.org/paper/21a1b6f4f56c1fb6d844c5a1d971c59ab9cf81f7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"3145905\",\"name\":\"Jingqiu Zhang\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0530-0\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"title\":\"Exploiting long-term temporal dynamics for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":\"1910.06737\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"50976845\",\"name\":\"Yida Zhao\"},{\"authorId\":\"40280182\",\"name\":\"Yuqing Song\"},{\"authorId\":\"143715671\",\"name\":\"Qin Jin\"},{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac87d8ef6da4be7d7822053355c0528c58d8ddf5\",\"title\":\"Integrating Temporal and Spatial Attentions for VATEX Video Captioning Challenge 2019\",\"url\":\"https://www.semanticscholar.org/paper/ac87d8ef6da4be7d7822053355c0528c58d8ddf5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2719746\",\"name\":\"Parag Jain\"},{\"authorId\":\"49774944\",\"name\":\"P. Agrawal\"},{\"authorId\":\"1746093\",\"name\":\"A. Mishra\"},{\"authorId\":\"3026786\",\"name\":\"M. Sukhwani\"},{\"authorId\":\"2039596\",\"name\":\"Anirban Laha\"},{\"authorId\":\"145590185\",\"name\":\"K. Sankaranarayanan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b551c5893504f581a76a4dd8a423f18a07b6681c\",\"title\":\"Generation from Sequence of Independent Short Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/b551c5893504f581a76a4dd8a423f18a07b6681c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1908.00943\",\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"49745735\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"199c2a410cf4430841907e27d5b7026efd95a6ec\",\"title\":\"Prediction and Description of Near-Future Activities in Video.\",\"url\":\"https://www.semanticscholar.org/paper/199c2a410cf4430841907e27d5b7026efd95a6ec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36251013\",\"name\":\"Wei Li\"},{\"authorId\":\"20412557\",\"name\":\"Dashan Guo\"},{\"authorId\":\"1706164\",\"name\":\"X. Fang\"}],\"doi\":\"10.1016/j.patrec.2017.10.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ba881ec9ed2b435468ba6bbdc1821bde7778417\",\"title\":\"Multimodal architecture for video captioning with memory networks and an attention mechanism\",\"url\":\"https://www.semanticscholar.org/paper/0ba881ec9ed2b435468ba6bbdc1821bde7778417\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2018},{\"arxivId\":\"1802.03594\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1016/j.csl.2019.04.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3aad607fca2bdd6a5387a3cd323d61c71852fa8\",\"title\":\"Online Learning for Effort Reduction in Interactive Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/e3aad607fca2bdd6a5387a3cd323d61c71852fa8\",\"venue\":\"Comput. Speech Lang.\",\"year\":2019},{\"arxivId\":\"1805.09039\",\"authors\":[{\"authorId\":\"1690722\",\"name\":\"S. Chatzis\"},{\"authorId\":\"15699963\",\"name\":\"Aristotelis Charalampous\"},{\"authorId\":\"46192035\",\"name\":\"Kyriacos Tolias\"},{\"authorId\":\"24281427\",\"name\":\"Sotiris A. Vassou\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"293f528b107b3fa74de65826f11245c27355f82a\",\"title\":\"Amortized Context Vector Inference for Sequence-to-Sequence Networks\",\"url\":\"https://www.semanticscholar.org/paper/293f528b107b3fa74de65826f11245c27355f82a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2736335\",\"name\":\"Q. Abbas\"},{\"authorId\":\"32041482\",\"name\":\"M. Ibrahim\"},{\"authorId\":\"144889214\",\"name\":\"M. Jaffar\"}],\"doi\":\"10.1007/s11042-017-5438-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c250c1b8f160e953c2ac0253860abd6a27b8f7f7\",\"title\":\"Video scene analysis: an overview and challenges on deep learning algorithms\",\"url\":\"https://www.semanticscholar.org/paper/c250c1b8f160e953c2ac0253860abd6a27b8f7f7\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2017},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.04119\",\"authors\":[{\"authorId\":\"145478041\",\"name\":\"Shikhar Sharma\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7b8810ad8ef9ddd024583f95a51559e6c1b8c754\",\"title\":\"Action Recognition using Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/7b8810ad8ef9ddd024583f95a51559e6c1b8c754\",\"venue\":\"NIPS 2015\",\"year\":2015},{\"arxivId\":\"1612.05571\",\"authors\":[{\"authorId\":\"145243593\",\"name\":\"D. Neil\"},{\"authorId\":\"2954078\",\"name\":\"Junhaeng Lee\"},{\"authorId\":\"1694635\",\"name\":\"T. Delbr\\u00fcck\"},{\"authorId\":\"1704961\",\"name\":\"Shih-Chii Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12002c640cac2a8852236719a8cbccc47aafd1c8\",\"title\":\"Delta Networks for Optimized Recurrent Network Computation\",\"url\":\"https://www.semanticscholar.org/paper/12002c640cac2a8852236719a8cbccc47aafd1c8\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"1611.09053\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Yi Yang\"}],\"doi\":\"10.1109/CVPR.2017.147\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"533d14e539ae5cdca0ece392487a2b19106d468a\",\"title\":\"Bidirectional Multirate Reconstruction for Temporal Modeling in Videos\",\"url\":\"https://www.semanticscholar.org/paper/533d14e539ae5cdca0ece392487a2b19106d468a\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2017/307\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e33bc5c83f2cea403a5521385ee8e2794b311275\",\"title\":\"MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e33bc5c83f2cea403a5521385ee8e2794b311275\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27733289\",\"name\":\"J. Shao\"},{\"authorId\":\"3284268\",\"name\":\"Changwen Qu\"},{\"authorId\":\"1690210\",\"name\":\"J. Li\"},{\"authorId\":\"2536684\",\"name\":\"Shujuan Peng\"}],\"doi\":\"10.3390/s18093039\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1742cc9437ecd0d56a8336a8c2b91c5e68ab11e9\",\"title\":\"A Lightweight Convolutional Neural Network Based on Visual Attention for SAR Image Target Classification\",\"url\":\"https://www.semanticscholar.org/paper/1742cc9437ecd0d56a8336a8c2b91c5e68ab11e9\",\"venue\":\"Sensors\",\"year\":2018},{\"arxivId\":\"1906.04375\",\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/CVPR.2019.00852\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"title\":\"Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"50080046\",\"name\":\"X. Li\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.1109/TIP.2019.2916757\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"title\":\"CAM-RNN: Co-Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/acc2cfe35343195a4f3d0df5d7841d47708208fb\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3863922\",\"name\":\"C. Yan\"},{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"48631703\",\"name\":\"Xingzheng Wang\"},{\"authorId\":\"5094646\",\"name\":\"Y. Zhang\"},{\"authorId\":\"145922541\",\"name\":\"Xinhong Hao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144954808\",\"name\":\"Q. Dai\"}],\"doi\":\"10.1109/TMM.2019.2924576\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d\",\"title\":\"STAT: Spatial-Temporal Attention Mechanism for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2125211\",\"name\":\"Yueting Zhuang\"},{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"1491414917\",\"name\":\"Xin Yan\"},{\"authorId\":\"4004957\",\"name\":\"W. Cheng\"},{\"authorId\":\"47122664\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"3290437\",\"name\":\"S. Pu\"},{\"authorId\":\"1384523745\",\"name\":\"Jun Xiao\"}],\"doi\":\"10.1145/3366710\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"title\":\"Multichannel Attention Refinement for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/26997b5e761bfa0f98331e297b6e9518fef3ece1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.12980\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1007/978-3-030-31332-6_2\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a1d0f9b4be8930804b994158f83f08b1d7252233\",\"title\":\"Interactive-predictive neural multimodal systems\",\"url\":\"https://www.semanticscholar.org/paper/a1d0f9b4be8930804b994158f83f08b1d7252233\",\"venue\":\"IbPRIA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49251978\",\"name\":\"J. Chen\"},{\"authorId\":\"41079034\",\"name\":\"Hong-Yang Chao\"}],\"doi\":\"10.1145/3394171.3416291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"598ad06c164043c45c952dbde37e0c75991e66aa\",\"title\":\"VideoTRM: Pre-training for Video Captioning Challenge 2020\",\"url\":\"https://www.semanticscholar.org/paper/598ad06c164043c45c952dbde37e0c75991e66aa\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":\"2007.09049\",\"authors\":[{\"authorId\":\"1810689822\",\"name\":\"Ganchao Tan\"},{\"authorId\":\"48929163\",\"name\":\"Daqing Liu\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"}],\"doi\":\"10.24963/ijcai.2020/104\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"title\":\"Learning to Discretely Compose Reasoning Module Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/a7e8aeca681e6409aa73ab0f70ff9e6fb891d071\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1858122289\",\"name\":\"Rahma Abed\"},{\"authorId\":\"2060089\",\"name\":\"S. Bahroun\"},{\"authorId\":\"69238535\",\"name\":\"E. Zagrouba\"}],\"doi\":\"10.1007/s11042-020-09385-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82af8b49a70300b0fad10e17be7e5416fec90ae1\",\"title\":\"KeyFrame extraction based on face quality measurement and convolutional neural network for efficient face recognition in videos\",\"url\":\"https://www.semanticscholar.org/paper/82af8b49a70300b0fad10e17be7e5416fec90ae1\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"1974929\",\"name\":\"Jiewu Xia\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1007/s11042-020-09674-z\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"title\":\"Double-channel language feature mining based model for video description\",\"url\":\"https://www.semanticscholar.org/paper/0d3f2e97df8488767e7d6f71f628e2169ec0969c\",\"venue\":\"Multim. Tools Appl.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4b5c35e70954a05ec4b836f166882982f459eefa\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/4b5c35e70954a05ec4b836f166882982f459eefa\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1711.11135\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"2928777\",\"name\":\"Wenhu Chen\"},{\"authorId\":\"46365930\",\"name\":\"Jiawei Wu\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"}],\"doi\":\"10.1109/CVPR.2018.00443\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"74b284a66e75b65f5970d05bac000fe91243ee49\",\"title\":\"Video Captioning via Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/74b284a66e75b65f5970d05bac000fe91243ee49\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"153576781\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/JIOT.2017.2779865\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"title\":\"Attention-in-Attention Networks for Surveillance Video Understanding in Internet of Things\",\"url\":\"https://www.semanticscholar.org/paper/d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"venue\":\"IEEE Internet of Things Journal\",\"year\":2018},{\"arxivId\":\"1607.02556\",\"authors\":[{\"authorId\":\"35585536\",\"name\":\"Jialin Wu\"},{\"authorId\":\"29644358\",\"name\":\"Gu Wang\"},{\"authorId\":\"3432961\",\"name\":\"Wukui Yang\"},{\"authorId\":\"7807689\",\"name\":\"Xiangyang Ji\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2288696b6558b7397bdebe3aed77bedec7b9c0a9\",\"title\":\"Action Recognition with Joint Attention on Multi-Level Deep Features\",\"url\":\"https://www.semanticscholar.org/paper/2288696b6558b7397bdebe3aed77bedec7b9c0a9\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2302223\",\"name\":\"Manolis Vasileiadis\"},{\"authorId\":\"4408876\",\"name\":\"C. Bouganis\"},{\"authorId\":\"15784009\",\"name\":\"G. Stavropoulos\"},{\"authorId\":\"143636644\",\"name\":\"D. Tzovaras\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"24db2c29d4a1b2b4130ff2123816873ed6b90a4e\",\"title\":\"Optimising 3D-CNN Design towards Human Pose Estimation on Low Power Devices\",\"url\":\"https://www.semanticscholar.org/paper/24db2c29d4a1b2b4130ff2123816873ed6b90a4e\",\"venue\":\"BMVC\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66767033\",\"name\":\"K. Randive\"},{\"authorId\":\"144532163\",\"name\":\"R. Mohan\"}],\"doi\":\"10.1007/978-3-030-16657-1_99\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"564300c955cf0a6b00a7e06b5c0664f80da6fe49\",\"title\":\"A State-of-Art Review on Automatic Video Annotation Techniques\",\"url\":\"https://www.semanticscholar.org/paper/564300c955cf0a6b00a7e06b5c0664f80da6fe49\",\"venue\":\"ISDA\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49868702\",\"name\":\"Ran Wei\"},{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1016/j.jvcir.2020.102751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"title\":\"Exploiting the local temporal information for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"}],\"doi\":\"10.1145/2911996.2912043\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"title\":\"Video Description Generation using Audio and Visual Cues\",\"url\":\"https://www.semanticscholar.org/paper/54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"venue\":\"ICMR\",\"year\":2016},{\"arxivId\":\"2007.08751\",\"authors\":[{\"authorId\":\"26385137\",\"name\":\"Noa Garcia\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"}],\"doi\":\"10.1007/978-3-030-58523-5_34\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"773e7d33411fc2cdd6829356b7ce8ed34e14cd65\",\"title\":\"Knowledge-Based Video Question Answering with Unsupervised Scene Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/773e7d33411fc2cdd6829356b7ce8ed34e14cd65\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100715052\",\"name\":\"Yi Tang\"},{\"authorId\":\"2568383\",\"name\":\"Wenbin Zou\"},{\"authorId\":\"151472634\",\"name\":\"Y. Hua\"},{\"authorId\":\"98466827\",\"name\":\"Z. Jin\"},{\"authorId\":\"15524303\",\"name\":\"Xia Li\"}],\"doi\":\"10.1016/j.neucom.2019.09.064\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a015dba578ac82532c0968053dc2fd109a6ea73f\",\"title\":\"Video salient object detection via spatiotemporal attention neural networks\",\"url\":\"https://www.semanticscholar.org/paper/a015dba578ac82532c0968053dc2fd109a6ea73f\",\"venue\":\"Neurocomputing\",\"year\":2020},{\"arxivId\":\"1708.04116\",\"authors\":[{\"authorId\":\"2857402\",\"name\":\"H. Park\"},{\"authorId\":\"145954697\",\"name\":\"C. Yoo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ca969c5ab22d8587d9fe1d0c8a1780aa26d5da3d\",\"title\":\"Early Improving Recurrent Elastic Highway Network\",\"url\":\"https://www.semanticscholar.org/paper/ca969c5ab22d8587d9fe1d0c8a1780aa26d5da3d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1610.04997\",\"authors\":[{\"authorId\":\"1975564\",\"name\":\"M. Zanfir\"},{\"authorId\":\"2045166\",\"name\":\"Elisabeta Marinoiu\"},{\"authorId\":\"1781120\",\"name\":\"C. Sminchisescu\"}],\"doi\":\"10.1007/978-3-319-54190-7_7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4ed613b6f0427d3ec4cad6c51dcc451786812959\",\"title\":\"Spatio-Temporal Attention Models for Grounded Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/4ed613b6f0427d3ec4cad6c51dcc451786812959\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907492\",\"name\":\"Felix Juefei-Xu\"},{\"authorId\":\"7476726\",\"name\":\"Eshan Verma\"},{\"authorId\":\"1794486\",\"name\":\"M. Savvides\"}],\"doi\":\"10.1007/978-3-319-61657-5_8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b2308649d960a41bca916390d6d041dd534f5c26\",\"title\":\"DeepGender2: A Generative Approach Toward Occlusion and Low-Resolution Robust Facial Gender Classification via Progressively Trained Attention Shift Convolutional Neural Networks (PTAS-CNN) and Deep Convolutional Generative Adversarial Networks (DCGAN)\",\"url\":\"https://www.semanticscholar.org/paper/b2308649d960a41bca916390d6d041dd534f5c26\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7644453\",\"name\":\"Hanqi Wang\"},{\"authorId\":\"1774936\",\"name\":\"Siliang Tang\"},{\"authorId\":\"46867455\",\"name\":\"Yin Sheng Zhang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"}],\"doi\":\"10.1145/3126686.3126715\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70f7bcfe2ce3789e62846c73e98feeaa319135e5\",\"title\":\"Learning Deep Contextual Attention Network for Narrative Photo Stream Captioning\",\"url\":\"https://www.semanticscholar.org/paper/70f7bcfe2ce3789e62846c73e98feeaa319135e5\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50854337\",\"name\":\"D. Xu\"},{\"authorId\":\"47122432\",\"name\":\"Zhou Zhao\"},{\"authorId\":\"145974111\",\"name\":\"Jun Xiao\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"7792071\",\"name\":\"X. He\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1145/3123266.3123427\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"057b80e235b10799d03876ad25465208a4c64caf\",\"title\":\"Video Question Answering via Gradually Refined Attention over Appearance and Motion\",\"url\":\"https://www.semanticscholar.org/paper/057b80e235b10799d03876ad25465208a4c64caf\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123607932\",\"name\":\"S. Chandar\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a0cec7f0f8b63f182ea0c52cb935580acabafcc\",\"title\":\"UOUS AND DISCRETE ADDRESSING SCHEMES\",\"url\":\"https://www.semanticscholar.org/paper/2a0cec7f0f8b63f182ea0c52cb935580acabafcc\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1804.08274\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2018.00782\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"title\":\"Jointly Localizing and Describing Events for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"5477477\",\"name\":\"Martin Renqiang Min\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"title\":\"Recent work often develops a probabilistic model of the caption , conditioned on a video\",\"url\":\"https://www.semanticscholar.org/paper/dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1708.09667\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3123420\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6199348281e14a5a127b539f5cdb92fcddbac17\",\"title\":\"Video Captioning with Guidance of Multimodal Latent Topics\",\"url\":\"https://www.semanticscholar.org/paper/a6199348281e14a5a127b539f5cdb92fcddbac17\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1505.05914\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"title\":\"A Multi-scale Multiple Instance Video Description Network\",\"url\":\"https://www.semanticscholar.org/paper/08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1903.10869\",\"authors\":[{\"authorId\":\"145062693\",\"name\":\"Anh Nguyen\"},{\"authorId\":\"3354627\",\"name\":\"Thanh-Toan Do\"},{\"authorId\":\"145950884\",\"name\":\"I. Reid\"},{\"authorId\":\"1745158\",\"name\":\"D. Caldwell\"},{\"authorId\":\"145887349\",\"name\":\"N. Tsagarakis\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fda87f56010b1e8d05a52a166fdb2750b4dec39b\",\"title\":\"V2CNet: A Deep Learning Framework to Translate Videos to Commands for Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/fda87f56010b1e8d05a52a166fdb2750b4dec39b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144132563\",\"name\":\"David Valdivieso L\\u00f3pez\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d57e59dcf27ee25e6540ecebdf0cc40e93180bb6\",\"title\":\"Design, implementation and evaluation of automated surveillance systems\",\"url\":\"https://www.semanticscholar.org/paper/d57e59dcf27ee25e6540ecebdf0cc40e93180bb6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1910.02602\",\"authors\":[{\"authorId\":\"1383481973\",\"name\":\"Yan Bin Ng\"},{\"authorId\":\"1688071\",\"name\":\"Basura Fernando\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"42b7b4bb2f96b21c33541a83606917be5eb6abbb\",\"title\":\"Human Action Sequence Classification\",\"url\":\"https://www.semanticscholar.org/paper/42b7b4bb2f96b21c33541a83606917be5eb6abbb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1145/3343031.3351072\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"db6035229a71a6c93d4f15c4a4280eb644228da4\",\"title\":\"Hierarchical Global-Local Temporal Modeling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/db6035229a71a6c93d4f15c4a4280eb644228da4\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"92104112\",\"name\":\"Tangming Chen\"},{\"authorId\":\"9398015\",\"name\":\"Qike Zhao\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"}],\"doi\":\"10.1007/978-3-030-33982-1_9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cf546bfb933bddbd9e7017525d9621405e549b9\",\"title\":\"Boundary Detector Encoder and Decoder with Soft Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1cf546bfb933bddbd9e7017525d9621405e549b9\",\"venue\":\"APWeb/WAIM Workshops\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"134649559\",\"name\":\"R. Kiziltepe\"},{\"authorId\":\"3000774\",\"name\":\"J. Gan\"},{\"authorId\":\"3361843\",\"name\":\"J. J. Escobar\"}],\"doi\":\"10.1007/978-3-030-20518-8_67\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb9a51bfd7be0aeb061f318c3d9201e07aa8c7d6\",\"title\":\"Combining Very Deep Convolutional Neural Networks and Recurrent Neural Networks for Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/bb9a51bfd7be0aeb061f318c3d9201e07aa8c7d6\",\"venue\":\"IWANN\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6516435\",\"name\":\"Mengxi Lin\"},{\"authorId\":\"1718406\",\"name\":\"N. Inoue\"},{\"authorId\":\"1704408\",\"name\":\"Koichi Shinoda\"}],\"doi\":\"10.1145/3126686.3126755\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4097fef623185557bb1842501cfdc97f812fc66d\",\"title\":\"CTC Network with Statistical Language Modeling for Action Sequence Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/4097fef623185557bb1842501cfdc97f812fc66d\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1802.10250\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACV.2019.00048\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99cdb10443a0543be3466c9231ff922bcc996843\",\"title\":\"Joint Event Detection and Description in Continuous Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/99cdb10443a0543be3466c9231ff922bcc996843\",\"venue\":\"2019 IEEE Winter Applications of Computer Vision Workshops (WACVW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"1728712\",\"name\":\"Boyang Li\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"title\":\"LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)\",\"url\":\"https://www.semanticscholar.org/paper/4efc523df04fe19b600e372b9cfc9acf2e0b21d8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1711.06778\",\"authors\":[{\"authorId\":\"3298267\",\"name\":\"Sarah Adel Bargal\"},{\"authorId\":\"144733334\",\"name\":\"A. Zunino\"},{\"authorId\":\"144118620\",\"name\":\"D. Kim\"},{\"authorId\":\"1701293\",\"name\":\"J. Zhang\"},{\"authorId\":\"1727204\",\"name\":\"Vittorio Murino\"},{\"authorId\":\"1749590\",\"name\":\"S. Sclaroff\"}],\"doi\":\"10.1109/CVPR.2018.00156\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad7fc80a2cc26221c077851f44afbd47e3ab6b46\",\"title\":\"Excitation Backprop for RNNs\",\"url\":\"https://www.semanticscholar.org/paper/ad7fc80a2cc26221c077851f44afbd47e3ab6b46\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20510630\",\"name\":\"Bahareh Nakisa\"},{\"authorId\":\"18059931\",\"name\":\"Mohammad Naim Rastgoo\"},{\"authorId\":\"94815327\",\"name\":\"A. Rakotonirainy\"},{\"authorId\":\"1737645\",\"name\":\"F. Maire\"},{\"authorId\":\"46916715\",\"name\":\"V. Chandran\"}],\"doi\":\"10.1109/ACCESS.2018.2868361\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f62c491dc93a77ba57f40f7e885e471024d139f\",\"title\":\"Long Short Term Memory Hyperparameter Optimization for a Neural Network Based Emotion Recognition Framework\",\"url\":\"https://www.semanticscholar.org/paper/2f62c491dc93a77ba57f40f7e885e471024d139f\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82912728\",\"name\":\"L. Maczyta\"},{\"authorId\":\"69340040\",\"name\":\"P. Bouthemy\"},{\"authorId\":\"1789744\",\"name\":\"O. Meur\"}],\"doi\":\"10.1016/j.patrec.2019.09.016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c1070e137192175345171188955b6538bc3fc456\",\"title\":\"CNN-based temporal detection of motion saliency in videos\",\"url\":\"https://www.semanticscholar.org/paper/c1070e137192175345171188955b6538bc3fc456\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2019},{\"arxivId\":\"1803.07950\",\"authors\":[{\"authorId\":\"4322411\",\"name\":\"L. Li\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"}],\"doi\":\"10.1109/WACV.2019.00042\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"title\":\"End-to-End Video Captioning With Multitask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/abcf7dd1e35575eaac12332aa4bc7575ccdd6965\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":\"1906.02792\",\"authors\":[{\"authorId\":\"74480447\",\"name\":\"Manjot Bilkhu\"},{\"authorId\":\"14506569\",\"name\":\"S. Wang\"},{\"authorId\":\"70060571\",\"name\":\"Tushar Dobhal\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b2cb203f6b09a3bf734c705c999da706b7a7c031\",\"title\":\"Attention is all you need for Videos: Self-attention based Video Summarization using Universal Transformers\",\"url\":\"https://www.semanticscholar.org/paper/b2cb203f6b09a3bf734c705c999da706b7a7c031\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1904.08492\",\"authors\":[{\"authorId\":\"3424093\",\"name\":\"Sumanth Chennupati\"},{\"authorId\":\"66443522\",\"name\":\"Ganesh Sistu\"},{\"authorId\":\"2601522\",\"name\":\"S. Yogamani\"},{\"authorId\":\"48341937\",\"name\":\"S. Rawashdeh\"}],\"doi\":\"10.1109/CVPRW.2019.00159\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e863a6b1bb39a38a88330f2609a560c3dfed6db5\",\"title\":\"MultiNet++: Multi-Stream Feature Aggregation and Geometric Loss Strategy for Multi-Task Learning\",\"url\":\"https://www.semanticscholar.org/paper/e863a6b1bb39a38a88330f2609a560c3dfed6db5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"144542135\",\"name\":\"D. Mahajan\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/CVPR.2018.00769\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3e94bffaa786b099a37d58e64e8dc870c7526b9\",\"title\":\"What Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and Datasets\",\"url\":\"https://www.semanticscholar.org/paper/c3e94bffaa786b099a37d58e64e8dc870c7526b9\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1810.12829\",\"authors\":[{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"40366185\",\"name\":\"Yukang Gan\"},{\"authorId\":\"2477116\",\"name\":\"Hejun Wu\"},{\"authorId\":\"1730284\",\"name\":\"N. Xiao\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/TIP.2018.2878956\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6ae6dfc11b2d741b9d9c757c10fec0f38eb28a9d\",\"title\":\"Cross-Modal Attentional Context Learning for RGB-D Object Detection\",\"url\":\"https://www.semanticscholar.org/paper/6ae6dfc11b2d741b9d9c757c10fec0f38eb28a9d\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46748462\",\"name\":\"E. Blasch\"},{\"authorId\":\"1847092\",\"name\":\"L. Grewe\"},{\"authorId\":\"1743503219\",\"name\":\"Edward L. Waltz\"},{\"authorId\":\"3292083\",\"name\":\"Paul Bendich\"},{\"authorId\":\"144658464\",\"name\":\"V. Pavlovic\"},{\"authorId\":\"3158478\",\"name\":\"I. Kadar\"},{\"authorId\":\"143732554\",\"name\":\"C. Chong\"}],\"doi\":\"10.1117/12.2559416\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"156f00ea76be4e0e79749a70c157bce2c5b51d21\",\"title\":\"Machine learning in/with information fusion for infrastructure understanding, panel summary\",\"url\":\"https://www.semanticscholar.org/paper/156f00ea76be4e0e79749a70c157bce2c5b51d21\",\"venue\":\"Defense + Commercial Sensing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"49730189\",\"name\":\"Boyang Li\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/WACVW.2019.00011\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32b3e8f7a673801d6bcfb482a72c52c78e96b006\",\"title\":\"Joint Event Detection and Description in Continuous Video Streams\",\"url\":\"https://www.semanticscholar.org/paper/32b3e8f7a673801d6bcfb482a72c52c78e96b006\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3424086\",\"name\":\"S. Sah\"},{\"authorId\":\"143695423\",\"name\":\"Thang Nguyen\"},{\"authorId\":\"66622154\",\"name\":\"Ray Ptucha\"}],\"doi\":\"10.1007/s10044-018-00770-3\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3ddfe22a67bdd1cc2b8f1a2e6663044690226933\",\"title\":\"Understanding temporal structure for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/3ddfe22a67bdd1cc2b8f1a2e6663044690226933\",\"venue\":\"Pattern Analysis and Applications\",\"year\":2019},{\"arxivId\":\"1605.02688\",\"authors\":[{\"authorId\":\"1388360943\",\"name\":\"Rami Al-Rfou\"},{\"authorId\":\"1815021\",\"name\":\"G. Alain\"},{\"authorId\":\"2634674\",\"name\":\"Amjad Almahairi\"},{\"authorId\":\"48765757\",\"name\":\"Christof Angerm\\u00fcller\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"3227028\",\"name\":\"Fr\\u00e9d\\u00e9ric Bastien\"},{\"authorId\":\"145040409\",\"name\":\"J. Bayer\"},{\"authorId\":\"144336979\",\"name\":\"A. Belikov\"},{\"authorId\":\"7330729\",\"name\":\"A. Belopolsky\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"47944877\",\"name\":\"Arnaud Bergeron\"},{\"authorId\":\"32837403\",\"name\":\"J. Bergstra\"},{\"authorId\":\"115295647\",\"name\":\"Valentin Bisson\"},{\"authorId\":\"32308836\",\"name\":\"Josh Bleecher Snyder\"},{\"authorId\":\"14362225\",\"name\":\"Nicolas Bouchard\"},{\"authorId\":\"1395619597\",\"name\":\"Nicolas Boulanger-Lewandowski\"},{\"authorId\":\"2900675\",\"name\":\"Xavier Bouthillier\"},{\"authorId\":\"2346028\",\"name\":\"A. D. Br\\u00e9bisson\"},{\"authorId\":\"1967465\",\"name\":\"Olivier Breuleux\"},{\"authorId\":\"153921980\",\"name\":\"Pierre Luc Carrier\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2292403\",\"name\":\"J. Chorowski\"},{\"authorId\":\"29848635\",\"name\":\"Paul F. Christiano\"},{\"authorId\":\"2348758\",\"name\":\"Tim Cooijmans\"},{\"authorId\":\"40638665\",\"name\":\"Marc-Alexandre C\\u00f4t\\u00e9\"},{\"authorId\":\"39977229\",\"name\":\"Myriam C\\u00f4t\\u00e9\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"2921469\",\"name\":\"Yann Dauphin\"},{\"authorId\":\"2460212\",\"name\":\"Olivier Delalleau\"},{\"authorId\":\"32604218\",\"name\":\"Julien Demouth\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"46573521\",\"name\":\"Laurent Dinh\"},{\"authorId\":\"2812151\",\"name\":\"Melanie Ducoffe\"},{\"authorId\":\"3074927\",\"name\":\"Vincent Dumoulin\"},{\"authorId\":\"3127597\",\"name\":\"S. Kahou\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"40557013\",\"name\":\"Ziye Fan\"},{\"authorId\":\"2345617\",\"name\":\"Orhan Firat\"},{\"authorId\":\"39844381\",\"name\":\"M. Germain\"},{\"authorId\":\"3119801\",\"name\":\"Xavier Glorot\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"48196087\",\"name\":\"M. Graham\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"48080691\",\"name\":\"P. Hamel\"},{\"authorId\":\"1405640115\",\"name\":\"Iban Harlouchet\"},{\"authorId\":\"114956664\",\"name\":\"J. Heng\"},{\"authorId\":\"2507883\",\"name\":\"Bal\\u00e1zs Hidasi\"},{\"authorId\":\"25056820\",\"name\":\"S. Honari\"},{\"authorId\":\"36399635\",\"name\":\"Arjun Jain\"},{\"authorId\":\"152857609\",\"name\":\"S\\u00e9bastien Jean\"},{\"authorId\":\"49104216\",\"name\":\"Kai Jia\"},{\"authorId\":\"3025583\",\"name\":\"M. Korobov\"},{\"authorId\":\"144592382\",\"name\":\"Vivek Kulkarni\"},{\"authorId\":\"49071560\",\"name\":\"Alex Lamb\"},{\"authorId\":\"3087941\",\"name\":\"Pascal Lamblin\"},{\"authorId\":\"153109766\",\"name\":\"E. Larsen\"},{\"authorId\":\"40201308\",\"name\":\"C\\u00e9sar Laurent\"},{\"authorId\":\"72490641\",\"name\":\"Sueryun Lee\"},{\"authorId\":\"47682610\",\"name\":\"S. Lefran\\u00e7ois\"},{\"authorId\":\"2387233\",\"name\":\"S. Lemieux\"},{\"authorId\":\"144828689\",\"name\":\"N. L\\u00e9onard\"},{\"authorId\":\"3146592\",\"name\":\"Zhouhan Lin\"},{\"authorId\":\"3245814\",\"name\":\"J. A. Livezey\"},{\"authorId\":\"40532172\",\"name\":\"C. Lorenz\"},{\"authorId\":\"102472217\",\"name\":\"J. Lowin\"},{\"authorId\":null,\"name\":\"Qianli Ma\"},{\"authorId\":\"1798462\",\"name\":\"Pierre-Antoine Manzagol\"},{\"authorId\":\"3422889\",\"name\":\"Olivier Mastropietro\"},{\"authorId\":\"1914552\",\"name\":\"Robert McGibbon\"},{\"authorId\":\"1710604\",\"name\":\"R. Memisevic\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1748421\",\"name\":\"Vincent Michalski\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"40479190\",\"name\":\"Alberto Orlandi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"121252563\",\"name\":\"M. Pezeshki\"},{\"authorId\":\"2402716\",\"name\":\"Colin Raffel\"},{\"authorId\":\"49577546\",\"name\":\"Daniel Renshaw\"},{\"authorId\":\"3146111\",\"name\":\"M. Rocklin\"},{\"authorId\":\"114117487\",\"name\":\"Adriana Romero\"},{\"authorId\":\"48127262\",\"name\":\"M. Roth\"},{\"authorId\":\"47696458\",\"name\":\"Peter Sadowski\"},{\"authorId\":\"3373139\",\"name\":\"J. Salvatier\"},{\"authorId\":\"47918629\",\"name\":\"F. Savard\"},{\"authorId\":\"1382154289\",\"name\":\"Jan Schl\\u00fcter\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"40116153\",\"name\":\"Gabriel Schwartz\"},{\"authorId\":\"48190457\",\"name\":\"I. Serban\"},{\"authorId\":\"1862138\",\"name\":\"Dmitriy Serdyuk\"},{\"authorId\":\"3197429\",\"name\":\"Samira Shabanian\"},{\"authorId\":\"39442397\",\"name\":\"\\u00c9tienne Simon\"},{\"authorId\":\"11115628\",\"name\":\"Sigurd Spieckermann\"},{\"authorId\":\"120638144\",\"name\":\"S. Subramanyam\"},{\"authorId\":\"3407592\",\"name\":\"Jakub Sygnowski\"},{\"authorId\":\"66454233\",\"name\":\"J\\u00e9r\\u00e9mie Tanguay\"},{\"authorId\":\"3220768\",\"name\":\"G. V. Tulder\"},{\"authorId\":\"153160559\",\"name\":\"Joseph P. Turian\"},{\"authorId\":\"19555508\",\"name\":\"S. Urban\"},{\"authorId\":\"120247189\",\"name\":\"Pascal Vincent\"},{\"authorId\":\"2077146\",\"name\":\"Francesco Visin\"},{\"authorId\":\"153559313\",\"name\":\"H. D. Vries\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"27358391\",\"name\":\"D. J. Webb\"},{\"authorId\":\"39561601\",\"name\":\"M. Willson\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"47936713\",\"name\":\"Lijun Xue\"},{\"authorId\":\"97709924\",\"name\":\"L. Yao\"},{\"authorId\":\"35097114\",\"name\":\"Saizheng Zhang\"},{\"authorId\":\"1774002\",\"name\":\"Y. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6b570069f14c7588e066f7138e1f21af59d62e61\",\"title\":\"Theano: A Python framework for fast computation of mathematical expressions\",\"url\":\"https://www.semanticscholar.org/paper/6b570069f14c7588e066f7138e1f21af59d62e61\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48637710\",\"name\":\"Yongqing Zhu\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1145/3343031.3350932\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"title\":\"Attention-based Densely Connected LSTM for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1c3eda8bd5c7b76bc61763948fa0df857052de44\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49831019\",\"name\":\"Pengcheng Wang\"},{\"authorId\":\"50341413\",\"name\":\"S. Li\"}],\"doi\":\"10.1117/12.2513868\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6f8e35dfcbd99269a38915f1b268452c9b5dab0\",\"title\":\"Structural-attentioned LSTM for action recognition based on skeleton\",\"url\":\"https://www.semanticscholar.org/paper/e6f8e35dfcbd99269a38915f1b268452c9b5dab0\",\"venue\":\"Other Conferences\",\"year\":2018},{\"arxivId\":\"2007.12402\",\"authors\":[{\"authorId\":\"1832280277\",\"name\":\"Ka Leong Cheng\"},{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"1559427865\",\"name\":\"Qifeng Chen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1007/978-3-030-58586-0_41\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"723f008092e1fb25e73bc1e4f7e1084a95727e89\",\"title\":\"Fully Convolutional Networks for Continuous Sign Language Recognition\",\"url\":\"https://www.semanticscholar.org/paper/723f008092e1fb25e73bc1e4f7e1084a95727e89\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2011.07231\",\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"91893932\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1109/cvpr42600.2020.00877\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cda672bd5487ec2c67d5c217dc84ed8fb786640\",\"title\":\"ActBERT: Learning Global-Local Video-Text Representations\",\"url\":\"https://www.semanticscholar.org/paper/8cda672bd5487ec2c67d5c217dc84ed8fb786640\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":\"2001.05614\",\"authors\":[{\"authorId\":\"49178142\",\"name\":\"H. Chen\"},{\"authorId\":\"38376468\",\"name\":\"J. Li\"},{\"authorId\":\"145460910\",\"name\":\"Xiaolin Hu\"}],\"doi\":\"10.3233/FAIA200204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f0c33980d7c011a8c657afb825220632e17b1568\",\"title\":\"Delving Deeper into the Decoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f0c33980d7c011a8c657afb825220632e17b1568\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51492139\",\"name\":\"X. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"77c5317e464468669358243b640681398809d8fd\",\"title\":\"Human Detection in a Sequence of Thermal Images using Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/77c5317e464468669358243b640681398809d8fd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"title\":\"Trainable performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1607.00036\",\"authors\":[{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"144631588\",\"name\":\"A. Chandar\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"906ac7584faf8ead6004be4cc5122320c87df59c\",\"title\":\"Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes\",\"url\":\"https://www.semanticscholar.org/paper/906ac7584faf8ead6004be4cc5122320c87df59c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1608.07068\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-46475-6_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"title\":\"Title Generation for User Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1611.09312\",\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"153925540\",\"name\":\"C. Grana\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2017.339\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"title\":\"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"2774427\",\"name\":\"Hanli Wang\"},{\"authorId\":\"49528055\",\"name\":\"Hanzhang Wang\"},{\"authorId\":\"3187665\",\"name\":\"Kaisheng Xu\"}],\"doi\":\"10.1145/3123266.3127895\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"title\":\"Richer Semantic Visual and Language Representation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/30795da8026e875faaffa3d6f2fa03c9c5d14c55\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1803.10861\",\"authors\":[{\"authorId\":\"145583891\",\"name\":\"Tuan-Hung Vu\"},{\"authorId\":\"17132791\",\"name\":\"W. Choi\"},{\"authorId\":\"1790643\",\"name\":\"S. Schulter\"},{\"authorId\":\"2099305\",\"name\":\"Manmohan Chandraker\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c4a1a5eb968ffcc3fc549753edf19ab23c8a3d2\",\"title\":\"Memory Warps for Learning Long-Term Online Video Representations\",\"url\":\"https://www.semanticscholar.org/paper/4c4a1a5eb968ffcc3fc549753edf19ab23c8a3d2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"title\":\"Empirical performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"Li Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"117aae1dc5b3aee679a690f7dab84e9a23add930\",\"title\":\"AGE AND VIDEO CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/117aae1dc5b3aee679a690f7dab84e9a23add930\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3429960\",\"name\":\"Youjiang Xu\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"2248826\",\"name\":\"R. Hong\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TIP.2018.2846664\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7fe2ab9f54242ef8609ef9bf988f008c7d42407c\",\"title\":\"Sequential Video VLAD: Training the Aggregation Locally and Temporally\",\"url\":\"https://www.semanticscholar.org/paper/7fe2ab9f54242ef8609ef9bf988f008c7d42407c\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30532805\",\"name\":\"Qingle Huang\"},{\"authorId\":\"2928799\",\"name\":\"Zicheng Liao\"}],\"doi\":\"10.5244/c.31.126\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"61cf3b276defcc82ccba3566da4a44a88740f013\",\"title\":\"A Convolutional Temporal Encoder for Video Caption Generation\",\"url\":\"https://www.semanticscholar.org/paper/61cf3b276defcc82ccba3566da4a44a88740f013\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":\"1907.03049\",\"authors\":[{\"authorId\":null,\"name\":\"Yu-Siang Wang\"},{\"authorId\":\"71309591\",\"name\":\"Hung-Ting Su\"},{\"authorId\":\"150053992\",\"name\":\"Chen-Hsi Chang\"},{\"authorId\":\"143822897\",\"name\":\"Zhe Yu Liu\"},{\"authorId\":\"31871157\",\"name\":\"Winston Hsu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"091ad302f5381bd131b41a57e16d802ff4ab9668\",\"title\":\"Video Question Generation via Cross-Modal Self-Attention Networks Learning\",\"url\":\"https://www.semanticscholar.org/paper/091ad302f5381bd131b41a57e16d802ff4ab9668\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145113946\",\"name\":\"J. Tang\"},{\"authorId\":\"144005516\",\"name\":\"Jing Wang\"},{\"authorId\":\"3233021\",\"name\":\"Zechao Li\"},{\"authorId\":\"3247966\",\"name\":\"J. Fu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3291925\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a437bb550d1df02188e4b145e01675551da36336\",\"title\":\"Show, Reward, and Tell\",\"url\":\"https://www.semanticscholar.org/paper/a437bb550d1df02188e4b145e01675551da36336\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000034\",\"name\":\"C. Huang\"},{\"authorId\":\"71422013\",\"name\":\"H. Wang\"}],\"doi\":\"10.1109/TCSVT.2019.2890899\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d756afea68f322e9cd6509d148efec6c3e266772\",\"title\":\"A Novel Key-Frames Selection Framework for Comprehensive Video Summarization\",\"url\":\"https://www.semanticscholar.org/paper/d756afea68f322e9cd6509d148efec6c3e266772\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3490384\",\"name\":\"Federico Bolelli\"},{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"51133784\",\"name\":\"F. Pollastri\"},{\"authorId\":\"1705203\",\"name\":\"C. Grana\"}],\"doi\":\"10.1109/IPAS.2018.8708893\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9184fe68648d1e7fcd1d9d153e3b888096f355b1\",\"title\":\"A Hierarchical Quasi-Recurrent approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9184fe68648d1e7fcd1d9d153e3b888096f355b1\",\"venue\":\"2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47056886\",\"name\":\"Xiangpeng Li\"},{\"authorId\":\"30076791\",\"name\":\"Zhilong Zhou\"},{\"authorId\":\"35153304\",\"name\":\"Lijiang Chen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0531-z\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"title\":\"Residual attention-based LSTM for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":\"1606.01305\",\"authors\":[{\"authorId\":\"145055042\",\"name\":\"David Krueger\"},{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"50458432\",\"name\":\"J\\u00e1nos Kram\\u00e1r\"},{\"authorId\":\"121252563\",\"name\":\"M. Pezeshki\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145604319\",\"name\":\"N. Ke\"},{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"98109738\",\"name\":\"Chris Pal\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9f0687bcd0a7d7fc91b8c5d36c003a38b8853105\",\"title\":\"Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations\",\"url\":\"https://www.semanticscholar.org/paper/9f0687bcd0a7d7fc91b8c5d36c003a38b8853105\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2301765\",\"name\":\"Tsung-Wei Ke\"},{\"authorId\":\"3172276\",\"name\":\"Che-Wei Lin\"},{\"authorId\":\"1805102\",\"name\":\"Tyng-Luh Liu\"},{\"authorId\":\"14489533\",\"name\":\"D. Geiger\"}],\"doi\":\"10.1007/978-3-319-54190-7_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1df74a5047e953766fa07dec356bba285c605a1\",\"title\":\"Variational Convolutional Networks for Human-Centric Annotations\",\"url\":\"https://www.semanticscholar.org/paper/d1df74a5047e953766fa07dec356bba285c605a1\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/2962719\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"title\":\"Semantic Feature Mining for Video Event Understanding\",\"url\":\"https://www.semanticscholar.org/paper/99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"venue\":\"TOMM\",\"year\":2016},{\"arxivId\":\"2011.11071\",\"authors\":[{\"authorId\":\"2028596941\",\"name\":\"Andreea-Maria Oncescu\"},{\"authorId\":\"143848064\",\"name\":\"Jo\\u00e3o F. Henriques\"},{\"authorId\":\"1614034792\",\"name\":\"Yang Liu\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"7641268\",\"name\":\"Samuel Albanie\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"title\":\"QuerYD: A video dataset with high-quality textual and audio narrations\",\"url\":\"https://www.semanticscholar.org/paper/6ee5d4a6ead378df9949daad5cf245c01563a3fa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1508.04843\",\"authors\":[{\"authorId\":\"2785521\",\"name\":\"Kisuk Lee\"},{\"authorId\":\"3134596\",\"name\":\"Aleksandar Zlateski\"},{\"authorId\":\"3173442\",\"name\":\"A. Vishwanathan\"},{\"authorId\":\"144924970\",\"name\":\"H. S. Seung\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d78ce0c12a55e235c423b57a5082f09b724bfb12\",\"title\":\"Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Prediction\",\"url\":\"https://www.semanticscholar.org/paper/d78ce0c12a55e235c423b57a5082f09b724bfb12\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"}],\"doi\":\"10.4233/UUID:FFF15717-71EC-402D-96E6-773884659F2C\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c90362daffb92ca848450ca33fe2c2d3f3665e2a\",\"title\":\"Models for supervised learning in sequence data\",\"url\":\"https://www.semanticscholar.org/paper/c90362daffb92ca848450ca33fe2c2d3f3665e2a\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31468750\",\"name\":\"J. Ye\"},{\"authorId\":\"145834008\",\"name\":\"Le Dong\"},{\"authorId\":\"49191636\",\"name\":\"Wenpu Dong\"},{\"authorId\":\"145901246\",\"name\":\"Ning Feng\"},{\"authorId\":\"145002061\",\"name\":\"N. Zhang\"}],\"doi\":\"10.1145/3321408.3322623\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ef5424c6cb47e17b9aeba447289af8aa1705c339\",\"title\":\"Policy multi-region integration for video description\",\"url\":\"https://www.semanticscholar.org/paper/ef5424c6cb47e17b9aeba447289af8aa1705c339\",\"venue\":\"ACM TUR-C\",\"year\":2019},{\"arxivId\":\"1911.01857\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2020.03.001\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"583222b6a573ad698207a0ebabb06685c4517558\",\"title\":\"Video Captioning with Text-based Dynamic Attention and Step-by-Step Learning\",\"url\":\"https://www.semanticscholar.org/paper/583222b6a573ad698207a0ebabb06685c4517558\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":\"1908.10136\",\"authors\":[{\"authorId\":\"2659862\",\"name\":\"J. Zhang\"},{\"authorId\":\"38083193\",\"name\":\"F. Shen\"},{\"authorId\":\"1390532590\",\"name\":\"Xing Xu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0446862cbdf61974e039a85d349d7f7864f42c1\",\"title\":\"Cooperative Cross-Stream Network for Discriminative Action Representation\",\"url\":\"https://www.semanticscholar.org/paper/f0446862cbdf61974e039a85d349d7f7864f42c1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1802.07862\",\"authors\":[{\"authorId\":\"29072828\",\"name\":\"Seungwhan Moon\"},{\"authorId\":\"152842060\",\"name\":\"Leonardo Neves\"},{\"authorId\":\"144714820\",\"name\":\"V. Carvalho\"}],\"doi\":\"10.18653/v1/N18-1078\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf6c578c16c2ad4b1f7f188ce0fe8ca52e766f8c\",\"title\":\"Multimodal Named Entity Recognition for Short Social Media Posts\",\"url\":\"https://www.semanticscholar.org/paper/cf6c578c16c2ad4b1f7f188ce0fe8ca52e766f8c\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1502892520\",\"name\":\"Xusheng Li\"},{\"authorId\":\"3295274\",\"name\":\"Zhisheng Hu\"},{\"authorId\":\"48016101\",\"name\":\"Haizhou Wang\"},{\"authorId\":\"1961262790\",\"name\":\"Yiwei Fu\"},{\"authorId\":\"113577839\",\"name\":\"Ping Chen\"},{\"authorId\":\"49716481\",\"name\":\"Minghui Zhu\"},{\"authorId\":\"1965870423\",\"name\":\"Peng Liu\"}],\"doi\":\"10.3233/JCS-191368\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fbf08834f4163a0609955185818f63564e5f84fd\",\"title\":\"DeepReturn: A deep neural network can learn how to detect previously-unseen ROP payloads without using any heuristics\",\"url\":\"https://www.semanticscholar.org/paper/fbf08834f4163a0609955185818f63564e5f84fd\",\"venue\":\"J. Comput. Secur.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2579920\",\"name\":\"Haojie Li\"},{\"authorId\":\"66939932\",\"name\":\"Sihang Wu\"},{\"authorId\":\"1860840\",\"name\":\"Shuangping Huang\"},{\"authorId\":\"144847940\",\"name\":\"K. Lam\"},{\"authorId\":\"67023635\",\"name\":\"Xiao-Fen Xing\"}],\"doi\":\"10.1109/ACCESS.2019.2958405\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"529ce7b768a1cd88d3bafe93a581ccfd0c441402\",\"title\":\"Deep Motion-Appearance Convolutions for Robust Visual Tracking\",\"url\":\"https://www.semanticscholar.org/paper/529ce7b768a1cd88d3bafe93a581ccfd0c441402\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2734498\",\"name\":\"N. Laokulrat\"},{\"authorId\":\"144801511\",\"name\":\"S. Le\"},{\"authorId\":\"8058716\",\"name\":\"Noriki Nishida\"},{\"authorId\":\"7412686\",\"name\":\"Raphael Shu\"},{\"authorId\":\"35257737\",\"name\":\"Yo Ehara\"},{\"authorId\":\"1764004\",\"name\":\"N. Okazaki\"},{\"authorId\":\"1768065\",\"name\":\"Yusuke Miyao\"},{\"authorId\":\"48731103\",\"name\":\"Hideki Nakayama\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bf05e710dae791f82cc639a09dbe5ec66fed2008\",\"title\":\"Generating Video Description using Sequence-to-sequence Model with Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/bf05e710dae791f82cc639a09dbe5ec66fed2008\",\"venue\":\"COLING\",\"year\":2016},{\"arxivId\":\"1611.05594\",\"authors\":[{\"authorId\":\"143891667\",\"name\":\"Long Chen\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"1406012647\",\"name\":\"Jun Xiao\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"104757141\",\"name\":\"Jian Shao\"},{\"authorId\":\"40366581\",\"name\":\"Wei Liu\"},{\"authorId\":\"144078686\",\"name\":\"Tat-Seng Chua\"}],\"doi\":\"10.1109/CVPR.2017.667\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"88513e738a95840de05a62f0e43d30a67b3c542e\",\"title\":\"SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning\",\"url\":\"https://www.semanticscholar.org/paper/88513e738a95840de05a62f0e43d30a67b3c542e\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"95941127\",\"name\":\"Y. Li\"},{\"authorId\":\"2443233\",\"name\":\"Xinggang Wang\"},{\"authorId\":\"1743698\",\"name\":\"Wenyu Liu\"},{\"authorId\":\"40254579\",\"name\":\"B. Feng\"}],\"doi\":\"10.1016/j.ins.2018.02.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee61698cd8a7482f66de186b6986d60b2ad876b9\",\"title\":\"Deep attention network for joint hand gesture localization and recognition using static RGB-D images\",\"url\":\"https://www.semanticscholar.org/paper/ee61698cd8a7482f66de186b6986d60b2ad876b9\",\"venue\":\"Inf. Sci.\",\"year\":2018},{\"arxivId\":\"1911.12018\",\"authors\":[{\"authorId\":\"98831710\",\"name\":\"B. Yang\"},{\"authorId\":\"1927674\",\"name\":\"Fenglin Liu\"},{\"authorId\":\"145586191\",\"name\":\"Can Zhang\"},{\"authorId\":\"35325151\",\"name\":\"Yuexian Zou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9191773630826b15a86148453365aae7703aec6b\",\"title\":\"Non-Autoregressive Coarse-to-Fine Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9191773630826b15a86148453365aae7703aec6b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26952440\",\"name\":\"Gil Keren\"},{\"authorId\":\"2169682\",\"name\":\"A. Mousa\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1776444\",\"name\":\"S. Zafeiriou\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"}],\"doi\":\"10.1145/3107990.3107996\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"736688ae77b4489efdd2d5584a50d967a7f30fdb\",\"title\":\"Deep learning for multisensorial and multimodal interaction\",\"url\":\"https://www.semanticscholar.org/paper/736688ae77b4489efdd2d5584a50d967a7f30fdb\",\"venue\":\"The Handbook of Multimodal-Multisensor Interfaces, Volume 2\",\"year\":2018},{\"arxivId\":\"1811.11524\",\"authors\":[{\"authorId\":\"46398922\",\"name\":\"Y. Liu\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"48380246\",\"name\":\"Yifeng Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1109/CVPR.2019.00372\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"990fb8c10628754e69fa8d0003d1fc0ed3e2027c\",\"title\":\"Multi-Granularity Generator for Temporal Action Proposal\",\"url\":\"https://www.semanticscholar.org/paper/990fb8c10628754e69fa8d0003d1fc0ed3e2027c\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2186316\",\"name\":\"H. Chen\"},{\"authorId\":\"3102340\",\"name\":\"Mingcong Song\"},{\"authorId\":\"9693830\",\"name\":\"J. Zhao\"},{\"authorId\":\"145279386\",\"name\":\"Yuting Dai\"},{\"authorId\":null,\"name\":\"Tao Li\"}],\"doi\":\"10.1145/3307650.3322260\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ad728e01d83ff880bd63735a0fc2156e908fd53\",\"title\":\"3D-based Video Recognition Acceleration by Leveraging Temporal Locality\",\"url\":\"https://www.semanticscholar.org/paper/0ad728e01d83ff880bd63735a0fc2156e908fd53\",\"venue\":\"2019 ACM/IEEE 46th Annual International Symposium on Computer Architecture (ISCA)\",\"year\":2019},{\"arxivId\":\"1904.12004\",\"authors\":[{\"authorId\":\"98243944\",\"name\":\"Chenglong Wang\"},{\"authorId\":\"3407947\",\"name\":\"R. Bunel\"},{\"authorId\":\"1729912\",\"name\":\"Krishnamurthy Dvijotham\"},{\"authorId\":\"2421691\",\"name\":\"Po-Sen Huang\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"}],\"doi\":\"10.1109/CVPR.2019.01254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59b439bde73d80dccf367d414e209d08d312c059\",\"title\":\"Knowing When to Stop: Evaluation and Verification of Conformity to Output-Size Specifications\",\"url\":\"https://www.semanticscholar.org/paper/59b439bde73d80dccf367d414e209d08d312c059\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1812.11004\",\"authors\":[{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1770664\",\"name\":\"X. Li\"},{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TPAMI.2019.2894139\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"title\":\"Hierarchical LSTMs with Adaptive Attention for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/c0343f9cc5f16166bda83815812c4c71ab3258e3\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144669461\",\"name\":\"Kuncheng Fang\"},{\"authorId\":\"144913277\",\"name\":\"Lian Zhou\"},{\"authorId\":\"145020731\",\"name\":\"Cheng Jin\"},{\"authorId\":\"7550713\",\"name\":\"Yuejie Zhang\"},{\"authorId\":\"35632219\",\"name\":\"Kangnian Weng\"},{\"authorId\":\"1689115\",\"name\":\"Tao Zhang\"},{\"authorId\":\"145631869\",\"name\":\"W. Fan\"}],\"doi\":\"10.1609/AAAI.V33I01.33018271\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"title\":\"Fully Convolutional Video Captioning with Coarse-to-Fine and Inherited Attention\",\"url\":\"https://www.semanticscholar.org/paper/506a3e330dbd2ecc17c6a6d4c239b1cce175b6b0\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":\"2001.04608\",\"authors\":[{\"authorId\":\"1527103472\",\"name\":\"Yixuan Li\"},{\"authorId\":\"50218816\",\"name\":\"Zixu Wang\"},{\"authorId\":\"48170350\",\"name\":\"Limin Wang\"},{\"authorId\":\"39914710\",\"name\":\"Gangshan Wu\"}],\"doi\":\"10.1007/978-3-030-58517-4_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6e3034cd8855616533d091dc1d70e969c20a42b\",\"title\":\"Actions as Moving Points\",\"url\":\"https://www.semanticscholar.org/paper/e6e3034cd8855616533d091dc1d70e969c20a42b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35900036\",\"name\":\"T. Wang\"},{\"authorId\":\"49298611\",\"name\":\"J. Li\"},{\"authorId\":\"3075750\",\"name\":\"M. Zhang\"},{\"authorId\":\"2643261\",\"name\":\"A. Zhu\"},{\"authorId\":\"2103629\",\"name\":\"H. Snoussi\"},{\"authorId\":\"145685545\",\"name\":\"Chang Choi\"}],\"doi\":\"10.1002/cpe.5302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d8bfa4233fc87c7f0b3f1aa0c4960ad13475a04e\",\"title\":\"An enhanced 3DCNN\\u2010ConvLSTM for spatiotemporal multimedia data analysis\",\"url\":\"https://www.semanticscholar.org/paper/d8bfa4233fc87c7f0b3f1aa0c4960ad13475a04e\",\"venue\":\"\",\"year\":2019}],\"corpusId\":623318,\"doi\":\"10.1109/ICCV.2015.512\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":135,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"references\":[{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2110842\",\"name\":\"Nicholas Morsillo\"},{\"authorId\":\"2482936\",\"name\":\"Gideon S. Mann\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":\"10.1007/978-3-642-12900-1_14\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a49acd70550c209965a6d39d7ff92d11f0a5b1b6\",\"title\":\"YouTube Scale, Large Vocabulary Video Annotation\",\"url\":\"https://www.semanticscholar.org/paper/a49acd70550c209965a6d39d7ff92d11f0a5b1b6\",\"venue\":\"Video Search and Mining\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"2428034\",\"name\":\"C. Bregler\"}],\"doi\":\"10.1007/978-3-642-15567-3_11\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d476b96be73fccc61f2076befbf5a468caa4180\",\"title\":\"Convolutional Learning of Spatio-temporal Features\",\"url\":\"https://www.semanticscholar.org/paper/4d476b96be73fccc61f2076befbf5a468caa4180\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":\"1212.0402\",\"authors\":[{\"authorId\":\"1799979\",\"name\":\"K. Soomro\"},{\"authorId\":\"40029556\",\"name\":\"A. Zamir\"},{\"authorId\":\"145103012\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"title\":\"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\"url\":\"https://www.semanticscholar.org/paper/da9e411fcf740569b6b356f330a1d0fc077c8d7c\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1162/neco.1997.9.8.1735\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"title\":\"Long Short-Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/44d2abe2175df8153f465f6c39b68b76a0d40ab9\",\"venue\":\"Neural Computation\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"2677488\",\"name\":\"\\u00c0gata Lapedriza\"},{\"authorId\":\"40599257\",\"name\":\"J. Xiao\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"143868587\",\"name\":\"A. Oliva\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9667f8264745b626c6173b1310e2ff0298b09cfc\",\"title\":\"Learning Deep Features for Scene Recognition using Places Database\",\"url\":\"https://www.semanticscholar.org/paper/9667f8264745b626c6173b1310e2ff0298b09cfc\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":\"10.1109/ICCV.2015.510\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"title\":\"Learning Spatiotemporal Features with 3D Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/d25c65d261ea0e6a458be4c50c40ffe5bc508f77\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A Barbu\"},{\"authorId\":null,\"name\":\"A Bridge\"},{\"authorId\":null,\"name\":\"Z Burchill\"},{\"authorId\":null,\"name\":\"D Coroian\"},{\"authorId\":null,\"name\":\"S Dickinson\"},{\"authorId\":null,\"name\":\"S Fidler\"},{\"authorId\":null,\"name\":\"A Michaux\"},{\"authorId\":null,\"name\":\"S Mussman\"},{\"authorId\":null,\"name\":\"S Narayanaswamy\"},{\"authorId\":null,\"name\":\"D Salvi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Video in sentences out. UAI\",\"url\":\"\",\"venue\":\"Video in sentences out. UAI\",\"year\":2012},{\"arxivId\":\"1502.03044\",\"authors\":[{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"title\":\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\",\"url\":\"https://www.semanticscholar.org/paper/4d8f2d14af5991d4f0d050d22216825cac3157bd\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1504.00325\",\"authors\":[{\"authorId\":\"39717886\",\"name\":\"Xinlei Chen\"},{\"authorId\":\"47395669\",\"name\":\"H. Fang\"},{\"authorId\":\"33493200\",\"name\":\"Tsung-Yi Lin\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144157872\",\"name\":\"Saurabh Gupta\"},{\"authorId\":\"3127283\",\"name\":\"Piotr Doll\\u00e1r\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"title\":\"Microsoft COCO Captions: Data Collection and Evaluation Server\",\"url\":\"https://www.semanticscholar.org/paper/696ca58d93f6404fea0fc75c62d1d7b378f47628\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1502.04681\",\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"title\":\"Unsupervised Learning of Video Representations using LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3355264\",\"name\":\"Kevin D. Tang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"50678963\",\"name\":\"D. Koller\"}],\"doi\":\"10.1109/CVPR.2012.6247808\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6acdddc36ea57ec84581e9e196665f246e8157ab\",\"title\":\"Learning latent temporal structure for complex event detection\",\"url\":\"https://www.semanticscholar.org/paper/6acdddc36ea57ec84581e9e196665f246e8157ab\",\"venue\":\"2012 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2012},{\"arxivId\":\"1312.6229\",\"authors\":[{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"2060028\",\"name\":\"D. Eigen\"},{\"authorId\":\"46447747\",\"name\":\"X. Zhang\"},{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1109b663453e78a59e4f66446d71720ac58cec25\",\"title\":\"OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/1109b663453e78a59e4f66446d71720ac58cec25\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C Szegedy\"},{\"authorId\":null,\"name\":\"W Liu\"},{\"authorId\":null,\"name\":\"Y Jia\"},{\"authorId\":null,\"name\":\"P Sermanet\"},{\"authorId\":null,\"name\":\"S Reed\"},{\"authorId\":null,\"name\":\"D Anguelov\"},{\"authorId\":null,\"name\":\"D Erhan\"},{\"authorId\":null,\"name\":\"V Vanhoucke\"},{\"authorId\":null,\"name\":\"A Rabinovich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Going deeper with convolutions. CVPR\",\"url\":\"\",\"venue\":\"Going deeper with convolutions. CVPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"143836295\",\"name\":\"W. Xu\"},{\"authorId\":\"41216159\",\"name\":\"Ming Yang\"},{\"authorId\":\"144782042\",\"name\":\"Kai Yu\"}],\"doi\":\"10.1109/TPAMI.2012.59\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"title\":\"3D Convolutional Neural Networks for Human Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/80bfcf1be2bf1b95cc6f36d229665cdf22d76190\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"35033378\",\"name\":\"M. M. Ullah\"},{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.5244/C.23.124\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a39e6968580762ac5ae3cd064e86e1849f3efb7f\",\"title\":\"Evaluation of Local Spatio-temporal Features for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/a39e6968580762ac5ae3cd064e86e1849f3efb7f\",\"venue\":\"BMVC\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153642390\",\"name\":\"David L. Chen\"},{\"authorId\":\"83415753\",\"name\":\"W. Dolan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"title\":\"Collecting Highly Parallel Data for Paraphrase Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/554a31ce91189cf6022ac677413ef2f8b9b40ca7\",\"venue\":\"ACL 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2013.61\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"title\":\"Translating Video Content to Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":\"1409.2329\",\"authors\":[{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97\",\"title\":\"Recurrent Neural Network Regularization\",\"url\":\"https://www.semanticscholar.org/paper/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1805076\",\"name\":\"G. Toderici\"},{\"authorId\":\"152821938\",\"name\":\"Sanketh Shetty\"},{\"authorId\":\"120906511\",\"name\":\"T. Leung\"},{\"authorId\":\"1694199\",\"name\":\"R. Sukthankar\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2014.223\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"title\":\"Large-Scale Video Classification with Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6d4c9c923e9f145d1c01a2de2afc38ec23c44253\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"1753355\",\"name\":\"Z. Harchaoui\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/TPAMI.2013.65\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"404352f5c18d4aca97f0cb660a31bf5d0df3fe0c\",\"title\":\"Temporal Localization of Actions with Actoms\",\"url\":\"https://www.semanticscholar.org/paper/404352f5c18d4aca97f0cb660a31bf5d0df3fe0c\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2170746\",\"name\":\"M. Hodosh\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"}],\"doi\":\"10.1613/jair.3994\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9814df8bd00ba999c4d1e305a7e9bca579dc7c75\",\"title\":\"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/9814df8bd00ba999c4d1e305a7e9bca579dc7c75\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":\"1409.4842\",\"authors\":[{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"46641766\",\"name\":\"W. Liu\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"48840704\",\"name\":\"Scott Reed\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"39863668\",\"name\":\"Andrew Rabinovich\"}],\"doi\":\"10.1109/CVPR.2015.7298594\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"title\":\"Going deeper with convolutions\",\"url\":\"https://www.semanticscholar.org/paper/e15cf50aa89fee8535703b9f9512fca5bfc43327\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1412.2306\",\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/TPAMI.2016.2598339\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"title\":\"Deep Visual-Semantic Alignments for Generating Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2017},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"title\":\"Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1412.6604\",\"authors\":[{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"143627859\",\"name\":\"Joan Bruna\"},{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2939803\",\"name\":\"Ronan Collobert\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"355f98e4827a1b6ad3f29d07ea2bcf9ad078295c\",\"title\":\"Video (language) modeling: a baseline for generative models of natural videos\",\"url\":\"https://www.semanticscholar.org/paper/355f98e4827a1b6ad3f29d07ea2bcf9ad078295c\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. Xu S. Ji\"},{\"authorId\":null,\"name\":\"M. Yang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"3 d convolutional neural networks for human action recognition\",\"url\":\"\",\"venue\":\"PAMI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123446103\",\"name\":\"Hilde Kuehne\"},{\"authorId\":\"119268487\",\"name\":\"Hueihan Jhuang\"},{\"authorId\":\"1930964\",\"name\":\"E. Garrote\"},{\"authorId\":\"145031878\",\"name\":\"T. Poggio\"},{\"authorId\":\"1981539\",\"name\":\"Thomas Serre\"}],\"doi\":\"10.1109/ICCV.2011.6126543\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b3b8848a311c501e704c45c6d50430ab7068956\",\"title\":\"HMDB: A large video database for human motion recognition\",\"url\":\"https://www.semanticscholar.org/paper/8b3b8848a311c501e704c45c6d50430ab7068956\",\"venue\":\"2011 International Conference on Computer Vision\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":\"1211.5590\",\"authors\":[{\"authorId\":\"3227028\",\"name\":\"Fr\\u00e9d\\u00e9ric Bastien\"},{\"authorId\":\"3087941\",\"name\":\"Pascal Lamblin\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"32837403\",\"name\":\"J. Bergstra\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"47944877\",\"name\":\"Arnaud Bergeron\"},{\"authorId\":\"14362225\",\"name\":\"Nicolas Bouchard\"},{\"authorId\":\"1923596\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"855d0f722d75cc56a66a00ede18ace96bafee6bd\",\"title\":\"Theano: new features and speed improvements\",\"url\":\"https://www.semanticscholar.org/paper/855d0f722d75cc56a66a00ede18ace96bafee6bd\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":\"1412.0767\",\"authors\":[{\"authorId\":\"1687325\",\"name\":\"Du Tran\"},{\"authorId\":\"1769383\",\"name\":\"Lubomir D. Bourdev\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"},{\"authorId\":\"1732879\",\"name\":\"L. Torresani\"},{\"authorId\":\"2210374\",\"name\":\"Manohar Paluri\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62\",\"title\":\"C3D: Generic Features for Video Analysis\",\"url\":\"https://www.semanticscholar.org/paper/bd243d77076b3b8fe046bd3dc6e8a02aa9b38d62\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1404.7828\",\"authors\":[{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1016/j.neunet.2014.09.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"193edd20cae92c6759c18ce93eeea96afd9528eb\",\"title\":\"Deep learning in neural networks: An overview\",\"url\":\"https://www.semanticscholar.org/paper/193edd20cae92c6759c18ce93eeea96afd9528eb\",\"venue\":\"Neural Networks\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/TPAMI.2012.162\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5cb6700d94c6118ee13f4f4fecac99f111189812\",\"title\":\"BabyTalk: Understanding and Generating Simple Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/5cb6700d94c6118ee13f4f4fecac99f111189812\",\"venue\":\"IEEE Trans. Pattern Anal. Mach. Intell.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2013.441\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d721f4d64b8e722222c876f0a0f226ed49476347\",\"title\":\"Action Recognition with Improved Trajectories\",\"url\":\"https://www.semanticscholar.org/paper/d721f4d64b8e722222c876f0a0f226ed49476347\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"title\":\"Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"venue\":\"COLING\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"36037226\",\"name\":\"R. Ducharme\"},{\"authorId\":\"120247189\",\"name\":\"Pascal Vincent\"},{\"authorId\":\"1909943744\",\"name\":\"Christian Janvin\"}],\"doi\":\"10.1162/153244303322533223\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c2b28f9354f667cd5bd07afc0471d8334430da7\",\"title\":\"A Neural Probabilistic Language Model\",\"url\":\"https://www.semanticscholar.org/paper/6c2b28f9354f667cd5bd07afc0471d8334430da7\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Bergstra\"},{\"authorId\":null,\"name\":\"O. Breuleux\"},{\"authorId\":null,\"name\":\"F. Bastien\"},{\"authorId\":null,\"name\":\"P. Lamblin\"},{\"authorId\":null,\"name\":\"R. Pascanu\"},{\"authorId\":null,\"name\":\"G. Desjardins\"},{\"authorId\":null,\"name\":\"J. Turian\"},{\"authorId\":null,\"name\":\"D. Warde-Farley\"},{\"authorId\":null,\"name\":\"Y. Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Theano: a CPU and GPU math expression compiler\",\"url\":\"\",\"venue\":\"Proceedings of the Python for Scientific Computing Conference (SciPy),\",\"year\":2010},{\"arxivId\":\"1411.4555\",\"authors\":[{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1726415\",\"name\":\"A. Toshev\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"}],\"doi\":\"10.1109/CVPR.2015.7298935\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"title\":\"Show and tell: A neural image caption generator\",\"url\":\"https://www.semanticscholar.org/paper/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1212.5701\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8729441d734782c3ed532a7d2d9611b438c0a09a\",\"title\":\"ADADELTA: An Adaptive Learning Rate Method\",\"url\":\"https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Bergstra\"},{\"authorId\":null,\"name\":\"Y. Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Random search for hyperparameter optimization\",\"url\":\"\",\"venue\":\"JMLR,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"O. Breuleux J. Bergstra\"},{\"authorId\":null,\"name\":\"F. Bastien\"},{\"authorId\":null,\"name\":\"P. Lamblin\"},{\"authorId\":null,\"name\":\"R. Pascanu\"},{\"authorId\":null,\"name\":\"G. Desjardins\"},{\"authorId\":null,\"name\":\"J. Turian\"},{\"authorId\":null,\"name\":\"D. Warde-Farley\"},{\"authorId\":null,\"name\":\"Y.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bengio . Theano : a CPU and GPU math expression compiler\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1408.5093\",\"authors\":[{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1782282\",\"name\":\"Evan Shelhamer\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"3049736\",\"name\":\"S. Karayev\"},{\"authorId\":\"144361581\",\"name\":\"J. Long\"},{\"authorId\":\"2983898\",\"name\":\"Ross B. Girshick\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1145/2647868.2654889\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6bdb186ec4726e00a8051119636d4df3b94043b5\",\"title\":\"Caffe: Convolutional Architecture for Fast Feature Embedding\",\"url\":\"https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5\",\"venue\":\"ACM Multimedia\",\"year\":2014},{\"arxivId\":\"1406.2199\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"title\":\"Two-Stream Convolutional Networks for Action Recognition in Videos\",\"url\":\"https://www.semanticscholar.org/paper/67dccc9a856b60bdc4d058d83657a089b8ad4486\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49693392\",\"name\":\"A. Kojima\"},{\"authorId\":\"46526487\",\"name\":\"Takeshi Tamura\"},{\"authorId\":\"145950023\",\"name\":\"K. Fukunaga\"}],\"doi\":\"10.1023/A:1020346032608\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d53a97a3dd7760b193c0d9a5293b60feff239059\",\"title\":\"Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions\",\"url\":\"https://www.semanticscholar.org/paper/d53a97a3dd7760b193c0d9a5293b60feff239059\",\"venue\":\"International Journal of Computer Vision\",\"year\":2004},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1407.1208\",\"authors\":[{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"3215950\",\"name\":\"R\\u00e9mi Lajugie\"},{\"authorId\":\"144570279\",\"name\":\"Francis R. Bach\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"144189388\",\"name\":\"J. Ponce\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"}],\"doi\":\"10.1007/978-3-319-10602-1_41\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16ae6b2ee079d290c6bf8f1ceac0fbc3477292fc\",\"title\":\"Weakly Supervised Action Labeling in Videos under Ordering Constraints\",\"url\":\"https://www.semanticscholar.org/paper/16ae6b2ee079d290c6bf8f1ceac0fbc3477292fc\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48950628\",\"name\":\"N. Dalal\"},{\"authorId\":\"1756114\",\"name\":\"B. Triggs\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1007/11744047_33\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"44f3ac3277c2eb6e5599739eb875888c46e21d4c\",\"title\":\"Human Detection Using Oriented Histograms of Flow and Appearance\",\"url\":\"https://www.semanticscholar.org/paper/44f3ac3277c2eb6e5599739eb875888c46e21d4c\",\"venue\":\"ECCV\",\"year\":2006},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":\"1204.2742\",\"authors\":[{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"},{\"authorId\":\"48540451\",\"name\":\"Alexander Bridge\"},{\"authorId\":\"3190146\",\"name\":\"Zachary Burchill\"},{\"authorId\":\"49081881\",\"name\":\"D. Coroian\"},{\"authorId\":\"1779136\",\"name\":\"S. Dickinson\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"38414598\",\"name\":\"A. Michaux\"},{\"authorId\":\"2587937\",\"name\":\"Sam Mussman\"},{\"authorId\":\"38052303\",\"name\":\"S. Narayanaswamy\"},{\"authorId\":\"2968009\",\"name\":\"D. Salvi\"},{\"authorId\":\"50269497\",\"name\":\"Lara Schmidt\"},{\"authorId\":\"2060623\",\"name\":\"Jiangnan Shangguan\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"},{\"authorId\":\"32655613\",\"name\":\"J. Waggoner\"},{\"authorId\":\"30102584\",\"name\":\"S. Wang\"},{\"authorId\":\"2223764\",\"name\":\"Jinlian Wei\"},{\"authorId\":\"1813304\",\"name\":\"Yifan Yin\"},{\"authorId\":\"48806246\",\"name\":\"Zhiqi Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"793c1c908672ea71aef9e1b41a46272aa27598f7\",\"title\":\"Video In Sentences Out\",\"url\":\"https://www.semanticscholar.org/paper/793c1c908672ea71aef9e1b41a46272aa27598f7\",\"venue\":\"UAI\",\"year\":2012}],\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"topics\":[{\"topic\":\"Convolutional neural network\",\"topicId\":\"29860\",\"url\":\"https://www.semanticscholar.org/topic/29860\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Recurrent neural network\",\"topicId\":\"16115\",\"url\":\"https://www.semanticscholar.org/topic/16115\"},{\"topic\":\"Flickr\",\"topicId\":\"67227\",\"url\":\"https://www.semanticscholar.org/topic/67227\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Dynamic voltage scaling\",\"topicId\":\"55538\",\"url\":\"https://www.semanticscholar.org/topic/55538\"},{\"topic\":\"METEOR\",\"topicId\":\"609251\",\"url\":\"https://www.semanticscholar.org/topic/609251\"},{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"BLEU\",\"topicId\":\"250421\",\"url\":\"https://www.semanticscholar.org/topic/250421\"},{\"topic\":\"Kinesiology\",\"topicId\":\"113188\",\"url\":\"https://www.semanticscholar.org/topic/113188\"},{\"topic\":\"Encoder\",\"topicId\":\"16744\",\"url\":\"https://www.semanticscholar.org/topic/16744\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Random neural network\",\"topicId\":\"136146\",\"url\":\"https://www.semanticscholar.org/topic/136146\"}],\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015}\n"