"{\"abstract\":\"Humans use rich natural language to describe and communicate visual perceptions. In order to provide natural language descriptions for visual content, this paper combines two important ingredients. First, we generate a rich semantic representation of the visual content including e.g. object and activity labels. To predict the semantic representation we learn a CRF to model the relationships between different components of the visual input. And second, we propose to formulate the generation of natural language as a machine translation problem using the semantic representation as source language and the generated sentences as target language. For this we exploit the power of a parallel corpus of videos and textual descriptions and adapt statistical machine translation to translate between our two languages. We evaluate our video descriptions on the TACoS dataset, which contains video snippets aligned with sentence descriptions. Using automatic evaluation and human judgments we show significant improvements over several baseline approaches, motivated by prior work. Our translation approach also shows improvements over related work on an image description task.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\",\"url\":\"https://www.semanticscholar.org/author/34849128\"},{\"authorId\":\"144369161\",\"name\":\"Wei Qiu\",\"url\":\"https://www.semanticscholar.org/author/144369161\"},{\"authorId\":\"144889265\",\"name\":\"Ivan Titov\",\"url\":\"https://www.semanticscholar.org/author/144889265\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\",\"url\":\"https://www.semanticscholar.org/author/1727272\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\",\"url\":\"https://www.semanticscholar.org/author/1717560\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\",\"url\":\"https://www.semanticscholar.org/author/48920094\"}],\"citationVelocity\":40,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145308148\",\"name\":\"L. Bai\"},{\"authorId\":\"144957598\",\"name\":\"K. Li\"},{\"authorId\":\"2030689\",\"name\":\"JianMeng Pei\"},{\"authorId\":\"49047853\",\"name\":\"S. Jiang\"}],\"doi\":\"10.1007/s00521-015-1846-7\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2af06b88444cfabf92f54bcb3a31e9afd149ae69\",\"title\":\"Main objects interaction activity recognition in real images\",\"url\":\"https://www.semanticscholar.org/paper/2af06b88444cfabf92f54bcb3a31e9afd149ae69\",\"venue\":\"Neural Computing and Applications\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2151048\",\"name\":\"Taiki Miyanishi\"},{\"authorId\":\"8425523\",\"name\":\"J. Hirayama\"},{\"authorId\":\"34772057\",\"name\":\"Takuya Maekawa\"},{\"authorId\":\"1716788\",\"name\":\"M. Kawanabe\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"686b14ce7f02516730de03f459cadb223a03765f\",\"title\":\"Generating an Event Timeline About Daily Activities From a Semantic Concept Stream\",\"url\":\"https://www.semanticscholar.org/paper/686b14ce7f02516730de03f459cadb223a03765f\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"61430daf2740a2081b89fe4cee5c8f7c7c7516ab\",\"title\":\"Contributions to large-scale learning for image classification. (Contributions \\u00e0 l'apprentissage grande \\u00e9chelle pour la classification d'images)\",\"url\":\"https://www.semanticscholar.org/paper/61430daf2740a2081b89fe4cee5c8f7c7c7516ab\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2782248\",\"name\":\"M. Y. K. Tani\"},{\"authorId\":\"2512422\",\"name\":\"Abdelghani Ghomari\"},{\"authorId\":\"1975525\",\"name\":\"Adel Lablack\"},{\"authorId\":\"3036685\",\"name\":\"Ioan Marius Bilasco\"}],\"doi\":\"10.1007/s13735-017-0133-z\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa6536b25f6131ec0aba6595f0106aaafee721fe\",\"title\":\"OVIS: ontology video surveillance indexing and retrieval system\",\"url\":\"https://www.semanticscholar.org/paper/aa6536b25f6131ec0aba6595f0106aaafee721fe\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3239379\",\"name\":\"J. Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1007/978-3-030-05710-7_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"title\":\"Hierarchical Vision-Language Alignment for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/5c5f321353dafe2a43ef25cb0d6e9714f833a5bb\",\"venue\":\"MMM\",\"year\":2019},{\"arxivId\":\"1512.01715\",\"authors\":[{\"authorId\":\"47935745\",\"name\":\"Hang Qi\"},{\"authorId\":\"47353858\",\"name\":\"Tianfu Wu\"},{\"authorId\":\"2649483\",\"name\":\"M. Lee\"},{\"authorId\":\"145380991\",\"name\":\"S. Zhu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d2b2cb1d5cc1aa30cf5be7bcb0494198934caabb\",\"title\":\"A Restricted Visual Turing Test for Deep Scene and Event Understanding\",\"url\":\"https://www.semanticscholar.org/paper/d2b2cb1d5cc1aa30cf5be7bcb0494198934caabb\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1608.07068\",\"authors\":[{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-46475-6_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"title\":\"Title Generation for User Generated Videos\",\"url\":\"https://www.semanticscholar.org/paper/65ba5f3927633293112cf1bbdf6641d4d15638cc\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1804.08274\",\"authors\":[{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"47636228\",\"name\":\"H. Chao\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2018.00782\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"title\":\"Jointly Localizing and Describing Events for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/19d7f83c3d7147f0eed1e1471438066eb4fe51fb\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1965909970\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"1755773\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930562\",\"name\":\"G. Chen\"},{\"authorId\":\"153016830\",\"name\":\"J. Guo\"}],\"doi\":\"10.1109/ACCESS.2020.3021857\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"title\":\"Understanding Objects in Video: Object-Oriented Video Captioning via Structured Trajectory and Adversarial Learning\",\"url\":\"https://www.semanticscholar.org/paper/e3a02eab3df6ec8bfaf0711cd1d87ab837fe437b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.1007/s00138-017-0825-7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"348ae58a90ee834517898e139c22e7c40d85506f\",\"title\":\"Generating natural language tags for video information management\",\"url\":\"https://www.semanticscholar.org/paper/348ae58a90ee834517898e139c22e7c40d85506f\",\"venue\":\"Machine Vision and Applications\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"1790251284\",\"name\":\"Lin Li\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"9594118\",\"name\":\"C. Gu\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/s11063-020-10352-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"title\":\"Adaptively Converting Auxiliary Attributes and Textual Embedding for Video Captioning Based on BiLSTM\",\"url\":\"https://www.semanticscholar.org/paper/3f1619990d5b61b84bfe268d2e1e7e60de43788e\",\"venue\":\"Neural Process. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b5c35e70954a05ec4b836f166882982f459eefa\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/4b5c35e70954a05ec4b836f166882982f459eefa\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51173816\",\"name\":\"Ren\\u00e9 Grzeszick\"},{\"authorId\":\"1749475\",\"name\":\"G. Fink\"}],\"doi\":\"10.5220/0006240901200129\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"113e2b0ccf307c590d64a2810558a5db98e37ab4\",\"title\":\"Zero-shot Object Prediction using Semantic Scene Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/113e2b0ccf307c590d64a2810558a5db98e37ab4\",\"venue\":\"VISIGRAPP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145810388\",\"name\":\"Zheng Zhou\"},{\"authorId\":\"144957598\",\"name\":\"K. Li\"},{\"authorId\":\"145308148\",\"name\":\"L. Bai\"}],\"doi\":\"10.1007/s00521-015-2171-x\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"081307db6d8d709af26f49d24041086bb09abfd1\",\"title\":\"A general description generator for human activity images based on deep understanding framework\",\"url\":\"https://www.semanticscholar.org/paper/081307db6d8d709af26f49d24041086bb09abfd1\",\"venue\":\"Neural Computing and Applications\",\"year\":2015},{\"arxivId\":\"1505.05254\",\"authors\":[{\"authorId\":\"2776254\",\"name\":\"Yedid Hoshen\"},{\"authorId\":\"144406261\",\"name\":\"Shmuel Peleg\"}],\"doi\":\"10.1109/ICIP.2015.7350790\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c393199f1540786b2aade63de51db3e43f933887\",\"title\":\"Live video synopsis for multiple cameras\",\"url\":\"https://www.semanticscholar.org/paper/c393199f1540786b2aade63de51db3e43f933887\",\"venue\":\"2015 IEEE International Conference on Image Processing (ICIP)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"37498905\",\"name\":\"L. Li\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1109/BigMM.2018.8499257\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7ae5f10acd306a7842a16542b6b236e0a964de10\",\"title\":\"Saliency-Based Spatiotemporal Attention for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7ae5f10acd306a7842a16542b6b236e0a964de10\",\"venue\":\"2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":null,\"name\":\"Ning Xu\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"title\":\"Tianjin University and National University of Singapore at TRECVID 2017: Video to Text Description\",\"url\":\"https://www.semanticscholar.org/paper/c1050e434df208b9745c24b367bfd7f7aabef1e7\",\"venue\":\"TRECVID\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47286885\",\"name\":\"Jingyi Hou\"},{\"authorId\":\"47149737\",\"name\":\"X. Wu\"},{\"authorId\":\"29367810\",\"name\":\"Wentian Zhao\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"7415267\",\"name\":\"Y. Jia\"}],\"doi\":\"10.1109/ICCV.2019.00901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"title\":\"Joint Syntax Representation Learning and Visual Cue Translation for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ce40cd5214d556e9b8ca8ca401597321cb29b8d6\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1502.08029\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1109/ICCV.2015.512\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f425b7abf2ed3172ed060df85bb1885860a297e\",\"title\":\"Describing Videos by Exploiting Temporal Structure\",\"url\":\"https://www.semanticscholar.org/paper/5f425b7abf2ed3172ed060df85bb1885860a297e\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144791489\",\"name\":\"M. B\\u00e1ez\"},{\"authorId\":\"144896044\",\"name\":\"F. Daniel\"},{\"authorId\":\"145866446\",\"name\":\"F. Casati\"}],\"doi\":\"10.1007/978-3-030-39540-7_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8b3bb72fe6f981dadfd52ca11c6e6860ea79b18\",\"title\":\"Conversational Web Interaction: Proposal of a Dialog-Based Natural Language Interaction Paradigm for the Web\",\"url\":\"https://www.semanticscholar.org/paper/b8b3bb72fe6f981dadfd52ca11c6e6860ea79b18\",\"venue\":\"CONVERSATIONS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2477939\",\"name\":\"Ashutosh Modi\"}],\"doi\":\"10.22028/D291-26779\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a64de8825079192dec4394e35eba1f749bd821d\",\"title\":\"Modeling common sense knowledge via scripts\",\"url\":\"https://www.semanticscholar.org/paper/3a64de8825079192dec4394e35eba1f749bd821d\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1510.01431\",\"authors\":[{\"authorId\":\"3175685\",\"name\":\"A. Mathews\"},{\"authorId\":\"33650938\",\"name\":\"Lexing Xie\"},{\"authorId\":\"33913193\",\"name\":\"Xuming He\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7da9c26ea68a31d119e8222d1a5c33ef136ebed8\",\"title\":\"SentiCap: Generating Image Descriptions with Sentiments\",\"url\":\"https://www.semanticscholar.org/paper/7da9c26ea68a31d119e8222d1a5c33ef136ebed8\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"2011.09530\",\"authors\":[{\"authorId\":\"153769937\",\"name\":\"H. Akbari\"},{\"authorId\":\"2542427\",\"name\":\"H. Palangi\"},{\"authorId\":\"120157163\",\"name\":\"Jianwei Yang\"},{\"authorId\":\"1845230025\",\"name\":\"Sudha Rao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"37409035\",\"name\":\"R. Fernandez\"},{\"authorId\":\"1748557\",\"name\":\"P. Smolensky\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"72197815\",\"name\":\"Shih-Fu Chang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"title\":\"Neuro-Symbolic Representations for Video Captioning: A Case for Leveraging Inductive Biases for Vision and Language\",\"url\":\"https://www.semanticscholar.org/paper/c2b4d96db34bd472e84c9234838cc4e808eb1ba9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.cviu.2019.102840\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b4c920c72a32196c1dd4aa18984f7b3c0b77861\",\"title\":\"Video captioning using boosted and parallel Long Short-Term Memory networks\",\"url\":\"https://www.semanticscholar.org/paper/1b4c920c72a32196c1dd4aa18984f7b3c0b77861\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":\"145200778\",\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":\"1693997\",\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":\"10.1145/3240508.3240538\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72f9116a04e584081635500e9f0789fa26e4d15f\",\"title\":\"Hierarchical Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/72f9116a04e584081635500e9f0789fa26e4d15f\",\"venue\":\"ACM Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"Li Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"117aae1dc5b3aee679a690f7dab84e9a23add930\",\"title\":\"AGE AND VIDEO CAPTIONING\",\"url\":\"https://www.semanticscholar.org/paper/117aae1dc5b3aee679a690f7dab84e9a23add930\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1109/ACCESS.2019.2942000\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"801827592d18c4e6170d88f8345465de4a8db7ca\",\"title\":\"Video Captioning With Adaptive Attention and Mixed Loss Optimization\",\"url\":\"https://www.semanticscholar.org/paper/801827592d18c4e6170d88f8345465de4a8db7ca\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":\"1604.07952\",\"authors\":[{\"authorId\":\"51173816\",\"name\":\"Ren\\u00e9 Grzeszick\"},{\"authorId\":\"1749475\",\"name\":\"G. Fink\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5ce3c9eb539d2f4a85880db65ba3890a0fd8c6c\",\"title\":\"Zero-shot object prediction and context modeling using semantic scene knowledge\",\"url\":\"https://www.semanticscholar.org/paper/f5ce3c9eb539d2f4a85880db65ba3890a0fd8c6c\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1604.03249\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.1007/978-3-319-50077-5_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"title\":\"Attributes as Semantic Units between Natural Language and Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ffd73d1956163a4160ec2c96b3ab256f79fc92e8\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"82564091\",\"name\":\"Ey\\u00fcp \\u00d6zer\"},{\"authorId\":\"1658966195\",\"name\":\"\\u0130lteber Nur Karap\\u0131nar\"},{\"authorId\":\"1658997114\",\"name\":\"Sena Ba\\u015fbu\\u011f\"},{\"authorId\":\"1657299979\",\"name\":\"S\\u00fcmeyye Turan\"},{\"authorId\":\"52219742\",\"name\":\"An\\u0131l Utku\"},{\"authorId\":\"153780569\",\"name\":\"M. Akcayol\"}],\"doi\":\"10.14569/ijacsa.2020.0110365\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f682a618e680e6c488f620a65cf5cf657fc6986b\",\"title\":\"Deep Learning based, a New Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/f682a618e680e6c488f620a65cf5cf657fc6986b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1704.03114\",\"authors\":[{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"46867115\",\"name\":\"Yuqi Zhang\"},{\"authorId\":\"1807606\",\"name\":\"D. Lin\"}],\"doi\":\"10.1109/CVPR.2017.352\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5fcd93997b7dde90594dc1caa27ba9d560bbe63d\",\"title\":\"Detecting Visual Relationships with Deep Relational Networks\",\"url\":\"https://www.semanticscholar.org/paper/5fcd93997b7dde90594dc1caa27ba9d560bbe63d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1804.00100\",\"authors\":[{\"authorId\":null,\"name\":\"Jingwen Wang\"},{\"authorId\":\"2093119\",\"name\":\"W. Jiang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"144391096\",\"name\":\"Yong Xu\"}],\"doi\":\"10.1109/CVPR.2018.00751\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"title\":\"Bidirectional Attentive Fusion with Context Gating for Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bb4e2d6a6e3e1067f21a4cad087fc91c671e495d\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1511.07067\",\"authors\":[{\"authorId\":\"2150275\",\"name\":\"S. Kottur\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"144915495\",\"name\":\"Jos\\u00e9 M. F. Moura\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2016.539\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"06599d41a3256245aa0cb2e9e56b29459c2e2c69\",\"title\":\"VisualWord2Vec (Vis-W2V): Learning Visually Grounded Word Embeddings Using Abstract Scenes\",\"url\":\"https://www.semanticscholar.org/paper/06599d41a3256245aa0cb2e9e56b29459c2e2c69\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e03b932fa6c87b8a698c86b4b4963bf6e4b45933\",\"title\":\"Matrix ? Why does Cypher betray Morpheus ? How does the movie end ?\",\"url\":\"https://www.semanticscholar.org/paper/e03b932fa6c87b8a698c86b4b4963bf6e4b45933\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39541577\",\"name\":\"Sheng Li\"},{\"authorId\":\"6018169\",\"name\":\"Zhiqiang Tao\"},{\"authorId\":\"104510214\",\"name\":\"K. Li\"},{\"authorId\":\"145692782\",\"name\":\"Yun Fu\"}],\"doi\":\"10.1109/TETCI.2019.2892755\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"title\":\"Visual to Text: Survey of Image and Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/125b0bde4ac0b4cb9453b205bc0c5c184af3dec2\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2757584\",\"name\":\"S. Matsuyoshi\"},{\"authorId\":\"2606962\",\"name\":\"Yugo Murawaki\"},{\"authorId\":\"1908467\",\"name\":\"Hirotaka Kameko\"},{\"authorId\":\"47096760\",\"name\":\"Shinsuke Mori\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a4103ca9c9336c43ae27c590763df9c7200eafc2\",\"title\":\"SGC-MEF: A Shogi Commentary Corpus Annotated with Modality Information\",\"url\":\"https://www.semanticscholar.org/paper/a4103ca9c9336c43ae27c590763df9c7200eafc2\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390818869\",\"name\":\"Jinlei Xu\"},{\"authorId\":\"144546140\",\"name\":\"T. Xu\"},{\"authorId\":\"123432231\",\"name\":\"Xin Tian\"},{\"authorId\":\"6681872\",\"name\":\"Chunping Liu\"},{\"authorId\":\"144911521\",\"name\":\"Y. Ji\"}],\"doi\":\"10.1109/IJCNN.2019.8851897\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf7b38dd24c20223e006066be4202d1da700af37\",\"title\":\"Context Gating with Short Temporal Information for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/bf7b38dd24c20223e006066be4202d1da700af37\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"124561dd9b4ae3b48847547afc2f366a8439aa76\",\"title\":\"Learning to Describe Video with Weak Supervision by Exploiting Negative Sentential Information\",\"url\":\"https://www.semanticscholar.org/paper/124561dd9b4ae3b48847547afc2f366a8439aa76\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"title\":\"Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/20ab42c9b93b6e41f6e1d7b546f87c5a871db020\",\"venue\":\"COLING\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1109/CVPR.2015.7298966\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"title\":\"Deep correlation for matching images and text\",\"url\":\"https://www.semanticscholar.org/paper/efb0e69bc640171d1f115bb286d865bec6f21a7f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1910.01210\",\"authors\":[{\"authorId\":\"1381902419\",\"name\":\"Mihir Prabhudesai\"},{\"authorId\":\"1693704\",\"name\":\"H. F. Tung\"},{\"authorId\":\"40604609\",\"name\":\"Syed Ashar Javed\"},{\"authorId\":\"51039185\",\"name\":\"Maximilian Sieb\"},{\"authorId\":\"34939798\",\"name\":\"Adam W. Harley\"},{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3245f7b672b382d9a122fe1730ae22113d61dbc4\",\"title\":\"Embodied Language Grounding with Implicit 3D Visual Feature Representations\",\"url\":\"https://www.semanticscholar.org/paper/3245f7b672b382d9a122fe1730ae22113d61dbc4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1502.06648\",\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-015-0851-8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0138ef03aa5cf7bf7d0ce70b3f98876af022ffcd\",\"title\":\"Recognizing Fine-Grained and Composite Activities Using Hand-Centric Features and Script Data\",\"url\":\"https://www.semanticscholar.org/paper/0138ef03aa5cf7bf7d0ce70b3f98876af022ffcd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451456\",\"name\":\"Tseng-Hung Chen\"},{\"authorId\":\"32970572\",\"name\":\"Kuo-Hao Zeng\"},{\"authorId\":\"2717138\",\"name\":\"W. T. Hsu\"},{\"authorId\":\"145718481\",\"name\":\"Min Sun\"}],\"doi\":\"10.1007/978-3-319-54407-6_18\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"title\":\"Video Captioning via Sentence Augmentation and Spatio-Temporal Attention\",\"url\":\"https://www.semanticscholar.org/paper/507d36a10ee5c3ca657bb2f41f9bb47552c30ed0\",\"venue\":\"ACCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/TPAMI.2014.2366143\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8403bf4e3060487cbc8acceb1fb256a4f1cfc76\",\"title\":\"Adopting Abstract Images for Semantic Scene Understanding\",\"url\":\"https://www.semanticscholar.org/paper/f8403bf4e3060487cbc8acceb1fb256a4f1cfc76\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"83039268\",\"name\":\"Soichiro Oura\"},{\"authorId\":\"2816822\",\"name\":\"T. Matsukawa\"},{\"authorId\":\"1690503\",\"name\":\"E. Suzuki\"}],\"doi\":\"10.1109/IJCNN.2018.8489668\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9e564f2ed4796e32fab8f9b90a52be8d6481a7fa\",\"title\":\"Multimodal Deep Neural Network with Image Sequence Features for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9e564f2ed4796e32fab8f9b90a52be8d6481a7fa\",\"venue\":\"2018 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2018},{\"arxivId\":\"1610.02947\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"7172307\",\"name\":\"Hyungjin Ko\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dc37dab102a0465098111b7ccf6f95b736397f2\",\"title\":\"End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/3dc37dab102a0465098111b7ccf6f95b736397f2\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1411.2539\",\"authors\":[{\"authorId\":\"3450996\",\"name\":\"Ryan Kiros\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"title\":\"Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models\",\"url\":\"https://www.semanticscholar.org/paper/2e36ea91a3c8fbff92be2989325531b4002e2afc\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49868702\",\"name\":\"Ran Wei\"},{\"authorId\":\"144065286\",\"name\":\"Li Mi\"},{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1016/j.jvcir.2020.102751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"title\":\"Exploiting the local temporal information for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4b43ca6f4615d5e384a9b404964a49ed21a14805\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"1902.10322\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"47398812\",\"name\":\"N. Akhtar\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1109/CVPR.2019.01277\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20888a7aebaf77a306c0886f165bd0d468db806d\",\"title\":\"Spatio-Temporal Dynamics and Semantic Attribute Enriched Visual Encoding for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/20888a7aebaf77a306c0886f165bd0d468db806d\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1703.07022\",\"authors\":[{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"1682058\",\"name\":\"H. Zhang\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.1109/ICCV.2017.364\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"428818a9edfb547431be6d7ec165c6af576c83d5\",\"title\":\"Recurrent Topic-Transition GAN for Visual Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/428818a9edfb547431be6d7ec165c6af576c83d5\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8387016\",\"name\":\"Ziwei Yang\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"},{\"authorId\":\"50219447\",\"name\":\"Zheng Wang\"}],\"doi\":\"10.1145/3123266.3123327\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"title\":\"Catching the Temporal Regions-of-Interest for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abc2e6431a7092fb11418b79ca1c41a76b811ea0\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3316824\",\"name\":\"Melissa Roemmele\"},{\"authorId\":\"2194124\",\"name\":\"Soja-Marie Morgens\"},{\"authorId\":\"35168946\",\"name\":\"Andrew S. Gordon\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.1145/2856767.2856793\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b62288a6585faa8d05a8c2b2d306f8fcf4b26df\",\"title\":\"Recognizing Human Actions in the Motion Trajectories of Shapes\",\"url\":\"https://www.semanticscholar.org/paper/4b62288a6585faa8d05a8c2b2d306f8fcf4b26df\",\"venue\":\"IUI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145308148\",\"name\":\"L. Bai\"},{\"authorId\":\"46554633\",\"name\":\"Lina Yang\"},{\"authorId\":\"145837495\",\"name\":\"Lin Huo\"},{\"authorId\":\"1705569\",\"name\":\"Taoshen Li\"}],\"doi\":\"10.1109/ICWAPR.2018.8521291\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3dedff7d7813af193589e44c204b08f34c297764\",\"title\":\"Image Description Generation by Modeling the Relationship Between Objects\",\"url\":\"https://www.semanticscholar.org/paper/3dedff7d7813af193589e44c204b08f34c297764\",\"venue\":\"2018 International Conference on Wavelet Analysis and Pattern Recognition (ICWAPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"3105474\",\"name\":\"Vignesh R. Paramathayalan\"},{\"authorId\":\"47775167\",\"name\":\"Teng Li\"},{\"authorId\":\"1742248\",\"name\":\"P. Moulin\"}],\"doi\":\"10.1007/s11263-016-0891-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4e7951a083bdac5c5d62c221de93181398240234\",\"title\":\"Multiple Granularity Modeling: A Coarse-to-Fine Framework for Fine-grained Action Analysis\",\"url\":\"https://www.semanticscholar.org/paper/4e7951a083bdac5c5d62c221de93181398240234\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":\"2012.07098\",\"authors\":[{\"authorId\":\"28282293\",\"name\":\"Begum Citamak\"},{\"authorId\":\"10791325\",\"name\":\"Ozan Caglayan\"},{\"authorId\":\"51214846\",\"name\":\"Menekse Kuyu\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"152827782\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"144695472\",\"name\":\"P. Madhyastha\"},{\"authorId\":\"1702974\",\"name\":\"Lucia Specia\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"title\":\"MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish\",\"url\":\"https://www.semanticscholar.org/paper/c2557b1a45412e1100d40ec1fe4073c3b00921f6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12984122\",\"name\":\"Pelin Dogan\"},{\"authorId\":\"144877478\",\"name\":\"M. Gross\"},{\"authorId\":\"31798873\",\"name\":\"J. Bazin\"}],\"doi\":\"10.1007/978-3-319-46604-0_43\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b21075184eece92f0d4362e38e6199cae131edbc\",\"title\":\"Label-Based Automatic Alignment of Video with Narrative Sentences\",\"url\":\"https://www.semanticscholar.org/paper/b21075184eece92f0d4362e38e6199cae131edbc\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c72ab3484bea5aa8abbd041d31f6b17c17513de\",\"title\":\"Expressing an Image Stream with a Sequence of Natural Sentences\",\"url\":\"https://www.semanticscholar.org/paper/1c72ab3484bea5aa8abbd041d31f6b17c17513de\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143715692\",\"name\":\"X. Hao\"},{\"authorId\":\"46468475\",\"name\":\"F. Zhou\"},{\"authorId\":\"33899331\",\"name\":\"Xiaoyong Li\"}],\"doi\":\"10.1109/ITNEC48623.2020.9084781\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"653de101370307afc2eba27d4e4c574441eb06da\",\"title\":\"Scene-Edge GRU for Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/653de101370307afc2eba27d4e4c574441eb06da\",\"venue\":\"2020 IEEE 4th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)\",\"year\":2020},{\"arxivId\":\"1611.06492\",\"authors\":[{\"authorId\":\"7284555\",\"name\":\"A. Jain\"},{\"authorId\":\"34762956\",\"name\":\"Abhinav Agarwalla\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"144240262\",\"name\":\"P. Mitra\"}],\"doi\":\"10.1109/CVPRW.2017.273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"title\":\"Recurrent Memory Addressing for Describing Videos\",\"url\":\"https://www.semanticscholar.org/paper/53a41c711b40e7fe3dc2b12e0790933d9c99a6e0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/ICCV.2013.117\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"092f57121e10dcb65a6c348dd8b529bb06ebfb89\",\"title\":\"Video Event Understanding Using Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/092f57121e10dcb65a6c348dd8b529bb06ebfb89\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff8eb9448c3d764e631f54e2936251f5ef5defcb\",\"title\":\"Generative Grammar Semantic Trees Parse Graphs Training Descriptions New Image Scene Graph Semantic Trees Generated Description Vision Models Training Images\",\"url\":\"https://www.semanticscholar.org/paper/ff8eb9448c3d764e631f54e2936251f5ef5defcb\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29544582\",\"name\":\"S. Tomori\"},{\"authorId\":\"1908467\",\"name\":\"Hirotaka Kameko\"},{\"authorId\":\"49584970\",\"name\":\"T. Ninomiya\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"},{\"authorId\":\"143946906\",\"name\":\"Yoshimasa Tsuruoka\"}],\"doi\":\"10.5715/JNLP.24.447\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4eb32ef819946d666bbbf88d4cba5806bf437c84\",\"title\":\"Improvement in Domain Specific Word Segmentation by Symbol Grounding\",\"url\":\"https://www.semanticscholar.org/paper/4eb32ef819946d666bbbf88d4cba5806bf437c84\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1907.09358\",\"authors\":[{\"authorId\":\"3219864\",\"name\":\"Aditya Mogadala\"},{\"authorId\":\"151119369\",\"name\":\"M. Kalimuthu\"},{\"authorId\":\"2561225\",\"name\":\"D. Klakow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f8a48678094adbe421d61d0045361bfc635a2900\",\"title\":\"Trends in Integration of Vision and Language Research: A Survey of Tasks, Datasets, and Methods\",\"url\":\"https://www.semanticscholar.org/paper/f8a48678094adbe421d61d0045361bfc635a2900\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1704.01518\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"39578749\",\"name\":\"Siyu Tang\"},{\"authorId\":\"2390510\",\"name\":\"Seong Joon Oh\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2017.447\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db2fecc8b1bd175d39687eb471360707a5fddb03\",\"title\":\"Generating Descriptions with Grounded and Co-referenced People\",\"url\":\"https://www.semanticscholar.org/paper/db2fecc8b1bd175d39687eb471360707a5fddb03\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1412.4729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.3115/v1/N15-1173\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cef41606f1e1324b683441e694f0e1c96387abf\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/8cef41606f1e1324b683441e694f0e1c96387abf\",\"venue\":\"HLT-NAACL\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144663763\",\"name\":\"Q. Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bc2856e70ad3c8fe439dec6cc6a2e03d6e090fb7\",\"title\":\"What value high level concepts in vision to language problems\",\"url\":\"https://www.semanticscholar.org/paper/bc2856e70ad3c8fe439dec6cc6a2e03d6e090fb7\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34382594\",\"name\":\"D. Francis\"},{\"authorId\":\"2086066\",\"name\":\"B. Huet\"}],\"doi\":\"10.1145/3347449.3357484\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"989730c00381805543baa470a2d6490cc5354a13\",\"title\":\"L-STAP: Learned Spatio-Temporal Adaptive Pooling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/989730c00381805543baa470a2d6490cc5354a13\",\"venue\":\"AI4TV@MM\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1759841\",\"name\":\"Xiaoshuai Sun\"},{\"authorId\":\"39767248\",\"name\":\"Jiewei Cao\"},{\"authorId\":null,\"name\":\"Chao Li\"},{\"authorId\":\"145081300\",\"name\":\"L. Zhu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e7696554d6881cd12ad2d0063b9663d566e13b6\",\"title\":\"Web-Based Semantic Fragment Discovery for On-Line Lingual-Visual Similarity\",\"url\":\"https://www.semanticscholar.org/paper/3e7696554d6881cd12ad2d0063b9663d566e13b6\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"144332826\",\"name\":\"Chen Kong\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":\"10.5244/C.29.93\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"63de6bb482d07f9e81ecbe9acfb62433b4ce6fe6\",\"title\":\"Generating Multi-sentence Natural Language Descriptions of Indoor Scenes\",\"url\":\"https://www.semanticscholar.org/paper/63de6bb482d07f9e81ecbe9acfb62433b4ce6fe6\",\"venue\":\"BMVC\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/WACV.2019.00026\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"183bf77d4f9b4eb227ba1d5a26eff5b6ab3d889d\",\"title\":\"Going Deeper With Semantics: Video Activity Interpretation Using Semantic Contextualization\",\"url\":\"https://www.semanticscholar.org/paper/183bf77d4f9b4eb227ba1d5a26eff5b6ab3d889d\",\"venue\":\"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51049553\",\"name\":\"L. Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"96c85be9860688405b99ea35e0e276c6bcf10a1a\",\"title\":\"Determining the best attributes for surveillance video keywords generation\",\"url\":\"https://www.semanticscholar.org/paper/96c85be9860688405b99ea35e0e276c6bcf10a1a\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47666014\",\"name\":\"Hongjun Chen\"},{\"authorId\":\"13032169\",\"name\":\"H. Li\"},{\"authorId\":\"9512265\",\"name\":\"Xueqin Wu\"}],\"doi\":\"10.1145/3380625.3380669\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a3b4b243afc2616eb2c3e24ec0abad76c2a80f7\",\"title\":\"Research on Feature Extraction and Multimodal Fusion of Video Caption Based on Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/1a3b4b243afc2616eb2c3e24ec0abad76c2a80f7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"145886114\",\"name\":\"Jun Guo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"title\":\"Object-Oriented Video Captioning with Temporal Graph and Prior Knowledge Building\",\"url\":\"https://www.semanticscholar.org/paper/745f54a822bdbd33cf08e65b665ab3f3528cdf78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.07335\",\"authors\":[{\"authorId\":\"16286752\",\"name\":\"Md. Mushfiqur Rahman\"},{\"authorId\":\"1945520590\",\"name\":\"Thasin Abedin\"},{\"authorId\":\"1945917684\",\"name\":\"Khondokar S. S. Prottoy\"},{\"authorId\":\"1945917586\",\"name\":\"Ayana Moshruba\"},{\"authorId\":\"32826273\",\"name\":\"F. H. Siddiqui\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e2a0c5a1d5ae87ae734da2ef9c5eac6b2146536b\",\"title\":\"Semantically Sensible Video Captioning (SSVC)\",\"url\":\"https://www.semanticscholar.org/paper/e2a0c5a1d5ae87ae734da2ef9c5eac6b2146536b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2296971\",\"name\":\"Iftekhar Naim\"},{\"authorId\":\"3193978\",\"name\":\"Y. C. Song\"},{\"authorId\":\"2579796\",\"name\":\"Qiguang Liu\"},{\"authorId\":\"144768480\",\"name\":\"Liang Huang\"},{\"authorId\":\"1690271\",\"name\":\"Henry A. Kautz\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1793218\",\"name\":\"Daniel Gildea\"}],\"doi\":\"10.3115/v1/N15-1017\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33b43af280c2e0120ab20eea6a1454c1572ff4f7\",\"title\":\"Discriminative Unsupervised Alignment of Natural Language Instructions with Corresponding Video Segments\",\"url\":\"https://www.semanticscholar.org/paper/33b43af280c2e0120ab20eea6a1454c1572ff4f7\",\"venue\":\"HLT-NAACL\",\"year\":2015},{\"arxivId\":\"1506.01698\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-24947-6_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"title\":\"The Long-Short Story of Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/49512d11c468dc2fe3fe832d8c4dc8e0a01b0a4b\",\"venue\":\"GCPR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52014393\",\"name\":\"Ut Austin\"},{\"authorId\":\"123312980\",\"name\":\"Austin\"},{\"authorId\":\"102704114\",\"name\":\"UMass Lowell\"},{\"authorId\":\"102898595\",\"name\":\"Lowell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"43795b7bac3d921c4e579964b54187bdbf6c6330\",\"title\":\"Translating Videos to Natural Language Using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/43795b7bac3d921c4e579964b54187bdbf6c6330\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1708.02478\",\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"1718099\",\"name\":\"A. Hanjalic\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1109/TNNLS.2018.2851077\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d78c47093fbf3d85225fd502674aba4a29b3987\",\"title\":\"From Deterministic to Generative: Multimodal Stochastic RNNs for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/7d78c47093fbf3d85225fd502674aba4a29b3987\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40773963\",\"name\":\"V. Wankhede\"},{\"authorId\":\"19940880\",\"name\":\"R. Kagalkar\"}],\"doi\":\"10.21917/ijivp.2017.0212\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f6dae322b36aded6bd3ed7842db5f1c07d474b0\",\"title\":\"SUPPORT VECTOR MACHINE BASED APPROACH FOR TRANSLATING VIDEO SCENERIES TO NATURAL LANGUAGE DESCRIPTIONS\",\"url\":\"https://www.semanticscholar.org/paper/8f6dae322b36aded6bd3ed7842db5f1c07d474b0\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8280077\",\"name\":\"Yuyu Guo\"},{\"authorId\":\"3145905\",\"name\":\"Jingqiu Zhang\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0530-0\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"title\":\"Exploiting long-term temporal dynamics for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/a304bea200da57e4a7ee3ca6ad36b5496763a6d0\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":\"1510.07712\",\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"40579682\",\"name\":\"J. Wang\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"46285992\",\"name\":\"Y. Yang\"},{\"authorId\":\"145738410\",\"name\":\"W. Xu\"}],\"doi\":\"10.1109/CVPR.2016.496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f678a0041f2c6f931168010e7418c500c3f14cdb\",\"title\":\"Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/f678a0041f2c6f931168010e7418c500c3f14cdb\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1505.06027\",\"authors\":[{\"authorId\":\"2329288\",\"name\":\"P. Bojanowski\"},{\"authorId\":\"3215950\",\"name\":\"R\\u00e9mi Lajugie\"},{\"authorId\":\"3024698\",\"name\":\"E. Grave\"},{\"authorId\":\"144570279\",\"name\":\"Francis R. Bach\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"144189388\",\"name\":\"J. Ponce\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":\"10.1109/ICCV.2015.507\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59ac98f3910dad473e7771ac61f796a038f1708f\",\"title\":\"Weakly-Supervised Alignment of Video with Text\",\"url\":\"https://www.semanticscholar.org/paper/59ac98f3910dad473e7771ac61f796a038f1708f\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jesus Perez-Martin\"},{\"authorId\":null,\"name\":\"Benjamin Bustos\"},{\"authorId\":\"30501537\",\"name\":\"J. P\\u00e9rez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cba8f7e9c9d153189e61f0e39bb56d52da5e0956\",\"title\":\"Improving Video Captioning with Temporal Composition of a Visual-Syntactic Embedding\",\"url\":\"https://www.semanticscholar.org/paper/cba8f7e9c9d153189e61f0e39bb56d52da5e0956\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1007/s11263-017-1033-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"828ac57f755db989e2886042a85278ae4823297c\",\"title\":\"Uncovering the Temporal Context for Video Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/828ac57f755db989e2886042a85278ae4823297c\",\"venue\":\"International Journal of Computer Vision\",\"year\":2017},{\"arxivId\":\"1609.06782\",\"authors\":[{\"authorId\":\"3099139\",\"name\":\"Zuxuan Wu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"35782003\",\"name\":\"Yanwei Fu\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"}],\"doi\":\"10.1145/3122865.3122867\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"title\":\"Deep Learning for Video Classification and Captioning\",\"url\":\"https://www.semanticscholar.org/paper/1a4b6ee6cd846ef5e3030a6ae59f026e5f50eda6\",\"venue\":\"Frontiers of Multimedia Research\",\"year\":2018},{\"arxivId\":\"1905.03966\",\"authors\":[{\"authorId\":\"1678473\",\"name\":\"W. Pei\"},{\"authorId\":\"49050519\",\"name\":\"Jiyuan Zhang\"},{\"authorId\":\"47119038\",\"name\":\"X. Wang\"},{\"authorId\":\"2265229\",\"name\":\"Lei Ke\"},{\"authorId\":\"2029246\",\"name\":\"Xiaoyong Shen\"},{\"authorId\":\"5068280\",\"name\":\"Yu-Wing Tai\"}],\"doi\":\"10.1109/CVPR.2019.00854\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"title\":\"Memory-Attended Recurrent Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/b12124f7bbdd3a99d6b392024806d0f3124380ac\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1612.01452\",\"authors\":[{\"authorId\":\"49675890\",\"name\":\"M. Simon\"},{\"authorId\":\"1679449\",\"name\":\"Erik Rodner\"},{\"authorId\":\"1728382\",\"name\":\"Joachim Denzler\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6d7f57f683be52ccf8d24ce21e72e2b44514511\",\"title\":\"ImageNet pre-trained models with batch normalization\",\"url\":\"https://www.semanticscholar.org/paper/a6d7f57f683be52ccf8d24ce21e72e2b44514511\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"title\":\"Story Understanding through Semantic Analysis and Automatic Alignment of Text and Video\",\"url\":\"https://www.semanticscholar.org/paper/39efe4ac24f9f1a9b68e210b84a3432505cfcac2\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50244843\",\"name\":\"E. Barsoum\"}],\"doi\":\"10.7916/d8-sq89-mm29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d8d9ab14c06bbbe084232517c8a67581d16d4ef0\",\"title\":\"Human Motion Anticipation and Recognition from RGB-D\",\"url\":\"https://www.semanticscholar.org/paper/d8d9ab14c06bbbe084232517c8a67581d16d4ef0\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/TIP.2020.2988435\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"title\":\"Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation\",\"url\":\"https://www.semanticscholar.org/paper/efb373e597cee2046d0616dd4a1d8a1d1e2c7ad3\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145948903\",\"name\":\"Raj Kumar Gupta\"},{\"authorId\":\"40341306\",\"name\":\"Y. Yang\"}],\"doi\":\"10.1007/978-3-319-50832-0_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b81efe739e5d188443e3bd4a27b51d6e854e86c1\",\"title\":\"Leveraging Multi-modal Analyses and Online Knowledge Base for Video Aboutness Generation\",\"url\":\"https://www.semanticscholar.org/paper/b81efe739e5d188443e3bd4a27b51d6e854e86c1\",\"venue\":\"ISVC\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740650\",\"name\":\"Jian Jhen Chen\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"144618699\",\"name\":\"F. Shen\"},{\"authorId\":\"2838253\",\"name\":\"C. He\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.1145/3132847.3132922\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3085671f6232aac4492ad861d09334b8f3a7e2a7\",\"title\":\"Movie Fill in the Blank with Adaptive Temporal Attention and Description Update\",\"url\":\"https://www.semanticscholar.org/paper/3085671f6232aac4492ad861d09334b8f3a7e2a7\",\"venue\":\"CIKM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1453580188\",\"name\":\"Bhavesh Ahuja\"},{\"authorId\":\"1721052388\",\"name\":\"Austin Coutinho\"},{\"authorId\":\"1721059094\",\"name\":\"Chandan Bhangale\"},{\"authorId\":\"1721051944\",\"name\":\"Chinmay Sankhe\"},{\"authorId\":\"34907829\",\"name\":\"S. Khedkar\"}],\"doi\":\"10.1109/icesc48915.2020.9155779\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"575064292c7bdfc268ae492f6bd5784ac9affd35\",\"title\":\"Video Analysis and Natural Language Description Generation System\",\"url\":\"https://www.semanticscholar.org/paper/575064292c7bdfc268ae492f6bd5784ac9affd35\",\"venue\":\"2020 International Conference on Electronics and Sustainable Communication Systems (ICESC)\",\"year\":2020},{\"arxivId\":\"1509.07225\",\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1109/ICCV.2015.298\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d925db7c9e3cca2e8fed644f750d218a48cd081\",\"title\":\"Automatic Concept Discovery from Parallel Text and Visual Corpora\",\"url\":\"https://www.semanticscholar.org/paper/4d925db7c9e3cca2e8fed644f750d218a48cd081\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2089428\",\"name\":\"D. P. Barrett\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eec326ea0a1a1044b011c2b454fe8b7ce1240a30\",\"title\":\"Learning in vision and robotics\",\"url\":\"https://www.semanticscholar.org/paper/eec326ea0a1a1044b011c2b454fe8b7ce1240a30\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"2012.13136\",\"authors\":[{\"authorId\":\"48394426\",\"name\":\"Naeha Sharif\"},{\"authorId\":\"39689790\",\"name\":\"L. White\"},{\"authorId\":\"1698675\",\"name\":\"M. Bennamoun\"},{\"authorId\":\"46641911\",\"name\":\"W. Liu\"},{\"authorId\":\"14752125\",\"name\":\"Syed Afaq Ali Shah\"}],\"doi\":\"10.1007/s11263-019-01206-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"47286105575aacaed5ef74af9ae007e258abc60a\",\"title\":\"LCEval: Learned Composite Metric for Caption Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/47286105575aacaed5ef74af9ae007e258abc60a\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1109/CRV.2017.51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6718f2feea2d16b894b738551c38871c8afee11b\",\"title\":\"Towards a Knowledge-Based Approach for Generating Video Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/6718f2feea2d16b894b738551c38871c8afee11b\",\"venue\":\"2017 14th Conference on Computer and Robot Vision (CRV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"153576781\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/JIOT.2017.2779865\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"title\":\"Attention-in-Attention Networks for Surveillance Video Understanding in Internet of Things\",\"url\":\"https://www.semanticscholar.org/paper/d6cdf8dfa20d35af8714062d1ac203e80550ab6f\",\"venue\":\"IEEE Internet of Things Journal\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7741774\",\"name\":\"Y. Hu\"},{\"authorId\":\"1724811\",\"name\":\"Z. Chen\"},{\"authorId\":\"51260253\",\"name\":\"Z. Zha\"},{\"authorId\":\"51239188\",\"name\":\"Fengcheng Wu\"}],\"doi\":\"10.1145/3343031.3351072\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db6035229a71a6c93d4f15c4a4280eb644228da4\",\"title\":\"Hierarchical Global-Local Temporal Modeling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/db6035229a71a6c93d4f15c4a4280eb644228da4\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"22858671\",\"name\":\"M. Ali\"},{\"authorId\":\"34872472\",\"name\":\"Facundo Ciancio\"},{\"authorId\":\"145270024\",\"name\":\"R. Zhao\"},{\"authorId\":\"2296971\",\"name\":\"Iftekhar Naim\"},{\"authorId\":\"144619896\",\"name\":\"M. Hoque\"}],\"doi\":\"10.1145/2971648.2971743\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d2e530fc49c24a005db3108cca283c81891687e5\",\"title\":\"ROC comment: automated descriptive and subjective captioning of behavioral videos\",\"url\":\"https://www.semanticscholar.org/paper/d2e530fc49c24a005db3108cca283c81891687e5\",\"venue\":\"UbiComp\",\"year\":2016},{\"arxivId\":\"1602.06539\",\"authors\":[{\"authorId\":\"2499431\",\"name\":\"Liangchen Liu\"},{\"authorId\":\"2331880\",\"name\":\"A. Wiliem\"},{\"authorId\":\"3104113\",\"name\":\"Shaokang Chen\"},{\"authorId\":\"144314685\",\"name\":\"Kun Zhao\"},{\"authorId\":\"144367279\",\"name\":\"B. Lovell\"}],\"doi\":\"10.1109/ISBA.2016.7477239\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"57df27685196fad070bd2da14ed865fda87d93a9\",\"title\":\"Determining the best attributes for surveillance video keywords generation\",\"url\":\"https://www.semanticscholar.org/paper/57df27685196fad070bd2da14ed865fda87d93a9\",\"venue\":\"2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2362276\",\"name\":\"Ekaterina Shutova\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"144608002\",\"name\":\"Gerard de Melo\"}],\"doi\":\"10.3115/v1/P15-1092\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b899edf9df955b64d2e2b0f6adb4b861a94d5e4d\",\"title\":\"Perceptually Grounded Selectional Preferences\",\"url\":\"https://www.semanticscholar.org/paper/b899edf9df955b64d2e2b0f6adb4b861a94d5e4d\",\"venue\":\"ACL\",\"year\":2015},{\"arxivId\":\"1601.03896\",\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.24963/ijcai.2017/704\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/6e9aebe54f76d85c6df7e80faa761ef0aec3d54c\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/CVPR.2015.7298878\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5838af587938e74b5758414c384dcf16dd6e1d1e\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/5838af587938e74b5758414c384dcf16dd6e1d1e\",\"venue\":\"CVPR\",\"year\":2015},{\"arxivId\":\"1806.00186\",\"authors\":[{\"authorId\":\"50978260\",\"name\":\"Nayyer Aafaq\"},{\"authorId\":\"1746166\",\"name\":\"Syed Zulqarnain Gilani\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"46332747\",\"name\":\"A. Mian\"}],\"doi\":\"10.1145/3355390\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"title\":\"Video Description\",\"url\":\"https://www.semanticscholar.org/paper/665a5673d33a90a1b71c0d5b1be127a76af43be7\",\"venue\":\"ACM Comput. Surv.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46503814\",\"name\":\"Lukas Diem\"},{\"authorId\":\"1736319\",\"name\":\"M. Zaharieva\"}],\"doi\":\"10.1109/CBMI.2015.7153602\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fa60a0aa335793b0cf56529d5f02cc4076292576\",\"title\":\"Interpretable video representation\",\"url\":\"https://www.semanticscholar.org/paper/fa60a0aa335793b0cf56529d5f02cc4076292576\",\"venue\":\"2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3449299\",\"name\":\"Atsushi Ushiku\"},{\"authorId\":\"144921213\",\"name\":\"H. Hashimoto\"},{\"authorId\":\"50594656\",\"name\":\"A. Hashimoto\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abd4152773ebb97b90163b9a6bbdf2075e825481\",\"title\":\"Procedural Text Generation from an Execution Video\",\"url\":\"https://www.semanticscholar.org/paper/abd4152773ebb97b90163b9a6bbdf2075e825481\",\"venue\":\"IJCNLP\",\"year\":2017},{\"arxivId\":\"1611.07837\",\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"2984407\",\"name\":\"Martin Renqiang Min\"},{\"authorId\":\"144702900\",\"name\":\"Zhe Gan\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d692d14b4277e6ad00b9030ad3b68141b3bbc21\",\"title\":\"Adaptive Feature Abstraction for Translating Video to Language\",\"url\":\"https://www.semanticscholar.org/paper/2d692d14b4277e6ad00b9030ad3b68141b3bbc21\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46510146\",\"name\":\"T. Sasada\"},{\"authorId\":\"48608426\",\"name\":\"S. Mori\"},{\"authorId\":\"2936312\",\"name\":\"Y. Yamakata\"},{\"authorId\":\"21617362\",\"name\":\"Hirokuni Maeta\"},{\"authorId\":\"1717105\",\"name\":\"Tatsuya Kawahara\"}],\"doi\":\"10.5715/JNLP.22.107\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2bcf85d5597d2f45c87ba59408c852d213234127\",\"title\":\"Definition of Recipe Terms and Corpus Annotation for their Automatic Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2bcf85d5597d2f45c87ba59408c852d213234127\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1511.06674\",\"authors\":[{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"1749627\",\"name\":\"Marius Leordeanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8120a64a73b89294990b3c1e4567b503869b8979\",\"title\":\"Stories in the Eye: Contextual Visual Interactions for Efficient Video to Language Translation\",\"url\":\"https://www.semanticscholar.org/paper/8120a64a73b89294990b3c1e4567b503869b8979\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1608.00187\",\"authors\":[{\"authorId\":\"1830034\",\"name\":\"Cewu Lu\"},{\"authorId\":\"145237361\",\"name\":\"R. Krishna\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-46448-0_51\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d9506257186023b78cf19ed4f9e77a4ae4fa0f0\",\"title\":\"Visual Relationship Detection with Language Priors\",\"url\":\"https://www.semanticscholar.org/paper/4d9506257186023b78cf19ed4f9e77a4ae4fa0f0\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2367268\",\"name\":\"Lijuan Zhou\"},{\"authorId\":\"1685696\",\"name\":\"W. Li\"},{\"authorId\":\"1719314\",\"name\":\"P. Ogunbona\"},{\"authorId\":\"1809184\",\"name\":\"Z. Zhang\"}],\"doi\":\"10.1016/j.patcog.2017.06.035\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c37001f836a4f53d59a5017518e87a651f3f8db\",\"title\":\"Semantic action recognition by learning a pose lexicon\",\"url\":\"https://www.semanticscholar.org/paper/3c37001f836a4f53d59a5017518e87a651f3f8db\",\"venue\":\"Pattern Recognit.\",\"year\":2017},{\"arxivId\":\"1809.08381\",\"authors\":[{\"authorId\":\"29132542\",\"name\":\"Meera Hahn\"},{\"authorId\":\"31601235\",\"name\":\"Nataniel Ruiz\"},{\"authorId\":\"2285263\",\"name\":\"Jean-Baptiste Alayrac\"},{\"authorId\":\"143991676\",\"name\":\"I. Laptev\"},{\"authorId\":\"144177248\",\"name\":\"James M. Rehg\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a7c9b3eb8737d29a779a0e21e355543c22ccca49\",\"title\":\"Learning to Localize and Align Fine-Grained Actions to Sparse Instructions\",\"url\":\"https://www.semanticscholar.org/paper/a7c9b3eb8737d29a779a0e21e355543c22ccca49\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1505.05914\",\"authors\":[{\"authorId\":\"46485395\",\"name\":\"Huijuan Xu\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2018561\",\"name\":\"Vasili Ramanishka\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"title\":\"A Multi-scale Multiple Instance Video Description Network\",\"url\":\"https://www.semanticscholar.org/paper/08b4577100d63d9e9fd8e35045e220e5cf640ce2\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2907739\",\"name\":\"Masoomeh Nabati\"},{\"authorId\":\"30756748\",\"name\":\"A. Behrad\"}],\"doi\":\"10.1016/j.ipm.2020.102302\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"title\":\"Multi-Sentence Video Captioning using Content-oriented Beam Searching and Multi-stage Refining Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/aca85e733323b2b364ae79fd0934edf2f1544ca1\",\"venue\":\"Inf. Process. Manag.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2296971\",\"name\":\"Iftekhar Naim\"},{\"authorId\":\"3193978\",\"name\":\"Y. C. Song\"},{\"authorId\":\"2579796\",\"name\":\"Qiguang Liu\"},{\"authorId\":\"1690271\",\"name\":\"Henry A. Kautz\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"},{\"authorId\":\"1793218\",\"name\":\"Daniel Gildea\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0378e48ffe916055328b95ce606f89e1b18fedcd\",\"title\":\"Unsupervised Alignment of Natural Language Instructions with Video Segments\",\"url\":\"https://www.semanticscholar.org/paper/0378e48ffe916055328b95ce606f89e1b18fedcd\",\"venue\":\"AAAI\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51288875\",\"name\":\"Y. Zhou\"},{\"authorId\":\"49941674\",\"name\":\"Zhenzhen Hu\"},{\"authorId\":\"3076466\",\"name\":\"X. Liu\"},{\"authorId\":\"39872583\",\"name\":\"M. Wang\"}],\"doi\":\"10.1007/978-3-030-00776-8_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"title\":\"Video Captioning Based on the Spatial-Temporal Saliency Tracing\",\"url\":\"https://www.semanticscholar.org/paper/ee34a697bc114a5284025648fb9b49f9cdf5e343\",\"venue\":\"PCM\",\"year\":2018},{\"arxivId\":\"1505.00468\",\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"118707418\",\"name\":\"M. Mitchell\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"}],\"doi\":\"10.1007/s11263-016-0966-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/97ad70a9fa3f99adf18030e5e38ebe3d90daa2db\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51209797\",\"name\":\"Johannes Rabold\"},{\"authorId\":\"1793426\",\"name\":\"M. Siebers\"},{\"authorId\":\"1727734\",\"name\":\"U. Schmid\"}],\"doi\":\"10.1007/978-3-319-99960-9_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6293fc91be733f01266413cbeb0e4385afdee5f\",\"title\":\"Explaining Black-Box Classifiers with ILP - Empowering LIME with Aleph to Approximate Non-linear Decisions with Relational Rules\",\"url\":\"https://www.semanticscholar.org/paper/a6293fc91be733f01266413cbeb0e4385afdee5f\",\"venue\":\"ILP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153152064\",\"name\":\"A. Liu\"},{\"authorId\":\"52196222\",\"name\":\"Y. Qiu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"153011269\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1109/ACCESS.2018.2879642\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"title\":\"A Fine-Grained Spatial-Temporal Attention Model for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e6b0247896a9d2eca0f4901032f5cfabd5b09dbe\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1812.02501\",\"authors\":[{\"authorId\":\"34678431\",\"name\":\"F. Sener\"},{\"authorId\":\"144031869\",\"name\":\"A. Yao\"}],\"doi\":\"10.1109/ICCV.2019.00095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15a36f9639f608c4567302de65355543fdcee910\",\"title\":\"Zero-Shot Anticipation for Instructional Activities\",\"url\":\"https://www.semanticscholar.org/paper/15a36f9639f608c4567302de65355543fdcee910\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"1806.11538\",\"authors\":[{\"authorId\":\"2180892\",\"name\":\"Yikang Li\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"},{\"authorId\":\"9349527\",\"name\":\"Yawen Cui\"},{\"authorId\":\"46865320\",\"name\":\"Jianping Shi\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1007/978-3-030-01246-5_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"acfe5b5c99be70fa3120d410e7be55b9fe299f40\",\"title\":\"Factorizable Net: An Efficient Subgraph-based Framework for Scene Graph Generation\",\"url\":\"https://www.semanticscholar.org/paper/acfe5b5c99be70fa3120d410e7be55b9fe299f40\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26994223\",\"name\":\"Yunbin Tu\"},{\"authorId\":\"144161025\",\"name\":\"C. Zhou\"},{\"authorId\":\"2011768695\",\"name\":\"Junjun Guo\"},{\"authorId\":\"2409659\",\"name\":\"Shengxiang Gao\"},{\"authorId\":\"121854326\",\"name\":\"Zhengtao Yu\"}],\"doi\":\"10.1016/j.patcog.2020.107702\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6686fadf7f7ef2283cc9286095db281f8520ec04\",\"title\":\"Enhancing the alignment between target words and corresponding frames for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/6686fadf7f7ef2283cc9286095db281f8520ec04\",\"venue\":\"Pattern Recognit.\",\"year\":2021},{\"arxivId\":\"2005.03804\",\"authors\":[{\"authorId\":\"3377097\",\"name\":\"A. Sharghi\"},{\"authorId\":\"1700665\",\"name\":\"N. Lobo\"},{\"authorId\":\"145103010\",\"name\":\"M. Shah\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7cd871b42efb42f507444386e4317efd7dfc10c\",\"title\":\"Text Synopsis Generation for Egocentric Videos\",\"url\":\"https://www.semanticscholar.org/paper/d7cd871b42efb42f507444386e4317efd7dfc10c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.13608\",\"authors\":[{\"authorId\":\"1739188006\",\"name\":\"Sheng-Yu Zhang\"},{\"authorId\":\"3856602\",\"name\":\"Ziqi Tan\"},{\"authorId\":\"145919748\",\"name\":\"Jin Yu\"},{\"authorId\":\"50144812\",\"name\":\"Z. Zhao\"},{\"authorId\":\"33870528\",\"name\":\"Kun Kuang\"},{\"authorId\":\"71328060\",\"name\":\"T. Jiang\"},{\"authorId\":\"1709595\",\"name\":\"Jingren Zhou\"},{\"authorId\":\"38385080\",\"name\":\"Hongxia Yang\"},{\"authorId\":\"32996440\",\"name\":\"F. Wu\"}],\"doi\":\"10.1145/3394486.3403325\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d271e93c7566b231e560c48b4cc4942077d762f9\",\"title\":\"Comprehensive Information Integration Modeling Framework for Video Titling\",\"url\":\"https://www.semanticscholar.org/paper/d271e93c7566b231e560c48b4cc4942077d762f9\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"}],\"doi\":\"10.15781/T2QR4P68H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"title\":\"Natural Language Video Description using Deep Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/191d4ba0825ff83afe91e94dafe27df8eb0202b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1911.00713\",\"authors\":[{\"authorId\":\"49780826\",\"name\":\"Hao Zhou\"},{\"authorId\":\"1750897\",\"name\":\"Chongyang Zhang\"},{\"authorId\":\"144541695\",\"name\":\"Chuanping Hu\"}],\"doi\":\"10.1145/3343031.3351024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad4333224c77079da0aee6b2b4a31c7f14cbd886\",\"title\":\"Visual Relationship Detection with Relative Location Mining\",\"url\":\"https://www.semanticscholar.org/paper/ad4333224c77079da0aee6b2b4a31c7f14cbd886\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48926662\",\"name\":\"Yibo Dai\"},{\"authorId\":\"47074418\",\"name\":\"C. Wang\"},{\"authorId\":\"145550812\",\"name\":\"J. Dong\"},{\"authorId\":\"145928755\",\"name\":\"C. Sun\"}],\"doi\":\"10.1007/s11042-019-7732-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fde4f99da84d9e7ba92933087775feb577109b62\",\"title\":\"Visual relationship detection based on bidirectional recurrent neural network\",\"url\":\"https://www.semanticscholar.org/paper/fde4f99da84d9e7ba92933087775feb577109b62\",\"venue\":\"Multimedia Tools and Applications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"50504401\",\"name\":\"Wei Chen\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"title\":\"Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework\",\"url\":\"https://www.semanticscholar.org/paper/1654e19de0187085e9d1da2d9e8718f49cd2f731\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1804399\",\"name\":\"Guang Li\"},{\"authorId\":\"22771932\",\"name\":\"Shubo Ma\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\"}],\"doi\":\"10.1145/2733373.2806314\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"39cc55356215fef3f975c74fd024441dcdc20b65\",\"title\":\"Summarization-based Video Caption via Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/39cc55356215fef3f975c74fd024441dcdc20b65\",\"venue\":\"ACM Multimedia\",\"year\":2015},{\"arxivId\":\"1605.03705\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/s11263-016-0987-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"title\":\"Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/154c22ca5eef149aedc8a986fa684ca1fd14e7dc\",\"venue\":\"International Journal of Computer Vision\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2017.398\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7fac20f3908c69bd336ea252e28c79f5abaa6dbe\",\"title\":\"Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/7fac20f3908c69bd336ea252e28c79f5abaa6dbe\",\"venue\":\"ICCV 2017\",\"year\":2017},{\"arxivId\":\"1506.01144\",\"authors\":[{\"authorId\":\"34902783\",\"name\":\"Qi Wu\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"2161037\",\"name\":\"L. Liu\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/CVPR.2016.29\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"title\":\"What Value Do Explicit High Level Concepts Have in Vision to Language Problems?\",\"url\":\"https://www.semanticscholar.org/paper/00fe3d95d0fd5f1433d81405bee772c4fe9af9c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5781871\",\"name\":\"Jiaqi Su\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"title\":\"Study of Video Captioning Problem\",\"url\":\"https://www.semanticscholar.org/paper/511f0041124d8d14bbcdc7f0e57f3bfe13a58e99\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":\"10.1090/QAM/1530\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e0b6b6921e93ee2dbc279c4a63c630156e5d1e9\",\"title\":\"Generating open world descriptions of video using common sense knowledge in a pattern theory framework\",\"url\":\"https://www.semanticscholar.org/paper/3e0b6b6921e93ee2dbc279c4a63c630156e5d1e9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2003.13942\",\"authors\":[{\"authorId\":\"52170427\",\"name\":\"Boxiao Pan\"},{\"authorId\":\"30017683\",\"name\":\"Haoye Cai\"},{\"authorId\":\"38485317\",\"name\":\"De-An Huang\"},{\"authorId\":\"144015229\",\"name\":\"Kuan-Hui Lee\"},{\"authorId\":\"1799820\",\"name\":\"Adrien Gaidon\"},{\"authorId\":\"46408185\",\"name\":\"E. Adeli\"},{\"authorId\":\"9200530\",\"name\":\"Juan Carlos Niebles\"}],\"doi\":\"10.1109/cvpr42600.2020.01088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a444d32f30d38fb0cb811fa1a9b601511244fb5b\",\"title\":\"Spatio-Temporal Graph for Video Captioning With Knowledge Distillation\",\"url\":\"https://www.semanticscholar.org/paper/a444d32f30d38fb0cb811fa1a9b601511244fb5b\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2909813\",\"name\":\"C. C. Tan\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1007/s13735-015-0090-3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34c1ae7fdd00b4e46959443728e1a586bd78c74a\",\"title\":\"On the use of commonsense ontology for multimedia event recounting\",\"url\":\"https://www.semanticscholar.org/paper/34c1ae7fdd00b4e46959443728e1a586bd78c74a\",\"venue\":\"International Journal of Multimedia Information Retrieval\",\"year\":2015},{\"arxivId\":\"1501.02530\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1721168\",\"name\":\"Niket Tandon\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/CVPR.2015.7298940\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"title\":\"A dataset for Movie Description\",\"url\":\"https://www.semanticscholar.org/paper/a5ea0da7b93452bec54b5034706f2255bfb5a8f3\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49420316\",\"name\":\"Yuan Liu\"},{\"authorId\":\"145950948\",\"name\":\"Xue Li\"},{\"authorId\":\"2558130\",\"name\":\"Z. Shi\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e8ce74a73bb0b3197d4194fcb638710d76970654\",\"title\":\"Video Captioning with Listwise Supervision\",\"url\":\"https://www.semanticscholar.org/paper/e8ce74a73bb0b3197d4194fcb638710d76970654\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1503.00064\",\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"144332826\",\"name\":\"Chen Kong\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c80c4ba8226ec556e1775e647f91bb8c126b5e57\",\"title\":\"Generating Multi-Sentence Lingual Descriptions of Indoor Scenes\",\"url\":\"https://www.semanticscholar.org/paper/c80c4ba8226ec556e1775e647f91bb8c126b5e57\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1708.09666\",\"authors\":[{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"}],\"doi\":\"10.1145/3078971.3079000\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"title\":\"Generating Video Descriptions with Topic Guidance\",\"url\":\"https://www.semanticscholar.org/paper/d953d9767070bdb1f4f1af9e2a923dff047353cf\",\"venue\":\"ICMR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"150213872\",\"name\":\"M. Hemalatha.\"},{\"authorId\":\"143783787\",\"name\":\"C. C. Sekhar\"}],\"doi\":\"10.1109/WACV45572.2020.9093344\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"509b25d45c6f5e3cafa48395c941611364e22efc\",\"title\":\"Domain-Specific Semantics Guided Approach to Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/509b25d45c6f5e3cafa48395c941611364e22efc\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1715610\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"},{\"authorId\":\"3775903\",\"name\":\"J. Wang\"},{\"authorId\":\"2699095\",\"name\":\"A. Dick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"55a7286f014cc6b51a3f50b1e6bc8acc8166f231\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and Their Related External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/55a7286f014cc6b51a3f50b1e6bc8acc8166f231\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1711.08097\",\"authors\":[{\"authorId\":\"8598253\",\"name\":\"Wang-Li Hao\"},{\"authorId\":\"145274329\",\"name\":\"Zhaoxiang Zhang\"},{\"authorId\":\"32561502\",\"name\":\"H. Guan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dde65325dc7600d02983a76bd54693f0050946a4\",\"title\":\"Integrating both Visual and Audio Cues for Enhanced Video Caption\",\"url\":\"https://www.semanticscholar.org/paper/dde65325dc7600d02983a76bd54693f0050946a4\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"145809603\",\"name\":\"N. Siddharth\"},{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":\"10.1613/jair.4556\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"083d1055f81dd7c9b41233a92b9768a857d1db58\",\"title\":\"A Compositional Framework for Grounding Language Inference, Generation, and Acquisition in Video\",\"url\":\"https://www.semanticscholar.org/paper/083d1055f81dd7c9b41233a92b9768a857d1db58\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2015},{\"arxivId\":\"1511.03416\",\"authors\":[{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"50499889\",\"name\":\"O. Groth\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/CVPR.2016.540\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"def584565d05d6a8ba94de6621adab9e301d375d\",\"title\":\"Visual7W: Grounded Question Answering in Images\",\"url\":\"https://www.semanticscholar.org/paper/def584565d05d6a8ba94de6621adab9e301d375d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1703.10476\",\"authors\":[{\"authorId\":\"38314306\",\"name\":\"Rakshith Shetty\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1739548\",\"name\":\"M. Fritz\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1109/ICCV.2017.445\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c0a6854b793ca8ad281513c184318b73d4868c4\",\"title\":\"Speaking the Same Language: Matching Machine to Human Captions by Adversarial Training\",\"url\":\"https://www.semanticscholar.org/paper/1c0a6854b793ca8ad281513c184318b73d4868c4\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"2001.06127\",\"authors\":[{\"authorId\":\"2691929\",\"name\":\"A. Cherian\"},{\"authorId\":\"46585209\",\"name\":\"J. Wang\"},{\"authorId\":\"1765212\",\"name\":\"C. Hori\"},{\"authorId\":\"34749896\",\"name\":\"T. Marks\"}],\"doi\":\"10.1109/WACV45572.2020.9093291\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e73fa178f729097428059af13b916275c7e92331\",\"title\":\"Spatio-Temporal Ranked-Attention Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e73fa178f729097428059af13b916275c7e92331\",\"venue\":\"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1394741222\",\"name\":\"Yuling Gui\"},{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"97522088\",\"name\":\"Ye Zhao\"}],\"doi\":\"10.1145/3347319.3356839\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"title\":\"Semantic Enhanced Encoder-Decoder Network (SEN) for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/9fc5e1793d9836d6c19cbd933d8b1fcc01dcc22f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1409200612\",\"name\":\"Muhannad A. R. I. Al-Omari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bea7ecf19bd9c823b2b4be05ed0a0ec81d0785b\",\"title\":\"Joint perceptual learning and natural language acquisition for autonomous robots\",\"url\":\"https://www.semanticscholar.org/paper/6bea7ecf19bd9c823b2b4be05ed0a0ec81d0785b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1604.02748\",\"authors\":[{\"authorId\":\"66508219\",\"name\":\"Y. Li\"},{\"authorId\":\"2317183\",\"name\":\"Yale Song\"},{\"authorId\":\"48749954\",\"name\":\"L. Cao\"},{\"authorId\":\"1739099\",\"name\":\"J. Tetreault\"},{\"authorId\":\"39420932\",\"name\":\"L. Goldberg\"},{\"authorId\":\"144633617\",\"name\":\"A. Jaimes\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/CVPR.2016.502\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"title\":\"TGIF: A New Dataset and Benchmark on Animated GIF Description\",\"url\":\"https://www.semanticscholar.org/paper/05f3f8f6f97db00bafa2efd2ac9aac570603c0c6\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1511.04590\",\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.5244/C.30.141\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"16aac81ae033f7295d82e5b679400d105170a3e1\",\"title\":\"Oracle Performance for Visual Captioning\",\"url\":\"https://www.semanticscholar.org/paper/16aac81ae033f7295d82e5b679400d105170a3e1\",\"venue\":\"BMVC\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144535340\",\"name\":\"Fei Yan\"},{\"authorId\":\"1712041\",\"name\":\"K. Mikolajczyk\"}],\"doi\":\"10.1007/978-3-319-16865-4_40\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c50caaa3983b4377f8bbf572e1c13fb49b3d601\",\"title\":\"Leveraging High Level Visual Information for Matching Images and Captions\",\"url\":\"https://www.semanticscholar.org/paper/5c50caaa3983b4377f8bbf572e1c13fb49b3d601\",\"venue\":\"ACCV\",\"year\":2014},{\"arxivId\":\"1604.03390\",\"authors\":[{\"authorId\":\"2853157\",\"name\":\"\\u00c1lvaro Peris\"},{\"authorId\":\"38950290\",\"name\":\"Marc Bola\\u00f1os\"},{\"authorId\":\"143601910\",\"name\":\"P. Radeva\"},{\"authorId\":\"1696761\",\"name\":\"F. Casacuberta\"}],\"doi\":\"10.1007/978-3-319-44781-0_1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"799271daced99bd88b3a3e15921d5e31d9ea8323\",\"title\":\"Video Description Using Bidirectional Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/799271daced99bd88b3a3e15921d5e31d9ea8323\",\"venue\":\"ICANN\",\"year\":2016},{\"arxivId\":\"1809.00204\",\"authors\":[{\"authorId\":\"31418786\",\"name\":\"S. Baier\"},{\"authorId\":\"10684484\",\"name\":\"Yunpu Ma\"},{\"authorId\":\"1700754\",\"name\":\"Volker Tresp\"}],\"doi\":\"10.1007/978-3-319-68288-4_4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9218f8704681a7f335812adf027062d67b371a15\",\"title\":\"Improving Visual Relationship Detection Using Semantic Modeling of Scene Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/9218f8704681a7f335812adf027062d67b371a15\",\"venue\":\"International Semantic Web Conference\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144841441\",\"name\":\"J. Xu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3123266.3123448\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"title\":\"Learning Multimodal Attention LSTM Networks for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ff172624dd0a3bd31ca925b73cd7295d596173e2\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"title\":\"Beyond Labels and Captions: Contextualizing Grounded Semantics for Explainable Visual Interpretation\",\"url\":\"https://www.semanticscholar.org/paper/267f3674d02ab3b53e0ac58e082380547b0bbf1c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2932516\",\"name\":\"J. Zhang\"}],\"doi\":\"10.7282/T3-KA2Q-B984\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b198f5cb09446433a8d3a181107f408d26d5a34\",\"title\":\"Scene graph parsing and its application in cross-modal reasoning tasks\",\"url\":\"https://www.semanticscholar.org/paper/7b198f5cb09446433a8d3a181107f408d26d5a34\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10798523\",\"name\":\"C. C. Park\"},{\"authorId\":\"47659605\",\"name\":\"Youngjin Kim\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/TPAMI.2017.2700381\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"17d4fee6b21c9277375d6cf0c9087828595009b6\",\"title\":\"Retrieval of Sentence Sequences for an Image Stream via Coherence Recurrent Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/17d4fee6b21c9277375d6cf0c9087828595009b6\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2786437\",\"name\":\"Linghui Li\"},{\"authorId\":\"46321465\",\"name\":\"Sheng Tang\"},{\"authorId\":\"1699819\",\"name\":\"Yongdong Zhang\"},{\"authorId\":\"4303531\",\"name\":\"Lixi Deng\"},{\"authorId\":\"144876831\",\"name\":\"Q. Tian\"}],\"doi\":\"10.1109/TMM.2017.2751140\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e0f1c89c4e099b14c4d77bd406be9f7b78d6f6d\",\"title\":\"GLA: Global\\u2013Local Attention for Image Description\",\"url\":\"https://www.semanticscholar.org/paper/2e0f1c89c4e099b14c4d77bd406be9f7b78d6f6d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2750075\",\"name\":\"Y. Pu\"},{\"authorId\":\"5477477\",\"name\":\"Martin Renqiang Min\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"title\":\"Recent work often develops a probabilistic model of the caption , conditioned on a video\",\"url\":\"https://www.semanticscholar.org/paper/dd44ea9ef28bb2d08d273fa71cc9c27cda90a244\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9434467\",\"name\":\"Suvarna L. Kattimani\"},{\"authorId\":\"73343355\",\"name\":\"Miss. Saba Parveen Bougdadi\"}],\"doi\":\"10.17148/IJARCCE.2017.64163\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d8b4461aa74729582e4b2a16683d752f3986e913\",\"title\":\"Analytical Review on Textual Queries Semantic Search based Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/d8b4461aa74729582e4b2a16683d752f3986e913\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948393\",\"name\":\"Linchao Zhu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"title\":\"Video representation learning with deep neural networks\",\"url\":\"https://www.semanticscholar.org/paper/04a82bb033a713ae88f2e3e2306822272c30ddd9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"74437294\",\"name\":\"Sk. Arif Ahmed\"},{\"authorId\":\"3320759\",\"name\":\"D. P. Dogra\"},{\"authorId\":\"32614479\",\"name\":\"S. Kar\"},{\"authorId\":\"40813600\",\"name\":\"P. Roy\"}],\"doi\":\"10.1007/978-981-10-7590-2_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"522b13ea02d6d62e54180bd13595eb0e40333d48\",\"title\":\"Natural Language Description of Surveillance Events\",\"url\":\"https://www.semanticscholar.org/paper/522b13ea02d6d62e54180bd13595eb0e40333d48\",\"venue\":\"ICITAM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3151799\",\"name\":\"Fudong Nian\"},{\"authorId\":\"47775167\",\"name\":\"Teng Li\"},{\"authorId\":\"47906413\",\"name\":\"Y. Wang\"},{\"authorId\":\"1730308\",\"name\":\"X. Wu\"},{\"authorId\":\"5796401\",\"name\":\"B. Ni\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1016/j.cviu.2017.06.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"94a86a758ae2608c00e9690e9951e805755bb1a1\",\"title\":\"Learning explicit video attributes from mid-level representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/94a86a758ae2608c00e9690e9951e805755bb1a1\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":\"1708.01641\",\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"39231399\",\"name\":\"O. Wang\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"1782755\",\"name\":\"Josef Sivic\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"145160921\",\"name\":\"Bryan C. Russell\"}],\"doi\":\"10.1109/ICCV.2017.618\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"title\":\"Localizing Moments in Video with Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/ee909ad489244016cf301bb7d7d8eeea423dbf35\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40061480\",\"name\":\"Z. Dong\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"50358603\",\"name\":\"S. Chen\"},{\"authorId\":\"1432791325\",\"name\":\"Wenxuan Liu\"},{\"authorId\":\"2000237078\",\"name\":\"Qi Cui\"},{\"authorId\":\"152283661\",\"name\":\"L. Zhong\"}],\"doi\":\"10.1007/978-3-030-55187-2_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78a1094e0968cf4e2b61c83100d971031597ae4b\",\"title\":\"Adaptive Attention Mechanism Based Semantic Compositional Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/78a1094e0968cf4e2b61c83100d971031597ae4b\",\"venue\":\"IntelliSys\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47740039\",\"name\":\"Jie Chen\"},{\"authorId\":\"145496509\",\"name\":\"Jie Shao\"},{\"authorId\":\"2838253\",\"name\":\"C. He\"}],\"doi\":\"10.1016/J.PATREC.2018.06.030\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c93fcf73554f2cb9e0bc21da82f9611ae631f55\",\"title\":\"Movie fill in the blank by joint learning from video and text with adaptive temporal attention\",\"url\":\"https://www.semanticscholar.org/paper/3c93fcf73554f2cb9e0bc21da82f9611ae631f55\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"}],\"doi\":\"10.22028/D291-26562\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ab1719f573a6c121d7d7da5053fe5f12de0182e7\",\"title\":\"Combining visual recognition and computational linguistics : linguistic knowledge for visual recognition and natural language descriptions of visual content\",\"url\":\"https://www.semanticscholar.org/paper/ab1719f573a6c121d7d7da5053fe5f12de0182e7\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"1931707\",\"name\":\"M. B\\u00e4uml\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"}],\"doi\":\"10.1109/CVPR.2015.7298792\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"45ca387a4080b6aee610783ed03d19bd1891503f\",\"title\":\"Book2Movie: Aligning video scenes with book chapters\",\"url\":\"https://www.semanticscholar.org/paper/45ca387a4080b6aee610783ed03d19bd1891503f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1705728\",\"name\":\"Hongliang Yu\"},{\"authorId\":\"1705434\",\"name\":\"S. Zhang\"},{\"authorId\":\"49933077\",\"name\":\"Louis-Philippe Morency\"}],\"doi\":\"10.18653/v1/D16-1185\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dfff89390ac3bd22e4153e0639d4b7382611fb0c\",\"title\":\"Unsupervised Text Recap Extraction for TV Series\",\"url\":\"https://www.semanticscholar.org/paper/dfff89390ac3bd22e4153e0639d4b7382611fb0c\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3492481\",\"name\":\"S. Cascianelli\"},{\"authorId\":\"2145503\",\"name\":\"G. Costante\"},{\"authorId\":\"2730000\",\"name\":\"T. A. Ciarfuglia\"},{\"authorId\":\"2634628\",\"name\":\"P. Valigi\"},{\"authorId\":\"2635260\",\"name\":\"M. L. Fravolini\"}],\"doi\":\"10.1109/LRA.2018.2793345\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"title\":\"Full-GRU Natural Language Video Description for Service Robotics Applications\",\"url\":\"https://www.semanticscholar.org/paper/7c550b86ff9ea8a58f4d9bddbbe34b340e84aff7\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2018},{\"arxivId\":\"1908.10072\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"67074535\",\"name\":\"W. Zhang\"},{\"authorId\":\"119897463\",\"name\":\"Wenhao Jiang\"},{\"authorId\":\"46584062\",\"name\":\"Junling Wang\"},{\"authorId\":\"46641690\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/ICCV.2019.00273\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"title\":\"Controllable Video Captioning With POS Sequence Guidance Based on Gated Fusion Network\",\"url\":\"https://www.semanticscholar.org/paper/5e4742e510a26cd55b19d3ba191b688e7fb8f8cf\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3367790\",\"name\":\"Seong Jae Hwang\"},{\"authorId\":\"3023295\",\"name\":\"S. Ravi\"},{\"authorId\":\"46641029\",\"name\":\"Zirui Tao\"},{\"authorId\":\"40147719\",\"name\":\"H. Kim\"},{\"authorId\":\"31604982\",\"name\":\"Maxwell D. Collins\"},{\"authorId\":\"144711711\",\"name\":\"V. Singh\"}],\"doi\":\"10.1109/CVPR.2018.00112\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b6d41795de1fd9a0da4227c83dc4dd038a229ec\",\"title\":\"Tensorize, Factorize and Regularize: Robust Visual Relationship Learning\",\"url\":\"https://www.semanticscholar.org/paper/1b6d41795de1fd9a0da4227c83dc4dd038a229ec\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1911.09989\",\"authors\":[{\"authorId\":\"1429191721\",\"name\":\"Menatallh Hammad\"},{\"authorId\":\"1429191719\",\"name\":\"May Hammad\"},{\"authorId\":\"31358369\",\"name\":\"M. ElShenawy\"}],\"doi\":\"10.1007/978-3-030-59830-3_21\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"title\":\"Characterizing the impact of using features extracted from pre-trained models on the quality of video captioning sequence-to-sequence models\",\"url\":\"https://www.semanticscholar.org/paper/86ac173f03a5dbfb1d64a84759aa920ed6c1aec1\",\"venue\":\"ICPRAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2964097\",\"name\":\"A. Ghosh\"},{\"authorId\":\"144549904\",\"name\":\"Yash Patel\"},{\"authorId\":\"3026786\",\"name\":\"M. Sukhwani\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.1007/978-3-319-46604-0_59\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd45cfc3751fa93d25766bddf66fc1d9c7f4f6d2\",\"title\":\"Dynamic Narratives for Heritage Tour\",\"url\":\"https://www.semanticscholar.org/paper/cd45cfc3751fa93d25766bddf66fc1d9c7f4f6d2\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9260404\",\"name\":\"Xiaotong Du\"},{\"authorId\":\"46685438\",\"name\":\"J. Yuan\"},{\"authorId\":\"1840183\",\"name\":\"H. Liu\"}],\"doi\":\"10.1007/978-3-030-00021-9_40\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"544ccc01b63be4a68fb3f2c318ee14b5fc036c37\",\"title\":\"Attention-Based Bidirectional Recurrent Neural Networks for Description Generation of Videos\",\"url\":\"https://www.semanticscholar.org/paper/544ccc01b63be4a68fb3f2c318ee14b5fc036c37\",\"venue\":\"ICCCS\",\"year\":2018},{\"arxivId\":\"1512.02902\",\"authors\":[{\"authorId\":\"2103464\",\"name\":\"Makarand Tapaswi\"},{\"authorId\":\"2214033\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1742325\",\"name\":\"R. Stiefelhagen\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":\"10.1109/CVPR.2016.501\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"title\":\"MovieQA: Understanding Stories in Movies through Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/1bdd75a37f7c601f01e9d31c2551fa9f2067ffd7\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2301765\",\"name\":\"Tsung-Wei Ke\"},{\"authorId\":\"3172276\",\"name\":\"Che-Wei Lin\"},{\"authorId\":\"1805102\",\"name\":\"Tyng-Luh Liu\"},{\"authorId\":\"14489533\",\"name\":\"D. Geiger\"}],\"doi\":\"10.1007/978-3-319-54190-7_8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1df74a5047e953766fa07dec356bba285c605a1\",\"title\":\"Variational Convolutional Networks for Human-Centric Annotations\",\"url\":\"https://www.semanticscholar.org/paper/d1df74a5047e953766fa07dec356bba285c605a1\",\"venue\":\"ACCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1963421\",\"name\":\"Stanislaw Antol\"},{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"},{\"authorId\":\"8553015\",\"name\":\"Jiasen Lu\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"51472503\",\"name\":\"Dhruv Batra\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/ICCV.2015.279\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"784da2a7b53a16d2243f747e14946cc5e3476af0\",\"title\":\"VQA: Visual Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/784da2a7b53a16d2243f747e14946cc5e3476af0\",\"venue\":\"ICCV\",\"year\":2015},{\"arxivId\":\"1505.00487\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2015.515\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"title\":\"Sequence to Sequence -- Video to Text\",\"url\":\"https://www.semanticscholar.org/paper/e58a110fa1e4ddf247d5c614d117d64bfbe135c4\",\"venue\":\"2015 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2015},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"2003.07758\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":\"10.1109/CVPRW50498.2020.00487\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"23e36087637e9d74815eba07990c38c02fecc966\",\"title\":\"Multi-modal Dense Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9260404\",\"name\":\"Xiaotong Du\"},{\"authorId\":\"46685438\",\"name\":\"J. Yuan\"},{\"authorId\":\"47652550\",\"name\":\"L. Hu\"},{\"authorId\":\"122907636\",\"name\":\"Yuke Dai\"}],\"doi\":\"10.1007/s00371-018-1591-x\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4170882122b559fc39ab3eafd66babe2429ba858\",\"title\":\"Description generation of open-domain videos incorporating multimodal features and bidirectional encoder\",\"url\":\"https://www.semanticscholar.org/paper/4170882122b559fc39ab3eafd66babe2429ba858\",\"venue\":\"The Visual Computer\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21617362\",\"name\":\"Hirokuni Maeta\"},{\"authorId\":\"46510146\",\"name\":\"T. Sasada\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"}],\"doi\":\"10.18653/v1/W15-2206\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"00d47ed91d423a9ba5f43b9d034d5d3a77c74a28\",\"title\":\"A Framework for Procedural Text Understanding\",\"url\":\"https://www.semanticscholar.org/paper/00d47ed91d423a9ba5f43b9d034d5d3a77c74a28\",\"venue\":\"IWPT\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807606\",\"name\":\"D. Lin\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"144332826\",\"name\":\"Chen Kong\"},{\"authorId\":\"2422559\",\"name\":\"R. Urtasun\"}],\"doi\":\"10.1109/CVPR.2014.340\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7afd833f484c8032e7fdc5f53188d2ebb0fb9934\",\"title\":\"Visual Semantic Search: Retrieving Videos via Complex Textual Queries\",\"url\":\"https://www.semanticscholar.org/paper/7afd833f484c8032e7fdc5f53188d2ebb0fb9934\",\"venue\":\"2014 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"},{\"authorId\":\"29072828\",\"name\":\"Seungwhan Moon\"},{\"authorId\":\"144398147\",\"name\":\"L. Sigal\"}],\"doi\":\"10.1109/CVPR.2015.7298927\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8460ce5b162e2bc12433036060b64e0b2c457c0f\",\"title\":\"Joint photo stream and blog post summarization and exploration\",\"url\":\"https://www.semanticscholar.org/paper/8460ce5b162e2bc12433036060b64e0b2c457c0f\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"title\":\"Trainable performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/e97f10c2a4d7edac33597692e6dc243bd86adf10\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48270284\",\"name\":\"Yaohui Zhu\"},{\"authorId\":\"1696610\",\"name\":\"S. Jiang\"},{\"authorId\":\"37670557\",\"name\":\"Xiangyang Li\"}],\"doi\":\"10.1109/ICME.2017.8019448\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f7127b9ae86d7fac305c6b824801e455e82f511\",\"title\":\"Visual relationship detection with object spatial distribution\",\"url\":\"https://www.semanticscholar.org/paper/1f7127b9ae86d7fac305c6b824801e455e82f511\",\"venue\":\"2017 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7408951\",\"name\":\"Jeff Donahue\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1f58798db460996501f224fff6cceada08f59f9\",\"title\":\"Transferrable Representations for Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d1f58798db460996501f224fff6cceada08f59f9\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5387396\",\"name\":\"Huidong Li\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"},{\"authorId\":\"3000498\",\"name\":\"Lejian Liao\"},{\"authorId\":\"151479762\",\"name\":\"Cuimei Peng\"}],\"doi\":\"10.1109/ICME.2019.00228\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1567fff9f411af320f678c66b812d2c963151678\",\"title\":\"REVnet: Bring Reviewing Into Video Captioning for a Better Description\",\"url\":\"https://www.semanticscholar.org/paper/1567fff9f411af320f678c66b812d2c963151678\",\"venue\":\"2019 IEEE International Conference on Multimedia and Expo (ICME)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2296971\",\"name\":\"Iftekhar Naim\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c6608fdd919f2bc4f8d7412bab287527dcbcf505\",\"title\":\"Unsupervised Alignment of Natural Language with Video\",\"url\":\"https://www.semanticscholar.org/paper/c6608fdd919f2bc4f8d7412bab287527dcbcf505\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2169614\",\"name\":\"Yashaswi Verma\"},{\"authorId\":\"1694502\",\"name\":\"C. Jawahar\"}],\"doi\":\"10.5244/C.28.97\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef3f78ebf6a36a1d49b8a8a6c8bddf9f906285b8\",\"title\":\"Im2Text and Text2Im: Associating Images and Texts for Cross-Modal Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/ef3f78ebf6a36a1d49b8a8a6c8bddf9f906285b8\",\"venue\":\"BMVC\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":\"40013375\",\"name\":\"Y. Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1145/3265845.3265851\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"title\":\"Sports Video Captioning by Attentive Motion Representation based Hierarchical Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/b1f62af29f1d13133b543e0b65ccf1a42ded7f25\",\"venue\":\"MMSports@MM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2801949\",\"name\":\"Aishwarya Agrawal\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42afa9a0166eeb45cb2e9b37e0a8eec482f78fe0\",\"title\":\"Visual Question Answering and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/42afa9a0166eeb45cb2e9b37e0a8eec482f78fe0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.10930\",\"authors\":[{\"authorId\":\"51174755\",\"name\":\"X. Zhang\"},{\"authorId\":\"47535646\",\"name\":\"C. Liu\"},{\"authorId\":\"32617816\",\"name\":\"Faliang Chang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abd8dd9da90b5b08a8ea67d937b8688101fa0f86\",\"title\":\"Guidance Module Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/abd8dd9da90b5b08a8ea67d937b8688101fa0f86\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144620591\",\"name\":\"X. Wu\"},{\"authorId\":\"144958813\",\"name\":\"Guanbin Li\"},{\"authorId\":\"2826839\",\"name\":\"Qingxing Cao\"},{\"authorId\":\"2523380\",\"name\":\"Qingge Ji\"},{\"authorId\":\"1737218\",\"name\":\"L. Lin\"}],\"doi\":\"10.1109/CVPR.2018.00714\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"title\":\"Interpretable Video Captioning via Trajectory Structured Localization\",\"url\":\"https://www.semanticscholar.org/paper/f66a2c5225551837b8894f94ae9feca0e406c9c1\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1908.00943\",\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"49745735\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"199c2a410cf4430841907e27d5b7026efd95a6ec\",\"title\":\"Prediction and Description of Near-Future Activities in Video.\",\"url\":\"https://www.semanticscholar.org/paper/199c2a410cf4430841907e27d5b7026efd95a6ec\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1505.01861\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.497\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"title\":\"Jointly Modeling Embedding and Translation to Bridge Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/68478207cf3e4fc44bf1602abe82c7ac7f288872\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7142317\",\"name\":\"Hideki Asoh\"},{\"authorId\":\"3236658\",\"name\":\"I. Kobayashi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f7c3a99923549601c81cd5e9659ca01e8a42f47\",\"title\":\"Zero-Shot Learning of Language Models for Describing Human Actions Based on Semantic Compositionality of Actions\",\"url\":\"https://www.semanticscholar.org/paper/7f7c3a99923549601c81cd5e9659ca01e8a42f47\",\"venue\":\"PACLIC\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49285626\",\"name\":\"An-An Liu\"},{\"authorId\":\"145857599\",\"name\":\"N. Xu\"},{\"authorId\":\"3026404\",\"name\":\"Yongkang Wong\"},{\"authorId\":\"47786844\",\"name\":\"J. Li\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"},{\"authorId\":\"1744045\",\"name\":\"M. Kankanhalli\"}],\"doi\":\"10.1016/j.cviu.2017.04.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"title\":\"Hierarchical & multimodal video captioning: Discovering and transferring multimodal knowledge for vision to language\",\"url\":\"https://www.semanticscholar.org/paper/96eb165fbc83dd0abbaf65eaa75e020e289e4a66\",\"venue\":\"Comput. Vis. Image Underst.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8047613\",\"name\":\"Tahmida Mahmud\"},{\"authorId\":\"15702255\",\"name\":\"M. Billah\"},{\"authorId\":\"26559284\",\"name\":\"M. Hasan\"},{\"authorId\":\"1404727582\",\"name\":\"A. Roy-Chowdhury\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"title\":\"Captioning Near-Future Activity Sequences\",\"url\":\"https://www.semanticscholar.org/paper/dc2aa32e5ee30ba71bd6fb708cd70bdea0cedbe8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"3009919\",\"name\":\"Shizhe Chen\"},{\"authorId\":\"49252656\",\"name\":\"Jia Chen\"},{\"authorId\":\"7661726\",\"name\":\"A. Hauptmann\"}],\"doi\":\"10.1145/3123266.3127901\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"title\":\"Knowing Yourself: Improving Video Caption via In-depth Recap\",\"url\":\"https://www.semanticscholar.org/paper/3609c92bbcad4eaa6e239112fc2cadbf87bb3c33\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40387883\",\"name\":\"Florian Patzelt\"},{\"authorId\":\"2112168\",\"name\":\"Robert Haschke\"},{\"authorId\":\"30258243\",\"name\":\"Helge J. Ritter\"}],\"doi\":\"10.1007/978-3-319-44781-0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf9ce9a9114a29b69e48acb1193d21dc6267f0d3\",\"title\":\"Artificial Neural Networks and Machine Learning \\u2013 ICANN 2016\",\"url\":\"https://www.semanticscholar.org/paper/bf9ce9a9114a29b69e48acb1193d21dc6267f0d3\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2016},{\"arxivId\":\"1708.03725\",\"authors\":[{\"authorId\":\"24057502\",\"name\":\"Sathyanarayanan N. Aakur\"},{\"authorId\":\"27398350\",\"name\":\"F. Souza\"},{\"authorId\":\"145306925\",\"name\":\"Sudeep Sarkar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc7a3573a464bca2cdca71f6f32e798464b85ee6\",\"title\":\"Exploiting Semantic Contextualization for Interpretation of Human Activity in Videos\",\"url\":\"https://www.semanticscholar.org/paper/bc7a3573a464bca2cdca71f6f32e798464b85ee6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1906.04375\",\"authors\":[{\"authorId\":\"153389599\",\"name\":\"Junchao Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.1109/CVPR.2019.00852\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"title\":\"Object-Aware Aggregation With Bidirectional Temporal Graph for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/6c9bd4d49d7bd70e1610c0f28fbd78ff97d0d0b5\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1581863540\",\"name\":\"Aidean Sharghi Karganroodi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"550b08b659d3b8e7f45bdc09602af2184791d082\",\"title\":\"Visual-Textual Video Synopsis Generation\",\"url\":\"https://www.semanticscholar.org/paper/550b08b659d3b8e7f45bdc09602af2184791d082\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145742542\",\"name\":\"W. Li\"},{\"authorId\":\"144536247\",\"name\":\"Weizhi Nie\"},{\"authorId\":\"2788104\",\"name\":\"Yuting Su\"}],\"doi\":\"10.1109/ACCESS.2018.2863943\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd22e6532211f679ba6057d15a801ba448b9915c\",\"title\":\"Human Action Recognition Based on Selected Spatio-Temporal Features via Bidirectional LSTM\",\"url\":\"https://www.semanticscholar.org/paper/cd22e6532211f679ba6057d15a801ba448b9915c\",\"venue\":\"IEEE Access\",\"year\":2018},{\"arxivId\":\"1511.05914\",\"authors\":[{\"authorId\":\"2089428\",\"name\":\"D. P. Barrett\"},{\"authorId\":\"144418348\",\"name\":\"R. Xu\"},{\"authorId\":\"2910174\",\"name\":\"Haonan Yu\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"}],\"doi\":\"10.1007/s00138-016-0768-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6a348493e0a3bd6b35cf02de9e71da675062841d\",\"title\":\"Collecting and annotating the large continuous action dataset\",\"url\":\"https://www.semanticscholar.org/paper/6a348493e0a3bd6b35cf02de9e71da675062841d\",\"venue\":\"Machine Vision and Applications\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1145/3239576.3239580\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"title\":\"Video Captioning using Hierarchical Multi-Attention Model\",\"url\":\"https://www.semanticscholar.org/paper/0f47d9d2d64c45246ae7882d81398e6274f7c8e6\",\"venue\":\"ICAIP '18\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46510146\",\"name\":\"T. Sasada\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"},{\"authorId\":\"1717105\",\"name\":\"Tatsuya Kawahara\"},{\"authorId\":\"2936312\",\"name\":\"Y. Yamakata\"}],\"doi\":\"10.1007/978-981-10-0515-2_11\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5739d6981aab4700cf16ef2430b5b4a0af7644c5\",\"title\":\"Named Entity Recognizer Trainable from Partially Annotated Data\",\"url\":\"https://www.semanticscholar.org/paper/5739d6981aab4700cf16ef2430b5b4a0af7644c5\",\"venue\":\"PACLING\",\"year\":2015},{\"arxivId\":\"2101.00359\",\"authors\":[{\"authorId\":\"151478904\",\"name\":\"Mingjian Zhu\"},{\"authorId\":\"153773171\",\"name\":\"Chenrui Duan\"},{\"authorId\":\"1409820051\",\"name\":\"Changbin Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"title\":\"Video Captioning in Compressed Video\",\"url\":\"https://www.semanticscholar.org/paper/93eb79ac45d6ff7632a57e782bf306276cf403fa\",\"venue\":\"\",\"year\":2021},{\"arxivId\":\"1905.11240\",\"authors\":[{\"authorId\":\"27629426\",\"name\":\"Shang-Yu Su\"},{\"authorId\":\"1725643\",\"name\":\"Yun-Nung (Vivian) Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"75187014e7efd823eecfa833217ebb3ea7187107\",\"title\":\"Bridging Dialogue Generation and Facial Expression Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/75187014e7efd823eecfa833217ebb3ea7187107\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1608.02367\",\"authors\":[{\"authorId\":\"3186326\",\"name\":\"Mayu Otani\"},{\"authorId\":\"1789677\",\"name\":\"Yuta Nakashima\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"},{\"authorId\":\"3111194\",\"name\":\"J. Heikkil\\u00e4\"},{\"authorId\":\"1771769\",\"name\":\"N. Yokoya\"}],\"doi\":\"10.1007/978-3-319-46604-0_46\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa76f655c2ad655080593a191c4b479ab9f18117\",\"title\":\"Learning Joint Representations of Videos and Sentences with Web Image Search\",\"url\":\"https://www.semanticscholar.org/paper/aa76f655c2ad655080593a191c4b479ab9f18117\",\"venue\":\"ECCV Workshops\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34876449\",\"name\":\"E. Aksoy\"},{\"authorId\":\"145942066\",\"name\":\"E. Ovchinnikova\"},{\"authorId\":\"9738883\",\"name\":\"Adil Orhan\"},{\"authorId\":\"7607499\",\"name\":\"Yezhou Yang\"},{\"authorId\":\"1722677\",\"name\":\"T. Asfour\"}],\"doi\":\"10.1109/LRA.2017.2669363\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"36894bf4d64c247baa96b81f0b83975e246c07e2\",\"title\":\"Unsupervised Linking of Visual Features to Textual Descriptions in Long Manipulation Activities\",\"url\":\"https://www.semanticscholar.org/paper/36894bf4d64c247baa96b81f0b83975e246c07e2\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2088976\",\"name\":\"Haoyang Tang\"},{\"authorId\":\"49604799\",\"name\":\"Meng Qian\"},{\"authorId\":\"10768609\",\"name\":\"Ziwei Sun\"},{\"authorId\":\"153409174\",\"name\":\"Cong Song\"}],\"doi\":\"10.1007/978-3-030-03766-6_67\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"873a9c8f60cc490dc0b7f41db68a14c1b411eea4\",\"title\":\"Visual Question Answer System Based on Bidirectional Recurrent Networks\",\"url\":\"https://www.semanticscholar.org/paper/873a9c8f60cc490dc0b7f41db68a14c1b411eea4\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886528\",\"name\":\"Guolong Wang\"},{\"authorId\":\"145458349\",\"name\":\"Z. Qin\"},{\"authorId\":\"2168639\",\"name\":\"Kaiping Xu\"},{\"authorId\":\"145489794\",\"name\":\"K. Huang\"},{\"authorId\":\"19204816\",\"name\":\"Shuxiong Ye\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"title\":\"Bridge Video and Text with Cascade Syntactic Structure\",\"url\":\"https://www.semanticscholar.org/paper/b10427999fbde2d90e3541c477e2f6ba4c8f08cc\",\"venue\":\"COLING\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48009996\",\"name\":\"Yitian Yuan\"},{\"authorId\":\"152309767\",\"name\":\"L. Ma\"},{\"authorId\":\"48093314\",\"name\":\"Jing-Wen Wang\"},{\"authorId\":\"40281988\",\"name\":\"Wenwu Zhu\"}],\"doi\":\"10.1145/3394171.3413908\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"title\":\"Controllable Video Captioning with an Exemplar Sentence\",\"url\":\"https://www.semanticscholar.org/paper/40b3dc37f80cb9981e7b77a8e898aa87e2e408e7\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"1611.07810\",\"authors\":[{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"}],\"doi\":\"10.1109/CVPR.2017.778\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"title\":\"A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering\",\"url\":\"https://www.semanticscholar.org/paper/120ae4cbdcfeaf2604983b3bc3d9a8e1ec37e376\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1411.4389\",\"authors\":[{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/TPAMI.2016.2599174\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f01fc808592ea7c473a69a6e7484040a435f36d9\",\"title\":\"Long-term recurrent convolutional networks for visual recognition and description\",\"url\":\"https://www.semanticscholar.org/paper/f01fc808592ea7c473a69a6e7484040a435f36d9\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1604.01729\",\"authors\":[{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.18653/v1/D16-1204\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"title\":\"Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text\",\"url\":\"https://www.semanticscholar.org/paper/d1ffd519ff274517ec6fd014ae67af0d0c68a969\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":\"2011.14752\",\"authors\":[{\"authorId\":\"47264639\",\"name\":\"Ashutosh Kumar Singh\"},{\"authorId\":\"2305086\",\"name\":\"Thoudam Doren Singh\"},{\"authorId\":\"1722399\",\"name\":\"Sivaji Bandyopadhyay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"title\":\"A Comprehensive Review on Recent Methods and Challenges of Video Description\",\"url\":\"https://www.semanticscholar.org/paper/baf5478fbf0a2f0ca2af287a35f3f5469afcd936\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8433849\",\"name\":\"Mengshi Qi\"},{\"authorId\":null,\"name\":\"Yunhong Wang\"},{\"authorId\":\"3079475\",\"name\":\"Annan Li\"},{\"authorId\":\"33642939\",\"name\":\"Jiebo Luo\"}],\"doi\":\"10.1109/tcsvt.2019.2921655\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"title\":\"Sports Video Captioning via Attentive Motion Representation and Group Relationship Modeling\",\"url\":\"https://www.semanticscholar.org/paper/cd3beba63f991507ee1e8fb5298eb83c1890caa7\",\"venue\":\"IEEE Transactions on Circuits and Systems for Video Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1908467\",\"name\":\"Hirotaka Kameko\"},{\"authorId\":\"144873535\",\"name\":\"S. Mori\"},{\"authorId\":\"143946906\",\"name\":\"Yoshimasa Tsuruoka\"}],\"doi\":\"10.18653/v1/D15-1277\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d311c41e230b9ac32caa0c3cf316e849685394c7\",\"title\":\"Can Symbol Grounding Improve Low-Level NLP? Word Segmentation as a Case Study\",\"url\":\"https://www.semanticscholar.org/paper/d311c41e230b9ac32caa0c3cf316e849685394c7\",\"venue\":\"EMNLP\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3095774\",\"name\":\"Dan Oneata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e46f6a5fcf304a9e240258eb15e6755226ddff2\",\"title\":\"Robust and efficient models for action recognition and localization\",\"url\":\"https://www.semanticscholar.org/paper/4e46f6a5fcf304a9e240258eb15e6755226ddff2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1911.01857\",\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2020.03.001\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"583222b6a573ad698207a0ebabb06685c4517558\",\"title\":\"Video Captioning with Text-based Dynamic Attention and Step-by-Step Learning\",\"url\":\"https://www.semanticscholar.org/paper/583222b6a573ad698207a0ebabb06685c4517558\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144957598\",\"name\":\"K. Li\"},{\"authorId\":\"145308148\",\"name\":\"L. Bai\"}],\"doi\":\"10.1109/IJCNN.2015.7280652\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13426f896836c6ef61592c68aa192079587f9a13\",\"title\":\"Generating image description by modeling spatial context of an image\",\"url\":\"https://www.semanticscholar.org/paper/13426f896836c6ef61592c68aa192079587f9a13\",\"venue\":\"2015 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1381902419\",\"name\":\"Mihir Prabhudesai\"},{\"authorId\":\"1693704\",\"name\":\"H. F. Tung\"},{\"authorId\":\"40604609\",\"name\":\"Syed Ashar Javed\"},{\"authorId\":\"51039185\",\"name\":\"Maximilian Sieb\"},{\"authorId\":\"34939798\",\"name\":\"Adam W. Harley\"},{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"}],\"doi\":\"10.1109/CVPR42600.2020.00229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"77e98ae704a70478fb1eea7625f54d103e427a58\",\"title\":\"Embodied Language Grounding With 3D Visual Feature Representations\",\"url\":\"https://www.semanticscholar.org/paper/77e98ae704a70478fb1eea7625f54d103e427a58\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2643775\",\"name\":\"Zhongyu Liu\"},{\"authorId\":\"153489843\",\"name\":\"T. Chen\"},{\"authorId\":\"3091544\",\"name\":\"Enjie Ding\"},{\"authorId\":\"46398350\",\"name\":\"Y. Liu\"},{\"authorId\":\"145909567\",\"name\":\"Wanli Yu\"}],\"doi\":\"10.1109/ACCESS.2020.3010872\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"title\":\"Attention-Based Convolutional LSTM for Describing Video\",\"url\":\"https://www.semanticscholar.org/paper/d6d66e02be2972957c2579cdc4dd46b5b0a5369d\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66687366\",\"name\":\"Vandana D. Edke\"},{\"authorId\":\"19940880\",\"name\":\"Ramesh Mahadev Kagalkar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea0b18709ebaf7a78aa7aa5a9ffdb9d34f9e1c54\",\"title\":\"Review Paper on Video Content Analysis into Text Description\",\"url\":\"https://www.semanticscholar.org/paper/ea0b18709ebaf7a78aa7aa5a9ffdb9d34f9e1c54\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46875241\",\"name\":\"Rui Shi Liang\"},{\"authorId\":\"1734697\",\"name\":\"Qingxin Zhu\"}],\"doi\":\"10.2991/ICMEIT-16.2016.74\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"72c3bc0e664ddefbd1e67380c23e6199c78cd426\",\"title\":\"Multi Semantic Feature Fusion Framework for Video Segmentation and Description\",\"url\":\"https://www.semanticscholar.org/paper/72c3bc0e664ddefbd1e67380c23e6199c78cd426\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1611.02261\",\"authors\":[{\"authorId\":\"2915023\",\"name\":\"Rasool Fakoor\"},{\"authorId\":\"40360972\",\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"1738740\",\"name\":\"S. B. Kang\"},{\"authorId\":\"143967473\",\"name\":\"Pushmeet Kohli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"title\":\"Memory-augmented Attention Modelling for Videos\",\"url\":\"https://www.semanticscholar.org/paper/249c2e960edb6b3b1f2922a3ea70fad4bae057ec\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"115674646\",\"name\":\"Sohan Chowdhury\"},{\"authorId\":\"115639927\",\"name\":\"Tanbirul Hashan\"},{\"authorId\":\"116900283\",\"name\":\"Afif Abdur Rahman\"},{\"authorId\":\"144894381\",\"name\":\"A. F. M. Saifuddin Saif\"}],\"doi\":\"10.5815/IJMSC.2019.02.02\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"db4f7246d698c012aaf77b4986d3be6b38a85c9c\",\"title\":\"Category Specific Prediction Modules for Visual Relation Recognition\",\"url\":\"https://www.semanticscholar.org/paper/db4f7246d698c012aaf77b4986d3be6b38a85c9c\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2767360\",\"name\":\"Guangyi Lv\"},{\"authorId\":\"50383766\",\"name\":\"Tong Xu\"},{\"authorId\":\"50384136\",\"name\":\"Qi Liu\"},{\"authorId\":\"144378760\",\"name\":\"E. Chen\"},{\"authorId\":\"50101712\",\"name\":\"W. He\"},{\"authorId\":\"92701189\",\"name\":\"Mingxiao An\"},{\"authorId\":\"3111271\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1007/978-3-030-16142-2_32\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f54a575a7a66e6ed6af7df2559b2e3d5b89bc314\",\"title\":\"Gossiping the Videos: An Embedding-Based Generative Adversarial Framework for Time-Sync Comments Generation\",\"url\":\"https://www.semanticscholar.org/paper/f54a575a7a66e6ed6af7df2559b2e3d5b89bc314\",\"venue\":\"PAKDD\",\"year\":2019},{\"arxivId\":\"1712.00796\",\"authors\":[{\"authorId\":\"7311172\",\"name\":\"Giorgos Bouritsas\"},{\"authorId\":\"2539459\",\"name\":\"Petros Koutras\"},{\"authorId\":\"2641229\",\"name\":\"A. Zlatintsi\"},{\"authorId\":\"1750686\",\"name\":\"P. Maragos\"}],\"doi\":\"10.1109/CVPR.2018.00516\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e1379e2f0509af074808c1e464ef78fb6abd5ba\",\"title\":\"Multimodal Visual Concept Learning with Weakly Supervised Techniques\",\"url\":\"https://www.semanticscholar.org/paper/9e1379e2f0509af074808c1e464ef78fb6abd5ba\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9546964\",\"name\":\"S. Chang\"}],\"doi\":\"10.1145/3122865\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"title\":\"Frontiers of Multimedia Research\",\"url\":\"https://www.semanticscholar.org/paper/8e87853672791ed5254a1cfc4b7582e7a41a89d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151487400\",\"name\":\"Chu-yi Li\"},{\"authorId\":\"9319341\",\"name\":\"Wei-yu Yu\"}],\"doi\":\"10.1117/12.2514651\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ddbc1542476237b6ace7b871e34269e790d35bad\",\"title\":\"Spatial-temporal attention in Bi-LSTM networks based on multiple features for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/ddbc1542476237b6ace7b871e34269e790d35bad\",\"venue\":\"Other Conferences\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1576151335\",\"name\":\"Trung Ky Nguyen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"36a8af51e773b19dead1ff121acb849532423c62\",\"title\":\"Story Generation from Smart Phone Data : A script approach. (G\\u00e9n\\u00e9ration d'histoires \\u00e0 partir de donn\\u00e9es de t\\u00e9l\\u00e9phone intelligentes : une approche de script)\",\"url\":\"https://www.semanticscholar.org/paper/36a8af51e773b19dead1ff121acb849532423c62\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"title\":\"Video Description Generation Incorporating Spatio-Temporal Features and a Soft-Attention Mechanism\",\"url\":\"https://www.semanticscholar.org/paper/db7228525912e197fe9b9dfffcb4175fbbc1a422\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1770771\",\"name\":\"N. Alharbi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bebb4393952052fec2c3b8cfb1adbf167b57737b\",\"title\":\"Describing human activities in video streams\",\"url\":\"https://www.semanticscholar.org/paper/bebb4393952052fec2c3b8cfb1adbf167b57737b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1755872\",\"name\":\"Jufeng Yang\"},{\"authorId\":\"40034021\",\"name\":\"Y. Sun\"},{\"authorId\":\"145819866\",\"name\":\"J. Liang\"},{\"authorId\":\"144651371\",\"name\":\"B. Ren\"},{\"authorId\":\"1696527\",\"name\":\"S. Lai\"}],\"doi\":\"10.1016/j.neucom.2018.03.078\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"adcd5dd4e851ffc0e3e23a4620095ef9b5ca2837\",\"title\":\"Image captioning by incorporating affective concepts learned from both visual and textual components\",\"url\":\"https://www.semanticscholar.org/paper/adcd5dd4e851ffc0e3e23a4620095ef9b5ca2837\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7146976\",\"name\":\"Shun-Po Chuang\"},{\"authorId\":\"35508795\",\"name\":\"Chia-Hung Wan\"},{\"authorId\":\"12257085\",\"name\":\"Pang-Chi Huang\"},{\"authorId\":\"3596543\",\"name\":\"Chi-Yu Yang\"},{\"authorId\":\"1706104\",\"name\":\"Hung-yi Lee\"}],\"doi\":\"10.1109/ASRU.2017.8268961\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"title\":\"Seeing and hearing too: Audio representation for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/dbc7526576ef2946dad04908f0d3a13532fb2c4e\",\"venue\":\"2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"}],\"doi\":\"10.1145/2911996.2912043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"title\":\"Video Description Generation using Audio and Visual Cues\",\"url\":\"https://www.semanticscholar.org/paper/54f4dba1875eb7fb32d21bea88df7c4a9412eccb\",\"venue\":\"ICMR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49268056\",\"name\":\"J. Hu\"},{\"authorId\":\"2287686\",\"name\":\"Xiangbo Shu\"}],\"doi\":\"10.1145/3339363.3339389\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8e21836e80d6641886a5a0a1f66fc7e54f6db76\",\"title\":\"Semantic BI-Embedded GRU for Fill-in-the-Blank Image Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/f8e21836e80d6641886a5a0a1f66fc7e54f6db76\",\"venue\":\"CSSE 2019\",\"year\":2019},{\"arxivId\":\"1803.11438\",\"authors\":[{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145698310\",\"name\":\"Lin Ma\"},{\"authorId\":\"37378985\",\"name\":\"Wei Zhang\"},{\"authorId\":\"46641573\",\"name\":\"W. Liu\"}],\"doi\":\"10.1109/CVPR.2018.00795\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"title\":\"Reconstruction Network for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/ba7405516e1408f0ee6e0d0a8c6d511ce33c0551\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1707.06029\",\"authors\":[{\"authorId\":\"7877122\",\"name\":\"Youngjae Yu\"},{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"4945045\",\"name\":\"Yeonhwa Kim\"},{\"authorId\":\"143912065\",\"name\":\"Kyung Yoo\"},{\"authorId\":\"2135453\",\"name\":\"S. Lee\"},{\"authorId\":\"1743920\",\"name\":\"Gunhee Kim\"}],\"doi\":\"10.1109/CVPR.2017.648\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"title\":\"Supervising Neural Attention Models for Video Captioning by Human Gaze Data\",\"url\":\"https://www.semanticscholar.org/paper/1a7f16a3b5acaf4aaf9581e2a5a15867e883a95d\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145040726\",\"name\":\"R. Bernardi\"},{\"authorId\":\"2588033\",\"name\":\"Ruken Cakici\"},{\"authorId\":\"50369944\",\"name\":\"Desmond Elliott\"},{\"authorId\":\"14364286\",\"name\":\"Aykut Erdem\"},{\"authorId\":\"152330322\",\"name\":\"Erkut Erdem\"},{\"authorId\":\"1398643531\",\"name\":\"N. Ikizler-Cinbis\"},{\"authorId\":\"1393020635\",\"name\":\"F. Keller\"},{\"authorId\":\"35347012\",\"name\":\"A. Muscat\"},{\"authorId\":\"2022124\",\"name\":\"Barbara Plank\"}],\"doi\":\"10.1613/jair.4900\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c162d791b63682d928c09578bd38c3dd61f78c8c\",\"title\":\"Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures\",\"url\":\"https://www.semanticscholar.org/paper/c162d791b63682d928c09578bd38c3dd61f78c8c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2016},{\"arxivId\":\"1906.01452\",\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"40892631\",\"name\":\"Bairui Wang\"},{\"authorId\":\"145499468\",\"name\":\"L. Ma\"},{\"authorId\":\"40474871\",\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/TPAMI.2019.2920899\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"83a3fe38887880bccc15daa740d8d5041f826d91\",\"title\":\"Reconstruct and Represent Video Contents for Captioning via Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/83a3fe38887880bccc15daa740d8d5041f826d91\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"2005.08271\",\"authors\":[{\"authorId\":\"47698311\",\"name\":\"Vladimir Iashin\"},{\"authorId\":\"2827962\",\"name\":\"Esa Rahtu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d87489d2facf197caafd24d0796523d55d47fb62\",\"title\":\"A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer\",\"url\":\"https://www.semanticscholar.org/paper/d87489d2facf197caafd24d0796523d55d47fb62\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39970828\",\"name\":\"J. Jacob\"},{\"authorId\":\"2457110\",\"name\":\"M. S. Elayidom\"},{\"authorId\":\"3109670\",\"name\":\"V. P. Devassia\"}],\"doi\":\"10.1007/978-3-030-51859-2_52\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be8c2dd6b91217e4eb5ac1e388c951bda8a343f6\",\"title\":\"An Innovative Approach for Aerial Video Surveillance Using Video Content Analysis and Indexing\",\"url\":\"https://www.semanticscholar.org/paper/be8c2dd6b91217e4eb5ac1e388c951bda8a343f6\",\"venue\":\"ICIP 2020\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9728275\",\"name\":\"Huanhou Xiao\"},{\"authorId\":\"153173208\",\"name\":\"J. Xu\"},{\"authorId\":\"34875762\",\"name\":\"J. Shi\"}],\"doi\":\"10.1016/j.patrec.2019.11.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"title\":\"Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into LSTM-based model\",\"url\":\"https://www.semanticscholar.org/paper/2eaffb8e5f6f8d11d0c5c012980b25829667f6d1\",\"venue\":\"Pattern Recognit. Lett.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"2319608\",\"name\":\"Armand Joulin\"},{\"authorId\":\"40085065\",\"name\":\"Percy Liang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-10590-1_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"title\":\"Linking People in Videos with \\\"Their\\\" Names Using Coreference Resolution\",\"url\":\"https://www.semanticscholar.org/paper/a0a083c7bb23db507a40a736953b1cca5a33b16d\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50050350\",\"name\":\"Y. Kang\"},{\"authorId\":\"144871286\",\"name\":\"Zhao Cai\"},{\"authorId\":\"33593082\",\"name\":\"C. Tan\"},{\"authorId\":\"144158674\",\"name\":\"Qian Huang\"},{\"authorId\":\"2748728\",\"name\":\"Hefu Liu\"}],\"doi\":\"10.1080/23270012.2020.1756939\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"db528269ef800727245c0fcb35b692d29c1ccdc9\",\"title\":\"Natural language processing (NLP) in management research: A literature review\",\"url\":\"https://www.semanticscholar.org/paper/db528269ef800727245c0fcb35b692d29c1ccdc9\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47540131\",\"name\":\"Ji Zhang\"},{\"authorId\":\"1712479\",\"name\":\"Mohamed Elhoseiny\"},{\"authorId\":\"145823372\",\"name\":\"S. Cohen\"},{\"authorId\":\"145907577\",\"name\":\"W. Chang\"},{\"authorId\":\"145159523\",\"name\":\"A. Elgammal\"}],\"doi\":\"10.1109/CVPR.2017.555\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"759a7e443f725be44ded970745c5422fd3196127\",\"title\":\"Relationship Proposal Networks\",\"url\":\"https://www.semanticscholar.org/paper/759a7e443f725be44ded970745c5422fd3196127\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1903.02930\",\"authors\":[{\"authorId\":\"49513989\",\"name\":\"Antonios Anastasopoulos\"},{\"authorId\":\"9567965\",\"name\":\"Shankar Kumar\"},{\"authorId\":\"39977619\",\"name\":\"H. Liao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4791dc0c4519988536e4846cbb24ae6382b8fdfd\",\"title\":\"Neural Language Modeling with Visual Features\",\"url\":\"https://www.semanticscholar.org/paper/4791dc0c4519988536e4846cbb24ae6382b8fdfd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47056886\",\"name\":\"Xiangpeng Li\"},{\"authorId\":\"30076791\",\"name\":\"Zhilong Zhou\"},{\"authorId\":\"35153304\",\"name\":\"Lijiang Chen\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"}],\"doi\":\"10.1007/s11280-018-0531-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"title\":\"Residual attention-based LSTM for video captioning\",\"url\":\"https://www.semanticscholar.org/paper/f7eb3ac4ccb30a2ce759094f3972a018575f74b6\",\"venue\":\"World Wide Web\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2234342\",\"name\":\"Lisa Anne Hendricks\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd7062e6f84750688fa96143209efc801e91f9bd\",\"title\":\"Visual Understanding through Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/dd7062e6f84750688fa96143209efc801e91f9bd\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1390024605\",\"name\":\"Ziad Al-Halah\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c0c7b90e0c7badaa95924530cb50d86444010ff\",\"title\":\"Semantic Attributes for Transfer Learning in Visual Recognition\",\"url\":\"https://www.semanticscholar.org/paper/6c0c7b90e0c7badaa95924530cb50d86444010ff\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21455137\",\"name\":\"Malcolm Doering\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"888cecaff8e2b6b7ec5bf4a3b043e277d1b6593b\",\"title\":\"Verb semantics as denoting change of state in the physical world\",\"url\":\"https://www.semanticscholar.org/paper/888cecaff8e2b6b7ec5bf4a3b043e277d1b6593b\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48902313\",\"name\":\"Wei Zhang\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"2066429\",\"name\":\"Shiai Zhu\"},{\"authorId\":\"30889568\",\"name\":\"Abdulmotaleb El Saddik\"}],\"doi\":\"10.1145/3279952\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"title\":\"Deep Learning\\u2013Based Multimedia Analytics\",\"url\":\"https://www.semanticscholar.org/paper/bb9e418469d018be7f5ac2c4b2435ccac50088a3\",\"venue\":\"ACM Trans. Multim. Comput. Commun. Appl.\",\"year\":2019},{\"arxivId\":\"1503.01070\",\"authors\":[{\"authorId\":\"1730844\",\"name\":\"Atousa Torabi\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"title\":\"Using Descriptive Video Services to Create a Large Data Source for Video Annotation Research\",\"url\":\"https://www.semanticscholar.org/paper/b1ddb2994e49a6a4f45e878c1cda7562b03177e6\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1603.02814\",\"authors\":[{\"authorId\":\"144663765\",\"name\":\"Qi Wu\"},{\"authorId\":\"1780381\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"144282676\",\"name\":\"Peng Wang\"},{\"authorId\":\"121177698\",\"name\":\"A. Dick\"},{\"authorId\":\"5546141\",\"name\":\"A. V. D. Hengel\"}],\"doi\":\"10.1109/TPAMI.2017.2708709\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"title\":\"Image Captioning and Visual Question Answering Based on Attributes and External Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/6acd385b2742f65359efb99543ebfb9a0d1b850f\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"47088868\",\"name\":\"Joshua R. Smith\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"title\":\"Empirical performance upper bounds for image and video captioning\",\"url\":\"https://www.semanticscholar.org/paper/4078c37c39dc5c7c65a5494651ba6dd443cf9269\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"title\":\"Large-scale video analysis and understanding\",\"url\":\"https://www.semanticscholar.org/paper/6bd7ff039ff38f4bb41f7a4b9a1f370ef02eed80\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"88739206\",\"name\":\"Samyan Qayyum Wahla\"},{\"authorId\":\"153581336\",\"name\":\"Sahar Waqar\"},{\"authorId\":\"35528948\",\"name\":\"M. A. Khan\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ce9e7e696c86c2d79c2c729d460d8a45fdf36548\",\"title\":\"The University of Sheffield and University of Engineering & Technology, Lahore at TRECVID 2016: Video to Text Description Task\",\"url\":\"https://www.semanticscholar.org/paper/ce9e7e696c86c2d79c2c729d460d8a45fdf36548\",\"venue\":\"TRECVID\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9381012\",\"name\":\"Ruihai Wu\"},{\"authorId\":\"46321208\",\"name\":\"K. Xu\"},{\"authorId\":\"47535039\",\"name\":\"Chenchen Liu\"},{\"authorId\":\"50845730\",\"name\":\"Nan Zhuang\"},{\"authorId\":\"145353089\",\"name\":\"Y. Mu\"}],\"doi\":\"10.1609/AAAI.V34I07.6913\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"268c9bf91af6c29c40f34ebe5aa205ba14578851\",\"title\":\"Localize, Assemble, and Predicate: Contextual Object Proposal Embedding for Visual Relation Detection\",\"url\":\"https://www.semanticscholar.org/paper/268c9bf91af6c29c40f34ebe5aa205ba14578851\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144762505\",\"name\":\"C. Sun\"},{\"authorId\":\"144862593\",\"name\":\"R. Nevatia\"}],\"doi\":\"10.1007/978-3-319-10590-1_50\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"88f7a3d6f0521803ca59fde45601e94c3a34a403\",\"title\":\"Semantic Aware Video Transcription Using Random Forest Classifiers\",\"url\":\"https://www.semanticscholar.org/paper/88f7a3d6f0521803ca59fde45601e94c3a34a403\",\"venue\":\"ECCV\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50594656\",\"name\":\"A. Hashimoto\"},{\"authorId\":\"46845576\",\"name\":\"Takumi Fujino\"},{\"authorId\":\"2374821\",\"name\":\"Jun Harashima\"},{\"authorId\":\"1774960\",\"name\":\"M. Iiyama\"},{\"authorId\":\"1681266\",\"name\":\"M. Minoh\"}],\"doi\":\"10.1145/3106668.3106675\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0d26fa15204c3b92021bdc5b30f8d92e3c843594\",\"title\":\"Learning Food Appearance by a Supervision with Recipe Text\",\"url\":\"https://www.semanticscholar.org/paper/0d26fa15204c3b92021bdc5b30f8d92e3c843594\",\"venue\":\"CEA@IJCAI\",\"year\":2017},{\"arxivId\":\"1804.10692\",\"authors\":[{\"authorId\":\"1693704\",\"name\":\"H. F. Tung\"},{\"authorId\":\"34939798\",\"name\":\"Adam W. Harley\"},{\"authorId\":\"9930869\",\"name\":\"Liang-Kang Huang\"},{\"authorId\":\"1705557\",\"name\":\"K. Fragkiadaki\"}],\"doi\":\"10.1109/CVPR.2018.00732\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d2558c26915e834b60b06be7da1d9db5d0897343\",\"title\":\"Reward Learning from Narrated Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/d2558c26915e834b60b06be7da1d9db5d0897343\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8275214\",\"name\":\"P. Tang\"},{\"authorId\":\"102599406\",\"name\":\"Y. Tan\"},{\"authorId\":\"46276803\",\"name\":\"J. Li\"},{\"authorId\":\"46513749\",\"name\":\"Bin Tan\"}],\"doi\":\"10.1016/j.jvcir.2020.102875\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"title\":\"Translating video into language by enhancing visual and language representations\",\"url\":\"https://www.semanticscholar.org/paper/32c722384bdf8ac2ade6d6e4ce3225077b124555\",\"venue\":\"J. Vis. Commun. Image Represent.\",\"year\":2020},{\"arxivId\":\"2007.02375\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3431141\",\"name\":\"Yehao Li\"},{\"authorId\":\"5891694\",\"name\":\"J. Luo\"},{\"authorId\":\"37184350\",\"name\":\"J. Xu\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"1490772804\",\"name\":\"Tao Mei\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"title\":\"Auto-captions on GIF: A Large-scale Video-sentence Dataset for Vision-language Pre-training\",\"url\":\"https://www.semanticscholar.org/paper/ad9d41b29f7b7b35278f466dc2eafedaf7f57db1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2059713\",\"name\":\"Xiaoshan Yang\"},{\"authorId\":\"1907582\",\"name\":\"T. Zhang\"},{\"authorId\":\"145194969\",\"name\":\"C. Xu\"}],\"doi\":\"10.1145/2962719\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"title\":\"Semantic Feature Mining for Video Event Understanding\",\"url\":\"https://www.semanticscholar.org/paper/99a34646fc41586e82d0712a6ea3c04deb15cad9\",\"venue\":\"TOMM\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3175685\",\"name\":\"A. Mathews\"}],\"doi\":\"10.1145/2733373.2807998\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9daf6b42e0c2ef52fcec844eeaf04df3b4f48f9a\",\"title\":\"Captioning Images Using Different Styles\",\"url\":\"https://www.semanticscholar.org/paper/9daf6b42e0c2ef52fcec844eeaf04df3b4f48f9a\",\"venue\":\"ACM Multimedia\",\"year\":2015},{\"arxivId\":\"1403.6173\",\"authors\":[{\"authorId\":\"34721166\",\"name\":\"Anna Rohrbach\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"113090874\",\"name\":\"W. Qiu\"},{\"authorId\":\"33985877\",\"name\":\"Annemarie Friedrich\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-319-11752-2_15\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"889e723cd6d581e120ee6776b231fdf69707ab50\",\"title\":\"Coherent Multi-sentence Video Description with Variable Level of Detail\",\"url\":\"https://www.semanticscholar.org/paper/889e723cd6d581e120ee6776b231fdf69707ab50\",\"venue\":\"GCPR\",\"year\":2014},{\"arxivId\":\"2003.03715\",\"authors\":[{\"authorId\":\"4492316\",\"name\":\"Fangyi Zhu\"},{\"authorId\":\"3090135\",\"name\":\"Jeng-Neng Hwang\"},{\"authorId\":\"46953683\",\"name\":\"Zhanyu Ma\"},{\"authorId\":\"143930563\",\"name\":\"G. Chen\"},{\"authorId\":\"145505204\",\"name\":\"J. Guo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d507f3088e5c8411bc06e274958cbe263169a39d\",\"title\":\"OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail Enhancement.\",\"url\":\"https://www.semanticscholar.org/paper/d507f3088e5c8411bc06e274958cbe263169a39d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721329\",\"name\":\"Q. Jin\"},{\"authorId\":\"1915796\",\"name\":\"Junwei Liang\"},{\"authorId\":\"48030229\",\"name\":\"Xiaozhu Lin\"}],\"doi\":\"10.21437/Interspeech.2016-380\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"title\":\"Generating Natural Video Descriptions via Multimodal Processing\",\"url\":\"https://www.semanticscholar.org/paper/2abae43b4a7fd85473bd6c906a0fcfc403968e87\",\"venue\":\"INTERSPEECH\",\"year\":2016}],\"corpusId\":5775821,\"doi\":\"10.1109/ICCV.2013.61\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":14,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2363529\",\"name\":\"Pradipto Das\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"38972663\",\"name\":\"Richard F. Doell\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":\"10.1109/CVPR.2013.340\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"title\":\"A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching\",\"url\":\"https://www.semanticscholar.org/paper/a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908\",\"venue\":\"2013 IEEE Conference on Computer Vision and Pattern Recognition\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"24138684\",\"name\":\"Dominikus Wetzel\"},{\"authorId\":\"1727272\",\"name\":\"Stefan Thater\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"}],\"doi\":\"10.1162/tacl_a_00207\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"title\":\"Grounding Action Descriptions in Videos\",\"url\":\"https://www.semanticscholar.org/paper/21b3007f967d39e1346bc91e0fc8b3f16121300c\",\"venue\":\"Transactions of the Association for Computational Linguistics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1717629\",\"name\":\"Yansong Feng\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1\",\"title\":\"How Many Words Is a Picture Worth? Automatic Caption Generation for News Images\",\"url\":\"https://www.semanticscholar.org/paper/c8b7e13a5d0c13dfde17c16f9cad2d50b442dba1\",\"venue\":\"ACL\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"title\":\"Im2Text: Describing Images Using 1 Million Captioned Photographs\",\"url\":\"https://www.semanticscholar.org/paper/8e080b98efbe65c02a116439205ca2344b9f7cd4\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34849128\",\"name\":\"Marcus Rohrbach\"},{\"authorId\":\"2485529\",\"name\":\"Michaela Regneri\"},{\"authorId\":\"1906895\",\"name\":\"M. Andriluka\"},{\"authorId\":\"40404576\",\"name\":\"S. Amin\"},{\"authorId\":\"1717560\",\"name\":\"Manfred Pinkal\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"}],\"doi\":\"10.1007/978-3-642-33718-5_11\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8623fe8b087cedcaac276e313f8fed6f0dfccc33\",\"title\":\"Script Data for Attribute-Based Recognition of Composite Activities\",\"url\":\"https://www.semanticscholar.org/paper/8623fe8b087cedcaac276e313f8fed6f0dfccc33\",\"venue\":\"ECCV\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6418392\",\"name\":\"P. Koehn\"}],\"doi\":\"10.5860/choice.47-6293\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"87dadbe571092cb67a4183730a885efaf42e634c\",\"title\":\"Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/87dadbe571092cb67a4183730a885efaf42e634c\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687120\",\"name\":\"S. Guadarrama\"},{\"authorId\":\"3006928\",\"name\":\"N. Krishnamoorthy\"},{\"authorId\":\"3163967\",\"name\":\"Girish Malkarnenkar\"},{\"authorId\":\"1811430\",\"name\":\"Subhashini Venugopalan\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"2903226\",\"name\":\"Kate Saenko\"}],\"doi\":\"10.1109/ICCV.2013.337\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d6a7a563640bf53953c4fda0997e4db176488510\",\"title\":\"YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition\",\"url\":\"https://www.semanticscholar.org/paper/d6a7a563640bf53953c4fda0997e4db176488510\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2193802\",\"name\":\"M. U. Khan\"},{\"authorId\":\"36794849\",\"name\":\"L. Zhang\"},{\"authorId\":\"1703592\",\"name\":\"Y. Gotoh\"}],\"doi\":\"10.1109/ICCVW.2011.6130425\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fbadf89b990acedf23e1df03d4869010d2dbc59e\",\"title\":\"Human Focused Video Description\",\"url\":\"https://www.semanticscholar.org/paper/fbadf89b990acedf23e1df03d4869010d2dbc59e\",\"venue\":\"2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"3077970\",\"name\":\"M. Cettolo\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"93f6dd2c761fdeac0af6d2253d57834439d7794f\",\"title\":\"IRSTLM: an open source toolkit for handling large scale language models\",\"url\":\"https://www.semanticscholar.org/paper/93f6dd2c761fdeac0af6d2253d57834439d7794f\",\"venue\":\"INTERSPEECH\",\"year\":2008},{\"arxivId\":\"1204.2742\",\"authors\":[{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"},{\"authorId\":\"48540451\",\"name\":\"Alexander Bridge\"},{\"authorId\":\"3190146\",\"name\":\"Zachary Burchill\"},{\"authorId\":\"49081881\",\"name\":\"D. Coroian\"},{\"authorId\":\"1779136\",\"name\":\"S. Dickinson\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"},{\"authorId\":\"38414598\",\"name\":\"A. Michaux\"},{\"authorId\":\"2587937\",\"name\":\"Sam Mussman\"},{\"authorId\":\"38052303\",\"name\":\"S. Narayanaswamy\"},{\"authorId\":\"2968009\",\"name\":\"D. Salvi\"},{\"authorId\":\"50269497\",\"name\":\"Lara Schmidt\"},{\"authorId\":\"2060623\",\"name\":\"Jiangnan Shangguan\"},{\"authorId\":\"1737754\",\"name\":\"J. Siskind\"},{\"authorId\":\"32655613\",\"name\":\"J. Waggoner\"},{\"authorId\":\"30102584\",\"name\":\"S. Wang\"},{\"authorId\":\"2223764\",\"name\":\"Jinlian Wei\"},{\"authorId\":\"1813304\",\"name\":\"Yifan Yin\"},{\"authorId\":\"48806246\",\"name\":\"Zhiqi Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"793c1c908672ea71aef9e1b41a46272aa27598f7\",\"title\":\"Video In Sentences Out\",\"url\":\"https://www.semanticscholar.org/paper/793c1c908672ea71aef9e1b41a46272aa27598f7\",\"venue\":\"UAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49501003\",\"name\":\"Margaret Mitchell\"},{\"authorId\":\"34176020\",\"name\":\"Jesse Dodge\"},{\"authorId\":\"46479604\",\"name\":\"Amit Goyal\"},{\"authorId\":\"1721910\",\"name\":\"Kota Yamaguchi\"},{\"authorId\":\"1714215\",\"name\":\"K. Stratos\"},{\"authorId\":\"1682965\",\"name\":\"Xufeng Han\"},{\"authorId\":\"40614240\",\"name\":\"A. Mensch\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"355de7460120ddc1150d9ce3756f9848983f7ff4\",\"title\":\"Midge: Generating Image Descriptions From Computer Vision Detections\",\"url\":\"https://www.semanticscholar.org/paper/355de7460120ddc1150d9ce3756f9848983f7ff4\",\"venue\":\"EACL\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Schmidt\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"UGM: Matlab code for undirected graphical models\",\"url\":\"\",\"venue\":\"di.ens.fr/\\u223cmschmidt/Software/UGM.html,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737809\",\"name\":\"A. Gupta\"},{\"authorId\":\"2577358\",\"name\":\"P. Srinivasan\"},{\"authorId\":\"46865129\",\"name\":\"J. Shi\"},{\"authorId\":\"1693428\",\"name\":\"L. Davis\"}],\"doi\":\"10.1109/CVPR.2009.5206492\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f3743f425faf5cae6fb1e55a864e8361027fff6c\",\"title\":\"Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos\",\"url\":\"https://www.semanticscholar.org/paper/f3743f425faf5cae6fb1e55a864e8361027fff6c\",\"venue\":\"CVPR\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2002316\",\"name\":\"F. Och\"},{\"authorId\":\"145322333\",\"name\":\"H. Ney\"}],\"doi\":\"10.1162/089120103321337421\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de2df29b0a0312de7270c3f5a0af6af5645cf91a\",\"title\":\"A Systematic Comparison of Various Statistical Alignment Models\",\"url\":\"https://www.semanticscholar.org/paper/de2df29b0a0312de7270c3f5a0af6af5645cf91a\",\"venue\":\"Computational Linguistics\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1755162\",\"name\":\"Philipp Koehn\"},{\"authorId\":\"152378023\",\"name\":\"Hieu T. Hoang\"},{\"authorId\":\"2539211\",\"name\":\"Alexandra Birch\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"46898156\",\"name\":\"B. Cowan\"},{\"authorId\":\"2529583\",\"name\":\"Wade Shen\"},{\"authorId\":\"145046497\",\"name\":\"C. Moran\"},{\"authorId\":\"1983801\",\"name\":\"R. Zens\"},{\"authorId\":\"1745899\",\"name\":\"Chris Dyer\"},{\"authorId\":\"143832874\",\"name\":\"Ondrej Bojar\"},{\"authorId\":\"4733879\",\"name\":\"A. Constantin\"},{\"authorId\":\"6376655\",\"name\":\"E. Herbst\"}],\"doi\":\"10.3115/1557769.1557821\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4ee2eab4c298c1824a9fb8799ad8eed21be38d21\",\"title\":\"Moses: Open Source Toolkit for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/4ee2eab4c298c1824a9fb8799ad8eed21be38d21\",\"venue\":\"ACL\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"},{\"authorId\":\"1888731\",\"name\":\"M. Hejrati\"},{\"authorId\":\"21160985\",\"name\":\"M. Sadeghi\"},{\"authorId\":\"145539241\",\"name\":\"P. Young\"},{\"authorId\":\"3125805\",\"name\":\"Cyrus Rashtchian\"},{\"authorId\":\"3118681\",\"name\":\"J. Hockenmaier\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/978-3-642-15561-1_2\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"title\":\"Every Picture Tells a Story: Generating Sentences from Images\",\"url\":\"https://www.semanticscholar.org/paper/eaaed23a2d94feb2f1c3ff22a25777c7a78f3141\",\"venue\":\"ECCV\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49693392\",\"name\":\"A. Kojima\"},{\"authorId\":\"46526487\",\"name\":\"Takeshi Tamura\"},{\"authorId\":\"145950023\",\"name\":\"K. Fukunaga\"}],\"doi\":\"10.1023/A:1020346032608\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d53a97a3dd7760b193c0d9a5293b60feff239059\",\"title\":\"Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions\",\"url\":\"https://www.semanticscholar.org/paper/d53a97a3dd7760b193c0d9a5293b60feff239059\",\"venue\":\"International Journal of Computer Vision\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145564333\",\"name\":\"G. Kulkarni\"},{\"authorId\":\"3128210\",\"name\":\"Visruth Premraj\"},{\"authorId\":\"2985883\",\"name\":\"Sagnik Dhar\"},{\"authorId\":\"50341924\",\"name\":\"Siming Li\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"}],\"doi\":\"10.1109/CVPR.2011.5995466\",\"intent\":[\"methodology\",\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"169b847e69c35cfd475eb4dcc561a24de11762ca\",\"title\":\"Baby talk: Understanding and generating simple image descriptions\",\"url\":\"https://www.semanticscholar.org/paper/169b847e69c35cfd475eb4dcc561a24de11762ca\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1710748\",\"name\":\"P. Hanckmann\"},{\"authorId\":\"1682184\",\"name\":\"K. Schutte\"},{\"authorId\":\"1909303\",\"name\":\"G. Burghouts\"}],\"doi\":\"10.1007/978-3-642-33863-2_37\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"022c941ea709824129fa13ecbbe131bdea2ceaa5\",\"title\":\"Automated Textual Descriptions for a Wide Range of Video Events with 48 Human Actions\",\"url\":\"https://www.semanticscholar.org/paper/022c941ea709824129fa13ecbbe131bdea2ceaa5\",\"venue\":\"ECCV Workshops\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2909813\",\"name\":\"C. C. Tan\"},{\"authorId\":\"1717861\",\"name\":\"Yu-Gang Jiang\"},{\"authorId\":\"143977389\",\"name\":\"C. Ngo\"}],\"doi\":\"10.1145/2072298.2072411\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"261ae2f6f426b5efd32ea5b981751d1c63c5309d\",\"title\":\"Towards textually describing complex video contents with audio-visual concept classifiers\",\"url\":\"https://www.semanticscholar.org/paper/261ae2f6f426b5efd32ea5b981751d1c63c5309d\",\"venue\":\"MM '11\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145592791\",\"name\":\"P. Kuznetsova\"},{\"authorId\":\"2004053\",\"name\":\"Vicente Ordonez\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"1685538\",\"name\":\"T. Berg\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":null,\"intent\":[\"methodology\",\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"2a0d0f6c5a69b264710df0230696f47c5918e2f2\",\"title\":\"Collective Generation of Natural Image Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/2a0d0f6c5a69b264710df0230696f47c5918e2f2\",\"venue\":\"ACL\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46506697\",\"name\":\"Heng Wang\"},{\"authorId\":\"2909350\",\"name\":\"Alexander Kl\\u00e4ser\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"},{\"authorId\":\"1689269\",\"name\":\"C. Liu\"}],\"doi\":\"10.1007/s11263-012-0594-8\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bbe0819a47a9f3f11dd34bb3ab44a997ef111088\",\"title\":\"Dense Trajectories and Motion Boundary Descriptors for Action Recognition\",\"url\":\"https://www.semanticscholar.org/paper/bbe0819a47a9f3f11dd34bb3ab44a997ef111088\",\"venue\":\"International Journal of Computer Vision\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2446509\",\"name\":\"P. D. Sahin\"},{\"authorId\":\"145602732\",\"name\":\"Kobus Barnard\"},{\"authorId\":\"145417505\",\"name\":\"J. F. Freitas\"},{\"authorId\":\"144016256\",\"name\":\"D. Forsyth\"}],\"doi\":\"10.1007/3-540-47979-1_7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d9f55b445f36578802e7eef4393cfa914b11620\",\"title\":\"Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary\",\"url\":\"https://www.semanticscholar.org/paper/6d9f55b445f36578802e7eef4393cfa914b11620\",\"venue\":\"ECCV\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49604675\",\"name\":\"P. Koehn\"},{\"authorId\":\"102811815\",\"name\":\"Marcello Federico\"},{\"authorId\":\"2529583\",\"name\":\"Wade Shen\"},{\"authorId\":\"1895952\",\"name\":\"N. Bertoldi\"},{\"authorId\":\"1389724108\",\"name\":\"Chris Callison-Burch\"},{\"authorId\":\"143832874\",\"name\":\"Ondrej Bojar\"},{\"authorId\":\"46898156\",\"name\":\"B. Cowan\"},{\"authorId\":\"1745899\",\"name\":\"Chris Dyer\"},{\"authorId\":\"152378023\",\"name\":\"Hieu T. Hoang\"},{\"authorId\":\"1983801\",\"name\":\"R. Zens\"},{\"authorId\":\"31542143\",\"name\":\"A. Constantin\"},{\"authorId\":\"6376655\",\"name\":\"E. Herbst\"},{\"authorId\":\"89224350\",\"name\":\"C. C. Moran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"99e8d34817ae10d7304521e89c5fbf908b9d856b\",\"title\":\"Open Source Toolkit for Statistical Machine Translation: Factored Translation Models and Lattice Decoding\",\"url\":\"https://www.semanticscholar.org/paper/99e8d34817ae10d7304521e89c5fbf908b9d856b\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34066479\",\"name\":\"Vignesh Ramanathan\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1109/ICCV.2013.117\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"092f57121e10dcb65a6c348dd8b529bb06ebfb89\",\"title\":\"Video Event Understanding Using Natural Language Descriptions\",\"url\":\"https://www.semanticscholar.org/paper/092f57121e10dcb65a6c348dd8b529bb06ebfb89\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145970060\",\"name\":\"A. Aker\"},{\"authorId\":\"1718590\",\"name\":\"R. Gaizauskas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8dbc756ea246f599250c09e3efd9bba9909a842\",\"title\":\"Generating Image Descriptions Using Dependency Relational Patterns\",\"url\":\"https://www.semanticscholar.org/paper/e8dbc756ea246f599250c09e3efd9bba9909a842\",\"venue\":\"ACL\",\"year\":2010}],\"title\":\"Translating Video Content to Natural Language Descriptions\",\"topics\":[{\"topic\":\"Statistical machine translation\",\"topicId\":\"42996\",\"url\":\"https://www.semanticscholar.org/topic/42996\"},{\"topic\":\"Parallel text\",\"topicId\":\"16792\",\"url\":\"https://www.semanticscholar.org/topic/16792\"},{\"topic\":\"BLEU\",\"topicId\":\"250421\",\"url\":\"https://www.semanticscholar.org/topic/250421\"},{\"topic\":\"N-gram\",\"topicId\":\"236768\",\"url\":\"https://www.semanticscholar.org/topic/236768\"},{\"topic\":\"Humans\",\"topicId\":\"732\",\"url\":\"https://www.semanticscholar.org/topic/732\"},{\"topic\":\"Audio description\",\"topicId\":\"1289167\",\"url\":\"https://www.semanticscholar.org/topic/1289167\"},{\"topic\":\"Intermediate representation\",\"topicId\":\"8015\",\"url\":\"https://www.semanticscholar.org/topic/8015\"},{\"topic\":\"Natural language generation\",\"topicId\":\"6196\",\"url\":\"https://www.semanticscholar.org/topic/6196\"},{\"topic\":\"Compiler\",\"topicId\":\"13817\",\"url\":\"https://www.semanticscholar.org/topic/13817\"},{\"topic\":\"Correctness (computer science)\",\"topicId\":\"12411\",\"url\":\"https://www.semanticscholar.org/topic/12411\"},{\"topic\":\"Relevance\",\"topicId\":\"503\",\"url\":\"https://www.semanticscholar.org/topic/503\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Conditional random field\",\"topicId\":\"85317\",\"url\":\"https://www.semanticscholar.org/topic/85317\"}],\"url\":\"https://www.semanticscholar.org/paper/e8cd37fbd8bd5e690eef5861cf92af8e002d4533\",\"venue\":\"2013 IEEE International Conference on Computer Vision\",\"year\":2013}\n"