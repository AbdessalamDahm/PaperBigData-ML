"{\"abstract\":\"We propose a policy improvement algorithm for Reinforcement Learning (RL) termed Rerouted Behavior Improvement (RBI). RBI is designed to take into account the evaluation errors of the Q-function. Such errors are common in RL when learning theQvalue from finite experience data. Greedy policies or even constrained policy optimization algorithms that ignore these errors may suffer from an improvement penalty (i.e., a policy impairment). To reduce the penalty, the idea of RBI is to attenuate rapid policy changes to actions that were rarely sampled. This approach is shown to avoid catastrophic performance degradation and reduce regret when learning from a batch of transition samples. Through a two-armed bandit example, we show that it also increases data efficiency when the optimal action has a high variance. We evaluate RBI in two tasks in the Atari Learning Environment: (1) learning from observations of multiple behavior policies and (2) iterative RL. Our results demonstrate the advantage of RBI over greedy policies and other constrained policy optimization algorithms both in learning from observations and in RL tasks.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"31148257\",\"name\":\"Elad Sarafian\",\"url\":\"https://www.semanticscholar.org/author/31148257\"},{\"authorId\":\"152473727\",\"name\":\"S. Kraus\",\"url\":\"https://www.semanticscholar.org/author/152473727\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":219176118,\"doi\":\"10.24963/ijcai.2020/392\",\"fieldsOfStudy\":null,\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"0a0cf1af377aec7279b548248ae9041ef3ebd379\",\"references\":[{\"arxivId\":\"1803.00933\",\"authors\":[{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2508525\",\"name\":\"D. Budden\"},{\"authorId\":\"1403998955\",\"name\":\"Gabriel Barth-Maron\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1f08598381af9146d0fd9a61b30d0e51a7331689\",\"title\":\"Distributed Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/1f08598381af9146d0fd9a61b30d0e51a7331689\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"69354293\",\"name\":\"\\u5433\\u653f\\u884d\"},{\"authorId\":\"3019713\",\"name\":\"Cheng-Yen Wu\"},{\"authorId\":\"69423194\",\"name\":\"\\u5f90\\u4fdd\\u7f85\"},{\"authorId\":\"2536501\",\"name\":\"Pau-Lo Hsu\"}],\"doi\":\"10.1007/978-1-4419-9863-7_100962\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c61d895f219e2f427d51a449d15a8173dce5d1a\",\"title\":\"Multi-Agent \\u99d5\\u99db\\u8f14\\u52a9\\u7cfb\\u7d71\",\"url\":\"https://www.semanticscholar.org/paper/7c61d895f219e2f427d51a449d15a8173dce5d1a\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144543541\",\"name\":\"P. Auer\"},{\"authorId\":\"1397171999\",\"name\":\"Nicol\\u00f2 Cesa-Bianchi\"},{\"authorId\":\"152138722\",\"name\":\"P. Fischer\"}],\"doi\":\"10.1023/A:1013689704352\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3\",\"title\":\"Finite-time Analysis of the Multiarmed Bandit Problem\",\"url\":\"https://www.semanticscholar.org/paper/1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Philip Thomas\"},{\"authorId\":null,\"name\":\"Georgios Theocharous\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh. High confidence policy improvement\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 2380\\u20132388,\",\"year\":2015},{\"arxivId\":\"1802.09477\",\"authors\":[{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"},{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"51174612\",\"name\":\"David Meger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4debb99c0c63bfaa97dd433bc2828e4dac81c48b\",\"title\":\"Addressing Function Approximation Error in Actor-Critic Methods\",\"url\":\"https://www.semanticscholar.org/paper/4debb99c0c63bfaa97dd433bc2828e4dac81c48b\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Quan Vuong\"},{\"authorId\":null,\"name\":\"Yiming Zhang\"},{\"authorId\":null,\"name\":\"W Keith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Trust region policy optimization Mastering the game of go without human knowledge High confidence policy improvement\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1446962550\",\"name\":\"Dock Bumpers\"},{\"authorId\":\"1446961266\",\"name\":\"Support Ledgers\"}],\"doi\":\"10.1023/A:1017189329742\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"title\":\"Volume 2\",\"url\":\"https://www.semanticscholar.org/paper/e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"venue\":\"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Javier Garc\\u0131a\"},{\"authorId\":null,\"name\":\"Fernando Fern\\u00e1ndez. A comprehensive survey on safe reinforc learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"16(1):1437\\u20131480,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":\"1606.01868\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"143810408\",\"name\":\"D. Saxton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e90fd78e8a3b98af3954aae5209703aa966603e\",\"title\":\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9937287\",\"name\":\"Ronan Fruit\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f77020cda7f77ee0d6e642ff94969c6d48a2721\",\"title\":\"Regret Minimization in MDPs with Options without Prior Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/7f77020cda7f77ee0d6e642ff94969c6d48a2721\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1710.02298\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"title\":\"Rainbow: Combining Improvements in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1705.10998\",\"authors\":[{\"authorId\":\"30525721\",\"name\":\"V. Kurin\"},{\"authorId\":\"2388416\",\"name\":\"Sebastian Nowozin\"},{\"authorId\":\"145186674\",\"name\":\"Katja Hofmann\"},{\"authorId\":\"39611591\",\"name\":\"Lucas Beyer\"},{\"authorId\":\"1789756\",\"name\":\"B. Leibe\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ab8efc8243b6c374374d407f25d38af41071830\",\"title\":\"The Atari Grand Challenge Dataset\",\"url\":\"https://www.semanticscholar.org/paper/9ab8efc8243b6c374374d407f25d38af41071830\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1763352\",\"name\":\"Y. Rabani\"}],\"doi\":\"10.1201/9781420010749.ch6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2b9f59ff70c69daa2d7b6960f36a51efe9ba60a0\",\"title\":\"Linear Programming\",\"url\":\"https://www.semanticscholar.org/paper/2b9f59ff70c69daa2d7b6960f36a51efe9ba60a0\",\"venue\":\"Handbook of Approximation Algorithms and Metaheuristics\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"OpenAI\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Openai five\",\"url\":\"\",\"venue\":\"https://blog.openai.com/openai-five/, 2018 - accessed May\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145343838\",\"name\":\"Andreas Krause\"},{\"authorId\":\"1706780\",\"name\":\"Cheng Soon Ong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a1a51422cbdfc651d7726e8c613fc44f7fb4fc1\",\"title\":\"Contextual Gaussian Process Bandit Optimization\",\"url\":\"https://www.semanticscholar.org/paper/5a1a51422cbdfc651d7726e8c613fc44f7fb4fc1\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael J Kearns\"},{\"authorId\":null,\"name\":\"Satinder P Singh. Bias-variance error bounds for temporal d COLT\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 142\\u2013147\",\"url\":\"\",\"venue\":\"Citeseer,\",\"year\":2000},{\"arxivId\":\"1511.06581\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"title\":\"Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1510.08906\",\"authors\":[{\"authorId\":\"2160071\",\"name\":\"C. Dann\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef\",\"title\":\"Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f4aa31b7ae2e03ee5a6b94f34cb3b6a554230aef\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144579461\",\"name\":\"Q. Vuong\"},{\"authorId\":\"49889218\",\"name\":\"Yiming Zhang\"},{\"authorId\":\"1829862\",\"name\":\"K. Ross\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"23f7167003c58cb38dc3b68536ca6729950b74b8\",\"title\":\"Supervised Policy Update\",\"url\":\"https://www.semanticscholar.org/paper/23f7167003c58cb38dc3b68536ca6729950b74b8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Pirotta\"},{\"authorId\":null,\"name\":\"Marcello Restelli\"},{\"authorId\":null,\"name\":\"Alessio Pecorino\"},{\"authorId\":null,\"name\":\"Daniele Calandriello. Safe policy iteration\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 307\\u2013315,\",\"year\":2013},{\"arxivId\":\"1809.06098\",\"authors\":[{\"authorId\":\"24717227\",\"name\":\"Alberto Maria Metelli\"},{\"authorId\":\"145388375\",\"name\":\"M. Papini\"},{\"authorId\":\"79787170\",\"name\":\"Francesco Faccio\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a58733db98ea7bf79dd600a87581dae27de5c96f\",\"title\":\"Policy Optimization via Importance Sampling\",\"url\":\"https://www.semanticscholar.org/paper/a58733db98ea7bf79dd600a87581dae27de5c96f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1905.03231\",\"authors\":[{\"authorId\":\"145388375\",\"name\":\"M. Papini\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"32a01db101b9576d9ab06b3eb29b6c63aa280e3b\",\"title\":\"Smoothing Policies and Safe Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/32a01db101b9576d9ab06b3eb29b6c63aa280e3b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735414\",\"name\":\"T. Knasel\"}],\"doi\":\"10.1016/0921-8890(88)90002-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edd77f310393f521669b209cbb6828fb45a8485d\",\"title\":\"Robotics and autonomous systems\",\"url\":\"https://www.semanticscholar.org/paper/edd77f310393f521669b209cbb6828fb45a8485d\",\"venue\":\"Robotics Auton. Syst.\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1836885\",\"name\":\"Brenna Argall\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"},{\"authorId\":\"1699032\",\"name\":\"B. Browning\"}],\"doi\":\"10.1016/j.robot.2008.10.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"title\":\"A survey of robot learning from demonstration\",\"url\":\"https://www.semanticscholar.org/paper/4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bbd0a9bea22b4cb7574528382e2f81893d8abc17\",\"title\":\"Bias-Variance Error Bounds for Temporal Difference Updates\",\"url\":\"https://www.semanticscholar.org/paper/bbd0a9bea22b4cb7574528382e2f81893d8abc17\",\"venue\":\"COLT\",\"year\":2000},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017}],\"title\":\"Constrained Policy Improvement for Efficient Reinforcement Learning\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/0a0cf1af377aec7279b548248ae9041ef3ebd379\",\"venue\":\"\",\"year\":2020}\n"