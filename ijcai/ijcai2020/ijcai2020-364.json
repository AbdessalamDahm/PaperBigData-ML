"{\"abstract\":\"The ability to transfer knowledge to novel environments and tasks is a sensible desiderata for general learning agents. Despite the apparent promises, transfer in RL is still an open and little exploited research area. In this paper, we take a brand-new perspective about transfer: we suggest that the ability to assign credit unveils structural invariants in the tasks that can be transferred to make RL more sample-efficient. Our main contribution is SECRET, a novel approach to transfer learning for RL that uses a backward-view credit assignment mechanism based on a self-attentive architecture. Two aspects are key to its generality: it learns to assign credit as a separate offline supervised process and exclusively modifies the reward function. Consequently, it can be supplemented by transfer methods that do not modify the reward function and it can be plugged on top of any RL algorithm.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"151047979\",\"name\":\"Johan Ferret\",\"url\":\"https://www.semanticscholar.org/author/151047979\"},{\"authorId\":\"151188653\",\"name\":\"Raphael Marinier\",\"url\":\"https://www.semanticscholar.org/author/151188653\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\",\"url\":\"https://www.semanticscholar.org/author/1737555\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\",\"url\":\"https://www.semanticscholar.org/author/1721354\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2003.13350\",\"authors\":[{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"67007190\",\"name\":\"Steven Kapturowski\"},{\"authorId\":\"2905900\",\"name\":\"P. Sprechmann\"},{\"authorId\":\"1492155662\",\"name\":\"Alex Vitvitskyi\"},{\"authorId\":\"1999078\",\"name\":\"Daniel Guo\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9651e1987a39d100dc0f6696a2077199e5075ea2\",\"title\":\"Agent57: Outperforming the Atari Human Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/9651e1987a39d100dc0f6696a2077199e5075ea2\",\"venue\":\"ICML\",\"year\":2020}],\"corpusId\":208247964,\"doi\":\"10.24963/ijcai.2020/364\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"ca37f9f7f03f9ee463d6dcfd84019f1fcd314b2c\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":\"1810.06721\",\"authors\":[{\"authorId\":\"35758795\",\"name\":\"Chia-Chun Hung\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3041463\",\"name\":\"Josh Abramson\"},{\"authorId\":\"145066085\",\"name\":\"Yan Wu\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"32561676\",\"name\":\"F. Carnevale\"},{\"authorId\":\"37968006\",\"name\":\"Arun Ahuja\"},{\"authorId\":\"89504302\",\"name\":\"G. Wayne\"}],\"doi\":\"10.1038/s41467-019-13073-w\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1456d9d2ee40b932e2e07762985a60f0bdff07f3\",\"title\":\"Optimizing agent behavior over long time scales by transporting value\",\"url\":\"https://www.semanticscholar.org/paper/1456d9d2ee40b932e2e07762985a60f0bdff07f3\",\"venue\":\"Nature Communications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":\"1703.03400\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518\",\"title\":\"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1611.02779\",\"authors\":[{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1745169\",\"name\":\"P. Bartlett\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"954b01151ff13aef416d27adc60cd9a076753b1a\",\"title\":\"RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/954b01151ff13aef416d27adc60cd9a076753b1a\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":\"1901.09330\",\"authors\":[{\"authorId\":\"19256172\",\"name\":\"Haosheng Zou\"},{\"authorId\":\"32453998\",\"name\":\"Tongzheng Ren\"},{\"authorId\":\"143848636\",\"name\":\"Dong Yan\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"145296845\",\"name\":\"J. Zhu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2636bff7d3bdccf9b39c5e1e7d86a77690f1c07d\",\"title\":\"Reward Shaping via Meta-Learning\",\"url\":\"https://www.semanticscholar.org/paper/2636bff7d3bdccf9b39c5e1e7d86a77690f1c07d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chia-Chun Hung\"},{\"authorId\":null,\"name\":\"Timothy P. Lillicrap\"},{\"authorId\":null,\"name\":\"Josh Abramson\"},{\"authorId\":null,\"name\":\"Yan Wu\"},{\"authorId\":null,\"name\":\"Mehdi Mirza\"},{\"authorId\":null,\"name\":\"Federico Carnevale\"},{\"authorId\":null,\"name\":\"Arun Ahuja\"},{\"authorId\":null,\"name\":\"Greg Wayne. Optimizing agent behavior over long time sc value\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1810.06721,\",\"year\":2018},{\"arxivId\":\"1511.06295\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"143959037\",\"name\":\"J. Kirkpatrick\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"title\":\"Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1703.03130\",\"authors\":[{\"authorId\":\"3146592\",\"name\":\"Zhouhan Lin\"},{\"authorId\":\"2521552\",\"name\":\"Minwei Feng\"},{\"authorId\":\"1790831\",\"name\":\"C. D. Santos\"},{\"authorId\":\"2482533\",\"name\":\"Mo Yu\"},{\"authorId\":\"144028698\",\"name\":\"B. Xiang\"},{\"authorId\":\"145218984\",\"name\":\"Bowen Zhou\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"204a4a70428f3938d2c538a4d74c7ae0416306d8\",\"title\":\"A Structured Self-attentive Sentence Embedding\",\"url\":\"https://www.semanticscholar.org/paper/204a4a70428f3938d2c538a4d74c7ae0416306d8\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Filip Wolski\"},{\"authorId\":null,\"name\":\"Prafulla Dhariwal\"},{\"authorId\":null,\"name\":\"Alec Radford\"},{\"authorId\":null,\"name\":\"Oleg Klimov. Proximal policy optimization algorithms\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1707.06347,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2726706\",\"name\":\"N. B\\u00e4uerle\"},{\"authorId\":\"1707387\",\"name\":\"U. Rieder\"}],\"doi\":\"10.1365/S13291-010-0007-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b2db5541059288472ca246acdca6ead949326864\",\"title\":\"Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/b2db5541059288472ca246acdca6ead949326864\",\"venue\":\"\",\"year\":2010},{\"arxivId\":\"1409.0473\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"title\":\"Neural Machine Translation by Jointly Learning to Align and Translate\",\"url\":\"https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Long-Ji Lin. Self-improving reactive agents based on reinf learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"planning and teaching\",\"url\":\"\",\"venue\":\"Machine learning,\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4562073\",\"name\":\"Chris Watkins\"},{\"authorId\":\"46902274\",\"name\":\"P. Dayan\"}],\"doi\":\"10.1007/BF00992698\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"03b7e51c52084ac1db5118342a00b5fbcfc587aa\",\"title\":\"Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/03b7e51c52084ac1db5118342a00b5fbcfc587aa\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mnih\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The first convolutional layer has 8 filters, a 3x3 kernel size and a stride\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Tieleman\"},{\"authorId\":null,\"name\":\"G. Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lecture 6.5\\u2014RMSProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning\",\"url\":\"\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1902.07198\",\"authors\":[{\"authorId\":\"29767024\",\"name\":\"Rishabh Agarwal\"},{\"authorId\":\"145246869\",\"name\":\"Chen Liang\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84b888b66d598f4fd5909c524a1818e1004a3254\",\"title\":\"Learning to Generalize from Sparse and Underspecified Rewards\",\"url\":\"https://www.semanticscholar.org/paper/84b888b66d598f4fd5909c524a1818e1004a3254\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66180619\",\"name\":\"OctoMiao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"223af46e6193561b406dd4956b6df3087c502349\",\"title\":\"Overcoming catastrophic forgetting in neural networks\",\"url\":\"https://www.semanticscholar.org/paper/223af46e6193561b406dd4956b6df3087c502349\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1506.02438\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"title\":\"High-Dimensional Continuous Control Using Generalized Advantage Estimation\",\"url\":\"https://www.semanticscholar.org/paper/d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1809.03702\",\"authors\":[{\"authorId\":\"145604319\",\"name\":\"N. Ke\"},{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"2361575\",\"name\":\"Olexa Bilaniuk\"},{\"authorId\":\"1737610\",\"name\":\"Jonathan Binas\"},{\"authorId\":\"144473519\",\"name\":\"M. Mozer\"},{\"authorId\":\"1972076\",\"name\":\"C. Pal\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60\",\"title\":\"Sparse Attentive Backtracking: Temporal CreditAssignment Through Reminding\",\"url\":\"https://www.semanticscholar.org/paper/fa0beb3f4d7f6e7e49b153af7e8a7c30f2937b60\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143931536\",\"name\":\"J. Howard\"},{\"authorId\":\"2884561\",\"name\":\"Sebastian Ruder\"}],\"doi\":\"10.18653/v1/P18-1031\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1e077413b25c4d34945cc2707e17e46ed4fe784a\",\"title\":\"Universal Language Model Fine-tuning for Text Classification\",\"url\":\"https://www.semanticscholar.org/paper/1e077413b25c4d34945cc2707e17e46ed4fe784a\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":\"1901.10995\",\"authors\":[{\"authorId\":\"66821245\",\"name\":\"Adrien Ecoffet\"},{\"authorId\":\"39378983\",\"name\":\"J. Huizinga\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c520bf47db3360ae3a52219771390a354ed8a91f\",\"title\":\"Go-Explore: a New Approach for Hard-Exploration Problems\",\"url\":\"https://www.semanticscholar.org/paper/c520bf47db3360ae3a52219771390a354ed8a91f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1803.03835\",\"authors\":[{\"authorId\":\"145063335\",\"name\":\"Simon Schmitt\"},{\"authorId\":\"27866722\",\"name\":\"J. Hudson\"},{\"authorId\":\"40501144\",\"name\":\"A. Z\\u00eddek\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"2786693\",\"name\":\"C. Doersch\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"3373807\",\"name\":\"Heinrich K\\u00fcttler\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ebf3b0c284cb776d89951e4e67a59df6403fc9a6\",\"title\":\"Kickstarting Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ebf3b0c284cb776d89951e4e67a59df6403fc9a6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1706.03762\",\"authors\":[{\"authorId\":\"40348417\",\"name\":\"Ashish Vaswani\"},{\"authorId\":\"1846258\",\"name\":\"Noam Shazeer\"},{\"authorId\":\"3877127\",\"name\":\"Niki Parmar\"},{\"authorId\":\"39328010\",\"name\":\"Jakob Uszkoreit\"},{\"authorId\":\"145024664\",\"name\":\"Llion Jones\"},{\"authorId\":\"19177000\",\"name\":\"Aidan N. Gomez\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"3443442\",\"name\":\"Illia Polosukhin\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"title\":\"Attention is All you Need\",\"url\":\"https://www.semanticscholar.org/paper/204e3073870fae3d05bcbc2f6a8e263d9b72e776\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1806.07857\",\"authors\":[{\"authorId\":\"1409265458\",\"name\":\"Jose A. Arjona-Medina\"},{\"authorId\":\"2340851\",\"name\":\"Michael Gillhofer\"},{\"authorId\":\"1410855857\",\"name\":\"Michael Widrich\"},{\"authorId\":\"2465270\",\"name\":\"Thomas Unterthiner\"},{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"bad355642cd299caca2328dae02563278ea74e8c\",\"title\":\"RUDDER: Return Decomposition for Delayed Rewards\",\"url\":\"https://www.semanticscholar.org/paper/bad355642cd299caca2328dae02563278ea74e8c\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1409.3215\",\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cea967b59209c6be22829699f05b8b1ac4dc092d\",\"title\":\"Sequence to Sequence Learning with Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/cea967b59209c6be22829699f05b8b1ac4dc092d\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1809.04474\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"2311318\",\"name\":\"Lasse Espeholt\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"145063335\",\"name\":\"Simon Schmitt\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":\"10.1609/AAAI.V33I01.33013796\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65769b53e71ea7c52b3a07ad32bd4fdade6a0173\",\"title\":\"Multi-task Deep Reinforcement Learning with PopArt\",\"url\":\"https://www.semanticscholar.org/paper/65769b53e71ea7c52b3a07ad32bd4fdade6a0173\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jane X. Wang\"},{\"authorId\":null,\"name\":\"Zeb Kurth-Nelson\"},{\"authorId\":null,\"name\":\"Dhruva Tirumala\"},{\"authorId\":null,\"name\":\"Hubert Soyer\"},{\"authorId\":null,\"name\":\"Joel Z. Leibo\"},{\"authorId\":null,\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":null,\"name\":\"Charles Blundell\"},{\"authorId\":null,\"name\":\"Dharshan Kumaran\"},{\"authorId\":null,\"name\":\"Matthew Botvinick. Learning to reinforcement learn\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1611.05763,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10703690\",\"name\":\"Pei Wang\"},{\"authorId\":\"1738080\",\"name\":\"B. Goertzel\"}],\"doi\":\"10.2991/978-94-91216-62-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4bf6e3e588b2c1acc93f1f7ee5caa92a14a9323\",\"title\":\"Theoretical Foundations of Artificial General Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e4bf6e3e588b2c1acc93f1f7ee5caa92a14a9323\",\"venue\":\"Atlantis Thinking Machines\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738080\",\"name\":\"B. Goertzel\"},{\"authorId\":\"51029460\",\"name\":\"Matt Ikl\\u00e9\"},{\"authorId\":\"14766909\",\"name\":\"Jared Wigmore\"}],\"doi\":\"10.2991/978-94-91216-62-6_8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5b6b54bceb0dee62a26b3d662e70dd1f5cdbe339\",\"title\":\"The Architecture of Human-Like General Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/5b6b54bceb0dee62a26b3d662e70dd1f5cdbe339\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1604.02201\",\"authors\":[{\"authorId\":\"2368067\",\"name\":\"Barret Zoph\"},{\"authorId\":\"2808366\",\"name\":\"Deniz Yuret\"},{\"authorId\":\"143823227\",\"name\":\"Jonathan May\"},{\"authorId\":null,\"name\":\"Kevin Knight\"}],\"doi\":\"10.18653/v1/D16-1163\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6\",\"title\":\"Transfer Learning for Low-Resource Neural Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36337192\",\"name\":\"Tim de Bruin\"},{\"authorId\":\"145739642\",\"name\":\"J. Kober\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"},{\"authorId\":\"1705222\",\"name\":\"Robert Babu\\u0161ka\"}],\"doi\":\"10.1109/LRA.2018.2800101\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"84bb62e3f40434a1e367d24783bd81432a5396d6\",\"title\":\"Integrating State Representation Learning Into Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/84bb62e3f40434a1e367d24783bd81432a5396d6\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2018},{\"arxivId\":\"1810.02274\",\"authors\":[{\"authorId\":\"2417003\",\"name\":\"Nikolay Savinov\"},{\"authorId\":\"150918315\",\"name\":\"Anton Raichuk\"},{\"authorId\":\"151188653\",\"name\":\"Raphael Marinier\"},{\"authorId\":\"40412424\",\"name\":\"D. Vincent\"},{\"authorId\":\"1742208\",\"name\":\"M. Pollefeys\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1802148\",\"name\":\"S. Gelly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71\",\"title\":\"Episodic Curiosity through Reachability\",\"url\":\"https://www.semanticscholar.org/paper/fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3414570\",\"name\":\"N. Mishra\"},{\"authorId\":\"22222033\",\"name\":\"Mostafa Rohaninejad\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e9c1e0d247b20a0683f4797d9ea248c3b53d424\",\"title\":\"A Simple Neural Attentive Meta-Learner\",\"url\":\"https://www.semanticscholar.org/paper/7e9c1e0d247b20a0683f4797d9ea248c3b53d424\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1810.04805\",\"authors\":[{\"authorId\":\"39172707\",\"name\":\"J. Devlin\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"3259253\",\"name\":\"Kristina Toutanova\"}],\"doi\":\"10.18653/v1/N19-1423\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\"title\":\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\",\"url\":\"https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992\",\"venue\":\"NAACL-HLT\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhouhan Lin\"},{\"authorId\":null,\"name\":\"Minwei Feng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"C\\u0131\\u0301cero Nogueira dos Santos\",\"url\":\"\",\"venue\":\"Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. In ICLR,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. D. Co-Reyes\"},{\"authorId\":null,\"name\":\"A. Gupta\"},{\"authorId\":null,\"name\":\"S. Sanjeev\"},{\"authorId\":null,\"name\":\"N. Altieri\"},{\"authorId\":null,\"name\":\"J. DeNero\"},{\"authorId\":null,\"name\":\"P. Abbeel\"},{\"authorId\":null,\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Metalearning language-guided policy learning\",\"url\":\"\",\"venue\":\"Proceedings of the International Conference on Learning Representations (ICLR 2019).\",\"year\":2019},{\"arxivId\":\"1606.04671\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"3422052\",\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"title\":\"Progressive Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1007/BF00992699\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"title\":\"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\"url\":\"https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Z. Lin\"},{\"authorId\":null,\"name\":\"M. Feng\"},{\"authorId\":null,\"name\":\"C. N. dos Santos\"},{\"authorId\":null,\"name\":\"M. Yu\"},{\"authorId\":null,\"name\":\"B. Xiang\"},{\"authorId\":null,\"name\":\"B. Zhou\"},{\"authorId\":null,\"name\":\"Y. Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A structured selfattentive sentence embedding\",\"url\":\"\",\"venue\":\"In Proceedings of the International Conference on Learning Representations (ICLR\",\"year\":2017},{\"arxivId\":\"1707.04175\",\"authors\":[{\"authorId\":\"1725303\",\"name\":\"Y. Teh\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"title\":\"Distral: Robust multitask reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1802.05365\",\"authors\":[{\"authorId\":\"39139825\",\"name\":\"Matthew E. Peters\"},{\"authorId\":\"50043859\",\"name\":\"Mark Neumann\"},{\"authorId\":\"2136562\",\"name\":\"Mohit Iyyer\"},{\"authorId\":\"40642935\",\"name\":\"Matt Gardner\"},{\"authorId\":\"143997772\",\"name\":\"Christopher Clark\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":\"10.18653/v1/N18-1202\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3febb2bed8865945e7fddc99efd791887bb7e14f\",\"title\":\"Deep contextualized word representations\",\"url\":\"https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":\"1804.00379\",\"authors\":[{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"2616163\",\"name\":\"Philemon Brakel\"},{\"authorId\":\"26958176\",\"name\":\"W. Fedus\"},{\"authorId\":\"1466543874\",\"name\":\"Soumye Singhal\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29aab768e642588352134a03c0368e1bdc1f1e8d\",\"title\":\"Recall Traces: Backtracking Models for Efficient Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/29aab768e642588352134a03c0368e1bdc1f1e8d\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1511.06342\",\"authors\":[{\"authorId\":\"3166516\",\"name\":\"Emilio Parisotto\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"title\":\"Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1802.04821\",\"authors\":[{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"2896187\",\"name\":\"Richard Y. Chen\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"3275284\",\"name\":\"Bradly C. Stadie\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"73735391\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c9660db0c94edfb2b1f3ab4f08eb80acd83a1c07\",\"title\":\"Evolved Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/c9660db0c94edfb2b1f3ab4f08eb80acd83a1c07\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Charles Beattie\"},{\"authorId\":null,\"name\":\"Joel Z Leibo\"},{\"authorId\":null,\"name\":\"Denis Teplyashin\"},{\"authorId\":null,\"name\":\"Tom Ward\"},{\"authorId\":null,\"name\":\"Marcus Wainwright\"},{\"authorId\":null,\"name\":\"Heinrich K\\u00fcttler\"},{\"authorId\":null,\"name\":\"Andrew Lefrancq\"},{\"authorId\":null,\"name\":\"Simon Green\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"V\\u0131\\u0301ctor Vald\\u00e9s\",\"url\":\"\",\"venue\":\"Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrei A. Rusu\"},{\"authorId\":null,\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":null,\"name\":\"Guillaume Desjardins\"},{\"authorId\":null,\"name\":\"Hubert Soyer\"},{\"authorId\":null,\"name\":\"James Kirkpatrick\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"Razvan Pascanu\"},{\"authorId\":null,\"name\":\"Raia Hadsell. Progressive neural networks\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1606.04671,\",\"year\":2016},{\"arxivId\":\"1611.05763\",\"authors\":[{\"authorId\":\"13256987\",\"name\":\"J. X. Wang\"},{\"authorId\":\"1399114225\",\"name\":\"Z. Kurth-Nelson\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"7794353\",\"name\":\"Dhruva Tirumala\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1409234386\",\"name\":\"Matt M. Botvinick\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"282a380fb5ac26d99667224cef8c630f6882704f\",\"title\":\"Learning to reinforcement learn\",\"url\":\"https://www.semanticscholar.org/paper/282a380fb5ac26d99667224cef8c630f6882704f\",\"venue\":\"CogSci\",\"year\":2017}],\"title\":\"Self-Attentional Credit Assignment for Transfer in Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Online and offline\",\"topicId\":\"12094\",\"url\":\"https://www.semanticscholar.org/topic/12094\"}],\"url\":\"https://www.semanticscholar.org/paper/ca37f9f7f03f9ee463d6dcfd84019f1fcd314b2c\",\"venue\":\"\",\"year\":2019}\n"