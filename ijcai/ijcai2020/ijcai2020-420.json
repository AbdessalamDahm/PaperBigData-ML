"{\"abstract\":\"This paper aims to learn multi-agent cooperation where each agent performs its actions in a decentralized way. In this case, it is very challenging to learn decentralized policies when the rewards are global and sparse. Recently, learning from demonstrations (LfD) provides a promising way to handle this challenge. However, in many practical tasks, the available demonstrations are often sub-optimal. To learn better policies from these sub-optimal demonstrations, this paper follows a centralized learning and decentralized execution framework and proposes a novel hybrid learning method based on multi-agent actor-critic. At first, the expert trajectory returns generated from demonstration actions are used to pre-train the centralized critic network. Then, multi-agent decisions are made by best response dynamics based on the critic and used to train the decentralized actor networks. Finally, the demonstrations are updated by the actor networks, and the critic and actor networks are learned jointly by running the above two steps alliteratively. We evaluate the proposed approach on a real-time strategy combat game. Experimental results show that the approach outperforms many competing demonstration-based methods.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2200852\",\"name\":\"Peixi Peng\",\"url\":\"https://www.semanticscholar.org/author/2200852\"},{\"authorId\":\"1757173\",\"name\":\"Junliang Xing\",\"url\":\"https://www.semanticscholar.org/author/1757173\"},{\"authorId\":\"50260540\",\"name\":\"L. Cao\",\"url\":\"https://www.semanticscholar.org/author/50260540\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":220483009,\"doi\":\"10.24963/ijcai.2020/420\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":false,\"paperId\":\"e6029a0c94d019a851162e6268fbaa3cfb57f040\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2327581\",\"name\":\"Eirini Kaldeli\"},{\"authorId\":\"1766996\",\"name\":\"A. Lazovik\"},{\"authorId\":\"1747132\",\"name\":\"Marco Aiello\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"533214fbe2ff4d2532b050a9c2dbffaa9f356eec\",\"title\":\"AAAI Conference on Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/533214fbe2ff4d2532b050a9c2dbffaa9f356eec\",\"venue\":\"AAAI 2011\",\"year\":2011},{\"arxivId\":\"1810.06339\",\"authors\":[{\"authorId\":\"2276894\",\"name\":\"Yuxi Li\"}],\"doi\":\"10.1201/9781351006620-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"title\":\"Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Hu\"},{\"authorId\":null,\"name\":\"Juntao Li\"},{\"authorId\":null,\"name\":\"Xi Li\"},{\"authorId\":null,\"name\":\"Gang Pan\"},{\"authorId\":null,\"name\":\"Mingliang Xu. Knowledge-guided agent-tactic-aware learning f micromanagement\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In International Joint Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 1471\\u2013 1477,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"7515048\",\"name\":\"Matej Vecer\\u00edk\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2533110\",\"name\":\"A. Sendonaris\"},{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1387885286\",\"name\":\"Gabriel Dulac-Arnold\"},{\"authorId\":\"70495322\",\"name\":\"J. Agapiou\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e3b0ea7209731c47b582215c6c67f9c691ad9863\",\"title\":\"Deep Q-learning From Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/e3b0ea7209731c47b582215c6c67f9c691ad9863\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1707.08817\",\"authors\":[{\"authorId\":\"7515048\",\"name\":\"Matej Vecer\\u00edk\"},{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"36881095\",\"name\":\"Jonathan Scholz\"},{\"authorId\":\"2555447\",\"name\":\"F. Wang\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"8282805\",\"name\":\"Thomas Roth\\u00f6rl\"},{\"authorId\":\"46534085\",\"name\":\"T. Lampe\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1bead9000a719cb258bac7320228055aee650d2c\",\"title\":\"Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards\",\"url\":\"https://www.semanticscholar.org/paper/1bead9000a719cb258bac7320228055aee650d2c\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xiaomin Lin\"},{\"authorId\":null,\"name\":\"Peter A. Beling\"},{\"authorId\":null,\"name\":\"Randy Cogill. Multiagent inverse reinforcement learning  games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"IEEE Transactions on Games\",\"url\":\"\",\"venue\":\"10(1):56\\u201368,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1799949\",\"name\":\"Frans A. Oliehoek\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"}],\"doi\":\"10.1007/978-3-319-28929-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"836ad0c693bc5ef171ee2b07b3f4d1bd2a0ae24c\",\"title\":\"A Concise Introduction to Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/836ad0c693bc5ef171ee2b07b3f4d1bd2a0ae24c\",\"venue\":\"SpringerBriefs in Intelligent Systems\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrew Y. Ng\"},{\"authorId\":null,\"name\":\"Stuart J. Russell. Algorithms for inverse reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 663\\u2013670,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2324546\",\"name\":\"Levi H. S. Lelis\"}],\"doi\":\"10.24963/ijcai.2017/522\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99ecef89ee0d8d74564a94b4ccc55f5acc1a3bef\",\"title\":\"Stratified Strategy Selection for Unit Control in Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/99ecef89ee0d8d74564a94b4ccc55f5acc1a3bef\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46636184\",\"name\":\"P. Peng\"},{\"authorId\":\"145001851\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ryan Lowe\"},{\"authorId\":null,\"name\":\"Yi Wu\"},{\"authorId\":null,\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Beling , and Randy Cogill . Multiagent inverse reinforcement learning for two - person zero - sum games\",\"url\":\"\",\"venue\":\"IEEE Transactions on Games\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1772287\",\"name\":\"Abdallah Saffidine\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"168c6eb518d4db46ad49912672c3bf5ea5b66b8a\",\"title\":\"Fast Heuristic Search for RTS Game Combat Scenarios\",\"url\":\"https://www.semanticscholar.org/paper/168c6eb518d4db46ad49912672c3bf5ea5b66b8a\",\"venue\":\"AIIDE\",\"year\":2012},{\"arxivId\":\"1111.0062\",\"authors\":[{\"authorId\":\"1799949\",\"name\":\"Frans A. Oliehoek\"},{\"authorId\":\"1723205\",\"name\":\"M. Spaan\"},{\"authorId\":\"31651045\",\"name\":\"N. Vlassis\"}],\"doi\":\"10.1613/jair.2447\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"19ee8bd6b7ba5ddefa723a018271002705018815\",\"title\":\"Optimal and Approximate Q-value Functions for Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/19ee8bd6b7ba5ddefa723a018271002705018815\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Aja Huang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Stefan Schaal . Learning from demonstration\",\"url\":\"\",\"venue\":\"Advances in Neural Information Processing Systems\",\"year\":null},{\"arxivId\":\"1807.09936\",\"authors\":[{\"authorId\":\"51453887\",\"name\":\"Jiaming Song\"},{\"authorId\":\"40046694\",\"name\":\"H. Ren\"},{\"authorId\":\"1779671\",\"name\":\"D. Sadigh\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aeeecd282e9a7065f37ca2246ca711264493041c\",\"title\":\"Multi-Agent Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/aeeecd282e9a7065f37ca2246ca711264493041c\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35147280\",\"name\":\"D. T. Nguyen\"},{\"authorId\":\"40305195\",\"name\":\"Akshat Kumar\"},{\"authorId\":\"1725253\",\"name\":\"H. C. Lau\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3ecc35641a8d104210f70d932a37c30040cc9eff\",\"title\":\"Credit Assignment For Collective Multiagent RL With Global Rewards\",\"url\":\"https://www.semanticscholar.org/paper/3ecc35641a8d104210f70d932a37c30040cc9eff\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98241663\",\"name\":\"M. V. Rossum\"}],\"doi\":\"10.1142/9789814360784_0003\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"title\":\"Neural Computation\",\"url\":\"https://www.semanticscholar.org/paper/2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"venue\":\"\",\"year\":1989},{\"arxivId\":\"1703.01030\",\"authors\":[{\"authorId\":\"144426657\",\"name\":\"Wen Sun\"},{\"authorId\":\"1978198\",\"name\":\"A. Venkatraman\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"3288815\",\"name\":\"B. Boots\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72a3da7491ebc09e319167dba4b2cdb1d285bcdc\",\"title\":\"Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction\",\"url\":\"https://www.semanticscholar.org/paper/72a3da7491ebc09e319167dba4b2cdb1d285bcdc\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1712542\",\"name\":\"T. Roughgarden\"},{\"authorId\":\"144144899\",\"name\":\"Kazuo Iwama\"}],\"doi\":\"10.1017/cbo9781316779309\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2be1e8dbab03522c2f5eff0aa63afff2fd610ff7\",\"title\":\"Twenty Lectures on Algorithmic Game Theory\",\"url\":\"https://www.semanticscholar.org/paper/2be1e8dbab03522c2f5eff0aa63afff2fd610ff7\",\"venue\":\"Bull. EATCS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Frans A Oliehoek\"},{\"authorId\":null,\"name\":\"Matthijs TJ Spaan\"},{\"authorId\":null,\"name\":\"Nikos Vlassis. Optimal\"},{\"authorId\":null,\"name\":\"approximate q-value functions for decentralized pomdps\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"32(1):289\\u2013353,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Churchill\"},{\"authorId\":null,\"name\":\"Michael Buro. Portfolio greedy search\"},{\"authorId\":null,\"name\":\"simulation for largescale combat in StarCraft\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IEEE\\u2019s Conference on Computational Intelligence in Games\",\"url\":\"\",\"venue\":\"pages 1\\u20138,\",\"year\":2013},{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1609.07845\",\"authors\":[{\"authorId\":\"40650191\",\"name\":\"Y. Chen\"},{\"authorId\":\"46331912\",\"name\":\"M. Liu\"},{\"authorId\":\"144019526\",\"name\":\"Michael Everett\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"}],\"doi\":\"10.1109/ICRA.2017.7989037\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"76dceabf5f6d3dcedc89746046b42009375978bc\",\"title\":\"Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/76dceabf5f6d3dcedc89746046b42009375978bc\",\"venue\":\"2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Hu\"},{\"authorId\":null,\"name\":\"Juntao Li\"},{\"authorId\":\"50079147\",\"name\":\"X. Li\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"},{\"authorId\":\"2285442\",\"name\":\"M. Xu\"}],\"doi\":\"10.24963/ijcai.2018/204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3416cb7c3462cfc52782c26900e912c46a2bd1c9\",\"title\":\"Knowledge-Guided Agent-Tactic-Aware Learning for StarCraft Micromanagement\",\"url\":\"https://www.semanticscholar.org/paper/3416cb7c3462cfc52782c26900e912c46a2bd1c9\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kagan Tumer\"},{\"authorId\":null,\"name\":\"Adrian K. Agogino. Distributed agent-based air traffic fl management\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Autonomous Agents and Multiagent Systems\",\"url\":\"\",\"venue\":\"pages 1\\u20138,\",\"year\":2007},{\"arxivId\":\"1606.03476\",\"authors\":[{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"title\":\"Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Stephane Ross\"},{\"authorId\":null,\"name\":\"Geoffrey J. Gordon\"},{\"authorId\":null,\"name\":\"J. Andrew Bagnell. A reduction of imitation learning\"},{\"authorId\":null,\"name\":\"structured prediction to no-regret online learning. In Intern Intelligence\"},{\"authorId\":null,\"name\":\"Statistics\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 15\",\"url\":\"\",\"venue\":\"pages 627\\u2013635,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2677378\",\"name\":\"Martin Hjelm\"},{\"authorId\":\"2484138\",\"name\":\"C. Ek\"},{\"authorId\":\"40143841\",\"name\":\"R. Detry\"},{\"authorId\":\"1704879\",\"name\":\"H. Kjellstr\\u00f6m\"},{\"authorId\":\"1731490\",\"name\":\"D. Kragic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0b7c26554f8ebd9889719bb560d6f78bb74b8d3b\",\"title\":\"International Conference on Robotics and Automation\",\"url\":\"https://www.semanticscholar.org/paper/0b7c26554f8ebd9889719bb560d6f78bb74b8d3b\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Churchill\"},{\"authorId\":null,\"name\":\"Abdallah Saffidine\"},{\"authorId\":null,\"name\":\"Michael Buro. Fast heuristic search for RTS game combat scenarios\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Artificial Intelligence and Interactive Digital Entertainment Conference\",\"url\":\"\",\"venue\":\"pages 112\\u2013117,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"References\"},{\"authorId\":null,\"name\":\"Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Xiaomin Lin, Peter A. Beling, and Randy Cogill. Multiagent inverse reinforcement learning for twoperson zero-sum games\",\"url\":\"\",\"venue\":\"Duc Thien Nguyen, Akshat Kumar, and Hoong Chuin Lau. Credit assignment for collective multiagent rl with global rewards\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gupta et al\"},{\"authorId\":null,\"name\":\"2017 Jayesh K Gupta\"},{\"authorId\":null,\"name\":\"Maxim Egorov\"},{\"authorId\":null,\"name\":\"Mykel Kochenderfer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cooperative multi-agent control\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145847467\",\"name\":\"D. Pomerleau\"}],\"doi\":\"10.1162/neco.1991.3.1.88\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d652a1980e743c7c85ff6066409ea1e3be4d685\",\"title\":\"Efficient Training of Artificial Neural Networks for Autonomous Navigation\",\"url\":\"https://www.semanticscholar.org/paper/8d652a1980e743c7c85ff6066409ea1e3be4d685\",\"venue\":\"Neural Computation\",\"year\":1991},{\"arxivId\":\"1706.02275\",\"authors\":[{\"authorId\":\"2054294\",\"name\":\"Ryan Lowe\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2080746\",\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"title\":\"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\"url\":\"https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Duc Thien Nguyen\"},{\"authorId\":null,\"name\":\"Akshat Kumar\"},{\"authorId\":null,\"name\":\"Hoong Chuin Lau\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Russell . Algorithms for inverse reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Levi H.S. Lelis. Stratified strategy selection for unit games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Joint Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 3735\\u20133741,\",\"year\":2017}],\"title\":\"Hybrid Learning for Multi-agent Cooperation with Sub-optimal Demonstrations\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/e6029a0c94d019a851162e6268fbaa3cfb57f040\",\"venue\":\"IJCAI\",\"year\":2020}\n"