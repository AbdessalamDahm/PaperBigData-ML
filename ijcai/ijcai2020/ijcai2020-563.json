"{\"abstract\":\"The formal synthesis of automated or autonomous agents has elicited strong interest from the artificial intelligence community in recent years. This problem space broadly entails the derivation of decision-making policies for agents acting in an environment such that a formal specification of behavior is satisfied. Popular formalisms for such specifications include the quintessential Linear Temporal Logic (LTL) and Computation Tree Logic (CTL) which reason over infinite sequences and trees, respectively, of states. However, the related and relevant problem of reasoning over the frequency with which states are visited infinitely and enforcing behavioral specifications on the same has received little attention. That problem, known as Steady-State Policy Synthesis (SSPS) or steadystate control, is the focus of this paper. Prior related work has been mostly confined to unichain Markov Decision Processes (MDPs), while a tractable solution to the general multichain setting heretofore remains elusive. In this paper, we provide a solution to the latter within the context of multichain MDPs over a class of policies that account for all possible transitions in the given MDP. The solution policy is derived from a novel linear program (LP) that encodes constraints on the limiting distributions of the Markov chain induced by said policy. We establish a one-to-one correspondence between the feasible solutions of the LP and the stationary distributions of the induced Markov chains. The derived policy is shown to maximize the reward among the constrained class of stationary policies and to satisfy the specification constraints even when it does not exercise all possible transitions.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"46893530\",\"name\":\"George K. Atia\",\"url\":\"https://www.semanticscholar.org/author/46893530\"},{\"authorId\":\"22352369\",\"name\":\"Andre Beckus\",\"url\":\"https://www.semanticscholar.org/author/22352369\"},{\"authorId\":\"69339617\",\"name\":\"Ismail Alkhouri\",\"url\":\"https://www.semanticscholar.org/author/69339617\"},{\"authorId\":\"145720050\",\"name\":\"A. Velasquez\",\"url\":\"https://www.semanticscholar.org/author/145720050\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"46893530\",\"name\":\"George K. Atia\"},{\"authorId\":\"22352369\",\"name\":\"Andre Beckus\"},{\"authorId\":\"69339617\",\"name\":\"Ismail Alkhouri\"},{\"authorId\":\"145720050\",\"name\":\"A. Velasquez\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"b8781f7339ee5996066ceb36fa3dd6112a3c0230\",\"title\":\"Verifiable Planning in Expected Reward Multichain MDPs Verifiable Planning in Expected Reward Multichain MDPs\",\"url\":\"https://www.semanticscholar.org/paper/b8781f7339ee5996066ceb36fa3dd6112a3c0230\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.02178\",\"authors\":[{\"authorId\":\"46893530\",\"name\":\"George K. Atia\"},{\"authorId\":\"22352369\",\"name\":\"Andre Beckus\"},{\"authorId\":\"69339617\",\"name\":\"Ismail Alkhouri\"},{\"authorId\":\"145720050\",\"name\":\"A. Velasquez\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"e9edf0321f168697ee26858dcb90a2e162342e89\",\"title\":\"Verifiable Planning in Expected Reward Multichain MDPs\",\"url\":\"https://www.semanticscholar.org/paper/e9edf0321f168697ee26858dcb90a2e162342e89\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":220485127,\"doi\":\"10.24963/ijcai.2020/563\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":false,\"paperId\":\"1a6d9762ee3c4c1b109c22e22ce5a6e665436fdb\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387890355\",\"name\":\"H. Kress-Gazit\"},{\"authorId\":\"145428350\",\"name\":\"M. Lahijanian\"},{\"authorId\":\"37209447\",\"name\":\"V. Raman\"}],\"doi\":\"10.1146/ANNUREV-CONTROL-060117-104838\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"811445a434f370327e7068f8295b12ad8638f356\",\"title\":\"Synthesis for Robots: Guarantees and Feedback for Robot Behavior\",\"url\":\"https://www.semanticscholar.org/paper/811445a434f370327e7068f8295b12ad8638f356\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113713833\",\"name\":\"S. J. Crawford\"},{\"authorId\":\"116971860\",\"name\":\"D. F. Kelley\"},{\"authorId\":\"137736247\",\"name\":\"In\\u00e9s Hern\\u00e1ndez \\u00c1vila\"}],\"doi\":\"10.1023/A:1017127612903\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"title\":\"Volume 1\",\"url\":\"https://www.semanticscholar.org/paper/6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sundararaman Akshay\"},{\"authorId\":null,\"name\":\"Nathalie Bertrand\"},{\"authorId\":null,\"name\":\"Serge Haddad\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and Lo\\u0131\\u0308c H\\u00e9lou\\u00ebt\",\"url\":\"\",\"venue\":\"The steadystate control problem for Markov decision processes. In Int. Conf. Quant. Eval., pages 290\\u2013304, Berlin Heidelberg,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"E. A. Feinberg. Adaptive computation of optimal nonra Programming\"},{\"authorId\":null,\"name\":\"Reinforcement Learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 96\\u2013100\",\"url\":\"\",\"venue\":\"March\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael Fisher\"},{\"authorId\":null,\"name\":\"Louise A Dennis\"},{\"authorId\":null,\"name\":\"Matthew P Webster. Verifying autonomous systems. Commun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ACM\",\"url\":\"\",\"venue\":\"56(9):84\\u201393,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102102473\",\"name\":\"M. Boussemart\"},{\"authorId\":\"30937542\",\"name\":\"N. Limnios\"},{\"authorId\":\"25590125\",\"name\":\"J. Fillion\"}],\"doi\":\"10.1081/STM-120002780\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8dddaccc7c16addb743a7d597c8841b0e3e0c8a6\",\"title\":\"Non-ergodic Markov decision processes with a constraint on the asymptotic failure rate: general class of policies\",\"url\":\"https://www.semanticscholar.org/paper/8dddaccc7c16addb743a7d597c8841b0e3e0c8a6\",\"venue\":\"\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1829862\",\"name\":\"K. Ross\"}],\"doi\":\"10.1287/opre.37.3.474\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"48fc23855beef6cbcea227bdfa0e810a856c95b9\",\"title\":\"Randomized and Past-Dependent Policies for Markov Decision Processes with Multiple Constraints\",\"url\":\"https://www.semanticscholar.org/paper/48fc23855beef6cbcea227bdfa0e810a856c95b9\",\"venue\":\"Oper. Res.\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. Tarjan. Depth-first search\"},{\"authorId\":null,\"name\":\"linear graph algorithms\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 12th Annual Symposium on Switching and Automata Theory\",\"url\":\"\",\"venue\":\"pages 114\\u2013121, Oct\",\"year\":1971},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47570159\",\"name\":\"P. Antsaklis\"},{\"authorId\":\"6954585\",\"name\":\"E. Kovacs\"},{\"authorId\":\"1744311\",\"name\":\"E. Chong\"},{\"authorId\":\"8564504\",\"name\":\"J. Grizzle\"},{\"authorId\":\"144303180\",\"name\":\"M. Krsti\\u0107\"},{\"authorId\":\"2700298\",\"name\":\"J. Spall\"},{\"authorId\":\"65826872\",\"name\":\"Y. Amamoto\"},{\"authorId\":\"2460665\",\"name\":\"D. Arzelier\"},{\"authorId\":\"144774377\",\"name\":\"A. Astolfi\"},{\"authorId\":\"1765717\",\"name\":\"J. Braslavsky\"},{\"authorId\":\"3166766\",\"name\":\"H. Chang\"},{\"authorId\":\"1708946\",\"name\":\"X. Chen\"},{\"authorId\":\"66578426\",\"name\":\"Shibaura Inst\"},{\"authorId\":\"1689638\",\"name\":\"A. Chiuso\"},{\"authorId\":\"1707671\",\"name\":\"J. Daafouz\"},{\"authorId\":\"2929862\",\"name\":\"F. Dabbene\"},{\"authorId\":\"50345162\",\"name\":\"G. Dullerud\"},{\"authorId\":\"152599370\",\"name\":\"M. Egerstedt\"},{\"authorId\":\"49815118\",\"name\":\"E. Fabre\"},{\"authorId\":\"144101313\",\"name\":\"A. Ferrara\"},{\"authorId\":\"31217167\",\"name\":\"H. Ishii\"},{\"authorId\":\"48082394\",\"name\":\"M. James\"},{\"authorId\":\"145565795\",\"name\":\"A. Loria\"},{\"authorId\":\"1789590\",\"name\":\"M. Malisoff\"},{\"authorId\":\"145581280\",\"name\":\"H. Marchand\"},{\"authorId\":\"65763790\",\"name\":\"Rennes-Bretagne Atlantique\"},{\"authorId\":\"48319064\",\"name\":\"L. Marconi\"},{\"authorId\":\"144939962\",\"name\":\"Kirsten Morris\"},{\"authorId\":\"1752117\",\"name\":\"F. Paganini\"},{\"authorId\":\"2193683\",\"name\":\"M. Prandini\"},{\"authorId\":\"1962798\",\"name\":\"S. Reveliotis\"},{\"authorId\":\"17722135\",\"name\":\"L. Schenato\"},{\"authorId\":\"1775285\",\"name\":\"H. Trentelman\"},{\"authorId\":\"122244178\",\"name\":\"Z. Wang\"},{\"authorId\":\"3064053\",\"name\":\"E. Weyer\"},{\"authorId\":\"2367263\",\"name\":\"J. Winkin\"}],\"doi\":\"10.1109/tac.9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"db0760d59f189a5906e0d2b431eeaec31842ddb5\",\"title\":\"IEEE TRANSACTIONS ON AUTOMATIC CONTROL\",\"url\":\"https://www.semanticscholar.org/paper/db0760d59f189a5906e0d2b431eeaec31842ddb5\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145720050\",\"name\":\"A. Velasquez\"}],\"doi\":\"10.24963/ijcai.2019/784\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"db3d2bf5051dcad25af311e660b98e1311d58a96\",\"title\":\"Steady-State Policy Synthesis for Verifiable Control\",\"url\":\"https://www.semanticscholar.org/paper/db3d2bf5051dcad25af311e660b98e1311d58a96\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Guy De Ghellinck. Les probl\\u00e8mes de d\\u00e9cisions s\\u00e9quentielles\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cahiers du Centre d\\u2019Etudes de Recherche Op\\u00e9rationnelle\",\"url\":\"\",\"venue\":\"2(2):161\\u2013179,\",\"year\":1960},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31764087\",\"name\":\"M. Wilkinson\"}],\"doi\":\"10.1038/sj.bdj.4806810\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70f6f070f45afdd58b85cafde3a2e9ea02f3fcdb\",\"title\":\"Management science\",\"url\":\"https://www.semanticscholar.org/paper/70f6f070f45afdd58b85cafde3a2e9ea02f3fcdb\",\"venue\":\"British Dental Journal\",\"year\":1989},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.1145/3388831\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e73c443da9d2d06b308a147898ab190fabb7928a\",\"title\":\"Proceedings of the 13th EAI International Conference on Performance Evaluation Methodologies and Tools\",\"url\":\"https://www.semanticscholar.org/paper/e73c443da9d2d06b308a147898ab190fabb7928a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152175853\",\"name\":\"L. Goddard\"}],\"doi\":\"10.1038/222304c0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"60ec5ddae05190537c72974b8f93beb46b8db857\",\"title\":\"Operations Research\",\"url\":\"https://www.semanticscholar.org/paper/60ec5ddae05190537c72974b8f93beb46b8db857\",\"venue\":\"Nature\",\"year\":1969},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102102473\",\"name\":\"M. Boussemart\"},{\"authorId\":\"30937542\",\"name\":\"N. Limnios\"}],\"doi\":\"10.1081/STA-120037268\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2cadab38ffceaa6fac79ca6901edc3f6d4e6a06\",\"title\":\"Markov Decision Processes with Asymptotic Average Failure Rate Constraint\",\"url\":\"https://www.semanticscholar.org/paper/a2cadab38ffceaa6fac79ca6901edc3f6d4e6a06\",\"venue\":\"\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34781713\",\"name\":\"A. Lazar\"}],\"doi\":\"10.1109/TAC.1983.1103166\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"18ef7d3c4ac68fbba8428bac3d7f7bb633c31d43\",\"title\":\"Optimal flow control of a class of queueing networks in equilibrium\",\"url\":\"https://www.semanticscholar.org/paper/18ef7d3c4ac68fbba8428bac3d7f7bb633c31d43\",\"venue\":\"\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joseph K Skwirzynski. New concepts in multi-user communication\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 43\",\"url\":\"\",\"venue\":\"Springer Science & Business Media,\",\"year\":1981},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"De Ghellinck\"},{\"authorId\":null,\"name\":\"Guy De Ghellinck\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Les probl\\u00e8mes de d\\u00e9cisions s\\u00e9quentielles\",\"url\":\"\",\"venue\":\"Finite State Markovian Decision Processes\",\"year\":1960},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143683893\",\"name\":\"S. Bhatnagar\"},{\"authorId\":\"33578174\",\"name\":\"K. Lakshmanan\"}],\"doi\":\"10.1007/s10957-012-9989-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f332ecd5d54adf0530a39dae189cf6b160ad5c0e\",\"title\":\"An Online Actor\\u2013Critic Algorithm with Function Approximation for Constrained Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/f332ecd5d54adf0530a39dae189cf6b160ad5c0e\",\"venue\":\"J. Optim. Theory Appl.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alvaro Velasquez. Steady-state policy synthesis for verif control\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 28th International Joint Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 5653\\u20135661. AAAI Press,\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143805379\",\"name\":\"Michael Fisher\"},{\"authorId\":\"1751408\",\"name\":\"L. Dennis\"},{\"authorId\":\"35022905\",\"name\":\"M. P. Webster\"}],\"doi\":\"10.1145/2494558\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4c3fa991eec79fa41aa6d79ccf8bbd54817f1a8\",\"title\":\"Verifying autonomous systems\",\"url\":\"https://www.semanticscholar.org/paper/e4c3fa991eec79fa41aa6d79ccf8bbd54817f1a8\",\"venue\":\"CACM\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1721050\",\"name\":\"R. Tarjan\"}],\"doi\":\"10.1137/0201010\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"385742fffcf113656f0d3cf6c06ef95cb8439dc6\",\"title\":\"Depth-First Search and Linear Graph Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/385742fffcf113656f0d3cf6c06ef95cb8439dc6\",\"venue\":\"SIAM J. Comput.\",\"year\":1972},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eitan Altman. Constrained Markov decision processes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CRC Press\",\"url\":\"\",\"venue\":\"Boca Raton,\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50040640\",\"name\":\"J. S. Edwards\"}],\"doi\":\"10.1057/jors.1983.218\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"54d70dd3b97eb317644851bb527eb2d4b3ff25d5\",\"title\":\"Linear Programming and Finite Markovian Control Problems\",\"url\":\"https://www.semanticscholar.org/paper/54d70dd3b97eb317644851bb527eb2d4b3ff25d5\",\"venue\":\"\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sofie Haesaert\"},{\"authorId\":null,\"name\":\"Alessandro Abate\"},{\"authorId\":null,\"name\":\"Paul MJ Van den Hof. Correct-by-design output feedback  systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 54th IEEE Conference on Decision and Control (CDC)\",\"url\":\"\",\"venue\":\"pages 6159\\u20136164,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3064175\",\"name\":\"S. Haesaert\"},{\"authorId\":\"144938187\",\"name\":\"A. Abate\"},{\"authorId\":\"1696349\",\"name\":\"P. M. Hof\"}],\"doi\":\"10.1109/CDC.2015.7403188\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1625f1bdcc573994cdc3a8a252a39af05d5b6262\",\"title\":\"Correct-by-design output feedback of LTI systems\",\"url\":\"https://www.semanticscholar.org/paper/1625f1bdcc573994cdc3a8a252a39af05d5b6262\",\"venue\":\"2015 54th IEEE Conference on Decision and Control (CDC)\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1692869\",\"name\":\"E. Feinberg\"}],\"doi\":\"10.1109/ADPRL.2009.4927531\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e8b79a71f15f80175f76a59284a033a11991c21\",\"title\":\"Adaptive computation of optimal nonrandomized policies in constrained average-reward MDPs\",\"url\":\"https://www.semanticscholar.org/paper/2e8b79a71f15f80175f76a59284a033a11991c21\",\"venue\":\"2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Cyrus Derman. Finite State Markovian Decision Processes. Press\"},{\"authorId\":null,\"name\":\"Inc.\"},{\"authorId\":null,\"name\":\"Orlando\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"FL\",\"url\":\"\",\"venue\":\"USA,\",\"year\":1970},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016}],\"title\":\"Steady-State Policy Synthesis in Multichain Markov Decision Processes\",\"topics\":[{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"}],\"url\":\"https://www.semanticscholar.org/paper/1a6d9762ee3c4c1b109c22e22ce5a6e665436fdb\",\"venue\":\"IJCAI\",\"year\":2020}\n"