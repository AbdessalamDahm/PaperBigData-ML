"{\"abstract\":\"We present RLCard, a Python platform for reinforcement learning research and development in card games. RLCard supports various card environments and several baseline algorithms with unified easy-to-use interfaces, aiming at bridging reinforcement learning and imperfect information games. The platform provides flexible configurations of state representation, action encoding, and reward design. RLCard also supports visualizations for algorithm debugging. In this demo, we showcase two representative environments and their visualization. We conclude this demo with challenges and research opportunities brought by RLCard. A video is available on YouTube1.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\",\"url\":\"https://www.semanticscholar.org/author/1759658\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\",\"url\":\"https://www.semanticscholar.org/author/51238382\"},{\"authorId\":\"29351303\",\"name\":\"Songyi Huang\",\"url\":\"https://www.semanticscholar.org/author/29351303\"},{\"authorId\":\"153842970\",\"name\":\"Y. Cao\",\"url\":\"https://www.semanticscholar.org/author/153842970\"},{\"authorId\":\"40149802\",\"name\":\"K. Reddy\",\"url\":\"https://www.semanticscholar.org/author/40149802\"},{\"authorId\":\"39827246\",\"name\":\"J. Vargas\",\"url\":\"https://www.semanticscholar.org/author/39827246\"},{\"authorId\":\"144426013\",\"name\":\"Alex Nguyen\",\"url\":\"https://www.semanticscholar.org/author/144426013\"},{\"authorId\":\"1381629357\",\"name\":\"Ruzhe Wei\",\"url\":\"https://www.semanticscholar.org/author/1381629357\"},{\"authorId\":\"50115480\",\"name\":\"Junyu Guo\",\"url\":\"https://www.semanticscholar.org/author/50115480\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\",\"url\":\"https://www.semanticscholar.org/author/48539382\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":220483358,\"doi\":\"10.24963/ijcai.2020/764\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"a13fdbb96a1c3429b1b79b15000e561e27ce0719\",\"references\":[{\"arxivId\":\"1910.04376\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"153842970\",\"name\":\"Y. Cao\"},{\"authorId\":\"50822341\",\"name\":\"Song-Yi Huang\"},{\"authorId\":\"1381629357\",\"name\":\"Ruzhe Wei\"},{\"authorId\":null,\"name\":\"Junyu Guo\"},{\"authorId\":\"48539647\",\"name\":\"Xia Hu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"891a964ba4c5b36d87066a2c38b841bd6cd18978\",\"title\":\"RLCard: A Toolkit for Reinforcement Learning in Card Games\",\"url\":\"https://www.semanticscholar.org/paper/891a964ba4c5b36d87066a2c38b841bd6cd18978\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1603.01121\",\"authors\":[{\"authorId\":\"37183181\",\"name\":\"J. Heinrich\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a1d2a7ef81960846b9cec00bce8eefa06ccc8796\",\"title\":\"Deep Reinforcement Learning from Self-Play in Imperfect-Information Games\",\"url\":\"https://www.semanticscholar.org/paper/a1d2a7ef81960846b9cec00bce8eefa06ccc8796\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"},{\"authorId\":\"1681530\",\"name\":\"Michael Bradley Johanson\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"},{\"authorId\":\"3248202\",\"name\":\"C. Piccione\"}],\"doi\":\"10.7939/R3Q23R282\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db20a76a702be3ad4e7cebf7eeb7f1898827cebd\",\"title\":\"Regret Minimization in Games with Incomplete Information\",\"url\":\"https://www.semanticscholar.org/paper/db20a76a702be3ad4e7cebf7eeb7f1898827cebd\",\"venue\":\"NIPS\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1906.08387\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"3364022\",\"name\":\"K. Zhou\"},{\"authorId\":\"121774683\",\"name\":\"X. Hu\"}],\"doi\":\"10.24963/ijcai.2019/589\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"title\":\"Experience Replay Optimization\",\"url\":\"https://www.semanticscholar.org/paper/2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"144514513\",\"name\":\"K. Waugh\"},{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.7939/R3319S48Q\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7b506eebe271b76bbf830508ab4daca4fbed6ef0\",\"title\":\"Monte Carlo Sampling for Regret Minimization in Extensive Games\",\"url\":\"https://www.semanticscholar.org/paper/7b506eebe271b76bbf830508ab4daca4fbed6ef0\",\"venue\":\"NIPS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005}],\"title\":\"RLCard: A Platform for Reinforcement Learning in Card Games\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/a13fdbb96a1c3429b1b79b15000e561e27ce0719\",\"venue\":\"IJCAI\",\"year\":2020}\n"