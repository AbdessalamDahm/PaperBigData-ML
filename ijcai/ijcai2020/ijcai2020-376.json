"{\"abstract\":\"In reinforcement learning, policy gradient algorithms optimize the policy directly and rely on sampling efficiently an environment. Nevertheless, while most sampling procedures are based on direct policy sampling, self-performance measures could be used to improve such sampling prior to each policy update. Following this line of thought, we introduce SAUNA, a method where non-informative transitions are rejected from the gradient update. The level of information is estimated according to the fraction of variance explained by the value function: a measure of the discrepancy between V and the empirical returns. In this work, we use this metric to select samples that are useful to learn from, and we demonstrate that this selection can significantly improve the performance of policy gradient methods. In this paper: (a) We define SAUNA's metric and introduce its method to filter transitions. (b) We conduct experiments on a set of benchmark continuous control problems. SAUNA significantly improves performance. (c) We investigate how SAUNA reliably selects samples with the most positive impact on learning and study its improvement on both performance and sample efficiency.\",\"arxivId\":\"1904.04025\",\"authors\":[{\"authorId\":\"1412725415\",\"name\":\"Yannis Flet-Berliac\",\"url\":\"https://www.semanticscholar.org/author/1412725415\"},{\"authorId\":\"47432313\",\"name\":\"Philippe Preux\",\"url\":\"https://www.semanticscholar.org/author/47432313\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1909.11939\",\"authors\":[{\"authorId\":\"1412725415\",\"name\":\"Yannis Flet-Berliac\"},{\"authorId\":\"47432313\",\"name\":\"Philippe Preux\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a5137ec6d09e1d32a3b46aefd32ef8939a57491\",\"title\":\"MERL: Multi-Head Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3a5137ec6d09e1d32a3b46aefd32ef8939a57491\",\"venue\":\"\",\"year\":2019}],\"corpusId\":218613622,\"doi\":\"10.24963/ijcai.2020/376\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"7310c425c1fb44f43f983a959e46f45c1947fc9a\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2319833\",\"name\":\"P. Werbos\"}],\"doi\":\"10.1109/CDC.1989.70114\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3764fb7db442dffb6c5e03ac376e8e117f5172e9\",\"title\":\"Neural networks for control and system identification\",\"url\":\"https://www.semanticscholar.org/paper/3764fb7db442dffb6c5e03ac376e8e117f5172e9\",\"venue\":\"Proceedings of the 28th IEEE Conference on Decision and Control,\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1657391174\",\"name\":\"G.V.T.V. Weerasooriya\"},{\"authorId\":\"1657391179\",\"name\":\"D. N. Jayatissa\"},{\"authorId\":\"1657391068\",\"name\":\"M. Rambanda\"}],\"doi\":\"10.1515/9783111576855-012\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f61efefa2a671529c4e6da4cb976d9437332f52e\",\"title\":\"G\",\"url\":\"https://www.semanticscholar.org/paper/f61efefa2a671529c4e6da4cb976d9437332f52e\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48974230\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/IJCNN.1991.170605\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"94db34f4b68189bfcba22beab33ee3b54f10b876\",\"title\":\"Curious model-building control systems\",\"url\":\"https://www.semanticscholar.org/paper/94db34f4b68189bfcba22beab33ee3b54f10b876\",\"venue\":\"[Proceedings] 1991 IEEE International Joint Conference on Neural Networks\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Hill\"},{\"authorId\":null,\"name\":\"A. Raffin\"},{\"authorId\":null,\"name\":\"M. Ernestus\"},{\"authorId\":null,\"name\":\"A. Gleave\"},{\"authorId\":null,\"name\":\"A. Kanervisto\"},{\"authorId\":null,\"name\":\"R. Traore\"},{\"authorId\":null,\"name\":\"P. Dhariwal\"},{\"authorId\":null,\"name\":\"C. Hesse\"},{\"authorId\":null,\"name\":\"O. Klimov\"},{\"authorId\":null,\"name\":\"A. Nichol\"},{\"authorId\":null,\"name\":\"M. Plappert\"},{\"authorId\":null,\"name\":\"A. Radford\"},{\"authorId\":null,\"name\":\"J. Schulman\"},{\"authorId\":null,\"name\":\"S. Sidor\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Y\",\"url\":\"\",\"venue\":\"Wu. Stable baselines\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"},{\"authorId\":\"1745219\",\"name\":\"S. Schaal\"}],\"doi\":\"10.1016/j.neunet.2008.02.003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb5b459c8a3e56064158fb3514eeab763486e437\",\"title\":\"Reinforcement learning of motor skills with policy gradients\",\"url\":\"https://www.semanticscholar.org/paper/eb5b459c8a3e56064158fb3514eeab763486e437\",\"venue\":\"Neural Networks\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Kv\\u00e5lseth\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Cautionary Note about R2\",\"url\":\"\",\"venue\":\"The American Statistician, 39(4):279\\u2013285\",\"year\":1985},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":\"10.1109/IROS.2012.6386109\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"title\":\"MuJoCo: A physics engine for model-based control\",\"url\":\"https://www.semanticscholar.org/paper/b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"venue\":\"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Schulman\",\"url\":\"\",\"venue\":\"Roboschool,\",\"year\":2017},{\"arxivId\":\"1506.02438\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"title\":\"High-Dimensional Continuous Control Using Generalized Advantage Estimation\",\"url\":\"https://www.semanticscholar.org/paper/d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1710.02298\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"title\":\"Rainbow: Combining Improvements in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1611.01224\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6a43d91c8d883e3463b358571125fa0ec7298b3a\",\"title\":\"Sample Efficient Actor-Critic with Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/6a43d91c8d883e3463b358571125fa0ec7298b3a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1910.13038\",\"authors\":[{\"authorId\":\"2800357\",\"name\":\"C. D. Freeman\"},{\"authorId\":\"2096458\",\"name\":\"Luke Metz\"},{\"authorId\":\"1389041357\",\"name\":\"David Ha\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc1d3f10eb4d8a92e2fe09e67f0352d7a0600afd\",\"title\":\"Learning to Predict Without Looking Ahead: World Models Without Forward Prediction\",\"url\":\"https://www.semanticscholar.org/paper/cc1d3f10eb4d8a92e2fe09e67f0352d7a0600afd\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1764769\",\"name\":\"G. Ditzler\"},{\"authorId\":\"1744102\",\"name\":\"M. Roveri\"},{\"authorId\":\"1785004\",\"name\":\"C. Alippi\"},{\"authorId\":\"1780024\",\"name\":\"R. Polikar\"}],\"doi\":\"10.1109/MCI.2015.2471196\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a7c9edc29726834bed60c837655cd13cbb2d814\",\"title\":\"Learning in Nonstationary Environments: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/3a7c9edc29726834bed60c837655cd13cbb2d814\",\"venue\":\"IEEE Computational Intelligence Magazine\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Z. Wang\"},{\"authorId\":null,\"name\":\"V. Bapst\"},{\"authorId\":null,\"name\":\"N. Heess\"},{\"authorId\":null,\"name\":\"V. Mnih\"},{\"authorId\":null,\"name\":\"R. Munos\"},{\"authorId\":null,\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and N\",\"url\":\"\",\"venue\":\"de Freitas. Sample efficient actor-critic with experience replay. In Proc. ICLR\",\"year\":2017},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"title\":\"Deterministic Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1812.02341\",\"authors\":[{\"authorId\":\"6062736\",\"name\":\"K. Cobbe\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"},{\"authorId\":\"144239765\",\"name\":\"Christopher Hesse\"},{\"authorId\":\"1837923\",\"name\":\"Taehoon Kim\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef2bc452812d6005ab0a66af6c3f97b6b0ba837e\",\"title\":\"Quantifying Generalization in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ef2bc452812d6005ab0a66af6c3f97b6b0ba837e\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1809.01999\",\"authors\":[{\"authorId\":\"39810222\",\"name\":\"David R Ha\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cae23343d2efddca3592b08a521a896af5098248\",\"title\":\"Recurrent World Models Facilitate Policy Evolution\",\"url\":\"https://www.semanticscholar.org/paper/cae23343d2efddca3592b08a521a896af5098248\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6b14af216f6a667c4e03c6964babe828c680a05a\",\"title\":\"On the sample complexity of reinforcement learning.\",\"url\":\"https://www.semanticscholar.org/paper/6b14af216f6a667c4e03c6964babe828c680a05a\",\"venue\":\"\",\"year\":2003},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1802.01561\",\"authors\":[{\"authorId\":\"2311318\",\"name\":\"Lasse Espeholt\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"48500289\",\"name\":\"Tom Ward\"},{\"authorId\":\"2895238\",\"name\":\"Yotam Doron\"},{\"authorId\":\"9559485\",\"name\":\"Vlad Firoiu\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"2768462\",\"name\":\"Iain Dunning\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"80196cdfcd0c6ce2953bf65a7f019971e2026386\",\"title\":\"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\",\"url\":\"https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1806.07937\",\"authors\":[{\"authorId\":\"49425560\",\"name\":\"A. Zhang\"},{\"authorId\":\"2482072\",\"name\":\"Nicolas Ballas\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a622be547caf0b1223626de5e69377c20ae11265\",\"title\":\"A Dissection of Overfitting and Generalization in Continuous Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a622be547caf0b1223626de5e69377c20ae11265\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1805.10966\",\"authors\":[{\"authorId\":\"2988592\",\"name\":\"G. Parisi\"},{\"authorId\":\"1780524\",\"name\":\"J. Tani\"},{\"authorId\":\"1798067\",\"name\":\"Cornelius Weber\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.3389/fnbot.2018.00078\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4b158ec2c64731c1fc6a293476f611a15fa32eb\",\"title\":\"Lifelong Learning of Spatiotemporal Representations With Dual-Memory Recurrent Self-Organization\",\"url\":\"https://www.semanticscholar.org/paper/a4b158ec2c64731c1fc6a293476f611a15fa32eb\",\"venue\":\"Front. Neurorobot.\",\"year\":2018},{\"arxivId\":\"1707.02286\",\"authors\":[{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"22216833\",\"name\":\"TB Dhruva\"},{\"authorId\":\"37118178\",\"name\":\"S. Sriram\"},{\"authorId\":\"144083287\",\"name\":\"Jay Lemmon\"},{\"authorId\":\"1879232\",\"name\":\"J. Merel\"},{\"authorId\":\"89504302\",\"name\":\"G. Wayne\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a762ae907b7dd71a59bd8bd98aba69dfe2de13a2\",\"title\":\"Emergence of Locomotion Behaviours in Rich Environments\",\"url\":\"https://www.semanticscholar.org/paper/a762ae907b7dd71a59bd8bd98aba69dfe2de13a2\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"},{\"authorId\":\"143918865\",\"name\":\"R. Huber\"}],\"doi\":\"10.1142/S012906579100011X\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dafb111b23786058bd8943a6f1a5b4afc68cb3b3\",\"title\":\"Learning to Generate Artificial Fovea Trajectories for Target Detection\",\"url\":\"https://www.semanticscholar.org/paper/dafb111b23786058bd8943a6f1a5b4afc68cb3b3\",\"venue\":\"Int. J. Neural Syst.\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b18833db0de9393d614d511e60821a1504fc6cd1\",\"title\":\"A Natural Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/b18833db0de9393d614d511e60821a1504fc6cd1\",\"venue\":\"NIPS\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144711425\",\"name\":\"T. Robinson\"},{\"authorId\":\"1998157\",\"name\":\"F. Fallside\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d26760e6524e78d4c029de7c82341040ab966d4\",\"title\":\"Dynamic reinforcement driven error propagation networks with application to game playing\",\"url\":\"https://www.semanticscholar.org/paper/3d26760e6524e78d4c029de7c82341040ab966d4\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"34f25a8704614163c4095b3ee2fc969b60de4698\",\"title\":\"Dropout: a simple way to prevent neural networks from overfitting\",\"url\":\"https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Lapan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep Reinforcement Learning Hands-On\",\"url\":\"\",\"venue\":\"Packt Publishing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Schmidhuber\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Curious modelbuilding control systems\",\"url\":\"\",\"venue\":\"Proc. IEEE IJCNN\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1708.05144\",\"authors\":[{\"authorId\":\"3374063\",\"name\":\"Yuhuai Wu\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"1785346\",\"name\":\"Roger B. Grosse\"},{\"authorId\":\"143997138\",\"name\":\"Shun Liao\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b6f2b163372e3417b687cc43313f2a630e7bca7\",\"title\":\"Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation\",\"url\":\"https://www.semanticscholar.org/paper/2b6f2b163372e3417b687cc43313f2a630e7bca7\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144826602\",\"name\":\"C. Anderson\"}],\"doi\":\"10.1109/TSMC.1983.6313077\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8a7acaf6469c06ae5876d92f013184db5897bb13\",\"title\":\"Neuronlike adaptive elements that can solve difficult learning control problems\",\"url\":\"https://www.semanticscholar.org/paper/8a7acaf6469c06ae5876d92f013184db5897bb13\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics\",\"year\":1983},{\"arxivId\":\"1909.11939\",\"authors\":[{\"authorId\":\"1412725415\",\"name\":\"Yannis Flet-Berliac\"},{\"authorId\":\"47432313\",\"name\":\"Philippe Preux\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3a5137ec6d09e1d32a3b46aefd32ef8939a57491\",\"title\":\"MERL: Multi-Head Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3a5137ec6d09e1d32a3b46aefd32ef8939a57491\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1703.01161\",\"authors\":[{\"authorId\":\"9948791\",\"name\":\"A. S. Vezhnevets\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"049c6e5736313374c6e594c34b9be89a3a09dced\",\"title\":\"FeUdal Networks for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/049c6e5736313374c6e594c34b9be89a3a09dced\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1510.09142\",\"authors\":[{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"37866714\",\"name\":\"Gregory Wayne\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6640f4e4beae786f301928d82a9f8eb037aa6935\",\"title\":\"Learning Continuous Control Policies by Stochastic Value Gradients\",\"url\":\"https://www.semanticscholar.org/paper/6640f4e4beae786f301928d82a9f8eb037aa6935\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":\"1810.12282\",\"authors\":[{\"authorId\":\"9955459\",\"name\":\"C. Packer\"},{\"authorId\":\"3384683\",\"name\":\"Katelyn Gao\"},{\"authorId\":\"36426383\",\"name\":\"J. Kos\"},{\"authorId\":\"2562966\",\"name\":\"Philipp Kr\\u00e4henb\\u00fchl\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"},{\"authorId\":\"143711382\",\"name\":\"D. Song\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"caea502325b6a82b1b437c62585992609b5aa542\",\"title\":\"Assessing Generalization in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/caea502325b6a82b1b437c62585992609b5aa542\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"985f2c1baba284e9b7b604b7169a2e2778540fe6\",\"title\":\"Temporal abstraction in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/985f2c1baba284e9b7b604b7169a2e2778540fe6\",\"venue\":\"ICML 2000\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102735693\",\"name\":\"Shun-ichi Amari\"}],\"doi\":\"10.1162/089976698300017746\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a767a341364de1f75bea85e0b12ba7d3586a461\",\"title\":\"Natural Gradient Works Efficiently in Learning\",\"url\":\"https://www.semanticscholar.org/paper/5a767a341364de1f75bea85e0b12ba7d3586a461\",\"venue\":\"Neural Computation\",\"year\":1998},{\"arxivId\":\"1702.08892\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"title\":\"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"4454080\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b1362879e77efef96ab552f5cb1198c2a67204d6\",\"title\":\"Introduction to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b1362879e77efef96ab552f5cb1198c2a67204d6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2765219\",\"name\":\"T. O. Kv\\u00e5lseth\"}],\"doi\":\"10.1080/00031305.1985.10479448\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"51b17299f66bfe5c282ab927de16450bcbc7bcb9\",\"title\":\"Cautionary Note about R 2\",\"url\":\"https://www.semanticscholar.org/paper/51b17299f66bfe5c282ab927de16450bcbc7bcb9\",\"venue\":\"\",\"year\":1985},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d0c61536927c2f5dc2ddb74664268a3623580b9c\",\"title\":\"Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/d0c61536927c2f5dc2ddb74664268a3623580b9c\",\"venue\":\"NIPS\",\"year\":2014}],\"title\":\"Only Relevant Information Matters: Filtering Out Noisy Samples To Boost RL\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Sampling (signal processing)\",\"topicId\":\"7839\",\"url\":\"https://www.semanticscholar.org/topic/7839\"},{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Boost\",\"topicId\":\"501287\",\"url\":\"https://www.semanticscholar.org/topic/501287\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Information\",\"topicId\":\"185548\",\"url\":\"https://www.semanticscholar.org/topic/185548\"},{\"topic\":\"Emoticon\",\"topicId\":\"55238\",\"url\":\"https://www.semanticscholar.org/topic/55238\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Discrepancy function\",\"topicId\":\"1146712\",\"url\":\"https://www.semanticscholar.org/topic/1146712\"}],\"url\":\"https://www.semanticscholar.org/paper/7310c425c1fb44f43f983a959e46f45c1947fc9a\",\"venue\":\"IJCAI\",\"year\":2020}\n"