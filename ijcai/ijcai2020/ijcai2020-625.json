"{\"abstract\":\"In real-world decision-making problems, for instance in the fields of finance, robotics or autonomous driving, keeping uncertainty under control is as important as maximizing expected returns. Risk aversion has been addressed in the reinforcement learning literature through risk measures related to the variance of returns. However, in many cases, the risk is measured not only on a long-term perspective, but also on the step-wise rewards (e.g., in trading, to ensure the stability of the investment bank, it is essential to monitor the risk of portfolio positions on a daily basis). In this paper, we define a novel measure of risk, which we call reward volatility, consisting of the variance of the rewards under the state-occupancy measure. We show that the reward volatility bounds the return variance so that reducing the former also constrains the latter. We derive a policy gradient theorem with a new objective function that exploits the mean-volatility relationship, and develop an actor-only algorithm. Furthermore, thanks to the linearity of the Bellman equations defined under the new objective function, it is possible to adapt the well-known policy gradient algorithms with monotonic improvement guarantees such as TRPO in a risk-averse manner. Finally, we test the proposed approach in two simulated financial environments.\",\"arxivId\":\"1912.03193\",\"authors\":[{\"authorId\":\"35468434\",\"name\":\"L. Bisi\",\"url\":\"https://www.semanticscholar.org/author/35468434\"},{\"authorId\":\"1451648449\",\"name\":\"Luca Sabbioni\",\"url\":\"https://www.semanticscholar.org/author/1451648449\"},{\"authorId\":\"1451651117\",\"name\":\"Edoardo Vittori\",\"url\":\"https://www.semanticscholar.org/author/1451651117\"},{\"authorId\":\"145388375\",\"name\":\"M. Papini\",\"url\":\"https://www.semanticscholar.org/author/145388375\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\",\"url\":\"https://www.semanticscholar.org/author/1792167\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2010.12245\",\"authors\":[{\"authorId\":\"1451651117\",\"name\":\"Edoardo Vittori\"},{\"authorId\":\"102233365\",\"name\":\"M. Trapletti\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f7a69a1bcc0b00f5ddd05be02934390332aa7a64\",\"title\":\"Option Hedging with Risk Averse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f7a69a1bcc0b00f5ddd05be02934390332aa7a64\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.04203\",\"authors\":[{\"authorId\":\"49306150\",\"name\":\"Thomas Spooner\"},{\"authorId\":\"2377870\",\"name\":\"Rahul Savani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c9c8239046207c84b723d28926f9569941aa63de\",\"title\":\"A Natural Actor-Critic Algorithm with Downside Risk Constraints\",\"url\":\"https://www.semanticscholar.org/paper/c9c8239046207c84b723d28926f9569941aa63de\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.01404\",\"authors\":[{\"authorId\":\"144397047\",\"name\":\"M. Kato\"},{\"authorId\":\"47107220\",\"name\":\"Kei Nakagawa\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0a107efa7f0c7e442f05dc3c83cc152b69cd1021\",\"title\":\"Policy Gradient with Expected Quadratic Utility Maximization: A New Mean-Variance Approach in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0a107efa7f0c7e442f05dc3c83cc152b69cd1021\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.10888\",\"authors\":[{\"authorId\":\"2503523\",\"name\":\"S. Zhang\"},{\"authorId\":\"145306563\",\"name\":\"B. Liu\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"11d558cf04914a19068338705526593fe7fb6cd3\",\"title\":\"Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/11d558cf04914a19068338705526593fe7fb6cd3\",\"venue\":\"\",\"year\":2020}],\"corpusId\":208858048,\"doi\":\"10.24963/ijcai.2020/625\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"2bcda4b9698f1315d40c692b403b379db6d276f5\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Yinlam Chow\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":null,\"name\":\"Shie Mannor. Sequential Decision Making With Coherent R Control\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"62(7):3323\\u20133338\",\"url\":\"\",\"venue\":\"July\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Dotan Di Castro\"},{\"authorId\":null,\"name\":\"Shie Mannor. Learning the variance of the reward-to-go\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):361\\u2013396,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S \\u2200s\"},{\"authorId\":null,\"name\":\"A. a\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Then, using the same argument as in Lemma 6 from (Papini, Pirotta, and Restelli 2019), the following upper bounds hold: \\u2016\\u2207J\\u03c0\\u2016 \\u2264 Rmax\\u03c8\",\"url\":\"\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael I. Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"1944485\",\"name\":\"L. Bascetta\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"dc5021a589afb15819e768733a1fc6476ea8b0a0\",\"title\":\"Adaptive Step-Size for Policy Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/dc5021a589afb15819e768733a1fc6476ea8b0a0\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"89070317\",\"name\":\"K. Dollman\"},{\"authorId\":\"144994290\",\"name\":\"J. Clark\"},{\"authorId\":\"120319563\",\"name\":\"M. Norell\"},{\"authorId\":\"48992355\",\"name\":\"Xu Xing\"},{\"authorId\":\"46474144\",\"name\":\"J. Choiniere\"}],\"doi\":\"10.1097/ccm.0000000000002891\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c0d787dc10292e51766091d2a203033bd86de5ff\",\"title\":\"? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? ?\",\"url\":\"https://www.semanticscholar.org/paper/c0d787dc10292e51766091d2a203033bd86de5ff\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1446962550\",\"name\":\"Dock Bumpers\"},{\"authorId\":\"1446961266\",\"name\":\"Support Ledgers\"}],\"doi\":\"10.1023/A:1017189329742\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"title\":\"Volume 2\",\"url\":\"https://www.semanticscholar.org/paper/e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"venue\":\"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"1944485\",\"name\":\"L. Bascetta\"}],\"doi\":\"10.1007/s10994-015-5484-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"483f98a7d9bf00081e8bbc431f9866998baaccb8\",\"title\":\"Policy gradient in Lipschitz Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/483f98a7d9bf00081e8bbc431f9866998baaccb8\",\"venue\":\"Machine Learning\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145016534\",\"name\":\"J. Moody\"},{\"authorId\":\"1869642\",\"name\":\"M. Saffell\"}],\"doi\":\"10.1109/72.935097\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c234f0a8cf6465cdede5f17ba7b62ac4092a510\",\"title\":\"Learning to trade via direct reinforcement\",\"url\":\"https://www.semanticscholar.org/paper/8c234f0a8cf6465cdede5f17ba7b62ac4092a510\",\"venue\":\"IEEE Trans. Neural Networks\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Javier Garc\\u00eda\"},{\"authorId\":null,\"name\":\"Fernando Fern\\u00e1ndez. A comprehensive survey on safe reinforc learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JMLR\",\"url\":\"\",\"venue\":\"16:1437\\u20131480,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yinlam Chow\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":null,\"name\":\"Lucas Janson\"},{\"authorId\":null,\"name\":\"Marco Pavone. Risk-constrained reinforcement learning wi criteria\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JMLR\",\"url\":\"\",\"venue\":\"18(1):6070\\u20136120,\",\"year\":2017},{\"arxivId\":\"1206.6404\",\"authors\":[{\"authorId\":\"9440777\",\"name\":\"D. Castro\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e27fb84fccd3d9724df8ce35fe14149da5de3251\",\"title\":\"Policy Gradients with Variance Related Risk Criteria\",\"url\":\"https://www.semanticscholar.org/paper/e27fb84fccd3d9724df8ce35fe14149da5de3251\",\"venue\":\"ICML\",\"year\":2012},{\"arxivId\":\"1502.03919\",\"authors\":[{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"44deb9664330c36484556379342b3bc78aaea046\",\"title\":\"Policy Gradient for Coherent Risk Measures\",\"url\":\"https://www.semanticscholar.org/paper/44deb9664330c36484556379342b3bc78aaea046\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1401932095\",\"name\":\"A. PrashanthL.\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26317ffdbfdbdadaf0338853eb4782f76432a627\",\"title\":\"Actor-Critic Algorithms for Risk-Sensitive Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/26317ffdbfdbdadaf0338853eb4782f76432a627\",\"venue\":\"ArXiv\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145388375\",\"name\":\"M. Papini\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"06a6df93adc728aaeae692c5ab1480805eda360a\",\"title\":\"Adaptive Batch Size for Safe Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/06a6df93adc728aaeae692c5ab1480805eda360a\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1403.6530\",\"authors\":[{\"authorId\":\"1401932095\",\"name\":\"A. PrashanthL.\"},{\"authorId\":\"103809454\",\"name\":\"Mohammad Ghavamzadeh\"}],\"doi\":\"10.1007/s10994-016-5569-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e1b05335f3ffd5bd1b7555e0b3a1b19f9ec07f0f\",\"title\":\"Variance-constrained actor-critic algorithms for discounted and average reward MDPs\",\"url\":\"https://www.semanticscholar.org/paper/e1b05335f3ffd5bd1b7555e0b3a1b19f9ec07f0f\",\"venue\":\"Machine Learning\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2273298\",\"name\":\"T. Morimura\"},{\"authorId\":\"67154907\",\"name\":\"Masashi Sugiyama\"},{\"authorId\":\"2785830\",\"name\":\"H. Kashima\"},{\"authorId\":\"40308003\",\"name\":\"H. Hachiya\"},{\"authorId\":\"145876882\",\"name\":\"Toshiyuki TANAKA\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ec26e05c2577154213e1668ddd374e4da663309\",\"title\":\"Nonparametric Return Distribution Approximation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ec26e05c2577154213e1668ddd374e4da663309\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"},{\"authorId\":\"144162125\",\"name\":\"J. Langford\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"523b4ce1c2a1336962444abc1dec215756c2f3e6\",\"title\":\"Approximately Optimal Approximate Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/523b4ce1c2a1336962444abc1dec215756c2f3e6\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c0f2c4104ef6e36bb67022001179887e6600d24d\",\"title\":\"A comprehensive survey on safe reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/c0f2c4104ef6e36bb67022001179887e6600d24d\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Yinlam Chow\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":null,\"name\":\"Shie Mannor. Policy Gradient for Coherent Risk Measures\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"page 9,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. H\\u00e4rdle\"},{\"authorId\":null,\"name\":\"L. Simar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Applied multivariate statistical analysis, volume 22007\",\"url\":\"\",\"venue\":\"Springer.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":\"10.1109/TAC.2016.2644871\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ac1a99e71e62dbdfa056f5927340fa59e6b7a0b\",\"title\":\"Sequential Decision Making With Coherent Risk\",\"url\":\"https://www.semanticscholar.org/paper/7ac1a99e71e62dbdfa056f5927340fa59e6b7a0b\",\"venue\":\"IEEE Transactions on Automatic Control\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jan Peters\"},{\"authorId\":null,\"name\":\"Stefan Schaal\"},{\"authorId\":null,\"name\":\"Pirotta\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Variance-constrained actorcritic algorithms for discounted and average reward mdps. CoRR, abs/1403\",\"url\":\"\",\"venue\":\"Machine Learning\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48234901\",\"name\":\"Y. Shen\"},{\"authorId\":\"40372969\",\"name\":\"Ruihong Huang\"},{\"authorId\":\"145309074\",\"name\":\"Chang Yan\"},{\"authorId\":\"1743272\",\"name\":\"K. Obermayer\"}],\"doi\":\"10.1109/cifer.2014.6924100\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f609b3e02187d373cd6cd824640c4109a2955f1d\",\"title\":\"Risk-averse reinforcement learning for algorithmic trading\",\"url\":\"https://www.semanticscholar.org/paper/f609b3e02187d373cd6cd824640c4109a2955f1d\",\"venue\":\"2014 IEEE Conference on Computational Intelligence for Financial Engineering & Economics (CIFEr)\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yun Shen\"},{\"authorId\":null,\"name\":\"Ruihong Huang\"},{\"authorId\":null,\"name\":\"Chang Yan\"},{\"authorId\":null,\"name\":\"Klaus Obermayer. Risk-averse reinforcement learning for  trading\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 391\\u2013398\",\"url\":\"\",\"venue\":\"March\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc C Steinbach\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Adaptive step - size for policy gradient methods Policy gradient in lipschitz markov decision processes\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1512.01629\",\"authors\":[{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"2542753\",\"name\":\"L. Janson\"},{\"authorId\":\"1696085\",\"name\":\"M. Pavone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89\",\"title\":\"Risk-Constrained Reinforcement Learning with Percentile Risk Criteria\",\"url\":\"https://www.semanticscholar.org/paper/759bbd8dd50cb4790cad7a3bccbdfcbfee5e3e89\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2017},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"L. A. Prashanth\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh. Variance-constrained actorcritic algo discounted\"},{\"authorId\":null,\"name\":\"average reward mdps\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1403.6530,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Sobel Matthew\"},{\"authorId\":null,\"name\":\"Sobel\"},{\"authorId\":null,\"name\":\"C Marc\"},{\"authorId\":null,\"name\":\"S Steinbach Richard\"},{\"authorId\":null,\"name\":\"Andrew G Sutton\"},{\"authorId\":null,\"name\":\"Barto\"},{\"authorId\":null,\"name\":\"Sutton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, and Shie Mannor. Policy Gradient for Coherent Risk Measures. CoRR, page 9\",\"url\":\"\",\"venue\":\"Aviv Tamar and Shie Mannor. Variance adjusted actor critic algorithms\",\"year\":1982},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2261881\",\"name\":\"M. Deisenroth\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":\"10.1561/2300000021\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6bfae6efa1110a57a4d8362721d152d78aae358\",\"title\":\"A Survey on Policy Search for Robotics\",\"url\":\"https://www.semanticscholar.org/paper/b6bfae6efa1110a57a4d8362721d152d78aae358\",\"venue\":\"Found. Trends Robotics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Pirotta\"},{\"authorId\":null,\"name\":\"Marcello Restelli\"},{\"authorId\":null,\"name\":\"Luca Bascetta. Adaptive step-size for policy gradient methods\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS 26\",\"url\":\"\",\"venue\":\"pages 1394\\u20131402. Curran Associates, Inc.,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Filip Wolski\"},{\"authorId\":null,\"name\":\"Prafulla Dhariwal\"},{\"authorId\":null,\"name\":\"Alec Radford\"},{\"authorId\":null,\"name\":\"Oleg Klimov. Proximal policy optimization algorithms\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1707.06347,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"2850430\",\"name\":\"A. Pecorino\"},{\"authorId\":\"2439765\",\"name\":\"Daniele Calandriello\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fe40cd17112743123b5eb2830a44c3ea9b70ebaa\",\"title\":\"Safe Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/fe40cd17112743123b5eb2830a44c3ea9b70ebaa\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":\"1707.02286\",\"authors\":[{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"22216833\",\"name\":\"TB Dhruva\"},{\"authorId\":\"37118178\",\"name\":\"S. Sriram\"},{\"authorId\":\"144083287\",\"name\":\"Jay Lemmon\"},{\"authorId\":\"1879232\",\"name\":\"J. Merel\"},{\"authorId\":\"89504302\",\"name\":\"G. Wayne\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a762ae907b7dd71a59bd8bd98aba69dfe2de13a2\",\"title\":\"Emergence of Locomotion Behaviours in Rich Environments\",\"url\":\"https://www.semanticscholar.org/paper/a762ae907b7dd71a59bd8bd98aba69dfe2de13a2\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Papini\"},{\"authorId\":null,\"name\":\"Matteo Pirotta\"},{\"authorId\":null,\"name\":\"Marcello Restelli. Adaptive batch size for safe policy gradients\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 3591\\u20133600,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew J. Sobel. The variance of discounted Markov decisi processes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Applied Probability\",\"url\":\"\",\"venue\":\"19(4):794\\u2013802,\",\"year\":1982},{\"arxivId\":\"1310.3697\",\"authors\":[{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c3843fa74ac0c06dfa086c5f425c235551106f6\",\"title\":\"Variance Adjusted Actor Critic Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/3c3843fa74ac0c06dfa086c5f425c235551106f6\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114788213\",\"name\":\"D. Signorini\"},{\"authorId\":\"4137433\",\"name\":\"J. Slattery\"},{\"authorId\":\"12113526\",\"name\":\"S. Dodds\"},{\"authorId\":\"26151757\",\"name\":\"V. Lane\"},{\"authorId\":\"143842868\",\"name\":\"P. Littlejohns\"}],\"doi\":\"10.1016/S0140-6736(95)92525-2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"20b844e395355b40fa5940c61362ec40e56027aa\",\"title\":\"Neural networks\",\"url\":\"https://www.semanticscholar.org/paper/20b844e395355b40fa5940c61362ec40e56027aa\",\"venue\":\"The Lancet\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Pirotta\"},{\"authorId\":null,\"name\":\"Marcello Restelli\"},{\"authorId\":null,\"name\":\"Alessio Pecorino\"},{\"authorId\":null,\"name\":\"Daniele Calandriello. Safe policy iteration\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 307\\u2013315,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"},{\"authorId\":\"1745219\",\"name\":\"S. Schaal\"}],\"doi\":\"10.1016/j.neunet.2008.02.003\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eb5b459c8a3e56064158fb3514eeab763486e437\",\"title\":\"Reinforcement learning of motor skills with policy gradients\",\"url\":\"https://www.semanticscholar.org/paper/eb5b459c8a3e56064158fb3514eeab763486e437\",\"venue\":\"Neural Networks\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"4454080\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b1362879e77efef96ab552f5cb1198c2a67204d6\",\"title\":\"Introduction to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b1362879e77efef96ab552f5cb1198c2a67204d6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"OpenAI\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Openai five\",\"url\":\"\",\"venue\":\"https://blog.openai.com/openai-five/, 2018 - accessed May\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S Sutton\"},{\"authorId\":null,\"name\":\"David A McAllester\"},{\"authorId\":null,\"name\":\"Satinder P Singh\"},{\"authorId\":null,\"name\":\"Yishay Mansour. Policy gradient methods for reinforcement approximation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 1057\\u20131063,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Papini\"},{\"authorId\":null,\"name\":\"Matteo Pirotta\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Marcello Restelli\",\"url\":\"\",\"venue\":\"Smoothing policies and safe policy gradients,\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46756560\",\"name\":\"Teodor Mihai Moldovan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c303f2d8584f27c3d5858d8a18701e2d8880f7d7\",\"title\":\"Risk Aversion in Markov Decision Processes via Near Optimal Chernoff Bounds\",\"url\":\"https://www.semanticscholar.org/paper/c303f2d8584f27c3d5858d8a18701e2d8880f7d7\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70359327\",\"name\":\"Vector Machines\"},{\"authorId\":\"4023505\",\"name\":\"P. Ti\\u00f1o\"}],\"doi\":\"10.1109/tnn.72\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e84b31f3822c9eb9bc22eccfee4acc96b32f8eea\",\"title\":\"IEEE Transactions on Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/e84b31f3822c9eb9bc22eccfee4acc96b32f8eea\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948080\",\"name\":\"M. J. Sobel\"}],\"doi\":\"10.2307/3213832\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ccac3bfde1981d1cd04239396703227dcaa5714\",\"title\":\"The variance of discounted Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/3ccac3bfde1981d1cd04239396703227dcaa5714\",\"venue\":\"\",\"year\":1982},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2581077\",\"name\":\"C. Heckler\"}],\"doi\":\"10.1198/tech.2005.s319\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dfa8dc739bf7625b276c402d56d1f9784e966c76\",\"title\":\"Applied Multivariate Statistical Analysis\",\"url\":\"https://www.semanticscholar.org/paper/dfa8dc739bf7625b276c402d56d1f9784e966c76\",\"venue\":\"Technometrics\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2900139\",\"name\":\"Marc C. Steinbach\"}],\"doi\":\"10.1137/S0036144500376650\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1935e0986939ea6ef2afa01eeef94dbfea6fb6da\",\"title\":\"Markowitz Revisited: Mean-Variance Models in Financial Portfolio Analysis\",\"url\":\"https://www.semanticscholar.org/paper/1935e0986939ea6ef2afa01eeef94dbfea6fb6da\",\"venue\":\"SIAM Rev.\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1509217815\",\"name\":\"Ina Ruck\"},{\"authorId\":\"1509224719\",\"name\":\"Amerika Nach\"},{\"authorId\":\"1509223858\",\"name\":\"Tagen Unwahrscheinlichkeit\"},{\"authorId\":\"1509221473\",\"name\":\"Torben L\\u00fctjen\"},{\"authorId\":\"1509227399\",\"name\":\"Wie Amerika IN Politische\"},{\"authorId\":\"1509224729\",\"name\":\"Echokammern Zerfiel\"},{\"authorId\":\"1509221527\",\"name\":\"Eva Marlene Hausteiner\"},{\"authorId\":\"13202061\",\"name\":\"M. Berg\"},{\"authorId\":\"1509227509\",\"name\":\"Josef Braml\"},{\"authorId\":\"1509224737\",\"name\":\"Innenansichten Von\"},{\"authorId\":\"1509217488\",\"name\":\"Trumps Aussenpolitik\"},{\"authorId\":\"119118596\",\"name\":\"Andrew B. Denison\"},{\"authorId\":\"116494131\",\"name\":\"G. Seesslen\"}],\"doi\":\"10.1016/S0140-6736(89)91380-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d4e16e5b733b12d7dca311b399c91db2cd89792\",\"title\":\"USA\",\"url\":\"https://www.semanticscholar.org/paper/2d4e16e5b733b12d7dca311b399c91db2cd89792\",\"venue\":\"The Lancet\",\"year\":1989},{\"arxivId\":\"1705.07798\",\"authors\":[{\"authorId\":\"1741549\",\"name\":\"Gergely Neu\"},{\"authorId\":\"143808510\",\"name\":\"A. Jonsson\"},{\"authorId\":\"145810673\",\"name\":\"V. G\\u00f3mez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e7d1e21409a90e66106722506aeb434ee7a18f3\",\"title\":\"A unified view of entropy-regularized Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/2e7d1e21409a90e66106722506aeb434ee7a18f3\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2536655\",\"name\":\"A. Gosavi\"},{\"authorId\":\"143719881\",\"name\":\"S. Das\"},{\"authorId\":\"34196753\",\"name\":\"S. L. Murray\"}],\"doi\":\"10.1109/ADPRL.2014.7010645\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f34c291b1c05cfe6c56edbf78e6708836e0e1264\",\"title\":\"Beyond exponential utility functions: A variance-adjusted approach for risk-averse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f34c291b1c05cfe6c56edbf78e6708836e0e1264\",\"venue\":\"2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)\",\"year\":2014},{\"arxivId\":\"1905.03231\",\"authors\":[{\"authorId\":\"145388375\",\"name\":\"M. Papini\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"32a01db101b9576d9ab06b3eb29b6c63aa280e3b\",\"title\":\"Smoothing Policies and Safe Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/32a01db101b9576d9ab06b3eb29b6c63aa280e3b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999}],\"title\":\"Risk-Averse Trust Region Optimization for Reward-Volatility Reduction\",\"topics\":[{\"topic\":\"Volatility\",\"topicId\":\"72569\",\"url\":\"https://www.semanticscholar.org/topic/72569\"},{\"topic\":\"Trust region\",\"topicId\":\"149255\",\"url\":\"https://www.semanticscholar.org/topic/149255\"},{\"topic\":\"Risk aversion\",\"topicId\":\"107174\",\"url\":\"https://www.semanticscholar.org/topic/107174\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Robotics\",\"topicId\":\"2759\",\"url\":\"https://www.semanticscholar.org/topic/2759\"},{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Shadow volume\",\"topicId\":\"533672\",\"url\":\"https://www.semanticscholar.org/topic/533672\"},{\"topic\":\"Autonomous car\",\"topicId\":\"642\",\"url\":\"https://www.semanticscholar.org/topic/642\"},{\"topic\":\"Risk measure\",\"topicId\":\"147841\",\"url\":\"https://www.semanticscholar.org/topic/147841\"},{\"topic\":\"Autonomous robot\",\"topicId\":\"1175\",\"url\":\"https://www.semanticscholar.org/topic/1175\"},{\"topic\":\"Whole Earth 'Lectronic Link\",\"topicId\":\"107464\",\"url\":\"https://www.semanticscholar.org/topic/107464\"},{\"topic\":\"Loss function\",\"topicId\":\"3650\",\"url\":\"https://www.semanticscholar.org/topic/3650\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"}],\"url\":\"https://www.semanticscholar.org/paper/2bcda4b9698f1315d40c692b403b379db6d276f5\",\"venue\":\"ArXiv\",\"year\":2019}\n"