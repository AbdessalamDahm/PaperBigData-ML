"{\"abstract\":\"Deep neural networks perform well on test data when they are highly overparameterized, which, however, also leads to large cost to train and deploy them. As a leading approach to address this problem, sparse neural networks have been widely used to significantly reduce the size of networks, making them more efficient during training and deployment, without compromising performance. Recently, sparse neural networks, either compressed from a pre-trained model or obtained by training from scratch, have been observed to be able to generalize as well as or even better than their dense counterparts. However, conventional techniques to find well fitted sparse sub-networks are expensive and the mechanisms underlying this phenomenon are far from clear. To tackle these problems, this Ph.D. research aims to study the generalization of sparse neural networks, and to propose more efficient approaches that can yield sparse neural networks with generalization bounds.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"47130544\",\"name\":\"S. Liu\",\"url\":\"https://www.semanticscholar.org/author/47130544\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":220480896,\"doi\":\"10.24963/ijcai.2020/735\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":false,\"paperId\":\"3b5b8e1820641300c38682d25f6f339d4150112d\",\"references\":[{\"arxivId\":\"1609.04836\",\"authors\":[{\"authorId\":\"2844898\",\"name\":\"N. Keskar\"},{\"authorId\":\"2205699\",\"name\":\"D. Mudigere\"},{\"authorId\":\"2784955\",\"name\":\"J. Nocedal\"},{\"authorId\":\"1711231\",\"name\":\"M. Smelyanskiy\"},{\"authorId\":\"144669504\",\"name\":\"P. Tang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ec5896b4490c6e127d1718ffc36a3439d84cb81\",\"title\":\"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima\",\"url\":\"https://www.semanticscholar.org/paper/8ec5896b4490c6e127d1718ffc36a3439d84cb81\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1901.09181\",\"authors\":[{\"authorId\":\"48642038\",\"name\":\"S. Liu\"},{\"authorId\":\"2571038\",\"name\":\"D. Mocanu\"},{\"authorId\":\"31401161\",\"name\":\"Amarsagar Reddy Ramapuram Matavalam\"},{\"authorId\":\"1835531\",\"name\":\"Y. Pei\"},{\"authorId\":\"1691997\",\"name\":\"M. Pechenizkiy\"}],\"doi\":\"10.1007/s00521-020-05136-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c37a1110d007a6c6a1e536527504eca5953a51f7\",\"title\":\"Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware\",\"url\":\"https://www.semanticscholar.org/paper/c37a1110d007a6c6a1e536527504eca5953a51f7\",\"venue\":\"Neural Computing and Applications\",\"year\":2020},{\"arxivId\":\"1506.02626\",\"authors\":[{\"authorId\":\"143840277\",\"name\":\"Song Han\"},{\"authorId\":\"47325862\",\"name\":\"J. Pool\"},{\"authorId\":\"145927488\",\"name\":\"John Tran\"},{\"authorId\":\"80724002\",\"name\":\"W. Dally\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ff9a37d766e3a4f39757f5e1b235a42dacf18ff\",\"title\":\"Learning both Weights and Connections for Efficient Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1902.05967\",\"authors\":[{\"authorId\":\"34516897\",\"name\":\"Hesham Mostafa\"},{\"authorId\":\"47119134\",\"name\":\"X. Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c703e42ac401ad734f440d56f6e19e6b2af86a60\",\"title\":\"Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization\",\"url\":\"https://www.semanticscholar.org/paper/c703e42ac401ad734f440d56f6e19e6b2af86a60\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1901.09208\",\"authors\":[{\"authorId\":\"48642038\",\"name\":\"S. Liu\"},{\"authorId\":\"2571038\",\"name\":\"D. Mocanu\"},{\"authorId\":\"1691997\",\"name\":\"M. Pechenizkiy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d3fadf0386dfaa9ead6510be7b0657b1c4772563\",\"title\":\"Intrinsically Sparse Long Short-Term Memory Networks\",\"url\":\"https://www.semanticscholar.org/paper/d3fadf0386dfaa9ead6510be7b0657b1c4772563\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1708.02182\",\"authors\":[{\"authorId\":\"3375440\",\"name\":\"Stephen Merity\"},{\"authorId\":\"2844898\",\"name\":\"N. Keskar\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"58c6f890a1ae372958b7decf56132fe258152722\",\"title\":\"Regularizing and Optimizing LSTM Language Models\",\"url\":\"https://www.semanticscholar.org/paper/58c6f890a1ae372958b7decf56132fe258152722\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2571038\",\"name\":\"D. Mocanu\"},{\"authorId\":\"50734687\",\"name\":\"Elena Mocanu\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"143773678\",\"name\":\"P. Nguyen\"},{\"authorId\":\"1970654\",\"name\":\"M. Gibescu\"},{\"authorId\":\"32651624\",\"name\":\"A. Liotta\"}],\"doi\":\"10.1038/s41467-018-04316-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6dbb9e4b2e3b67dc4e1634989511f67d41373dd0\",\"title\":\"Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science\",\"url\":\"https://www.semanticscholar.org/paper/6dbb9e4b2e3b67dc4e1634989511f67d41373dd0\",\"venue\":\"Nature Communications\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"1747317\",\"name\":\"J. Denker\"},{\"authorId\":\"1759839\",\"name\":\"S. Solla\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e7297db245c3feb1897720b173a59fe7e36babb7\",\"title\":\"Optimal Brain Damage\",\"url\":\"https://www.semanticscholar.org/paper/e7297db245c3feb1897720b173a59fe7e36babb7\",\"venue\":\"NIPS\",\"year\":1989},{\"arxivId\":\"1906.11626\",\"authors\":[{\"authorId\":\"48642038\",\"name\":\"S. Liu\"},{\"authorId\":\"2571038\",\"name\":\"D. Mocanu\"},{\"authorId\":\"144729146\",\"name\":\"M. Pechenizkiy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1b679254cb90f8a9f5a4dc56a309452a354f4b15\",\"title\":\"On improving deep learning generalization with adaptive sparse connectivity\",\"url\":\"https://www.semanticscholar.org/paper/1b679254cb90f8a9f5a4dc56a309452a354f4b15\",\"venue\":\"ArXiv\",\"year\":2019}],\"title\":\"Learning Sparse Neural Networks for Better Generalization\",\"topics\":[{\"topic\":\"Neural network software\",\"topicId\":\"172426\",\"url\":\"https://www.semanticscholar.org/topic/172426\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"}],\"url\":\"https://www.semanticscholar.org/paper/3b5b8e1820641300c38682d25f6f339d4150112d\",\"venue\":\"IJCAI\",\"year\":2020}\n"