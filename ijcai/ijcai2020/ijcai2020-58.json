"{\"abstract\":\"Though limited in real-world decision making, most multi-agent reinforcement learning (MARL) models assume perfectly rational agents -- a property hardly met due to individual's cognitive limitation and/or the tractability of the decision problem. In this paper, we introduce generalized recursive reasoning (GR2) as a novel framework to model agents with different \\\\emph{hierarchical} levels of rationality; our framework enables agents to exhibit varying levels of \\\"thinking\\\" ability thereby allowing higher-level agents to best respond to various less sophisticated learners. We contribute both theoretically and empirically. On the theory side, we devise the hierarchical framework of GR2 through probabilistic graphical models and prove the existence of a perfect Bayesian equilibrium. Within the GR2, we propose a practical actor-critic solver, and demonstrate its convergent property to a stationary point in two-player games through Lyapunov analysis. On the empirical side, we validate our findings on a variety of MARL benchmarks. Precisely, we first illustrate the hierarchical thinking process on the Keynes Beauty Contest, and then demonstrate significant improvements compared to state-of-the-art opponent modeling baselines on the normal-form games and the cooperative navigation benchmark.\",\"arxivId\":\"1901.09216\",\"authors\":[{\"authorId\":\"50531782\",\"name\":\"Ying Wen\",\"url\":\"https://www.semanticscholar.org/author/50531782\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\",\"url\":\"https://www.semanticscholar.org/author/49307876\"},{\"authorId\":\"46584512\",\"name\":\"J. Wang\",\"url\":\"https://www.semanticscholar.org/author/46584512\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2011.00583\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2000281109\",\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"title\":\"An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.11628\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2892430\",\"name\":\"Rasul Tutunov\"},{\"authorId\":\"73774257\",\"name\":\"Phu Sakulwongtana\"},{\"authorId\":\"46257744\",\"name\":\"H. Ammar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf19bf42670be3fa927a3e146ba2cb04596e5ecf\",\"title\":\"$\\\\alpha^{\\\\alpha}$-Rank: Practically Scaling $\\\\alpha$-Rank through Stochastic Optimisation\",\"url\":\"https://www.semanticscholar.org/paper/cf19bf42670be3fa927a3e146ba2cb04596e5ecf\",\"venue\":\"\",\"year\":2019}],\"corpusId\":210839238,\"doi\":\"10.24963/ijcai.2020/58\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"dd34893d012cf9c391e5af72a51d6bf8a220b2b3\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2447023\",\"name\":\"H. J. Marquez\"}],\"doi\":\"10.1109/tac.2004.831172\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"029af492b4d8a839f28507e20c06204d2e629432\",\"title\":\"Nonlinear Control Systems: Analysis and Design\",\"url\":\"https://www.semanticscholar.org/paper/029af492b4d8a839f28507e20c06204d2e629432\",\"venue\":\"IEEE Transactions on Automatic Control\",\"year\":2004},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3081854\",\"name\":\"Tim Genewein\"},{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\"},{\"authorId\":\"2354563\",\"name\":\"D. A. Braun\"}],\"doi\":\"10.3389/frobt.2015.00027\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7814c02fdadf0ac83677220a1b66507d96ba6a60\",\"title\":\"Bounded Rationality, Abstraction, and Hierarchical Decision-Making: An Information-Theoretic Optimality Principle\",\"url\":\"https://www.semanticscholar.org/paper/7814c02fdadf0ac83677220a1b66507d96ba6a60\",\"venue\":\"Front. Robot. AI\",\"year\":2015},{\"arxivId\":\"1802.03216\",\"authors\":[{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\"},{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"}],\"doi\":\"10.24963/ijcai.2018/37\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae\",\"title\":\"Balancing Two-Player Stochastic Games with Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1804.09817\",\"authors\":[{\"authorId\":\"34765120\",\"name\":\"E. Wei\"},{\"authorId\":\"40617392\",\"name\":\"D. Wicke\"},{\"authorId\":\"2434084\",\"name\":\"David Freelan\"},{\"authorId\":\"1706276\",\"name\":\"S. Luke\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"cfbdca4d494fc2a50c25d69486c15b195b42b989\",\"title\":\"Multiagent Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/cfbdca4d494fc2a50c25d69486c15b195b42b989\",\"venue\":\"AAAI Spring Symposia\",\"year\":2018},{\"arxivId\":\"1905.08087\",\"authors\":[{\"authorId\":\"152307789\",\"name\":\"Zheng Tian\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"8200689\",\"name\":\"Zhichen Gong\"},{\"authorId\":\"1383122034\",\"name\":\"Faiz Punakkath\"},{\"authorId\":\"9399556\",\"name\":\"Shihao Zou\"},{\"authorId\":\"48094081\",\"name\":\"J. Wang\"}],\"doi\":\"10.24963/IJCAI.2019/85\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6dd89f30a5a0e8c5404f23d05c27ef32149b9179\",\"title\":\"A Regularized Opponent Model with Maximum Entropy Objective\",\"url\":\"https://www.semanticscholar.org/paper/6dd89f30a5a0e8c5404f23d05c27ef32149b9179\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1805.00909\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba\",\"title\":\"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\",\"url\":\"https://www.semanticscholar.org/paper/6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48057653\",\"name\":\"O. Bagasra\"}],\"doi\":\"10.1073/PNAS.95.17.10344-D\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"title\":\"PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES.\",\"url\":\"https://www.semanticscholar.org/paper/edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"venue\":\"Science\",\"year\":1915},{\"arxivId\":\"1809.03738\",\"authors\":[{\"authorId\":\"47558309\",\"name\":\"Y. Chen\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"3077686\",\"name\":\"Y. Su\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"37510526\",\"name\":\"Dell Zhang\"},{\"authorId\":\"39811558\",\"name\":\"J. Wang\"},{\"authorId\":\"49957601\",\"name\":\"Han Liu\"}],\"doi\":\"10.1145/3356464.3357707\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5dbcdbc41f85d2200266ce5b6c993e6fba794539\",\"title\":\"Factorized Q-learning for large-scale multi-agent systems\",\"url\":\"https://www.semanticscholar.org/paper/5dbcdbc41f85d2200266ce5b6c993e6fba794539\",\"venue\":\"DAI\",\"year\":2019},{\"arxivId\":\"1703.10069\",\"authors\":[{\"authorId\":\"144189270\",\"name\":\"P. Peng\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"145001852\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":\"95115833\",\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"76dfb1ab698963f3776fe894b3743db4a5419a5f\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/76dfb1ab698963f3776fe894b3743db4a5419a5f\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"87972593\",\"name\":\"J. Schumpeter\"},{\"authorId\":\"16992205\",\"name\":\"J. M. Keynes\"}],\"doi\":\"10.2307/2278703\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c92757aaf4c9c0ad24f8486fccfc9ad3f7054b28\",\"title\":\"The General Theory of Employment, Interest and Money.\",\"url\":\"https://www.semanticscholar.org/paper/c92757aaf4c9c0ad24f8486fccfc9ad3f7054b28\",\"venue\":\"\",\"year\":1936},{\"arxivId\":\"1901.06085\",\"authors\":[{\"authorId\":\"35972571\",\"name\":\"M. Shum\"},{\"authorId\":\"1390033240\",\"name\":\"Max Kleiman-Weiner\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":\"10.1609/aaai.v33i01.33016163\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bac030b06e42c6b93fdebaab7260c320f11bb8e8\",\"title\":\"Theory of Minds: Understanding Behavior in Groups Through Inverse Planning\",\"url\":\"https://www.semanticscholar.org/paper/bac030b06e42c6b93fdebaab7260c320f11bb8e8\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1802.07740\",\"authors\":[{\"authorId\":\"3422052\",\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":\"3054009\",\"name\":\"Frank Perbet\"},{\"authorId\":\"143745193\",\"name\":\"H. Song\"},{\"authorId\":\"40313479\",\"name\":\"C. Zhang\"},{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"},{\"authorId\":\"46378362\",\"name\":\"M. Botvinick\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"856d5dcba4772328b8fb784494e3d41d39669b0d\",\"title\":\"Machine Theory of Mind\",\"url\":\"https://www.semanticscholar.org/paper/856d5dcba4772328b8fb784494e3d41d39669b0d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1608.04471\",\"authors\":[{\"authorId\":\"47362268\",\"name\":\"Qiang Liu\"},{\"authorId\":\"2848320\",\"name\":\"Dilin Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"768f7353718c6d95f2d63f954f2236369a409135\",\"title\":\"Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/768f7353718c6d95f2d63f954f2236369a409135\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Piotr J Gmytrasiewicz\"},{\"authorId\":null,\"name\":\"Prashant Doshi. A framework for sequential planning in mult settings\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"24:49\\u201379,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2050820\",\"name\":\"R. McKelvey\"},{\"authorId\":\"1910640\",\"name\":\"T. Palfrey\"}],\"doi\":\"10.1006/GAME.1995.1023\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6ceef2b165aa5baabca8be97bb3ba016324852a9\",\"title\":\"Quantal Response Equilibria for Normal Form Games\",\"url\":\"https://www.semanticscholar.org/paper/6ceef2b165aa5baabca8be97bb3ba016324852a9\",\"venue\":\"\",\"year\":1995},{\"arxivId\":\"1901.09207\",\"authors\":[{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"48846956\",\"name\":\"R. Luo\"},{\"authorId\":null,\"name\":\"Jun Wang\"},{\"authorId\":\"144282470\",\"name\":\"W. Pan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"47a5219cb2ae1ee1c446fc4166e37fe75d91f0b1\",\"title\":\"Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/47a5219cb2ae1ee1c446fc4166e37fe75d91f0b1\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2211151\",\"name\":\"C. Camerer\"},{\"authorId\":\"39643451\",\"name\":\"T. Ho\"},{\"authorId\":\"37713813\",\"name\":\"J. Chong\"}],\"doi\":\"10.1162/0033553041502225\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"84d8b2edc284f57701399c7a99d5a74897e4ace2\",\"title\":\"A Cognitive Hierarchy Model of Games\",\"url\":\"https://www.semanticscholar.org/paper/84d8b2edc284f57701399c7a99d5a74897e4ace2\",\"venue\":\"\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Caroline Claus\"},{\"authorId\":null,\"name\":\"Craig Boutilier. The dynamics of reinforcement learning  systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AAAI\",\"url\":\"\",\"venue\":\"1998:746\\u2013752,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3335161\",\"name\":\"G. Devetag\"},{\"authorId\":\"2210986\",\"name\":\"M. Warglien\"}],\"doi\":\"10.1016/S0167-4870(02)00202-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e58ae054f84ca6a57679e88708f4b0cfbc68b438\",\"title\":\"Games and Phone Numbers: Do Short Term Memory Bounds Affect Strategic Behavior?\",\"url\":\"https://www.semanticscholar.org/paper/e58ae054f84ca6a57679e88708f4b0cfbc68b438\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"TimGenewein\"},{\"authorId\":null,\"name\":\"Felix Leibfried\"},{\"authorId\":null,\"name\":\"Jordi Grau-Moya\"},{\"authorId\":null,\"name\":\"Daniel Alexander Braun. Bounded rationality\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"abstraction\",\"url\":\"\",\"venue\":\"and hierarchical decision-making: An information-theoretic optimality principle. Frontiers in Robotics and AI, 2:27,\",\"year\":2015},{\"arxivId\":\"1802.05438\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"48846956\",\"name\":\"R. Luo\"},{\"authorId\":\"36069327\",\"name\":\"Minne Li\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"39811558\",\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"633870d249f03d39849224adc3381712fbb23ed8\",\"title\":\"Mean Field Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/633870d249f03d39849224adc3381712fbb23ed8\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1610.03295\",\"authors\":[{\"authorId\":\"1389955537\",\"name\":\"S. Shalev-Shwartz\"},{\"authorId\":\"40364567\",\"name\":\"Shaked Shammah\"},{\"authorId\":\"3140335\",\"name\":\"A. Shashua\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"75a760c6bd5ae15e0fc489a074bc42bc1fc4e697\",\"title\":\"Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving\",\"url\":\"https://www.semanticscholar.org/paper/75a760c6bd5ae15e0fc489a074bc42bc1fc4e697\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1709.08071\",\"authors\":[{\"authorId\":\"1961238\",\"name\":\"Stefano V. Albrecht\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1016/j.artint.2018.01.002\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9bf2d6526b480f95fed9b0e0a1125cf07d11e01d\",\"title\":\"Autonomous agents modelling other agents: A comprehensive survey and open problems\",\"url\":\"https://www.semanticscholar.org/paper/9bf2d6526b480f95fed9b0e0a1125cf07d11e01d\",\"venue\":\"Artif. Intell.\",\"year\":2018},{\"arxivId\":\"1702.08165\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"title\":\"Reinforcement Learning with Deep Energy-Based Policies\",\"url\":\"https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1601.00670\",\"authors\":[{\"authorId\":\"1796335\",\"name\":\"D. Blei\"},{\"authorId\":\"3081817\",\"name\":\"A. Kucukelbir\"},{\"authorId\":\"2755213\",\"name\":\"J. McAuliffe\"}],\"doi\":\"10.1080/01621459.2017.1285773\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f24d7a6e1c88828e18d16c6db20f5329f6a6827\",\"title\":\"Variational Inference: A Review for Statisticians\",\"url\":\"https://www.semanticscholar.org/paper/6f24d7a6e1c88828e18d16c6db20f5329f6a6827\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ryan Lowe\"},{\"authorId\":null,\"name\":\"Yi Wu\"},{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"OpenAI Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Igor Mordatch. Multiagent actor-critic for mixed cooper environments\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 6379\\u20136390,\",\"year\":2017},{\"arxivId\":\"1803.06073\",\"authors\":[{\"authorId\":\"51112227\",\"name\":\"A. Taylor\"},{\"authorId\":\"3175450\",\"name\":\"Bryan Van Scoy\"},{\"authorId\":\"1723445\",\"name\":\"Laurent Lessard\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff35df07aa3ece939a9380f5fdced03925513cfe\",\"title\":\"Lyapunov Functions for First-Order Methods: Tight Automated Convergence Guarantees\",\"url\":\"https://www.semanticscholar.org/paper/ff35df07aa3ece939a9380f5fdced03925513cfe\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Robin Hunicke. The case for dynamic difficulty adjustmen games\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 2005 ACM SIGCHI ACE\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145187018\",\"name\":\"D. Levin\"},{\"authorId\":\"47059450\",\"name\":\"Luyao Zhang\"}],\"doi\":\"10.2139/ssrn.2934696\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"9ec00d33ec459d08c3c2c43c09d61edea0a3f55b\",\"title\":\"Bridging Level-K to Nash Equilibrium\",\"url\":\"https://www.semanticscholar.org/paper/9ec00d33ec459d08c3c2c43c09d61edea0a3f55b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144524233\",\"name\":\"D. Gooch\"}],\"doi\":\"10.1145/2043236.2043240\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"951e4467820029305bd729887d7ab4b9a0772c4a\",\"title\":\"Communications of the ACM\",\"url\":\"https://www.semanticscholar.org/paper/951e4467820029305bd729887d7ab4b9a0772c4a\",\"venue\":\"XRDS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46692357\",\"name\":\"E. Margolis\"},{\"authorId\":\"144850691\",\"name\":\"R. Samuels\"},{\"authorId\":\"47719936\",\"name\":\"S. Stich\"}],\"doi\":\"10.5860/choice.49-6814\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e441e3f53ad1009c91f21f48b9fa47713d63c7f9\",\"title\":\"The Oxford handbook of philosophy of cognitive science\",\"url\":\"https://www.semanticscholar.org/paper/e441e3f53ad1009c91f21f48b9fa47713d63c7f9\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102106648\",\"name\":\"C. Macdougall\"},{\"authorId\":\"122492158\",\"name\":\"H. H. Wilson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"130218d9a3321b2991a72498a35d020d98f5f2ec\",\"title\":\"The decision and the organization\",\"url\":\"https://www.semanticscholar.org/paper/130218d9a3321b2991a72498a35d020d98f5f2ec\",\"venue\":\"\",\"year\":1965},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"},{\"authorId\":\"1725492\",\"name\":\"V. Lesser\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"593678c350e955f9fe4e0b1ac7f51a74b026709a\",\"title\":\"Multi-Agent Learning with Policy Prediction\",\"url\":\"https://www.semanticscholar.org/paper/593678c350e955f9fe4e0b1ac7f51a74b026709a\",\"venue\":\"AAAI\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701353\",\"name\":\"Y. Shoham\"},{\"authorId\":\"38076874\",\"name\":\"R. Powers\"},{\"authorId\":\"3050250\",\"name\":\"Trond Grenager\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a69e33ce29c451655d917bf9387ed57e115fcfc7\",\"title\":\"Multi-Agent Reinforcement Learning:a critical survey\",\"url\":\"https://www.semanticscholar.org/paper/a69e33ce29c451655d917bf9387ed57e115fcfc7\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"16992205\",\"name\":\"J. M. Keynes\"}],\"doi\":\"10.2307/1882087\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a2d3b58dc5048da7b1d6e707edc472cee105ea1a\",\"title\":\"The General Theory of Employment\",\"url\":\"https://www.semanticscholar.org/paper/a2d3b58dc5048da7b1d6e707edc472cee105ea1a\",\"venue\":\"\",\"year\":1937},{\"arxivId\":\"1609.05559\",\"authors\":[{\"authorId\":\"91070223\",\"name\":\"He He\"},{\"authorId\":\"1389036863\",\"name\":\"Jordan L. Boyd-Graber\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5\",\"title\":\"Opponent Modeling in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46636184\",\"name\":\"P. Peng\"},{\"authorId\":\"145001851\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yaodong Yang\"},{\"authorId\":null,\"name\":\"Lantao Yu\"},{\"authorId\":null,\"name\":\"Yiwei Bai\"},{\"authorId\":null,\"name\":\"Ying Wen\"},{\"authorId\":null,\"name\":\"Weinan Zhang\"},{\"authorId\":null,\"name\":\"Jun Wang. A study of ai population dynamics with milli learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 17th AAMAS\",\"url\":\"\",\"venue\":\"pages 2133\\u20132135,\",\"year\":2018}],\"title\":\"Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning\",\"topics\":[{\"topic\":\"Nash equilibrium\",\"topicId\":\"17596\",\"url\":\"https://www.semanticscholar.org/topic/17596\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Rationality\",\"topicId\":\"954\",\"url\":\"https://www.semanticscholar.org/topic/954\"},{\"topic\":\"Game theory\",\"topicId\":\"17593\",\"url\":\"https://www.semanticscholar.org/topic/17593\"},{\"topic\":\"Recursion (computer science)\",\"topicId\":\"2419\",\"url\":\"https://www.semanticscholar.org/topic/2419\"},{\"topic\":\"Numerous\",\"topicId\":\"16861\",\"url\":\"https://www.semanticscholar.org/topic/16861\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Randomness\",\"topicId\":\"726\",\"url\":\"https://www.semanticscholar.org/topic/726\"},{\"topic\":\"Converge\",\"topicId\":\"205534\",\"url\":\"https://www.semanticscholar.org/topic/205534\"},{\"topic\":\"Reasoning - publishing subsection\",\"topicId\":\"450845\",\"url\":\"https://www.semanticscholar.org/topic/450845\"},{\"topic\":\"Categories\",\"topicId\":\"1167\",\"url\":\"https://www.semanticscholar.org/topic/1167\"},{\"topic\":\"Behavior\",\"topicId\":\"3332\",\"url\":\"https://www.semanticscholar.org/topic/3332\"},{\"topic\":\"Actor model\",\"topicId\":\"250489\",\"url\":\"https://www.semanticscholar.org/topic/250489\"},{\"topic\":\"Embedding\",\"topicId\":\"3610\",\"url\":\"https://www.semanticscholar.org/topic/3610\"},{\"topic\":\"Kind of quantity - Equilibrium\",\"topicId\":\"56101\",\"url\":\"https://www.semanticscholar.org/topic/56101\"}],\"url\":\"https://www.semanticscholar.org/paper/dd34893d012cf9c391e5af72a51d6bf8a220b2b3\",\"venue\":\"IJCAI\",\"year\":2020}\n"