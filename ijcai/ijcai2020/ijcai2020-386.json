"{\"abstract\":\"A major challenge in reinforcement learning is exploration, when local dithering methods such as epsilon-greedy sampling are insufficient to solve a given task. Many recent methods have proposed to intrinsically motivate an agent to seek novel states, driving the agent to discover improved reward. However, while state-novelty exploration methods are suitable for tasks where novel observations correlate well with improved reward, they may not explore more efficiently than epsilon-greedy approaches in environments where the two are not well-correlated. In this paper, we distinguish between exploration tasks in which seeking novel states aids in finding new reward, and those where it does not, such as goal-conditioned tasks and escaping local reward maxima. We propose a new exploration objective, maximizing the reward prediction error (RPE) of a value function trained to predict extrinsic reward. We then propose a deep reinforcement learning method, QXplore, which exploits the temporal difference error of a Q-function to solve hard exploration tasks in high-dimensional MDPs. We demonstrate the exploration behavior of QXplore on several OpenAI Gym MuJoCo tasks and Atari games and observe that QXplore is comparable to or better than a baseline state-novelty method in all cases, outperforming the baseline on tasks where state novelty is not well-correlated with improved reward.\",\"arxivId\":\"1906.08189\",\"authors\":[{\"authorId\":\"1410466749\",\"name\":\"Riley Simmons-Edler\",\"url\":\"https://www.semanticscholar.org/author/1410466749\"},{\"authorId\":\"35488707\",\"name\":\"Ben Eisner\",\"url\":\"https://www.semanticscholar.org/author/35488707\"},{\"authorId\":\"30814776\",\"name\":\"D. Yang\",\"url\":\"https://www.semanticscholar.org/author/30814776\"},{\"authorId\":\"30814125\",\"name\":\"Anthony Bisulco\",\"url\":\"https://www.semanticscholar.org/author/30814125\"},{\"authorId\":\"49688913\",\"name\":\"Eric Mitchell\",\"url\":\"https://www.semanticscholar.org/author/49688913\"},{\"authorId\":\"115507096\",\"name\":\"S. Seung\",\"url\":\"https://www.semanticscholar.org/author/115507096\"},{\"authorId\":\"11131177\",\"name\":\"D. Lee\",\"url\":\"https://www.semanticscholar.org/author/11131177\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2010.02255\",\"authors\":[{\"authorId\":\"46212062\",\"name\":\"Sebastian Flennerhag\"},{\"authorId\":\"13256987\",\"name\":\"J. X. Wang\"},{\"authorId\":\"2905900\",\"name\":\"P. Sprechmann\"},{\"authorId\":\"2077146\",\"name\":\"Francesco Visin\"},{\"authorId\":\"51980959\",\"name\":\"Alexandre Galashov\"},{\"authorId\":\"67007190\",\"name\":\"Steven Kapturowski\"},{\"authorId\":\"2311858\",\"name\":\"Diana Borsa\"},{\"authorId\":\"1599360864\",\"name\":\"Nicolas Heess\"},{\"authorId\":\"2284885\",\"name\":\"A. Barreto\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a8d0e1adc158e0a82f99b02dfb0eb30e62c61443\",\"title\":\"Temporal Difference Uncertainties as a Signal for Exploration\",\"url\":\"https://www.semanticscholar.org/paper/a8d0e1adc158e0a82f99b02dfb0eb30e62c61443\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":216641976,\"doi\":\"10.24963/ijcai.2020/386\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"7b20998b1191a155abe8bb8777c02eaf3eb8a4d6\",\"references\":[{\"arxivId\":\"1808.04355\",\"authors\":[{\"authorId\":\"3080409\",\"name\":\"Yuri Burda\"},{\"authorId\":\"144632352\",\"name\":\"H. Edwards\"},{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ca14dce53be20d3d23d4f0db844a8389ab619db3\",\"title\":\"Large-Scale Study of Curiosity-Driven Learning\",\"url\":\"https://www.semanticscholar.org/paper/ca14dce53be20d3d23d4f0db844a8389ab619db3\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1705.05363\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"33932184\",\"name\":\"Pulkit Agrawal\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPRW.2017.70\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"225ab689f41cef1dc18237ef5dab059a49950abf\",\"title\":\"Curiosity-Driven Exploration by Self-Supervised Prediction\",\"url\":\"https://www.semanticscholar.org/paper/225ab689f41cef1dc18237ef5dab059a49950abf\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1707.01495\",\"authors\":[{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"},{\"authorId\":\"150074096\",\"name\":\"Dwight Crow\"},{\"authorId\":\"6672240\",\"name\":\"Alex Ray\"},{\"authorId\":\"145540305\",\"name\":\"J. Schneider\"},{\"authorId\":\"39492797\",\"name\":\"Rachel H Fong\"},{\"authorId\":\"2930640\",\"name\":\"P. Welinder\"},{\"authorId\":\"39593364\",\"name\":\"Bob McGrew\"},{\"authorId\":\"48104547\",\"name\":\"Josh Tobin\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"429ed4c9845d0abd1f8204e1d7705919559bc2a2\",\"title\":\"Hindsight Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/429ed4c9845d0abd1f8204e1d7705919559bc2a2\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34257663\",\"name\":\"Clement Gehring\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4e589a03517aef45106b042a5fd594d9ef54c68\",\"title\":\"Smart exploration in reinforcement learning using absolute temporal difference errors\",\"url\":\"https://www.semanticscholar.org/paper/e4e589a03517aef45106b042a5fd594d9ef54c68\",\"venue\":\"AAMAS\",\"year\":2013},{\"arxivId\":\"1903.10605\",\"authors\":[{\"authorId\":\"1410466749\",\"name\":\"Riley Simmons-Edler\"},{\"authorId\":\"35488707\",\"name\":\"Ben Eisner\"},{\"authorId\":\"49688913\",\"name\":\"Eric Mitchell\"},{\"authorId\":\"144924970\",\"name\":\"H. S. Seung\"},{\"authorId\":\"145675444\",\"name\":\"D. Lee\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab8b9bf57675a4a7c28cbb1dc176382886d0f332\",\"title\":\"Q-Learning for Continuous Actions with Cross-Entropy Guided Policies\",\"url\":\"https://www.semanticscholar.org/paper/ab8b9bf57675a4a7c28cbb1dc176382886d0f332\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1583061741\",\"name\":\"Adrien Ali Taiga\"},{\"authorId\":\"26958176\",\"name\":\"W. Fedus\"},{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"cf76a4b70f96ebd7971b89e373a0bfc6b4e47875\",\"title\":\"On Bonus Based Exploration Methods In The Arcade Learning Environment\",\"url\":\"https://www.semanticscholar.org/paper/cf76a4b70f96ebd7971b89e373a0bfc6b4e47875\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Bellemare\"},{\"authorId\":null,\"name\":\"Sriram Srinivasan\"},{\"authorId\":null,\"name\":\"Georg Ostrovski\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"David Saxton\"},{\"authorId\":null,\"name\":\"Remi Munos. Unifying count-based exploration\"},{\"authorId\":null,\"name\":\"intrinsic motivation\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 1471\\u20131479,\",\"year\":2016},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrychowicz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation\",\"url\":\"\",\"venue\":\"ICML, 2019. Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence\",\"year\":2013},{\"arxivId\":\"1703.01260\",\"authors\":[{\"authorId\":\"2550764\",\"name\":\"Justin Fu\"},{\"authorId\":\"1388383230\",\"name\":\"John D. Co-Reyes\"},{\"authorId\":\"152198491\",\"name\":\"Sergey Levine\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"850d78496304829d16d14701e4d81692f088f47d\",\"title\":\"EX2: Exploration with Exemplar Models for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/850d78496304829d16d14701e4d81692f088f47d\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bradly C Stadie\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Incentivizing Exploration In Reinforcement Models\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1507.0,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"317cd4522b1f4a6f889743578143bb8823623f8b\",\"title\":\"VIME: Variational Information Maximizing Exploration\",\"url\":\"https://www.semanticscholar.org/paper/317cd4522b1f4a6f889743578143bb8823623f8b\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1901.10995\",\"authors\":[{\"authorId\":\"66821245\",\"name\":\"Adrien Ecoffet\"},{\"authorId\":\"39378983\",\"name\":\"J. Huizinga\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c520bf47db3360ae3a52219771390a354ed8a91f\",\"title\":\"Go-Explore: a New Approach for Hard-Exploration Problems\",\"url\":\"https://www.semanticscholar.org/paper/c520bf47db3360ae3a52219771390a354ed8a91f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pablo Samuel Castro\"},{\"authorId\":null,\"name\":\"Subhodeep Moitra\"},{\"authorId\":null,\"name\":\"Carles Gelada\"},{\"authorId\":null,\"name\":\"Saurabh Kumar\"},{\"authorId\":null,\"name\":\"G Marc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bellemare\",\"url\":\"\",\"venue\":\"Dopamine: A Research Framework for Deep Reinforcement Learning.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33317877\",\"name\":\"Zhang-Wei Hong\"},{\"authorId\":\"41020222\",\"name\":\"Tsu-Jui Fu\"},{\"authorId\":\"48441912\",\"name\":\"Tzu-Yun Shann\"},{\"authorId\":\"30595482\",\"name\":\"Y. Chang\"},{\"authorId\":\"49010592\",\"name\":\"Chun-Yi Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca8e08f684d0409effbb19a69bfbfe3cef4348f7\",\"title\":\"Adversarial Exploration Strategy for Self-Supervised Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/ca8e08f684d0409effbb19a69bfbfe3cef4348f7\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1810.12894\",\"authors\":[{\"authorId\":\"3080409\",\"name\":\"Yuri Burda\"},{\"authorId\":\"144632352\",\"name\":\"H. Edwards\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4cb3fd057949624aa4f0bbe7a6dcc8777ff04758\",\"title\":\"Exploration by Random Network Distillation\",\"url\":\"https://www.semanticscholar.org/paper/4cb3fd057949624aa4f0bbe7a6dcc8777ff04758\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1806.10293\",\"authors\":[{\"authorId\":\"48313860\",\"name\":\"D. Kalashnikov\"},{\"authorId\":\"17818078\",\"name\":\"A. Irpan\"},{\"authorId\":\"143970835\",\"name\":\"P. Pastor\"},{\"authorId\":\"46920727\",\"name\":\"J. Ibarz\"},{\"authorId\":\"49199760\",\"name\":\"A. Herzog\"},{\"authorId\":\"145116380\",\"name\":\"Eric Jang\"},{\"authorId\":\"47202040\",\"name\":\"Deirdre Quillen\"},{\"authorId\":\"29891985\",\"name\":\"Ethan Holly\"},{\"authorId\":\"1729262\",\"name\":\"Mrinal Kalakrishnan\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eb37e7b76d26b75463df22b2a3aa32b6a765c672\",\"title\":\"QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/eb37e7b76d26b75463df22b2a3aa32b6a765c672\",\"venue\":\"CoRL\",\"year\":2018},{\"arxivId\":\"1804.04012\",\"authors\":[{\"authorId\":\"41019330\",\"name\":\"Leshem Choshen\"},{\"authorId\":\"113940484\",\"name\":\"Lior Fox\"},{\"authorId\":\"2934154\",\"name\":\"Y. Loewenstein\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"89e9c05ea3b45628beba8134fd5c873dd55003e8\",\"title\":\"DORA The Explorer: Directed Outreaching Reinforcement Action-Selection\",\"url\":\"https://www.semanticscholar.org/paper/89e9c05ea3b45628beba8134fd5c873dd55003e8\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1812.06110\",\"authors\":[{\"authorId\":\"39163115\",\"name\":\"P. S. Castro\"},{\"authorId\":\"3316330\",\"name\":\"Subhodeep Moitra\"},{\"authorId\":\"52382152\",\"name\":\"Carles Gelada\"},{\"authorId\":\"39703750\",\"name\":\"S. Kumar\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e372ed2b688de0e4dcffbec1d2abdd0fc7ea27a\",\"title\":\"Dopamine: A Research Framework for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8e372ed2b688de0e4dcffbec1d2abdd0fc7ea27a\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1802.09477\",\"authors\":[{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"},{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"51174612\",\"name\":\"David Meger\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4debb99c0c63bfaa97dd433bc2828e4dac81c48b\",\"title\":\"Addressing Function Approximation Error in Actor-Critic Methods\",\"url\":\"https://www.semanticscholar.org/paper/4debb99c0c63bfaa97dd433bc2828e4dac81c48b\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Riashat Islam\"},{\"authorId\":null,\"name\":\"Peter Henderson\"},{\"authorId\":null,\"name\":\"Maziar Gomrokchi\"},{\"authorId\":null,\"name\":\"Doina Precup. Reproducibility of Benchmarked Deep Reinfo Control\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1708.0,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1806.03335\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"},{\"authorId\":\"51042571\",\"name\":\"Albin Cassirer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f802802b3af5a22b79ac65d033ba3cbee33da91b\",\"title\":\"Randomized Prior Functions for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f802802b3af5a22b79ac65d033ba3cbee33da91b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144867807\",\"name\":\"S. Thrun\"},{\"authorId\":\"144208331\",\"name\":\"K. M\\u00f6ller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c9a2e378e17bd86f30059ae29ce8c2ef7272acb\",\"title\":\"Active Exploration in Dynamic Environments\",\"url\":\"https://www.semanticscholar.org/paper/8c9a2e378e17bd86f30059ae29ce8c2ef7272acb\",\"venue\":\"NIPS\",\"year\":1991},{\"arxivId\":\"1611.04717\",\"authors\":[{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"22966825\",\"name\":\"Davis Foote\"},{\"authorId\":\"47541311\",\"name\":\"Adam Stooke\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf\",\"title\":\"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J\\u00fcrgen Schmidhuber. Adaptive confidence\"},{\"authorId\":null,\"name\":\"adaptive curiosity. In Institut fur Informatik\"},{\"authorId\":null,\"name\":\"Technische Universitat Munchen\"},{\"authorId\":null,\"name\":\"Arcisstr\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"21\",\"url\":\"\",\"venue\":\"800 Munchen 2. Citeseer,\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9182876\",\"name\":\"HyoungSeok Kim\"},{\"authorId\":\"65924935\",\"name\":\"Jaekyeom Kim\"},{\"authorId\":\"144662720\",\"name\":\"Yeonwoo Jeong\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"2133680\",\"name\":\"Hyun Oh Song\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"51ecd565f8a394b1907b3d37722435a573ce7b72\",\"title\":\"EMI: Exploration with Mutual Information\",\"url\":\"https://www.semanticscholar.org/paper/51ecd565f8a394b1907b3d37722435a573ce7b72\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1808.00177\",\"authors\":[{\"authorId\":\"1471481702\",\"name\":\"OpenAI: Marcin Andrychowicz\"},{\"authorId\":\"40566201\",\"name\":\"Bowen Baker\"},{\"authorId\":\"36045639\",\"name\":\"Maciek Chociej\"},{\"authorId\":\"1944541\",\"name\":\"R. J\\u00f3zefowicz\"},{\"authorId\":\"39593364\",\"name\":\"Bob McGrew\"},{\"authorId\":\"2713380\",\"name\":\"Jakub W. Pachocki\"},{\"authorId\":\"6817951\",\"name\":\"Arthur Petron\"},{\"authorId\":\"3407285\",\"name\":\"Matthias Plappert\"},{\"authorId\":\"46818887\",\"name\":\"Glenn Powell\"},{\"authorId\":\"6672240\",\"name\":\"Alex Ray\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"2700360\",\"name\":\"S. Sidor\"},{\"authorId\":\"48104547\",\"name\":\"Josh Tobin\"},{\"authorId\":\"2930640\",\"name\":\"P. Welinder\"},{\"authorId\":\"2617057\",\"name\":\"Lilian Weng\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":\"10.1177/0278364919887447\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d37a34c204a8beefcaef4dddddb7a90c16e973d4\",\"title\":\"Learning dexterous in-hand manipulation\",\"url\":\"https://www.semanticscholar.org/paper/d37a34c204a8beefcaef4dddddb7a90c16e973d4\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2020},{\"arxivId\":\"1606.01868\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"143810408\",\"name\":\"D. Saxton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6e90fd78e8a3b98af3954aae5209703aa966603e\",\"title\":\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1802.05054\",\"authors\":[{\"authorId\":\"102281182\",\"name\":\"C\\u00e9dric Colas\"},{\"authorId\":\"3211142\",\"name\":\"Olivier Sigaud\"},{\"authorId\":\"1720664\",\"name\":\"Pierre-Yves Oudeyer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1fdf0319b4a1db62c611980351e5f4c2f08958cd\",\"title\":\"GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/1fdf0319b4a1db62c611980351e5f4c2f08958cd\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1511.06581\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"title\":\"Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1507.00814\",\"authors\":[{\"authorId\":\"3275284\",\"name\":\"Bradly C. Stadie\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2470fcf0f89082de874ac9133ccb3a8667dd89a8\",\"title\":\"Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models\",\"url\":\"https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1709.06560\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"33690ff21ef1efb576410e656f2e60c89d0307d6\",\"title\":\"Deep Reinforcement Learning that Matters\",\"url\":\"https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6\",\"venue\":\"AAAI\",\"year\":2018}],\"title\":\"Reward Prediction Error as an Exploration Objective in Deep RL\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Temporal difference learning\",\"topicId\":\"102033\",\"url\":\"https://www.semanticscholar.org/topic/102033\"},{\"topic\":\"Sparse matrix\",\"topicId\":\"126\",\"url\":\"https://www.semanticscholar.org/topic/126\"},{\"topic\":\"Quantum state\",\"topicId\":\"59448\",\"url\":\"https://www.semanticscholar.org/topic/59448\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Information seeking behavior\",\"topicId\":\"14808\",\"url\":\"https://www.semanticscholar.org/topic/14808\"}],\"url\":\"https://www.semanticscholar.org/paper/7b20998b1191a155abe8bb8777c02eaf3eb8a4d6\",\"venue\":\"\",\"year\":2019}\n"