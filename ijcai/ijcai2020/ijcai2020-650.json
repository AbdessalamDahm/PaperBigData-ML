"{\"abstract\":\"Deep reinforcement learning applied to visionbased problems like Atari games maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating image processing from decisionmaking, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations, and Direct Residuals Sparse Coding encodes observations by aiming for highest information inclusion. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game\\u2019s controls). These are still capable of achieving results comparable\\u2014and occasionally superior\\u2014to state-of-the-art techniques which use two orders of magnitude more neurons.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2004477\",\"name\":\"Giuseppe Cuccu\",\"url\":\"https://www.semanticscholar.org/author/2004477\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\",\"url\":\"https://www.semanticscholar.org/author/1810053\"},{\"authorId\":\"1393644275\",\"name\":\"P. Cudr\\u00e9-Mauroux\",\"url\":\"https://www.semanticscholar.org/author/1393644275\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":220484142,\"doi\":\"10.24963/ijcai.2020/651\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":false,\"paperId\":\"030b276c519351e72abfced55a166f7ca451b696\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"30811893\",\"name\":\"M. Pelikan\"},{\"authorId\":\"145139401\",\"name\":\"J. Branke\"}],\"doi\":\"10.1145/1830483\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"675e7481f50c2568f575070de4a2e45afc478e71\",\"title\":\"Proceedings of the 12th annual conference on Genetic and evolutionary computation\",\"url\":\"https://www.semanticscholar.org/paper/675e7481f50c2568f575070de4a2e45afc478e71\",\"venue\":\"\",\"year\":2010},{\"arxivId\":\"1806.01363\",\"authors\":[{\"authorId\":\"2004477\",\"name\":\"Giuseppe Cuccu\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"},{\"authorId\":\"1393644275\",\"name\":\"P. Cudr\\u00e9-Mauroux\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6201fdf649a54892cce3786dc88ad0d074661243\",\"title\":\"Playing Atari with Six Neurons\",\"url\":\"https://www.semanticscholar.org/paper/6201fdf649a54892cce3786dc88ad0d074661243\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143901530\",\"name\":\"Xin Yao\"}],\"doi\":\"10.7551/mitpress/9898.003.0014\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ac303258fd7f522fd3e4f172b97bb17eb888598\",\"title\":\"Evolving Artificial Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/5ac303258fd7f522fd3e4f172b97bb17eb888598\",\"venue\":\"\",\"year\":1999},{\"arxivId\":\"1708.07902\",\"authors\":[{\"authorId\":\"2775866\",\"name\":\"Niels Justesen\"},{\"authorId\":\"14171685\",\"name\":\"Philip Bontrager\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"},{\"authorId\":\"1745664\",\"name\":\"S. Risi\"}],\"doi\":\"10.1109/TG.2019.2896986\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1f2bc5d57ccbf5a04e7fea87f1f4db464f533ca8\",\"title\":\"Deep Learning for Video Game Playing\",\"url\":\"https://www.semanticscholar.org/paper/1f2bc5d57ccbf5a04e7fea87f1f4db464f533ca8\",\"venue\":\"IEEE Transactions on Games\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1683199\",\"name\":\"A. Freitas\"}],\"doi\":\"10.1007/978-0-387-30164-8_272\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cde5e660ed0ad83cbe8af274ee39dc97e2242d2\",\"title\":\"Evolutionary Computation\",\"url\":\"https://www.semanticscholar.org/paper/8cde5e660ed0ad83cbe8af274ee39dc97e2242d2\",\"venue\":\"Encyclopedia of Machine Learning\",\"year\":2010},{\"arxivId\":\"1712.06567\",\"authors\":[{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"8309711\",\"name\":\"V. Madhavan\"},{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ba3ace39f1f1afb6651ef4c0e4b8317fd9d48fcf\",\"title\":\"Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ba3ace39f1f1afb6651ef4c0e4b8317fd9d48fcf\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1710.02298\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"title\":\"Rainbow: Combining Improvements in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742820\",\"name\":\"D. Floreano\"},{\"authorId\":\"2515603\",\"name\":\"P. D\\u00fcrr\"},{\"authorId\":\"2288013\",\"name\":\"C. Mattiussi\"}],\"doi\":\"10.1007/s12065-007-0002-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf7bdff21a875e5d043514ed0714fafae77e1492\",\"title\":\"Neuroevolution: from architectures to learning\",\"url\":\"https://www.semanticscholar.org/paper/cf7bdff21a875e5d043514ed0714fafae77e1492\",\"venue\":\"Evol. Intell.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1686788\",\"name\":\"R. Miikkulainen\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1109/TCIAIG.2013.2294713\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf6c64b87459a3164ad54128fa085328c401c09f\",\"title\":\"A Neuroevolution Approach to General Atari Game Playing\",\"url\":\"https://www.semanticscholar.org/paper/cf6c64b87459a3164ad54128fa085328c401c09f\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2014},{\"arxivId\":\"1703.03864\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"4ee802a58d32aa049d549d06be440ac947b53987\",\"title\":\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Greg Brockman\"},{\"authorId\":null,\"name\":\"Vicki Cheung\"},{\"authorId\":null,\"name\":\"Ludwig Pettersson\"},{\"authorId\":null,\"name\":\"Jonas Schneider\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Jie Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Wojciech Zaremba\",\"url\":\"\",\"venue\":\"Openai gym,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50881207\",\"name\":\"Barteld Kooi\"},{\"authorId\":\"2512398\",\"name\":\"H. V. Ditmarsch\"},{\"authorId\":\"1706100\",\"name\":\"W. Hoek\"}],\"doi\":\"10.5555/3306127\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"682646e296ad9e559edf75e7f222b0a825273d54\",\"title\":\"Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems\",\"url\":\"https://www.semanticscholar.org/paper/682646e296ad9e559edf75e7f222b0a825273d54\",\"venue\":\"AAMAS 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A Rusu\"},{\"authorId\":null,\"name\":\"Georg Ostrovski\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The arcade learning environment : An evaluation platform for general agents Neuroevolution for reinforcement learning using evolution strategies . In Evolutionary Computation , 2003 . CEC \\u2019 03\",\"url\":\"\",\"venue\":\"Deep learning for video game playing . IEEE Transactions on Games\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christian Igel. Neuroevolution for reinforcement learning us Computation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"2003\",\"url\":\"\",\"venue\":\"CEC\\u201903. The 2003 Congress on, volume 4, pages 2588\\u20132595. IEEE,\",\"year\":2003},{\"arxivId\":\"1712.06560\",\"authors\":[{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"8309711\",\"name\":\"V. Madhavan\"},{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2064020586d5832b55f80a7dffea1fd90a5d94dd\",\"title\":\"Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\",\"url\":\"https://www.semanticscholar.org/paper/2064020586d5832b55f80a7dffea1fd90a5d94dd\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1410.7326\",\"authors\":[{\"authorId\":\"1745664\",\"name\":\"S. Risi\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/TCIAIG.2015.2494596\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f7cfe0129d7b06ea0e5b15894ba3b8bb3f57bed\",\"title\":\"Neuroevolution in Games: State of the Art and Open Challenges\",\"url\":\"https://www.semanticscholar.org/paper/3f7cfe0129d7b06ea0e5b15894ba3b8bb3f57bed\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145528658\",\"name\":\"G. Kendall\"}],\"doi\":\"10.1109/TCIAIG.2015.2409514\",\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"9046f46d088eee7be4af8be5ffea602394a937c0\",\"title\":\"Editorial: IEEE Transactions on Computational Intelligence and AI in Games\",\"url\":\"https://www.semanticscholar.org/paper/9046f46d088eee7be4af8be5ffea602394a937c0\",\"venue\":\"IEEE Trans. Comput. Intell. AI Games\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Yao. Evolving artificial neural networks\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Proceedings of the IEEE\",\"url\":\"\",\"venue\":\"87(9):1423\\u20131447,\",\"year\":1999},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748824\",\"name\":\"C. Igel\"}],\"doi\":\"10.1109/CEC.2003.1299414\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"04377cba52732168d3e55e229bae2ee1ad4d43a2\",\"title\":\"Neuroevolution for reinforcement learning using evolution strategies\",\"url\":\"https://www.semanticscholar.org/paper/04377cba52732168d3e55e229bae2ee1ad4d43a2\",\"venue\":\"The 2003 Congress on Evolutionary Computation, 2003. CEC '03.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1686193\",\"name\":\"Georgios N. Yannakakis\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1007/978-3-319-63519-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c134e874b0c59792979af74ceefe2e23921a504e\",\"title\":\"Artificial Intelligence and Games\",\"url\":\"https://www.semanticscholar.org/paper/c134e874b0c59792979af74ceefe2e23921a504e\",\"venue\":\"Springer International Publishing\",\"year\":2018}],\"title\":\"Playing Atari with Six Neurons (Extended Abstract)\",\"topics\":[{\"topic\":\"Atari\",\"topicId\":\"20108\",\"url\":\"https://www.semanticscholar.org/topic/20108\"}],\"url\":\"https://www.semanticscholar.org/paper/030b276c519351e72abfced55a166f7ca451b696\",\"venue\":\"IJCAI\",\"year\":2020}\n"