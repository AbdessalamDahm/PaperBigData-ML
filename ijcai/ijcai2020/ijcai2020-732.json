"{\"abstract\":\"Deep reinforcement learning (DRL) methods traditionally struggle with tasks where environment rewards are sparse or delayed, which entails that exploration remains one of the key challenges of DRL. Instead of solely relying on extrinsic rewards, many state-of-the-art methods use intrinsic curiosity as exploration signal. While they hold promise of better local exploration, discovering global exploration strategies is beyond the reach of current methods. We propose a novel end-to-end intrinsic reward formulation that introduces high-level exploration in reinforcement learning. Our curiosity signal is driven by a fast reward that deals with local exploration and a slow reward that incentivizes long-time horizon exploration strategies. We formulate curiosity as the error in an agent\\u2019s ability to reconstruct the observations given their contexts. Experimental results show that this high-level exploration enables our agents to outperform prior work in several Atari games.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"32823060\",\"name\":\"Nicolas Bougie\",\"url\":\"https://www.semanticscholar.org/author/32823060\"},{\"authorId\":\"143758327\",\"name\":\"R. Ichise\",\"url\":\"https://www.semanticscholar.org/author/143758327\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":220485143,\"doi\":\"10.24963/ijcai.2020/733\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":false,\"paperId\":\"33752826b7d31f798bb6fd149c8eb705fa8b5b7c\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"317cd4522b1f4a6f889743578143bb8823623f8b\",\"title\":\"VIME: Variational Information Maximizing Exploration\",\"url\":\"https://www.semanticscholar.org/paper/317cd4522b1f4a6f889743578143bb8823623f8b\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1810.12894\",\"authors\":[{\"authorId\":\"3080409\",\"name\":\"Yuri Burda\"},{\"authorId\":\"144632352\",\"name\":\"H. Edwards\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4cb3fd057949624aa4f0bbe7a6dcc8777ff04758\",\"title\":\"Exploration by Random Network Distillation\",\"url\":\"https://www.semanticscholar.org/paper/4cb3fd057949624aa4f0bbe7a6dcc8777ff04758\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1810.02274\",\"authors\":[{\"authorId\":\"2417003\",\"name\":\"Nikolay Savinov\"},{\"authorId\":\"150918315\",\"name\":\"Anton Raichuk\"},{\"authorId\":\"151188653\",\"name\":\"Raphael Marinier\"},{\"authorId\":\"40412424\",\"name\":\"D. Vincent\"},{\"authorId\":\"1742208\",\"name\":\"M. Pollefeys\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1802148\",\"name\":\"S. Gelly\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71\",\"title\":\"Episodic Curiosity through Reachability\",\"url\":\"https://www.semanticscholar.org/paper/fdfeeb14bbde2ab31b18e56b92d362dcd1b14f71\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":\"1606.01868\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"143810408\",\"name\":\"D. Saxton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e90fd78e8a3b98af3954aae5209703aa966603e\",\"title\":\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1705.05363\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"33932184\",\"name\":\"Pulkit Agrawal\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPRW.2017.70\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"225ab689f41cef1dc18237ef5dab059a49950abf\",\"title\":\"Curiosity-Driven Exploration by Self-Supervised Prediction\",\"url\":\"https://www.semanticscholar.org/paper/225ab689f41cef1dc18237ef5dab059a49950abf\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017}],\"title\":\"Towards High-Level Intrinsic Exploration in Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/33752826b7d31f798bb6fd149c8eb705fa8b5b7c\",\"venue\":\"IJCAI\",\"year\":2020}\n"