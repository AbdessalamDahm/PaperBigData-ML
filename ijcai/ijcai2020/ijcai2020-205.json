"{\"abstract\":\"Evolution Strategies (ES) are a class of blackbox optimization algorithms and have been widely applied to solve problems, e.g., in reinforcement learning (RL), where the true gradient is unavailable. ES estimate the gradient of an objective function with respect to the parameters by randomly sampling search directions and evaluating parameter perturbations in these directions. However, the gradient estimator of ES tends to have a high variance for high-dimensional optimization, thus requiring a large number of samples and making ES inefficient. In this paper, we propose a new ES algorithm SGES, which utilizes historical estimated gradients to construct a low-dimensional subspace for sampling search directions, and adjusts the importance of this subspace adaptively. We prove that the variance of the gradient estimator of SGES can be much smaller than that of Vanilla ES; meanwhile, its bias can be well bounded. Empirical results on benchmark black-box functions and a set of popular RL tasks exhibit the superior performance of SGES over state-of-the-art ES algorithms.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"108324392\",\"name\":\"Fei-Yu Liu\",\"url\":\"https://www.semanticscholar.org/author/108324392\"},{\"authorId\":\"25841722\",\"name\":\"Ziniu Li\",\"url\":\"https://www.semanticscholar.org/author/25841722\"},{\"authorId\":\"50726415\",\"name\":\"C. Qian\",\"url\":\"https://www.semanticscholar.org/author/50726415\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"25841722\",\"name\":\"Ziniu Li\"},{\"authorId\":\"5819016\",\"name\":\"Xiong-Hui Chen\"}],\"doi\":\"10.1007/978-3-030-64096-5_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"10a700d749b11018e97de03ce9c11da6313a930c\",\"title\":\"Efficient Exploration by Novelty-Pursuit\",\"url\":\"https://www.semanticscholar.org/paper/10a700d749b11018e97de03ce9c11da6313a930c\",\"venue\":\"DAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"1729566\",\"name\":\"E. Elkind\"},{\"authorId\":\"145644809\",\"name\":\"Yang Gao\"}],\"doi\":\"10.1007/978-3-030-64096-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a469b594a1525a2c7d260570eae46d78690b171\",\"title\":\"Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24\\u201327, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/4a469b594a1525a2c7d260570eae46d78690b171\",\"venue\":\"DAI\",\"year\":2020}],\"corpusId\":220480934,\"doi\":\"10.24963/ijcai.2020/205\",\"fieldsOfStudy\":[\"Psychology\",\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"ad154aa2b6442d724d1093752143392334548e34\",\"references\":[{\"arxivId\":\"1805.07917\",\"authors\":[{\"authorId\":\"3440874\",\"name\":\"S. Khadka\"},{\"authorId\":\"1711099\",\"name\":\"Kagan Tumer\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5805a80b63ed0a605e5469e321a7e3c42eaf324\",\"title\":\"Evolution-Guided Policy Gradient in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d5805a80b63ed0a605e5469e321a7e3c42eaf324\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47338828\",\"name\":\"F. Friedrichs\"},{\"authorId\":\"1748824\",\"name\":\"C. Igel\"}],\"doi\":\"10.1016/j.neucom.2004.11.022\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4eb71cce6b2bf1989a79eff679f413456a16157\",\"title\":\"Evolutionary tuning of multiple SVM parameters\",\"url\":\"https://www.semanticscholar.org/paper/b4eb71cce6b2bf1989a79eff679f413456a16157\",\"venue\":\"ESANN\",\"year\":2004},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1706.01905\",\"authors\":[{\"authorId\":\"3407285\",\"name\":\"Matthias Plappert\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"2700360\",\"name\":\"S. Sidor\"},{\"authorId\":\"2896187\",\"name\":\"Richard Y. Chen\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1722677\",\"name\":\"T. Asfour\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"142497432fe179ddb6ffe600c64a837ec6179550\",\"title\":\"Parameter Space Noise for Exploration\",\"url\":\"https://www.semanticscholar.org/paper/142497432fe179ddb6ffe600c64a837ec6179550\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1711.00937\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f466157848d1a7772fb6d02cdac9a7a5e7ef982e\",\"title\":\"Neural Discrete Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3108029\",\"name\":\"P. Vidnerov\\u00e1\"},{\"authorId\":\"1769759\",\"name\":\"R. Neruda\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de2b4bed257ff949a6ba5d40843b986598b0a7b7\",\"title\":\"Evolution Strategies for Deep Neural Network Models Design\",\"url\":\"https://www.semanticscholar.org/paper/de2b4bed257ff949a6ba5d40843b986598b0a7b7\",\"venue\":\"ITAT\",\"year\":2017},{\"arxivId\":\"1712.06568\",\"authors\":[{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"144910178\",\"name\":\"J. Chen\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"}],\"doi\":\"10.1145/3205455.3205474\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"81a28e2f46115648beb4a2700b2b9a9fca476fbc\",\"title\":\"ES is more than just a traditional finite-difference approximator\",\"url\":\"https://www.semanticscholar.org/paper/81a28e2f46115648beb4a2700b2b9a9fca476fbc\",\"venue\":\"GECCO\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145704247\",\"name\":\"J. Martens\"},{\"authorId\":\"35188630\",\"name\":\"G. Dahl\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa7bfd2304201afbb19971ebde87b17e40242e91\",\"title\":\"On the importance of initialization and momentum in deep learning\",\"url\":\"https://www.semanticscholar.org/paper/aa7bfd2304201afbb19971ebde87b17e40242e91\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2595993\",\"name\":\"Horia Mania\"},{\"authorId\":\"40895205\",\"name\":\"Aurelia Guy\"},{\"authorId\":\"9229182\",\"name\":\"B. Recht\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"758ba118d7f161fe019d81c4f64a9069c342aa74\",\"title\":\"Simple random search of static linear policies is competitive for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/758ba118d7f161fe019d81c4f64a9069c342aa74\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1106.4487\",\"authors\":[{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"},{\"authorId\":\"48974230\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/CEC.2008.4631255\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dd5cf95a7af93d2733120d177c593989b19b98fe\",\"title\":\"Natural Evolution Strategies\",\"url\":\"https://www.semanticscholar.org/paper/dd5cf95a7af93d2733120d177c593989b19b98fe\",\"venue\":\"2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)\",\"year\":2008},{\"arxivId\":\"1703.03864\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4ee802a58d32aa049d549d06be440ac947b53987\",\"title\":\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143676697\",\"name\":\"Y. Nesterov\"},{\"authorId\":\"39226769\",\"name\":\"V. Spokoiny\"}],\"doi\":\"10.1007/s10208-015-9296-2\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"21a0b0fbdde1aee56fe10e69e897decaf21f43a6\",\"title\":\"Random Gradient-Free Minimization of Convex Functions\",\"url\":\"https://www.semanticscholar.org/paper/21a0b0fbdde1aee56fe10e69e897decaf21f43a6\",\"venue\":\"Found. Comput. Math.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2333223\",\"name\":\"Niru Maheswaranathan\"},{\"authorId\":\"2096458\",\"name\":\"Luke Metz\"},{\"authorId\":\"152763475\",\"name\":\"George Tucker\"},{\"authorId\":\"30097914\",\"name\":\"Dami Choi\"},{\"authorId\":\"1407546430\",\"name\":\"Jascha Sohl-Dickstein\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"72cd0216fb7e9889b61e58776ae1a242f98ab067\",\"title\":\"Guided evolutionary strategies: augmenting random search with surrogate gradients\",\"url\":\"https://www.semanticscholar.org/paper/72cd0216fb7e9889b61e58776ae1a242f98ab067\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1743344\",\"name\":\"H. Schwefel\"}],\"doi\":\"10.1007/BF01876146\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03c01ddfff6ca3a7e9e22dc313140d7658c688f1\",\"title\":\"Evolution strategies: A family of non-linear optimization techniques based on imitating some principles of organic evolution\",\"url\":\"https://www.semanticscholar.org/paper/03c01ddfff6ca3a7e9e22dc313140d7658c688f1\",\"venue\":\"Ann. Oper. Res.\",\"year\":1984},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1910.01215\",\"authors\":[{\"authorId\":\"32725720\",\"name\":\"Xingyou Song\"},{\"authorId\":\"35893472\",\"name\":\"W. Gao\"},{\"authorId\":\"51285938\",\"name\":\"Yuxiang Yang\"},{\"authorId\":\"1805203\",\"name\":\"Krzysztof Choromanski\"},{\"authorId\":\"3124110\",\"name\":\"Aldo Pacchiano\"},{\"authorId\":\"11501567\",\"name\":\"Yunhao Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bb00e80623d78433424b3cdd2a01f79b171b45f5\",\"title\":\"ES-MAML: Simple Hessian-Free Meta Learning\",\"url\":\"https://www.semanticscholar.org/paper/bb00e80623d78433424b3cdd2a01f79b171b45f5\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1604.00772\",\"authors\":[{\"authorId\":\"144539890\",\"name\":\"N. Hansen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7c6409ec154ba64f5eb63d8c6e9f419ce1472289\",\"title\":\"The CMA Evolution Strategy: A Tutorial\",\"url\":\"https://www.semanticscholar.org/paper/7c6409ec154ba64f5eb63d8c6e9f419ce1472289\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. Colas\"},{\"authorId\":null,\"name\":\"O. Sigaud\"},{\"authorId\":null,\"name\":\"P. Y. Oudeyer\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"GEPPG: Decoupling exploration and exploitation in deep reinforcement learning algorithms\",\"url\":\"\",\"venue\":\"Proceedings of the 35th International Conference on Machine Learning (ICML), pages 1038\\u2013 1047, Stockholm, Sweden\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1805203\",\"name\":\"Krzysztof Choromanski\"},{\"authorId\":\"3124110\",\"name\":\"Aldo Pacchiano\"},{\"authorId\":\"1410302742\",\"name\":\"Jack Parker-Holder\"},{\"authorId\":\"11501567\",\"name\":\"Yunhao Tang\"},{\"authorId\":\"72156637\",\"name\":\"Vikas Sindhwani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c2f28b786611b67f19dfb91660ce980157e5b3cc\",\"title\":\"From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization\",\"url\":\"https://www.semanticscholar.org/paper/c2f28b786611b67f19dfb91660ce980157e5b3cc\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1712.06560\",\"authors\":[{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"8309711\",\"name\":\"V. Madhavan\"},{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2064020586d5832b55f80a7dffea1fd90a5d94dd\",\"title\":\"Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\",\"url\":\"https://www.semanticscholar.org/paper/2064020586d5832b55f80a7dffea1fd90a5d94dd\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1810.01222\",\"authors\":[{\"authorId\":\"51227642\",\"name\":\"Alo\\u00efs Pourchot\"},{\"authorId\":\"3211142\",\"name\":\"Olivier Sigaud\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d9e08e872c69e5acfed2a93ec6f6d623cfd0c680\",\"title\":\"CEM-RL: Combining evolutionary and gradient-based methods for policy search\",\"url\":\"https://www.semanticscholar.org/paper/d9e08e872c69e5acfed2a93ec6f6d623cfd0c680\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Rapin\"},{\"authorId\":null,\"name\":\"O. Teytaud\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Nevergrad - A gradient-free optimization platform\",\"url\":\"\",\"venue\":\"http://GitHub.com/ FacebookResearch/Nevergrad\",\"year\":2018},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1712.06563\",\"authors\":[{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"144910178\",\"name\":\"J. Chen\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"}],\"doi\":\"10.1145/3205455.3205473\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a830e847f46425f21e34239ba75f026280c0706\",\"title\":\"Safe mutations for deep and recurrent neural networks through output gradients\",\"url\":\"https://www.semanticscholar.org/paper/0a830e847f46425f21e34239ba75f026280c0706\",\"venue\":\"GECCO\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Madhavan\"},{\"authorId\":null,\"name\":\"J. Lehman\"},{\"authorId\":null,\"name\":\"K. O. Stanley\"},{\"authorId\":null,\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Decoupling exploration and exploitation in deep reinforcement learning algorithms\",\"url\":\"\",\"venue\":\"Proceedings of the 35 th International Conference on Machine Learning ( ICML )\",\"year\":null},{\"arxivId\":\"1804.02395\",\"authors\":[{\"authorId\":\"1805203\",\"name\":\"Krzysztof Choromanski\"},{\"authorId\":\"144845456\",\"name\":\"M. Rowland\"},{\"authorId\":\"1808676\",\"name\":\"V. Sindhwani\"},{\"authorId\":\"145369890\",\"name\":\"R. Turner\"},{\"authorId\":\"145689461\",\"name\":\"Adrian Weller\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6200c3ef5af9b343456e8ff933e48816fa05b788\",\"title\":\"Structured Evolution with Compact Architectures for Scalable Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/6200c3ef5af9b343456e8ff933e48816fa05b788\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13844906\",\"name\":\"A. Auger\"},{\"authorId\":\"144539890\",\"name\":\"N. Hansen\"}],\"doi\":\"10.1145/2330784.2330919\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b40255df3f0876133d7787ba5c14615881911a72\",\"title\":\"Tutorial CMA-ES: evolution strategies and covariance matrix adaptation\",\"url\":\"https://www.semanticscholar.org/paper/b40255df3f0876133d7787ba5c14615881911a72\",\"venue\":\"GECCO '12\",\"year\":2012}],\"title\":\"Self-Guided Evolution Strategies with Historical Estimated Gradients\",\"topics\":[{\"topic\":\"Evolution strategy\",\"topicId\":\"64343\",\"url\":\"https://www.semanticscholar.org/topic/64343\"},{\"topic\":\"Color gradient\",\"topicId\":\"307413\",\"url\":\"https://www.semanticscholar.org/topic/307413\"}],\"url\":\"https://www.semanticscholar.org/paper/ad154aa2b6442d724d1093752143392334548e34\",\"venue\":\"IJCAI\",\"year\":2020}\n"