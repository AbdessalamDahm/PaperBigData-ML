"{\"abstract\":\"Policy distillation, which transfers a teacher policy to a student policy has achieved great success in challenging tasks of deep reinforcement learning. This teacher-student framework requires a well-trained teacher model which is computationally expensive. Moreover, the performance of the student model could be limited by the teacher model if the teacher model is not optimal. In the light of collaborative learning, we study the feasibility of involving joint intellectual efforts from diverse perspectives of student models. In this work, we introduce dual policy distillation(DPD), a student-student framework in which two learners operate on the same environment to explore different perspectives of the environment and extract knowledge from each other to enhance their learning. The key challenge in developing this dual learning framework is to identify the beneficial knowledge from the peer learner for contemporary learning-based reinforcement learning algorithms, since it is unclear whether the knowledge distilled from an imperfect and noisy peer learner would be helpful. To address the challenge, we theoretically justify that distilling knowledge from a peer learner will lead to policy improvement and propose a disadvantageous distillation strategy based on the theoretical results. The conducted experiments on several continuous control tasks show that the proposed framework achieves superior performance with a learning-based agent and function approximation without the use of expensive teacher models.\",\"arxivId\":\"2006.04061\",\"authors\":[{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\",\"url\":\"https://www.semanticscholar.org/author/51238382\"},{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\",\"url\":\"https://www.semanticscholar.org/author/1759658\"},{\"authorId\":\"48513905\",\"name\":\"Yuening Li\",\"url\":\"https://www.semanticscholar.org/author/48513905\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\",\"url\":\"https://www.semanticscholar.org/author/48539382\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2009.07415\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"101486331\",\"name\":\"Mingyang Wan\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"258d7bd6a30dc8ce187fc9486ae7d858c0149d19\",\"title\":\"Meta-AAD: Active Anomaly Detection with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/258d7bd6a30dc8ce187fc9486ae7d858c0149d19\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.15097\",\"authors\":[{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"3364022\",\"name\":\"K. Zhou\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\"}],\"doi\":\"10.1145/3394486.3403088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"058fa6b41bab3dccfc6ba07a83a99a65fc41d1bc\",\"title\":\"Policy-GNN: Aggregation Optimization for Graph Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/058fa6b41bab3dccfc6ba07a83a99a65fc41d1bc\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":\"2006.05525\",\"authors\":[{\"authorId\":\"37233332\",\"name\":\"J. Gou\"},{\"authorId\":\"2425630\",\"name\":\"B. Yu\"},{\"authorId\":\"144555237\",\"name\":\"S. Maybank\"},{\"authorId\":\"145047838\",\"name\":\"Dacheng Tao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1728cb805a9573b59330890ba9723e73d6c3c974\",\"title\":\"Knowledge Distillation: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/1728cb805a9573b59330890ba9723e73d6c3c974\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":219531752,\"doi\":\"10.24963/ijcai.2020/431\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"f87cd3cb63ee30a9598af86349389708e859589e\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Oh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a critical survey. Web manuscript\",\"url\":\"\",\"venue\":\"Proximal policy optimization algorithms\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145050960\",\"name\":\"F. Silva\"},{\"authorId\":\"8848899\",\"name\":\"R. Glatt\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d9db652bbf8d11d688657cfa7d1cccb4abe17b35\",\"title\":\"Simultaneously Learning and Advising in Multiagent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d9db652bbf8d11d688657cfa7d1cccb4abe17b35\",\"venue\":\"AAMAS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1799133\",\"name\":\"P. Dillenbourg\"},{\"authorId\":\"1799133\",\"name\":\"P. Dillenbourg\"},{\"authorId\":\"1605198613\",\"name\":\"Dillenbourg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"177944150565195ee9c3e28dc6b032200cfda059\",\"title\":\"Collaborative Learning: Cognitive and Computational Approaches\",\"url\":\"https://www.semanticscholar.org/paper/177944150565195ee9c3e28dc6b032200cfda059\",\"venue\":\"\",\"year\":1999},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Tan\"},{\"authorId\":null,\"name\":\"Teh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Multi-agent reinforcement learning: Independent vs. cooperative agents\",\"url\":\"\",\"venue\":\"Raia Hadsell, Nicolas Heess, and Razvan Pascanu\",\"year\":1993},{\"arxivId\":\"1906.08387\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"3364022\",\"name\":\"K. Zhou\"},{\"authorId\":\"121774683\",\"name\":\"X. Hu\"}],\"doi\":\"10.24963/ijcai.2019/589\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"title\":\"Experience Replay Optimization\",\"url\":\"https://www.semanticscholar.org/paper/2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152300037\",\"name\":\"J. Young\"}],\"doi\":\"10.1038/230260b0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b6cea23b4a07b66e3122252f47f9cc715b469cf9\",\"title\":\"Machine Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b6cea23b4a07b66e3122252f47f9cc715b469cf9\",\"venue\":\"Nature\",\"year\":1971},{\"arxivId\":\"1603.00448\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"04162cb8cfaa0f7e37586823ff4ad0bff09ed21d\",\"title\":\"Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/04162cb8cfaa0f7e37586823ff4ad0bff09ed21d\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50086584\",\"name\":\"Haiyan Yin\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"title\":\"Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1702.05796\",\"authors\":[{\"authorId\":\"3002019\",\"name\":\"Kaixiang Lin\"},{\"authorId\":\"49183901\",\"name\":\"S. Wang\"},{\"authorId\":\"145487992\",\"name\":\"Jiayu Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"46e46dffe4f8b724ec51179b3be1ae321fdb2d39\",\"title\":\"Collaborative Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/46e46dffe4f8b724ec51179b3be1ae321fdb2d39\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35533477\",\"name\":\"T. Xu\"},{\"authorId\":\"47362455\",\"name\":\"Q. Liu\"},{\"authorId\":\"145927744\",\"name\":\"Liang Zhao\"},{\"authorId\":\"144439558\",\"name\":\"J. Peng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"40df15ce1de6eb0366179f4726aa55a3e50141fc\",\"title\":\"Learning to Explore via Meta-Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/40df15ce1de6eb0366179f4726aa55a3e50141fc\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1910.04376\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"153842970\",\"name\":\"Y. Cao\"},{\"authorId\":\"50822341\",\"name\":\"Song-Yi Huang\"},{\"authorId\":\"1381629357\",\"name\":\"Ruzhe Wei\"},{\"authorId\":null,\"name\":\"Junyu Guo\"},{\"authorId\":\"48539647\",\"name\":\"Xia Hu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"891a964ba4c5b36d87066a2c38b841bd6cd18978\",\"title\":\"RLCard: A Toolkit for Reinforcement Learning in Card Games\",\"url\":\"https://www.semanticscholar.org/paper/891a964ba4c5b36d87066a2c38b841bd6cd18978\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144041292\",\"name\":\"M. Bain\"},{\"authorId\":\"1804253\",\"name\":\"C. Sammut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f4731d5133cb96ab30e08bf39dffa874aebf487\",\"title\":\"A Framework for Behavioural Cloning\",\"url\":\"https://www.semanticscholar.org/paper/1f4731d5133cb96ab30e08bf39dffa874aebf487\",\"venue\":\"Machine Intelligence 15\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael Bain\"},{\"authorId\":null,\"name\":\"Claude Sommut\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A framework for behavioural cloning. Machine Intelligence\",\"url\":\"\",\"venue\":\"and Sommut\",\"year\":1999},{\"arxivId\":\"1606.03137\",\"authors\":[{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777\",\"title\":\"Cooperative Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Daochen Zha\"},{\"authorId\":null,\"name\":\"Kwei-Herng Lai\"},{\"authorId\":null,\"name\":\"Kaixiong Zhou\"},{\"authorId\":null,\"name\":\"Xia Hu. Experience replay optimization\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 4243\\u20134249,\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Chelsea Finn\"},{\"authorId\":null,\"name\":\"Trevor Darrell\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. End-to-end training of deep visuomotor policies\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):1334\\u20131373,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50027-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"title\":\"Markov Games as a Framework for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Hado P van Hasselt\"},{\"authorId\":null,\"name\":\"David Silver. Meta-gradient reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 2396\\u20132407,\",\"year\":2018},{\"arxivId\":\"1707.04175\",\"authors\":[{\"authorId\":\"1725303\",\"name\":\"Y. Teh\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"title\":\"Distral: Robust multitask reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":null,\"name\":\"Dong-Ki Kim\"},{\"authorId\":null,\"name\":\"Miao Liu\"},{\"authorId\":null,\"name\":\"Gerald Tesauro\"},{\"authorId\":null,\"name\":\"Matthew Riemer\"},{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"Murray Campbell\"},{\"authorId\":null,\"name\":\"Jonathan P How. Learning to teach in cooperative multiagent learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 6128\\u20136136,\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2449382\",\"name\":\"T. Hubert\"},{\"authorId\":\"144522726\",\"name\":\"L. Baker\"},{\"authorId\":\"40227832\",\"name\":\"Matthew Lai\"},{\"authorId\":\"34848283\",\"name\":\"A. Bolton\"},{\"authorId\":\"1519062204\",\"name\":\"Yutian Chen\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"88791868\",\"name\":\"F. Hui\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature24270\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c27db32efa8137cbf654902f8f728f338e55cd1c\",\"title\":\"Mastering the game of Go without human knowledge\",\"url\":\"https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c\",\"venue\":\"Nature\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701353\",\"name\":\"Y. Shoham\"},{\"authorId\":\"38076874\",\"name\":\"R. Powers\"},{\"authorId\":\"3050250\",\"name\":\"Trond Grenager\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a69e33ce29c451655d917bf9387ed57e115fcfc7\",\"title\":\"Multi-Agent Reinforcement Learning:a critical survey\",\"url\":\"https://www.semanticscholar.org/paper/a69e33ce29c451655d917bf9387ed57e115fcfc7\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tianbing Xu\"},{\"authorId\":null,\"name\":\"Qiang Liu\"},{\"authorId\":null,\"name\":\"Liang Zhao\"},{\"authorId\":null,\"name\":\"Jian Peng. Learning to explore via meta-policy gradient\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 5463\\u20135472,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jonathan Ho\"},{\"authorId\":null,\"name\":\"Stefano Ermon. Generative adversarial imitation learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 4565\\u20134573,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Junhyuk Oh\"},{\"authorId\":null,\"name\":\"Yijie Guo\"},{\"authorId\":null,\"name\":\"Satinder Singh\"},{\"authorId\":null,\"name\":\"Honglak Lee. Self-imitation learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 3878\\u20133887,\",\"year\":2018},{\"arxivId\":\"1805.07830\",\"authors\":[{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"7184125\",\"name\":\"D. Kim\"},{\"authorId\":\"46331912\",\"name\":\"M. Liu\"},{\"authorId\":\"1699108\",\"name\":\"G. Tesauro\"},{\"authorId\":\"40497459\",\"name\":\"M. Riemer\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"143903370\",\"name\":\"Murray Campbell\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"}],\"doi\":\"10.1609/aaai.v33i01.33016128\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"beddac10ad8dcda206088421ffeb167f491c4ae8\",\"title\":\"Learning to Teach in Cooperative Multiagent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/beddac10ad8dcda206088421ffeb167f491c4ae8\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1511.06295\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"143959037\",\"name\":\"J. Kirkpatrick\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"title\":\"Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L Littman. Markov games as a framework for multi-a learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Machine learning proceedings 1994\",\"url\":\"\",\"venue\":\"pages 157\\u2013163. Elsevier,\",\"year\":1994},{\"arxivId\":\"1606.03476\",\"authors\":[{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"title\":\"Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":null,\"name\":\"Stuart J Russell\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Anca Dragan. Cooperative inverse reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 3909\\u2013 3917,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1799133\",\"name\":\"P. Dillenbourg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bfd5edc4f63530cb0d4eae03bb660c945a4f8e3\",\"title\":\"Collaborative Learning: Cognitive and Computational Approaches. Advances in Learning and Instruction Series.\",\"url\":\"https://www.semanticscholar.org/paper/7bfd5edc4f63530cb0d4eae03bb660c945a4f8e3\",\"venue\":\"\",\"year\":1999},{\"arxivId\":\"1803.05044\",\"authors\":[{\"authorId\":\"35533477\",\"name\":\"T. Xu\"},{\"authorId\":\"47362455\",\"name\":\"Q. Liu\"},{\"authorId\":\"145927744\",\"name\":\"Liang Zhao\"},{\"authorId\":\"144439558\",\"name\":\"J. Peng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f567f1e8972ff31a7ced59c329e7d75da645baf\",\"title\":\"Learning to Explore with Meta-Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/7f567f1e8972ff31a7ced59c329e7d75da645baf\",\"venue\":\"ICML 2018\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"7515048\",\"name\":\"Matej Vecer\\u00edk\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2533110\",\"name\":\"A. Sendonaris\"},{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1387885286\",\"name\":\"Gabriel Dulac-Arnold\"},{\"authorId\":\"70495322\",\"name\":\"J. Agapiou\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3b0ea7209731c47b582215c6c67f9c691ad9863\",\"title\":\"Deep Q-learning From Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/e3b0ea7209731c47b582215c6c67f9c691ad9863\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1504.00702\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6b8a1b80891c96c28cc6340267b58186157e536\",\"title\":\"End-to-End Training of Deep Visuomotor Policies\",\"url\":\"https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":\"1902.02186\",\"authors\":[{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"35880964\",\"name\":\"Siddhant M. Jayakumar\"},{\"authorId\":\"1782475\",\"name\":\"G. Swirszcz\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff84c46d4653782218549bd99631130df2d2859e\",\"title\":\"Distilling Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/ff84c46d4653782218549bd99631130df2d2859e\",\"venue\":\"AISTATS\",\"year\":2019},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1703.03864\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ee802a58d32aa049d549d06be440ac947b53987\",\"title\":\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Da Silva\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yoav Shoham, Rob Powers, and Trond Grenager. Multi-agent reinforcement learning: a critical survey. Web manuscript\",\"url\":\"\",\"venue\":\"Pierre Dillenbourg. Collaborative learning: Cognitive and computational approaches. advances in learning and instruction series. ERIC\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Felipe Leno Da Silva\"},{\"authorId\":null,\"name\":\"Ruben Glatt\"},{\"authorId\":null,\"name\":\"Anna Helena Reali Costa. Simultaneously learning\"},{\"authorId\":null,\"name\":\"advising in multiagent reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 1100\\u20131108,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Tan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Multi-agent reinforcement learning: Independent vs\",\"url\":\"\",\"venue\":\"cooperative agents. In ICML,\",\"year\":1993},{\"arxivId\":\"1805.09801\",\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2a49a71c9d40051a03c4445fe49025bc75d9eeb6\",\"title\":\"Meta-Gradient Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2a49a71c9d40051a03c4445fe49025bc75d9eeb6\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1015330.1015430\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"title\":\"Apprenticeship learning via inverse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":\"1806.05635\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1857914\",\"name\":\"Yijie Guo\"},{\"authorId\":\"145537841\",\"name\":\"S. Singh\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d397f4cf400f6ffcb1b8e3db27bb75966a0513cf\",\"title\":\"Self-Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/d397f4cf400f6ffcb1b8e3db27bb75966a0513cf\",\"venue\":\"ICML\",\"year\":2018}],\"title\":\"Dual Policy Distillation\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Requirement prioritization\",\"topicId\":\"1329589\",\"url\":\"https://www.semanticscholar.org/topic/1329589\"},{\"topic\":\"RL (complexity)\",\"topicId\":\"3597734\",\"url\":\"https://www.semanticscholar.org/topic/3597734\"},{\"topic\":\"Analysis of algorithms\",\"topicId\":\"13372\",\"url\":\"https://www.semanticscholar.org/topic/13372\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Approximation\",\"topicId\":\"3247\",\"url\":\"https://www.semanticscholar.org/topic/3247\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"}],\"url\":\"https://www.semanticscholar.org/paper/f87cd3cb63ee30a9598af86349389708e859589e\",\"venue\":\"ArXiv\",\"year\":2020}\n"