"{\"abstract\":\"Deep reinforcement learning (DRL) increases the successful applications of reinforcement learning (RL) techniques but also brings challenges such as low sample efficiency. In this work, I propose generalized representation learning methods to obtain compact state space suitable for RL from a raw observation state. I expect my new methods will increase sample efficiency of RL by understandable representations of state and therefore improve the performance of RL.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"1485919236\",\"name\":\"Hanhua Zhu\",\"url\":\"https://www.semanticscholar.org/author/1485919236\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":220483683,\"doi\":\"10.24963/ijcai.2020/748\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":false,\"is_publisher_licensed\":false,\"paperId\":\"c7f4a6630d8a17402803df9b8ffd966d6881bb2e\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2634261\",\"name\":\"Marvin Zhang\"},{\"authorId\":\"2425230\",\"name\":\"S. Vikram\"},{\"authorId\":\"145040987\",\"name\":\"Laura Smith\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"143945326\",\"name\":\"M. Johnson\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee9893ff2aa325ff3c9920f247436c514fd8b512\",\"title\":\"SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ee9893ff2aa325ff3c9920f247436c514fd8b512\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1802.01561\",\"authors\":[{\"authorId\":\"2311318\",\"name\":\"Lasse Espeholt\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"48500289\",\"name\":\"Tom Ward\"},{\"authorId\":\"2895238\",\"name\":\"Yotam Doron\"},{\"authorId\":\"9559485\",\"name\":\"Vlad Firoiu\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"2768462\",\"name\":\"Iain Dunning\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"80196cdfcd0c6ce2953bf65a7f019971e2026386\",\"title\":\"IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures\",\"url\":\"https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1807.04742\",\"authors\":[{\"authorId\":\"3422774\",\"name\":\"Ashvin Nair\"},{\"authorId\":\"144401061\",\"name\":\"Vitchyr H. Pong\"},{\"authorId\":\"35904540\",\"name\":\"Murtaza Dalal\"},{\"authorId\":\"8527563\",\"name\":\"Shikhar Bahl\"},{\"authorId\":\"145466641\",\"name\":\"S. Lin\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3aadab924520c58be81781aafd51e6807e9c4576\",\"title\":\"Visual Reinforcement Learning with Imagined Goals\",\"url\":\"https://www.semanticscholar.org/paper/3aadab924520c58be81781aafd51e6807e9c4576\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1907.00953\",\"authors\":[{\"authorId\":\"49250083\",\"name\":\"Alex X. Lee\"},{\"authorId\":\"3195183\",\"name\":\"Anusha Nagabandi\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69d1ee8a99f55e9228f33fdb3a0339541ad1201c\",\"title\":\"Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model\",\"url\":\"https://www.semanticscholar.org/paper/69d1ee8a99f55e9228f33fdb3a0339541ad1201c\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1801.10395\",\"authors\":[{\"authorId\":\"35177479\",\"name\":\"A. Doerr\"},{\"authorId\":\"37755468\",\"name\":\"C. Daniel\"},{\"authorId\":\"1725192\",\"name\":\"Martin Schiegg\"},{\"authorId\":\"1403827878\",\"name\":\"D. Nguyen-Tuong\"},{\"authorId\":\"47738949\",\"name\":\"S. Schaal\"},{\"authorId\":\"120284556\",\"name\":\"Marie-Eve Toussaint\"},{\"authorId\":\"2715093\",\"name\":\"Sebastian Trimpe\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"408570c02ba213a856bc8186c62a4e5bf91a18de\",\"title\":\"Probabilistic Recurrent State-Space Models\",\"url\":\"https://www.semanticscholar.org/paper/408570c02ba213a856bc8186c62a4e5bf91a18de\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1485919236\",\"name\":\"Hanhua Zhu\"},{\"authorId\":\"48999027\",\"name\":\"Tomoyuki Kaneko\"}],\"doi\":\"10.1109/TAAI48200.2019.8959896\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"288c3cfed6168b2b3c100b16e09f8fac41d5120a\",\"title\":\"Deep Residual Attention Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/288c3cfed6168b2b3c100b16e09f8fac41d5120a\",\"venue\":\"2019 International Conference on Technologies and Applications of Arti\\ufb01cial Intelligence (TAAI)\",\"year\":2019},{\"arxivId\":\"1704.06904\",\"authors\":[{\"authorId\":\"1682816\",\"name\":\"Fei Wang\"},{\"authorId\":\"9563639\",\"name\":\"Mengqing Jiang\"},{\"authorId\":null,\"name\":\"Chen Qian\"},{\"authorId\":\"1692609\",\"name\":\"S. Yang\"},{\"authorId\":null,\"name\":\"Cheng Li\"},{\"authorId\":\"1720776\",\"name\":\"H. Zhang\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"},{\"authorId\":\"50295995\",\"name\":\"X. Tang\"}],\"doi\":\"10.1109/CVPR.2017.683\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"77d30cf9a34fb6b50979c6a68863099da9a060ad\",\"title\":\"Residual Attention Network for Image Classification\",\"url\":\"https://www.semanticscholar.org/paper/77d30cf9a34fb6b50979c6a68863099da9a060ad\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017}],\"title\":\"Generalized Representation Learning Methods for Deep Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"}],\"url\":\"https://www.semanticscholar.org/paper/c7f4a6630d8a17402803df9b8ffd966d6881bb2e\",\"venue\":\"IJCAI\",\"year\":2020}\n"