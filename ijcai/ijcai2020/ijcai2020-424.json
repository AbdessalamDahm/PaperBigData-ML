"{\"abstract\":\"Transfer Learning (TL) has shown great potential to accelerate Reinforcement Learning (RL) by leveraging prior knowledge from past learned policies of relevant tasks. Existing transfer approaches either explicitly computes the similarity between tasks or select appropriate source policies to provide guided explorations for the target task. However, how to directly optimize the target policy by alternatively utilizing knowledge from appropriate source policies without explicitly measuring the similarity is currently missing. In this paper, we propose a novel Policy Transfer Framework (PTF) to accelerate RL by taking advantage of this idea. Our framework learns when and which source policy is the best to reuse for the target policy and when to terminate it by modeling multi-policy transfer as the option learning problem. PTF can be easily combined with existing deep RL approaches. Experimental results show it significantly accelerates the learning process and surpasses state-of-the-art policy transfer methods in terms of learning efficiency and final performance in both discrete and continuous action spaces.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"3449314\",\"name\":\"Tianpei Yang\",\"url\":\"https://www.semanticscholar.org/author/3449314\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\",\"url\":\"https://www.semanticscholar.org/author/40513470\"},{\"authorId\":\"1889014\",\"name\":\"Zhaopeng Meng\",\"url\":\"https://www.semanticscholar.org/author/1889014\"},{\"authorId\":\"2079174\",\"name\":\"Zongzhang Zhang\",\"url\":\"https://www.semanticscholar.org/author/2079174\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\",\"url\":\"https://www.semanticscholar.org/author/1776850\"},{\"authorId\":\"73416400\",\"name\":\"Yingfeng Cheng\",\"url\":\"https://www.semanticscholar.org/author/73416400\"},{\"authorId\":\"153645633\",\"name\":\"Changjie Fan\",\"url\":\"https://www.semanticscholar.org/author/153645633\"},{\"authorId\":\"2209431\",\"name\":\"Weixun Wang\",\"url\":\"https://www.semanticscholar.org/author/2209431\"},{\"authorId\":\"2032869\",\"name\":\"W. Liu\",\"url\":\"https://www.semanticscholar.org/author/2032869\"},{\"authorId\":\"2870663\",\"name\":\"Zhaodong Wang\",\"url\":\"https://www.semanticscholar.org/author/2870663\"},{\"authorId\":\"3255510\",\"name\":\"Jiajie Peng\",\"url\":\"https://www.semanticscholar.org/author/3255510\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.14942\",\"authors\":[{\"authorId\":\"1701335\",\"name\":\"D. Graves\"},{\"authorId\":\"144576352\",\"name\":\"Jun Jin\"},{\"authorId\":null,\"name\":\"Jun Luo\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0173f2a1c07a65832d73404b7816434dfa515661\",\"title\":\"LISPR: An Options Framework for Policy Reuse with Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0173f2a1c07a65832d73404b7816434dfa515661\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.14289\",\"authors\":[{\"authorId\":\"1701335\",\"name\":\"D. Graves\"},{\"authorId\":\"21072518\",\"name\":\"Johannes C G\\u00fcnther\"},{\"authorId\":\"1491232654\",\"name\":\"Jun Luo\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"756fc6ceb0e103cdb0a48ec01942cbe1c4a2cd07\",\"title\":\"Affordance as general value function: A computational model\",\"url\":\"https://www.semanticscholar.org/paper/756fc6ceb0e103cdb0a48ec01942cbe1c4a2cd07\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":219676678,\"doi\":\"10.24963/ijcai.2020/424\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"b6456512bbb44079972cd7e3a5d7652c76fd26c9\",\"references\":[{\"arxivId\":\"1709.04571\",\"authors\":[{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"26389489\",\"name\":\"Martin Klissarov\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96e81cabed55630f2ad3e1346300bd7a7a17f060\",\"title\":\"When Waiting is not an Option : Learning Options with a Deliberation Cost\",\"url\":\"https://www.semanticscholar.org/paper/96e81cabed55630f2ad3e1346300bd7a7a17f060\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2554720\",\"name\":\"Timothy A. Mann\"},{\"authorId\":\"3187297\",\"name\":\"Daniel J. Mankowitz\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"943dcbcb2869a3892d319d1098c61e5b140df2ab\",\"title\":\"Time-Regularized Interrupting Options (TRIO)\",\"url\":\"https://www.semanticscholar.org/paper/943dcbcb2869a3892d319d1098c61e5b140df2ab\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1803.03835\",\"authors\":[{\"authorId\":\"145063335\",\"name\":\"Simon Schmitt\"},{\"authorId\":\"27866722\",\"name\":\"J. Hudson\"},{\"authorId\":\"40501144\",\"name\":\"A. Z\\u00eddek\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"2786693\",\"name\":\"C. Doersch\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"3373807\",\"name\":\"Heinrich K\\u00fcttler\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ebf3b0c284cb776d89951e4e67a59df6403fc9a6\",\"title\":\"Kickstarting Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ebf3b0c284cb776d89951e4e67a59df6403fc9a6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41016725\",\"name\":\"Thomas Kipf\"},{\"authorId\":\"47002813\",\"name\":\"Yujia Li\"},{\"authorId\":\"2791430\",\"name\":\"Hanjun Dai\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"1398105826\",\"name\":\"Alvaro Sanchez-Gonzalez\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"2019153\",\"name\":\"P. Battaglia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fbf03bf621ffee283911e765d525a75fc0d11bae\",\"title\":\"CompILE: Compositional Imitation Learning and Execution\",\"url\":\"https://www.semanticscholar.org/paper/fbf03bf621ffee283911e765d525a75fc0d11bae\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pierre-Luc Bacon\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"Doina Precup. The option-critic architecture\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 31st AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 1726\\u20131734,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":\"10.1109/IROS.2012.6386109\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"title\":\"MuJoCo: A physics engine for model-based control\",\"url\":\"https://www.semanticscholar.org/paper/b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"venue\":\"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2012},{\"arxivId\":\"1902.09996\",\"authors\":[{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"2311858\",\"name\":\"Diana Borsa\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"34108fe028c7bd0571160edbc105bf50874f23ea\",\"title\":\"The Termination Critic\",\"url\":\"https://www.semanticscholar.org/paper/34108fe028c7bd0571160edbc105bf50874f23ea\",\"venue\":\"AISTATS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bdc5a10aa5805808cfca58ac527ddc23e737bee8\",\"title\":\"Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining\",\"url\":\"https://www.semanticscholar.org/paper/bdc5a10aa5805808cfca58ac527ddc23e737bee8\",\"venue\":\"NIPS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"2394697\",\"name\":\"Y. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b3412ded0375f8fe7336e82dc534eed994cac088\",\"title\":\"Transfer Learning via Inter-Task Mappings for Temporal Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/b3412ded0375f8fe7336e82dc534eed994cac088\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2007},{\"arxivId\":\"1705.10479\",\"authors\":[{\"authorId\":\"1944801\",\"name\":\"Karol Hausman\"},{\"authorId\":\"2527420\",\"name\":\"Yevgen Chebotar\"},{\"authorId\":\"1745219\",\"name\":\"S. Schaal\"},{\"authorId\":\"1732493\",\"name\":\"G. Sukhatme\"},{\"authorId\":\"35198686\",\"name\":\"Joseph J. Lim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97b16661aada70a28d2a791cf597427e2aa0ad33\",\"title\":\"Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/97b16661aada70a28d2a791cf597427e2aa0ad33\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f\",\"title\":\"PAC-inspired Option Discovery in Lifelong Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1801.00690\",\"authors\":[{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"2895238\",\"name\":\"Yotam Doron\"},{\"authorId\":\"50654556\",\"name\":\"Alistair Muldal\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"3422141\",\"name\":\"Y. Li\"},{\"authorId\":\"40550616\",\"name\":\"D. Casas\"},{\"authorId\":\"2508525\",\"name\":\"D. Budden\"},{\"authorId\":\"2799799\",\"name\":\"Abbas Abdolmaleki\"},{\"authorId\":\"1879232\",\"name\":\"J. Merel\"},{\"authorId\":\"8455031\",\"name\":\"Andrew Lefrancq\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a9a3ed69c94a3e1c08ef1f833d9199f57736238b\",\"title\":\"DeepMind Control Suite\",\"url\":\"https://www.semanticscholar.org/paper/a9a3ed69c94a3e1c08ef1f833d9199f57736238b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jaderberg et al\"},{\"authorId\":null,\"name\":\"2017 Max Jaderberg\"},{\"authorId\":null,\"name\":\"Valentin Dalibard\"},{\"authorId\":null,\"name\":\"Simon Osindero\"},{\"authorId\":null,\"name\":\"Wojciech M. Czarnecki\"},{\"authorId\":null,\"name\":\"Jeff Donahue\"},{\"authorId\":null,\"name\":\"Ali Razavi\"},{\"authorId\":null,\"name\":\"Oriol Vinyals\"},{\"authorId\":null,\"name\":\"Tim Green\"},{\"authorId\":null,\"name\":\"Iain Dunning\"},{\"authorId\":null,\"name\":\"Karen Simonyan\"},{\"authorId\":null,\"name\":\"Chrisantha Fernando\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Population based training\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Romain Laroche\"},{\"authorId\":null,\"name\":\"Merwan Barlier. Transfer reinforcement learning with shar dynamics\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 31st AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 2147\\u20132153,\",\"year\":2017},{\"arxivId\":\"1511.06295\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"143959037\",\"name\":\"J. Kirkpatrick\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"title\":\"Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1510.02879\",\"authors\":[{\"authorId\":\"10197529\",\"name\":\"Janarthanan Rajendran\"},{\"authorId\":\"2943530\",\"name\":\"Aravind S. Lakshminarayanan\"},{\"authorId\":\"2361078\",\"name\":\"Mitesh M. Khapra\"},{\"authorId\":\"32899078\",\"name\":\"Prasanna Parthasarathi\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8bada64d0b04a66165b464a51c29f61e1f474f2a\",\"title\":\"Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain\",\"url\":\"https://www.semanticscholar.org/paper/8bada64d0b04a66165b464a51c29f61e1f474f2a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1712.00004\",\"authors\":[{\"authorId\":\"26389489\",\"name\":\"Martin Klissarov\"},{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4adfe39f84cfda351f5acf49f846eab6f21ccb67\",\"title\":\"Learnings Options End-to-End for Continuous Action Tasks\",\"url\":\"https://www.semanticscholar.org/paper/4adfe39f84cfda351f5acf49f846eab6f21ccb67\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48534728\",\"name\":\"P. Thomas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3df673220e67cc55d40fbe495c9fba999d820613\",\"title\":\"Bias in Natural Actor-Critic Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/3df673220e67cc55d40fbe495c9fba999d820613\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jinhua Song\"},{\"authorId\":null,\"name\":\"Yang Gao\"},{\"authorId\":null,\"name\":\"Hao Wang\"},{\"authorId\":null,\"name\":\"Bo An. Measuring the distance between finite Markov d processes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Autonomous Agents and Multiagent Systems\",\"url\":\"\",\"venue\":\"pages 468\\u2013476,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50881207\",\"name\":\"Barteld Kooi\"},{\"authorId\":\"2512398\",\"name\":\"H. V. Ditmarsch\"},{\"authorId\":\"1706100\",\"name\":\"W. Hoek\"}],\"doi\":\"10.5555/3306127\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"682646e296ad9e559edf75e7f222b0a825273d54\",\"title\":\"Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems\",\"url\":\"https://www.semanticscholar.org/paper/682646e296ad9e559edf75e7f222b0a825273d54\",\"venue\":\"AAMAS 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1411490468\",\"name\":\"Fernando Fern\\u00e1ndez-Rebollo\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1145/1160633.1160762\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f\",\"title\":\"Probabilistic policy reuse in a reinforcement learning agent\",\"url\":\"https://www.semanticscholar.org/paper/9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f\",\"venue\":\"AAMAS '06\",\"year\":2006},{\"arxivId\":\"1609.05140\",\"authors\":[{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"15b26d8cb35d7e795c8832fe08794224ee1e9f84\",\"title\":\"The Option-Critic Architecture\",\"url\":\"https://www.semanticscholar.org/paper/15b26d8cb35d7e795c8832fe08794224ee1e9f84\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50185605\",\"name\":\"Jinhua Song\"},{\"authorId\":\"145644823\",\"name\":\"Y. Gao\"},{\"authorId\":\"40021281\",\"name\":\"H. Wang\"},{\"authorId\":\"143706345\",\"name\":\"Bo An\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"84d9cf32975989a48931e651df13af6d1b7857ca\",\"title\":\"Measuring the Distance Between Finite Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/84d9cf32975989a48931e651df13af6d1b7857ca\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"235585741d5d2eddb85949226ddf4f581fd79dc1\",\"title\":\"Policy Transfer using Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/235585741d5d2eddb85949226ddf4f581fd79dc1\",\"venue\":\"AAMAS\",\"year\":2015},{\"arxivId\":\"1711.11289\",\"authors\":[{\"authorId\":\"34594615\",\"name\":\"Himanshu Sahni\"},{\"authorId\":\"39703750\",\"name\":\"S. Kumar\"},{\"authorId\":\"50342768\",\"name\":\"Farhan Tejani\"},{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"37b5980cf1a202fbec2fec28b831b0f90b8d217a\",\"title\":\"Learning to Compose Skills\",\"url\":\"https://www.semanticscholar.org/paper/37b5980cf1a202fbec2fec28b831b0f90b8d217a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"3450836\",\"name\":\"Merwan Barlier\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"4aa5334b7683019ec60794a483a18b48557a22c2\",\"title\":\"Transfer Reinforcement Learning with Shared Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/4aa5334b7683019ec60794a483a18b48557a22c2\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yuval Tassa\"},{\"authorId\":null,\"name\":\"Yotam Doron\"},{\"authorId\":null,\"name\":\"Alistair Muldal\"},{\"authorId\":null,\"name\":\"Tom Erez\"},{\"authorId\":null,\"name\":\"Yazhe Li\"},{\"authorId\":null,\"name\":\"Diego de Las Casas\"},{\"authorId\":null,\"name\":\"David Budden\"},{\"authorId\":null,\"name\":\"Abbas Abdolmaleki\"},{\"authorId\":null,\"name\":\"Josh Merel\"},{\"authorId\":null,\"name\":\"Andrew Lefrancq\"},{\"authorId\":null,\"name\":\"Timothy P. Lillicrap\"},{\"authorId\":null,\"name\":\"Martin A. Riedmiller. DeepMind Control Suite\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1801.00690,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Emma Brunskill\"},{\"authorId\":null,\"name\":\"Lihong Li. PAC-inspired option discovery in lifelong rein learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 316\\u2013324,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Philip Thomas. Bias in natural actor-critic algorithms\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 441\\u2013448,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Siyuan Li\"},{\"authorId\":null,\"name\":\"Chongjie Zhang. An optimal online method of selecting sourc learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 3562\\u20133570,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Anna Harutyunyan\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9. Policy transfer using reward shaping\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Autonomous Agents and Multiagent Systems\",\"url\":\"\",\"venue\":\"pages 181\\u2013188,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Fernando Fern\\u00e1ndez\"},{\"authorId\":null,\"name\":\"Manuela Veloso. Probabilistic policy reuse in a reinforcem agent\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Autonomous Agents and Multiagent Systems\",\"url\":\"\",\"venue\":\"pages 720\\u2013727,\",\"year\":2006},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1709.08201\",\"authors\":[{\"authorId\":\"145270413\",\"name\":\"S. Li\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fca20f28ced5eb09421ce94fd2dc4e730bdcea17\",\"title\":\"An Optimal Online Method of Selecting Source Policies for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/fca20f28ced5eb09421ce94fd2dc4e730bdcea17\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50086584\",\"name\":\"Haiyan Yin\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"title\":\"Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"References\"},{\"authorId\":null,\"name\":\"Bacon\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Timothy Mann\"},{\"authorId\":null,\"name\":\"Daniel Mankowitz\"},{\"authorId\":null,\"name\":\"Shie Mannor. Time-regularized interrupting options\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1350\\u20131358,\",\"year\":2014}],\"title\":\"Efficient Deep Reinforcement Learning via Adaptive Policy Transfer\",\"topics\":[],\"url\":\"https://www.semanticscholar.org/paper/b6456512bbb44079972cd7e3a5d7652c76fd26c9\",\"venue\":\"\",\"year\":2020}\n"