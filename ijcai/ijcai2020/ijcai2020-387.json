"{\"abstract\":\"In batch reinforcement learning (RL), one often constrains a learned policy to be close to the behavior (data-generating) policy, e.g., by constraining the learned action distribution to differ from the behavior policy by some maximum degree that is the same at each state. This can cause batch RL to be overly conservative, unable to exploit large policy changes at frequently-visited, high-confidence states without risking poor performance at sparsely-visited states. To remedy this, we propose residual policies, where the allowable deviation of the learned policy is state-action-dependent. We derive a new for RL method, BRPO, which learns both the policy and allowable deviation that jointly maximize a lower bound on policy performance. We show that BRPO achieves the state-of-the-art performance in a number of tasks.\",\"arxivId\":\"2002.05522\",\"authors\":[{\"authorId\":\"2459821\",\"name\":\"Sungryull Sohn\",\"url\":\"https://www.semanticscholar.org/author/2459821\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\",\"url\":\"https://www.semanticscholar.org/author/1819830\"},{\"authorId\":\"147126120\",\"name\":\"Jayden Ooi\",\"url\":\"https://www.semanticscholar.org/author/147126120\"},{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\",\"url\":\"https://www.semanticscholar.org/author/7624658\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\",\"url\":\"https://www.semanticscholar.org/author/1697141\"},{\"authorId\":\"2226805\",\"name\":\"Ed Huai-hsin Chi\",\"url\":\"https://www.semanticscholar.org/author/2226805\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\",\"url\":\"https://www.semanticscholar.org/author/145646162\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2006.03647\",\"authors\":[{\"authorId\":\"145930468\",\"name\":\"Tatsuya Matsushima\"},{\"authorId\":\"47646151\",\"name\":\"Hiroki Furuta\"},{\"authorId\":\"49484314\",\"name\":\"Y. Matsuo\"},{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"79ebde314ab90d066cee3b82193ef05666323394\",\"title\":\"Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization\",\"url\":\"https://www.semanticscholar.org/paper/79ebde314ab90d066cee3b82193ef05666323394\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":211096804,\"doi\":\"10.24963/ijcai.2020/387\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"9fb46f703d5a16e7e5261d5bdd1b385550d775b9\",\"references\":[{\"arxivId\":\"1907.00456\",\"authors\":[{\"authorId\":\"3106683\",\"name\":\"N. Jaques\"},{\"authorId\":\"2214185\",\"name\":\"Asma Ghandeharioun\"},{\"authorId\":\"40385687\",\"name\":\"J. Shen\"},{\"authorId\":\"145727063\",\"name\":\"C. Ferguson\"},{\"authorId\":\"123371831\",\"name\":\"\\u00c0gata Lapedriza\"},{\"authorId\":\"49522657\",\"name\":\"Noah J. Jones\"},{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"1719389\",\"name\":\"Rosalind W. Picard\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"57daffd65a5d73a439903f3e50950c21c9eba687\",\"title\":\"Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog\",\"url\":\"https://www.semanticscholar.org/paper/57daffd65a5d73a439903f3e50950c21c9eba687\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389654226\",\"name\":\"Brendan O'Donoghue\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5592edb72f1b0dea936ff8baa5ce81fd136b6f59\",\"title\":\"Combining policy gradient and Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/5592edb72f1b0dea936ff8baa5ce81fd136b6f59\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1901.01992\",\"authors\":[{\"authorId\":\"1388837087\",\"name\":\"Yasin Abbasi-Yadkori\"},{\"authorId\":\"1745169\",\"name\":\"P. Bartlett\"},{\"authorId\":\"1683647\",\"name\":\"X. Chen\"},{\"authorId\":\"145465606\",\"name\":\"Alan Malek\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6d29cce7230fbe144ce4326e4e9af9c5f3195154\",\"title\":\"Large-Scale Markov Decision Problems via the Linear Programming Dual\",\"url\":\"https://www.semanticscholar.org/paper/6d29cce7230fbe144ce4326e4e9af9c5f3195154\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1759633\",\"name\":\"A. R. Mahmood\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2be700f497c1d4ec4905158341a6dcab92a85b6f\",\"title\":\"Weighted importance sampling for off-policy learning with linear function approximation\",\"url\":\"https://www.semanticscholar.org/paper/2be700f497c1d4ec4905158341a6dcab92a85b6f\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47104078\",\"name\":\"Yangyang Xu\"},{\"authorId\":\"6833606\",\"name\":\"Wotao Yin\"}],\"doi\":\"10.1137/120887795\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9540779ab5e8fd4637287e73950d5d0a701f51fe\",\"title\":\"A Block Coordinate Descent Method for Regularized Multiconvex Optimization with Applications to Nonnegative Tensor Factorization and Completion\",\"url\":\"https://www.semanticscholar.org/paper/9540779ab5e8fd4637287e73950d5d0a701f51fe\",\"venue\":\"SIAM J. Imaging Sci.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S Boyd\"},{\"authorId\":null,\"name\":\"L Vandenberghe\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Convex optimization. Cambridge university press\",\"url\":\"\",\"venue\":\"\",\"year\":2004},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1843103\",\"name\":\"Stephen P. Boyd\"},{\"authorId\":\"2014414\",\"name\":\"L. Vandenberghe\"}],\"doi\":\"10.1017/CBO9780511804441\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4f607f03272e4d62708f5b2441355f9e005cb452\",\"title\":\"Convex Optimization\",\"url\":\"https://www.semanticscholar.org/paper/4f607f03272e4d62708f5b2441355f9e005cb452\",\"venue\":\"IEEE Transactions on Automatic Control\",\"year\":2006},{\"arxivId\":\"1910.01708\",\"authors\":[{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"},{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"103809454\",\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1534d860ab98b64d23ffa741d0ae52b6cadbf503\",\"title\":\"Benchmarking Batch Deep Reinforcement Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/1534d860ab98b64d23ffa741d0ae52b6cadbf503\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.05224\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"3429810\",\"name\":\"Michael J. Ahn\"},{\"authorId\":\"2843704\",\"name\":\"Hugo Ponte\"},{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"46511537\",\"name\":\"V. Kumar\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a18334c63972f1d81ca1e7b74fa833e38aaad708\",\"title\":\"Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real\",\"url\":\"https://www.semanticscholar.org/paper/a18334c63972f1d81ca1e7b74fa833e38aaad708\",\"venue\":\"CoRL\",\"year\":2019},{\"arxivId\":\"1611.01626\",\"authors\":[{\"authorId\":\"1389654226\",\"name\":\"Brendan O'Donoghue\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c40dd8f235aabe6efbb93c59c0536adf491f9ead\",\"title\":\"PGQ: Combining policy gradient and Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/c40dd8f235aabe6efbb93c59c0536adf491f9ead\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1712.06924\",\"authors\":[{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"31480268\",\"name\":\"P. Trichelair\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7eb0db941fbf19857631ec1c907f2888ea308779\",\"title\":\"Safe Policy Improvement with Baseline Bootstrapping\",\"url\":\"https://www.semanticscholar.org/paper/7eb0db941fbf19857631ec1c907f2888ea308779\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3130430\",\"name\":\"D. Farias\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"}],\"doi\":\"10.1287/opre.51.6.850.24925\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a14ac38f66996913c4d7f3a3141294a602fd8f3\",\"title\":\"The Linear Programming Approach to Approximate Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8a14ac38f66996913c4d7f3a3141294a602fd8f3\",\"venue\":\"Oper. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1620879412\",\"name\":\"Adam S. Opalski\"},{\"authorId\":\"1617806812\",\"name\":\"Artur Ruszczak\"},{\"authorId\":\"1620876654\",\"name\":\"Yurii Promovych\"},{\"authorId\":\"1620871083\",\"name\":\"Micha\\u0142 Horka\"},{\"authorId\":\"1620871057\",\"name\":\"Ladislav Derzsi\"},{\"authorId\":\"1620871097\",\"name\":\"Piotr\"},{\"authorId\":\"1620877859\",\"name\":\"Garstecki\"}],\"doi\":\"10.1515/9783111697888-007\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ae2118598c88a81d0bafcdc2d9706ff17944c307\",\"title\":\"E\",\"url\":\"https://www.semanticscholar.org/paper/ae2118598c88a81d0bafcdc2d9706ff17944c307\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":\"1803.07482\",\"authors\":[{\"authorId\":\"47342871\",\"name\":\"Ethan Knight\"},{\"authorId\":\"40893633\",\"name\":\"Osher Lerner\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c15bcd8e91e648560e7adb6287a3369d663c56e0\",\"title\":\"Natural Gradient Deep Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/c15bcd8e91e648560e7adb6287a3369d663c56e0\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144202584\",\"name\":\"S. Lange\"},{\"authorId\":\"2777657\",\"name\":\"T. Gabel\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":\"10.1007/978-3-642-27645-3_2\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5c65d095600d6c647426fa3bc45031b208882d5f\",\"title\":\"Batch Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5c65d095600d6c647426fa3bc45031b208882d5f\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49406642\",\"name\":\"D. Hunter\"},{\"authorId\":\"143953625\",\"name\":\"K. Lange\"}],\"doi\":\"10.1198/0003130042836\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"81f2cf2c4be45fe177b4b7ace8f5563bea27d47a\",\"title\":\"A Tutorial on MM Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/81f2cf2c4be45fe177b4b7ace8f5563bea27d47a\",\"venue\":\"\",\"year\":2004},{\"arxivId\":\"1812.06298\",\"authors\":[{\"authorId\":\"39047272\",\"name\":\"T. Silver\"},{\"authorId\":\"145254624\",\"name\":\"Kelsey R. Allen\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c21a1d8844e6b58fd74b2b94c512a23497029c1\",\"title\":\"Residual Policy Learning\",\"url\":\"https://www.semanticscholar.org/paper/8c21a1d8844e6b58fd74b2b94c512a23497029c1\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Qs\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"This approach of combining the optimal Bellman operator with the on-policy counterpart also belongs to the general class of hybrid on/off-policy RL algorithms [O\\u2019Donoghue\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Brockman\"},{\"authorId\":null,\"name\":\"V. Cheung\"},{\"authorId\":null,\"name\":\"L. Pettersson\"},{\"authorId\":null,\"name\":\"J. Schneider\"},{\"authorId\":null,\"name\":\"J. Schulman\"},{\"authorId\":null,\"name\":\"J. Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and W\",\"url\":\"\",\"venue\":\"Zaremba. Openai gym\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1708497\",\"name\":\"A. Gretton\"},{\"authorId\":\"1704422\",\"name\":\"K. Borgwardt\"},{\"authorId\":\"1733256\",\"name\":\"Malte J. Rasch\"},{\"authorId\":\"1707625\",\"name\":\"B. Sch\\u00f6lkopf\"},{\"authorId\":\"46234526\",\"name\":\"Alex Smola\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"d01e91dca60c52d5ad7ac4288ea576f8787d39c4\",\"title\":\"A Kernel Approach to Comparing Distributions\",\"url\":\"https://www.semanticscholar.org/paper/d01e91dca60c52d5ad7ac4288ea576f8787d39c4\",\"venue\":\"AAAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2353983\",\"name\":\"L. Faybusovich\"},{\"authorId\":\"1738947\",\"name\":\"J. Moore\"}],\"doi\":\"10.1007/BF02683337\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2cc4303142255872f8294ec4778cd2668aa6812b\",\"title\":\"Infinite-dimensional quadratic optimization: Interior-point methods and control applications\",\"url\":\"https://www.semanticscholar.org/paper/2cc4303142255872f8294ec4778cd2668aa6812b\",\"venue\":\"\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1703.00443\",\"authors\":[{\"authorId\":\"1773498\",\"name\":\"Brandon Amos\"},{\"authorId\":\"145116464\",\"name\":\"J. Z. Kolter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0076b232181e4e5be58dce8354a813ad2bbf663a\",\"title\":\"OptNet: Differentiable Optimization as a Layer in Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0076b232181e4e5be58dce8354a813ad2bbf663a\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1811.00260\",\"authors\":[{\"authorId\":\"144363634\",\"name\":\"Jason Gauci\"},{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"2397352\",\"name\":\"Yitao Liang\"},{\"authorId\":\"1894596\",\"name\":\"Kittipat Virochsiri\"},{\"authorId\":\"144047307\",\"name\":\"Y. He\"},{\"authorId\":\"51010350\",\"name\":\"Zachary Kaden\"},{\"authorId\":\"49540857\",\"name\":\"V. Narayanan\"},{\"authorId\":\"3202244\",\"name\":\"Xiaohui Ye\"},{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82ef41d2065e366ce1130d36b53881e06f1af2ed\",\"title\":\"Horizon: Facebook's Open Source Applied Reinforcement Learning Platform\",\"url\":\"https://www.semanticscholar.org/paper/82ef41d2065e366ce1130d36b53881e06f1af2ed\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1812.03201\",\"authors\":[{\"authorId\":\"51307365\",\"name\":\"T. Johannink\"},{\"authorId\":\"8527563\",\"name\":\"Shikhar Bahl\"},{\"authorId\":\"3422774\",\"name\":\"Ashvin Nair\"},{\"authorId\":\"2591708\",\"name\":\"Jianlan Luo\"},{\"authorId\":\"48823024\",\"name\":\"A. Kumar\"},{\"authorId\":\"2132769\",\"name\":\"M. Loskyll\"},{\"authorId\":\"32027425\",\"name\":\"J. A. Ojea\"},{\"authorId\":\"2419277\",\"name\":\"Eugen Solowjow\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.1109/ICRA.2019.8794127\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ae4d32f05cf40e4cc01c69d7787149a258c95eda\",\"title\":\"Residual Reinforcement Learning for Robot Control\",\"url\":\"https://www.semanticscholar.org/paper/ae4d32f05cf40e4cc01c69d7787149a258c95eda\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":\"1607.03842\",\"authors\":[{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"145630605\",\"name\":\"M. Petrik\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"title\":\"Safe Policy Improvement by Minimizing Robust Baseline Regret\",\"url\":\"https://www.semanticscholar.org/paper/bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"2850430\",\"name\":\"A. Pecorino\"},{\"authorId\":\"2439765\",\"name\":\"Daniele Calandriello\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fe40cd17112743123b5eb2830a44c3ea9b70ebaa\",\"title\":\"Safe Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/fe40cd17112743123b5eb2830a44c3ea9b70ebaa\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":\"1906.00949\",\"authors\":[{\"authorId\":\"19121746\",\"name\":\"A. Kumar\"},{\"authorId\":\"2550764\",\"name\":\"Justin Fu\"},{\"authorId\":\"145499435\",\"name\":\"G. Tucker\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd\",\"title\":\"Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction\",\"url\":\"https://www.semanticscholar.org/paper/82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"},{\"authorId\":\"144162125\",\"name\":\"J. Langford\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"523b4ce1c2a1336962444abc1dec215756c2f3e6\",\"title\":\"Approximately Optimal Approximate Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/523b4ce1c2a1336962444abc1dec215756c2f3e6\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31474779\",\"name\":\"A. Shapiro\"},{\"authorId\":\"3016531\",\"name\":\"D. Dentcheva\"},{\"authorId\":\"1996654\",\"name\":\"A. Ruszczynski\"}],\"doi\":\"10.1137/1.9780898718751\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f1af9d46eaee472ceb53b6de6bbf8c30ac281716\",\"title\":\"Lectures on Stochastic Programming - Modeling and Theory\",\"url\":\"https://www.semanticscholar.org/paper/f1af9d46eaee472ceb53b6de6bbf8c30ac281716\",\"venue\":\"MOS-SIAM Series on Optimization\",\"year\":2009},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D Lange\"},{\"authorId\":null,\"name\":\"K Hunter\"},{\"authorId\":null,\"name\":\"Lange\"},{\"authorId\":null,\"name\":\"Jaques\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration\",\"url\":\"\",\"venue\":\"Way offpolicy batch deep reinforcement learning of implicit human preferences in dialog\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1621190045\",\"name\":\"Yu-Qin Cao\"},{\"authorId\":\"1617945342\",\"name\":\"Lei Yuan\"},{\"authorId\":\"1618266074\",\"name\":\"Qin Zhao\"},{\"authorId\":\"1617846354\",\"name\":\"Jian-Lin Yuan\"},{\"authorId\":\"1618093177\",\"name\":\"Yung-Fu Chang\"},{\"authorId\":\"1618081648\",\"name\":\"Xin-Tian Wen\"},{\"authorId\":\"1618259371\",\"name\":\"Rui Wu\"},{\"authorId\":\"1617850008\",\"name\":\"Xiao-Bo Huang\"},{\"authorId\":\"1617935592\",\"name\":\"Xin-Feng Han\"},{\"authorId\":\"1618215030\",\"name\":\"Xiao-Ping Ma\"},{\"authorId\":\"1618230784\",\"name\":\"San-Jie Cao\"}],\"doi\":\"10.1515/9783111419787-003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1f2981c3146fbd9300bd8fbfd0401ba65f8ce5a\",\"title\":\"H\",\"url\":\"https://www.semanticscholar.org/paper/f1f2981c3146fbd9300bd8fbfd0401ba65f8ce5a\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":\"1511.06295\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"143959037\",\"name\":\"J. Kirkpatrick\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"title\":\"Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"E Knight\"},{\"authorId\":null,\"name\":\"O Lerner\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Natural gradient deep Q-learning. ArXiv\",\"url\":\"\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145739642\",\"name\":\"J. Kober\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":\"10.1109/MRA.2010.936952\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"14b287bf01bc1cfc656be1493aadd21cca3ae8d3\",\"title\":\"Imitation and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/14b287bf01bc1cfc656be1493aadd21cca3ae8d3\",\"venue\":\"IEEE Robotics & Automation Magazine\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Pirotta\"},{\"authorId\":null,\"name\":\"Marcello Restelli\"},{\"authorId\":null,\"name\":\"Alessio Pecorino\"},{\"authorId\":null,\"name\":\"Daniele Calandriello. Safe policy iteration\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 307\\u2013315,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2659632\",\"name\":\"F. Bolley\"},{\"authorId\":\"16614521\",\"name\":\"C. Villani\"}],\"doi\":\"10.5802/AFST.1095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a66d82bf630a73c7d8fa792477f85122525e470b\",\"title\":\"Weighted Csisz\\u00e1r-Kullback-Pinsker inequalities and applications to transportation inequalities\",\"url\":\"https://www.semanticscholar.org/paper/a66d82bf630a73c7d8fa792477f85122525e470b\",\"venue\":\"\",\"year\":2005},{\"arxivId\":\"1812.02900\",\"authors\":[{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5285cb8faada5de8a92a47622950f6cfd476ac1d\",\"title\":\"Off-Policy Deep Reinforcement Learning without Exploration\",\"url\":\"https://www.semanticscholar.org/paper/5285cb8faada5de8a92a47622950f6cfd476ac1d\",\"venue\":\"ICML\",\"year\":2019}],\"title\":\"BRPO: Batch Residual Policy Optimization\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/9fb46f703d5a16e7e5261d5bdd1b385550d775b9\",\"venue\":\"ArXiv\",\"year\":2020}\n"