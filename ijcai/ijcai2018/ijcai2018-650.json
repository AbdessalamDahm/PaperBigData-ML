"{\"abstract\":\"In many practical uses of reinforcement learning (RL) the set of actions available at a given state is a random variable, with realizations governed by an exogenous stochastic process. Somewhat surprisingly, the foundations for such sequential decision processes have been unaddressed. In this work, we formalize and investigate MDPs with stochastic action sets (SAS-MDPs) to provide these foundations. We show that optimal policies and value functions in this model have a structure that admits a compact representation. From an RL perspective, we show that Q-learning with sampled action sets is sound. In model-based settings, we consider two important special cases: when individual actions are available with independent probabilities; and a sampling-based model for unknown distributions. We develop poly-time value and policy iteration methods for both cases; and in the first, we offer a poly-time linear programming solution.\",\"arxivId\":\"1805.02363\",\"authors\":[{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\",\"url\":\"https://www.semanticscholar.org/author/145646162\"},{\"authorId\":\"145741057\",\"name\":\"Alon Cohen\",\"url\":\"https://www.semanticscholar.org/author/145741057\"},{\"authorId\":\"2898312\",\"name\":\"Amit Daniely\",\"url\":\"https://www.semanticscholar.org/author/2898312\"},{\"authorId\":\"1809983\",\"name\":\"Avinatan Hassidim\",\"url\":\"https://www.semanticscholar.org/author/1809983\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\",\"url\":\"https://www.semanticscholar.org/author/144830983\"},{\"authorId\":\"3174612\",\"name\":\"Ofer Meshi\",\"url\":\"https://www.semanticscholar.org/author/3174612\"},{\"authorId\":\"47182790\",\"name\":\"M. Mladenov\",\"url\":\"https://www.semanticscholar.org/author/47182790\"},{\"authorId\":\"50319359\",\"name\":\"D. Schuurmans\",\"url\":\"https://www.semanticscholar.org/author/50319359\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1905.12767\",\"authors\":[{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"95115822\",\"name\":\"J. Wang\"},{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"3065516\",\"name\":\"R. Agarwal\"},{\"authorId\":\"144693688\",\"name\":\"R. Wu\"},{\"authorId\":\"2061550\",\"name\":\"Heng-Tze Cheng\"},{\"authorId\":\"134426275\",\"name\":\"Morgane Lustman\"},{\"authorId\":\"48372476\",\"name\":\"V. Gatto\"},{\"authorId\":\"3453941\",\"name\":\"P. Covington\"},{\"authorId\":\"35055954\",\"name\":\"Jim McFadden\"},{\"authorId\":\"2158476\",\"name\":\"T. Chandra\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"91fc156ee07c67b7abea9eac9a873bdf9c944fd8\",\"title\":\"Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology\",\"url\":\"https://www.semanticscholar.org/paper/91fc156ee07c67b7abea9eac9a873bdf9c944fd8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.01770\",\"authors\":[{\"authorId\":\"2232505\",\"name\":\"Yash Chandak\"},{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"},{\"authorId\":\"71309987\",\"name\":\"C. Nota\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"}],\"doi\":\"10.1609/AAAI.V34I04.5739\",\"intent\":[\"result\",\"background\"],\"isInfluential\":true,\"paperId\":\"1a074c546e54155809492a80310d5799013c4d85\",\"title\":\"Lifelong Learning with a Changing Action Set\",\"url\":\"https://www.semanticscholar.org/paper/1a074c546e54155809492a80310d5799013c4d85\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1909.04847\",\"authors\":[{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"145344473\",\"name\":\"C. Hsu\"},{\"authorId\":\"47182790\",\"name\":\"M. Mladenov\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1734069\",\"name\":\"J. Wang\"},{\"authorId\":\"144265846\",\"name\":\"R. Wu\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f81372f1c489c0ffa695ba6f623bb71dc2dc60ed\",\"title\":\"RecSim: A Configurable Simulation Platform for Recommender Systems\",\"url\":\"https://www.semanticscholar.org/paper/f81372f1c489c0ffa695ba6f623bb71dc2dc60ed\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.01772\",\"authors\":[{\"authorId\":\"2232505\",\"name\":\"Yash Chandak\"},{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"},{\"authorId\":\"66908272\",\"name\":\"Blossom Metevier\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"}],\"doi\":\"10.1609/AAAI.V34I04.5740\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2c02fb1fb0753db1c4012158f01341c1118c6b09\",\"title\":\"Reinforcement Learning When All Actions are Not Always Available\",\"url\":\"https://www.semanticscholar.org/paper/2c02fb1fb0753db1c4012158f01341c1118c6b09\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1909.12397\",\"authors\":[{\"authorId\":\"34609551\",\"name\":\"Moonkyung Ryu\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"},{\"authorId\":\"121732142\",\"name\":\"R. Anderson\"},{\"authorId\":\"144707259\",\"name\":\"Christian Tjandraatmadja\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e33b29f068bd0326aee3dcd3d34c44f23d1a21c0\",\"title\":\"CAQL: Continuous Action Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/e33b29f068bd0326aee3dcd3d34c44f23d1a21c0\",\"venue\":\"ICLR\",\"year\":2020}],\"corpusId\":26072136,\"doi\":\"10.24963/ijcai.2018/650\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"592d1e73ee2d0c2e13da8bf52f735ebfb1a1efa1\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"5425962\",\"name\":\"J. Pierce\"}],\"doi\":\"10.1016/0166-218x(90)90036-c\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7cd97b0579ea3e4be968b4f52ce83ad06f2e7349\",\"title\":\"Geometric Algorithms and Combinatorial Optimization\",\"url\":\"https://www.semanticscholar.org/paper/7cd97b0579ea3e4be968b4f52ce83ad06f2e7349\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1762737\",\"name\":\"Nikolay Archak\"},{\"authorId\":\"1728881\",\"name\":\"V. Mirrokni\"},{\"authorId\":\"144963537\",\"name\":\"S. Muthukrishnan\"}],\"doi\":\"10.1007/978-3-642-35311-6_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d04e6ed536d72c2354416fb5e078aea7f7bf17f5\",\"title\":\"Budget Optimization for Online Campaigns with Positive Carryover Effects\",\"url\":\"https://www.semanticscholar.org/paper/d04e6ed536d72c2354416fb5e078aea7f7bf17f5\",\"venue\":\"WINE\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martin Gr\\u00f6tschel\"},{\"authorId\":null,\"name\":\"L\\u00e1szlo Lov\\u00e1sz\"},{\"authorId\":null,\"name\":\"Alexander Schrijver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Geometric Algorithms and Combinatorial Optimization, volume 2 of Algorithms and Combinatorics\",\"url\":\"\",\"venue\":\"\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Tseng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Solving h-horizon\",\"url\":\"\",\"venue\":\"stationary Markov decision problems in time proportional to log(h). Operations Research Letters, 9(5):287\\u2013297\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745732\",\"name\":\"M. Charikar\"},{\"authorId\":\"1683442\",\"name\":\"Ravi Kumar\"},{\"authorId\":\"145503401\",\"name\":\"P. Raghavan\"},{\"authorId\":\"145941159\",\"name\":\"Sridhar Rajagopalan\"},{\"authorId\":\"49365095\",\"name\":\"A. Tomkins\"}],\"doi\":\"10.1145/301250.301280\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b77b27ada1af932c83c1503b12445ef749adfd7\",\"title\":\"On targeting Markov segments\",\"url\":\"https://www.semanticscholar.org/paper/5b77b27ada1af932c83c1503b12445ef749adfd7\",\"venue\":\"STOC '99\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145185457\",\"name\":\"Y. Ye\"}],\"doi\":\"10.1287/moor.1110.0516\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fb9fc0e06d4adef81415559b6311ce916af8b5a4\",\"title\":\"The Simplex and Policy-Iteration Methods Are Strongly Polynomial for the Markov Decision Problem with a Fixed Discount Rate\",\"url\":\"https://www.semanticscholar.org/paper/fb9fc0e06d4adef81415559b6311ce916af8b5a4\",\"venue\":\"Math. Oper. Res.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H Christos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Papadimitriou and Mihalis Yannakakis . Shortest paths without a map\",\"url\":\"\",\"venue\":\"Theoretical Computer Science\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9258975\",\"name\":\"E. Nikolova\"},{\"authorId\":\"1743286\",\"name\":\"D. Karger\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6cea8252824075af8df7f418e86cd618a9b22b04\",\"title\":\"Route Planning under Uncertainty: The Canadian Traveller Problem\",\"url\":\"https://www.semanticscholar.org/paper/6cea8252824075af8df7f418e86cd618a9b22b04\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Boutilier et al\"},{\"authorId\":null,\"name\":\"2018 C. Boutilier\"},{\"authorId\":null,\"name\":\"A. Cohen\"},{\"authorId\":null,\"name\":\"A. Hassidim\"},{\"authorId\":null,\"name\":\"Y. Mansour\"},{\"authorId\":null,\"name\":\"O. Meshi\"},{\"authorId\":null,\"name\":\"M. Mladenov\"},{\"authorId\":null,\"name\":\"D. Schuurmans\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Planning and learning in Markov decision processes with stochastic action\",\"url\":\"\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"50449995\",\"name\":\"L. Newnham\"},{\"authorId\":\"145949771\",\"name\":\"D. Barker\"},{\"authorId\":\"37019042\",\"name\":\"Suzanne Weller\"},{\"authorId\":\"37064242\",\"name\":\"Jason McFall\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69235e974adc94428021af15ae9cfb6b5c90fe55\",\"title\":\"Concurrent Reinforcement Learning from Customer Interactions\",\"url\":\"https://www.semanticscholar.org/paper/69235e974adc94428021af15ae9cfb6b5c90fe55\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145382322\",\"name\":\"T. Li\"},{\"authorId\":\"143881194\",\"name\":\"Ning Liu\"},{\"authorId\":\"143909715\",\"name\":\"Jun Yan\"},{\"authorId\":\"40516745\",\"name\":\"G. Wang\"},{\"authorId\":\"143832417\",\"name\":\"F. Bai\"},{\"authorId\":\"48354590\",\"name\":\"Z. Chen\"}],\"doi\":\"10.1145/1592748.1592750\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d85db37191f042caf723fe160792d8a8917af9ae\",\"title\":\"A Markov chain model for integrating behavioral targeting into contextual advertising\",\"url\":\"https://www.semanticscholar.org/paper/d85db37191f042caf723fe160792d8a8917af9ae\",\"venue\":\"KDD Workshop on Data Mining and Audience Intelligence for Advertising\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0c11db37dd24ca3a866481303bebfc37a93a37d0\",\"title\":\"Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees\",\"url\":\"https://www.semanticscholar.org/paper/0c11db37dd24ca3a866481303bebfc37a93a37d0\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2538104\",\"name\":\"M. Mladenov\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"},{\"authorId\":\"3174612\",\"name\":\"Ofer Meshi\"},{\"authorId\":\"1684677\",\"name\":\"G. Elidan\"},{\"authorId\":\"2409297\",\"name\":\"Tyler Lu\"}],\"doi\":\"10.24963/ijcai.2017/346\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eeb9f3dce2a79f548f7e6c8f7157ae68233ccb2c\",\"title\":\"Logistic Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/eeb9f3dce2a79f548f7e6c8f7157ae68233ccb2c\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2930822\",\"name\":\"G. Polychronopoulos\"},{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":\"10.1002/(SICI)1097-0037(199603)27:2%3C133::AID-NET5%3E3.0.CO;2-L\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e795a076e84730bcb90e4debbf8127e0d48ca6a\",\"title\":\"Stochastic shortest path problems with recourse\",\"url\":\"https://www.semanticscholar.org/paper/1e795a076e84730bcb90e4debbf8127e0d48ca6a\",\"venue\":\"Networks\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2080218\",\"name\":\"Varun Kanade\"},{\"authorId\":\"145057514\",\"name\":\"H. McMahan\"},{\"authorId\":\"40634580\",\"name\":\"B. Bryan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c38fb868b36a7f041886e9aca3de934cfa2fb7ce\",\"title\":\"Sleeping Experts and Bandits with Stochastic Action Availability and Adversarial Rewards\",\"url\":\"https://www.semanticscholar.org/paper/c38fb868b36a7f041886e9aca3de934cfa2fb7ce\",\"venue\":\"AISTATS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1762737\",\"name\":\"Nikolay Archak\"},{\"authorId\":\"1728881\",\"name\":\"V. Mirrokni\"},{\"authorId\":\"144963537\",\"name\":\"S. Muthukrishnan\"}],\"doi\":\"10.1145/1772690.1772695\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba2f5da6fb13e6ace6043603c0b7242d473a8f4b\",\"title\":\"Mining advertiser-specific user behavior using adfactors\",\"url\":\"https://www.semanticscholar.org/paper/ba2f5da6fb13e6ace6043603c0b7242d473a8f4b\",\"venue\":\"WWW '10\",\"year\":2010},{\"arxivId\":\"1008.0530\",\"authors\":[{\"authorId\":\"3248952\",\"name\":\"T. Hansen\"},{\"authorId\":\"3215910\",\"name\":\"Peter Bro Miltersen\"},{\"authorId\":\"1750794\",\"name\":\"U. Zwick\"}],\"doi\":\"10.1145/2432622.2432623\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6818db9d53c65082c0d83e757bb0e3a580c32a79\",\"title\":\"Strategy Iteration Is Strongly Polynomial for 2-Player Turn-Based Stochastic Games with a Constant Discount Factor\",\"url\":\"https://www.semanticscholar.org/paper/6818db9d53c65082c0d83e757bb0e3a580c32a79\",\"venue\":\"JACM\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1682823\",\"name\":\"P. Tseng\"}],\"doi\":\"10.1016/0167-6377(90)90022-W\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d8bed554b0f08880fafadfa6d66e90f783cc9a7\",\"title\":\"Solving H-horizon, stationary Markov decision problems in time proportional to log(H)\",\"url\":\"https://www.semanticscholar.org/paper/9d8bed554b0f08880fafadfa6d66e90f783cc9a7\",\"venue\":\"\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144102674\",\"name\":\"C. Papadimitriou\"},{\"authorId\":\"1748179\",\"name\":\"M. Yannakakis\"}],\"doi\":\"10.1016/0304-3975(91)90263-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff187225ff569e01e751ebf004076350d4456a51\",\"title\":\"Shortest Paths Without a Map\",\"url\":\"https://www.semanticscholar.org/paper/ff187225ff569e01e751ebf004076350d4456a51\",\"venue\":\"Theor. Comput. Sci.\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4562073\",\"name\":\"Chris Watkins\"},{\"authorId\":\"46902274\",\"name\":\"P. Dayan\"}],\"doi\":\"10.1007/BF00992698\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03b7e51c52084ac1db5118342a00b5fbcfc587aa\",\"title\":\"Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/03b7e51c52084ac1db5118342a00b5fbcfc587aa\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Mnih\"},{\"authorId\":null,\"name\":\"K. Kavukcuoglu\"},{\"authorId\":null,\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Andrei A Rusu\",\"url\":\"\",\"venue\":\"J. Veness, Marc G Bellemare, A. Graves, M. Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529\\u2013533\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10099703\",\"name\":\"U. Meister\"},{\"authorId\":\"3361770\",\"name\":\"U. Holzbaur\"}],\"doi\":\"10.1007/BF01720771\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b75a0d64840ac2315c57686dd2b2573c88335429\",\"title\":\"A polynomial time bound for Howard's policy improvement algorithm\",\"url\":\"https://www.semanticscholar.org/paper/b75a0d64840ac2315c57686dd2b2573c88335429\",\"venue\":\"\",\"year\":1986},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2633757\",\"name\":\"Robert D. Kleinberg\"},{\"authorId\":\"1399048849\",\"name\":\"Alexandru Niculescu-Mizil\"},{\"authorId\":\"2800897\",\"name\":\"Y. Sharma\"}],\"doi\":\"10.1007/s10994-010-5178-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3a4bccef686b08da0ef273fcac01baccd32e8f5\",\"title\":\"Regret bounds for sleeping experts and bandits\",\"url\":\"https://www.semanticscholar.org/paper/e3a4bccef686b08da0ef273fcac01baccd32e8f5\",\"venue\":\"Machine Learning\",\"year\":2010},{\"arxivId\":\"1210.4847\",\"authors\":[{\"authorId\":\"39617345\",\"name\":\"K. Amin\"},{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"7217471\",\"name\":\"P. Key\"},{\"authorId\":\"2071649\",\"name\":\"Anton Schwaighofer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"47b4904001f4bcde5088a5c8a4ad2a18364d64c0\",\"title\":\"Budget Optimization for Sponsored Search: Censored Learning in MDPs\",\"url\":\"https://www.semanticscholar.org/paper/47b4904001f4bcde5088a5c8a4ad2a18364d64c0\",\"venue\":\"UAI\",\"year\":2012}],\"title\":\"Planning and Learning with Stochastic Action Sets\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Linear programming\",\"topicId\":\"12656\",\"url\":\"https://www.semanticscholar.org/topic/12656\"},{\"topic\":\"Time complexity\",\"topicId\":\"3448\",\"url\":\"https://www.semanticscholar.org/topic/3448\"},{\"topic\":\"Stochastic process\",\"topicId\":\"21302\",\"url\":\"https://www.semanticscholar.org/topic/21302\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Polynomial\",\"topicId\":\"10269\",\"url\":\"https://www.semanticscholar.org/topic/10269\"},{\"topic\":\"Iterative method\",\"topicId\":\"304\",\"url\":\"https://www.semanticscholar.org/topic/304\"},{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Time value of money\",\"topicId\":\"538384\",\"url\":\"https://www.semanticscholar.org/topic/538384\"},{\"topic\":\"Iteration\",\"topicId\":\"11823\",\"url\":\"https://www.semanticscholar.org/topic/11823\"},{\"topic\":\"SAS\",\"topicId\":\"44546\",\"url\":\"https://www.semanticscholar.org/topic/44546\"}],\"url\":\"https://www.semanticscholar.org/paper/592d1e73ee2d0c2e13da8bf52f735ebfb1a1efa1\",\"venue\":\"IJCAI\",\"year\":2018}\n"