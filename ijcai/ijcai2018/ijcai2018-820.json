"{\"abstract\":\"Reinforcement learning is a major tool to realize intelligent agents that can be autonomously adaptive to the environment. With deep models, reinforcement learning has shown great potential in complex tasks such as playing games from pixels. However, current reinforcement learning techniques are still suffer from requiring a huge amount of interaction data, which could result in unbearable cost in realworld applications. In this article, we share our understanding of the problem, and discuss possible ways to alleviate the sample cost of reinforcement learning, from the aspects of exploration, optimization, environment modeling, experience transfer, and abstraction. We also discuss some challenges in real-world applications, with the hope of inspiring future researches.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"48623698\",\"name\":\"Yang Yu\",\"url\":\"https://www.semanticscholar.org/author/48623698\"}],\"citationVelocity\":8,\"citations\":[{\"arxivId\":\"2007.06741\",\"authors\":[{\"authorId\":\"123306106\",\"name\":\"J. Ara'ujo\"},{\"authorId\":\"34659351\",\"name\":\"M\\u00e1rio A. T. Figueiredo\"},{\"authorId\":\"2402165\",\"name\":\"M. Botto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4f859b368344877a95dfc61b1d7218d410b008e2\",\"title\":\"Single-partition adaptive Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/4f859b368344877a95dfc61b1d7218d410b008e2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.01226\",\"authors\":[{\"authorId\":\"40892942\",\"name\":\"Charles W. L. Gadd\"},{\"authorId\":\"34066178\",\"name\":\"M. Heinonen\"},{\"authorId\":\"91540704\",\"name\":\"Harri L\\u00e4hdesm\\u00e4ki\"},{\"authorId\":\"69057684\",\"name\":\"S. Kaski\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c1cfffd55c7dd73c58145c5bc89758b92478b312\",\"title\":\"Sample-efficient reinforcement learning using deep Gaussian processes\",\"url\":\"https://www.semanticscholar.org/paper/c1cfffd55c7dd73c58145c5bc89758b92478b312\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.04947\",\"authors\":[{\"authorId\":\"9169532\",\"name\":\"Hardik Meisheri\"},{\"authorId\":\"1402365052\",\"name\":\"Omkar Shelke\"},{\"authorId\":\"1384198417\",\"name\":\"R. Verma\"},{\"authorId\":\"2512658\",\"name\":\"H. Khadilkar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c2c7b21d3eb35cb19ed8ff2fca37a291dcf518e2\",\"title\":\"Accelerating Training in Pommerman with Imitation and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c2c7b21d3eb35cb19ed8ff2fca37a291dcf518e2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1740789359\",\"name\":\"Jaamas Track\"},{\"authorId\":\"1422535836\",\"name\":\"P. Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.5555/3398761.3399105\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4bc07de96eb0bc57c480831a3d18ec18444ee229\",\"title\":\"A Very Condensed Survey and Critique of Multiagent Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4bc07de96eb0bc57c480831a3d18ec18444ee229\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"2002.12089\",\"authors\":[{\"authorId\":\"1387548078\",\"name\":\"K. Lin\"},{\"authorId\":\"144513168\",\"name\":\"L. Gong\"},{\"authorId\":null,\"name\":\"Xudong Li\"},{\"authorId\":\"148244892\",\"name\":\"Te Sun\"},{\"authorId\":\"66979217\",\"name\":\"Binhao Chen\"},{\"authorId\":\"153903838\",\"name\":\"Chengliang Liu\"},{\"authorId\":\"48805770\",\"name\":\"Z. Zhang\"},{\"authorId\":\"143698720\",\"name\":\"Jian Pu\"},{\"authorId\":\"2247926\",\"name\":\"Junping Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c79edba0079d9b64359a085a96661c3d125b159\",\"title\":\"Exploration-efficient Deep Reinforcement Learning with Demonstration Guidance for Robot Control\",\"url\":\"https://www.semanticscholar.org/paper/4c79edba0079d9b64359a085a96661c3d125b159\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151234813\",\"name\":\"Takeshi Morinibu\"},{\"authorId\":\"1500411089\",\"name\":\"Tomohiro Noda\"},{\"authorId\":\"66869190\",\"name\":\"Tanaka Shota\"}],\"doi\":\"10.1109/IIAI-AAI.2019.00120\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d41d0878a4b4857132484dc9f26cb88c7470b3d3\",\"title\":\"Application of Deep Reinforcement Learning in Residential Preconditioning for Radiation Temperature\",\"url\":\"https://www.semanticscholar.org/paper/d41d0878a4b4857132484dc9f26cb88c7470b3d3\",\"venue\":\"2019 8th International Congress on Advanced Applied Informatics (IIAI-AAI)\",\"year\":2019},{\"arxivId\":\"1904.05759\",\"authors\":[{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"144036350\",\"name\":\"C. Gao\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"970398fa7759b0559d8a6267975b6ed5ccd3f7a9\",\"title\":\"Safer Deep RL with Shallow MCTS: A Case Study in Pommerman\",\"url\":\"https://www.semanticscholar.org/paper/970398fa7759b0559d8a6267975b6ed5ccd3f7a9\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.02141\",\"authors\":[{\"authorId\":\"108635252\",\"name\":\"J. P. Ara'ujo\"},{\"authorId\":\"34659351\",\"name\":\"M\\u00e1rio A. T. Figueiredo\"},{\"authorId\":\"2402165\",\"name\":\"M. Botto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3eb008550f4f2400096737d51b67ecef29830cfa\",\"title\":\"Control with adaptive Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/3eb008550f4f2400096737d51b67ecef29830cfa\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.10295\",\"authors\":[{\"authorId\":\"19597437\",\"name\":\"A. Antoniou\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"06125bab53c6629ff24f2ea72080bb4ba59e2741\",\"title\":\"Learning to learn via Self-Critique\",\"url\":\"https://www.semanticscholar.org/paper/06125bab53c6629ff24f2ea72080bb4ba59e2741\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1908.00169\",\"authors\":[{\"authorId\":\"150350159\",\"name\":\"Yadan Luo\"},{\"authorId\":\"145622169\",\"name\":\"Zi Huang\"},{\"authorId\":\"145527564\",\"name\":\"Z. Zhang\"},{\"authorId\":\"31115284\",\"name\":\"Jingjing Li\"},{\"authorId\":\"6897666\",\"name\":\"Yang Yang\"}],\"doi\":\"10.1145/3343031.3350961\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250\",\"title\":\"Curiosity-driven Reinforcement Learning for Diverse Visual Paragraph Generation\",\"url\":\"https://www.semanticscholar.org/paper/d4c8f6c4a2b744fcfd82a7d7c8041d87d2b5c250\",\"venue\":\"ACM Multimedia\",\"year\":2019},{\"arxivId\":\"1810.05587\",\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1007/s10458-019-09421-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b\",\"title\":\"Is multiagent deep reinforcement learning the answer or the question? A brief survey\",\"url\":\"https://www.semanticscholar.org/paper/3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1907.10827\",\"authors\":[{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5606ab54dcbbc56c8f0ebea9e7324db0b93d468\",\"title\":\"Terminal Prediction as an Auxiliary Task for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5606ab54dcbbc56c8f0ebea9e7324db0b93d468\",\"venue\":\"AIIDE\",\"year\":2019},{\"arxivId\":\"2003.13676\",\"authors\":[{\"authorId\":\"143774543\",\"name\":\"Pieter Libin\"},{\"authorId\":\"1598440606\",\"name\":\"Arno Moonens\"},{\"authorId\":\"32638491\",\"name\":\"T. Verstraeten\"},{\"authorId\":\"1598440545\",\"name\":\"Fabian Perez-Sanjines\"},{\"authorId\":\"2554551\",\"name\":\"N. Hens\"},{\"authorId\":\"114575298\",\"name\":\"P. Lemey\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"334e52105a093249c6c8c3cf901cb1ddc30bd32f\",\"title\":\"Deep reinforcement learning for large-scale epidemic control\",\"url\":\"https://www.semanticscholar.org/paper/334e52105a093249c6c8c3cf901cb1ddc30bd32f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1904.08149\",\"authors\":[{\"authorId\":\"108098245\",\"name\":\"Ozan \\u00c7atal\"},{\"authorId\":\"107925135\",\"name\":\"Johannes Nauta\"},{\"authorId\":\"2413244\",\"name\":\"Tim Verbelen\"},{\"authorId\":\"34209448\",\"name\":\"P. Simoens\"},{\"authorId\":\"1733741\",\"name\":\"B. Dhoedt\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb42e9061caa1600a8b317f86bfc24169cd152f2\",\"title\":\"Bayesian policy selection using active inference\",\"url\":\"https://www.semanticscholar.org/paper/cb42e9061caa1600a8b317f86bfc24169cd152f2\",\"venue\":\"ICLR 2019\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"25841722\",\"name\":\"Ziniu Li\"},{\"authorId\":\"5819016\",\"name\":\"Xiong-Hui Chen\"}],\"doi\":\"10.1007/978-3-030-64096-5_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"10a700d749b11018e97de03ce9c11da6313a930c\",\"title\":\"Efficient Exploration by Novelty-Pursuit\",\"url\":\"https://www.semanticscholar.org/paper/10a700d749b11018e97de03ce9c11da6313a930c\",\"venue\":\"DAI\",\"year\":2020},{\"arxivId\":\"1812.00045\",\"authors\":[{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c72b6c5300348b603b315f5025ace9a015fe75f\",\"title\":\"Using Monte Carlo Tree Search as a Demonstrator within Asynchronous Deep RL\",\"url\":\"https://www.semanticscholar.org/paper/5c72b6c5300348b603b315f5025ace9a015fe75f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1907.11703\",\"authors\":[{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9df5429838b18f3a64f1d60b62ebb5bd1ae0b075\",\"title\":\"Action Guidance with MCTS for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9df5429838b18f3a64f1d60b62ebb5bd1ae0b075\",\"venue\":\"AIIDE\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"1729566\",\"name\":\"E. Elkind\"},{\"authorId\":\"145644809\",\"name\":\"Yang Gao\"}],\"doi\":\"10.1007/978-3-030-64096-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a469b594a1525a2c7d260570eae46d78690b171\",\"title\":\"Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24\\u201327, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/4a469b594a1525a2c7d260570eae46d78690b171\",\"venue\":\"DAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1007/s10458-019-09421-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"390364c3986d05ad71a40a967b9cc12aa30e4305\",\"title\":\"A survey and critique of multiagent deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/390364c3986d05ad71a40a967b9cc12aa30e4305\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49044709\",\"name\":\"M. T. Young\"},{\"authorId\":\"2601893\",\"name\":\"J. Hinkle\"},{\"authorId\":\"46903833\",\"name\":\"R. Kannan\"},{\"authorId\":\"47941567\",\"name\":\"A. Ramanathan\"}],\"doi\":\"10.1016/j.jpdc.2019.07.008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6c0c8e92ffe3deb590cd305710ae68dd9eafd07\",\"title\":\"Distributed Bayesian optimization of deep reinforcement learning algorithms\",\"url\":\"https://www.semanticscholar.org/paper/c6c0c8e92ffe3deb590cd305710ae68dd9eafd07\",\"venue\":\"J. Parallel Distributed Comput.\",\"year\":2020},{\"arxivId\":\"1911.10121\",\"authors\":[{\"authorId\":\"32638491\",\"name\":\"T. Verstraeten\"},{\"authorId\":\"2367164\",\"name\":\"P. Libin\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":\"10.3233/FAIA200266\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"296fb80df1d480e1f6c36824f70fb3122dae0fac\",\"title\":\"Fleet Control using Coregionalized Gaussian Process Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/296fb80df1d480e1f6c36824f70fb3122dae0fac\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":\"1906.09674\",\"authors\":[{\"authorId\":\"3002019\",\"name\":\"Kaixiang Lin\"},{\"authorId\":\"145487992\",\"name\":\"Jiayu Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3ec6c900567a94a9a4525a729a07025c1353f0bc\",\"title\":\"Ranking Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/3ec6c900567a94a9a4525a729a07025c1353f0bc\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2010.11876\",\"authors\":[{\"authorId\":\"145020314\",\"name\":\"T. Xu\"},{\"authorId\":\"25841722\",\"name\":\"Ziniu Li\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83551ac1e6358182a2c0f2ec223fb3c6f736c8e1\",\"title\":\"Error Bounds of Imitating Policies and Environments\",\"url\":\"https://www.semanticscholar.org/paper/83551ac1e6358182a2c0f2ec223fb3c6f736c8e1\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":51609598,\"doi\":\"10.24963/ijcai.2018/820\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"1cb6edbedc4a1ac5c32f61a435a23264e42a9071\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"46507182\",\"name\":\"Hong Wang\"},{\"authorId\":\"143838424\",\"name\":\"H. Qian\"},{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"59c017d98cbac27481504cd38073e894fad7251b\",\"title\":\"Noisy Derivative-Free Optimization With Value Suppression\",\"url\":\"https://www.semanticscholar.org/paper/59c017d98cbac27481504cd38073e894fad7251b\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":\"10.1007/978-3-642-27645-3_10\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"92808faf1694ff4125a923925cd204e8f96a5f55\",\"title\":\"Evolutionary Computation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/92808faf1694ff4125a923925cd204e8f96a5f55\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1696373\",\"name\":\"N. Chentanez\"}],\"doi\":\"10.21236/ada440280\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12d6fde053e2c7174a76fe1bbdb97dd039a3b662\",\"title\":\"Intrinsically Motivated Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/12d6fde053e2c7174a76fe1bbdb97dd039a3b662\",\"venue\":\"NIPS\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"52e2ac397f0c8d5f533959905df899bc328d9f85\",\"title\":\"Reinforcement Learning with Hierarchies of Machines\",\"url\":\"https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85\",\"venue\":\"NIPS\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":\"1710.06537\",\"authors\":[{\"authorId\":\"32200465\",\"name\":\"X. Peng\"},{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":\"10.1109/ICRA.2018.8460528\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0af8cdb71ce9e5bf37ad2a11f05af293cfe62172\",\"title\":\"Sim-to-Real Transfer of Robotic Control with Dynamics Randomization\",\"url\":\"https://www.semanticscholar.org/paper/0af8cdb71ce9e5bf37ad2a11f05af293cfe62172\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"1388843622\",\"name\":\"Greg Wayne\"},{\"authorId\":\"47447264\",\"name\":\"M. Reynolds\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"1841008\",\"name\":\"Ivo Danihelka\"},{\"authorId\":\"1398898827\",\"name\":\"Agnieszka Grabska-Barwinska\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"34505275\",\"name\":\"Tiago Ramalho\"},{\"authorId\":\"70495322\",\"name\":\"J. Agapiou\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"2910877\",\"name\":\"K. Hermann\"},{\"authorId\":\"3185820\",\"name\":\"Yori Zwols\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"144025665\",\"name\":\"A. Cain\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"145593792\",\"name\":\"C. Summerfield\"},{\"authorId\":\"1685771\",\"name\":\"P. Blunsom\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature20101\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"784ee73d5363c711118f784428d1ab89f019daa5\",\"title\":\"Hybrid computing using a neural network with dynamic external memory\",\"url\":\"https://www.semanticscholar.org/paper/784ee73d5363c711118f784428d1ab89f019daa5\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chao Zhang\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\"}],\"doi\":\"10.24963/ijcai.2018/425\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ede887518958ecb65bfd7e91e4bdb23d11febda\",\"title\":\"Learning Environmental Calibration Actions for Policy Self-Evolution\",\"url\":\"https://www.semanticscholar.org/paper/7ede887518958ecb65bfd7e91e4bdb23d11febda\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1604.06778\",\"authors\":[{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"title\":\"Benchmarking Deep Reinforcement Learning for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xue Bin Peng\"},{\"authorId\":null,\"name\":\"Marcin Andrychowicz\"},{\"authorId\":null,\"name\":\"Wojciech Zaremba\"},{\"authorId\":null,\"name\":\"Murtaza Dalal\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", David Silver , et al . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143838424\",\"name\":\"H. Qian\"},{\"authorId\":\"3379756\",\"name\":\"Yi-Qi Hu\"},{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1d4a8d8d32442158b8c0ca09e356c0240b09a21\",\"title\":\"Derivative-Free Optimization of High-Dimensional Non-Convex Functions by Sequential Random Embeddings\",\"url\":\"https://www.semanticscholar.org/paper/d1d4a8d8d32442158b8c0ca09e356c0240b09a21\",\"venue\":\"IJCAI\",\"year\":2016},{\"arxivId\":\"1702.03849\",\"authors\":[{\"authorId\":\"1693598\",\"name\":\"M. Raginsky\"},{\"authorId\":\"1680046\",\"name\":\"A. Rakhlin\"},{\"authorId\":\"1750943\",\"name\":\"Matus Telgarsky\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83dfd3b0e077d816e9f7506dd12552c18bbdb790\",\"title\":\"Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis\",\"url\":\"https://www.semanticscholar.org/paper/83dfd3b0e077d816e9f7506dd12552c18bbdb790\",\"venue\":\"COLT\",\"year\":2017},{\"arxivId\":\"1206.2944\",\"authors\":[{\"authorId\":\"144108062\",\"name\":\"Jasper Snoek\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"1722180\",\"name\":\"R. Adams\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e2089ae76fe914706e6fa90081a79c8fe01611e\",\"title\":\"Practical Bayesian Optimization of Machine Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":\"1707.06203\",\"authors\":[{\"authorId\":\"2026995\",\"name\":\"S\\u00e9bastien Racani\\u00e8re\"},{\"authorId\":\"143947744\",\"name\":\"T. Weber\"},{\"authorId\":\"40634311\",\"name\":\"David P. Reichert\"},{\"authorId\":\"1981334\",\"name\":\"Lars Buesing\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"1748523\",\"name\":\"Danilo Jimenez Rezende\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"47002813\",\"name\":\"Yujia Li\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2019153\",\"name\":\"P. Battaglia\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"600bfe5f0597ebd84898f0c4270ddfb3750594f5\",\"title\":\"Imagination-Augmented Agents for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/600bfe5f0597ebd84898f0c4270ddfb3750594f5\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1512.07679\",\"authors\":[{\"authorId\":\"1387885286\",\"name\":\"Gabriel Dulac-Arnold\"},{\"authorId\":\"143811482\",\"name\":\"R. Evans\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1814162\",\"name\":\"Peter Sunehag\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"50697911\",\"name\":\"J. Hunt\"},{\"authorId\":\"2554720\",\"name\":\"Timothy A. Mann\"},{\"authorId\":\"144588860\",\"name\":\"T. Weber\"},{\"authorId\":\"49491434\",\"name\":\"Thomas Degris\"},{\"authorId\":\"48303781\",\"name\":\"Ben Coppin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b2aff88ee03e82993c066c3e698d51da62d5496\",\"title\":\"Deep Reinforcement Learning in Large Discrete Action Spaces\",\"url\":\"https://www.semanticscholar.org/paper/3b2aff88ee03e82993c066c3e698d51da62d5496\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1703.03400\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518\",\"title\":\"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1805.10000\",\"authors\":[{\"authorId\":\"3431100\",\"name\":\"J. Shi\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"2825549\",\"name\":\"Qing Da\"},{\"authorId\":\"145891473\",\"name\":\"Shi-Yong Chen\"},{\"authorId\":\"144157362\",\"name\":\"Anxiang Zeng\"}],\"doi\":\"10.1609/aaai.v33i01.33014902\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b4a4edebbb25211333098a54c060847e54c1a991\",\"title\":\"Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b4a4edebbb25211333098a54c060847e54c1a991\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1603.06318\",\"authors\":[{\"authorId\":\"2749311\",\"name\":\"Zhiting Hu\"},{\"authorId\":\"2378954\",\"name\":\"Xuezhe Ma\"},{\"authorId\":\"1808004\",\"name\":\"Zhengzhong Liu\"},{\"authorId\":\"144547315\",\"name\":\"E. Hovy\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.18653/v1/P16-1228\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a9de2fc7ae3230652f27fd95e7175da8cf44f075\",\"title\":\"Harnessing Deep Neural Networks with Logic Rules\",\"url\":\"https://www.semanticscholar.org/paper/a9de2fc7ae3230652f27fd95e7175da8cf44f075\",\"venue\":\"ACL\",\"year\":2016},{\"arxivId\":\"1602.02867\",\"authors\":[{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"152198491\",\"name\":\"Sergey Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"8234443\",\"name\":\"G. Thomas\"}],\"doi\":\"10.24963/ijcai.2017/700\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"84680b30a20775e5d319419a7f3f2a93e57c2a61\",\"title\":\"Value Iteration Networks\",\"url\":\"https://www.semanticscholar.org/paper/84680b30a20775e5d319419a7f3f2a93e57c2a61\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145043884\",\"name\":\"H. Beyer\"},{\"authorId\":\"1743344\",\"name\":\"H. Schwefel\"}],\"doi\":\"10.1023/A:1015059928466\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bdbfccbecc0b89e465121297d401e25b47a62e20\",\"title\":\"Evolution strategies \\u2013 A comprehensive introduction\",\"url\":\"https://www.semanticscholar.org/paper/bdbfccbecc0b89e465121297d401e25b47a62e20\",\"venue\":\"Natural Computing\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145891473\",\"name\":\"Shi-Yong Chen\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"2825549\",\"name\":\"Qing Da\"},{\"authorId\":\"145373529\",\"name\":\"J. Tan\"},{\"authorId\":\"51117905\",\"name\":\"Hai-Kuan Huang\"},{\"authorId\":\"13350128\",\"name\":\"Hai-Hong Tang\"}],\"doi\":\"10.1145/3219819.3220122\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc33a717d943aeae8c512069869b197b2d17aadb\",\"title\":\"Stabilizing Reinforcement Learning in Dynamic Environment with Application to Online Recommendation\",\"url\":\"https://www.semanticscholar.org/paper/dc33a717d943aeae8c512069869b197b2d17aadb\",\"venue\":\"KDD\",\"year\":2018},{\"arxivId\":\"1703.01161\",\"authors\":[{\"authorId\":\"9948791\",\"name\":\"A. S. Vezhnevets\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"049c6e5736313374c6e594c34b9be89a3a09dced\",\"title\":\"FeUdal Networks for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/049c6e5736313374c6e594c34b9be89a3a09dced\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1704.03012\",\"authors\":[{\"authorId\":\"10104623\",\"name\":\"Carlos Florensa\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3deecaee4ec1a37de3cb10420eaabff067669e17\",\"title\":\"Stochastic Neural Networks for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3deecaee4ec1a37de3cb10420eaabff067669e17\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48623509\",\"name\":\"Y. Yu\"},{\"authorId\":\"145891473\",\"name\":\"Shi-Yong Chen\"},{\"authorId\":\"2825549\",\"name\":\"Qing Da\"},{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\"}],\"doi\":\"10.1109/TNNLS.2018.2803729\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d45811bec4a01aa86d31c6978e3feea5c35b0d23\",\"title\":\"Reusable Reinforcement Learning via Shallow Trails\",\"url\":\"https://www.semanticscholar.org/paper/d45811bec4a01aa86d31c6978e3feea5c35b0d23\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2018},{\"arxivId\":\"1803.00710\",\"authors\":[{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2825549\",\"name\":\"Qing Da\"},{\"authorId\":\"144157362\",\"name\":\"Anxiang Zeng\"},{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"},{\"authorId\":\"50125871\",\"name\":\"Yinghui Xu\"}],\"doi\":\"10.1145/3219819.3219846\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7da41fd9949af81b84fb983524f850f7d47918fa\",\"title\":\"Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application\",\"url\":\"https://www.semanticscholar.org/paper/7da41fd9949af81b84fb983524f850f7d47918fa\",\"venue\":\"KDD\",\"year\":2018},{\"arxivId\":\"1802.09081\",\"authors\":[{\"authorId\":\"144401061\",\"name\":\"Vitchyr H. Pong\"},{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"35904540\",\"name\":\"Murtaza Dalal\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"852c931b5d9f9d4256befd725ee4185945c4964c\",\"title\":\"Temporal Difference Models: Model-Free Deep RL for Model-Based Control\",\"url\":\"https://www.semanticscholar.org/paper/852c931b5d9f9d4256befd725ee4185945c4964c\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1711.09846\",\"authors\":[{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"2795508\",\"name\":\"Valentin Dalibard\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"143653164\",\"name\":\"Ali Razavi\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"143897708\",\"name\":\"T. Green\"},{\"authorId\":\"2768462\",\"name\":\"Iain Dunning\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"143939165\",\"name\":\"C. Fernando\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"af10f3c1c0859aa620623f760c8a29e78f177f7f\",\"title\":\"Population Based Training of Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/af10f3c1c0859aa620623f760c8a29e78f177f7f\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3379756\",\"name\":\"Yi-Qi Hu\"},{\"authorId\":\"143838424\",\"name\":\"H. Qian\"},{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e8fce9c402e98dc10beea9c9a955fd62d5aa2b9\",\"title\":\"Sequential Classification-Based Optimization for Direct Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/4e8fce9c402e98dc10beea9c9a955fd62d5aa2b9\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pierre-Luc Bacon\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The optioncritic architecture\",\"url\":\"\",\"venue\":\"In AAAI,\",\"year\":2017},{\"arxivId\":\"1712.06567\",\"authors\":[{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"8309711\",\"name\":\"V. Madhavan\"},{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba3ace39f1f1afb6651ef4c0e4b8317fd9d48fcf\",\"title\":\"Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ba3ace39f1f1afb6651ef4c0e4b8317fd9d48fcf\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1711.04574\",\"authors\":[{\"authorId\":\"143811482\",\"name\":\"R. Evans\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"}],\"doi\":\"10.1613/JAIR.5714\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4df7bbe3ca7806f39a490c99f17867a0ac299bc3\",\"title\":\"Learning Explanatory Rules from Noisy Data\",\"url\":\"https://www.semanticscholar.org/paper/4df7bbe3ca7806f39a490c99f17867a0ac299bc3\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2018},{\"arxivId\":\"1706.10295\",\"authors\":[{\"authorId\":\"39067762\",\"name\":\"Meire Fortunato\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"10698483\",\"name\":\"Jacob Menick\"},{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"123588356\",\"name\":\"Vlad Mnih\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"},{\"authorId\":\"79608109\",\"name\":\"O. Pietquin\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4cd76f8353f0c4852cc432fc0e7a5f2b91ae6ce5\",\"title\":\"Noisy Networks for Exploration\",\"url\":\"https://www.semanticscholar.org/paper/4cd76f8353f0c4852cc432fc0e7a5f2b91ae6ce5\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"1729320\",\"name\":\"A. Bonarini\"}],\"doi\":\"10.1145/1390156.1390225\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"title\":\"Transfer of samples in batch reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":\"1802.01173\",\"authors\":[{\"authorId\":\"2204237\",\"name\":\"Wang-Zhou Dai\"},{\"authorId\":\"50536989\",\"name\":\"Qiu-Ling Xu\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fcbcc3d96fb2c253ac51d5af9640069b0f6639ec\",\"title\":\"Tunneling Neural Perception and Logic Reasoning through Abductive Learning\",\"url\":\"https://www.semanticscholar.org/paper/fcbcc3d96fb2c253ac51d5af9640069b0f6639ec\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1711.11289\",\"authors\":[{\"authorId\":\"34594615\",\"name\":\"Himanshu Sahni\"},{\"authorId\":\"39703750\",\"name\":\"S. Kumar\"},{\"authorId\":\"50342768\",\"name\":\"Farhan Tejani\"},{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"37b5980cf1a202fbec2fec28b831b0f90b8d217a\",\"title\":\"Learning to Compose Skills\",\"url\":\"https://www.semanticscholar.org/paper/37b5980cf1a202fbec2fec28b831b0f90b8d217a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1706.01905\",\"authors\":[{\"authorId\":\"3407285\",\"name\":\"Matthias Plappert\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"2700360\",\"name\":\"S. Sidor\"},{\"authorId\":\"2896187\",\"name\":\"Richard Y. Chen\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1722677\",\"name\":\"T. Asfour\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"142497432fe179ddb6ffe600c64a837ec6179550\",\"title\":\"Parameter Space Noise for Exploration\",\"url\":\"https://www.semanticscholar.org/paper/142497432fe179ddb6ffe600c64a837ec6179550\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1705.05363\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"33932184\",\"name\":\"Pulkit Agrawal\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPRW.2017.70\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"225ab689f41cef1dc18237ef5dab059a49950abf\",\"title\":\"Curiosity-Driven Exploration by Self-Supervised Prediction\",\"url\":\"https://www.semanticscholar.org/paper/225ab689f41cef1dc18237ef5dab059a49950abf\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1815875\",\"name\":\"E. Ferrante\"},{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":\"10.1145/1402821.1402864\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"403d32b1a44d47e0e6a63a04396541423861ca2a\",\"title\":\"Transfer of task representation in reinforcement learning using policy-based proto-value functions\",\"url\":\"https://www.semanticscholar.org/paper/403d32b1a44d47e0e6a63a04396541423861ca2a\",\"venue\":\"AAMAS\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1790646\",\"name\":\"P. Dayan\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1678bd32846b1aded5b1e80a617170812e80f562\",\"title\":\"Feudal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1678bd32846b1aded5b1e80a617170812e80f562\",\"venue\":\"NIPS\",\"year\":1992}],\"title\":\"Towards Sample Efficient Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Artificial intelligence\",\"topicId\":\"8286\",\"url\":\"https://www.semanticscholar.org/topic/8286\"},{\"topic\":\"Pixel\",\"topicId\":\"4254\",\"url\":\"https://www.semanticscholar.org/topic/4254\"},{\"topic\":\"Intelligent agent\",\"topicId\":\"18636\",\"url\":\"https://www.semanticscholar.org/topic/18636\"},{\"topic\":\"Bloom (shader effect)\",\"topicId\":\"72556\",\"url\":\"https://www.semanticscholar.org/topic/72556\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"YANG\",\"topicId\":\"811002\",\"url\":\"https://www.semanticscholar.org/topic/811002\"}],\"url\":\"https://www.semanticscholar.org/paper/1cb6edbedc4a1ac5c32f61a435a23264e42a9071\",\"venue\":\"IJCAI\",\"year\":2018}\n"