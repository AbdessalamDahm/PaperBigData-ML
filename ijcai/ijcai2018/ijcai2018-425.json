"{\"abstract\":\"Reinforcement learning in physical world is often expensive. Simulators are commonly employed to train policies. Due to the simulation error, trainedin-simulator policies are hard to be directly deployed in physical world. Therefore, how to efficiently reuse these policies to the real environment is a key issue. To address this issue, this paper presents a policy self-evolution process: in the target environment, the agent firstly executes a few calibration actions to perceive the environment, and then reuses the previous policies according to the observation of the environment. In this way, the mission of policy learning in the target environment is reduced to the task of environment identification through executing the calibration actions, which needs much less samples than learning a policy from scratch. We propose the POSEC (POlicy Self-Evolution by Calibration) approach, which learns the most informative calibration actions for policy self-evolution. Taking three robotic arm controlling tasks as the test beds, we show that the proposed method can learn a fine policy for a new arm with only a few (e.g. five) samples of the target environment.\",\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chao Zhang\",\"url\":null},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\",\"url\":\"https://www.semanticscholar.org/author/144705629\"},{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\",\"url\":\"https://www.semanticscholar.org/author/145624000\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"152786469\",\"name\":\"Maryam Zar\\u00e9\"},{\"authorId\":\"13118145\",\"name\":\"A. Ayub\"},{\"authorId\":\"153152072\",\"name\":\"Aishan Liu\"},{\"authorId\":\"1396632591\",\"name\":\"Sweekar Sudhakara\"},{\"authorId\":\"40290009\",\"name\":\"A. Wagner\"},{\"authorId\":\"1703046\",\"name\":\"Rebecca J. Passonneau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"87e413916b3a6c7aba244395cef5b870c8648b53\",\"title\":\"Dialogue Policies for Learning Board Games through Multimodal Communication\",\"url\":\"https://www.semanticscholar.org/paper/87e413916b3a6c7aba244395cef5b870c8648b53\",\"venue\":\"SIGdial\",\"year\":2020},{\"arxivId\":\"1810.05751\",\"authors\":[{\"authorId\":\"70461341\",\"name\":\"Wenhao Yu\"},{\"authorId\":\"1688533\",\"name\":\"C. Liu\"},{\"authorId\":\"1713189\",\"name\":\"Greg Turk\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0e31f993257d0b25d7c18eb75a4ba39f7563bd81\",\"title\":\"Policy Transfer with Strategy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/0e31f993257d0b25d7c18eb75a4ba39f7563bd81\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"}],\"doi\":\"10.24963/ijcai.2018/820\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cb6edbedc4a1ac5c32f61a435a23264e42a9071\",\"title\":\"Towards Sample Efficient Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1cb6edbedc4a1ac5c32f61a435a23264e42a9071\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1809.03548\",\"authors\":[{\"authorId\":\"51261829\",\"name\":\"Isac Arnekvist\"},{\"authorId\":\"1731490\",\"name\":\"D. Kragic\"},{\"authorId\":\"3128110\",\"name\":\"Johannes A. Stork\"}],\"doi\":\"10.1109/ICRA.2019.8793556\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0aecfe5ffbba42d26b5483078c25688da90f8fca\",\"title\":\"VPE: Variational Policy Embedding for Transfer Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0aecfe5ffbba42d26b5483078c25688da90f8fca\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":\"1808.05331\",\"authors\":[{\"authorId\":\"34469457\",\"name\":\"R. Liu\"},{\"authorId\":\"18119746\",\"name\":\"Shichao Cheng\"},{\"authorId\":null,\"name\":\"Yi He\"},{\"authorId\":\"144309189\",\"name\":\"Xin Fan\"},{\"authorId\":\"33383055\",\"name\":\"Zhouchen Lin\"},{\"authorId\":\"145865683\",\"name\":\"Zhongxuan Luo\"}],\"doi\":\"10.1109/TPAMI.2019.2920591\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6b3a40e477c230b5694d33f5131407374f99caea\",\"title\":\"On the Convergence of Learning-Based Iterative Methods for Nonconvex Inverse Problems\",\"url\":\"https://www.semanticscholar.org/paper/6b3a40e477c230b5694d33f5131407374f99caea\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2020},{\"arxivId\":\"1905.13719\",\"authors\":[{\"authorId\":\"22676240\",\"name\":\"Wen-Ji Zhou\"},{\"authorId\":\"47112122\",\"name\":\"Yang Yu\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"144155590\",\"name\":\"Kai Guan\"},{\"authorId\":\"80892810\",\"name\":\"Tangjie Lv\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\"}],\"doi\":\"10.24963/ijcai.2019/618\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"30e7f96c987c0f8a7cc64ce8a0599a7d31870c05\",\"title\":\"Reinforcement Learning Experience Reuse with Policy Residual Representation\",\"url\":\"https://www.semanticscholar.org/paper/30e7f96c987c0f8a7cc64ce8a0599a7d31870c05\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2010.11876\",\"authors\":[{\"authorId\":\"145020314\",\"name\":\"T. Xu\"},{\"authorId\":\"25841722\",\"name\":\"Ziniu Li\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"83551ac1e6358182a2c0f2ec223fb3c6f736c8e1\",\"title\":\"Error Bounds of Imitating Policies and Environments\",\"url\":\"https://www.semanticscholar.org/paper/83551ac1e6358182a2c0f2ec223fb3c6f736c8e1\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":29153111,\"doi\":\"10.24963/ijcai.2018/425\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"7ede887518958ecb65bfd7e91e4bdb23d11febda\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145624000\",\"name\":\"Z. Zhou\"}],\"doi\":\"10.1007/s11704-016-6906-3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a5ae9087733111e23f89b744c3fae5ec1be0ce75\",\"title\":\"Learnware: on the future of machine learning\",\"url\":\"https://www.semanticscholar.org/paper/a5ae9087733111e23f89b744c3fae5ec1be0ce75\",\"venue\":\"Frontiers of Computer Science\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3379756\",\"name\":\"Yi-Qi Hu\"},{\"authorId\":\"143838424\",\"name\":\"H. Qian\"},{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e8fce9c402e98dc10beea9c9a955fd62d5aa2b9\",\"title\":\"Sequential Classification-Based Optimization for Direct Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/4e8fce9c402e98dc10beea9c9a955fd62d5aa2b9\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1683199\",\"name\":\"A. Freitas\"}],\"doi\":\"10.1007/978-0-387-30164-8_272\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8cde5e660ed0ad83cbe8af274ee39dc97e2242d2\",\"title\":\"Evolutionary Computation\",\"url\":\"https://www.semanticscholar.org/paper/8cde5e660ed0ad83cbe8af274ee39dc97e2242d2\",\"venue\":\"Encyclopedia of Machine Learning\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"1729320\",\"name\":\"A. Bonarini\"}],\"doi\":\"10.1145/1390156.1390225\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"title\":\"Transfer of samples in batch reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48623698\",\"name\":\"Yang Yu\"},{\"authorId\":\"143838424\",\"name\":\"H. Qian\"},{\"authorId\":\"3379756\",\"name\":\"Yi-Qi Hu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f9cda74ec68c52d68c6bf21399d6c0d6f14828e5\",\"title\":\"Derivative-Free Optimization via Classification\",\"url\":\"https://www.semanticscholar.org/paper/f9cda74ec68c52d68c6bf21399d6c0d6f14828e5\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R\\u00e9mi Munos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Joel Veness , Marc G . Bellemare , Alex Graves , Martin Riedmiller , Andreas K . Fidje - land , and Georg Ostrovski . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1710.06537\",\"authors\":[{\"authorId\":\"32200465\",\"name\":\"X. Peng\"},{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":\"10.1109/ICRA.2018.8460528\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"0af8cdb71ce9e5bf37ad2a11f05af293cfe62172\",\"title\":\"Sim-to-Real Transfer of Robotic Control with Dynamics Randomization\",\"url\":\"https://www.semanticscholar.org/paper/0af8cdb71ce9e5bf37ad2a11f05af293cfe62172\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2307752\",\"name\":\"B. Ratitch\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":\"10.1007/978-3-540-30115-8_33\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb9c1662c351be67f4072163ccfe7002f84fd2e2\",\"title\":\"Sparse Distributed Memories for On-Line Value-Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/cb9c1662c351be67f4072163ccfe7002f84fd2e2\",\"venue\":\"ECML\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marcello Restelli Alessandro Lazaric\"},{\"authorId\":null,\"name\":\"Andrea Bonarini\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Vecchi . Optimization by simulated anneal\",\"url\":\"\",\"venue\":\"Science\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":\"10.1561/2200000038\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"541571ba3ca170e5429454d9ad190a867263effd\",\"title\":\"From Bandits to Monte-Carlo Tree Search: The Optimistic Principle Applied to Optimization and Planning\",\"url\":\"https://www.semanticscholar.org/paper/541571ba3ca170e5429454d9ad190a867263effd\",\"venue\":\"Found. Trends Mach. Learn.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Restelli Lazaric\"},{\"authorId\":null,\"name\":\"A. Bonarini\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Gelatt , and M . P . Vecchi . Optimization by simulated annealing\",\"url\":\"\",\"venue\":\"Science\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2261881\",\"name\":\"M. Deisenroth\"},{\"authorId\":\"3472959\",\"name\":\"C. Rasmussen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60b7d47758a71978e74edff6dd8dea4d9c791d7a\",\"title\":\"PILCO: A Model-Based and Data-Efficient Approach to Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/60b7d47758a71978e74edff6dd8dea4d9c791d7a\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"},{\"authorId\":\"1807998\",\"name\":\"I. Tsang\"},{\"authorId\":\"145193332\",\"name\":\"James T. Kwok\"},{\"authorId\":\"152290618\",\"name\":\"Qiang Yang\"}],\"doi\":\"10.1109/TNN.2010.2091281\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1dae4d61cd74cc919ecc638bde6b7125728ea97b\",\"title\":\"Domain Adaptation via Transfer Component Analysis\",\"url\":\"https://www.semanticscholar.org/paper/1dae4d61cd74cc919ecc638bde6b7125728ea97b\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nicholas K. Jong\"},{\"authorId\":null,\"name\":\"Peter Stone. Model-based exploration in continuous state spaces\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 7th International Symposium on Abstraction\",\"url\":\"\",\"venue\":\"Reformulation, and Approximation, pages 258\\u2013 272, Berlin,Germany,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145847131\",\"name\":\"S. Kirkpatrick\"},{\"authorId\":\"5882723\",\"name\":\"C. D. Gelatt\"},{\"authorId\":\"1855550\",\"name\":\"M. P. Vecchi\"}],\"doi\":\"10.1126/science.220.4598.671\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd5061631a4d11fa394f4421700ebf7e78dcbc59\",\"title\":\"Optimization by Simulated Annealing\",\"url\":\"https://www.semanticscholar.org/paper/dd5061631a4d11fa394f4421700ebf7e78dcbc59\",\"venue\":\"Science\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4682880\",\"name\":\"M. F. Balcan\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a0442c88be8f16e180d8e365235950d1f7dff436\",\"title\":\"Proceedings of the 33rd International Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/a0442c88be8f16e180d8e365235950d1f7dff436\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143966629\",\"name\":\"M. Crosby\"},{\"authorId\":\"35407423\",\"name\":\"Ronald P. A. Petrick\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b01fa6d46999789a1549c8fef1f004a2a9f6451a\",\"title\":\"Association for the Advancement of Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b01fa6d46999789a1549c8fef1f004a2a9f6451a\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1703.03400\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518\",\"title\":\"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\",\"url\":\"https://www.semanticscholar.org/paper/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398777358\",\"name\":\"J. Hern\\u00e1ndez-Orallo\"},{\"authorId\":\"47840704\",\"name\":\"Peter A. Flach\"},{\"authorId\":\"144134367\",\"name\":\"C. Ferri\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"488a3f9092093d4f52b25df4342ec79f0327a325\",\"title\":\"Proceedings of the 28th International Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/488a3f9092093d4f52b25df4342ec79f0327a325\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"100655694\",\"name\":\"A. Moore\"}],\"doi\":\"10.1145/1143844\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"title\":\"Proceedings of the 23rd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"venue\":\"ICML 2008\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745716\",\"name\":\"Nicholas K. Jong\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1007/978-3-540-73580-9_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6cc0472053f0ec7dc2fa16af7b0b22d6457be014\",\"title\":\"Model-Based Exploration in Continuous State Spaces\",\"url\":\"https://www.semanticscholar.org/paper/6cc0472053f0ec7dc2fa16af7b0b22d6457be014\",\"venue\":\"SARA\",\"year\":2007},{\"arxivId\":\"1604.06778\",\"authors\":[{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"title\":\"Benchmarking Deep Reinforcement Learning for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1740042\",\"name\":\"L. D. Raedt\"},{\"authorId\":\"47840704\",\"name\":\"Peter A. Flach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2fe57172dbf564420fd1e898461047c542b51acf\",\"title\":\"Proceedings of the 12th European Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/2fe57172dbf564420fd1e898461047c542b51acf\",\"venue\":\"\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2067577\",\"name\":\"B. Shahriari\"},{\"authorId\":\"1754860\",\"name\":\"Kevin Swersky\"},{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1722180\",\"name\":\"R. Adams\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":\"10.1109/JPROC.2015.2494218\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a2586e0a5f8bb4e35aa0763a6b8bca428af6bd2\",\"title\":\"Taking the Human Out of the Loop: A Review of Bayesian Optimization\",\"url\":\"https://www.semanticscholar.org/paper/0a2586e0a5f8bb4e35aa0763a6b8bca428af6bd2\",\"venue\":\"Proceedings of the IEEE\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14212205\",\"name\":\"D. Liu\"},{\"authorId\":\"1398217037\",\"name\":\"M. Abu-Khalaf\"},{\"authorId\":\"1683616\",\"name\":\"A. M. Alimi\"},{\"authorId\":\"144826602\",\"name\":\"C. Anderson\"},{\"authorId\":\"71018001\",\"name\":\"A. Fausto\"},{\"authorId\":\"3836512\",\"name\":\"A. T. Azar\"},{\"authorId\":\"115675135\",\"name\":\"B. Baesens\"},{\"authorId\":\"1764788\",\"name\":\"G. Battistelli\"},{\"authorId\":\"1398734187\",\"name\":\"E. Bayro-Corrochano\"},{\"authorId\":\"1929254\",\"name\":\"S. Boht\\u00e9\"},{\"authorId\":\"2613481\",\"name\":\"P. Bouboulis\"},{\"authorId\":\"1409071182\",\"name\":\"Padua Braga\"},{\"authorId\":\"2123613\",\"name\":\"C. Cervellera\"},{\"authorId\":\"1703849\",\"name\":\"B. Chen\"},{\"authorId\":\"1781965\",\"name\":\"S. Cruces\"},{\"authorId\":\"39293304\",\"name\":\"Qiong-Hai Dai\"},{\"authorId\":\"7416642\",\"name\":\"S. Damelin\"},{\"authorId\":\"103338302\",\"name\":\"Daoyi Dong\"},{\"authorId\":\"1384339708\",\"name\":\"E. El-Alfy\"},{\"authorId\":\"48225830\",\"name\":\"K. Fahd\"},{\"authorId\":\"145319527\",\"name\":\"S. Arabia\"},{\"authorId\":\"143845181\",\"name\":\"D. Elizondo\"},{\"authorId\":\"3138895\",\"name\":\"M. Filippone\"},{\"authorId\":\"145692771\",\"name\":\"Y. Fu\"},{\"authorId\":\"2264182\",\"name\":\"Giorgio Gnecco\"},{\"authorId\":\"2198278\",\"name\":\"Haibo He\"},{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"1815835\",\"name\":\"P. Kidmose\"},{\"authorId\":\"1779619\",\"name\":\"R. M. Kil\"},{\"authorId\":\"2073142\",\"name\":\"R. Legenstein\"},{\"authorId\":\"47892555\",\"name\":\"Hongyi Li\"},{\"authorId\":\"46946977\",\"name\":\"Zhijun Li\"},{\"authorId\":\"47663266\",\"name\":\"Jinling Liang\"},{\"authorId\":\"150152476\",\"name\":\"Juwei Lu\"},{\"authorId\":\"1723706\",\"name\":\"Wenlian Lu\"},{\"authorId\":\"9074167\",\"name\":\"Jiancheng Lv\"},{\"authorId\":\"144893748\",\"name\":\"A. Madureira\"},{\"authorId\":\"152748915\",\"name\":\"M. Panella\"},{\"authorId\":\"1780024\",\"name\":\"R. Polikar\"},{\"authorId\":\"2353770\",\"name\":\"D. Prokhorov\"},{\"authorId\":\"1744102\",\"name\":\"M. Roveri\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"},{\"authorId\":\"46581758\",\"name\":\"Madhusudana Shashanka\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"1721547\",\"name\":\"I. \\u0160krjanc\"},{\"authorId\":\"40560020\",\"name\":\"Y. Song\"},{\"authorId\":\"1771979\",\"name\":\"S. Squartini\"},{\"authorId\":\"1776000\",\"name\":\"Changyin Sun\"},{\"authorId\":\"144732378\",\"name\":\"T. Tanaka\"},{\"authorId\":\"3134548\",\"name\":\"H. Tang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"},{\"authorId\":\"4023505\",\"name\":\"P. Ti\\u00f1o\"},{\"authorId\":\"2706487\",\"name\":\"D. Wang\"},{\"authorId\":\"48325417\",\"name\":\"M. J. Watts\"},{\"authorId\":\"1717781\",\"name\":\"Qinglai Wei\"},{\"authorId\":\"50313481\",\"name\":\"Stefan Wermter\"},{\"authorId\":\"32239759\",\"name\":\"M. Wiering\"},{\"authorId\":\"33455548\",\"name\":\"Jonathan Wu\"},{\"authorId\":\"145382103\",\"name\":\"S. Xie\"},{\"authorId\":\"144016434\",\"name\":\"D. Xu\"}],\"doi\":\"10.1109/tnnls.2013.2286276\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"41eb6f471abaa7d111baefda111e488f8ffb39a4\",\"title\":\"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\",\"url\":\"https://www.semanticscholar.org/paper/41eb6f471abaa7d111baefda111e488f8ffb39a4\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S. Sutton\"},{\"authorId\":null,\"name\":\"Andrew G. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Strehl , LiHong Li , Eric Wiewiora , John Langford , and Michael L . Littman . PAC model - free reinforcement learning\",\"url\":\"\",\"venue\":\"Proceedings of the 23 rd International Conference on Machine Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5952316\",\"name\":\"L. Christophorou\"}],\"doi\":\"10.1007/978-3-319-90713-0_3\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bccfe3fc62312408a4b27cb46b15a19e0176f995\",\"title\":\"Science\",\"url\":\"https://www.semanticscholar.org/paper/bccfe3fc62312408a4b27cb46b15a19e0176f995\",\"venue\":\"Emerging Dynamics: Science, Energy, Society and Values\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"4454080\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1362879e77efef96ab552f5cb1198c2a67204d6\",\"title\":\"Introduction to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b1362879e77efef96ab552f5cb1198c2a67204d6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"A. Andrei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Prasad Tadepalli , and Alan Fern . Transfer in variable - reward hierarchical reinforcement learning\",\"url\":\"\",\"venue\":\"Machine Learning\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2725621\",\"name\":\"N. Mehta\"},{\"authorId\":\"145986014\",\"name\":\"Sriraam Natarajan\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"}],\"doi\":\"10.1007/s10994-008-5061-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"36fcc32aac99de1f67f83da33f134721258ea943\",\"title\":\"Transfer in variable-reward hierarchical reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/36fcc32aac99de1f67f83da33f134721258ea943\",\"venue\":\"Machine Learning\",\"year\":2008},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144539890\",\"name\":\"N. Hansen\"},{\"authorId\":\"2069398\",\"name\":\"A. Ostermeier\"}],\"doi\":\"10.1162/106365601750190398\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f1bdebedf07fd444628c955568f0d51e1a26835e\",\"title\":\"Completely Derandomized Self-Adaptation in Evolution Strategies\",\"url\":\"https://www.semanticscholar.org/paper/f1bdebedf07fd444628c955568f0d51e1a26835e\",\"venue\":\"Evolutionary Computation\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"1766844\",\"name\":\"Eric Wiewiora\"},{\"authorId\":\"152677062\",\"name\":\"J. Langford\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1145/1143844.1143955\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"187f3f984e6f375178f41827ab90c4e748773fa7\",\"title\":\"PAC model-free reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/187f3f984e6f375178f41827ab90c4e748773fa7\",\"venue\":\"ICML '06\",\"year\":2006}],\"title\":\"Learning Environmental Calibration Actions for Policy Self-Evolution\",\"topics\":[{\"topic\":\"Robotic arm\",\"topicId\":\"41235\",\"url\":\"https://www.semanticscholar.org/topic/41235\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Robot\",\"topicId\":\"6657\",\"url\":\"https://www.semanticscholar.org/topic/6657\"},{\"topic\":\"Simulation\",\"topicId\":\"194\",\"url\":\"https://www.semanticscholar.org/topic/194\"},{\"topic\":\"Information\",\"topicId\":\"185548\",\"url\":\"https://www.semanticscholar.org/topic/185548\"},{\"topic\":\"Long short-term memory\",\"topicId\":\"117199\",\"url\":\"https://www.semanticscholar.org/topic/117199\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Turing test\",\"topicId\":\"117343\",\"url\":\"https://www.semanticscholar.org/topic/117343\"},{\"topic\":\"Evolution\",\"topicId\":\"91583\",\"url\":\"https://www.semanticscholar.org/topic/91583\"}],\"url\":\"https://www.semanticscholar.org/paper/7ede887518958ecb65bfd7e91e4bdb23d11febda\",\"venue\":\"IJCAI\",\"year\":2018}\n"