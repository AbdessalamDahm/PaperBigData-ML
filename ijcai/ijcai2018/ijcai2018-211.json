"{\"abstract\":\"Curriculum learning is often introduced as a leverage to improve the agent training for complex tasks, where the goal is to generate a sequence of easier subtasks for an agent to train on, such that final performance or learning speed is improved. However, conventional curriculum is mainly designed for one agent with fixed action space and sequential simple-to-hard training manner. Instead, we present a novel curriculum learning strategy by introducing the concept of master-slave agents and enabling flexible action setting for agent training. Multiple agents, referred as master agent for the target task and slave agents for the subtasks, are trained concurrently within different action spaces by sharing a perception network with an asynchronous strategy. Extensive evaluation on the VizDoom platform demonstrates the joint learning of master agent and slave agents mutually benefit each other. Significant improvement is obtained over A3C in terms of learning speed and performance.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"12116553\",\"name\":\"Y. Wu\",\"url\":\"https://www.semanticscholar.org/author/12116553\"},{\"authorId\":null,\"name\":\"Wei Zhang\",\"url\":null},{\"authorId\":\"145592727\",\"name\":\"K. Song\",\"url\":\"https://www.semanticscholar.org/author/145592727\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"48004001\",\"name\":\"Fengming Li\"},{\"authorId\":\"145774776\",\"name\":\"Qi Jiang\"},{\"authorId\":\"5174051\",\"name\":\"Sisi Zhang\"},{\"authorId\":\"47447535\",\"name\":\"M. Wei\"},{\"authorId\":\"152810591\",\"name\":\"Rui Song\"}],\"doi\":\"10.1016/J.NEUCOM.2019.01.087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"16283934c4902676a95e305b8cb8ec086c8d8e98\",\"title\":\"Robot skill acquisition in assembly process using deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/16283934c4902676a95e305b8cb8ec086c8d8e98\",\"venue\":\"Neurocomputing\",\"year\":2019},{\"arxivId\":\"2012.03204\",\"authors\":[{\"authorId\":\"50982491\",\"name\":\"Hangtian Jia\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"81185915\",\"name\":\"Chunxu Ren\"},{\"authorId\":\"80892810\",\"name\":\"Tangjie Lv\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8922d5cd990903e594bcd4de96c6251361919c70\",\"title\":\"Fever Basketball: A Complex, Flexible, and Asynchronized Sports Game Environment for Multi-agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8922d5cd990903e594bcd4de96c6251361919c70\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.06178\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"137124626\",\"name\":\"Christiano Coletto Christakou\"},{\"authorId\":\"138689850\",\"name\":\"R. Gutierrez\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":\"10.24963/ijcai.2019/320\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1988a0134839f36449bbacc4589cc8b37e46a965\",\"title\":\"Curriculum Learning for Cumulative Return Maximization\",\"url\":\"https://www.semanticscholar.org/paper/1988a0134839f36449bbacc4589cc8b37e46a965\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"137124626\",\"name\":\"Christiano Coletto Christakou\"},{\"authorId\":\"138689850\",\"name\":\"Ricardo Luna Gutierrez\"},{\"authorId\":\"119248365\",\"name\":\"Matt\\u00e9o\"},{\"authorId\":\"1396761376\",\"name\":\"Leonetti\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"820161394caffa7d72eab652033512f30ea2e6d4\",\"title\":\"Learning for Cumulative Return Maximization\",\"url\":\"https://www.semanticscholar.org/paper/820161394caffa7d72eab652033512f30ea2e6d4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51245276\",\"name\":\"Yuechen Wu\"},{\"authorId\":\"152829349\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"145564459\",\"name\":\"X. Xie\"},{\"authorId\":\"1485043101\",\"name\":\"Bing Yu\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":\"143828252\",\"name\":\"L. Ma\"}],\"doi\":\"10.1109/ICSME46990.2020.00074\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bebb3899d454ba43069401304b4cfd2eed9a3d2a\",\"title\":\"Regression Testing of Massively Multiplayer Online Role-Playing Games\",\"url\":\"https://www.semanticscholar.org/paper/bebb3899d454ba43069401304b4cfd2eed9a3d2a\",\"venue\":\"2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48017303\",\"name\":\"H. Wang\"},{\"authorId\":null,\"name\":\"Wenjing Yang\"},{\"authorId\":\"3441469\",\"name\":\"Wanrong Huang\"},{\"authorId\":\"49290329\",\"name\":\"Zhipeng Lin\"},{\"authorId\":\"3261878\",\"name\":\"Yuhua Tang\"}],\"doi\":\"10.1007/978-3-030-04239-4_27\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"317953dfe87a892c7275c8d1b816d3bae419ecd0\",\"title\":\"Multi-feature Fusion for Deep Reinforcement Learning: Sequential Control of Mobile Robots\",\"url\":\"https://www.semanticscholar.org/paper/317953dfe87a892c7275c8d1b816d3bae419ecd0\",\"venue\":\"ICONIP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49039623\",\"name\":\"W. Zhang\"},{\"authorId\":\"145592727\",\"name\":\"K. Song\"},{\"authorId\":\"2924438\",\"name\":\"Xuewen Rong\"},{\"authorId\":\"47002225\",\"name\":\"Yibin Li\"}],\"doi\":\"10.1109/TASE.2018.2877499\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d74c483561c985226d5c4ad498631f976ef681f\",\"title\":\"Coarse-to-Fine UAV Target Tracking With Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7d74c483561c985226d5c4ad498631f976ef681f\",\"venue\":\"IEEE Transactions on Automation Science and Engineering\",\"year\":2019},{\"arxivId\":\"1908.06976\",\"authors\":[{\"authorId\":\"1387894646\",\"name\":\"A. Aubret\"},{\"authorId\":\"153442621\",\"name\":\"L. Matignon\"},{\"authorId\":\"1730965\",\"name\":\"S. Hassas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"895735cace0de940aa647dbafc046b7f30316fe5\",\"title\":\"A survey on intrinsic motivation in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/895735cace0de940aa647dbafc046b7f30316fe5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1901.11478\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":\"10.1109/DEVLRN.2019.8850690\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1e658c39cf8695c3363457903f5bc898231611dd\",\"title\":\"An Optimization Framework for Task Sequencing in Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e658c39cf8695c3363457903f5bc898231611dd\",\"venue\":\"2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)\",\"year\":2019}],\"corpusId\":51608980,\"doi\":\"10.24963/ijcai.2018/211\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"title\":\"Source Task Creation for Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":\"1611.05397\",\"authors\":[{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d7bd6e3addd8bc8e2e154048300eea15f030ed33\",\"title\":\"Reinforcement Learning with Unsupervised Auxiliary Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d7bd6e3addd8bc8e2e154048300eea15f030ed33\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709638\",\"name\":\"L. Busoniu\"},{\"authorId\":\"1705222\",\"name\":\"Robert Babu\\u0161ka\"},{\"authorId\":\"1724741\",\"name\":\"B. D. Schutter\"}],\"doi\":\"10.1109/ICARCV.2006.345353\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1624d7fab22dc91aefd2ae4e2575a4386fbc3c09\",\"title\":\"Multi-Agent Reinforcement Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/1624d7fab22dc91aefd2ae4e2575a4386fbc3c09\",\"venue\":\"2006 9th International Conference on Control, Automation, Robotics and Vision\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sanmit Narvekar\"},{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Peter Stone. Autonomous task sequencing for customized c learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 2536\\u20132542,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144056302\",\"name\":\"A. Wilson\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"},{\"authorId\":\"145527877\",\"name\":\"Soumya Ray\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":\"10.1145/1273496.1273624\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"title\":\"Multi-task reinforcement learning: a hierarchical Bayesian approach\",\"url\":\"https://www.semanticscholar.org/paper/ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"venue\":\"ICML '07\",\"year\":2007},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1603.02199\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"143970835\",\"name\":\"P. Pastor\"},{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"47202040\",\"name\":\"Deirdre Quillen\"}],\"doi\":\"10.1177/0278364917710318\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"494e2d5b40dcebde349f9872c7317e5003f9c5d2\",\"title\":\"Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection\",\"url\":\"https://www.semanticscholar.org/paper/494e2d5b40dcebde349f9872c7317e5003f9c5d2\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37378985\",\"name\":\"Wei Zhang\"},{\"authorId\":\"145844262\",\"name\":\"Qi Chen\"},{\"authorId\":\"41052788\",\"name\":\"W. Zhang\"},{\"authorId\":\"153003116\",\"name\":\"Xuanyu He\"}],\"doi\":\"10.1016/j.neucom.2017.09.012\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9e1792a547b77e6eba38f21390002b04a5582014\",\"title\":\"Long-range terrain perception using convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/9e1792a547b77e6eba38f21390002b04a5582014\",\"venue\":\"Neurocomputing\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M Pawan Kumar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Benjamin Packer\",\"url\":\"\",\"venue\":\"and Daphne Koller. Self-paced learning for latent variable models. In NIPS, pages 1189\\u20131197\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Adria Puigdomenech Badia\"},{\"authorId\":null,\"name\":\"Mehdi Mirza\"},{\"authorId\":null,\"name\":\"Alex Graves\"},{\"authorId\":null,\"name\":\"Timothy Lillicrap\"},{\"authorId\":null,\"name\":\"Tim Harley\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu. Asynchronous methods for deep reinfor learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 1928\\u20131937,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3717791\",\"name\":\"M. Kumar\"},{\"authorId\":\"1409971380\",\"name\":\"Ben Packer\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a049555721f17ed79a97fd492c8fc9a3f8f8aa17\",\"title\":\"Self-Paced Learning for Latent Variable Models\",\"url\":\"https://www.semanticscholar.org/paper/a049555721f17ed79a97fd492c8fc9a3f8f8aa17\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"1758755\",\"name\":\"Kathryn E. Merrick\"},{\"authorId\":\"1713460\",\"name\":\"H. Abbass\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"}],\"doi\":\"10.24963/ijcai.2017/461\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"13ec391c21ded02ed3be31ff19f90d529a431e89\",\"title\":\"Multi-Task Deep Reinforcement Learning for Continuous Action Control\",\"url\":\"https://www.semanticscholar.org/paper/13ec391c21ded02ed3be31ff19f90d529a431e89\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1620412960\",\"name\":\"Aravaipa Canyon Basin\"}],\"doi\":\"10.1023/A:1017141413812\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b25cc9ad99bd18c68d09065904326ce1d062bffe\",\"title\":\"Volume 3\",\"url\":\"https://www.semanticscholar.org/paper/b25cc9ad99bd18c68d09065904326ce1d062bffe\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1707.05300\",\"authors\":[{\"authorId\":\"10104623\",\"name\":\"Carlos Florensa\"},{\"authorId\":\"145641013\",\"name\":\"David Held\"},{\"authorId\":\"3331786\",\"name\":\"Markus Wulfmeier\"},{\"authorId\":null,\"name\":\"Michael Zhang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9862caed8ee93321c78b0196e0b7eef516b545ba\",\"title\":\"Reverse Curriculum Generation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9862caed8ee93321c78b0196e0b7eef516b545ba\",\"venue\":\"CoRL\",\"year\":2017},{\"arxivId\":\"1605.02097\",\"authors\":[{\"authorId\":\"3407787\",\"name\":\"Michal Kempka\"},{\"authorId\":\"3407043\",\"name\":\"Marek Wydmuch\"},{\"authorId\":\"3407668\",\"name\":\"Grzegorz Runc\"},{\"authorId\":\"3407634\",\"name\":\"Jakub Toczek\"},{\"authorId\":\"2146303\",\"name\":\"Wojciech Ja\\u015bkowski\"}],\"doi\":\"10.1109/CIG.2016.7860433\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a473f545318325ba23b7a6b477485d29777ba873\",\"title\":\"ViZDoom: A Doom-based AI research platform for visual reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/a473f545318325ba23b7a6b477485d29777ba873\",\"venue\":\"2016 IEEE Conference on Computational Intelligence and Games (CIG)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.24963/ijcai.2017/353\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b7708394600130c8c5403178976fcfca1972f3db\",\"title\":\"Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b7708394600130c8c5403178976fcfca1972f3db\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Lu Jiang\"},{\"authorId\":null,\"name\":\"Deyu Meng\"},{\"authorId\":null,\"name\":\"Qian Zhao\"},{\"authorId\":null,\"name\":\"Shiguang Shan\"},{\"authorId\":null,\"name\":\"Alexander G Hauptmann. Self-paced curriculum learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"page 6,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47346762\",\"name\":\"M. Svetlik\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"9578995\",\"name\":\"Rishi Shah\"},{\"authorId\":\"145314605\",\"name\":\"Nick Walker\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"42750855b95449ba74921bfecc84e2257a2c5b19\",\"title\":\"Automatic Curriculum Graph Generation for Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/42750855b95449ba74921bfecc84e2257a2c5b19\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":null,\"name\":\"Qi Chen\"},{\"authorId\":null,\"name\":\"Weidong Zhang\"},{\"authorId\":null,\"name\":\"Xuanyu He. Long-range terrain perception using convolutio networks\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Neurocomputing\",\"url\":\"\",\"venue\":\"275:781\\u2013 787,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Matteo Leonetti\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Asyn - chronous methods for deep reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoshua Bengio\"},{\"authorId\":null,\"name\":\"J\\u00e9r\\u00f4me Louradour\"},{\"authorId\":null,\"name\":\"Ronan Collobert\"},{\"authorId\":null,\"name\":\"Jason Weston. Curriculum learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 41\\u201348,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hui Li\"},{\"authorId\":\"2585822\",\"name\":\"X. Liao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":\"10.1145/1577069.1577109\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5b5ca31260ec10b2c1f59d3dd3aa1d987a8a76f\",\"title\":\"Multi-task Reinforcement Learning in Partially Observable Stochastic Environments\",\"url\":\"https://www.semanticscholar.org/paper/d5b5ca31260ec10b2c1f59d3dd3aa1d987a8a76f\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sanmit Narvekar\"},{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Matteo Leonetti\"},{\"authorId\":null,\"name\":\"Peter Stone. Source task creation for curriculum learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 566\\u2013574,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1611.01796\",\"authors\":[{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"38666915\",\"name\":\"D. Klein\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7\",\"title\":\"Modular Multitask Reinforcement Learning with Policy Sketches\",\"url\":\"https://www.semanticscholar.org/paper/3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50146658\",\"name\":\"W. Klement\"},{\"authorId\":\"47840704\",\"name\":\"Peter A. Flach\"},{\"authorId\":\"1743642\",\"name\":\"N. Japkowicz\"},{\"authorId\":\"1749003\",\"name\":\"S. Matwin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4086e9b6dda8d1b02e1889a464c9a94032e9999d\",\"title\":\"Canadian Conference on Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/4086e9b6dda8d1b02e1889a464c9a94032e9999d\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143949863\",\"name\":\"M. Swamy\"},{\"authorId\":\"71777847\",\"name\":\"H. Zhang\"},{\"authorId\":\"1711097\",\"name\":\"A. Kot\"},{\"authorId\":\"48483889\",\"name\":\"Y. Hu\"},{\"authorId\":\"2192575\",\"name\":\"H. Yu\"},{\"authorId\":\"98043734\",\"name\":\"Panasonic Information\"},{\"authorId\":\"153465382\",\"name\":\"J. Dill\"},{\"authorId\":\"144302962\",\"name\":\"O. Au\"},{\"authorId\":\"89199161\",\"name\":\"Clearwater Bay\"},{\"authorId\":\"1678349\",\"name\":\"A. Nehorai\"},{\"authorId\":\"24284687\",\"name\":\"D. Taylor\"},{\"authorId\":\"97489961\",\"name\":\"F. G. Olshani\"},{\"authorId\":\"1681662\",\"name\":\"G. Gielen\"},{\"authorId\":\"69467609\",\"name\":\"M. Pedram\"},{\"authorId\":\"1409217926\",\"name\":\"Vice President\\u2014Publications\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b7c15c397346073d97651c80a5e73cbd2800d8e1\",\"title\":\"IEEE TRANSACTIONS ON MULTIMEDIA STEERING COMMITTEE MEMBERS\",\"url\":\"https://www.semanticscholar.org/paper/b7c15c397346073d97651c80a5e73cbd2800d8e1\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98264506\",\"name\":\"Yuxin Wu\"},{\"authorId\":\"39402399\",\"name\":\"Yuandong Tian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"add7b8b65355d5408a1ffb93a94b0ae688806bc4\",\"title\":\"Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/add7b8b65355d5408a1ffb93a94b0ae688806bc4\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Maxwell Svetlik\"},{\"authorId\":null,\"name\":\"Matteo Leonetti\"},{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Rishi Shah\"},{\"authorId\":null,\"name\":\"Nick Walker\"},{\"authorId\":null,\"name\":\"Peter Stone. Automatic curriculum graph generation for r agents\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 2590\\u20132596,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhaoyang Yang\"},{\"authorId\":null,\"name\":\"Kathryn Merrick\"},{\"authorId\":null,\"name\":\"Hussein Abbass\"},{\"authorId\":null,\"name\":\"Lianwen Jin. Multi-task deep reinforcement learning for co control\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 3301\\u20133307,\",\"year\":2017}],\"title\":\"Master-Slave Curriculum Design for Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Master/slave (technology)\",\"topicId\":\"385239\",\"url\":\"https://www.semanticscholar.org/topic/385239\"},{\"topic\":\"Doom\",\"topicId\":\"355660\",\"url\":\"https://www.semanticscholar.org/topic/355660\"}],\"url\":\"https://www.semanticscholar.org/paper/b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"venue\":\"IJCAI\",\"year\":2018}\n"