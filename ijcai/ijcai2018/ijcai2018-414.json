"{\"abstract\":\"Recently, a new multi-step temporal learning algorithm, called $Q(\\\\sigma)$, unifies $n$-step Tree-Backup (when $\\\\sigma=0$) and $n$-step Sarsa (when $\\\\sigma=1$) by introducing a sampling parameter $\\\\sigma$. However, similar to other multi-step temporal-difference learning algorithms, $Q(\\\\sigma)$ needs much memory consumption and computation time. Eligibility trace is an important mechanism to transform the off-line updates into efficient on-line ones which consume less memory and computation time. In this paper, we further develop the original $Q(\\\\sigma)$, combine it with eligibility traces and propose a new algorithm, called $Q(\\\\sigma ,\\\\lambda)$, in which $\\\\lambda$ is trace-decay parameter. This idea unifies Sarsa$(\\\\lambda)$ (when $\\\\sigma =1$) and $Q^{\\\\pi}(\\\\lambda)$ (when $\\\\sigma =0$). Furthermore, we give an upper error bound of $Q(\\\\sigma ,\\\\lambda)$ policy evaluation algorithm. We prove that $Q(\\\\sigma,\\\\lambda)$ control algorithm can converge to the optimal value function exponentially. We also empirically compare it with conventional temporal-difference learning methods. Results show that, with an intermediate value of $\\\\sigma$, $Q(\\\\sigma ,\\\\lambda)$ creates a mixture of the existing algorithms that can learn the optimal value significantly faster than the extreme end ($\\\\sigma=0$, or $1$).\",\"arxivId\":\"1802.03171\",\"authors\":[{\"authorId\":\"144114964\",\"name\":\"L. Yang\",\"url\":\"https://www.semanticscholar.org/author/144114964\"},{\"authorId\":\"35747042\",\"name\":\"Minhao Shi\",\"url\":\"https://www.semanticscholar.org/author/35747042\"},{\"authorId\":\"104171063\",\"name\":\"Q. Zheng\",\"url\":\"https://www.semanticscholar.org/author/104171063\"},{\"authorId\":\"49320509\",\"name\":\"Wenjia Meng\",\"url\":\"https://www.semanticscholar.org/author/49320509\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\",\"url\":\"https://www.semanticscholar.org/author/46452405\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"150196660\",\"name\":\"Long Yang\"},{\"authorId\":\"46867976\",\"name\":\"Y. Zhang\"},{\"authorId\":\"48809559\",\"name\":\"Qian Zheng\"},{\"authorId\":\"143714918\",\"name\":\"Pengfei Li\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"c66eacb38ff79829f139ed3be0b7fd7b54b96cd8\",\"title\":\"Gradient Q(\\u03c3, \\u03bb): A Unified Algorithm with Function Approximation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c66eacb38ff79829f139ed3be0b7fd7b54b96cd8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7219560\",\"name\":\"A. Overgaard\"},{\"authorId\":\"31936248\",\"name\":\"B. Nielsen\"},{\"authorId\":\"1404556933\",\"name\":\"Carsten Skovmose Kalles\\u00f8e\"},{\"authorId\":\"2389040\",\"name\":\"J. Bendtsen\"}],\"doi\":\"10.1109/CCTA.2019.8920398\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"08c9e2de39e89a3f8a6df5599753af2762288c74\",\"title\":\"Reinforcement Learning for Mixing Loop Control with Flow Variable Eligibility Trace\",\"url\":\"https://www.semanticscholar.org/paper/08c9e2de39e89a3f8a6df5599753af2762288c74\",\"venue\":\"2019 IEEE Conference on Control Technology and Applications (CCTA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"147637011\",\"name\":\"Mengwen Yuan\"},{\"authorId\":null,\"name\":\"Xi Wu\"},{\"authorId\":\"144539156\",\"name\":\"R. Yan\"},{\"authorId\":\"3134548\",\"name\":\"H. Tang\"}],\"doi\":\"10.1162/neco_a_01238\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5acd85d789ab0a747968f8fa36b2a8f1614af71c\",\"title\":\"Reinforcement Learning in Spiking Neural Networks with Stochastic and Deterministic Synapses\",\"url\":\"https://www.semanticscholar.org/paper/5acd85d789ab0a747968f8fa36b2a8f1614af71c\",\"venue\":\"Neural Computation\",\"year\":2019},{\"arxivId\":\"1905.07237\",\"authors\":[{\"authorId\":\"9645178\",\"name\":\"Longxiang Shi\"},{\"authorId\":\"66841497\",\"name\":\"Shijian Li\"},{\"authorId\":\"145767169\",\"name\":\"L. Cao\"},{\"authorId\":\"150196660\",\"name\":\"Long Yang\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8cb0eeca66b5ea086731b0a26af4ef48a90a4d45\",\"title\":\"TBQ($\\\\sigma$): Improving Efficiency of Trace Utilization for Off-Policy Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8cb0eeca66b5ea086731b0a26af4ef48a90a4d45\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3420880\",\"name\":\"T. P. D. Homem\"},{\"authorId\":\"145275454\",\"name\":\"Paulo E. Santos\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"},{\"authorId\":\"32999467\",\"name\":\"Reinaldo A. C. Bianchi\"},{\"authorId\":\"9217979\",\"name\":\"R. L. M\\u00e1ntaras\"}],\"doi\":\"10.1016/j.artint.2020.103258\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f087d3ff8da57e553349449c3a141227c34fd713\",\"title\":\"Qualitative case-based reasoning and learning\",\"url\":\"https://www.semanticscholar.org/paper/f087d3ff8da57e553349449c3a141227c34fd713\",\"venue\":\"Artif. Intell.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151049110\",\"name\":\"Juan Fernando Hern\\u00e1ndez Garc\\u00eda\"}],\"doi\":\"10.7939/R3-6K85-6P33\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3101258d4c6a7ac6e80abb26c600042aabe7f517\",\"title\":\"Unifying n-Step Temporal-Difference Action-Value Methods\",\"url\":\"https://www.semanticscholar.org/paper/3101258d4c6a7ac6e80abb26c600042aabe7f517\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1806.06953\",\"authors\":[{\"authorId\":\"49320509\",\"name\":\"Wenjia Meng\"},{\"authorId\":\"104171063\",\"name\":\"Q. Zheng\"},{\"authorId\":\"144114964\",\"name\":\"L. Yang\"},{\"authorId\":\"11123083\",\"name\":\"P. Li\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"}],\"doi\":\"10.1109/TNNLS.2019.2948892\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"51701777d547f768bb6e545b951a35fae766c5f5\",\"title\":\"Qualitative Measurements of Policy Discrepancy for Return-Based Deep Q-Network\",\"url\":\"https://www.semanticscholar.org/paper/51701777d547f768bb6e545b951a35fae766c5f5\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":\"1909.02877\",\"authors\":[{\"authorId\":\"150196660\",\"name\":\"Long Yang\"},{\"authorId\":\"48378687\",\"name\":\"Y. Zhang\"},{\"authorId\":\"48809559\",\"name\":\"Qian Zheng\"},{\"authorId\":\"143714918\",\"name\":\"Pengfei Li\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"f3fb22b547ec3234fc5c04bcfcc25f487b6cc797\",\"title\":\"Gradient Q$(\\\\sigma, \\\\lambda)$: A Unified Algorithm with Function Approximation for Reinforcement Learning.\",\"url\":\"https://www.semanticscholar.org/paper/f3fb22b547ec3234fc5c04bcfcc25f487b6cc797\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9645178\",\"name\":\"Longxiang Shi\"},{\"authorId\":\"66841497\",\"name\":\"Shijian Li\"},{\"authorId\":\"145767169\",\"name\":\"L. Cao\"},{\"authorId\":\"144114964\",\"name\":\"L. Yang\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"40ef079611709ce282119e18a0abcc6bfccbb023\",\"title\":\"TBQ(\\u03c3): Improving Efficiency of Trace Utilization for Off-Policy Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/40ef079611709ce282119e18a0abcc6bfccbb023\",\"venue\":\"AAMAS\",\"year\":2019}],\"corpusId\":40655048,\"doi\":\"10.24963/ijcai.2018/414\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"d0b4a50a792f0741d4a925c9f76f4070a18b5450\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748153\",\"name\":\"H. V. Seijen\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f9979d1d2e4e47d58df5e4ce3aef84a46e997352\",\"title\":\"True Online TD(lambda)\",\"url\":\"https://www.semanticscholar.org/paper/f9979d1d2e4e47d58df5e4ce3aef84a46e997352\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R\\u00e9mi Harutyunyan\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Q (\\u03bb) with offpolicy corrections\",\"url\":\"\",\"venue\":\"Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings, volume 9925, page 305. Springer,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1790646\",\"name\":\"P. Dayan\"}],\"doi\":\"10.1023/A:1007495401240\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4fbf2fd29e9f6c5f6e72408d6b152dc8c99d70ac\",\"title\":\"Analytical Mean Squared Error Curves for Temporal Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/4fbf2fd29e9f6c5f6e72408d6b152dc8c99d70ac\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael J Kearns\"},{\"authorId\":null,\"name\":\"Satinder P Singh. Bias-variance error bounds for temporal d updates\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In COLT\",\"url\":\"\",\"venue\":\"pages 142\\u2013147,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1022633531479\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"title\":\"Learning to Predict by the Methods of Temporal Differences\",\"url\":\"https://www.semanticscholar.org/paper/a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"venue\":\"Machine Learning\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1446962550\",\"name\":\"Dock Bumpers\"},{\"authorId\":\"1446961266\",\"name\":\"Support Ledgers\"}],\"doi\":\"10.1023/A:1017189329742\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"title\":\"Volume 2\",\"url\":\"https://www.semanticscholar.org/paper/e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"venue\":\"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.\",\"year\":2005},{\"arxivId\":\"1703.01327\",\"authors\":[{\"authorId\":\"8119874\",\"name\":\"Kristopher De Asis\"},{\"authorId\":\"1398240048\",\"name\":\"J. Hernandez-Garcia\"},{\"authorId\":\"34454457\",\"name\":\"G. Holland\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e2e8d2ae77ffaf9b526d28bd7ed4fb555e50ee24\",\"title\":\"Multi-step Reinforcement Learning: A Unifying Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/e2e8d2ae77ffaf9b526d28bd7ed4fb555e50ee24\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Harm Van Seijen\"},{\"authorId\":null,\"name\":\"A Rupam Mahmood\"},{\"authorId\":null,\"name\":\"Patrick M Pilarski\"},{\"authorId\":null,\"name\":\"Marlos C Machado\"},{\"authorId\":null,\"name\":\"Richard S Sutton. True online temporal-difference learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(145):1\\u2013 40,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786249\",\"name\":\"D. Bertsekas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a82db864e472b5aa6313596ef9919f64e3363b1f\",\"title\":\"Dynamic Programming and Optimal Control\",\"url\":\"https://www.semanticscholar.org/paper/a82db864e472b5aa6313596ef9919f64e3363b1f\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113713833\",\"name\":\"S. J. Crawford\"},{\"authorId\":\"116971860\",\"name\":\"D. F. Kelley\"},{\"authorId\":\"137736247\",\"name\":\"In\\u00e9s Hern\\u00e1ndez \\u00c1vila\"}],\"doi\":\"10.1023/A:1017127612903\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"title\":\"Volume 1\",\"url\":\"https://www.semanticscholar.org/paper/6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1397596967\",\"name\":\"D. L. Corgan\"}],\"doi\":\"10.1136/bmj.2.340.13-a\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5a2e406b1734cba02cce5b0c24a02f42ec262a7d\",\"title\":\"King's College\",\"url\":\"https://www.semanticscholar.org/paper/5a2e406b1734cba02cce5b0c24a02f42ec262a7d\",\"venue\":\"British medical journal\",\"year\":1867},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1162/neco.1994.6.6.1185\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2cca494cd58f483547a4bd059b319a915e5751bc\",\"title\":\"On the Convergence of Stochastic Iterative Dynamic Programming Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/2cca494cd58f483547a4bd059b319a915e5751bc\",\"venue\":\"Neural Computation\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"22069cd4504656d3bb85748a4d43be7a4d7d5545\",\"title\":\"Temporal credit assignment in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/22069cd4504656d3bb85748a4d43be7a4d7d5545\",\"venue\":\"\",\"year\":1984},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dimitri P. Bertsekas\"},{\"authorId\":null,\"name\":\"John N. Tsitsiklis. Neuro Dynamic Programming\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Athena scientific Belmont\",\"url\":\"\",\"venue\":\"MA,\",\"year\":1996},{\"arxivId\":\"1512.04087\",\"authors\":[{\"authorId\":\"1748153\",\"name\":\"H. V. Seijen\"},{\"authorId\":\"1759633\",\"name\":\"A. R. Mahmood\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b59a0c0bb0e8d8dc631ec987322e47feda53fd55\",\"title\":\"True Online Temporal-Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/b59a0c0bb0e8d8dc631ec987322e47feda53fd55\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gavin A Rummery\"},{\"authorId\":null,\"name\":\"Mahesan Niranjan. On-line Q-learning using connectionist systems\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"volume 37\",\"url\":\"\",\"venue\":\"University of Cambridge, Department of Engineering,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Satinder Singh\"},{\"authorId\":null,\"name\":\"Peter Dayan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Harm Seijen and Rich Sutton . True online td ( lambda )\",\"url\":\"\",\"venue\":\"International Conference on Machine Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bbd0a9bea22b4cb7574528382e2f81893d8abc17\",\"title\":\"Bias-Variance Error Bounds for Temporal Difference Updates\",\"url\":\"https://www.semanticscholar.org/paper/bbd0a9bea22b4cb7574528382e2f81893d8abc17\",\"venue\":\"COLT\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748153\",\"name\":\"H. V. Seijen\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"32239759\",\"name\":\"M. Wiering\"}],\"doi\":\"10.1109/ADPRL.2009.4927542\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"823ed45723c980f753d16fdd29f22e63cdfdcd3f\",\"title\":\"A theoretical and empirical analysis of Expected Sarsa\",\"url\":\"https://www.semanticscholar.org/paper/823ed45723c980f753d16fdd29f22e63cdfdcd3f\",\"venue\":\"2009 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1018012322525\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c21accc28f1ab02404948ca4c315ef4a8596ed1\",\"title\":\"Reinforcement Learning with Replacing Eligibility Traces\",\"url\":\"https://www.semanticscholar.org/paper/4c21accc28f1ab02404948ca4c315ef4a8596ed1\",\"venue\":\"Machine Learning\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Harm Van Seijen\"},{\"authorId\":null,\"name\":\"A Rupam Mah-mood\"},{\"authorId\":null,\"name\":\"Patrick M Pilarski\"},{\"authorId\":null,\"name\":\"Marlos C Machado\"},{\"authorId\":null,\"name\":\"Richard S Sutton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"True online temporal - difference learn\",\"url\":\"\",\"venue\":\"A theoretical and empirical analysis of expected sarsa . In Adaptive Dynamic Programming and Reinforcement Learning , 2009 . ADPRL \\u2019 09\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30022423\",\"name\":\"Markus Dumke\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c120f47f2cb8f7acbe69810b765e37d63cea89d6\",\"title\":\"Double Q(\\u03c3) and Q(\\u03c3, \\u03bb): Unifying Reinforcement Learning Control Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/c120f47f2cb8f7acbe69810b765e37d63cea89d6\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Markus Dumke\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Double q(\\u03c3) and q(\\u03c3\",\"url\":\"\",\"venue\":\"\\u03bb): Unifying reinforcement learning control algorithms. arXiv preprint arXiv:1711.01569,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A Rupam Mah-mood\"},{\"authorId\":null,\"name\":\"Patrick M Pilarski\"},{\"authorId\":null,\"name\":\"Marlos C Machado\"},{\"authorId\":null,\"name\":\"Richard S Sutton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"True online temporal - difference learn\",\"url\":\"\",\"venue\":\"Adaptive Dynamic Programming and Reinforcement Learning , 2009 . ADPRL \\u2019 09\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbeb58496711887bf563ad7b0a2860fd46bcd725\",\"title\":\"Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding\",\"url\":\"https://www.semanticscholar.org/paper/cbeb58496711887bf563ad7b0a2860fd46bcd725\",\"venue\":\"NIPS\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":\"10.1023/A:1007678930559\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"712ec1bd9287ac210a7630ce03ca2b0930ebd351\",\"title\":\"Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/712ec1bd9287ac210a7630ce03ca2b0930ebd351\",\"venue\":\"Machine Learning\",\"year\":2004}],\"title\":\"A Unified Approach for Multi-step Temporal-Difference Learning with Eligibility Traces in Reinforcement Learning\",\"topics\":[{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Temporal difference learning\",\"topicId\":\"102033\",\"url\":\"https://www.semanticscholar.org/topic/102033\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Tracing (software)\",\"topicId\":\"2918\",\"url\":\"https://www.semanticscholar.org/topic/2918\"},{\"topic\":\"Time complexity\",\"topicId\":\"3448\",\"url\":\"https://www.semanticscholar.org/topic/3448\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"},{\"topic\":\"Online and offline\",\"topicId\":\"12094\",\"url\":\"https://www.semanticscholar.org/topic/12094\"},{\"topic\":\"Computer memory\",\"topicId\":\"126111\",\"url\":\"https://www.semanticscholar.org/topic/126111\"},{\"topic\":\"Backup\",\"topicId\":\"8128\",\"url\":\"https://www.semanticscholar.org/topic/8128\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Sampling (signal processing)\",\"topicId\":\"7839\",\"url\":\"https://www.semanticscholar.org/topic/7839\"},{\"topic\":\"Converge\",\"topicId\":\"205534\",\"url\":\"https://www.semanticscholar.org/topic/205534\"},{\"topic\":\"State-Action-Reward-State-Action\",\"topicId\":\"1541747\",\"url\":\"https://www.semanticscholar.org/topic/1541747\"}],\"url\":\"https://www.semanticscholar.org/paper/d0b4a50a792f0741d4a925c9f76f4070a18b5450\",\"venue\":\"IJCAI\",\"year\":2018}\n"