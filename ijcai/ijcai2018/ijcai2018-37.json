"{\"abstract\":\"Within the context of video games the notion of perfectly rational agents can be undesirable as it leads to uninteresting situations, where humans face tough adversarial decision makers. Current frameworks for stochastic games and reinforcement learning prohibit tuneable strategies as they seek optimal performance. In this paper, we enable such tuneable behaviour by generalising soft Q-learning to stochastic games, where more than one agent interact strategically. We contribute both theoretically and empirically. On the theory side, we show that games with soft Q-learning exhibit a unique value and generalise team games and zero-sum games far beyond these two extremes to cover a continuous spectrum of gaming behaviour. Experimentally, we show how tuning agents' constraints affect performance and demonstrate, through a neural network architecture, how to reliably balance games with high-dimensional representations.\",\"arxivId\":\"1802.03216\",\"authors\":[{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\",\"url\":\"https://www.semanticscholar.org/author/1399315491\"},{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\",\"url\":\"https://www.semanticscholar.org/author/2505365\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\",\"url\":\"https://www.semanticscholar.org/author/1398842047\"}],\"citationVelocity\":10,\"citations\":[{\"arxivId\":\"1901.10923\",\"authors\":[{\"authorId\":\"41127915\",\"name\":\"David Mguni\"},{\"authorId\":\"12417004\",\"name\":\"Joel Jennings\"},{\"authorId\":\"9164659\",\"name\":\"Sergio Valcarcel Macua\"},{\"authorId\":\"1388364386\",\"name\":\"Emilio Sison\"},{\"authorId\":\"1742685\",\"name\":\"S. Ceppi\"},{\"authorId\":\"2643564\",\"name\":\"E. Cote\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"75b49b22b211fc0f4596f991a9919320dd870042\",\"title\":\"Coordinating the Crowd: Inducing Desirable Equilibria in Non-Cooperative Systems\",\"url\":\"https://www.semanticscholar.org/paper/75b49b22b211fc0f4596f991a9919320dd870042\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":\"1901.09216\",\"authors\":[{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"46584512\",\"name\":\"J. Wang\"}],\"doi\":\"10.24963/ijcai.2020/58\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dd34893d012cf9c391e5af72a51d6bf8a220b2b3\",\"title\":\"Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/dd34893d012cf9c391e5af72a51d6bf8a220b2b3\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"1907.11543\",\"authors\":[{\"authorId\":\"15485601\",\"name\":\"Yagiz Savas\"},{\"authorId\":\"39724710\",\"name\":\"M. Ahmadi\"},{\"authorId\":\"1777396\",\"name\":\"T. Tanaka\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":\"10.1109/CDC40024.2019.9029555\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f92cf9999f8c3f7b0f4ef38ce5124ac6295f3da\",\"title\":\"Entropy-Regularized Stochastic Games\",\"url\":\"https://www.semanticscholar.org/paper/4f92cf9999f8c3f7b0f4ef38ce5124ac6295f3da\",\"venue\":\"2019 IEEE 58th Conference on Decision and Control (CDC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73251689\",\"name\":\"Johan K\\u00e4llstr\\u00f6m\"},{\"authorId\":\"1711918\",\"name\":\"F. Heintz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fa0ae03b38e77f2151833b252784c81829c88725\",\"title\":\"Tunable Dynamics in Agent-Based Simulation using Multi-Objective Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/fa0ae03b38e77f2151833b252784c81829c88725\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765540\",\"name\":\"S. Kuznetsov\"},{\"authorId\":\"35234816\",\"name\":\"A. Panov\"},{\"authorId\":\"40913232\",\"name\":\"P. Chen\"},{\"authorId\":\"1381434830\",\"name\":\"Alfredo Cuzzocrea\"},{\"authorId\":\"40609330\",\"name\":\"Xiao-Yong Du\"},{\"authorId\":\"144408238\",\"name\":\"Orhun Kara\"},{\"authorId\":\"144003898\",\"name\":\"Ting Liu\"},{\"authorId\":\"1731022\",\"name\":\"K. Sivalingam\"},{\"authorId\":\"145514740\",\"name\":\"D. Slezak\"},{\"authorId\":\"1704749\",\"name\":\"T. Washio\"},{\"authorId\":\"50031361\",\"name\":\"X. Yang\"},{\"authorId\":\"145337089\",\"name\":\"S. Barbosa\"}],\"doi\":\"10.1007/978-3-030-30763-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a317fdeb8021cd4daf6e5dac741105010e29de85\",\"title\":\"Artificial Intelligence: 17th Russian Conference, RCAI 2019, Ulyanovsk, Russia, October 21\\u201325, 2019, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/a317fdeb8021cd4daf6e5dac741105010e29de85\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143891653\",\"name\":\"Hao Jiang\"},{\"authorId\":\"117855573\",\"name\":\"Dian-xi Shi\"},{\"authorId\":\"144406347\",\"name\":\"C. Xue\"},{\"authorId\":null,\"name\":\"Yajie Wang\"},{\"authorId\":\"2038462380\",\"name\":\"Gongju Wang\"},{\"authorId\":\"32041155\",\"name\":\"Y. Zhang\"}],\"doi\":\"10.1109/SMC42975.2020.9283033\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f7ebf2ece5b64dc8ed38883b1539eaca33337db\",\"title\":\"Friend-or-Foe Deep Deterministic Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/4f7ebf2ece5b64dc8ed38883b1539eaca33337db\",\"venue\":\"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\",\"year\":2020},{\"arxivId\":\"2007.01174\",\"authors\":[{\"authorId\":\"1785336654\",\"name\":\"Luca Viano\"},{\"authorId\":\"1678908\",\"name\":\"Y. Huang\"},{\"authorId\":\"2197201\",\"name\":\"P. Kamalaruban\"},{\"authorId\":\"1678641\",\"name\":\"V. Cevher\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f390dd8546e8cc668cd40788739ee8ced1194edb\",\"title\":\"Robust Inverse Reinforcement Learning under Transition Dynamics Mismatch\",\"url\":\"https://www.semanticscholar.org/paper/f390dd8546e8cc668cd40788739ee8ced1194edb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145786746\",\"name\":\"Tiago Pinto\"},{\"authorId\":\"145579951\",\"name\":\"Zita A. Vale\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e41f7b69455f61d5ce68aec53f21bbedbe9fd4a6\",\"title\":\"ALBidS: A Decision Support System for Strategic Bidding in Electricity Markets\",\"url\":\"https://www.semanticscholar.org/paper/e41f7b69455f61d5ce68aec53f21bbedbe9fd4a6\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":\"2009.00162\",\"authors\":[{\"authorId\":\"1753611736\",\"name\":\"Qifan Zhang\"},{\"authorId\":\"143863136\",\"name\":\"Yue Guan\"},{\"authorId\":\"144185115\",\"name\":\"P. Tsiotras\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ab07222d9ea0bceadb24c720a32755118b85ec78\",\"title\":\"Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation\",\"url\":\"https://www.semanticscholar.org/paper/ab07222d9ea0bceadb24c720a32755118b85ec78\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39724710\",\"name\":\"M. Ahmadi\"},{\"authorId\":\"22697629\",\"name\":\"Suda Bharadwaj\"},{\"authorId\":\"1777396\",\"name\":\"T. Tanaka\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":\"10.1109/ALLERTON.2018.8636069\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"48207c6bf64b4d69e69f78066c448d753b6c4596\",\"title\":\"Stochastic Games with Sensing Costs\",\"url\":\"https://www.semanticscholar.org/paper/48207c6bf64b4d69e69f78066c448d753b6c4596\",\"venue\":\"2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41127915\",\"name\":\"David Mguni\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6e281232b3baa2abee1fffc98cd9e7f2656ddfcc\",\"title\":\"Efficient Reinforcement Dynamic Mechanism Design\",\"url\":\"https://www.semanticscholar.org/paper/6e281232b3baa2abee1fffc98cd9e7f2656ddfcc\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118180163\",\"name\":\"Francisco Silva\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f3d4c025528bd392702bb80cd48548d34a0062a4\",\"title\":\"Decision Support System for Opponents Selection in Electricity Markets Bilateral Negotiations Demonstration\",\"url\":\"https://www.semanticscholar.org/paper/f3d4c025528bd392702bb80cd48548d34a0062a4\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2892430\",\"name\":\"Rasul Tutunov\"},{\"authorId\":\"73774257\",\"name\":\"Phu Sakulwongtana\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"}],\"doi\":\"10.5555/3398761.3398942\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fac3d20b252309d3cf35dc2c47871b4616f49e36\",\"title\":\"\\u03b1\\u03b1-Rank: Practically Scaling \\u03b1-Rank through Stochastic Optimisation\",\"url\":\"https://www.semanticscholar.org/paper/fac3d20b252309d3cf35dc2c47871b4616f49e36\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"1708.01867\",\"authors\":[{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ca7507536d6c0183d270e2a852a06e1edab50fb\",\"title\":\"An Information-Theoretic Optimality Principle for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ca7507536d6c0183d270e2a852a06e1edab50fb\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1909.11628\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2892430\",\"name\":\"Rasul Tutunov\"},{\"authorId\":\"73774257\",\"name\":\"Phu Sakulwongtana\"},{\"authorId\":\"46257744\",\"name\":\"H. Ammar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cf19bf42670be3fa927a3e146ba2cb04596e5ecf\",\"title\":\"$\\\\alpha^{\\\\alpha}$-Rank: Practically Scaling $\\\\alpha$-Rank through Stochastic Optimisation\",\"url\":\"https://www.semanticscholar.org/paper/cf19bf42670be3fa927a3e146ba2cb04596e5ecf\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2011.00583\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2000281109\",\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"title\":\"An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.09842\",\"authors\":[{\"authorId\":\"30554760\",\"name\":\"Karush Suri\"},{\"authorId\":\"3427664\",\"name\":\"X. Q. Shi\"},{\"authorId\":\"1705037\",\"name\":\"K. Plataniotis\"},{\"authorId\":\"2443061\",\"name\":\"Y. Lawryshyn\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c2c1d54fed5f128baf675345ce060c472e47fc7\",\"title\":\"Energy-based Surprise Minimization for Multi-Agent Value Factorization\",\"url\":\"https://www.semanticscholar.org/paper/7c2c1d54fed5f128baf675345ce060c472e47fc7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.11142\",\"authors\":[{\"authorId\":\"49958235\",\"name\":\"H. Liu\"},{\"authorId\":\"47203052\",\"name\":\"W. Wu\"}],\"doi\":\"10.1109/tsg.2020.3041620\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf4a3f983ffd1e7c2eafac5ce63157b60fa5ffeb\",\"title\":\"Two-stage Deep Reinforcement Learning for Inverter-based Volt-VAR Control in Active Distribution Networks\",\"url\":\"https://www.semanticscholar.org/paper/bf4a3f983ffd1e7c2eafac5ce63157b60fa5ffeb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47319339\",\"name\":\"S. Li\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"8756055\",\"name\":\"Xinyue Cui\"},{\"authorId\":\"88880715\",\"name\":\"Honghua Dong\"},{\"authorId\":\"47324743\",\"name\":\"F. Fang\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":\"10.1609/AAAI.V33I01.33014213\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b224d0f575237feb681717b7e74157dc0bd500df\",\"title\":\"Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/b224d0f575237feb681717b7e74157dc0bd500df\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753611736\",\"name\":\"Qifan Zhang\"},{\"authorId\":\"143863136\",\"name\":\"Yue Guan\"},{\"authorId\":\"144185115\",\"name\":\"P. Tsiotras\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c9f7081129e87b3c6c07f996a4e2e1b4d7ac1401\",\"title\":\"ENTROPY-REGULARIZED POLICY APPROXIMATION\",\"url\":\"https://www.semanticscholar.org/paper/c9f7081129e87b3c6c07f996a4e2e1b4d7ac1401\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.02705\",\"authors\":[{\"authorId\":\"120434407\",\"name\":\"Minki Kang\"},{\"authorId\":\"48324018\",\"name\":\"Moonsu Han\"},{\"authorId\":\"35788904\",\"name\":\"Sung Ju Hwang\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.493\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"27b0c5a5ae9ebcfacf7a97a13392d87df18bae88\",\"title\":\"Neural Mask Generator: Learning to Generate Adaptive Word Maskings for Language Model Adaptation\",\"url\":\"https://www.semanticscholar.org/paper/27b0c5a5ae9ebcfacf7a97a13392d87df18bae88\",\"venue\":\"EMNLP\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145786746\",\"name\":\"T. Pinto\"},{\"authorId\":\"145579951\",\"name\":\"Z. Vale\"}],\"doi\":\"10.24963/ijcai.2019/957\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e74ff3b8d0d0d9995e3c1d9005064343504956f\",\"title\":\"AiD-EM: Adaptive Decision Support for Electricity Markets Negotiations\",\"url\":\"https://www.semanticscholar.org/paper/0e74ff3b8d0d0d9995e3c1d9005064343504956f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2002.07066\",\"authors\":[{\"authorId\":\"2308486\",\"name\":\"Q. Xie\"},{\"authorId\":\"51310474\",\"name\":\"Yudong Chen\"},{\"authorId\":\"50218397\",\"name\":\"Zhaoran Wang\"},{\"authorId\":\"150358650\",\"name\":\"Zhuoran Yang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"82507e35c2e15d62e61df6681cd21b852e7f453d\",\"title\":\"Learning Zero-Sum Simultaneous-Move Markov Games Using Function Approximation and Correlated Equilibrium\",\"url\":\"https://www.semanticscholar.org/paper/82507e35c2e15d62e61df6681cd21b852e7f453d\",\"venue\":\"COLT 2020\",\"year\":2020},{\"arxivId\":\"1905.08087\",\"authors\":[{\"authorId\":\"152307789\",\"name\":\"Zheng Tian\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"8200689\",\"name\":\"Zhichen Gong\"},{\"authorId\":\"1383122034\",\"name\":\"Faiz Punakkath\"},{\"authorId\":\"9399556\",\"name\":\"Shihao Zou\"},{\"authorId\":\"48094081\",\"name\":\"J. Wang\"}],\"doi\":\"10.24963/IJCAI.2019/85\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6dd89f30a5a0e8c5404f23d05c27ef32149b9179\",\"title\":\"A Regularized Opponent Model with Maximum Entropy Objective\",\"url\":\"https://www.semanticscholar.org/paper/6dd89f30a5a0e8c5404f23d05c27ef32149b9179\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2012.13962\",\"authors\":[{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"27040809\",\"name\":\"Vincent Dutordoir\"},{\"authorId\":\"72651925\",\"name\":\"S. John\"},{\"authorId\":\"3110071\",\"name\":\"N. Durrande\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff26681afc5d50331470d960495d26de48e7b415\",\"title\":\"A Tutorial on Sparse Gaussian Processes and Variational Inference\",\"url\":\"https://www.semanticscholar.org/paper/ff26681afc5d50331470d960495d26de48e7b415\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1810332\",\"name\":\"V. Stefanuk\"},{\"authorId\":\"1419483000\",\"name\":\"Tatjana O. Zemtsova\"}],\"doi\":\"10.1007/978-3-030-30763-9_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f17ac3aae88cd37af270d666383c3730eb704b30\",\"title\":\"Chaotic Phenomena in Collective Behavior\",\"url\":\"https://www.semanticscholar.org/paper/f17ac3aae88cd37af270d666383c3730eb704b30\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2006.10611\",\"authors\":[{\"authorId\":\"80722683\",\"name\":\"Manish Prajapat\"},{\"authorId\":\"3371922\",\"name\":\"Kamyar Azizzadenesheli\"},{\"authorId\":\"2373969\",\"name\":\"Alexander Liniger\"},{\"authorId\":\"1740159\",\"name\":\"Yisong Yue\"},{\"authorId\":\"2047844\",\"name\":\"Anima Anandkumar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c657d513a0f7f6966bb18da4c3bbba49c9b572bd\",\"title\":\"Competitive Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/c657d513a0f7f6966bb18da4c3bbba49c9b572bd\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.03755\",\"authors\":[{\"authorId\":\"2892430\",\"name\":\"Rasul Tutunov\"},{\"authorId\":\"49140611\",\"name\":\"Minne Li\"},{\"authorId\":\"48094081\",\"name\":\"J. Wang\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f8271d02eff22a84a845e2ff8b133179f75de069\",\"title\":\"Compositional ADAM: An Adaptive Compositional Solver\",\"url\":\"https://www.semanticscholar.org/paper/f8271d02eff22a84a845e2ff8b133179f75de069\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.11928\",\"authors\":[{\"authorId\":\"36567789\",\"name\":\"R. Qin\"},{\"authorId\":\"1432234123\",\"name\":\"Jing-Cheng Pang\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c60f8f72b59548367ceccb2dbafb558fb28f9cb\",\"title\":\"Improving Fictitious Play Reinforcement Learning with Expanding Models\",\"url\":\"https://www.semanticscholar.org/paper/0c60f8f72b59548367ceccb2dbafb558fb28f9cb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2009.04374\",\"authors\":[{\"authorId\":\"1702307338\",\"name\":\"Nenad Tomasev\"},{\"authorId\":\"1722403\",\"name\":\"U. Paquet\"},{\"authorId\":\"1838779\",\"name\":\"D. Hassabis\"},{\"authorId\":\"9407045\",\"name\":\"V. Kramnik\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"371902505ee0d6fbe08cb88df9e4b2eafc15876c\",\"title\":\"Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess\",\"url\":\"https://www.semanticscholar.org/paper/371902505ee0d6fbe08cb88df9e4b2eafc15876c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145359325\",\"name\":\"Y. Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50394552\",\"name\":\"Rui Lu\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1f25082b3d766ad4aea0d984b61bb59845dbe950\",\"title\":\"Multi-Agent Generalized Recursive Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/1f25082b3d766ad4aea0d984b61bb59845dbe950\",\"venue\":\"ArXiv\",\"year\":2019}],\"corpusId\":3635401,\"doi\":\"10.24963/ijcai.2018/37\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":4,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae\",\"references\":[{\"arxivId\":\"1512.08562\",\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"3314041\",\"name\":\"Ari Pakman\"},{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a026fd65af4ba3575e64174de56fee093fa3330\",\"title\":\"Taming the Noise in Reinforcement Learning via Soft Updates\",\"url\":\"https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":\"1204.6481\",\"authors\":[{\"authorId\":\"145981974\",\"name\":\"Pedro A. Ortega\"},{\"authorId\":\"2354563\",\"name\":\"D. A. Braun\"}],\"doi\":\"10.1098/rspa.2012.0683\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"234d71c4daf3f4c39bce840b349463070e86bf27\",\"title\":\"Thermodynamics as a theory of decision-making with information-processing costs\",\"url\":\"https://www.semanticscholar.org/paper/234d71c4daf3f4c39bce840b349463070e86bf27\",\"venue\":\"Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences\",\"year\":2013},{\"arxivId\":\"1610.01698\",\"authors\":[{\"authorId\":\"145981974\",\"name\":\"Pedro A. Ortega\"},{\"authorId\":\"2100708\",\"name\":\"Alan A. Stocker\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee0a5219eec042b3991d8f50b3680bca3bff3f03\",\"title\":\"Human Decision-Making under Limited Time\",\"url\":\"https://www.semanticscholar.org/paper/ee0a5219eec042b3991d8f50b3680bca3bff3f03\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tuomas Haarnoja\"},{\"authorId\":null,\"name\":\"Haoran Tang\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Sergey Levine. Reinforcement learning with deep energy-ba policies\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1352\\u20131361,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"},{\"authorId\":\"1799704\",\"name\":\"D. Polani\"}],\"doi\":\"10.1007/978-1-4419-1452-1_19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"42fd4c00b5b0a1fee99ddaa0b0e0ae5a2b6e204e\",\"title\":\"Information Theory of Decisions and Actions\",\"url\":\"https://www.semanticscholar.org/paper/42fd4c00b5b0a1fee99ddaa0b0e0ae5a2b6e204e\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Mguni\"},{\"authorId\":null,\"name\":\"Joel Jennings\"},{\"authorId\":null,\"name\":\"Enrique Munoz de Cote. Decentralised learning in systems w many\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"many strategic agents\",\"url\":\"\",\"venue\":\"AAAI Conference on Artificial Intelligence,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7410831\",\"name\":\"M. A. Goodrich\"},{\"authorId\":\"1803820\",\"name\":\"A. Schultz\"}],\"doi\":\"10.1561/1100000005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c13829ba72c0752484d984d55d803d4ad7271e1\",\"title\":\"Human-Robot Interaction: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/8c13829ba72c0752484d984d55d803d4ad7271e1\",\"venue\":\"Found. Trends Hum. Comput. Interact.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L Littman. Markov games as a framework for multi-a learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 11th International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"1994, pages 157\\u2013163,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145208437\",\"name\":\"Hiroshi Ishii\"},{\"authorId\":\"1382283412\",\"name\":\"Newton Lee\"},{\"authorId\":\"1705082\",\"name\":\"S. Natkin\"},{\"authorId\":\"41211990\",\"name\":\"K. Tsushima\"}],\"doi\":\"10.1145/1178477\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e715cfcd022140ce864759922be32e46da0ecab1\",\"title\":\"Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology\",\"url\":\"https://www.semanticscholar.org/paper/e715cfcd022140ce864759922be32e46da0ecab1\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387138786\",\"name\":\"Marc M\\u00e9zard\"},{\"authorId\":\"145710121\",\"name\":\"F. Alcar\\u00e1z\"}],\"doi\":\"10.1088/1742-5468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f681b9acc7cf914b5b9281bdfc8087d95c4cafe\",\"title\":\"Journal of Statistical Mechanics: theory and experiment\",\"url\":\"https://www.semanticscholar.org/paper/0f681b9acc7cf914b5b9281bdfc8087d95c4cafe\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"116802468\",\"name\":\"C. Sitthi-amorn\"},{\"authorId\":\"7335603\",\"name\":\"V. Poshyachinda\"}],\"doi\":\"10.1016/0140-6736(93)91823-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3fcbb37dba2369fb3bf6058a2e25ee02d6fe3c82\",\"title\":\"Bias\",\"url\":\"https://www.semanticscholar.org/paper/3fcbb37dba2369fb3bf6058a2e25ee02d6fe3c82\",\"venue\":\"The Lancet\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6f0f835e60a423ddb1b94d54b07ca991a621431d\",\"title\":\"Friend-or-Foe Q-learning in General-Sum Games\",\"url\":\"https://www.semanticscholar.org/paper/6f0f835e60a423ddb1b94d54b07ca991a621431d\",\"venue\":\"ICML\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1007/978-1-4899-7687-1_720\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6337bd57197c818fb5ab539004d64b0cf51ce2d5\",\"title\":\"Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/6337bd57197c818fb5ab539004d64b0cf51ce2d5\",\"venue\":\"Encyclopedia of Machine Learning and Data Mining\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10295260\",\"name\":\"N. Stanietsky\"},{\"authorId\":\"122651067\",\"name\":\"H. \\u0160imi\\u0107\"},{\"authorId\":\"3888282\",\"name\":\"J. Arapovi\\u0107\"},{\"authorId\":\"46403067\",\"name\":\"Amir Toporik\"},{\"authorId\":\"144555680\",\"name\":\"O. Levy\"},{\"authorId\":\"18814437\",\"name\":\"A. Novik\"},{\"authorId\":\"4660267\",\"name\":\"Z. Levine\"},{\"authorId\":\"11537061\",\"name\":\"Meirav Beiman\"},{\"authorId\":\"3502722\",\"name\":\"L. Dassa\"},{\"authorId\":\"3714556\",\"name\":\"Hagit Achdout\"},{\"authorId\":\"1397179736\",\"name\":\"Noam Stern-Ginossar\"},{\"authorId\":\"16241718\",\"name\":\"Pinhas Tsukerman\"},{\"authorId\":\"4700253\",\"name\":\"S. Jonji\\u0107\"},{\"authorId\":\"3851561\",\"name\":\"O. Mandelboim\"}],\"doi\":\"10.1073/pnas.0903474106\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"87c3c6b98f3cfa972cae3164eabadfd01c1a35c3\",\"title\":\"The interaction of TIGIT with PVR and PVRL2 inhibits human NK cell cytotoxicity\",\"url\":\"https://www.semanticscholar.org/paper/87c3c6b98f3cfa972cae3164eabadfd01c1a35c3\",\"venue\":\"Proceedings of the National Academy of Sciences\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L Littman. Friend or foe q-learning in general-sum games\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In In Proceedings of the 18th Int\",\"url\":\"\",\"venue\":\"Conf. on Machine Learning. Citeseer,\",\"year\":2001},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Roy Fox\"},{\"authorId\":null,\"name\":\"Ari Pakman\"},{\"authorId\":null,\"name\":\"Naftali Tishby. Taming the noise in reinforcement learning Intelligence\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 202\\u2013211\",\"url\":\"\",\"venue\":\"AUAI Press,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48057653\",\"name\":\"O. Bagasra\"}],\"doi\":\"10.1073/PNAS.95.17.10344-D\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"title\":\"PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES.\",\"url\":\"https://www.semanticscholar.org/paper/edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"venue\":\"Science\",\"year\":1915},{\"arxivId\":\"physics/0505066\",\"authors\":[{\"authorId\":\"1792269\",\"name\":\"H. Kappen\"}],\"doi\":\"10.1088/1742-5468/2005/11/P11011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b107acedcf1953ff498ff459f915845962c47674\",\"title\":\"Path integrals and symmetry breaking for optimal control theory\",\"url\":\"https://www.semanticscholar.org/paper/b107acedcf1953ff498ff459f915845962c47674\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bart van den Broek\"},{\"authorId\":null,\"name\":\"Wim Wiegerinck\"},{\"authorId\":null,\"name\":\"Bert Kappen. Risk sensitive path integral control. In P Intelligence\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"UAI\\u201910\",\"url\":\"\",\"venue\":\"pages 615\\u2013 622, Arlington, Virginia, United States,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jan Peters\"},{\"authorId\":null,\"name\":\"Katharina M\\u00fclling\"},{\"authorId\":null,\"name\":\"Yasemin Alt\\u00fcn. Relative entropy policy search. In Proceedi Intelligence\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 1607\\u20131612\",\"url\":\"\",\"venue\":\"AAAI Press,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50027-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"title\":\"Markov Games as a Framework for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 32nd International Conference on Machine Learning (ICML-15)\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":\"1604.02080\",\"authors\":[{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\"},{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"3081854\",\"name\":\"Tim Genewein\"},{\"authorId\":\"2354563\",\"name\":\"D. A. Braun\"}],\"doi\":\"10.1007/978-3-319-46227-1_30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a472aeb7312383822da385f03141f2decb3d124\",\"title\":\"Planning with Information-Processing Constraints and Model Uncertainty in Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/3a472aeb7312383822da385f03141f2decb3d124\",\"venue\":\"ECML/PKDD\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"327950ac8d59054c4b4a37f901ac9bdbb8168087\",\"title\":\"A Generalized Reinforcement-Learning Model: Convergence and Applications\",\"url\":\"https://www.semanticscholar.org/paper/327950ac8d59054c4b4a37f901ac9bdbb8168087\",\"venue\":\"ICML\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145768994\",\"name\":\"Tatiana V. Guy\"},{\"authorId\":\"73567912\",\"name\":\"Miroslav Krn\"},{\"authorId\":\"1696678\",\"name\":\"D. Wolpert\"}],\"doi\":\"10.1007/978-3-642-24647-0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"103e676aec18872622fb660357134da546200da3\",\"title\":\"Decision Making with Imperfect Decision Makers\",\"url\":\"https://www.semanticscholar.org/paper/103e676aec18872622fb660357134da546200da3\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2283302\",\"name\":\"M. Osborne\"},{\"authorId\":\"2318339\",\"name\":\"A. Rubinstein\"}],\"doi\":\"10.2307/136062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef336fe9c04559654936413f4910a54b7ae5028c\",\"title\":\"A Course in Game Theory\",\"url\":\"https://www.semanticscholar.org/paper/ef336fe9c04559654936413f4910a54b7ae5028c\",\"venue\":\"\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Falk Lieder\"},{\"authorId\":null,\"name\":\"Tom Griffiths\"},{\"authorId\":null,\"name\":\"Noah Goodman. Burn-in\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"bias\",\"url\":\"\",\"venue\":\"and the rationality of anchoring. In Advances in neural information processing systems, pages 2690\\u20132798,\",\"year\":2012},{\"arxivId\":\"1702.08165\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"title\":\"Reinforcement Learning with Deep Energy-Based Policies\",\"url\":\"https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Daniel A Braun\"},{\"authorId\":null,\"name\":\"Pedro A Ortega\"},{\"authorId\":null,\"name\":\"Evangelos Theodorou\"},{\"authorId\":null,\"name\":\"Stefan Schaal. Path integral control\"},{\"authorId\":null,\"name\":\"bounded rationality\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Adaptive Dynamic Programming And Reinforcement Learning (ADPRL)\",\"url\":\"\",\"venue\":\"2011 IEEE Symposium on, pages 202\\u2013209. IEEE,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27049951\",\"name\":\"F. Verloove\"},{\"authorId\":\"3340900\",\"name\":\"E. Robbrecht\"}],\"doi\":\"10.1023/A:1017123022676\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ee449f7a2c0bf0620109fede6446213407d7fa6\",\"title\":\"Volume 39\",\"url\":\"https://www.semanticscholar.org/paper/1ee449f7a2c0bf0620109fede6446213407d7fa6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202750\",\"name\":\"Falk Lieder\"},{\"authorId\":\"1799860\",\"name\":\"T. Griffiths\"},{\"authorId\":\"144002017\",\"name\":\"Noah D. Goodman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd3a07b80b9bd8bb5ebc2e69a8ebff3d2bad8481\",\"title\":\"\\\"Burn-in, bias, and the rationality of anchoring\\\"\",\"url\":\"https://www.semanticscholar.org/paper/bd3a07b80b9bd8bb5ebc2e69a8ebff3d2bad8481\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pedro A Ortega\"},{\"authorId\":null,\"name\":\"A DanielABraun.Thermodynamicsasatheoryofdecision-mak\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 469\",\"url\":\"\",\"venue\":\"page 20120683. The Royal Society,\",\"year\":2013},{\"arxivId\":\"1210.4849\",\"authors\":[{\"authorId\":\"2026742\",\"name\":\"L. Agussurja\"},{\"authorId\":\"1725253\",\"name\":\"H. C. Lau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6be8a8f986641c505a78b51fa6aa6cb6f63fbcf5\",\"title\":\"Toward Large-Scale Agent Guidance in an Urban Taxi Service\",\"url\":\"https://www.semanticscholar.org/paper/6be8a8f986641c505a78b51fa6aa6cb6f63fbcf5\",\"venue\":\"UAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A Rusu\"},{\"authorId\":null,\"name\":\"Joel Veness\"},{\"authorId\":null,\"name\":\"Marc G Belle-mare\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Alex Graves , Martin Riedmiller , Andreas K Fidje - land , Georg Ostrovski , et al . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"ceedings of the 11 th International Conference on Machine Learning\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Naftali Tishby\"},{\"authorId\":null,\"name\":\"Daniel Polani. Information theory of decisions\"},{\"authorId\":null,\"name\":\"actions. In Perceptionaction cycle\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 601\\u2013636\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2011},{\"arxivId\":\"1203.3523\",\"authors\":[{\"authorId\":\"1855798\",\"name\":\"B. V. Broek\"},{\"authorId\":\"1783429\",\"name\":\"W. Wiegerinck\"},{\"authorId\":\"1792269\",\"name\":\"H. Kappen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c98925129a36d1bd5aa448a9c6430dc23ccdf8e4\",\"title\":\"Risk Sensitive Path Integral Control\",\"url\":\"https://www.semanticscholar.org/paper/c98925129a36d1bd5aa448a9c6430dc23ccdf8e4\",\"venue\":\"UAI\",\"year\":2010},{\"arxivId\":\"1708.01867\",\"authors\":[{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1ca7507536d6c0183d270e2a852a06e1edab50fb\",\"title\":\"An Information-Theoretic Optimality Principle for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ca7507536d6c0183d270e2a852a06e1edab50fb\",\"venue\":\"ArXiv\",\"year\":2017}],\"title\":\"Balancing Two-Player Stochastic Games with Soft Q-Learning\",\"topics\":[{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Network architecture\",\"topicId\":\"58473\",\"url\":\"https://www.semanticscholar.org/topic/58473\"},{\"topic\":\"Rational agent\",\"topicId\":\"459071\",\"url\":\"https://www.semanticscholar.org/topic/459071\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"}],\"url\":\"https://www.semanticscholar.org/paper/8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae\",\"venue\":\"IJCAI\",\"year\":2018}\n"