"{\"abstract\":\"Large state and action spaces are very challenging to reinforcement learning. However, in many domains there is a set of algorithms available, which estimate the best action given a state. Hence, agents can either directly learn a performance-maximizing mapping from states to actions, or from states to algorithms. We investigate several aspects of this dilemma, showing sufficient conditions for learning over algorithms to outperform over actions for a finite number of training iterations. We present synthetic experiments to further study such systems. Finally, we propose a function approximation approach, demonstrating the effectiveness of learning over algorithms in real-time strategy games.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2226999\",\"name\":\"A. Tavares\",\"url\":\"https://www.semanticscholar.org/author/2226999\"},{\"authorId\":\"47369103\",\"name\":\"S. Anbalagan\",\"url\":\"https://www.semanticscholar.org/author/47369103\"},{\"authorId\":\"2211313\",\"name\":\"Leandro Soriano Marcolino\",\"url\":\"https://www.semanticscholar.org/author/2211313\"},{\"authorId\":\"1687688\",\"name\":\"L. Chaimowicz\",\"url\":\"https://www.semanticscholar.org/author/1687688\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1906.07371\",\"authors\":[{\"authorId\":\"40223874\",\"name\":\"Philippe Morere\"},{\"authorId\":\"2065760\",\"name\":\"Lionel Ott\"},{\"authorId\":\"145726169\",\"name\":\"F. Ramos\"}],\"doi\":\"10.1109/LRA.2019.2920285\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0831c2a2a042c1637d68e0ac35ad3f3ffb399240\",\"title\":\"Learning to Plan Hierarchically From Curriculum\",\"url\":\"https://www.semanticscholar.org/paper/0831c2a2a042c1637d68e0ac35ad3f3ffb399240\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1740910046\",\"name\":\"Lukasz Pelcner\"},{\"authorId\":\"10130648\",\"name\":\"Shaling Li\"},{\"authorId\":\"1643964736\",\"name\":\"Matheus Do Carmo Alves\"},{\"authorId\":\"2211313\",\"name\":\"Leandro Soriano Marcolino\"},{\"authorId\":\"47570947\",\"name\":\"A. Collins\"}],\"doi\":\"10.5555/3398761.3398880\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c8a33762a5c661484980f98d8d5c5dcfb1e0a0e\",\"title\":\"Real-time Learning and Planning in Environments with Swarms: A Hierarchical and a Parameter-based Simulation Approach\",\"url\":\"https://www.semanticscholar.org/paper/5c8a33762a5c661484980f98d8d5c5dcfb1e0a0e\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2226999\",\"name\":\"A. Tavares\"},{\"authorId\":\"1687688\",\"name\":\"L. Chaimowicz\"}],\"doi\":\"10.1109/CIG.2018.8490427\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4180aa0dd3d4eea212b88a1195122de96011e4bf\",\"title\":\"Tabular Reinforcement Learning in Real-Time Strategy Games via Options\",\"url\":\"https://www.semanticscholar.org/paper/4180aa0dd3d4eea212b88a1195122de96011e4bf\",\"venue\":\"2018 IEEE Conference on Computational Intelligence and Games (CIG)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2226999\",\"name\":\"A. Tavares\"},{\"authorId\":\"144175187\",\"name\":\"D. K. S. Vieira\"},{\"authorId\":\"1395075011\",\"name\":\"Tiago Negrisoli\"},{\"authorId\":\"1687688\",\"name\":\"L. Chaimowicz\"}],\"doi\":\"10.1109/TG.2018.2880147\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60c72eed3ef5d3a72085e01c88adb1ce85ad30c3\",\"title\":\"Algorithm Selection in Adversarial Settings: From Experiments to Tournaments in StarCraft\",\"url\":\"https://www.semanticscholar.org/paper/60c72eed3ef5d3a72085e01c88adb1ce85ad30c3\",\"venue\":\"IEEE Transactions on Games\",\"year\":2019}],\"corpusId\":51606693,\"doi\":\"10.24963/ijcai.2018/377\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"75304fa42b400b234248821081d828eac0dcd10d\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leandro Soriano Marcolino\"},{\"authorId\":null,\"name\":\"Albert Xin Jiang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Milind Tambe\",\"url\":\"\",\"venue\":\"Multi-agent team formation: diversity beats strength? In IJCAI,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas G. Dietterich. Hierarchical reinforcement learning decomposition\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"13:227\\u2013303,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":\"10.1109/CIG.2013.6633643\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f78a522c99cd3056dbfbbeae9d3410f08ab12027\",\"title\":\"Portfolio greedy search and simulation for large-scale combat in starcraft\",\"url\":\"https://www.semanticscholar.org/paper/f78a522c99cd3056dbfbbeae9d3410f08ab12027\",\"venue\":\"2013 IEEE Conference on Computational Inteligence in Games (CIG)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Santiago Onta\\u00f1\\u00f3n\"},{\"authorId\":null,\"name\":\"Gabriel Synnaeve\"},{\"authorId\":null,\"name\":\"Alberto Uriarte\"},{\"authorId\":null,\"name\":\"Florian Richoux\"},{\"authorId\":null,\"name\":\"David Churchill\"},{\"authorId\":null,\"name\":\"Mike Preuss. A survey of real-time strategy game AI research\"},{\"authorId\":null,\"name\":\"competition in StarCraft\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"IEEE Transactions on Computational Intelligence and AI in Games (TCIAIG)\",\"url\":\"\",\"venue\":\"5(4):293\\u2013311,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1710.04805\",\"authors\":[{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"}],\"doi\":\"10.1613/jair.5398\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08eda51118455347ee3ceb51deddb2bca8f65aba\",\"title\":\"Combinatorial Multi-armed Bandits for Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/08eda51118455347ee3ceb51deddb2bca8f65aba\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. John\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Rice . The algorithm selection problem\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nicolas A. Barriga\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Marius Stanescu , and Michael Buro . Game Tree Search Based on Non - Deterministic Action Scripts in Real - Time Strategy Games\",\"url\":\"\",\"venue\":\"IEEE Transactions on Games\",\"year\":null},{\"arxivId\":\"1111.2249\",\"authors\":[{\"authorId\":\"47776045\",\"name\":\"L. Xu\"},{\"authorId\":\"144661829\",\"name\":\"F. Hutter\"},{\"authorId\":\"2470869\",\"name\":\"H. Hoos\"},{\"authorId\":\"1388404060\",\"name\":\"Kevin Leyton-Brown\"}],\"doi\":\"10.1613/jair.2490\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72489f377f08ac720cd31d9eed20706abfed58bb\",\"title\":\"SATzilla: Portfolio-based Algorithm Selection for SAT\",\"url\":\"https://www.semanticscholar.org/paper/72489f377f08ac720cd31d9eed20706abfed58bb\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Santiago Onta\\u00f1\\u00f3n. Combinatorial multi-armed bandits for rea games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"58,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Gavin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Rice . The algorithm selection problem\",\"url\":\"\",\"venue\":\"\",\"year\":1976},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1784072\",\"name\":\"M. Lagoudakis\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"79936bfab7ab464afba9e653f1921d147a883a2c\",\"title\":\"Algorithm Selection using Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/79936bfab7ab464afba9e653f1921d147a883a2c\",\"venue\":\"ICML\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2914912\",\"name\":\"Nicolas A. Barriga\"},{\"authorId\":\"36145068\",\"name\":\"M. St\\u0103nescu\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"65ba1f799f6d031b08af2049f044d08eb3e65ce6\",\"title\":\"Puppet Search: Enhancing Scripted Behavior by Look-Ahead Search with Applications to Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/65ba1f799f6d031b08af2049f044d08eb3e65ce6\",\"venue\":\"AIIDE\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14171685\",\"name\":\"Philip Bontrager\"},{\"authorId\":\"143840345\",\"name\":\"A. Khalifa\"},{\"authorId\":\"143869428\",\"name\":\"A. Mendes\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7d73179c58581f0516cb4fa1bb141e6d59568701\",\"title\":\"Matching Games and Algorithms for General Video Game Playing\",\"url\":\"https://www.semanticscholar.org/paper/7d73179c58581f0516cb4fa1bb141e6d59568701\",\"venue\":\"AIIDE\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2324546\",\"name\":\"Levi H. S. Lelis\"}],\"doi\":\"10.24963/ijcai.2017/522\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"99ecef89ee0d8d74564a94b4ccc55f5acc1a3bef\",\"title\":\"Stratified Strategy Selection for Unit Control in Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/99ecef89ee0d8d74564a94b4ccc55f5acc1a3bef\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"},{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"38814203\",\"name\":\"Alberto Uriarte\"},{\"authorId\":\"2127603\",\"name\":\"Florian Richoux\"},{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1950379\",\"name\":\"M. Preuss\"}],\"doi\":\"10.1109/TCIAIG.2013.2286295\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"593487efa8e3c8657ad40104d237bdacf674fe53\",\"title\":\"A Survey of Real-Time Strategy Game AI Research and Competition in StarCraft\",\"url\":\"https://www.semanticscholar.org/paper/593487efa8e3c8657ad40104d237bdacf674fe53\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37202259\",\"name\":\"J. Meigs\"}],\"doi\":\"10.1515/cclm.1994.32.8.631\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c3391bde2bb1b3d737913ee8caa01492a782732\",\"title\":\"WHO Technical Report\",\"url\":\"https://www.semanticscholar.org/paper/5c3391bde2bb1b3d737913ee8caa01492a782732\",\"venue\":\"The Yale Journal of Biology and Medicine\",\"year\":1954},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39815165\",\"name\":\"Franisek Sailer\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"}],\"doi\":\"10.1109/CIG.2007.368082\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fd7f664ec01c74d89525f15b663139b9be76c181\",\"title\":\"Adversarial Planning Through Strategy Simulation\",\"url\":\"https://www.semanticscholar.org/paper/fd7f664ec01c74d89525f15b663139b9be76c181\",\"venue\":\"2007 IEEE Symposium on Computational Intelligence and Games\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2226999\",\"name\":\"A. Tavares\"},{\"authorId\":\"32980577\",\"name\":\"Hector Azpurua\"},{\"authorId\":\"39490212\",\"name\":\"A. Santos\"},{\"authorId\":\"1687688\",\"name\":\"L. Chaimowicz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1dcc02e761ec5a506b173850766d254bcdd66b20\",\"title\":\"Rock, Paper, StarCraft: Strategy Selection in Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/1dcc02e761ec5a506b173850766d254bcdd66b20\",\"venue\":\"AIIDE\",\"year\":2016},{\"arxivId\":\"cs/9905014\",\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1613/jair.639\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c96ca25d889251e20e33d01f24eec175301ab94\",\"title\":\"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/4c96ca25d889251e20e33d01f24eec175301ab94\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2000},{\"arxivId\":\"1709.03480\",\"authors\":[{\"authorId\":\"145299681\",\"name\":\"Nicolas A. Barriga\"},{\"authorId\":\"36145068\",\"name\":\"M. St\\u0103nescu\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4dbf1e6a2fd016cbdc5a12956d4410f65b636465\",\"title\":\"Combining Strategic Learning with Tactical Search in Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/4dbf1e6a2fd016cbdc5a12956d4410f65b636465\",\"venue\":\"AIIDE\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Anderson Tavares\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Hector Azp\\u00farua , Amanda Santos , and Luiz Chaimowicz . Rock , Paper , StarCraft : Strategy Selection in Real - Time Strategy Games\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52327071\",\"name\":\"Collectif\"}],\"doi\":\"10.1016/0029-554x(64)90226-5\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"929f54530b8cf0484df4d26b737038747de20bd5\",\"title\":\"Advances in Computers\",\"url\":\"https://www.semanticscholar.org/paper/929f54530b8cf0484df4d26b737038747de20bd5\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1777994\",\"name\":\"A. Jiang\"},{\"authorId\":\"2211313\",\"name\":\"Leandro Soriano Marcolino\"},{\"authorId\":\"1689184\",\"name\":\"A. Procaccia\"},{\"authorId\":\"145714168\",\"name\":\"T. Sandholm\"},{\"authorId\":\"39195827\",\"name\":\"Nisarg Shah\"},{\"authorId\":\"143736701\",\"name\":\"Milind Tambe\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5274cb8fad76186a9011133d3b0289d76adab4c\",\"title\":\"Diverse Randomized Agents Vote to Win\",\"url\":\"https://www.semanticscholar.org/paper/e5274cb8fad76186a9011133d3b0289d76adab4c\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2211313\",\"name\":\"Leandro Soriano Marcolino\"},{\"authorId\":\"49507370\",\"name\":\"Haifeng Xu\"},{\"authorId\":\"1777994\",\"name\":\"A. Jiang\"},{\"authorId\":\"143736701\",\"name\":\"Milind Tambe\"},{\"authorId\":\"1740910\",\"name\":\"E. Bowring\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42c5b4d8c539635f531a5782a104ee302a45a974\",\"title\":\"Give a Hard Problem to a Diverse Team: Exploring Large Action Spaces\",\"url\":\"https://www.semanticscholar.org/paper/42c5b4d8c539635f531a5782a104ee302a45a974\",\"venue\":\"AAAI\",\"year\":2014}],\"title\":\"Algorithms or Actions? A Study in Large-Scale Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Approximation\",\"topicId\":\"3247\",\"url\":\"https://www.semanticscholar.org/topic/3247\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Iteration\",\"topicId\":\"11823\",\"url\":\"https://www.semanticscholar.org/topic/11823\"},{\"topic\":\"Real-time locating system\",\"topicId\":\"811893\",\"url\":\"https://www.semanticscholar.org/topic/811893\"},{\"topic\":\"Synthetic intelligence\",\"topicId\":\"1588157\",\"url\":\"https://www.semanticscholar.org/topic/1588157\"}],\"url\":\"https://www.semanticscholar.org/paper/75304fa42b400b234248821081d828eac0dcd10d\",\"venue\":\"IJCAI\",\"year\":2018}\n"