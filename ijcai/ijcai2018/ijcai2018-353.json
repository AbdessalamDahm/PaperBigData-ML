"{\"abstract\":\"Temporal-difference (TD) learning is an attractive, computationally efficient framework for modelfree reinforcement learning. Q-learning is one of the most widely used TD learning technique that enables an agent to learn the optimal action-value function, i.e. Q-value function. Contrary to its widespread use, Q-learning has only been proven to converge on Markov Decision Processes (MDPs) and Q-uniform abstractions of finite-state MDPs. On the other hand, most real-world problems are inherently non-Markovian: the full true state of the environment is not revealed by recent observations. In this paper, we investigate the behavior of Q-learning when applied to non-MDP and non-ergodic domains which may have infinitely many underlying states. We prove that the convergence guarantee of Q-learning can be extended to a class of such non-MDP problems, in particular, to some non-stationary domains. We show that state-uniformity of the optimal Q-value function is a necessary and sufficient condition for Q-learning to converge even in the case of infinitely many internal states.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"51056119\",\"name\":\"Sultan Javed Majeed\",\"url\":\"https://www.semanticscholar.org/author/51056119\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\",\"url\":\"https://www.semanticscholar.org/author/144154444\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.10200\",\"authors\":[{\"authorId\":\"51056119\",\"name\":\"Sultan Javed Majeed\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8769e44ab5d1c04435f895e9c97b4bcf752bb913\",\"title\":\"Exact Reduction of Huge Action Spaces in General Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8769e44ab5d1c04435f895e9c97b4bcf752bb913\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.13490\",\"authors\":[{\"authorId\":\"38562041\",\"name\":\"Khimya Khetarpal\"},{\"authorId\":\"40497459\",\"name\":\"M. Riemer\"},{\"authorId\":\"113766340\",\"name\":\"I. Rish\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28d988a4eac1d714234036e351f417a535fe49b2\",\"title\":\"Towards Continual Reinforcement Learning: A Review and Perspectives\",\"url\":\"https://www.semanticscholar.org/paper/28d988a4eac1d714234036e351f417a535fe49b2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2002.05518\",\"authors\":[{\"authorId\":\"7981071\",\"name\":\"Kavosh Asadi\"},{\"authorId\":\"143795784\",\"name\":\"D. Abel\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5bd4abd0a1336f7d880b985acc18082affa56197\",\"title\":\"Learning State Abstractions for Transfer in Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/5bd4abd0a1336f7d880b985acc18082affa56197\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":51605615,\"doi\":\"10.24963/ijcai.2018/353\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"66d76444255be0ac378a0a93ee0379fc721a386f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22069cd4504656d3bb85748a4d43be7a4d7d5545\",\"title\":\"Temporal credit assignment in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/22069cd4504656d3bb85748a4d43be7a4d7d5545\",\"venue\":\"\",\"year\":1984},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478313\",\"name\":\"L. Lin\"},{\"authorId\":\"40975594\",\"name\":\"Tom Michael Mitchell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f2eb733470921af04df5c611a6a3c76c281ce498\",\"title\":\"Memory Approaches to Reinforcement Learning in Non-Markovian Domains\",\"url\":\"https://www.semanticscholar.org/paper/f2eb733470921af04df5c611a6a3c76c281ce498\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4eb68f4c2ecfce149e631b909dad37b451d38dba\",\"title\":\"Memoryless policies: theoretical limitations and practical results\",\"url\":\"https://www.semanticscholar.org/paper/4eb68f4c2ecfce149e631b909dad37b451d38dba\",\"venue\":\"\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50042-8\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a579d06ac278e14948f67748cd651e4eb617ae4e\",\"title\":\"Learning Without State-Estimation in Partially Observable Markovian Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/a579d06ac278e14948f67748cd651e4eb617ae4e\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Anthony R Cassandra\"},{\"authorId\":null,\"name\":\"Leslie Pack Kaelbling\"},{\"authorId\":null,\"name\":\"Michael L Littman. Acting optimaly in partially observable domains\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AAAI\",\"url\":\"\",\"venue\":\"pages 1023\\u20131023,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720087\",\"name\":\"S. Whitehead\"},{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1016/0004-3702(94)00012-P\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cfe5191354fd9188c52500042fe205f31dd14c5f\",\"title\":\"Reinforcement Learning of Non-Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/cfe5191354fd9188c52500042fe205f31dd14c5f\",\"venue\":\"Artif. Intell.\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1844179\",\"name\":\"L. Baird\"}],\"doi\":\"10.1016/b978-1-55860-377-6.50013-x\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f518bffb712a298bff18248c67f6fc0181018ae6\",\"title\":\"Residual Algorithms: Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/f518bffb712a298bff18248c67f6fc0181018ae6\",\"venue\":\"ICML\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Learning to Predict by the Method of Temporal Differences\",\"url\":\"\",\"venue\":\"Machine Learning\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Theodore J Perkins\"},{\"authorId\":null,\"name\":\"Mark D Pendrith. On the existence of fixed points for Q-learning\"},{\"authorId\":null,\"name\":\"Sarsa in partially observable domains\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 490\\u2013497,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"144926179\",\"name\":\"T. Walsh\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ca9a2d326b9de48c095a6cb5912e1990d2c5ab46\",\"title\":\"Towards a Unified Theory of State Abstraction for MDPs\",\"url\":\"https://www.semanticscholar.org/paper/ca9a2d326b9de48c095a6cb5912e1990d2c5ab46\",\"venue\":\"ISAIM\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"135258665\",\"name\":\"\\u4e2d\\u5d8b \\u548c\\u4e45\"},{\"authorId\":\"1397341011\",\"name\":\"\\u30ab\\u30ba\\u30d2\\u30b5 \\u30ca\\u30ab\\u30b7\\u30de\"},{\"authorId\":\"1393010664\",\"name\":\"Nakashima Kazuhisa\"}],\"doi\":\"10.1787/oif-2007-7-en\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f80dd219794f55733d0e6176ff78d9587cda2e7\",\"title\":\"\\u74b0\\u5883 Environment \\u306b\\u3064\\u3044\\u3066\",\"url\":\"https://www.semanticscholar.org/paper/7f80dd219794f55733d0e6176ff78d9587cda2e7\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145175901\",\"name\":\"M. Nivat\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"400a433d9f882f3218b516b4be656ff79e4711fb\",\"title\":\"Theoretical Computer Science Volume 213-214\",\"url\":\"https://www.semanticscholar.org/paper/400a433d9f882f3218b516b4be656ff79e4711fb\",\"venue\":\"\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Anthony R Cassandra. Optimal Policies for Partially Observ August\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Report CS-94-14\",\"url\":\"\",\"venue\":\"Brown Univ,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"070168f4a6ad7b616839c04fbf3524658f1f34a4\",\"title\":\"Universal Artificial Intelligence: Sequential Decisions Based on Algorithmic Probability (Texts in Theoretical Computer Science. An EATCS Series)\",\"url\":\"https://www.semanticscholar.org/paper/070168f4a6ad7b616839c04fbf3524658f1f34a4\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143753639\",\"name\":\"A. McCallum\"}],\"doi\":\"10.1016/b978-1-55860-377-6.50055-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f555d8cd2cb4643f7367b3beecc9a1a0f82b41d3\",\"title\":\"Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State\",\"url\":\"https://www.semanticscholar.org/paper/f555d8cd2cb4643f7367b3beecc9a1a0f82b41d3\",\"venue\":\"ICML\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738444\",\"name\":\"S. Thrun\"}],\"doi\":\"10.1145/504729.504754\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f8d7bdfc3ed0ff793f1236730486b3d5cf946aa\",\"title\":\"Probabilistic robotics\",\"url\":\"https://www.semanticscholar.org/paper/3f8d7bdfc3ed0ff793f1236730486b3d5cf946aa\",\"venue\":\"CACM\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1772443\",\"name\":\"M. Nickles\"},{\"authorId\":\"1748257\",\"name\":\"Achim Rettinger\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"08e563367f61f2858b85be83668f3ff6224c2408\",\"title\":\"Partially Observable Markov Decision Processes with Behavioral Norms\",\"url\":\"https://www.semanticscholar.org/paper/08e563367f61f2858b85be83668f3ff6224c2408\",\"venue\":\"Normative Multi-Agent Systems\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786249\",\"name\":\"D. Bertsekas\"},{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":\"10.1109/CDC.1995.478953\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"23508659f45852d7385f29c15b33aa46d3a64cb5\",\"title\":\"Neuro-dynamic programming: an overview\",\"url\":\"https://www.semanticscholar.org/paper/23508659f45852d7385f29c15b33aa46d3a64cb5\",\"venue\":\"Proceedings of 1995 34th IEEE Conference on Decision and Control\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mark D Pendrith\"},{\"authorId\":null,\"name\":\"Michael J McGarity. An analysis of direct reinforcement le domains\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 421\\u2013429,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1016/j.tcs.2016.07.032\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c1fde6d42001146e3b97ff8cb59cc20af086933\",\"title\":\"Extreme state aggregation beyond Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/0c1fde6d42001146e3b97ff8cb59cc20af086933\",\"venue\":\"Theor. Comput. Sci.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144497046\",\"name\":\"N. Nilsson\"}],\"doi\":\"10.7551/mitpress/11723.003.0006\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b886f2c097b635ee9550ca29fff7dcbbb7727ff7\",\"title\":\"Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b886f2c097b635ee9550ca29fff7dcbbb7727ff7\",\"venue\":\"IFIP Congress\",\"year\":1974}],\"title\":\"On Q-learning Convergence for Non-Markov Decision Processes\",\"topics\":[{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"}],\"url\":\"https://www.semanticscholar.org/paper/66d76444255be0ac378a0a93ee0379fc721a386f\",\"venue\":\"IJCAI\",\"year\":2018}\n"