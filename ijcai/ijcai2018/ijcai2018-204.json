"{\"abstract\":\"As an important and challenging problem in artificial intelligence (AI) game playing, StarCraft micromanagement involves a dynamically adversarial game playing process with complex multi-agent control within a large action space. In this paper, we propose a novel knowledge-guided agent-tactic-aware learning scheme, that is, opponent-guided tactic learning (OGTL), to cope with this micromanagement problem. In principle, the proposed scheme takes a two-stage cascaded learning strategy which is capable of not only transferring the human tactic knowledge from the human-made opponent agents to our AI agents but also improving the adversarial ability. With the power of reinforcement learning, such a knowledge-guided agent-tactic-aware scheme has the ability to guide the AI agents to achieve a high winning-rate performance while accelerating the policy exploration process in a tactic-interpretable fashion. Experimental results demonstrate the effectiveness of the proposed scheme against the state-of-the-art approaches in several benchmark combat scenarios.\",\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Hu\",\"url\":null},{\"authorId\":null,\"name\":\"Juntao Li\",\"url\":null},{\"authorId\":\"50079147\",\"name\":\"X. Li\",\"url\":\"https://www.semanticscholar.org/author/50079147\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\",\"url\":\"https://www.semanticscholar.org/author/46452405\"},{\"authorId\":\"2285442\",\"name\":\"M. Xu\",\"url\":\"https://www.semanticscholar.org/author/2285442\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1901.04626\",\"authors\":[{\"authorId\":\"67097221\",\"name\":\"Liudmyla Nechepurenko\"},{\"authorId\":\"40645368\",\"name\":\"Viktor Voss\"},{\"authorId\":\"50138199\",\"name\":\"V. Gritsenko\"}],\"doi\":\"10.1007/978-3-030-61705-9_26\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d8c03fd1992b82ae9ad2acb2c49e024d9354a32a\",\"title\":\"Comparing Knowledge-based Reinforcement Learning to Neural Networks in a Strategy Game\",\"url\":\"https://www.semanticscholar.org/paper/d8c03fd1992b82ae9ad2acb2c49e024d9354a32a\",\"venue\":\"HAIS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122337224\",\"name\":\"B. Bernstein\"},{\"authorId\":\"123596918\",\"name\":\"Jasper C. M. Geurtz\"},{\"authorId\":\"2849401\",\"name\":\"V. J. Koeman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d3d1ce5285c9811b62354d2e5d73853839e06716\",\"title\":\"Evaluating the Effectiveness of Multi-Agent Organisational Paradigms in a Real-Time Strategy Environment: Engineering Multiagent Systems Track\",\"url\":\"https://www.semanticscholar.org/paper/d3d1ce5285c9811b62354d2e5d73853839e06716\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2200852\",\"name\":\"Peixi Peng\"},{\"authorId\":\"1757173\",\"name\":\"Junliang Xing\"},{\"authorId\":\"50260540\",\"name\":\"L. Cao\"},{\"authorId\":\"2448517\",\"name\":\"Lisen Mu\"},{\"authorId\":\"48908475\",\"name\":\"C. Huang\"}],\"doi\":\"10.24963/ijcai.2019/181\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"bbe4c6dd398780207b381c6e12900724fb98b7ee\",\"title\":\"Learning Deep Decentralized Policy Network by Collective Rewards for Real-Time Combat Game\",\"url\":\"https://www.semanticscholar.org/paper/bbe4c6dd398780207b381c6e12900724fb98b7ee\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1810.06339\",\"authors\":[{\"authorId\":\"2276894\",\"name\":\"Yuxi Li\"}],\"doi\":\"10.1201/9781351006620-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"title\":\"Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1812.01825\",\"authors\":[{\"authorId\":\"2200852\",\"name\":\"Peixi Peng\"},{\"authorId\":\"1757173\",\"name\":\"Junliang Xing\"},{\"authorId\":\"50047967\",\"name\":\"Lu Pang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"58e3ef2a4a9f5654b3ae31f42fc37230f99322b6\",\"title\":\"Cooperative Multi-Agent Policy Gradients with Sub-optimal Demonstration\",\"url\":\"https://www.semanticscholar.org/paper/58e3ef2a4a9f5654b3ae31f42fc37230f99322b6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2200852\",\"name\":\"Peixi Peng\"},{\"authorId\":\"1757173\",\"name\":\"Junliang Xing\"},{\"authorId\":\"50260540\",\"name\":\"L. Cao\"}],\"doi\":\"10.24963/ijcai.2020/420\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e6029a0c94d019a851162e6268fbaa3cfb57f040\",\"title\":\"Hybrid Learning for Multi-agent Cooperation with Sub-optimal Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/e6029a0c94d019a851162e6268fbaa3cfb57f040\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2186088\",\"name\":\"Tristan Charrier\"},{\"authorId\":\"151495603\",\"name\":\"S. Gamblin\"},{\"authorId\":\"3084788\",\"name\":\"Alexandre Niveau\"},{\"authorId\":\"3340580\",\"name\":\"Fran\\u00e7ois Schwarzentruber\"}],\"doi\":\"10.24963/ijcai.2019/934\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"202201b347e78bdfebc467a5b6b91dcbefb77e82\",\"title\":\"Hintikka's World: Scalable Higher-order Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/202201b347e78bdfebc467a5b6b91dcbefb77e82\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1902.04043\",\"authors\":[{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"51918914\",\"name\":\"Tim G. J. Rudner\"},{\"authorId\":\"1929828\",\"name\":\"Chia-Man Hung\"},{\"authorId\":\"143635540\",\"name\":\"P. Torr\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82055eed2ba0d7156a54c586249742c848e5d565\",\"title\":\"The StarCraft Multi-Agent Challenge\",\"url\":\"https://www.semanticscholar.org/paper/82055eed2ba0d7156a54c586249742c848e5d565\",\"venue\":\"AAMAS\",\"year\":2019}],\"corpusId\":51604576,\"doi\":\"10.24963/ijcai.2018/204\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"3416cb7c3462cfc52782c26900e912c46a2bd1c9\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1701.07274\",\"authors\":[{\"authorId\":\"2276894\",\"name\":\"Yuxi Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f1e9e56d80146766bc2316efbc54d8b770a23df\",\"title\":\"Deep Reinforcement Learning: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/9f1e9e56d80146766bc2316efbc54d8b770a23df\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1605.06676\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0772905d40b9afa3dc087a88184f09f3b3e1464f\",\"title\":\"Learning to Communicate with Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0772905d40b9afa3dc087a88184f09f3b3e1464f\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1504.00702\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6b8a1b80891c96c28cc6340267b58186157e536\",\"title\":\"End-to-End Training of Deep Visuomotor Policies\",\"url\":\"https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49614927\",\"name\":\"S. Wender\"},{\"authorId\":\"1809241\",\"name\":\"I. Watson\"}],\"doi\":\"10.1109/CIG.2012.6374183\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1308c8326203caec91a7c44ffd1dfe86dd227c7f\",\"title\":\"Applying reinforcement learning to small scale combat in the real-time strategy game StarCraft:Broodwar\",\"url\":\"https://www.semanticscholar.org/paper/1308c8326203caec91a7c44ffd1dfe86dd227c7f\",\"venue\":\"2012 IEEE Conference on Computational Intelligence and Games (CIG)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xuijun Li\"},{\"authorId\":null,\"name\":\"Yun-Nung Chen\"},{\"authorId\":null,\"name\":\"Lihong Li\"},{\"authorId\":null,\"name\":\"Jianfeng Gao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"End - to - end training of deep visuomotor policies\",\"url\":\"\",\"venue\":\"The Journal of Machine Learning Research\",\"year\":2016},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48057653\",\"name\":\"O. Bagasra\"}],\"doi\":\"10.1073/PNAS.95.17.10344-D\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"title\":\"PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES.\",\"url\":\"https://www.semanticscholar.org/paper/edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"venue\":\"Science\",\"year\":1915},{\"arxivId\":\"1702.08887\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"143635540\",\"name\":\"P. Torr\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0\",\"title\":\"Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3ac0fea1e5395cfb0dc1f0ee2b921fe22b23fed0\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1611.00625\",\"authors\":[{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"3082019\",\"name\":\"Alex Auvolat\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"47733973\",\"name\":\"Timoth\\u00e9e Lacroix\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"2127603\",\"name\":\"Florian Richoux\"},{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4c4076a4e3a1ea24f292e68dbb5eeb99e1ea35d\",\"title\":\"TorchCraft: a Library for Machine Learning Research on Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/d4c4076a4e3a1ea24f292e68dbb5eeb99e1ea35d\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46636184\",\"name\":\"P. Peng\"},{\"authorId\":\"145001851\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Chelsea Finn\"},{\"authorId\":null,\"name\":\"Trevor Darrell\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. End-to-end training of deep visuomotor policies\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):1334\\u20131373,\",\"year\":2016},{\"arxivId\":\"1605.07736\",\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"50295c19e177480ba3599300de1ab837cc62b08c\",\"title\":\"Learning Multiagent Communication with Backpropagation\",\"url\":\"https://www.semanticscholar.org/paper/50295c19e177480ba3599300de1ab837cc62b08c\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1712.07305\",\"authors\":[{\"authorId\":\"46601892\",\"name\":\"Xiangyu Kong\"},{\"authorId\":\"1894653\",\"name\":\"B. Xin\"},{\"authorId\":\"32324034\",\"name\":\"Fangchen Liu\"},{\"authorId\":\"36637369\",\"name\":\"Y. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd1ed642617c362acd072e70cf8c8ea229430b42\",\"title\":\"Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/fd1ed642617c362acd072e70cf8c8ea229430b42\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"},{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"38814203\",\"name\":\"Alberto Uriarte\"},{\"authorId\":\"2127603\",\"name\":\"Florian Richoux\"},{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1950379\",\"name\":\"M. Preuss\"}],\"doi\":\"10.1109/TCIAIG.2013.2286295\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"593487efa8e3c8657ad40104d237bdacf674fe53\",\"title\":\"A Survey of Real-Time Strategy Game AI Research and Competition in StarCraft\",\"url\":\"https://www.semanticscholar.org/paper/593487efa8e3c8657ad40104d237bdacf674fe53\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1703.01008\",\"authors\":[{\"authorId\":\"47058148\",\"name\":\"Xiujun Li\"},{\"authorId\":\"1725643\",\"name\":\"Yun-Nung (Vivian) Chen\"},{\"authorId\":\"37715548\",\"name\":\"L. Li\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"661965662c2d5d744e8f556e79122d1aa9d13197\",\"title\":\"End-to-End Task-Completion Neural Dialogue Systems\",\"url\":\"https://www.semanticscholar.org/paper/661965662c2d5d744e8f556e79122d1aa9d13197\",\"venue\":\"IJCNLP\",\"year\":2017},{\"arxivId\":\"1703.10069\",\"authors\":[{\"authorId\":\"144189270\",\"name\":\"P. Peng\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"145001852\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":\"95115833\",\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"76dfb1ab698963f3776fe894b3743db4a5419a5f\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets: Emergence of Human-level Coordination in Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/76dfb1ab698963f3776fe894b3743db4a5419a5f\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2449382\",\"name\":\"T. Hubert\"},{\"authorId\":\"144522726\",\"name\":\"L. Baker\"},{\"authorId\":\"40227832\",\"name\":\"Matthew Lai\"},{\"authorId\":\"34848283\",\"name\":\"A. Bolton\"},{\"authorId\":\"1519062204\",\"name\":\"Yutian Chen\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"88791868\",\"name\":\"F. Hui\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature24270\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c27db32efa8137cbf654902f8f728f338e55cd1c\",\"title\":\"Mastering the game of Go without human knowledge\",\"url\":\"https://www.semanticscholar.org/paper/c27db32efa8137cbf654902f8f728f338e55cd1c\",\"venue\":\"Nature\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"},{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"454d0168492917fdb87f2ed034686c8edd4cbe90\",\"title\":\"Episodic Exploration for Deep Deterministic Policies for StarCraft Micromanagement\",\"url\":\"https://www.semanticscholar.org/paper/454d0168492917fdb87f2ed034686c8edd4cbe90\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"48099761\",\"name\":\"K. Paliwal\"}],\"doi\":\"10.1109/78.650093\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e23c34414e66118ecd9b08cf0cd4d016f59b0b85\",\"title\":\"Bidirectional recurrent neural networks\",\"url\":\"https://www.semanticscholar.org/paper/e23c34414e66118ecd9b08cf0cd4d016f59b0b85\",\"venue\":\"IEEE Trans. Signal Process.\",\"year\":1997},{\"arxivId\":\"1609.02993\",\"authors\":[{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"},{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ddbeb2a1cf1a8a43beb7a775c3cdb080718f69a1\",\"title\":\"Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ddbeb2a1cf1a8a43beb7a775c3cdb080718f69a1\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1705.08926\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2b292ff89d808fba10579871591a22f1649cd039\",\"title\":\"Counterfactual Multi-Agent Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/2b292ff89d808fba10579871591a22f1649cd039\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143796510\",\"name\":\"L. S. Shapley\"}],\"doi\":\"10.1073/PNAS.39.10.1095\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"397ba11c5034891cd3f50406379e53ac4bd13f2c\",\"title\":\"Stochastic Games*\",\"url\":\"https://www.semanticscholar.org/paper/397ba11c5034891cd3f50406379e53ac4bd13f2c\",\"venue\":\"Proceedings of the National Academy of Sciences\",\"year\":1953},{\"arxivId\":\"1708.02300\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D17-1103\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"title\":\"Reinforced Video Captioning with Entailment Rewards\",\"url\":\"https://www.semanticscholar.org/paper/53bed2d3d75c4320ad5af4a85e31bf92e3c704ef\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"FaceCerts Darko Kirovski\"},{\"authorId\":\"1698689\",\"name\":\"N. Jojic\"}],\"doi\":\"10.1109/tsp.2004.824755\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a37293c27cd7cc31f0e6a3b7171cc826285def8e\",\"title\":\"Ieee Transactions on Signal Processing: Supplement on Secure Media 1 Facecerts Ieee Transactions on Signal Processing: Supplement on Secure Media 2\",\"url\":\"https://www.semanticscholar.org/paper/a37293c27cd7cc31f0e6a3b7171cc826285def8e\",\"venue\":\"\",\"year\":2003},{\"arxivId\":\"1611.03673\",\"authors\":[{\"authorId\":\"144705062\",\"name\":\"P. Mirowski\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"143740871\",\"name\":\"F. Viola\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"3577056\",\"name\":\"Andy Ballard\"},{\"authorId\":\"4194027\",\"name\":\"Andrea Banino\"},{\"authorId\":\"1715051\",\"name\":\"Misha Denil\"},{\"authorId\":\"2558463\",\"name\":\"R. Goroshin\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d35b05f440b5ba00d9429139edef7182bf9f7ce7\",\"title\":\"Learning to Navigate in Complex Environments\",\"url\":\"https://www.semanticscholar.org/paper/d35b05f440b5ba00d9429139edef7182bf9f7ce7\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113713833\",\"name\":\"S. J. Crawford\"},{\"authorId\":\"116971860\",\"name\":\"D. F. Kelley\"},{\"authorId\":\"137736247\",\"name\":\"In\\u00e9s Hern\\u00e1ndez \\u00c1vila\"}],\"doi\":\"10.1023/A:1017127612903\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"title\":\"Volume 1\",\"url\":\"https://www.semanticscholar.org/paper/6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"venue\":\"\",\"year\":1998}],\"title\":\"Knowledge-Guided Agent-Tactic-Aware Learning for StarCraft Micromanagement\",\"topics\":[{\"topic\":\"StarCraft\",\"topicId\":\"136807\",\"url\":\"https://www.semanticscholar.org/topic/136807\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Artificial intelligence\",\"topicId\":\"8286\",\"url\":\"https://www.semanticscholar.org/topic/8286\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"}],\"url\":\"https://www.semanticscholar.org/paper/3416cb7c3462cfc52782c26900e912c46a2bd1c9\",\"venue\":\"IJCAI\",\"year\":2018}\n"