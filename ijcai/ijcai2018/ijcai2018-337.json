"{\"abstract\":\"Reinforcement learning (RL) algorithms have made huge progress in recent years by leveraging the power of deep neural networks (DNN). Despite the success, deep RL algorithms are known to be sample inefficient, often requiring many rounds of interaction with the environments to obtain satisfactory performance. Recently, episodic memory based RL has attracted attention due to its ability to latch on good actions quickly. In this paper, we present a simple yet effective biologically inspired RL algorithm called Episodic Memory Deep Q-Networks (EMDQN), which leverages episodic memory to supervise an agent during training. Experiments show that our proposed method can lead to better sample efficiency and is more likely to find good policies. It only requires 1/5 of the interactions of DQN to achieve many state-of-the-art performances on Atari games, significantly outperforming regular DQN and other episodic memory based RL algorithms.\",\"arxivId\":\"1805.07603\",\"authors\":[{\"authorId\":\"41123614\",\"name\":\"Zichuan Lin\",\"url\":\"https://www.semanticscholar.org/author/41123614\"},{\"authorId\":\"49909972\",\"name\":\"Tianqi Zhao\",\"url\":\"https://www.semanticscholar.org/author/49909972\"},{\"authorId\":\"145789924\",\"name\":\"G. Yang\",\"url\":\"https://www.semanticscholar.org/author/145789924\"},{\"authorId\":\"38645922\",\"name\":\"L. Zhang\",\"url\":\"https://www.semanticscholar.org/author/38645922\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1909.09902\",\"authors\":[{\"authorId\":\"9329915\",\"name\":\"Pawel Ladosz\"},{\"authorId\":\"1407517292\",\"name\":\"E. Ben-Iwhiwhu\"},{\"authorId\":\"1409976585\",\"name\":\"Yang Hu\"},{\"authorId\":\"3268923\",\"name\":\"Nicholas Ketz\"},{\"authorId\":\"2062432\",\"name\":\"Soheil Kolouri\"},{\"authorId\":\"1753673\",\"name\":\"J. Krichmar\"},{\"authorId\":\"2888448\",\"name\":\"Praveen K. Pilly\"},{\"authorId\":\"2818113\",\"name\":\"A. Soltoggio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66663794098de46faea43bcbc22f79fbb0b70c31\",\"title\":\"Deep Reinforcement Learning with Modulated Hebbian plus Q Network Architecture\",\"url\":\"https://www.semanticscholar.org/paper/66663794098de46faea43bcbc22f79fbb0b70c31\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1903.00827\",\"authors\":[{\"authorId\":\"39539779\",\"name\":\"Zhizheng Zhang\"},{\"authorId\":\"14584879\",\"name\":\"J. Chen\"},{\"authorId\":\"143912275\",\"name\":\"Zhibo Chen\"},{\"authorId\":\"50135568\",\"name\":\"W. Li\"}],\"doi\":\"10.1109/tcyb.2019.2939174\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a9c324a86cfff5998402845e43e1bba4c42d209\",\"title\":\"Asynchronous Episodic Deep Deterministic Policy Gradient: Towards Continuous Control in Computationally Complex Environments\",\"url\":\"https://www.semanticscholar.org/paper/6a9c324a86cfff5998402845e43e1bba4c42d209\",\"venue\":\"IEEE transactions on cybernetics\",\"year\":2019},{\"arxivId\":\"1904.06736\",\"authors\":[{\"authorId\":\"37718331\",\"name\":\"Dhruv Ramani\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f5a06a8c701dec711844ab092b6b570f6751a9e1\",\"title\":\"A Short Survey On Memory Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f5a06a8c701dec711844ab092b6b570f6751a9e1\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1901.08004\",\"authors\":[{\"authorId\":\"50316498\",\"name\":\"Z. Zhang\"},{\"authorId\":\"49404032\",\"name\":\"Haozheng Li\"},{\"authorId\":\"41204443\",\"name\":\"L. Zhang\"},{\"authorId\":\"66561896\",\"name\":\"Tianyin Zheng\"},{\"authorId\":\"143691153\",\"name\":\"T. Zhang\"},{\"authorId\":\"40609445\",\"name\":\"Xiong Hao\"},{\"authorId\":\"144870404\",\"name\":\"Xiaoxin Chen\"},{\"authorId\":\"83359287\",\"name\":\"M. Chen\"},{\"authorId\":\"66199867\",\"name\":\"Fangxu Xiao\"},{\"authorId\":\"145172489\",\"name\":\"W. Zhou\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3f2893ebbdfab2dd8d87c565024577655095e54\",\"title\":\"Hierarchical Reinforcement Learning for Multi-agent MOBA Game\",\"url\":\"https://www.semanticscholar.org/paper/b3f2893ebbdfab2dd8d87c565024577655095e54\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"72658013\",\"name\":\"Dongchil Kim\"},{\"authorId\":\"9149547\",\"name\":\"Sungjoo Park\"}],\"doi\":\"10.1109/ICUFN.2019.8806045\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb09de4c76309bf335894e6dfb85b4e57c361c70\",\"title\":\"Enhanced Model-Free Deep-Q Network based PTZ Camera Control Method\",\"url\":\"https://www.semanticscholar.org/paper/eb09de4c76309bf335894e6dfb85b4e57c361c70\",\"venue\":\"2019 Eleventh International Conference on Ubiquitous and Future Networks (ICUFN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48924218\",\"name\":\"Guangxiang Zhu\"},{\"authorId\":\"41123614\",\"name\":\"Zichuan Lin\"},{\"authorId\":\"145789924\",\"name\":\"G. Yang\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8c3d0de141260c3344624aeef397af00c926b8f2\",\"title\":\"Episodic Reinforcement Learning with Associative Memory\",\"url\":\"https://www.semanticscholar.org/paper/8c3d0de141260c3344624aeef397af00c926b8f2\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1749791292\",\"name\":\"Dujia Yang\"},{\"authorId\":\"7461033\",\"name\":\"X. Qin\"},{\"authorId\":\"7925561\",\"name\":\"Xiaodong Xu\"},{\"authorId\":\"150059845\",\"name\":\"Chensheng Li\"},{\"authorId\":\"143659301\",\"name\":\"Guo Wei\"}],\"doi\":\"10.1109/ACCESS.2020.3009329\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f715558b65fd4f3c6966505c237d9a622947010b\",\"title\":\"Sample Efficient Reinforcement Learning Method via High Efficient Episodic Memory\",\"url\":\"https://www.semanticscholar.org/paper/f715558b65fd4f3c6966505c237d9a622947010b\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"1904.01790\",\"authors\":[{\"authorId\":\"35592630\",\"name\":\"Daichi Nishio\"},{\"authorId\":\"34989052\",\"name\":\"S. Yamane\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe46b8e91a30814c3813d1a1fe3ddfbaddc5ee3c\",\"title\":\"Random Projection in Neural Episodic Control\",\"url\":\"https://www.semanticscholar.org/paper/fe46b8e91a30814c3813d1a1fe3ddfbaddc5ee3c\",\"venue\":\"ACML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41123614\",\"name\":\"Zichuan Lin\"},{\"authorId\":\"9154316\",\"name\":\"L. Zhao\"},{\"authorId\":\"152441498\",\"name\":\"Jiang Bian\"},{\"authorId\":\"82620854\",\"name\":\"Tao Qin\"},{\"authorId\":\"145789924\",\"name\":\"G. Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5217d5a820712bb57c34790132bc904ea1d2d2a\",\"title\":\"Unified Policy Optimization for Robust Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d5217d5a820712bb57c34790132bc904ea1d2d2a\",\"venue\":\"ACML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144953653\",\"name\":\"Samuel J. Meyer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"30cf5e4952326f39a936a516ee7afa88450161ef\",\"title\":\"Opleiding Informatica Psychology-Inspired Memory Sampling in a Deep Q Network\",\"url\":\"https://www.semanticscholar.org/paper/30cf5e4952326f39a936a516ee7afa88450161ef\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1857914\",\"name\":\"Yijie Guo\"},{\"authorId\":\"2031471973\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"3009779\",\"name\":\"M. Moczulski\"},{\"authorId\":\"1600202282\",\"name\":\"Shengyu Feng\"},{\"authorId\":\"50328457\",\"name\":\"S. Bengio\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a28899f255c1ca35b6254adc1a0cd64fc20c2ce9\",\"title\":\"Memory Based Trajectory-conditioned Policies for Learning from Sparse Rewards\",\"url\":\"https://www.semanticscholar.org/paper/a28899f255c1ca35b6254adc1a0cd64fc20c2ce9\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2010.08891\",\"authors\":[{\"authorId\":\"1999953423\",\"name\":\"Aayam Shrestha\"},{\"authorId\":\"1607486000\",\"name\":\"Stefan Lee\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14e6502e4a63635ef01790471f45f369de72a1c0\",\"title\":\"DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs\",\"url\":\"https://www.semanticscholar.org/paper/14e6502e4a63635ef01790471f45f369de72a1c0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.01099\",\"authors\":[{\"authorId\":\"49191378\",\"name\":\"A. Subramanian\"},{\"authorId\":\"1389561411\",\"name\":\"Sharad Chitlangia\"},{\"authorId\":\"3180819\",\"name\":\"V. Baths\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eeae1c65a31ae935706d18404e0f73807d6678c0\",\"title\":\"Psychological and Neural Evidence for Reinforcement Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/eeae1c65a31ae935706d18404e0f73807d6678c0\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":29151151,\"doi\":\"10.24963/ijcai.2018/337\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"06b030f6a7d409e02cedd98321af3a675c011a88\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1611.01606\",\"authors\":[{\"authorId\":\"46463180\",\"name\":\"F. He\"},{\"authorId\":\"47909037\",\"name\":\"Yang Liu\"},{\"authorId\":\"2068227\",\"name\":\"Alexander G. Schwing\"},{\"authorId\":\"144439558\",\"name\":\"J. Peng\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"85d8b1b3483c7f4db999e7cf6b3e6231954c43dc\",\"title\":\"Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening\",\"url\":\"https://www.semanticscholar.org/paper/85d8b1b3483c7f4db999e7cf6b3e6231954c43dc\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mnih et al\"},{\"authorId\":null,\"name\":\"2015 Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A Rusu\"},{\"authorId\":null,\"name\":\"Joel Veness\"},{\"authorId\":null,\"name\":\"Marc G Bellemare\"},{\"authorId\":null,\"name\":\"Alex Graves\"},{\"authorId\":null,\"name\":\"Martin Riedmiller\"},{\"authorId\":null,\"name\":\"Andreas K Fidjeland\"},{\"authorId\":null,\"name\":\"Georg Ostrovski\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Human-level control through\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1703.01988\",\"authors\":[{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2825051\",\"name\":\"B. Uria\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"37088dec26231bc5a4937054ebc862bb83a3db4d\",\"title\":\"Neural Episodic Control\",\"url\":\"https://www.semanticscholar.org/paper/37088dec26231bc5a4937054ebc862bb83a3db4d\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Michael BoardmanAbstract\"}],\"doi\":\"10.2307/2311524\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c94e9ca95c12c405b261f06eb1950ca9793d5085\",\"title\":\"Contemporary Mathematics\",\"url\":\"https://www.semanticscholar.org/paper/c94e9ca95c12c405b261f06eb1950ca9793d5085\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hado Van Hasselt\"},{\"authorId\":null,\"name\":\"Arthur Guez\"},{\"authorId\":null,\"name\":\"David Silver. Deep reinforcement learning with double q-learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 2094\\u20132100,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40599065\",\"name\":\"M. Lengyel\"},{\"authorId\":\"1790646\",\"name\":\"P. Dayan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c8e337a12df57783edb75eace2b8d67270a6823c\",\"title\":\"Hippocampal Contributions to Control: The Third Way\",\"url\":\"https://www.semanticscholar.org/paper/c8e337a12df57783edb75eace2b8d67270a6823c\",\"venue\":\"NIPS\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1701.08878\",\"authors\":[{\"authorId\":\"32140696\",\"name\":\"Smruti Amarjyoti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df7c41dd4f7f0b52344078acf756a379d57f6bef\",\"title\":\"Deep Reinforcement Learning for Robotic Manipulation - The state of the art\",\"url\":\"https://www.semanticscholar.org/paper/df7c41dd4f7f0b52344078acf756a379d57f6bef\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling network architectures for deep reinforcement learning\",\"url\":\"\",\"venue\":\"Proceedings of The 33rd International Conference on Machine Learning\",\"year\":1992},{\"arxivId\":\"1606.02647\",\"authors\":[{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"3382781\",\"name\":\"Tom Stepleton\"},{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc3e905bfb27d21675ee1720413e007b014b37d3\",\"title\":\"Safe and Efficient Off-Policy Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/dc3e905bfb27d21675ee1720413e007b014b37d3\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"644a079073969a92674f69483c4a85679d066545\",\"title\":\"Double Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/644a079073969a92674f69483c4a85679d066545\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144524233\",\"name\":\"D. Gooch\"}],\"doi\":\"10.1145/2043236.2043240\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"951e4467820029305bd729887d7ab4b9a0772c4a\",\"title\":\"Communications of the ACM\",\"url\":\"https://www.semanticscholar.org/paper/951e4467820029305bd729887d7ab4b9a0772c4a\",\"venue\":\"XRDS\",\"year\":2011},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks\",\"url\":\"\",\"venue\":\"Faster deep reinforcement learning by optimality tightening\",\"year\":1984},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50537578\",\"name\":\"W. Johnson\"}],\"doi\":\"10.1090/conm/026/737400\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1d0635cda34b8af995313848a0c42bac6efe79ec\",\"title\":\"Extensions of Lipschitz mappings into Hilbert space\",\"url\":\"https://www.semanticscholar.org/paper/1d0635cda34b8af995313848a0c42bac6efe79ec\",\"venue\":\"\",\"year\":1984},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1831199\",\"name\":\"S. Gershman\"},{\"authorId\":\"1784997\",\"name\":\"N. Daw\"}],\"doi\":\"10.1146/annurev-psych-122414-033625\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d98ed6612f4d7efc3211131378d592de10bfc636\",\"title\":\"Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework\",\"url\":\"https://www.semanticscholar.org/paper/d98ed6612f4d7efc3211131378d592de10bfc636\",\"venue\":\"Annual review of psychology\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Long-H Lin. Self-improving reactive agents based on reinf learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"planning and teaching\",\"url\":\"\",\"venue\":\"Machine learning, 8(3/4):69\\u201397,\",\"year\":1992},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35022714\",\"name\":\"S. Girgin\"},{\"authorId\":\"33372443\",\"name\":\"M. Loth\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"34682317\",\"name\":\"P. Preux\"},{\"authorId\":\"1757258\",\"name\":\"D. Ryabko\"}],\"doi\":\"10.1007/978-3-540-89722-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"be70608051078bf1b132348cf1fb9cb2a79648a5\",\"title\":\"Recent Advances in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/be70608051078bf1b132348cf1fb9cb2a79648a5\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"3382781\",\"name\":\"Tom Stepleton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8b1d450a1cadfe4906231a6c35f7e149778df5bf\",\"title\":\"Q($\\\\lambda$) with Off-Policy Corrections\",\"url\":\"https://www.semanticscholar.org/paper/8b1d450a1cadfe4906231a6c35f7e149778df5bf\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S Sutton. Dyna\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"an integrated architecture for learning\",\"url\":\"\",\"venue\":\"planning, and reacting. ACM SIGART Bulletin, 2(4):160\\u2013163,\",\"year\":1991},{\"arxivId\":\"1510.09142\",\"authors\":[{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"37866714\",\"name\":\"Gregory Wayne\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6640f4e4beae786f301928d82a9f8eb037aa6935\",\"title\":\"Learning Continuous Control Policies by Stochastic Value Gradients\",\"url\":\"https://www.semanticscholar.org/paper/6640f4e4beae786f301928d82a9f8eb037aa6935\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145842423\",\"name\":\"C. Pennartz\"},{\"authorId\":\"40580968\",\"name\":\"R. Ito\"},{\"authorId\":\"144193091\",\"name\":\"P. Verschure\"},{\"authorId\":\"144273828\",\"name\":\"F. Battaglia\"},{\"authorId\":\"1854466\",\"name\":\"T. Robbins\"}],\"doi\":\"10.1016/j.tins.2011.08.001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e2a1acb572c8e9aae9b692234a9738765507a4ed\",\"title\":\"The hippocampal\\u2013striatal axis in learning, prediction and goal-directed behavior\",\"url\":\"https://www.semanticscholar.org/paper/e2a1acb572c8e9aae9b692234a9738765507a4ed\",\"venue\":\"Trends in Neurosciences\",\"year\":2011},{\"arxivId\":\"1606.04460\",\"authors\":[{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"2825051\",\"name\":\"B. Uria\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"3422141\",\"name\":\"Y. Li\"},{\"authorId\":\"144893251\",\"name\":\"Avraham Ruderman\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"34269227\",\"name\":\"Jack W. Rae\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ba378579fb44007db9f02699889721dcd2b5b3a0\",\"title\":\"Model-Free Episodic Control\",\"url\":\"https://www.semanticscholar.org/paper/ba378579fb44007db9f02699889721dcd2b5b3a0\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1825717826\",\"name\":\"G. BellemareMarc\"},{\"authorId\":\"1825717619\",\"name\":\"NaddafYavar\"},{\"authorId\":\"1825717841\",\"name\":\"VenessJoel\"},{\"authorId\":\"1644031986\",\"name\":\"BowlingMichael\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f3b7d98ce1d818cbf17b7944ef3d40a824cf35b\",\"title\":\"The arcade learning environment\",\"url\":\"https://www.semanticscholar.org/paper/5f3b7d98ce1d818cbf17b7944ef3d40a824cf35b\",\"venue\":\"\",\"year\":2013},{\"arxivId\":\"1603.00748\",\"authors\":[{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d358d41c69450b171327ebd99462b6afef687269\",\"title\":\"Continuous Deep Q-Learning with Model-based Acceleration\",\"url\":\"https://www.semanticscholar.org/paper/d358d41c69450b171327ebd99462b6afef687269\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144867807\",\"name\":\"S. Thrun\"},{\"authorId\":\"47035862\",\"name\":\"A. Schwartz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"26b8747eb4d7fb4d4fc45707606d5e969b9afb0c\",\"title\":\"Issues in Using Function Approximation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/26b8747eb4d7fb4d4fc45707606d5e969b9afb0c\",\"venue\":\"\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1007/BF00114726\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d0a7ebd3bc2d25deee869e8ef3dd80f9278607d\",\"title\":\"Reinforcement learning with replacing eligibility traces\",\"url\":\"https://www.semanticscholar.org/paper/5d0a7ebd3bc2d25deee869e8ef3dd80f9278607d\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sebastian Thrun\"},{\"authorId\":null,\"name\":\"Anton Schwartz. Issues in using function approximation f learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 1993 Connectionist Models Summer School Hillsdale\",\"url\":\"\",\"venue\":\"NJ. Lawrence Erlbaum,\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4682880\",\"name\":\"M. F. Balcan\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a0442c88be8f16e180d8e365235950d1f7dff436\",\"title\":\"Proceedings of the 33rd International Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/a0442c88be8f16e180d8e365235950d1f7dff436\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696575\",\"name\":\"J. Bentley\"}],\"doi\":\"10.1145/361002.361007\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc73bacd6a00442570d15e122604ad6862b8663d\",\"title\":\"Multidimensional binary search trees used for associative searching\",\"url\":\"https://www.semanticscholar.org/paper/cc73bacd6a00442570d15e122604ad6862b8663d\",\"venue\":\"CACM\",\"year\":1975},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398948869\",\"name\":\"Richard S. Sutton\"}],\"doi\":\"10.1145/122344.122377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"831edc3d67457db83da40d260e93bfd7559347ae\",\"title\":\"Dyna, an integrated architecture for learning, planning, and reacting\",\"url\":\"https://www.semanticscholar.org/paper/831edc3d67457db83da40d260e93bfd7559347ae\",\"venue\":\"SGAR\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1007/BF00992699\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"title\":\"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\"url\":\"https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"venue\":\"Machine Learning\",\"year\":2004}],\"title\":\"Episodic Memory Deep Q-Networks\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"},{\"topic\":\"Atari\",\"topicId\":\"20108\",\"url\":\"https://www.semanticscholar.org/topic/20108\"},{\"topic\":\"Performance\",\"topicId\":\"3097\",\"url\":\"https://www.semanticscholar.org/topic/3097\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"}],\"url\":\"https://www.semanticscholar.org/paper/06b030f6a7d409e02cedd98321af3a675c011a88\",\"venue\":\"IJCAI\",\"year\":2018}\n"