"{\"abstract\":\"Model-based strategies for control are critical to obtain sample efficient learning. Dyna is a planning paradigm that naturally interleaves learning and planning, by simulating one-step experience to update the action-value function. This elegant planning strategy has been mostly explored in the tabular setting. The aim of this paper is to revisit sample-based planning, in stochastic and continuous domains with learned models. We first highlight the flexibility afforded by a model over Experience Replay (ER). Replay-based methods can be seen as stochastic planning methods that repeatedly sample from a buffer of recent agent-environment interactions and perform updates to improve data efficiency. We show that a model, as opposed to a replay buffer, is particularly useful for specifying which states to sample from during planning, such as predecessor states that propagate information in reverse from a state more quickly. We introduce a semi-parametric model learning approach, called Reweighted Experience Models (REMs), that makes it simple to sample next states or predecessors. We demonstrate that REM-Dyna exhibits similar advantages over replay-based methods in learning in continuous state problems, and that the performance gap grows when moving to stochastic domains, of increasing size.\",\"arxivId\":\"1806.04624\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\",\"url\":\"https://www.semanticscholar.org/author/7303313\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\",\"url\":\"https://www.semanticscholar.org/author/145602215\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\",\"url\":\"https://www.semanticscholar.org/author/145240145\"},{\"authorId\":\"145690938\",\"name\":\"Andrew Patterson\",\"url\":\"https://www.semanticscholar.org/author/145690938\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\",\"url\":\"https://www.semanticscholar.org/author/144542337\"}],\"citationVelocity\":7,\"citations\":[{\"arxivId\":\"1912.03851\",\"authors\":[{\"authorId\":\"1452681131\",\"name\":\"Ujwal Padam Tewari\"},{\"authorId\":\"1452681997\",\"name\":\"Vishal Bidawatka\"},{\"authorId\":\"3107607\",\"name\":\"Varsha Raveendran\"},{\"authorId\":\"47725683\",\"name\":\"V. Sudhakaran\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dad2ad9598cbace92c6d81dd7865a15c235819b3\",\"title\":\"Intelligent Coordination among Multiple Traffic Intersections Using Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/dad2ad9598cbace92c6d81dd7865a15c235819b3\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1807.05827\",\"authors\":[{\"authorId\":\"2082029\",\"name\":\"G. Novati\"},{\"authorId\":\"1802604\",\"name\":\"P. Koumoutsakos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0eefcf9b702b50b7c47300736c5ee0080a1639db\",\"title\":\"Remember and Forget for Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/0eefcf9b702b50b7c47300736c5ee0080a1639db\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"2006.04363\",\"authors\":[{\"authorId\":\"1419479157\",\"name\":\"Taher Jafferjee\"},{\"authorId\":\"29905816\",\"name\":\"Ehsan Imani\"},{\"authorId\":\"1739814743\",\"name\":\"Erin J. Talvitie\"},{\"authorId\":\"114860989\",\"name\":\"M. White\"},{\"authorId\":\"1739899695\",\"name\":\"Micheal Bowling\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"3d2cc26da7001b2ecb711c431e5e31e081792caf\",\"title\":\"Hallucinating Value: A Pitfall of Dyna-style Planning with Imperfect Environment Models\",\"url\":\"https://www.semanticscholar.org/paper/3d2cc26da7001b2ecb711c431e5e31e081792caf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1806.01825\",\"authors\":[{\"authorId\":\"34454457\",\"name\":\"G. Holland\"},{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a1bde6e920ec93d1529a8d60d334c0606a0d79f\",\"title\":\"The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7a1bde6e920ec93d1529a8d60d334c0606a0d79f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2007.09569\",\"authors\":[{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"},{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"114860989\",\"name\":\"M. White\"},{\"authorId\":\"5689899\",\"name\":\"Amir-massoud Farahmand\"},{\"authorId\":\"40609469\",\"name\":\"Hengshuai Yao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2dd22a920c224a130979e0ad4ceec7f28b4e7c03\",\"title\":\"Beyond Prioritized Replay: Sampling States in Model-Based RL via Simulated Priorities\",\"url\":\"https://www.semanticscholar.org/paper/2dd22a920c224a130979e0ad4ceec7f28b4e7c03\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.13490\",\"authors\":[{\"authorId\":\"38562041\",\"name\":\"Khimya Khetarpal\"},{\"authorId\":\"40497459\",\"name\":\"M. Riemer\"},{\"authorId\":\"113766340\",\"name\":\"I. Rish\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28d988a4eac1d714234036e351f417a535fe49b2\",\"title\":\"Towards Continual Reinforcement Learning: A Review and Perspectives\",\"url\":\"https://www.semanticscholar.org/paper/28d988a4eac1d714234036e351f417a535fe49b2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1904.01139\",\"authors\":[{\"authorId\":\"3403061\",\"name\":\"Yannick Schroecker\"},{\"authorId\":\"46196063\",\"name\":\"Mel Vecer\\u00edk\"},{\"authorId\":\"36881095\",\"name\":\"Jonathan Scholz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6eae428eed3e5f1965a14dd2c26acf5400df473\",\"title\":\"Generative predecessor models for sample-efficient imitation learning\",\"url\":\"https://www.semanticscholar.org/paper/b6eae428eed3e5f1965a14dd2c26acf5400df473\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"2011.13093\",\"authors\":[{\"authorId\":\"98258868\",\"name\":\"Sang-Hwa Lee\"},{\"authorId\":\"102523405\",\"name\":\"J. Lee\"},{\"authorId\":\"1688962\",\"name\":\"I. Hasuo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66f5ff2e794bb6e29c3047941a979bea6fb571c5\",\"title\":\"Predictive PER: Balancing Priority and Diversity towards Stable Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/66f5ff2e794bb6e29c3047941a979bea6fb571c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.05243\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22ade45a75c1ce8ae63feae8d5381316493cefe8\",\"title\":\"When to use parametric models in reinforcement learning?\",\"url\":\"https://www.semanticscholar.org/paper/22ade45a75c1ce8ae63feae8d5381316493cefe8\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10343823\",\"name\":\"Ignacio Carlucho\"},{\"authorId\":\"145825862\",\"name\":\"M. D. Paula\"},{\"authorId\":\"145095913\",\"name\":\"G. G. Acosta\"}],\"doi\":\"10.1016/J.ESWA.2019.06.066\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"24f9cec3c63d0a37a709bbccdf55c55a7770614d\",\"title\":\"Double Q-PID algorithm for mobile robot control\",\"url\":\"https://www.semanticscholar.org/paper/24f9cec3c63d0a37a709bbccdf55c55a7770614d\",\"venue\":\"Expert Syst. Appl.\",\"year\":2019},{\"arxivId\":\"2007.06700\",\"authors\":[{\"authorId\":\"26958176\",\"name\":\"W. Fedus\"},{\"authorId\":\"3377142\",\"name\":\"Prajit Ramachandran\"},{\"authorId\":\"147866847\",\"name\":\"Rishabh Agarwal\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"144845452\",\"name\":\"M. Rowland\"},{\"authorId\":\"1484070054\",\"name\":\"Will Dabney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"517498898024107a75f5a4ea84f4129e300af603\",\"title\":\"Revisiting Fundamentals of Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/517498898024107a75f5a4ea84f4129e300af603\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"2002.05822\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"},{\"authorId\":\"5689899\",\"name\":\"Amir-massoud Farahmand\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"fe1b8c54eb4a53a4893acd8779cf17fadefde9e5\",\"title\":\"Frequency-based Search-control in Dyna\",\"url\":\"https://www.semanticscholar.org/paper/fe1b8c54eb4a53a4893acd8779cf17fadefde9e5\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2010.13685\",\"authors\":[{\"authorId\":\"2003609520\",\"name\":\"Veronica Chelu\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a64277b0ffb2c47450e7b22406e83c8ff5ce8247\",\"title\":\"Forethought and Hindsight in Credit Assignment\",\"url\":\"https://www.semanticscholar.org/paper/a64277b0ffb2c47450e7b22406e83c8ff5ce8247\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1907.04543\",\"authors\":[{\"authorId\":\"29767024\",\"name\":\"Rishabh Agarwal\"},{\"authorId\":\"50319359\",\"name\":\"D. Schuurmans\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0367c44eb9447c1f482e059c4c5a5708109c966b\",\"title\":\"An Optimistic Perspective on Offline Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0367c44eb9447c1f482e059c4c5a5708109c966b\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144704982\",\"name\":\"Y. Wan\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"49c2b28f762ada831588bdf1bef3c3cba46022ed\",\"title\":\"Model-based Reinforcement Learning with Non-linear Expectation Models and Stochastic Environments\",\"url\":\"https://www.semanticscholar.org/paper/49c2b28f762ada831588bdf1bef3c3cba46022ed\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8606501\",\"name\":\"A. Sharma\"},{\"authorId\":\"153852211\",\"name\":\"M. Pal\"},{\"authorId\":\"143660674\",\"name\":\"S. Anand\"},{\"authorId\":\"25374912\",\"name\":\"Sanjit K. Kaul\"}],\"doi\":\"10.1109/BigMM50055.2020.00029\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8d0e8b26a69af477622db792e7d4b99fd55aa142\",\"title\":\"Stratified Sampling Based Experience Replay for Efficient Camera Selection Decisions\",\"url\":\"https://www.semanticscholar.org/paper/8d0e8b26a69af477622db792e7d4b99fd55aa142\",\"venue\":\"2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"eb5f4f0cb8503beefdf8a5efd5061f61cc64764e\",\"title\":\"F REQUENCY-BASED S EARCH-CONTROL IN D YNA\",\"url\":\"https://www.semanticscholar.org/paper/eb5f4f0cb8503beefdf8a5efd5061f61cc64764e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1904.01191\",\"authors\":[{\"authorId\":\"144704982\",\"name\":\"Yi Wan\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.24963/ijcai.2019/506\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5d014e75d60340a952101b8cbc0e440cc8580873\",\"title\":\"Planning with Expectation Models\",\"url\":\"https://www.semanticscholar.org/paper/5d014e75d60340a952101b8cbc0e440cc8580873\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1906.07791\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"40609469\",\"name\":\"Hengshuai Yao\"},{\"authorId\":\"5689899\",\"name\":\"Amir-massoud Farahmand\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":\"10.24963/ijcai.2019/445\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"d6a114f9a96e6f893bf6b368f18a944b3e423b74\",\"title\":\"Hill Climbing on Value Estimates for Search-control in Dyna\",\"url\":\"https://www.semanticscholar.org/paper/d6a114f9a96e6f893bf6b368f18a944b3e423b74\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"1906.08387\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"3364022\",\"name\":\"K. Zhou\"},{\"authorId\":\"121774683\",\"name\":\"X. Hu\"}],\"doi\":\"10.24963/ijcai.2019/589\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"title\":\"Experience Replay Optimization\",\"url\":\"https://www.semanticscholar.org/paper/2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"venue\":\"IJCAI\",\"year\":2019}],\"corpusId\":48354217,\"doi\":\"10.24963/ijcai.2018/666\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dirk Ormoneit\"},{\"authorId\":null,\"name\":\"\\u015aaunak Sen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"KernelBased Reinforcement Learning\",\"url\":\"\",\"venue\":\"Machine Learning,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145042015\",\"name\":\"M. McCloskey\"},{\"authorId\":\"48909510\",\"name\":\"N. J. Cohen\"}],\"doi\":\"10.1016/S0079-7421(08)60536-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c213af6582c0d518a6e8e14217611c733eeb1ef1\",\"title\":\"Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem\",\"url\":\"https://www.semanticscholar.org/paper/c213af6582c0d518a6e8e14217611c733eeb1ef1\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"I J Goodfellow\"},{\"authorId\":null,\"name\":\"J Pouget-Abadie\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"MehMdi Mirza\",\"url\":\"\",\"venue\":\"B Xu, D Warde-Farley, S Ozair, A C Courville, and Y Bengio. Generative Adversarial Nets. In Advances in Neural Information Processing Systems\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2592414\",\"name\":\"Harm Vanseijen\"},{\"authorId\":\"47549459\",\"name\":\"Rich Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22dc2f3a4afea29ce76fb02a156943afadd6cbd7\",\"title\":\"A Deeper Look at Planning as Learning from Replay\",\"url\":\"https://www.semanticscholar.org/paper/22dc2f3a4afea29ce76fb02a156943afadd6cbd7\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"1404459229\",\"name\":\"J. Shawe-Taylor\"},{\"authorId\":\"36006960\",\"name\":\"Ronnie Stafford\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5ddd28f0370aa8d2a322a329b1373344aaea9ace\",\"title\":\"Compressed Conditional Mean Embeddings for Model-Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5ddd28f0370aa8d2a322a329b1373344aaea9ace\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145304454\",\"name\":\"M. Schlegel\"},{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"2809410\",\"name\":\"Jiecao Chen\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"485fab5085e689f3301935a4e57dfa4762bbdad0\",\"title\":\"Adapting Kernel Representations Online Using Submodular Maximization\",\"url\":\"https://www.semanticscholar.org/paper/485fab5085e689f3301935a4e57dfa4762bbdad0\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1503.05571\",\"authors\":[{\"authorId\":\"1815021\",\"name\":\"G. Alain\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"145095579\",\"name\":\"L. Yao\"},{\"authorId\":\"2965424\",\"name\":\"J. Yosinski\"},{\"authorId\":\"1398746441\",\"name\":\"Eric Thibodeau-Laufer\"},{\"authorId\":\"35097114\",\"name\":\"Saizheng Zhang\"},{\"authorId\":\"145467703\",\"name\":\"P. Vincent\"}],\"doi\":\"10.1093/IMAIAI/IAW003\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"94618b949f8f18a18f7289dc742162996d376433\",\"title\":\"GSNs : Generative Stochastic Networks\",\"url\":\"https://www.semanticscholar.org/paper/94618b949f8f18a18f7289dc742162996d376433\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1602.06346\",\"authors\":[{\"authorId\":\"3429927\",\"name\":\"B. Pires\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8a9c12060a8f7d6c1f17948424b306ec9bd685c\",\"title\":\"Policy Error Bounds for Model-Based Reinforcement Learning with Factored Linear Models\",\"url\":\"https://www.semanticscholar.org/paper/e8a9c12060a8f7d6c1f17948424b306ec9bd685c\",\"venue\":\"COLT\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2440747\",\"name\":\"R. French\"}],\"doi\":\"10.1016/S1364-6613(99)01294-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2722b9e5ab8da95f03e578bb65879c452c105385\",\"title\":\"Catastrophic forgetting in connectionist networks\",\"url\":\"https://www.semanticscholar.org/paper/2722b9e5ab8da95f03e578bb65879c452c105385\",\"venue\":\"Trends in Cognitive Sciences\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1007/BF00992699\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"title\":\"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\"url\":\"https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"52cc45c96a5c94c9c3464a4466ebdf1eb5f2a9b3\",\"title\":\"Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/52cc45c96a5c94c9c3464a4466ebdf1eb5f2a9b3\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689289\",\"name\":\"A. Barreto\"},{\"authorId\":\"2907660\",\"name\":\"Rafael L. Beirigo\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e479e8738b153b20fc2e49692d3d3a0297299c5\",\"title\":\"Incremental Stochastic Factorization for Online Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e479e8738b153b20fc2e49692d3d3a0297299c5\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"08883c7ca6a500342975e01cc5efc4b45e43ebec\",\"title\":\"Learning of Non-Parametric Control Policies with High-Dimensional State Features\",\"url\":\"https://www.semanticscholar.org/paper/08883c7ca6a500342975e01cc5efc4b45e43ebec\",\"venue\":\"AISTATS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1603.00748\",\"authors\":[{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d358d41c69450b171327ebd99462b6afef687269\",\"title\":\"Continuous Deep Q-Learning with Model-based Acceleration\",\"url\":\"https://www.semanticscholar.org/paper/d358d41c69450b171327ebd99462b6afef687269\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Long-Ji Lin. Self-Improving Reactive Agents Based On Reinf Learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Planning and Teaching\",\"url\":\"\",\"venue\":\"Machine Learning,\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689289\",\"name\":\"A. Barreto\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66d548ade9f5f829bb409a58d13a1831f5ff72c1\",\"title\":\"Reinforcement Learning using Kernel-Based Stochastic Factorization\",\"url\":\"https://www.semanticscholar.org/paper/66d548ade9f5f829bb409a58d13a1831f5ff72c1\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1729571\",\"name\":\"Kihyuk Sohn\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"3084614\",\"name\":\"Xinchen Yan\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f25e17eb717e5894e0404ea634451332f85d287\",\"title\":\"Learning Structured Output Representation using Deep Conditional Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/3f25e17eb717e5894e0404ea634451332f85d287\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1612.06018\",\"authors\":[{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ea22d6190b1ae38fefd391e3d6c48d8807d72626\",\"title\":\"Self-Correcting Models for Model-Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ea22d6190b1ae38fefd391e3d6c48d8807d72626\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1406.3332\",\"authors\":[{\"authorId\":\"2599292\",\"name\":\"J. Mairal\"},{\"authorId\":\"2155775\",\"name\":\"Piotr Koniusz\"},{\"authorId\":\"1753355\",\"name\":\"Z. Harchaoui\"},{\"authorId\":\"2462253\",\"name\":\"C. Schmid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6eee8041415facc9823d6b93ddb67a3773a3e1e3\",\"title\":\"Convolutional Kernel Networks\",\"url\":\"https://www.semanticscholar.org/paper/6eee8041415facc9823d6b93ddb67a3773a3e1e3\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2882882\",\"name\":\"W. Wolovich\"}],\"doi\":\"10.1007/978-1-4612-6392-0_3\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c3b7cb5b960d36dae507ac422f805f342cfd2ff8\",\"title\":\"The State Space\",\"url\":\"https://www.semanticscholar.org/paper/c3b7cb5b960d36dae507ac422f805f342cfd2ff8\",\"venue\":\"\",\"year\":1974},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760402\",\"name\":\"A. Moore\"},{\"authorId\":\"8483722\",\"name\":\"C. Atkeson\"}],\"doi\":\"10.1007/BF00993104\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74921bc8762812345b7010746faa22988b85252e\",\"title\":\"Prioritized sweeping: Reinforcement learning with less data and less time\",\"url\":\"https://www.semanticscholar.org/paper/74921bc8762812345b7010746faa22988b85252e\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1206.4655\",\"authors\":[{\"authorId\":\"1688482\",\"name\":\"Steffen Gr\\u00fcnew\\u00e4lder\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"36235599\",\"name\":\"Luca Baldassarre\"},{\"authorId\":\"1704699\",\"name\":\"M. Pontil\"},{\"authorId\":\"1708497\",\"name\":\"A. Gretton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba31e44c8e5df7f899e4e497c56e17dd5476d85a\",\"title\":\"Modelling transition dynamics in MDPs with RKHS embeddings\",\"url\":\"https://www.semanticscholar.org/paper/ba31e44c8e5df7f899e4e497c56e17dd5476d85a\",\"venue\":\"ICML\",\"year\":2012},{\"arxivId\":\"1312.6211\",\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"48499418\",\"name\":\"Xia Da\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0407b605b8f55db72e2545586bfe8e946b691b70\",\"title\":\"An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0407b605b8f55db72e2545586bfe8e946b691b70\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"1753432\",\"name\":\"J. Schneider\"}],\"doi\":\"10.1109/ROBOT.2001.932842\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"84b23b154ef3083839a4da8c460a1e1c110ea63b\",\"title\":\"Autonomous helicopter control using reinforcement learning policy search methods\",\"url\":\"https://www.semanticscholar.org/paper/84b23b154ef3083839a4da8c460a1e1c110ea63b\",\"venue\":\"Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164)\",\"year\":2001},{\"arxivId\":\"1206.3285\",\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"1979505\",\"name\":\"A. Geramifard\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"eaf557c627d441deb0b4b5e033a3a4f5518f4715\",\"title\":\"Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping\",\"url\":\"https://www.semanticscholar.org/paper/eaf557c627d441deb0b4b5e033a3a4f5518f4715\",\"venue\":\"UAI\",\"year\":2008},{\"arxivId\":\"1206.5278\",\"authors\":[{\"authorId\":\"35008797\",\"name\":\"M. Holmes\"},{\"authorId\":\"1703070\",\"name\":\"Alexander G. Gray\"},{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"}],\"doi\":\"10.5555/3020488.3020510\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"15b57f983a2676538f850591fdb9695ac607dd69\",\"title\":\"Fast Nonparametric Conditional Density Estimation\",\"url\":\"https://www.semanticscholar.org/paper/15b57f983a2676538f850591fdb9695ac607dd69\",\"venue\":\"UAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/j.jcss.2007.08.009\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"237a1cf18ed83bb3ad852b34f443c6c1ff3336c1\",\"title\":\"An analysis of model-based Interval Estimation for Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/237a1cf18ed83bb3ad852b34f443c6c1ff3336c1\",\"venue\":\"J. Comput. Syst. Sci.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681967\",\"name\":\"Branislav Kveton\"},{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"326620406d4ea97d5b1f2e9ff47f04ebb01f438b\",\"title\":\"Kernel-Based Reinforcement Learning on Representative States\",\"url\":\"https://www.semanticscholar.org/paper/326620406d4ea97d5b1f2e9ff47f04ebb01f438b\",\"venue\":\"AAAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2261881\",\"name\":\"M. Deisenroth\"},{\"authorId\":\"3472959\",\"name\":\"C. Rasmussen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"60b7d47758a71978e74edff6dd8dea4d9c791d7a\",\"title\":\"PILCO: A Model-Based and Data-Efficient Approach to Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/60b7d47758a71978e74edff6dd8dea4d9c791d7a\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689289\",\"name\":\"A. Barreto\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":\"10.1613/jair.4301\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9d50b0cb6b78c40247e04f5f1ddfb6e28758e0e2\",\"title\":\"Policy Iteration Based on Stochastic Factorization\",\"url\":\"https://www.semanticscholar.org/paper/9d50b0cb6b78c40247e04f5f1ddfb6e28758e0e2\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47918185\",\"name\":\"Jing Peng\"},{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1177/105971239300100403\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c5fc10768d83ba42a360d861eb96d79fec5d52f4\",\"title\":\"Efficient Learning and Planning Within the Dyna Framework\",\"url\":\"https://www.semanticscholar.org/paper/c5fc10768d83ba42a360d861eb96d79fec5d52f4\",\"venue\":\"Adapt. Behav.\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40609469\",\"name\":\"Hengshuai Yao\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"3429927\",\"name\":\"B. Pires\"},{\"authorId\":\"31513325\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1109/ADPRL.2014.7010633\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7846349200936bee0a178c334c2f533bcf538eaa\",\"title\":\"Pseudo-MDPs and factored linear action models\",\"url\":\"https://www.semanticscholar.org/paper/7846349200936bee0a178c334c2f533bcf538eaa\",\"venue\":\"2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)\",\"year\":2014},{\"arxivId\":\"1609.01995\",\"authors\":[{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d761eb8c4385388dab330a6b0b986c9791783fe\",\"title\":\"Unifying Task Specification in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1d761eb8c4385388dab330a6b0b986c9791783fe\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1312.0286\",\"authors\":[{\"authorId\":\"49437682\",\"name\":\"William L. Hamilton\"},{\"authorId\":\"2852410\",\"name\":\"Mahdi Milani Fard\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d20f8ac93ef57f8d49b7d35bc699dbd0f23de5d1\",\"title\":\"Efficient learning and planning with compressed predictive states\",\"url\":\"https://www.semanticscholar.org/paper/d20f8ac93ef57f8d49b7d35bc699dbd0f23de5d1\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2014}],\"title\":\"Organizing Experience: a Deeper Look at Replay Mechanisms for Sample-Based Planning in Continuous State Domains\",\"topics\":[{\"topic\":\"Parametric model\",\"topicId\":\"40004\",\"url\":\"https://www.semanticscholar.org/topic/40004\"},{\"topic\":\"Semiparametric model\",\"topicId\":\"226048\",\"url\":\"https://www.semanticscholar.org/topic/226048\"},{\"topic\":\"MicroWorlds\",\"topicId\":\"9452\",\"url\":\"https://www.semanticscholar.org/topic/9452\"},{\"topic\":\"Semiconductor industry\",\"topicId\":\"76540\",\"url\":\"https://www.semanticscholar.org/topic/76540\"},{\"topic\":\"Linear model\",\"topicId\":\"41514\",\"url\":\"https://www.semanticscholar.org/topic/41514\"},{\"topic\":\"Coefficient\",\"topicId\":\"3129\",\"url\":\"https://www.semanticscholar.org/topic/3129\"},{\"topic\":\"Table (information)\",\"topicId\":\"200\",\"url\":\"https://www.semanticscholar.org/topic/200\"},{\"topic\":\"Network model\",\"topicId\":\"20353\",\"url\":\"https://www.semanticscholar.org/topic/20353\"},{\"topic\":\"Learning rule\",\"topicId\":\"42269\",\"url\":\"https://www.semanticscholar.org/topic/42269\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Programming paradigm\",\"topicId\":\"29522\",\"url\":\"https://www.semanticscholar.org/topic/29522\"},{\"topic\":\"Simulation\",\"topicId\":\"194\",\"url\":\"https://www.semanticscholar.org/topic/194\"},{\"topic\":\"Erd\\u0151s\\u2013R\\u00e9nyi model\",\"topicId\":\"327073\",\"url\":\"https://www.semanticscholar.org/topic/327073\"},{\"topic\":\"Organizing (structure)\",\"topicId\":\"7824\",\"url\":\"https://www.semanticscholar.org/topic/7824\"}],\"url\":\"https://www.semanticscholar.org/paper/ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"venue\":\"IJCAI\",\"year\":2018}\n"