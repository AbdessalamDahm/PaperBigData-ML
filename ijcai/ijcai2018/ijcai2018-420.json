"{\"abstract\":\"In deep reinforcement learning (RL) tasks, an efficient exploration mechanism should be able to encourage an agent to take actions that lead to less frequent states which may yield higher accumulative future return. However, both knowing about the future and evaluating the frequentness of states are non-trivial tasks, especially for deep RL domains, where a state is represented by high-dimensional image frames. In this paper, we propose a novel informed exploration framework for deep RL, where we build the capability for an RL agent to predict over the future transitions and evaluate the frequentness for the predicted future frames in a meaningful manner. To this end, we train a deep prediction model to predict future frames given a state-action pair, and a convolutional autoencoder model to hash over the seen frames. In addition, to utilize the counts derived from the seen frames to evaluate the frequentness for the predicted frames, we tackle the challenge of matching the predicted future frames and their corresponding seen frames at the latent feature level. In this way, we derive a reliable metric for evaluating the novelty of the future direction pointed by each action, and hence inform the agent to explore the least frequent one.\",\"arxivId\":\"1707.00524\",\"authors\":[{\"authorId\":\"50086584\",\"name\":\"Haiyan Yin\",\"url\":\"https://www.semanticscholar.org/author/50086584\"},{\"authorId\":\"3359642\",\"name\":\"J. Chen\",\"url\":\"https://www.semanticscholar.org/author/3359642\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\",\"url\":\"https://www.semanticscholar.org/author/1746914\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"152782062\",\"name\":\"Xin-Qiang Cai\"},{\"authorId\":\"22424919\",\"name\":\"Yao-Xiang Ding\"},{\"authorId\":\"2192443\",\"name\":\"Yuan Jiang\"},{\"authorId\":\"145624000\",\"name\":\"Zhi-Hua Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9fd69b5eeee945c12ff8efde85a6fe8be34d784f\",\"title\":\"Expert-Level Atari Imitation Learning from Demonstrations Only\",\"url\":\"https://www.semanticscholar.org/paper/9fd69b5eeee945c12ff8efde85a6fe8be34d784f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35273604\",\"name\":\"C. Sun\"},{\"authorId\":\"15328841\",\"name\":\"Renmin Wang\"},{\"authorId\":\"8140131\",\"name\":\"R. Li\"},{\"authorId\":\"100551849\",\"name\":\"Jiao Wu\"},{\"authorId\":\"47027378\",\"name\":\"X. Hu\"}],\"doi\":\"10.1109/IJCNN.2019.8852234\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"12e9394725a085a89114d2c981ce4a76ec6f37cf\",\"title\":\"Efficient and Scalable Exploration via Estimation-Error\",\"url\":\"https://www.semanticscholar.org/paper/12e9394725a085a89114d2c981ce4a76ec6f37cf\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":\"1905.04819\",\"authors\":[{\"authorId\":\"15394275\",\"name\":\"Yilun Du\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c31f9302a48b8c40fe89650a154ad58e5103894b\",\"title\":\"Task-Agnostic Dynamics Priors for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c31f9302a48b8c40fe89650a154ad58e5103894b\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"2011.11693\",\"authors\":[{\"authorId\":\"39534622\",\"name\":\"Rika Antonova\"},{\"authorId\":\"3134625\",\"name\":\"A. Varava\"},{\"authorId\":\"40917932\",\"name\":\"P. Shi\"},{\"authorId\":\"1634990988\",\"name\":\"J. F. Carvalho\"},{\"authorId\":\"1731490\",\"name\":\"D. Kragic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aed1641c003f0726fa55840addf1243427553b04\",\"title\":\"Sequential Topological Representations for Predictive Models of Deformable Objects\",\"url\":\"https://www.semanticscholar.org/paper/aed1641c003f0726fa55840addf1243427553b04\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":13958363,\"doi\":\"10.24963/ijcai.2018/420\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"ea10749c0ef01294d5beef9a5c901d899a2e1f6e\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1705.05363\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"33932184\",\"name\":\"Pulkit Agrawal\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPRW.2017.70\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"225ab689f41cef1dc18237ef5dab059a49950abf\",\"title\":\"Curiosity-Driven Exploration by Self-Supervised Prediction\",\"url\":\"https://www.semanticscholar.org/paper/225ab689f41cef1dc18237ef5dab059a49950abf\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1703.01310\",\"authors\":[{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"12f67fb182bc934fc95ce97acff553d83e2ca72e\",\"title\":\"Count-Based Exploration with Neural Density Models\",\"url\":\"https://www.semanticscholar.org/paper/12f67fb182bc934fc95ce97acff553d83e2ca72e\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1696373\",\"name\":\"N. Chentanez\"}],\"doi\":\"10.21236/ada440280\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"12d6fde053e2c7174a76fe1bbdb97dd039a3b662\",\"title\":\"Intrinsically Motivated Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/12d6fde053e2c7174a76fe1bbdb97dd039a3b662\",\"venue\":\"NIPS\",\"year\":2004},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145557639\",\"name\":\"V. Nair\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a538b05ebb01a40323997629e171c91aa28b8e2f\",\"title\":\"Rectified Linear Units Improve Restricted Boltzmann Machines\",\"url\":\"https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":\"1511.06342\",\"authors\":[{\"authorId\":\"3166516\",\"name\":\"Emilio Parisotto\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"title\":\"Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Junhyuk Oh\"},{\"authorId\":null,\"name\":\"Xiaoxiao Guo\"},{\"authorId\":null,\"name\":\"Honglak Lee\"},{\"authorId\":null,\"name\":\"Richard L Lewis\"},{\"authorId\":null,\"name\":\"Satinder Singh. Action-conditional video prediction using d games\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 2845\\u20132853.\",\"year\":2015},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1606.01868\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"143810408\",\"name\":\"D. Saxton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e90fd78e8a3b98af3954aae5209703aa966603e\",\"title\":\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1611.04717\",\"authors\":[{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"22966825\",\"name\":\"Davis Foote\"},{\"authorId\":\"47541311\",\"name\":\"Adam Stooke\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf\",\"title\":\"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1504.00702\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6b8a1b80891c96c28cc6340267b58186157e536\",\"title\":\"End-to-End Training of Deep Visuomotor Policies\",\"url\":\"https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Haiyan Yin\"},{\"authorId\":null,\"name\":\"Sinno Jialin Pan. Knowledge transfer for deep reinforcem replay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 1640\\u20131646,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/j.jcss.2007.08.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"237a1cf18ed83bb3ad852b34f443c6c1ff3336c1\",\"title\":\"An analysis of model-based Interval Estimation for Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/237a1cf18ed83bb3ad852b34f443c6c1ff3336c1\",\"venue\":\"J. Comput. Syst. Sci.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mnih et al\"},{\"authorId\":null,\"name\":\"2015 V. Mnih\"},{\"authorId\":null,\"name\":\"K. Kavukcuoglu\"},{\"authorId\":null,\"name\":\"D. Silver\"},{\"authorId\":null,\"name\":\"A. a Rusu\"},{\"authorId\":null,\"name\":\"J. Veness\"},{\"authorId\":null,\"name\":\"M. G. Bellemare\"},{\"authorId\":null,\"name\":\"A. Graves\"},{\"authorId\":null,\"name\":\"M. Riedmiller\"},{\"authorId\":null,\"name\":\"A. K. Fidjeland\"},{\"authorId\":null,\"name\":\"G. Ostrovski\"},{\"authorId\":null,\"name\":\"S. Petersen\"},{\"authorId\":null,\"name\":\"A. Sadik C. Beattie\"},{\"authorId\":null,\"name\":\"I. Antonoglou\"},{\"authorId\":null,\"name\":\"D. Kumaran H. King\"},{\"authorId\":null,\"name\":\"D. Wierstra\"},{\"authorId\":null,\"name\":\"S. Legg\"},{\"authorId\":null,\"name\":\"D. Hassabis\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Human-level control\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745732\",\"name\":\"M. Charikar\"}],\"doi\":\"10.1145/509907.509965\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"42bb0ac384fb87933be67f63b98d90a45d2fe6e9\",\"title\":\"Similarity estimation techniques from rounding algorithms\",\"url\":\"https://www.semanticscholar.org/paper/42bb0ac384fb87933be67f63b98d90a45d2fe6e9\",\"venue\":\"STOC '02\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1602.04621\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b63e34276aa98d5345efa7fe09bb06d8a9d8f52\",\"title\":\"Deep Exploration via Bootstrapped DQN\",\"url\":\"https://www.semanticscholar.org/paper/4b63e34276aa98d5345efa7fe09bb06d8a9d8f52\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Moses S Charikar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The arcade learning environment : An evaluation platform for general agents Unifying count - based exploration and intrinsic motivation\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1605.09128\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"46660962\",\"name\":\"Valliappa Chockalingam\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5129a9cbb6de3c6579f6a7d974394d392ac29829\",\"title\":\"Control of Memory, Active Perception, and Action in Minecraft\",\"url\":\"https://www.semanticscholar.org/paper/5129a9cbb6de3c6579f6a7d974394d392ac29829\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1511.06295\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"143959037\",\"name\":\"J. Kirkpatrick\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"title\":\"Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ian Osband\"},{\"authorId\":null,\"name\":\"Charles Blundell\"},{\"authorId\":null,\"name\":\"Alexander Pritzel\"},{\"authorId\":null,\"name\":\"Benjamin Van Roy. Deep exploration via bootstrapped dqn\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 4026\\u20134034,\",\"year\":2016},{\"arxivId\":\"1511.06581\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"title\":\"Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ziyu Wang\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"Matteo Hessel\"},{\"authorId\":null,\"name\":\"Hado van Hasselt\"},{\"authorId\":null,\"name\":\"Marc Lanctot\"},{\"authorId\":null,\"name\":\"Nando de Freitas. Dueling network architectures for deep learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 1995\\u20132003,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50086584\",\"name\":\"Haiyan Yin\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"title\":\"Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S Sutton\"},{\"authorId\":null,\"name\":\"Andrew G Barto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Reinforcement learning: An introduction, volume 1\",\"url\":\"\",\"venue\":\"MIT press Cambridge,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Moses S Charikar. Similarity estimation techniques from STOC\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 380\\u2013388\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2002},{\"arxivId\":\"1312.6114\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"1678311\",\"name\":\"M. Welling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"title\":\"Auto-Encoding Variational Bayes\",\"url\":\"https://www.semanticscholar.org/paper/5f5dc5b9a2ba710937e2c413b37b053cd673df02\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tom Schaul Ziyu Wang\"},{\"authorId\":null,\"name\":\"Matteo Hessel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Hado van Hasselt , Marc Lanctot , and Nando de Freitas . Dueling network architectures for deep reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1606.04460\",\"authors\":[{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"2825051\",\"name\":\"B. Uria\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"3422141\",\"name\":\"Y. Li\"},{\"authorId\":\"144893251\",\"name\":\"Avraham Ruderman\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"34269227\",\"name\":\"Jack W. Rae\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba378579fb44007db9f02699889721dcd2b5b3a0\",\"title\":\"Model-Free Episodic Control\",\"url\":\"https://www.semanticscholar.org/paper/ba378579fb44007db9f02699889721dcd2b5b3a0\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1507.00814\",\"authors\":[{\"authorId\":\"3275284\",\"name\":\"Bradly C. Stadie\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2470fcf0f89082de874ac9133ccb3a8667dd89a8\",\"title\":\"Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models\",\"url\":\"https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1507.08750\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4257bc131c36504a04382290cbc27ca8bb27813\",\"title\":\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1703.01732\",\"authors\":[{\"authorId\":\"3381809\",\"name\":\"Joshua Achiam\"},{\"authorId\":\"144797536\",\"name\":\"S. Sastry\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6757aedcb53142bc439ec64bfd0b056d99b1881\",\"title\":\"Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d6757aedcb53142bc439ec64bfd0b056d99b1881\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1706.08090\",\"authors\":[{\"authorId\":\"34104547\",\"name\":\"J. Martin\"},{\"authorId\":\"9763678\",\"name\":\"S. N. Sasikumar\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.24963/ijcai.2017/344\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f810eb4777fd05317951ebaa7a3f5835ee84cf4\",\"title\":\"Count-Based Exploration in Feature Space for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0f810eb4777fd05317951ebaa7a3f5835ee84cf4\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36359915\",\"name\":\"M. Panella\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ff15528cbb9c47bb2324d0299c76bf331994882\",\"title\":\"Associate Editor of the Journal of Computer and System Sciences\",\"url\":\"https://www.semanticscholar.org/paper/9ff15528cbb9c47bb2324d0299c76bf331994882\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"317cd4522b1f4a6f889743578143bb8823623f8b\",\"title\":\"VIME: Variational Information Maximizing Exploration\",\"url\":\"https://www.semanticscholar.org/paper/317cd4522b1f4a6f889743578143bb8823623f8b\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Bellemare\"},{\"authorId\":null,\"name\":\"Sriram Srinivasan\"},{\"authorId\":null,\"name\":\"Georg Ostrovski\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"David Saxton\"},{\"authorId\":null,\"name\":\"Remi Munos. Unifying count-based exploration\"},{\"authorId\":null,\"name\":\"intrinsic motivation\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 1471\\u20131479,\",\"year\":2016}],\"title\":\"Hashing Over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Autoencoder\",\"topicId\":\"433939\",\"url\":\"https://www.semanticscholar.org/topic/433939\"},{\"topic\":\"Hash function\",\"topicId\":\"18740\",\"url\":\"https://www.semanticscholar.org/topic/18740\"}],\"url\":\"https://www.semanticscholar.org/paper/ea10749c0ef01294d5beef9a5c901d899a2e1f6e\",\"venue\":\"IJCAI\",\"year\":2018}\n"