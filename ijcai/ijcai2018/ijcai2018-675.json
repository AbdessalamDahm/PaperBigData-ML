"{\"abstract\":\"Reinforcement learning and symbolic planning have both been used to build intelligent autonomous agents. Reinforcement learning relies on learning from interactions with real world, which often requires an unfeasibly large amount of experience. Symbolic planning relies on manually crafted symbolic knowledge, which may not be robust to domain uncertainties and changes. In this paper we present a unified framework {\\\\em PEORL} that integrates symbolic planning with hierarchical reinforcement learning (HRL) to cope with decision-making in a dynamic environment with uncertainties. \\nSymbolic plans are used to guide the agent's task execution and learning, and the learned experience is fed back to symbolic knowledge to improve planning. This method leads to rapid policy search and robust symbolic plans in complex domains. The framework is tested on benchmark domains of HRL.\",\"arxivId\":\"1804.07779\",\"authors\":[{\"authorId\":\"5427755\",\"name\":\"F. Yang\",\"url\":\"https://www.semanticscholar.org/author/5427755\"},{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\",\"url\":\"https://www.semanticscholar.org/author/40431009\"},{\"authorId\":\"49167156\",\"name\":\"B. Liu\",\"url\":\"https://www.semanticscholar.org/author/49167156\"},{\"authorId\":\"145495375\",\"name\":\"S. Gustafson\",\"url\":\"https://www.semanticscholar.org/author/145495375\"}],\"citationVelocity\":16,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"47015403\",\"name\":\"G. A. Kiselev\"},{\"authorId\":\"35234816\",\"name\":\"A. Panov\"}],\"doi\":\"10.1007/978-3-030-60337-3_16\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bc21cc6865bcc885b46b69e4e75bb295e4eabdfa\",\"title\":\"Q-Learning of Spatial Actions for Hierarchical Planner of Cognitive Agents\",\"url\":\"https://www.semanticscholar.org/paper/bc21cc6865bcc885b46b69e4e75bb295e4eabdfa\",\"venue\":\"ICR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144810417\",\"name\":\"Craig Innes\"},{\"authorId\":\"1876168\",\"name\":\"Alex Lascarides\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f387260f0acc70ecc0fe2b3db488180023f92db1\",\"title\":\"Explorer Learning Factored Markov Decision Processes with Unawareness\",\"url\":\"https://www.semanticscholar.org/paper/f387260f0acc70ecc0fe2b3db488180023f92db1\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1903.05937\",\"authors\":[{\"authorId\":\"144077615\",\"name\":\"L. Serafini\"},{\"authorId\":\"145919532\",\"name\":\"P. Traverso\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"d81cc4487a7beaabf903869cca4a90a376dd1c8f\",\"title\":\"Incremental Learning of Discrete Planning Domains from Continuous Perceptions\",\"url\":\"https://www.semanticscholar.org/paper/d81cc4487a7beaabf903869cca4a90a376dd1c8f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1904.05381\",\"authors\":[{\"authorId\":\"48305274\",\"name\":\"Xudong Sun\"},{\"authorId\":\"3020541\",\"name\":\"J. Lin\"},{\"authorId\":\"1686924\",\"name\":\"B. Bischl\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f79d1a9c60f5e64609c93f1c81c9de3bda9d0e0\",\"title\":\"ReinBo: Machine Learning pipeline search and configuration with Bayesian Optimization embedded Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7f79d1a9c60f5e64609c93f1c81c9de3bda9d0e0\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47060588\",\"name\":\"Zehong Cao\"},{\"authorId\":\"1739181054\",\"name\":\"Kaichiu Wong\"},{\"authorId\":\"1379648894\",\"name\":\"Quan Bai\"},{\"authorId\":\"30773936\",\"name\":\"Chin-Teng Lin\"}],\"doi\":\"10.5555/3398761.3399087\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c5d2189100f34dd342fdd4714ffaad674f82e21\",\"title\":\"Hierarchical and Non-Hierarchical Multi-Agent Interactions Based on Unity Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1c5d2189100f34dd342fdd4714ffaad674f82e21\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144719761\",\"name\":\"A. Ronzhin\"},{\"authorId\":\"46343645\",\"name\":\"G. Rigoll\"},{\"authorId\":\"30973808\",\"name\":\"R. Meshcheryakov\"}],\"doi\":\"10.1007/978-3-030-60337-3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bc5922b2292b4a3c06854899c7f39f6a1d96ec35\",\"title\":\"Interactive Collaborative Robotics: 5th International Conference, ICR 2020, St Petersburg, Russia, October 7-9, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/bc5922b2292b4a3c06854899c7f39f6a1d96ec35\",\"venue\":\"ICR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153520442\",\"name\":\"Xudong Sun\"},{\"authorId\":\"3020541\",\"name\":\"J. Lin\"},{\"authorId\":\"1686924\",\"name\":\"B. Bischl\"}],\"doi\":\"10.1007/978-3-030-43823-4_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"932275338b49c5e45100e33eb8c0138afaf23c84\",\"title\":\"ReinBo: Machine Learning Pipeline Conditional Hierarchy Search and Configuration with Bayesian Optimization Embedded Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/932275338b49c5e45100e33eb8c0138afaf23c84\",\"venue\":\"PKDD/ECML Workshops\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1450580449\",\"name\":\"Cristina Menendez-Romero\"},{\"authorId\":\"144796459\",\"name\":\"F. Winkler\"},{\"authorId\":\"2573148\",\"name\":\"Christian Dornhege\"},{\"authorId\":\"1725973\",\"name\":\"W. Burgard\"}],\"doi\":\"10.1109/ITSC45102.2020.9294190\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fdb5c530c6daf7f0971676fd61751503036d3acf\",\"title\":\"Maneuver Planning and Learning: a Lane Selection Approach for Highly Automated Vehicles in Highway Scenarios.\",\"url\":\"https://www.semanticscholar.org/paper/fdb5c530c6daf7f0971676fd61751503036d3acf\",\"venue\":\"2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)\",\"year\":2020},{\"arxivId\":\"2008.08548\",\"authors\":[{\"authorId\":\"1628280424\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"1388270562\",\"name\":\"Mohan Sridharan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ce808e43af42c095e3dce2dff340e8d6f65f17c2\",\"title\":\"A Survey of Knowledge-based Sequential Decision Making under Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/ce808e43af42c095e3dce2dff340e8d6f65f17c2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145306763\",\"name\":\"Saeid Amiri\"},{\"authorId\":\"40387301\",\"name\":\"Mohammad Shokrolah Shirazi\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"88c03a748d15723e8c3527c9cde0906ca21855bd\",\"title\":\"Robot Sequential Decision Making using LSTM-based Learning and Logical-probabilistic Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/88c03a748d15723e8c3527c9cde0906ca21855bd\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145306763\",\"name\":\"S. Amiri\"},{\"authorId\":\"40387301\",\"name\":\"M. Shirazi\"},{\"authorId\":\"40295359\",\"name\":\"ShiQi Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2b387c3dd4e40991981483aa44a5f396856f4c41\",\"title\":\"Leveraging Supervised Learning and Automated Reasoning for Robot Sequential Decision-Making\",\"url\":\"https://www.semanticscholar.org/paper/2b387c3dd4e40991981483aa44a5f396856f4c41\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1810.07096\",\"authors\":[{\"authorId\":\"144077615\",\"name\":\"L. Serafini\"},{\"authorId\":\"145919532\",\"name\":\"P. Traverso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1430a2a315a8ea475010280266b6795d24d4b1be\",\"title\":\"Incremental learning abstract discrete planning domains and mappings to continuous perceptions\",\"url\":\"https://www.semanticscholar.org/paper/1430a2a315a8ea475010280266b6795d24d4b1be\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2010.01909\",\"authors\":[{\"authorId\":\"2362527\",\"name\":\"Sunandita Patra\"},{\"authorId\":\"2387853\",\"name\":\"J. Mason\"},{\"authorId\":\"52089272\",\"name\":\"Malik Ghallab\"},{\"authorId\":\"29946800\",\"name\":\"Dana Nau\"},{\"authorId\":\"1557382581\",\"name\":\"P. Traverso\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c8b504080b8f3e0af81682368d2c5f9d3f8c9c10\",\"title\":\"Deliberative Acting, Online Planning and Learning with Hierarchical Operational Models\",\"url\":\"https://www.semanticscholar.org/paper/c8b504080b8f3e0af81682368d2c5f9d3f8c9c10\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.00090\",\"authors\":[{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"49167156\",\"name\":\"B. Liu\"},{\"authorId\":\"145495375\",\"name\":\"S. Gustafson\"}],\"doi\":\"10.1609/AAAI.V33I01.33012970\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"335afb1066996d2c1e2b33da28a45bbf851e1be6\",\"title\":\"SDRL: Interpretable and Data-efficient Deep Reinforcement Learning Leveraging Symbolic Planning\",\"url\":\"https://www.semanticscholar.org/paper/335afb1066996d2c1e2b33da28a45bbf851e1be6\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1393291635\",\"name\":\"Le\\u00f3n Illanes\"},{\"authorId\":\"1441353365\",\"name\":\"X. Yan\"},{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"418a2d5a6c6027e079357d872a0596ec5b344289\",\"title\":\"Symbolic Plans as High-Level Instructions for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/418a2d5a6c6027e079357d872a0596ec5b344289\",\"venue\":\"ICAPS\",\"year\":2020},{\"arxivId\":\"1807.02303\",\"authors\":[{\"authorId\":\"2075619\",\"name\":\"Konstantinos Chatzilygeroudis\"},{\"authorId\":\"143874876\",\"name\":\"Vassilis Vassiliades\"},{\"authorId\":\"50707365\",\"name\":\"F. Stulp\"},{\"authorId\":\"1773227\",\"name\":\"S. Calinon\"},{\"authorId\":\"145312416\",\"name\":\"Jean-Baptiste Mouret\"}],\"doi\":\"10.1109/TRO.2019.2958211\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c66f90f0ca7f12847a6152f4fe8d4e10eb34162\",\"title\":\"A Survey on Policy Search Algorithms for Learning Robot Controllers in a Handful of Trials\",\"url\":\"https://www.semanticscholar.org/paper/8c66f90f0ca7f12847a6152f4fe8d4e10eb34162\",\"venue\":\"IEEE Transactions on Robotics\",\"year\":2020},{\"arxivId\":\"2003.03726\",\"authors\":[{\"authorId\":\"1491176297\",\"name\":\"Kei Kase\"},{\"authorId\":\"1977464\",\"name\":\"C. Paxton\"},{\"authorId\":\"3342503\",\"name\":\"Hammad Mazhar\"},{\"authorId\":\"50527812\",\"name\":\"Tetsuya Ogata\"},{\"authorId\":\"145197953\",\"name\":\"D. Fox\"}],\"doi\":\"10.1109/ICRA40945.2020.9196597\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b3deae9eb5e15d70b581561b5246bfd522abe853\",\"title\":\"Transferable Task Execution from Pixels through Deep Planning Domain Learning\",\"url\":\"https://www.semanticscholar.org/paper/b3deae9eb5e15d70b581561b5246bfd522abe853\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":\"1905.07030\",\"authors\":[{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cb10a26d45a3efc0f3a5b9c464243a063b6956f4\",\"title\":\"Knowledge-Based Sequential Decision-Making Under Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/cb10a26d45a3efc0f3a5b9c464243a063b6956f4\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144077615\",\"name\":\"L. Serafini\"},{\"authorId\":\"145919532\",\"name\":\"P. Traverso\"}],\"doi\":\"10.1007/978-3-030-35166-3_33\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2fea52c9ea26b91761e051dafeef247ef013045f\",\"title\":\"Learning abstract planning domains and mappings to real world perceptions\",\"url\":\"https://www.semanticscholar.org/paper/2fea52c9ea26b91761e051dafeef247ef013045f\",\"venue\":\"AI*IA\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leonardo Lamanna\"},{\"authorId\":null,\"name\":\"Alfonso E. Gerevini\"},{\"authorId\":null,\"name\":\"Alessandro Saetti\"},{\"authorId\":null,\"name\":\"Luciano Serafini\"},{\"authorId\":null,\"name\":\"Paolo Traverso\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"9dbfe15981311081db7ae576690ed81c1badba92\",\"title\":\"On-line Learning of Planning Domains from Sensor Data in PAL: Scaling up to Large State Spaces\",\"url\":\"https://www.semanticscholar.org/paper/9dbfe15981311081db7ae576690ed81c1badba92\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1810.06339\",\"authors\":[{\"authorId\":\"2276894\",\"name\":\"Yuxi Li\"}],\"doi\":\"10.1201/9781351006620-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"title\":\"Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.02168\",\"authors\":[{\"authorId\":\"65950947\",\"name\":\"Alexander Elkholy\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"145495375\",\"name\":\"S. Gustafson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9f4ae17db46e604337aa62ddb2c340af5a306553\",\"title\":\"Interpretable Automated Machine Learning in Maana(TM) Knowledge Platform\",\"url\":\"https://www.semanticscholar.org/paper/9f4ae17db46e604337aa62ddb2c340af5a306553\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"145495375\",\"name\":\"S. Gustafson\"},{\"authorId\":\"65950947\",\"name\":\"Alexander Elkholy\"},{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\"},{\"authorId\":\"49167156\",\"name\":\"B. Liu\"}],\"doi\":\"10.1007/978-3-030-04735-1_11\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0766ce78e785baf184176fbbc8e77121b9a1be59\",\"title\":\"Program Search for Machine Learning Pipelines Leveraging Symbolic Planning and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0766ce78e785baf184176fbbc8e77121b9a1be59\",\"venue\":\"GPTP\",\"year\":2018},{\"arxivId\":\"2012.11792\",\"authors\":[{\"authorId\":\"51887973\",\"name\":\"Mehrdad Zakershahrak\"},{\"authorId\":\"69932254\",\"name\":\"Samira Ghodratnama\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0cc33ee14a76f7964ee528950baa0981934e899a\",\"title\":\"Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0cc33ee14a76f7964ee528950baa0981934e899a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.13037\",\"authors\":[{\"authorId\":\"3379438\",\"name\":\"Vasanth Sarathy\"},{\"authorId\":\"19185302\",\"name\":\"Daniel Kasenberg\"},{\"authorId\":\"1808903\",\"name\":\"Shivam Goel\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"121848090\",\"name\":\"matthias. scheutz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9090c0d302eb6f310c55ae95b360c6606717a366\",\"title\":\"SPOTTER: Extending Symbolic Planning Operators through Targeted Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9090c0d302eb6f310c55ae95b360c6606717a366\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1508.03891\",\"authors\":[{\"authorId\":\"1714890\",\"name\":\"M. Sridharan\"},{\"authorId\":\"1719720\",\"name\":\"M. Gelfond\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"1688492\",\"name\":\"J. Wyatt\"}],\"doi\":\"10.1613/jair.1.11524\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ec89db916408d7111367e03392128a152ab3f2f7\",\"title\":\"A Refinement-Based Architecture for Knowledge Representation and Reasoning in Robotics\",\"url\":\"https://www.semanticscholar.org/paper/ec89db916408d7111367e03392128a152ab3f2f7\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2019},{\"arxivId\":\"1903.03411\",\"authors\":[{\"authorId\":\"46923056\",\"name\":\"Thiago Freitas dos Santos\"},{\"authorId\":\"145275454\",\"name\":\"Paulo E. Santos\"},{\"authorId\":\"2235554\",\"name\":\"L. A. Ferreira\"},{\"authorId\":\"32999467\",\"name\":\"Reinaldo A. C. Bianchi\"},{\"authorId\":\"2021768\",\"name\":\"Pedro Cabalar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e7d88802cc6aa3d9d89dadefbcc44ea8a8742cf6\",\"title\":\"Heuristics, Answer Set Programming and Markov Decision Process for Solving a Set of Spatial Puzzles\",\"url\":\"https://www.semanticscholar.org/paper/e7d88802cc6aa3d9d89dadefbcc44ea8a8742cf6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1906.07268\",\"authors\":[{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\"},{\"authorId\":\"5427755\",\"name\":\"Fangkai Yang\"},{\"authorId\":\"31383516\",\"name\":\"Bo Liu\"},{\"authorId\":\"32640517\",\"name\":\"Steven C. Gustafson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5166a59103dc45868b6481441779419e3e25089f\",\"title\":\"A Joint Planning and Learning Framework for Human-Aided Decision-Making.\",\"url\":\"https://www.semanticscholar.org/paper/5166a59103dc45868b6481441779419e3e25089f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2007.01223\",\"authors\":[{\"authorId\":\"46318120\",\"name\":\"Nathan Hunt\"},{\"authorId\":\"13781103\",\"name\":\"N. Fulton\"},{\"authorId\":\"1745665\",\"name\":\"Sara Magliacane\"},{\"authorId\":\"46685773\",\"name\":\"N. Ho\\u00e0ng\"},{\"authorId\":\"3225635\",\"name\":\"Subhro Das\"},{\"authorId\":\"1389870240\",\"name\":\"Armando Solar-Lezama\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0655bd7f2722b9ffe69613ac2c11bcb9b5eb9aa3\",\"title\":\"Verifiably Safe Exploration for End-to-End Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0655bd7f2722b9ffe69613ac2c11bcb9b5eb9aa3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1901.05322\",\"authors\":[{\"authorId\":\"145306763\",\"name\":\"S. Amiri\"},{\"authorId\":\"40387301\",\"name\":\"M. Shirazi\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"}],\"doi\":\"10.1609/AAAI.V34I03.5659\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c149626b93c8949fb8b181c6220d8e8e8a558f98\",\"title\":\"Learning and Reasoning for Robot Sequential Decision Making under Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/c149626b93c8949fb8b181c6220d8e8e8a558f98\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1809.11074\",\"authors\":[{\"authorId\":\"51235122\",\"name\":\"Keting Lu\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"052cf374d8902f21de720bd209ebdda39167764c\",\"title\":\"Robot Representing and Reasoning with Knowledge from Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/052cf374d8902f21de720bd209ebdda39167764c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2004.08672\",\"authors\":[{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"91572436d1ce321bedf99dda022a0a9000a7db4b\",\"title\":\"iCORPP: Interleaved Commonsense Reasoning and Probabilistic Planning on Robots\",\"url\":\"https://www.semanticscholar.org/paper/91572436d1ce321bedf99dda022a0a9000a7db4b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.10147\",\"authors\":[{\"authorId\":\"2236890\",\"name\":\"Manfred Eppe\"},{\"authorId\":\"32237116\",\"name\":\"Christian Gumbsch\"},{\"authorId\":\"2991958\",\"name\":\"Matthias Kerzel\"},{\"authorId\":\"145431492\",\"name\":\"Phuong D. H. Nguyen\"},{\"authorId\":\"1732540\",\"name\":\"M. Butz\"},{\"authorId\":\"47291450\",\"name\":\"Stefan Wermter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"59c3241de85803984182982e959a77fe1e0c78ac\",\"title\":\"Hierarchical principles of embodied reinforcement learning: A review\",\"url\":\"https://www.semanticscholar.org/paper/59c3241de85803984182982e959a77fe1e0c78ac\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144810417\",\"name\":\"C. Innes\"},{\"authorId\":\"1876168\",\"name\":\"A. Lascarides\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0b1473e80382afdfc99e928074b43934c99f913\",\"title\":\"Learning Factored Markov Decision Processes with Unawareness Extended Abstract\",\"url\":\"https://www.semanticscholar.org/paper/a0b1473e80382afdfc99e928074b43934c99f913\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2005.09833\",\"authors\":[{\"authorId\":\"51235122\",\"name\":\"Keting Lu\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"49697607\",\"name\":\"Xiaoping Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c674570759cc7553512fdf823513de355aa6c592\",\"title\":\"Learning and Reasoning for Robot Dialog and Navigation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/c674570759cc7553512fdf823513de355aa6c592\",\"venue\":\"SIGdial\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1393291635\",\"name\":\"Le\\u00f3n Illanes\"},{\"authorId\":\"145026971\",\"name\":\"Xi Yan\"},{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70c39d1ae13502ca89bfe694f24b433743ae0a78\",\"title\":\"Symbolic Planning and Model-Free Reinforcement Learning: Training Taskable Agents\",\"url\":\"https://www.semanticscholar.org/paper/70c39d1ae13502ca89bfe694f24b433743ae0a78\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1393291635\",\"name\":\"Le\\u00f3n Illanes\"},{\"authorId\":\"145026971\",\"name\":\"Xi Yan\"},{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3349746553fd7cb991d9b34e90ef77ed856f53b9\",\"title\":\"Leveraging Symbolic Planning Models in Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3349746553fd7cb991d9b34e90ef77ed856f53b9\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2235554\",\"name\":\"L. A. Ferreira\"},{\"authorId\":\"46923056\",\"name\":\"Thiago Freitas dos Santos\"},{\"authorId\":\"32999467\",\"name\":\"Reinaldo A. C. Bianchi\"},{\"authorId\":\"145275454\",\"name\":\"Paulo E. Santos\"}],\"doi\":\"10.1007/978-3-030-35288-2_17\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c77573b5f97a2d59d5ed511e6a71097dbf07884\",\"title\":\"Solving Safety Problems with Ensemble Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1c77573b5f97a2d59d5ed511e6a71097dbf07884\",\"venue\":\"Australasian Conference on Artificial Intelligence\",\"year\":2019},{\"arxivId\":\"1811.08955\",\"authors\":[{\"authorId\":\"48751979\",\"name\":\"Yuqian Jiang\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ceca04fc0a4f3f11bc77ff0d66da548efbdcaec2\",\"title\":\"Integrating Task-Motion Planning with Reinforcement Learning for Robust Decision Making in Mobile Robots\",\"url\":\"https://www.semanticscholar.org/paper/ceca04fc0a4f3f11bc77ff0d66da548efbdcaec2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"40107085\",\"name\":\"Bo Liu\"},{\"authorId\":\"145495375\",\"name\":\"S. Gustafson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5a6e5a4844c402d174b062424aa73bc4d0efc4cf\",\"title\":\"PACMAN: A Planner-Actor-Critic Architecture for Human-Centered Planning and Learning\",\"url\":\"https://www.semanticscholar.org/paper/5a6e5a4844c402d174b062424aa73bc4d0efc4cf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1902.10619\",\"authors\":[{\"authorId\":\"144810417\",\"name\":\"C. Innes\"},{\"authorId\":\"1876168\",\"name\":\"A. Lascarides\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fc67db4505063805cd837b7c39c7b0bbe24ddefe\",\"title\":\"Learning Factored Markov Decision Processes with Unawareness\",\"url\":\"https://www.semanticscholar.org/paper/fc67db4505063805cd837b7c39c7b0bbe24ddefe\",\"venue\":\"UAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48751979\",\"name\":\"Yuqian Jiang\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1109/IROS40897.2019.8967680\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b8a046f37b7451770e05250113fbb77de59b1f7\",\"title\":\"Task-Motion Planning with Reinforcement Learning for Adaptable Mobile Service Robots\",\"url\":\"https://www.semanticscholar.org/paper/4b8a046f37b7451770e05250113fbb77de59b1f7\",\"venue\":\"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2019},{\"arxivId\":\"2004.11456\",\"authors\":[{\"authorId\":\"1656712399\",\"name\":\"Yohei Hayamizu\"},{\"authorId\":\"145306763\",\"name\":\"S. Amiri\"},{\"authorId\":\"1383234530\",\"name\":\"Kishan Chandan\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"2093424\",\"name\":\"K. Takadama\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b9037d81b7cb4fcf9823b7f49ee9924a7df27b21\",\"title\":\"Guided Dyna-Q for Mobile Robot Exploration and Navigation\",\"url\":\"https://www.semanticscholar.org/paper/b9037d81b7cb4fcf9823b7f49ee9924a7df27b21\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.13482\",\"authors\":[{\"authorId\":null,\"name\":\"Yi Wang\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"70248997\",\"name\":\"J. Lee\"}],\"doi\":\"10.1017/S1471068419000371\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b37783662484754b56bb1fec123ed1bbd6962aa\",\"title\":\"Bridging Commonsense Reasoning and Probabilistic Planning via a Probabilistic Action Language\",\"url\":\"https://www.semanticscholar.org/paper/7b37783662484754b56bb1fec123ed1bbd6962aa\",\"venue\":\"Theory Pract. Log. Program.\",\"year\":2019},{\"arxivId\":\"2003.03932\",\"authors\":[{\"authorId\":\"2362527\",\"name\":\"Sunandita Patra\"},{\"authorId\":\"144123868\",\"name\":\"J. Mason\"},{\"authorId\":\"152784261\",\"name\":\"A. Kumar\"},{\"authorId\":\"52089272\",\"name\":\"Malik Ghallab\"},{\"authorId\":\"1557382581\",\"name\":\"P. Traverso\"},{\"authorId\":\"29946800\",\"name\":\"Dana Nau\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff21ec8c5eec48a0a70db1d07a38543f9fb0166a\",\"title\":\"Integrating Acting, Planning and Learning in Hierarchical Operational Models\",\"url\":\"https://www.semanticscholar.org/paper/ff21ec8c5eec48a0a70db1d07a38543f9fb0166a\",\"venue\":\"ICAPS\",\"year\":2020},{\"arxivId\":\"1904.00512\",\"authors\":[{\"authorId\":\"40013369\",\"name\":\"Y. Wang\"},{\"authorId\":\"1735744\",\"name\":\"J. Lee\"}],\"doi\":\"10.1007/978-3-030-20528-7_17\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9726da1ec6638b6b4f60f17fd9455a2fa55d7095\",\"title\":\"Elaboration Tolerant Representation of Markov Decision Process via Decision-Theoretic Extension of Probabilistic Action Language pBC+\",\"url\":\"https://www.semanticscholar.org/paper/9726da1ec6638b6b4f60f17fd9455a2fa55d7095\",\"venue\":\"LPNMR\",\"year\":2019},{\"arxivId\":\"1909.09209\",\"authors\":[{\"authorId\":\"40431009\",\"name\":\"Daoming Lyu\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"2204092\",\"name\":\"Binghang Liu\"},{\"authorId\":\"32640517\",\"name\":\"S. Gustafson\"}],\"doi\":\"10.4204/EPTCS.306.23\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"6bec5817123301354e1b2ca4caf07a89449ab2e6\",\"title\":\"A Human-Centered Data-Driven Planner-Actor-Critic Architecture via Logic Programming\",\"url\":\"https://www.semanticscholar.org/paper/6bec5817123301354e1b2ca4caf07a89449ab2e6\",\"venue\":\"ICLP Technical Communications\",\"year\":2019}],\"corpusId\":5068169,\"doi\":\"10.24963/ijcai.2018/675\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"cf4d2e27da4ff7ee6dbd0e7a844db833444437d1\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1719720\",\"name\":\"M. Gelfond\"},{\"authorId\":\"2134090\",\"name\":\"V. Lifschitz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f1cf6d4b23d382f3f8ab12dc924a1e8bd1bb995\",\"title\":\"Action Languages\",\"url\":\"https://www.semanticscholar.org/paper/9f1cf6d4b23d382f3f8ab12dc924a1e8bd1bb995\",\"venue\":\"Electron. Trans. Artif. Intell.\",\"year\":1998},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27009404\",\"name\":\"M. Ryan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fae8f070c3444918ef625dbaae0d243ce036a123\",\"title\":\"Using Abstract Models of Behaviours to Automatically Generate Reinforcement Learning Hierarchies\",\"url\":\"https://www.semanticscholar.org/paper/fae8f070c3444918ef625dbaae0d243ce036a123\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2726706\",\"name\":\"N. B\\u00e4uerle\"},{\"authorId\":\"1707387\",\"name\":\"U. Rieder\"}],\"doi\":\"10.1365/S13291-010-0007-2\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b2db5541059288472ca246acdca6ead949326864\",\"title\":\"Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/b2db5541059288472ca246acdca6ead949326864\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Jim\\u00e9nez\"},{\"authorId\":null,\"name\":\"F. Fern\\u00e1ndez\"},{\"authorId\":null,\"name\":\"D. Borrajo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Integrating planning\",\"url\":\"\",\"venue\":\"execution, and learning to improve plan execution. Computational Intelligence\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Lifschitz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"What is answer set programming? In Proceedings of the AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 1594\\u20131597. MIT Press\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351852\",\"name\":\"P. Khandelwal\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"2134090\",\"name\":\"V. Lifschitz\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1498b26f1e5fb2e4119bd368006582f2486eabef\",\"title\":\"Planning in Action Language BC while Learning Action Costs for Mobile Robots\",\"url\":\"https://www.semanticscholar.org/paper/1498b26f1e5fb2e4119bd368006582f2486eabef\",\"venue\":\"ICAPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"2351852\",\"name\":\"P. Khandelwal\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f67ecfec665d7b1c8eff2881c6749eab69ff83c\",\"title\":\"Dynamically Constructed (PO)MDPs for Adaptive Robot Planning\",\"url\":\"https://www.semanticscholar.org/paper/5f67ecfec665d7b1c8eff2881c6749eab69ff83c\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. McDermott\"},{\"authorId\":null,\"name\":\"M. Ghallab\"},{\"authorId\":null,\"name\":\"A. Howe\"},{\"authorId\":null,\"name\":\"C. Knoblock\"},{\"authorId\":null,\"name\":\"A. Ram\"},{\"authorId\":null,\"name\":\"M. Veloso\"},{\"authorId\":null,\"name\":\"D. Weld\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and D\",\"url\":\"\",\"venue\":\"Wilkins. Pddl-the planning domain definition language.\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27009404\",\"name\":\"M. Ryan\"},{\"authorId\":\"1862271\",\"name\":\"Mark D. Pendrith\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d74b1329ebe27eb839ccdbd64213a22761304351\",\"title\":\"RL-TOPS: An Architecture for Modularity and Re-Use in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d74b1329ebe27eb839ccdbd64213a22761304351\",\"venue\":\"ICML\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leonetti et al\"},{\"authorId\":null,\"name\":\"2012 M. Leonetti\"},{\"authorId\":null,\"name\":\"L. Iocchi\"},{\"authorId\":null,\"name\":\"F. Patrizi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Automatic generation and learning of finite-state\",\"url\":\"\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742667\",\"name\":\"A. Cimatti\"},{\"authorId\":\"1707579\",\"name\":\"M. Pistore\"},{\"authorId\":\"145919532\",\"name\":\"P. Traverso\"}],\"doi\":\"10.1016/S1574-6526(07)03022-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58b772683f9f8eb0738a9b8bd5e12f282bd2750d\",\"title\":\"Automated Planning\",\"url\":\"https://www.semanticscholar.org/paper/58b772683f9f8eb0738a9b8bd5e12f282bd2750d\",\"venue\":\"Handbook of Knowledge Representation\",\"year\":2008},{\"arxivId\":\"1705.04569\",\"authors\":[{\"authorId\":\"2994034\",\"name\":\"Mutsunori Banbara\"},{\"authorId\":\"37952584\",\"name\":\"B. Kaufmann\"},{\"authorId\":\"28063835\",\"name\":\"M. Ostrowski\"},{\"authorId\":\"1809337\",\"name\":\"Torsten Schaub\"}],\"doi\":\"10.1017/S1471068417000138\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"df2eea25fec79fd280231ed9175584413683c7a3\",\"title\":\"Clingcon: The next generation\",\"url\":\"https://www.semanticscholar.org/paper/df2eea25fec79fd280231ed9175584413683c7a3\",\"venue\":\"Theory Pract. Log. Program.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2134090\",\"name\":\"V. Lifschitz\"}],\"doi\":\"10.1007/978-3-030-24658-7\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bbd86e320b0852e54bd3609ec0fd3c4dd4d2a57e\",\"title\":\"Answer Set Programming\",\"url\":\"https://www.semanticscholar.org/paper/bbd86e320b0852e54bd3609ec0fd3c4dd4d2a57e\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"}],\"doi\":\"10.1023/A:1017980312899\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"0b3d76978d77ca14103173968c858691422d2907\",\"title\":\"On Average Versus Discounted Reward Temporal-Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/0b3d76978d77ca14103173968c858691422d2907\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":\"1705.08080\",\"authors\":[{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"152462964\",\"name\":\"Daniel Gordon\"},{\"authorId\":\"3386570\",\"name\":\"Eric Kolve\"},{\"authorId\":\"145197953\",\"name\":\"D. Fox\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"51230553\",\"name\":\"A. Gupta\"},{\"authorId\":\"3012475\",\"name\":\"R. Mottaghi\"},{\"authorId\":\"47465174\",\"name\":\"Ali Farhadi\"}],\"doi\":\"10.1109/ICCV.2017.60\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4558b932075a862c72bb98bbce5f08590f563b14\",\"title\":\"Visual Semantic Planning Using Deep Successor Representations\",\"url\":\"https://www.semanticscholar.org/paper/4558b932075a862c72bb98bbce5f08590f563b14\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2351852\",\"name\":\"P. Khandelwal\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"2964199\",\"name\":\"I. Gori\"},{\"authorId\":\"47346762\",\"name\":\"M. Svetlik\"},{\"authorId\":\"2814124\",\"name\":\"Priyanka Khante\"},{\"authorId\":\"2134090\",\"name\":\"V. Lifschitz\"},{\"authorId\":\"1705627\",\"name\":\"J. Aggarwal\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1177/0278364916688949\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e4c6d47c34d64aa5883d16a64df82603f7e9706\",\"title\":\"BWIBots: A platform for bridging the gap between AI and human\\u2013robot interaction research\",\"url\":\"https://www.semanticscholar.org/paper/1e4c6d47c34d64aa5883d16a64df82603f7e9706\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1712013\",\"name\":\"L. Iocchi\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1016/J.ARTINT.2016.07.004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"12da91d3fd7757b423068e11440f5d999f63ca61\",\"title\":\"A synthesis of automated planning and reinforcement learning for efficient, robust decision-making\",\"url\":\"https://www.semanticscholar.org/paper/12da91d3fd7757b423068e11440f5d999f63ca61\",\"venue\":\"Artif. Intell.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50484601\",\"name\":\"Wolfgang Faber\"}],\"doi\":\"10.1007/978-3-642-39784-4_4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee7fcea1757c684769a8055d20b18528b7568aae\",\"title\":\"Answer Set Programming\",\"url\":\"https://www.semanticscholar.org/paper/ee7fcea1757c684769a8055d20b18528b7568aae\",\"venue\":\"Reasoning Web\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1728891\",\"name\":\"Sergio Jim\\u00e9nez Celorrio\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"},{\"authorId\":\"1788554\",\"name\":\"D. Borrajo\"}],\"doi\":\"10.1111/j.1467-8640.2012.00447.x\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"89e9133a8ab188c393f6601dc95c92253a0e6c7c\",\"title\":\"INTEGRATING PLANNING, EXECUTION, AND LEARNING TO IMPROVE PLAN EXECUTION\",\"url\":\"https://www.semanticscholar.org/paper/89e9133a8ab188c393f6601dc95c92253a0e6c7c\",\"venue\":\"Comput. Intell.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1684547\",\"name\":\"I. Menache\"},{\"authorId\":\"2730263\",\"name\":\"Amit Hoze\"},{\"authorId\":\"3028611\",\"name\":\"U. Klein\"}],\"doi\":\"10.1145/1015330.1015355\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42af0ed020c2caecafb7dbe826064d7f9ba2022b\",\"title\":\"Dynamic abstraction in reinforcement learning via clustering\",\"url\":\"https://www.semanticscholar.org/paper/42af0ed020c2caecafb7dbe826064d7f9ba2022b\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":\"1109.6051\",\"authors\":[{\"authorId\":\"3208076\",\"name\":\"M. Helmert\"}],\"doi\":\"10.1613/jair.1705\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"535b3d2d052c4c6d8a7e72e5e7b575aa39bb8b0f\",\"title\":\"The Fast Downward Planning System\",\"url\":\"https://www.semanticscholar.org/paper/535b3d2d052c4c6d8a7e72e5e7b575aa39bb8b0f\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1850503\",\"name\":\"S. Mahadevan\"}],\"doi\":\"10.1023/A:1022140919877\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0a8149fb5aa8a5684e7d530c264451a5cb9250f5\",\"title\":\"Recent Advances in Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0a8149fb5aa8a5684e7d530c264451a5cb9250f5\",\"venue\":\"Discret. Event Dyn. Syst.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Mahadevan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Average reward reinforcement learning: Foundations\",\"url\":\"\",\"venue\":\"algorithms, and empirical results. Machine Learning, 22:159\\u2013195\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1728609\",\"name\":\"M. Hanheide\"},{\"authorId\":\"1713618\",\"name\":\"M. G\\u00f6belbecker\"},{\"authorId\":\"2009471\",\"name\":\"G. Horn\"},{\"authorId\":\"1741287\",\"name\":\"Andrzej Pronobis\"},{\"authorId\":\"2551916\",\"name\":\"K. Sj\\u00f6\\u00f6\"},{\"authorId\":\"1692382\",\"name\":\"A. Aydemir\"},{\"authorId\":\"1770066\",\"name\":\"P. Jensfelt\"},{\"authorId\":\"1723510\",\"name\":\"Charles Gretton\"},{\"authorId\":\"143654598\",\"name\":\"R. Dearden\"},{\"authorId\":\"153463596\",\"name\":\"M. Jan\\u00edcek\"},{\"authorId\":\"1731104\",\"name\":\"H. Zender\"},{\"authorId\":\"1708708\",\"name\":\"G. Kruijff\"},{\"authorId\":\"143939898\",\"name\":\"Nick Hawes\"},{\"authorId\":\"1688492\",\"name\":\"J. Wyatt\"}],\"doi\":\"10.1016/j.artint.2015.08.008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78a0e406b4b03d60af44dc529686a15f30b1da37\",\"title\":\"Robot task planning and explanation in open and uncertain worlds\",\"url\":\"https://www.semanticscholar.org/paper/78a0e406b4b03d60af44dc529686a15f30b1da37\",\"venue\":\"Artif. Intell.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2791038\",\"name\":\"S. Niekum\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"21d3c5df0000dd42f82ea4e22c9aa2ef869e55c8\",\"title\":\"Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery\",\"url\":\"https://www.semanticscholar.org/paper/21d3c5df0000dd42f82ea4e22c9aa2ef869e55c8\",\"venue\":\"Lifelong Learning\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2987223\",\"name\":\"C. Hogg\"},{\"authorId\":\"1721385\",\"name\":\"U. Kuter\"},{\"authorId\":\"1403737823\",\"name\":\"Hector Mu\\u00f1oz-Avila\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b7fc1f84991904e172361847f004d1f841b7073b\",\"title\":\"Learning Methods to Generate Good Plans: Integrating HTN Learning and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b7fc1f84991904e172361847f004d1f841b7073b\",\"venue\":\"AAAI\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1850503\",\"name\":\"S. Mahadevan\"}],\"doi\":\"10.1007/BF00114727\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a62d15d6b73c3323e69270ab995aafa6a692a59c\",\"title\":\"Average reward reinforcement learning: Foundations, algorithms, and empirical results\",\"url\":\"https://www.semanticscholar.org/paper/a62d15d6b73c3323e69270ab995aafa6a692a59c\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"52e2ac397f0c8d5f533959905df899bc328d9f85\",\"title\":\"Reinforcement Learning with Hierarchies of Machines\",\"url\":\"https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85\",\"venue\":\"NIPS\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47035862\",\"name\":\"A. Schwartz\"}],\"doi\":\"10.1016/b978-1-55860-307-3.50045-9\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"99b2fd28dcab3657c5f1271a05223f4740e4b65c\",\"title\":\"A Reinforcement Learning Method for Maximizing Undiscounted Rewards\",\"url\":\"https://www.semanticscholar.org/paper/99b2fd28dcab3657c5f1271a05223f4740e4b65c\",\"venue\":\"ICML\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143805236\",\"name\":\"J. McCarthy\"}],\"doi\":\"10.1016/B978-0-934613-03-3.50033-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"76c880bd5bfa3c627a9497cd6f1ee0afa093e131\",\"title\":\"SOME PHILOSOPHICAL PROBLEMS FROM THE STANDPOINT OF ARTI CIAL INTELLIGENCE\",\"url\":\"https://www.semanticscholar.org/paper/76c880bd5bfa3c627a9497cd6f1ee0afa093e131\",\"venue\":\"\",\"year\":1981},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. G. Barto\"},{\"authorId\":null,\"name\":\"S. Mahadevan. Recent advances in hierarchical reinfor Systems\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"13(1-2):41\\u201377\",\"url\":\"\",\"venue\":\"January\",\"year\":2003},{\"arxivId\":\"1703.00956\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8423cc50c18d68f797adaa4f571f5e4efbe325a5\",\"title\":\"A Laplacian Framework for Option Discovery in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8423cc50c18d68f797adaa4f571f5e4efbe325a5\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680510\",\"name\":\"T. Dean\"}],\"doi\":\"10.1145/234313.234352\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08e84c652cdc82a25bfd70c95fede4f9a46fc6b9\",\"title\":\"Automated planning\",\"url\":\"https://www.semanticscholar.org/paper/08e84c652cdc82a25bfd70c95fede4f9a46fc6b9\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2495237\",\"name\":\"M. Gebser\"},{\"authorId\":\"37952584\",\"name\":\"B. Kaufmann\"},{\"authorId\":\"1809337\",\"name\":\"Torsten Schaub\"}],\"doi\":\"10.1016/j.artint.2012.04.001\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"03cf19cd3e5a4bb9aedcbbd0d2056c2d02c503f9\",\"title\":\"Conflict-driven answer set solving: From theory to practice\",\"url\":\"https://www.semanticscholar.org/paper/03cf19cd3e5a4bb9aedcbbd0d2056c2d02c503f9\",\"venue\":\"Artif. Intell.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1712013\",\"name\":\"L. Iocchi\"},{\"authorId\":\"1698994\",\"name\":\"F. Patrizi\"}],\"doi\":\"10.1007/978-3-642-33185-5_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c582673b0126bf8d25bf5f6811daadc6aea5d11e\",\"title\":\"Automatic Generation and Learning of Finite-State Controllers\",\"url\":\"https://www.semanticscholar.org/paper/c582673b0126bf8d25bf5f6811daadc6aea5d11e\",\"venue\":\"AIMSA\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47586296\",\"name\":\"J. Babb\"},{\"authorId\":\"70248997\",\"name\":\"J. Lee\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"245f15c07764311f3fd4ac87cd67214ba7e6b57d\",\"title\":\"Action language BC +: preliminary report\",\"url\":\"https://www.semanticscholar.org/paper/245f15c07764311f3fd4ac87cd67214ba7e6b57d\",\"venue\":\"AAAI 2015\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34526205\",\"name\":\"K. Chen\"},{\"authorId\":\"5427755\",\"name\":\"F. Yang\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b46a8b884d5a991535539532c6f64f6e2d89416\",\"title\":\"Planning with Task-Oriented Knowledge Acquisition for a Service Robot\",\"url\":\"https://www.semanticscholar.org/paper/1b46a8b884d5a991535539532c6f64f6e2d89416\",\"venue\":\"IJCAI\",\"year\":2016}],\"title\":\"PEORL: Integrating Symbolic Planning and Hierarchical Reinforcement Learning for Robust Decision-Making\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Unified Framework\",\"topicId\":\"105596\",\"url\":\"https://www.semanticscholar.org/topic/105596\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Autonomous robot\",\"topicId\":\"1175\",\"url\":\"https://www.semanticscholar.org/topic/1175\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Granular computing\",\"topicId\":\"217565\",\"url\":\"https://www.semanticscholar.org/topic/217565\"},{\"topic\":\"Call of Duty: Black Ops\",\"topicId\":\"2395412\",\"url\":\"https://www.semanticscholar.org/topic/2395412\"}],\"url\":\"https://www.semanticscholar.org/paper/cf4d2e27da4ff7ee6dbd0e7a844db833444437d1\",\"venue\":\"IJCAI\",\"year\":2018}\n"