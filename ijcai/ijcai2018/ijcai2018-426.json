"{\"abstract\":\"In typical reinforcement learning (RL), the environment is assumed given and the goal of the learning is to identify an optimal policy for the agent taking actions through its interactions with the environment. In this paper, we extend this setting by considering the environment is not given, but controllable and learnable through its interaction with the agent at the same time. This extension is motivated by environment design scenarios in the real-world, including game design, shopping space design and traffic signal design. Theoretically, we find a dual Markov decision process (MDP) w.r.t. the environment to that w.r.t. the agent, and derive a policy gradient solution to optimizing the parametrized environment. Furthermore, discontinuous environments are addressed by a proposed general generative framework. Our experiments on a Maze game design task show the effectiveness of the proposed algorithms in generating diverse and challenging Mazes against various agent settings.\",\"arxivId\":\"1707.01310\",\"authors\":[{\"authorId\":null,\"name\":\"Haifeng Zhang\",\"url\":null},{\"authorId\":\"39055225\",\"name\":\"J. Wang\",\"url\":\"https://www.semanticscholar.org/author/39055225\"},{\"authorId\":\"145385776\",\"name\":\"Zhiming Zhou\",\"url\":\"https://www.semanticscholar.org/author/145385776\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\",\"url\":\"https://www.semanticscholar.org/author/8031058\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\",\"url\":\"https://www.semanticscholar.org/author/50531782\"},{\"authorId\":\"1811427\",\"name\":\"Y. Yu\",\"url\":\"https://www.semanticscholar.org/author/1811427\"},{\"authorId\":\"39201759\",\"name\":\"W. Li\",\"url\":\"https://www.semanticscholar.org/author/39201759\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"122562941\",\"name\":\"L'eonard Hussenot\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"9528245c15ccb6e7e7541f9fdf2225eb94208dbf\",\"title\":\"Targeted Attacks on Deep Reinforcement Learning Agents through Adversarial Observations\",\"url\":\"https://www.semanticscholar.org/paper/9528245c15ccb6e7e7541f9fdf2225eb94208dbf\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1905.12282\",\"authors\":[{\"authorId\":\"122562941\",\"name\":\"L'eonard Hussenot\"},{\"authorId\":\"50675536\",\"name\":\"M. Geist\"},{\"authorId\":\"79608109\",\"name\":\"O. Pietquin\"}],\"doi\":\"10.5555/3398761.3398828\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b4044f9ba81a40a9376cfb61fc0bc53af7643688\",\"title\":\"CopyCAT: : Taking Control of Neural Policies with Constant Attacks\",\"url\":\"https://www.semanticscholar.org/paper/b4044f9ba81a40a9376cfb61fc0bc53af7643688\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"1811.05685\",\"authors\":[{\"authorId\":\"49724536\",\"name\":\"Haifeng Zhang\"},{\"authorId\":\"2270169\",\"name\":\"Zilong Guo\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"145834074\",\"name\":\"Han Cai\"},{\"authorId\":\"3321390\",\"name\":\"Chris Wang\"},{\"authorId\":\"1811427\",\"name\":\"Y. Yu\"},{\"authorId\":\"50135429\",\"name\":\"W. Li\"},{\"authorId\":\"37199474\",\"name\":\"J. Wang\"}],\"doi\":\"10.1109/ACCESS.2019.2953486\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"17f27f55971d9ae1595cbd04a337dad2a1906626\",\"title\":\"Layout Design for Intelligent Warehouse by Evolution With Fitness Approximation\",\"url\":\"https://www.semanticscholar.org/paper/17f27f55971d9ae1595cbd04a337dad2a1906626\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35255277\",\"name\":\"F. Saitoh\"}],\"doi\":\"10.1007/978-3-030-36808-1_40\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"46a408eab7285089c5aee407e3e02372ffbb7523\",\"title\":\"Knowledge Reuse of Learning Agent Based on Factor Information of Behavioral Rules\",\"url\":\"https://www.semanticscholar.org/paper/46a408eab7285089c5aee407e3e02372ffbb7523\",\"venue\":\"ICONIP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"41070378\",\"name\":\"Anand Bodas\"},{\"authorId\":\"47252160\",\"name\":\"Bhargav Upadhyay\"},{\"authorId\":\"49869117\",\"name\":\"Chetan Nadiger\"},{\"authorId\":\"47803155\",\"name\":\"Sherine Abdelhak\"}],\"doi\":\"10.1109/INFOCT.2018.8356853\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aef3f312d140fb9878e705ec2c733c3e9ce3cae1\",\"title\":\"Reinforcement learning for game personalization on edge devices\",\"url\":\"https://www.semanticscholar.org/paper/aef3f312d140fb9878e705ec2c733c3e9ce3cae1\",\"venue\":\"2018 International Conference on Information and Computer Technologies (ICICT)\",\"year\":2018}],\"corpusId\":44058894,\"doi\":\"10.24963/ijcai.2018/426\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"e7815702c62779c47ce09bbb569f1f13dcb3b3a1\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"143794407\",\"name\":\"X. Hao\"},{\"authorId\":\"8273966\",\"name\":\"Guigang Zhang\"},{\"authorId\":\"144153753\",\"name\":\"Shang Ma\"}],\"doi\":\"10.1142/S1793351X16500045\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f8d648c52edf74e41b0996128aa536e13cc7e82\",\"title\":\"Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/4f8d648c52edf74e41b0996128aa536e13cc7e82\",\"venue\":\"Int. J. Semantic Comput.\",\"year\":2016},{\"arxivId\":\"1306.6189\",\"authors\":[{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"143719795\",\"name\":\"Huan Xu\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a537240b8385ed7e920e711e2f51a1c0c507c431\",\"title\":\"Scaling Up Robust MDPs by Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a537240b8385ed7e920e711e2f51a1c0c507c431\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. S. Sutton\"},{\"authorId\":null,\"name\":\"A. G. Barto\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Reinforcement learning: An introduction, volume 1\",\"url\":\"\",\"venue\":\"MIT press Cambridge.\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689609\",\"name\":\"N. Nisan\"},{\"authorId\":\"2770424\",\"name\":\"Amir Ronen\"}],\"doi\":\"10.1006/game.1999.0790\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5057d1e96811e58a98ebeef884b7b81b43e1c61d\",\"title\":\"Algorithmic Mechanism Design\",\"url\":\"https://www.semanticscholar.org/paper/5057d1e96811e58a98ebeef884b7b81b43e1c61d\",\"venue\":\"Games Econ. Behav.\",\"year\":2001},{\"arxivId\":\"1701.07875\",\"authors\":[{\"authorId\":\"2877311\",\"name\":\"Mart\\u00edn Arjovsky\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"},{\"authorId\":\"52184096\",\"name\":\"L. Bottou\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f85b7376769473d2bed56f855f115e23d727094\",\"title\":\"Wasserstein GAN\",\"url\":\"https://www.semanticscholar.org/paper/2f85b7376769473d2bed56f855f115e23d727094\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"4454080\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b1362879e77efef96ab552f5cb1198c2a67204d6\",\"title\":\"Introduction to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b1362879e77efef96ab552f5cb1198c2a67204d6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1822613\",\"name\":\"J. Marks\"},{\"authorId\":\"3006861\",\"name\":\"V. Hom\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a1318b50d6ceb03fff8659e9e9f0e71d42291ab\",\"title\":\"Automatic Design of Balanced Board Games\",\"url\":\"https://www.semanticscholar.org/paper/0a1318b50d6ceb03fff8659e9e9f0e71d42291ab\",\"venue\":\"AIIDE\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49684355\",\"name\":\"M. Heger\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50021-0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f5d5a699770808228a2de1b5f99e76ce4bd2b9ee\",\"title\":\"Consideration of Risk in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f5d5a699770808228a2de1b5f99e76ce4bd2b9ee\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":\"1109.2135\",\"authors\":[{\"authorId\":\"1387476126\",\"name\":\"P. J. Gmytrasiewicz\"},{\"authorId\":\"152614492\",\"name\":\"P. Doshi\"}],\"doi\":\"10.1613/jair.1579\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5320c1cbfa93b9a8ab8daa2a0e491b18bfe27894\",\"title\":\"A Framework for Sequential Planning in Multi-Agent Settings\",\"url\":\"https://www.semanticscholar.org/paper/5320c1cbfa93b9a8ab8daa2a0e491b18bfe27894\",\"venue\":\"ISAIM\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709638\",\"name\":\"L. Busoniu\"},{\"authorId\":\"1705222\",\"name\":\"Robert Babu\\u0161ka\"},{\"authorId\":\"1724741\",\"name\":\"B. D. Schutter\"}],\"doi\":\"10.1109/TSMCC.2007.913919\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4aece8df7bd59e2fbfedbf5729bba41abc56d870\",\"title\":\"A Comprehensive Survey of Multiagent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4aece8df7bd59e2fbfedbf5729bba41abc56d870\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Mnih\"},{\"authorId\":null,\"name\":\"K. Kavukcuoglu\"},{\"authorId\":null,\"name\":\"D. Silver\"},{\"authorId\":null,\"name\":\"A. A. Rusu\"},{\"authorId\":null,\"name\":\"J. Veness\"},{\"authorId\":null,\"name\":\"M. G. Bellemare\"},{\"authorId\":null,\"name\":\"A. Graves\"},{\"authorId\":null,\"name\":\"M. Riedmiller\"},{\"authorId\":null,\"name\":\"A. K. Fidjeland\"},{\"authorId\":null,\"name\":\"G Ostrovski\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Humanlevel control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144539890\",\"name\":\"N. Hansen\"},{\"authorId\":\"7831740\",\"name\":\"S. M\\u00fcller\"},{\"authorId\":\"1802604\",\"name\":\"P. Koumoutsakos\"}],\"doi\":\"10.1162/106365603321828970\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"26afab5607f4bfaf2fb9f786e4ed4f2d93c88e84\",\"title\":\"Reducing the Time Complexity of the Derandomized Evolution Strategy with Covariance Matrix Adaptation (CMA-ES)\",\"url\":\"https://www.semanticscholar.org/paper/26afab5607f4bfaf2fb9f786e4ed4f2d93c88e84\",\"venue\":\"Evolutionary Computation\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47782330\",\"name\":\"B. Abdulhai\"},{\"authorId\":\"51371491\",\"name\":\"R. Pringle\"},{\"authorId\":\"69037486\",\"name\":\"G. J. Karakoulas\"}],\"doi\":\"10.1061/(ASCE)0733-947X(2003)129:3(278)\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c1309a5cb147856f785253d99424685cf5c12fc\",\"title\":\"Reinforcement learning for true adaptive traffic signal control\",\"url\":\"https://www.semanticscholar.org/paper/6c1309a5cb147856f785253d99424685cf5c12fc\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2019448\",\"name\":\"Halim Ceylan\"},{\"authorId\":\"34139480\",\"name\":\"M. Bell\"}],\"doi\":\"10.1016/S0191-2615(03)00015-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"563215a4c6458ff1f33d0e1e363a2236db72e90e\",\"title\":\"Traffic signal timing optimisation based on genetic algorithm approach, including drivers\\u2019 routing\",\"url\":\"https://www.semanticscholar.org/paper/563215a4c6458ff1f33d0e1e363a2236db72e90e\",\"venue\":\"\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2925279\",\"name\":\"Robin Hunicke\"},{\"authorId\":\"49850193\",\"name\":\"M. Leblanc\"},{\"authorId\":\"2088944\",\"name\":\"Robert Zubek\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2b134e5c46eec50f69c702c0b4aa29687d5d8fba\",\"title\":\"MDA : A Formal Approach to Game Design and Game Research\",\"url\":\"https://www.semanticscholar.org/paper/2b134e5c46eec50f69c702c0b4aa29687d5d8fba\",\"venue\":\"\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2697474\",\"name\":\"Chris Gaskett\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c610c97c56e9e3108af9ac00faacce5dbd2ac0c\",\"title\":\"Reinforcement learning under circumstances beyond its control\",\"url\":\"https://www.semanticscholar.org/paper/7c610c97c56e9e3108af9ac00faacce5dbd2ac0c\",\"venue\":\"\",\"year\":2003},{\"arxivId\":\"1908.01420\",\"authors\":[{\"authorId\":\"3194972\",\"name\":\"Alexander Zook\"},{\"authorId\":\"2757194\",\"name\":\"Mark O. Riedl\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ea72a7045f18a8d1018b4ef151c4dbcb97f1c91\",\"title\":\"Automatic Game Design via Mechanic Generation\",\"url\":\"https://www.semanticscholar.org/paper/4ea72a7045f18a8d1018b4ef151c4dbcb97f1c91\",\"venue\":\"AAAI\",\"year\":2014},{\"arxivId\":\"cs/9605103\",\"authors\":[{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"1760402\",\"name\":\"A. Moore\"}],\"doi\":\"10.1613/jair.301\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"12d1d070a53d4084d88a77b8b143bad51c40c38f\",\"title\":\"Reinforcement Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/12d1d070a53d4084d88a77b8b143bad51c40c38f\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760402\",\"name\":\"A. Moore\"},{\"authorId\":\"8483722\",\"name\":\"C. Atkeson\"}],\"doi\":\"10.1007/BF00993104\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74921bc8762812345b7010746faa22988b85252e\",\"title\":\"Prioritized sweeping: Reinforcement learning with less data and less time\",\"url\":\"https://www.semanticscholar.org/paper/74921bc8762812345b7010746faa22988b85252e\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0f2c4104ef6e36bb67022001179887e6600d24d\",\"title\":\"A comprehensive survey on safe reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/c0f2c4104ef6e36bb67022001179887e6600d24d\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1404778056\",\"name\":\"Oriol Carbonell-Nicolau\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f0e4ccdd1d1f775194aac57fbec5e295483121c\",\"title\":\"Games and Economic Behavior\",\"url\":\"https://www.semanticscholar.org/paper/5f0e4ccdd1d1f775194aac57fbec5e295483121c\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3274000\",\"name\":\"J. Hu\"},{\"authorId\":\"1796536\",\"name\":\"Michael P. Wellman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dc4e9aa01abf579b6e5c8fc43261f9062e77a7f9\",\"title\":\"Nash Q-Learning for General-Sum Stochastic Games\",\"url\":\"https://www.semanticscholar.org/paper/dc4e9aa01abf579b6e5c8fc43261f9062e77a7f9\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1703.01161\",\"authors\":[{\"authorId\":\"9948791\",\"name\":\"A. S. Vezhnevets\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"049c6e5736313374c6e594c34b9be89a3a09dced\",\"title\":\"FeUdal Networks for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/049c6e5736313374c6e594c34b9be89a3a09dced\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Halim Ceylan\"},{\"authorId\":null,\"name\":\"Michael GH Bell. Traffic signal timing optimisation based approach\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"including drivers\\u2019 routing\",\"url\":\"\",\"venue\":\"Transportation Research Part B: Methodological,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"},{\"authorId\":\"1686193\",\"name\":\"Georgios N. Yannakakis\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"145317122\",\"name\":\"C. Browne\"}],\"doi\":\"10.1109/TCIAIG.2011.2148116\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3288d7575f451d2e95f57cefc9566691ff272f1c\",\"title\":\"Search-Based Procedural Content Generation: A Taxonomy and Survey\",\"url\":\"https://www.semanticscholar.org/paper/3288d7575f451d2e95f57cefc9566691ff272f1c\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"title\":\"Deterministic Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50414121\",\"name\":\"J. Morimoto\"},{\"authorId\":\"1714997\",\"name\":\"K. Doya\"}],\"doi\":\"10.1162/0899766053011528\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69a23bc408b5f324a9d4e8f665b0fab4ea1b0841\",\"title\":\"Robust Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69a23bc408b5f324a9d4e8f665b0fab4ea1b0841\",\"venue\":\"Neural Computation\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ian Goodfellow\"},{\"authorId\":null,\"name\":\"Jean PougetAbadie\"},{\"authorId\":null,\"name\":\"Mehdi Mirza\"},{\"authorId\":null,\"name\":\"Bing Xu\"},{\"authorId\":null,\"name\":\"David Warde-Farley\"},{\"authorId\":null,\"name\":\"Sherjil Ozair\"},{\"authorId\":null,\"name\":\"Aaron Courville\"},{\"authorId\":null,\"name\":\"Yoshua Bengio. Generative adversarial nets\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 2672\\u20132680,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50027-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"title\":\"Markov Games as a Framework for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthias Plappert\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"keras-rl\",\"url\":\"\",\"venue\":\"https:// github.com/keras-rl/keras-rl,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Julian Togelius\"},{\"authorId\":null,\"name\":\"Jurgen Schmidhuber. An experiment in automatic game desig Intelligence\"},{\"authorId\":null,\"name\":\"Games\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"2008\",\"url\":\"\",\"venue\":\"CIG\\u201908. IEEE Symposium On. IEEE,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1016/b978-1-55860-141-3.50030-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5f8a0858fb82ce0e50b55446577a70e40137aaf\",\"title\":\"Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/b5f8a0858fb82ce0e50b55446577a70e40137aaf\",\"venue\":\"ML\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/CIG.2008.5035629\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ad5b0b1a0ba1cf354e3eec41a7038f05ac0462e\",\"title\":\"An experiment in automatic game design\",\"url\":\"https://www.semanticscholar.org/paper/4ad5b0b1a0ba1cf354e3eec41a7038f05ac0462e\",\"venue\":\"2008 IEEE Symposium On Computational Intelligence and Games\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741147143\",\"name\":\"Vijay Kumar Kaul\"}],\"doi\":\"10.2307/j.ctt5hh7c0.12\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff62aa2bfd3b4db63307c8a684e3c4edb39d990c\",\"title\":\"Planning\",\"url\":\"https://www.semanticscholar.org/paper/ff62aa2bfd3b4db63307c8a684e3c4edb39d990c\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1701.02490\",\"authors\":[{\"authorId\":\"145834074\",\"name\":\"Han Cai\"},{\"authorId\":\"144931569\",\"name\":\"Kan Ren\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"2228452\",\"name\":\"Kleanthis Malialis\"},{\"authorId\":\"39055225\",\"name\":\"J. Wang\"},{\"authorId\":\"1811427\",\"name\":\"Y. Yu\"},{\"authorId\":\"2914928\",\"name\":\"Defeng Guo\"}],\"doi\":\"10.1145/3018661.3018702\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6e6cf4dda8d375f057c4ee4a4e15f66fe532b601\",\"title\":\"Real-Time Bidding by Reinforcement Learning in Display Advertising\",\"url\":\"https://www.semanticscholar.org/paper/6e6cf4dda8d375f057c4ee4a4e15f66fe532b601\",\"venue\":\"WSDM\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alan Penn\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The complexity of the elementary interface: shopping space\",\"url\":\"\",\"venue\":\"Proceedings to the 5th International Space Syntax Symposium. Akkelies van Nes,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722628\",\"name\":\"A. Nilim\"},{\"authorId\":\"1701847\",\"name\":\"L. Ghaoui\"}],\"doi\":\"10.1287/opre.1050.0216\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6db16608fccddef51202af84112b34cfebfbe20a\",\"title\":\"Robust Control of Markov Decision Processes with Uncertain Transition Matrices\",\"url\":\"https://www.semanticscholar.org/paper/6db16608fccddef51202af84112b34cfebfbe20a\",\"venue\":\"Oper. Res.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144123639\",\"name\":\"Jonathan Sorg\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8e767ae28510558344f90b0be08344d2b3769cd6\",\"title\":\"Reward Design via Online Gradient Ascent\",\"url\":\"https://www.semanticscholar.org/paper/8e767ae28510558344f90b0be08344d2b3769cd6\",\"venue\":\"NIPS\",\"year\":2010}],\"title\":\"Learning to Design Games: Strategic Environments in Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Video game design\",\"topicId\":\"316185\",\"url\":\"https://www.semanticscholar.org/topic/316185\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Automated planning and scheduling\",\"topicId\":\"3649\",\"url\":\"https://www.semanticscholar.org/topic/3649\"},{\"topic\":\"Gradient method\",\"topicId\":\"61331\",\"url\":\"https://www.semanticscholar.org/topic/61331\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"}],\"url\":\"https://www.semanticscholar.org/paper/e7815702c62779c47ce09bbb569f1f13dcb3b3a1\",\"venue\":\"IJCAI\",\"year\":2018}\n"