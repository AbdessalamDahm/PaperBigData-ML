"{\"abstract\":\"Reinforcement learning (RL) has had many successes when learning autonomously. This paper and accompanying talk consider how to make use of a non-technical human participant, when available. In particular, we consider the case where a human could 1) provide demonstrations of good behavior, 2) provide online evaluative feedback, or 3) define a curriculum of tasks for the agent to learn on. In all cases, our work has shown such information can be effectively leveraged. After giving a high-level overview of this work, we will highlight a set of open questions and suggest where future work could be usefully focused.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\",\"url\":\"https://www.semanticscholar.org/author/39286677\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1901.05322\",\"authors\":[{\"authorId\":\"145306763\",\"name\":\"S. Amiri\"},{\"authorId\":\"40387301\",\"name\":\"M. Shirazi\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"}],\"doi\":\"10.1609/AAAI.V34I03.5659\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c149626b93c8949fb8b181c6220d8e8e8a558f98\",\"title\":\"Learning and Reasoning for Robot Sequential Decision Making under Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/c149626b93c8949fb8b181c6220d8e8e8a558f98\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1811.04272\",\"authors\":[{\"authorId\":\"1718764\",\"name\":\"Chao Yu\"},{\"authorId\":\"3449314\",\"name\":\"Tianpei Yang\"},{\"authorId\":\"12005987\",\"name\":\"Wenxuan Zhu\"},{\"authorId\":\"49370494\",\"name\":\"Dongxu Wang\"},{\"authorId\":\"46438968\",\"name\":\"Guangliang Li\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6724a592f272f9aa2facb59f802d5b9ca8911b89\",\"title\":\"Learning Shaping Strategies in Human-in-the-loop Interactive Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/6724a592f272f9aa2facb59f802d5b9ca8911b89\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122694181\",\"name\":\"Sushrut Bhalla\"},{\"authorId\":\"40963426\",\"name\":\"Sriram Ganapathi Subramanian\"},{\"authorId\":\"32755716\",\"name\":\"Mark Crowley\"}],\"doi\":\"10.1007/978-3-030-47358-7_7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"af472ae1b05d746cf13d5e71aeab5cf11e0dbaba\",\"title\":\"Deep Multi Agent Reinforcement Learning for Autonomous Driving\",\"url\":\"https://www.semanticscholar.org/paper/af472ae1b05d746cf13d5e71aeab5cf11e0dbaba\",\"venue\":\"Canadian Conference on AI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144136725\",\"name\":\"C. Yu\"},{\"authorId\":\"3449314\",\"name\":\"Tianpei Yang\"},{\"authorId\":\"12005987\",\"name\":\"Wenxuan Zhu\"},{\"authorId\":\"97436309\",\"name\":\"Yinzhao Dong\"},{\"authorId\":\"46438968\",\"name\":\"Guangliang Li\"}],\"doi\":\"10.5555/3398761.3399076\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"535d575b7a9874f22225bf8751639930b7eb8599\",\"title\":\"Interactive RL via Online Human Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/535d575b7a9874f22225bf8751639930b7eb8599\",\"venue\":\"AAMAS\",\"year\":2020}],\"corpusId\":51610104,\"doi\":\"10.24963/ijcai.2018/817\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"2769ea248be3ccdcd72c6c1c2bcc55d496c779a0\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2811287\",\"name\":\"Halit Bener Suay\"},{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2896f821c991824fc0bc96949e4baedc37fee06a\",\"title\":\"Learning from Demonstration for Shaping through Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2896f821c991824fc0bc96949e4baedc37fee06a\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1682788\",\"name\":\"A. Thomaz\"},{\"authorId\":\"1711777\",\"name\":\"C. Breazeal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0fcd047a1a6e391c8a6c140967355ec2a0ad8c9e\",\"title\":\"Reinforcement Learning with Human Teachers: Evidence of Feedback and Guidance with Implications for Learning Performance\",\"url\":\"https://www.semanticscholar.org/paper/0fcd047a1a6e391c8a6c140967355ec2a0ad8c9e\",\"venue\":\"AAAI\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"32182645\",\"name\":\"R. Loftin\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"145630067\",\"name\":\"D. Roberts\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14371054fc71f64d5c93d8e503e8d813b775b09e\",\"title\":\"A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans\",\"url\":\"https://www.semanticscholar.org/paper/14371054fc71f64d5c93d8e503e8d813b775b09e\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Peter Stone , and Yaxin Liu . Transfer Learning via Inter - Task Mappings for Temporal Difference Learning\",\"url\":\"\",\"venue\":\"Journal of Machine Learning Research\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M Pawan Kumar\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Benjamin Packer\",\"url\":\"\",\"venue\":\"and Daphne Koller. Self-paced learning for latent variable models. In Proc. of NIPS, pages 1189\\u20131197\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Karakovskiy\"},{\"authorId\":null,\"name\":\"J. Togelius. The Mario AI benchmark\"},{\"authorId\":null,\"name\":\"competitions. IEEE Transactions on Computational Intelligence\"},{\"authorId\":null,\"name\":\"AI in Games\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"4(1):55\\u201367\",\"url\":\"\",\"venue\":\"March\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"title\":\"Source Task Creation for Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Peter Stone , and Yaxin Liu . Transfer Learning via Inter - Task Mappings for Temporal Difference Learning\",\"url\":\"\",\"venue\":\"Journal of Machine Learning Research\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144626042\",\"name\":\"N. Jennings\"},{\"authorId\":\"9076478\",\"name\":\"K. Sycara\"},{\"authorId\":\"144885648\",\"name\":\"M. Wooldridge\"}],\"doi\":\"10.2174/9781608058242114010003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4e52bef2d8656d6732107393d67b1c371123574\",\"title\":\"Autonomous Agents and Multi-Agent Systems\",\"url\":\"https://www.semanticscholar.org/paper/d4e52bef2d8656d6732107393d67b1c371123574\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abdeslam Boularias\"},{\"authorId\":null,\"name\":\"Jens Kober\"},{\"authorId\":null,\"name\":\"Jan Peters. Relative entropy inverse reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AI Stats\",\"url\":\"\",\"venue\":\"pages 182\\u2013189,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoshua Bengio\"},{\"authorId\":null,\"name\":\"J\\u00e9r\\u00f4me Louradour\"},{\"authorId\":null,\"name\":\"Ronan Collobert\"},{\"authorId\":null,\"name\":\"Jason Weston. Curriculum learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 41\\u201348,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"2394697\",\"name\":\"Y. Liu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3412ded0375f8fe7336e82dc534eed994cac088\",\"title\":\"Transfer Learning via Inter-Task Mappings for Temporal Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/b3412ded0375f8fe7336e82dc534eed994cac088\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143817111\",\"name\":\"N. Cercone\"}],\"doi\":\"10.1007/978-981-10-0515-2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"521c0a9a920562c681aa01de5471d226f99c050f\",\"title\":\"Computational Linguistics\",\"url\":\"https://www.semanticscholar.org/paper/521c0a9a920562c681aa01de5471d226f99c050f\",\"venue\":\"Communications in Computer and Information Science\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Peter Stone. Cross-Domain Transfer for Reinforcement Lea Proc\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of ICML\",\"url\":\"\",\"venue\":\"pages 879-886,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"4454080\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1362879e77efef96ab552f5cb1198c2a67204d6\",\"title\":\"Introduction to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b1362879e77efef96ab552f5cb1198c2a67204d6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153615767\",\"name\":\"K. O. S. A. R. Miikkulainen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc7e431b4d8c432a3903042d8c63441ab0cfe3d4\",\"title\":\"Evolving Neural Network Agents in the NERO Video Game\",\"url\":\"https://www.semanticscholar.org/paper/dc7e431b4d8c432a3903042d8c63441ab0cfe3d4\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32182645\",\"name\":\"R. Loftin\"},{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"145522949\",\"name\":\"Jeff Huang\"},{\"authorId\":\"145630067\",\"name\":\"D. Roberts\"}],\"doi\":\"10.1007/s10458-015-9283-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8497e28e813e4d81f46be0c7908b5427f4fdbccd\",\"title\":\"Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning\",\"url\":\"https://www.semanticscholar.org/paper/8497e28e813e4d81f46be0c7908b5427f4fdbccd\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1597735.1597738\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"256c3bd45ab7452bb51721eb25d3367bb654225e\",\"title\":\"Interactively shaping agents via human reinforcement: the TAMER framework\",\"url\":\"https://www.semanticscholar.org/paper/256c3bd45ab7452bb51721eb25d3367bb654225e\",\"venue\":\"K-CAP '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", and Sonia Chernova . Learning from Demonstration for Shaping through Inverse Reinforcement Learning\",\"url\":\"\",\"venue\":\"Keepaway Soccer : From Machine Learning Testbed to Benchmark . In RoboCup - 2005 : Robot Soccer World Cup IX\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kenneth O. Stanley\"},{\"authorId\":null,\"name\":\"Bobby D. Bryant\"},{\"authorId\":null,\"name\":\"Risto Miikkulainen. Evolving neural network agents in th game\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In CIG\",\"url\":\"\",\"venue\":\"pages 182\\u2013189,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"1745029\",\"name\":\"M. V. D. Panne\"}],\"doi\":\"10.1007/978-3-642-30353-1_31\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6813efeb6a3ebbf04097308148e0b398204abee7\",\"title\":\"Curriculum Learning for Motor Skills\",\"url\":\"https://www.semanticscholar.org/paper/6813efeb6a3ebbf04097308148e0b398204abee7\",\"venue\":\"Canadian Conference on AI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sanmit Narvekar\"},{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Matteo Leonetti\"},{\"authorId\":null,\"name\":\"Peter Stone. Source task creation for curriculum learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 566-574,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Halit Bener Suay\"},{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Sonia Chernova. Learning from Demonstration for Shaping Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAMAS\",\"url\":\"\",\"venue\":\"pages 429\\u2013437,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46438968\",\"name\":\"Guangliang Li\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"},{\"authorId\":\"145581907\",\"name\":\"Hayley Hung\"}],\"doi\":\"10.1007/s10458-015-9308-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5477147ec61b85a9667002fe32671edcc35642ed\",\"title\":\"Using informative behavior to increase engagement while learning from human reward\",\"url\":\"https://www.semanticscholar.org/paper/5477147ec61b85a9667002fe32671edcc35642ed\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peter F Brown\"},{\"authorId\":null,\"name\":\"John Cocke\"},{\"authorId\":null,\"name\":\"Stephen A Della Pietra\"},{\"authorId\":null,\"name\":\"Vincent J Della Pietra\"},{\"authorId\":null,\"name\":\"Fredrick Jelinek\"},{\"authorId\":null,\"name\":\"John D Lafferty\"},{\"authorId\":null,\"name\":\"Robert L Mercer\"},{\"authorId\":null,\"name\":\"Paul S Roossin. A statistical approach to machine translation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Computational linguistics\",\"url\":\"\",\"venue\":\"16(2):79\\u201385,\",\"year\":1990},{\"arxivId\":\"1805.05769\",\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"1389208435\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":\"10.24963/ijcai.2017/534\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51f4ea9831e23777bf1cbfed165c1cffceb209ec\",\"title\":\"Leveraging Human Knowledge in Tabular Reinforcement Learning: A Study of Human Subjects\",\"url\":\"https://www.semanticscholar.org/paper/51f4ea9831e23777bf1cbfed165c1cffceb209ec\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32538203\",\"name\":\"Peter F. Brown\"},{\"authorId\":\"144716964\",\"name\":\"J. Cocke\"},{\"authorId\":\"2714577\",\"name\":\"S. D. Pietra\"},{\"authorId\":\"39944066\",\"name\":\"V. D. Pietra\"},{\"authorId\":\"2472759\",\"name\":\"F. Jelinek\"},{\"authorId\":\"1739581\",\"name\":\"J. Lafferty\"},{\"authorId\":\"2474650\",\"name\":\"R. Mercer\"},{\"authorId\":\"3069902\",\"name\":\"P. Roossin\"}],\"doi\":\"10.7551/mitpress/5779.003.0039\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a1066659ec1afee9dce586f6f49b7d44527827e1\",\"title\":\"A Statistical Approach to Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/a1066659ec1afee9dce586f6f49b7d44527827e1\",\"venue\":\"Comput. Linguistics\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144756076\",\"name\":\"Y. Lee\"},{\"authorId\":\"1794409\",\"name\":\"K. Grauman\"}],\"doi\":\"10.1109/CVPR.2011.5995523\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"8a1440354e299d5cac6f80f1f4102cc6c6546311\",\"title\":\"Learning the easy things first: Self-paced visual category discovery\",\"url\":\"https://www.semanticscholar.org/paper/8a1440354e299d5cac6f80f1f4102cc6c6546311\",\"venue\":\"CVPR 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2870663\",\"name\":\"Zhaodong Wang\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.24963/ijcai.2017/422\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"585182e93bc33daacfc7cf52744eae569e180c52\",\"title\":\"Improving Reinforcement Learning with Confidence-Based Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/585182e93bc33daacfc7cf52744eae569e180c52\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2811287\",\"name\":\"Halit Bener Suay\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"aaab75af076bbdbd4d9c5cd7a2c332b03f9b58d4\",\"title\":\"Integrating reinforcement learning with human demonstrations of varying ability\",\"url\":\"https://www.semanticscholar.org/paper/aaab75af076bbdbd4d9c5cd7a2c332b03f9b58d4\",\"venue\":\"AAMAS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrej Karpathy\"},{\"authorId\":null,\"name\":\"Michiel Van De Panne. Curriculum learning for motor skills\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Advances in AI\",\"url\":\"\",\"venue\":\"pages 1015\\u20131023,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47346762\",\"name\":\"M. Svetlik\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"9578995\",\"name\":\"Rishi Shah\"},{\"authorId\":\"145314605\",\"name\":\"Nick Walker\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42750855b95449ba74921bfecc84e2257a2c5b19\",\"title\":\"Automatic Curriculum Graph Generation for Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/42750855b95449ba74921bfecc84e2257a2c5b19\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"1693696\",\"name\":\"S. Devlin\"},{\"authorId\":\"2528631\",\"name\":\"Peter Vrancx\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dfb5a933d2bfa67b9980eb35d39b0e947a434f8e\",\"title\":\"Expressing Arbitrary Reward Functions as Potential-Based Advice\",\"url\":\"https://www.semanticscholar.org/paper/dfb5a933d2bfa67b9980eb35d39b0e947a434f8e\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Peter Stone\"},{\"authorId\":null,\"name\":\"Yaxin Liu. Transfer Learning via Inter-Task Mappings for Learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"8(1):2125\\u20132167,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"145805766\",\"name\":\"G. Kuhlmann\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2394697\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/11780519_9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ca0ea624b068042ca34264ddaa5e42620ceee875\",\"title\":\"Keepaway Soccer: From Machine Learning Testbed to Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/ca0ea624b068042ca34264ddaa5e42620ceee875\",\"venue\":\"RoboCup\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"32182645\",\"name\":\"R. Loftin\"},{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"145630067\",\"name\":\"D. Roberts\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ae9d1104c2f6321c67cb864413b10e39e4fdc724\",\"title\":\"Training an Agent to Ground Commands with Reward and Punishment\",\"url\":\"https://www.semanticscholar.org/paper/ae9d1104c2f6321c67cb864413b10e39e4fdc724\",\"venue\":\"AAAI 2014\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3717791\",\"name\":\"M. Kumar\"},{\"authorId\":\"1409971380\",\"name\":\"Ben Packer\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a049555721f17ed79a97fd492c8fc9a3f8f8aa17\",\"title\":\"Self-Paced Learning for Latent Variable Models\",\"url\":\"https://www.semanticscholar.org/paper/a049555721f17ed79a97fd492c8fc9a3f8f8aa17\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Halit Bener Suay\"},{\"authorId\":null,\"name\":\"Sonia Chernova. Integrating Reinforcement Learning with Proc\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAMAS\",\"url\":\"\",\"venue\":\"pages 617\\u2013624,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2463017\",\"name\":\"S. Karakovskiy\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/TCIAIG.2012.2188528\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"title\":\"The Mario AI Benchmark and Competitions\",\"url\":\"https://www.semanticscholar.org/paper/6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1273496.1273607\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"9db77cb43da46de6b0c5350348d74c949112e1e1\",\"title\":\"Cross-domain transfer for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/9db77cb43da46de6b0c5350348d74c949112e1e1\",\"venue\":\"ICML '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1836885\",\"name\":\"Brenna Argall\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"},{\"authorId\":\"1699032\",\"name\":\"B. Browning\"}],\"doi\":\"10.1016/j.robot.2008.10.024\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"title\":\"A survey of robot learning from demonstration\",\"url\":\"https://www.semanticscholar.org/paper/4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2209847\",\"name\":\"Abdeslam Boularias\"},{\"authorId\":\"145739642\",\"name\":\"J. Kober\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d4b16ae0b925eb7277cfe62ecac427e1427636f0\",\"title\":\"Relative Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d4b16ae0b925eb7277cfe62ecac427e1427636f0\",\"venue\":\"AISTATS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Peter Stone. Cross-Domain Transfer for Reinforcement Lea Proc\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"of ICML\",\"url\":\"\",\"venue\":\"pages 879-886,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Anna Harutyunyan\"},{\"authorId\":null,\"name\":\"Sam Devlin\"},{\"authorId\":null,\"name\":\"Peter Vrancx\"},{\"authorId\":null,\"name\":\"Ann Nowe. Expressing arbitrary reward functions as pot advice\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 2652\\u20132658,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhaodong Wang\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor. Improving Reinforcement Learning with C Demonstrations\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 3027\\u20133033,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Brenna D. Argall\"},{\"authorId\":null,\"name\":\"Sonia Chernova\"},{\"authorId\":null,\"name\":\"Manuela Veloso\"},{\"authorId\":null,\"name\":\"Brett Browning. A survey of robot learning from demonstr Syst.\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"57(5):469\\u2013483\",\"url\":\"\",\"venue\":\"May\",\"year\":2009}],\"title\":\"Improving Reinforcement Learning with Human Input\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"}],\"url\":\"https://www.semanticscholar.org/paper/2769ea248be3ccdcd72c6c1c2bcc55d496c779a0\",\"venue\":\"IJCAI\",\"year\":2018}\n"