"{\"abstract\":\"As it achieves a goal on behalf of its human user, an autonomous agent\\u2019s actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn\\u2019t explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2481388\",\"name\":\"S. Zhang\",\"url\":\"https://www.semanticscholar.org/author/2481388\"},{\"authorId\":\"1726491\",\"name\":\"E. Durfee\",\"url\":\"https://www.semanticscholar.org/author/1726491\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\",\"url\":\"https://www.semanticscholar.org/author/1699868\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1902.09725\",\"authors\":[{\"authorId\":\"49277224\",\"name\":\"A. Turner\"},{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":\"10.1145/3375627.3375851\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ed0b6aa983df3d6f458a6cb0ef53cb89157c4eb\",\"title\":\"Conservative Agency via Attainable Utility Preservation\",\"url\":\"https://www.semanticscholar.org/paper/9ed0b6aa983df3d6f458a6cb0ef53cb89157c4eb\",\"venue\":\"AIES\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2481388\",\"name\":\"S. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2eedf12b51f713f639a28c2f8dd28dec20417526\",\"title\":\"OnQuerying for Safe Optimality in Factored Markov Decision Processes Extended Abstract\",\"url\":\"https://www.semanticscholar.org/paper/2eedf12b51f713f639a28c2f8dd28dec20417526\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3305291\",\"name\":\"Sandhya Saisubramanian\"},{\"authorId\":\"1783184\",\"name\":\"Ece Kamar\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":\"10.5555/3398761.3399049\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd595732d5d2590c8f2c2b7390f5d2134d49f2eb\",\"title\":\"Mitigating the Negative Side Effects of Reasoning with Imperfect Models: A Multi-Objective Approach\",\"url\":\"https://www.semanticscholar.org/paper/cd595732d5d2590c8f2c2b7390f5d2134d49f2eb\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3305291\",\"name\":\"Sandhya Saisubramanian\"},{\"authorId\":\"1783184\",\"name\":\"Ece Kamar\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":\"10.24963/ijcai.2020/50\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"57d93b683a9fa660f751f6ba980b6dd8a3126f14\",\"title\":\"A Multi-Objective Approach to Mitigate Negative Side Effects\",\"url\":\"https://www.semanticscholar.org/paper/57d93b683a9fa660f751f6ba980b6dd8a3126f14\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49277224\",\"name\":\"A. Turner\"},{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e3bc514867234a59c63b3fc9c80a636ec6f5117\",\"title\":\"Conservative Agency\",\"url\":\"https://www.semanticscholar.org/paper/8e3bc514867234a59c63b3fc9c80a636ec6f5117\",\"venue\":\"AISafety@IJCAI\",\"year\":2019},{\"arxivId\":\"2010.07877\",\"authors\":[{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"33613300\",\"name\":\"R. Ngo\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"532d734c9d622cc6dd6aa6cb6b4dce032ad591aa\",\"title\":\"Avoiding Side Effects By Considering Future Tasks\",\"url\":\"https://www.semanticscholar.org/paper/532d734c9d622cc6dd6aa6cb6b4dce032ad591aa\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1811.07871\",\"authors\":[{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"145055042\",\"name\":\"David Krueger\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"51965508\",\"name\":\"Vishal Maini\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c6f913e4baa7f2c85363c0625c87003ad3b3a14c\",\"title\":\"Scalable agent alignment via reward modeling: a research direction\",\"url\":\"https://www.semanticscholar.org/paper/c6f913e4baa7f2c85363c0625c87003ad3b3a14c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.09355\",\"authors\":[{\"authorId\":\"3305291\",\"name\":\"Sandhya Saisubramanian\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a74c2b163534cd8b9ce41ad0ad08b2a1c6a0db95\",\"title\":\"Minimizing the Negative Side Effects of Planning with Reduced Models\",\"url\":\"https://www.semanticscholar.org/paper/a74c2b163534cd8b9ce41ad0ad08b2a1c6a0db95\",\"venue\":\"SafeAI@AAAI\",\"year\":2019},{\"arxivId\":\"2006.06547\",\"authors\":[{\"authorId\":\"49277224\",\"name\":\"A. Turner\"},{\"authorId\":\"13002147\",\"name\":\"Neale Ratzlaff\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6f028e8611417d813ed7939f2434e3fb8c1d641\",\"title\":\"Avoiding Side Effects in Complex Environments\",\"url\":\"https://www.semanticscholar.org/paper/b6f028e8611417d813ed7939f2434e3fb8c1d641\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1806.01186\",\"authors\":[{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0039dc54599ef609b3e7044f3b7cdfac354f79c2\",\"title\":\"Penalizing Side Effects using Stepwise Relative Reachability\",\"url\":\"https://www.semanticscholar.org/paper/0039dc54599ef609b3e7044f3b7cdfac354f79c2\",\"venue\":\"AISafety@IJCAI\",\"year\":2019},{\"arxivId\":\"2008.12146\",\"authors\":[{\"authorId\":\"3305291\",\"name\":\"Sandhya Saisubramanian\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"1783184\",\"name\":\"Ece Kamar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e2f4eabc957d9145db4df3e1c446a79ac26be7b5\",\"title\":\"Avoiding Negative Side Effects due to Incomplete Knowledge of AI Systems\",\"url\":\"https://www.semanticscholar.org/paper/e2f4eabc957d9145db4df3e1c446a79ac26be7b5\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":51609060,\"doi\":\"10.24963/ijcai.2018/676\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"de3c83b4e7b6ab907b050a1feb52e25f44ee41bd\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144389412\",\"name\":\"Kevin Regan\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"58e69a23f1d8a587923415f5a270b893c6e74c7e\",\"title\":\"Robust Policy Computation in Reward-Uncertain MDPs Using Nondominated Policies\",\"url\":\"https://www.semanticscholar.org/paper/58e69a23f1d8a587923415f5a270b893c6e74c7e\",\"venue\":\"AAAI\",\"year\":2010},{\"arxivId\":\"1205.4810\",\"authors\":[{\"authorId\":\"46756560\",\"name\":\"Teodor Mihai Moldovan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d66ab178e941e3a79954c858d3c1bcdee8e1d17d\",\"title\":\"Safe Exploration in Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/d66ab178e941e3a79954c858d3c1bcdee8e1d17d\",\"venue\":\"ICML\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47983192\",\"name\":\"Michael Laskey\"},{\"authorId\":\"3414099\",\"name\":\"S. Staszak\"},{\"authorId\":\"3414447\",\"name\":\"Wesley Yu-Shu Hsieh\"},{\"authorId\":\"144138312\",\"name\":\"Jeffrey Mahler\"},{\"authorId\":\"1881469\",\"name\":\"Florian T. Pokorny\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"},{\"authorId\":\"144344283\",\"name\":\"Ken Goldberg\"}],\"doi\":\"10.1109/ICRA.2016.7487167\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7427b806a5a7d3002e6b730c7706cdd9df509ab4\",\"title\":\"SHIV: Reducing supervisor burden in DAgger using support vectors for efficient learning from demonstrations in high dimensional state spaces\",\"url\":\"https://www.semanticscholar.org/paper/7427b806a5a7d3002e6b730c7706cdd9df509ab4\",\"venue\":\"2016 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2016},{\"arxivId\":\"1711.02827\",\"authors\":[{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"3458938\",\"name\":\"Smitha Milli\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"59094d64844ee21e32560fb08db6d53cc3af0c51\",\"title\":\"Inverse Reward Design\",\"url\":\"https://www.semanticscholar.org/paper/59094d64844ee21e32560fb08db6d53cc3af0c51\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2972848\",\"name\":\"Alexander Hans\"},{\"authorId\":\"2766932\",\"name\":\"Daniel Schneega\\u00df\"},{\"authorId\":\"34962728\",\"name\":\"Anton Maximilian Sch\\u00e4fer\"},{\"authorId\":\"1699265\",\"name\":\"S. Udluft\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ee27e9db2ae248d1254107852311117c4cda1c9\",\"title\":\"Safe exploration for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/5ee27e9db2ae248d1254107852311117c4cda1c9\",\"venue\":\"ESANN\",\"year\":2008},{\"arxivId\":\"1105.5460\",\"authors\":[{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"},{\"authorId\":\"39971338\",\"name\":\"T. Dean\"},{\"authorId\":\"38413017\",\"name\":\"S. Hanks\"}],\"doi\":\"10.1613/jair.575\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ae2b62d22b291ce13f07ca20f318a130ad949ab\",\"title\":\"Decision-Theoretic Planning: Structural Assumptions and Computational Leverage\",\"url\":\"https://www.semanticscholar.org/paper/5ae2b62d22b291ce13f07ca20f318a130ad949ab\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143935580\",\"name\":\"R. Cohn\"},{\"authorId\":\"1726491\",\"name\":\"E. Durfee\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c81a772e2c7b69d08af259fee0bab2d28a5d582f\",\"title\":\"Comparing Action-Query Strategies in Semi-Autonomous Agents\",\"url\":\"https://www.semanticscholar.org/paper/c81a772e2c7b69d08af259fee0bab2d28a5d582f\",\"venue\":\"AAAI\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2414946\",\"name\":\"S. Witwicki\"},{\"authorId\":\"1726491\",\"name\":\"E. Durfee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5d6bd41dd2ba6e04e0ad6e58587446225eb48072\",\"title\":\"Influence-Based Policy Abstraction for Weakly-Coupled Dec-POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/5d6bd41dd2ba6e04e0ad6e58587446225eb48072\",\"venue\":\"ICAPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2699820\",\"name\":\"D. A. Dolgov\"},{\"authorId\":\"1726491\",\"name\":\"E. Durfee\"}],\"doi\":\"10.1007/s10472-006-9038-x\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d335bd13e5446c301a92c3491a90a2d79e3779d8\",\"title\":\"Symmetric approximate linear programming for factored MDPs with application to constrained problems\",\"url\":\"https://www.semanticscholar.org/paper/d335bd13e5446c301a92c3491a90a2d79e3779d8\",\"venue\":\"Annals of Mathematics and Artificial Intelligence\",\"year\":2006},{\"arxivId\":\"1705.09990\",\"authors\":[{\"authorId\":\"3458938\",\"name\":\"Smitha Milli\"},{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":\"10.24963/ijcai.2017/662\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f9e4431e844cece73223f521e6d8e4a812b0ef1\",\"title\":\"Should Robots be Obedient?\",\"url\":\"https://www.semanticscholar.org/paper/0f9e4431e844cece73223f521e6d8e4a812b0ef1\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722628\",\"name\":\"A. Nilim\"},{\"authorId\":\"1701847\",\"name\":\"L. Ghaoui\"}],\"doi\":\"10.1287/opre.1050.0216\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6db16608fccddef51202af84112b34cfebfbe20a\",\"title\":\"Robust Control of Markov Decision Processes with Uncertain Transition Matrices\",\"url\":\"https://www.semanticscholar.org/paper/6db16608fccddef51202af84112b34cfebfbe20a\",\"venue\":\"Oper. Res.\",\"year\":2005},{\"arxivId\":\"1705.10528\",\"authors\":[{\"authorId\":\"3381809\",\"name\":\"Joshua Achiam\"},{\"authorId\":\"145641013\",\"name\":\"David Held\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a4193d0b042643a8bb9ec262ed7f9d509bdb12e\",\"title\":\"Constrained Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/7a4193d0b042643a8bb9ec262ed7f9d509bdb12e\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1705.05427\",\"authors\":[{\"authorId\":\"39617345\",\"name\":\"K. Amin\"},{\"authorId\":null,\"name\":\"Nan Jiang\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7251354318a25b0f304bfe756c8749d492106139\",\"title\":\"Repeated Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7251354318a25b0f304bfe756c8749d492106139\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1711.09883\",\"authors\":[{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"145981974\",\"name\":\"Pedro A. Ortega\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"8455031\",\"name\":\"Andrew Lefrancq\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d09bec5af4eef5038e48b26b6c14098f95997114\",\"title\":\"AI Safety Gridworlds\",\"url\":\"https://www.semanticscholar.org/paper/d09bec5af4eef5038e48b26b6c14098f95997114\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1606.06565\",\"authors\":[{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"},{\"authorId\":\"153301219\",\"name\":\"Chris Olah\"},{\"authorId\":\"5164568\",\"name\":\"J. Steinhardt\"},{\"authorId\":\"29848635\",\"name\":\"Paul F. Christiano\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143767989\",\"name\":\"Dan Man\\u00e9\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e86f71ca2948d17b003a5f068db1ecb2b77827f7\",\"title\":\"Concrete Problems in AI Safety\",\"url\":\"https://www.semanticscholar.org/paper/e86f71ca2948d17b003a5f068db1ecb2b77827f7\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2481388\",\"name\":\"S. Zhang\"},{\"authorId\":\"1726491\",\"name\":\"E. Durfee\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d55ae48e6173d039f5c1d1d1802160f8e13eebc\",\"title\":\"Approximately-Optimal Queries for Planning in Reward-Uncertain Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/9d55ae48e6173d039f5c1d1d1802160f8e13eebc\",\"venue\":\"ICAPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2188691\",\"name\":\"P. Viappiani\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":\"10.1145/1639714.1639732\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"92d34b829ea9440da66b1751bed74013956b012b\",\"title\":\"Regret-based optimal recommendation sets in conversational recommender systems\",\"url\":\"https://www.semanticscholar.org/paper/92d34b829ea9440da66b1751bed74013956b012b\",\"venue\":\"RecSys '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1403725425\",\"name\":\"F. Teichteil-K\\u00f6nigsbuch\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5633b907038340ad9dafa02cc389379f56bc1dc9\",\"title\":\"Stochastic Safest and Shortest Path Problems\",\"url\":\"https://www.semanticscholar.org/paper/5633b907038340ad9dafa02cc389379f56bc1dc9\",\"venue\":\"AAAI\",\"year\":2012},{\"arxivId\":\"1205.2619\",\"authors\":[{\"authorId\":\"144389412\",\"name\":\"Kevin Regan\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d00197e159695d93796a97e8a625d7f1f3ad099\",\"title\":\"Regret-based Reward Elicitation for Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/8d00197e159695d93796a97e8a625d7f1f3ad099\",\"venue\":\"UAI\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144834415\",\"name\":\"Paul Weng\"},{\"authorId\":\"2486264\",\"name\":\"B. Zanuttini\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4de2844ed244a5d0cb25324e691b9b2f97c1f155\",\"title\":\"Interactive Value Iteration for Markov Decision Processes with Unknown Rewards\",\"url\":\"https://www.semanticscholar.org/paper/4de2844ed244a5d0cb25324e691b9b2f97c1f155\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":\"1210.4875\",\"authors\":[{\"authorId\":\"6247481\",\"name\":\"A. Kolobov\"},{\"authorId\":\"2674444\",\"name\":\"Mausam\"},{\"authorId\":\"1780531\",\"name\":\"Daniel S. Weld\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b879ee4a07228cdbbaa1c6e31ffa3693c857892\",\"title\":\"A Theory of Goal-Oriented MDPs with Dead Ends\",\"url\":\"https://www.semanticscholar.org/paper/7b879ee4a07228cdbbaa1c6e31ffa3693c857892\",\"venue\":\"UAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0f2c4104ef6e36bb67022001179887e6600d24d\",\"title\":\"A comprehensive survey on safe reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/c0f2c4104ef6e36bb67022001179887e6600d24d\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2015}],\"title\":\"Minimax-Regret Querying on Side Effects for Safe Optimality in Factored Markov Decision Processes\",\"topics\":[{\"topic\":\"Minimax\",\"topicId\":\"41706\",\"url\":\"https://www.semanticscholar.org/topic/41706\"},{\"topic\":\"Regret (decision theory)\",\"topicId\":\"528786\",\"url\":\"https://www.semanticscholar.org/topic/528786\"},{\"topic\":\"Autonomous agent\",\"topicId\":\"29926\",\"url\":\"https://www.semanticscholar.org/topic/29926\"},{\"topic\":\"Side effect (computer science)\",\"topicId\":\"156892\",\"url\":\"https://www.semanticscholar.org/topic/156892\"},{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Heuristic\",\"topicId\":\"4146\",\"url\":\"https://www.semanticscholar.org/topic/4146\"},{\"topic\":\"Automated planning and scheduling\",\"topicId\":\"3649\",\"url\":\"https://www.semanticscholar.org/topic/3649\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"},{\"topic\":\"Autonomous robot\",\"topicId\":\"1175\",\"url\":\"https://www.semanticscholar.org/topic/1175\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"}],\"url\":\"https://www.semanticscholar.org/paper/de3c83b4e7b6ab907b050a1feb52e25f44ee41bd\",\"venue\":\"IJCAI\",\"year\":2018}\n"