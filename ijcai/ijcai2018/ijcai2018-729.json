"{\"abstract\":\"We consider the problem of off-policy policy selection in reinforcement learning: using historical data generated from running one policy to compare two or more policies. We show that approaches based on importance sampling can be unfair\\u2014they can select the worse of two policies more often than not. We give two examples where the unfairness of importance sampling could be practically concerning. We then present sufficient conditions to theoretically guarantee fairness and a related notion of safety. Finally, we provide a practical importance sampling-based estimator to help mitigate one of the systematic sources of unfairness resulting from using importance sampling for policy selection.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"3396583\",\"name\":\"Shayan Doroudi\",\"url\":\"https://www.semanticscholar.org/author/3396583\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\",\"url\":\"https://www.semanticscholar.org/author/143640165\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\",\"url\":\"https://www.semanticscholar.org/author/2563117\"}],\"citationVelocity\":7,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"31411877\",\"name\":\"Aniruddh Raghu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"963d5eacfb0a16e5901c5ff044324db6d35f7928\",\"title\":\"Reinforcement Learning for Sepsis Treatment: Baselines and Analysis\",\"url\":\"https://www.semanticscholar.org/paper/963d5eacfb0a16e5901c5ff044324db6d35f7928\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.09490\",\"authors\":[{\"authorId\":\"144030228\",\"name\":\"P. Awasthi\"},{\"authorId\":\"145115014\",\"name\":\"Corinna Cortes\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"},{\"authorId\":\"78659204\",\"name\":\"M. Mohri\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"802a086540c20ce2c48986f1679283c5a7438de8\",\"title\":\"Beyond Individual and Group Fairness\",\"url\":\"https://www.semanticscholar.org/paper/802a086540c20ce2c48986f1679283c5a7438de8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1908.01618\",\"authors\":[{\"authorId\":\"50046611\",\"name\":\"N. Hussain\"},{\"authorId\":\"1749677\",\"name\":\"E. Erzin\"},{\"authorId\":\"143765782\",\"name\":\"T. M. Sezgin\"},{\"authorId\":\"48474048\",\"name\":\"Y. Yemez\"}],\"doi\":\"10.21437/interspeech.2019-2521\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aa892c6ec1b7f62db82681688c57a629c5f0bf85\",\"title\":\"Speech Driven Backchannel Generation using Deep Q-Network for Enhancing Engagement in Human-Robot Interaction\",\"url\":\"https://www.semanticscholar.org/paper/aa892c6ec1b7f62db82681688c57a629c5f0bf85\",\"venue\":\"INTERSPEECH\",\"year\":2019},{\"arxivId\":\"1810.08810\",\"authors\":[{\"authorId\":\"2082393\",\"name\":\"A. Chouldechova\"},{\"authorId\":\"1682008\",\"name\":\"A. Roth\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8285e1b5536ce11d55462ae757f61c75ec6773c6\",\"title\":\"The Frontiers of Fairness in Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/8285e1b5536ce11d55462ae757f61c75ec6773c6\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34719248\",\"name\":\"Josiah P. Hanna\"}],\"doi\":\"10.26153/TSW/7716\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4f1d5b1473bf6e46c989dadbe1cf3d3a66a01e71\",\"title\":\"Data efficient reinforcement learning with off-policy and simulated data\",\"url\":\"https://www.semanticscholar.org/paper/4f1d5b1473bf6e46c989dadbe1cf3d3a66a01e71\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.06919\",\"authors\":[{\"authorId\":\"46235335\",\"name\":\"Mengjiao Yang\"},{\"authorId\":\"144445937\",\"name\":\"Bo Dai\"},{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"145499435\",\"name\":\"G. Tucker\"},{\"authorId\":\"50319359\",\"name\":\"D. Schuurmans\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2431a6b99b5d8a46a4b9a98c9c5d3f546aeb13d\",\"title\":\"Offline Policy Selection under Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/a2431a6b99b5d8a46a4b9a98c9c5d3f546aeb13d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1807.01066\",\"authors\":[{\"authorId\":\"31411877\",\"name\":\"Aniruddh Raghu\"},{\"authorId\":\"34772064\",\"name\":\"Omer Gottesman\"},{\"authorId\":\"47908694\",\"name\":\"Y. Liu\"},{\"authorId\":\"6204450\",\"name\":\"M. Komorowski\"},{\"authorId\":\"144793784\",\"name\":\"Aldo Faisal\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bf38b796a0f3da7c61b13846144c0961a5a40405\",\"title\":\"Behaviour Policy Estimation in Off-Policy Policy Evaluation: Calibration Matters\",\"url\":\"https://www.semanticscholar.org/paper/bf38b796a0f3da7c61b13846144c0961a5a40405\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47701912\",\"name\":\"P. Schulam\"},{\"authorId\":\"1932128\",\"name\":\"S. Saria\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0af8d24b14614499393b50f71070d68cf299d52c\",\"title\":\"Reliable Decision Support using Counterfactual Models\",\"url\":\"https://www.semanticscholar.org/paper/0af8d24b14614499393b50f71070d68cf299d52c\",\"venue\":\"NIPS 2017\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34719248\",\"name\":\"Josiah P. Hanna\"},{\"authorId\":\"2791038\",\"name\":\"S. Niekum\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"da31c0c0b7ef941b9315c0824403193408abaec6\",\"title\":\"Evaluation with an Estimated Behavior Policy\",\"url\":\"https://www.semanticscholar.org/paper/da31c0c0b7ef941b9315c0824403193408abaec6\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1908.02037\",\"authors\":[{\"authorId\":\"50046611\",\"name\":\"N. Hussain\"},{\"authorId\":\"1749677\",\"name\":\"E. Erzin\"},{\"authorId\":\"143765782\",\"name\":\"T. M. Sezgin\"},{\"authorId\":\"48474048\",\"name\":\"Y. Yemez\"}],\"doi\":\"10.1109/ACII.2019.8925443\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2688ddf96c2454d275dfc9a21b2d43961599eab0\",\"title\":\"Batch Recurrent Q-Learning for Backchannel Generation Towards Engaging Agents\",\"url\":\"https://www.semanticscholar.org/paper/2688ddf96c2454d275dfc9a21b2d43961599eab0\",\"venue\":\"2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)\",\"year\":2019},{\"arxivId\":\"1910.12008\",\"authors\":[{\"authorId\":\"1954250\",\"name\":\"Aditya Grover\"},{\"authorId\":\"5175376\",\"name\":\"Kristy Choi\"},{\"authorId\":\"1978777\",\"name\":\"Rui Shu\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7ffa8d713c020316bea9347ac16944716c1d8c4\",\"title\":\"Fair Generative Modeling via Weak Supervision\",\"url\":\"https://www.semanticscholar.org/paper/d7ffa8d713c020316bea9347ac16944716c1d8c4\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24717227\",\"name\":\"Alberto Maria Metelli\"},{\"authorId\":\"145388375\",\"name\":\"M. Papini\"},{\"authorId\":\"2021278873\",\"name\":\"Nico Montali\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4ee74a7e57bfad8199e1794c934c6b263321fde\",\"title\":\"Importance Sampling Techniques for Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a4ee74a7e57bfad8199e1794c934c6b263321fde\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2020},{\"arxivId\":\"1809.06098\",\"authors\":[{\"authorId\":\"24717227\",\"name\":\"Alberto Maria Metelli\"},{\"authorId\":\"145388375\",\"name\":\"M. Papini\"},{\"authorId\":\"79787170\",\"name\":\"Francesco Faccio\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a58733db98ea7bf79dd600a87581dae27de5c96f\",\"title\":\"Policy Optimization via Importance Sampling\",\"url\":\"https://www.semanticscholar.org/paper/a58733db98ea7bf79dd600a87581dae27de5c96f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1909.09141\",\"authors\":[{\"authorId\":\"3422145\",\"name\":\"Elliot Creager\"},{\"authorId\":\"40373515\",\"name\":\"David Madras\"},{\"authorId\":\"1695317\",\"name\":\"T. Pitassi\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8fa088f56b93cd016a31e7f2a1a506bfc6fb2e14\",\"title\":\"Causal Modeling for Fairness in Dynamical Systems\",\"url\":\"https://www.semanticscholar.org/paper/8fa088f56b93cd016a31e7f2a1a506bfc6fb2e14\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3396583\",\"name\":\"Shayan Doroudi\"},{\"authorId\":\"1779915\",\"name\":\"V. Aleven\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":\"10.1007/s40593-019-00187-x\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5c006725aeff2c749acc9f407cceb97652c33a9\",\"title\":\"Where\\u2019s the Reward?\",\"url\":\"https://www.semanticscholar.org/paper/c5c006725aeff2c749acc9f407cceb97652c33a9\",\"venue\":\"International Journal of Artificial Intelligence in Education\",\"year\":2019},{\"arxivId\":\"1806.01347\",\"authors\":[{\"authorId\":\"35362968\",\"name\":\"Josiah Hanna\"},{\"authorId\":\"2791038\",\"name\":\"S. Niekum\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dbba10eb7fb37626ea554cf7f64ef514ddd642be\",\"title\":\"Importance Sampling Policy Evaluation with an Estimated Behavior Policy\",\"url\":\"https://www.semanticscholar.org/paper/dbba10eb7fb37626ea554cf7f64ef514ddd642be\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2765354\",\"name\":\"H. Kono\"},{\"authorId\":\"1578470558\",\"name\":\"Ren Katayama\"},{\"authorId\":\"72860029\",\"name\":\"Yusaku Takakuwa\"},{\"authorId\":\"153939557\",\"name\":\"Wen Wen\"},{\"authorId\":\"2716116\",\"name\":\"Tsuyoshi Suzuki\"}],\"doi\":\"10.14569/ijacsa.2019.0101202\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd0cd98da852290705c2519857af818a63b3370a\",\"title\":\"Activation and Spreading Sequence for Spreading Activation Policy Selection Method in Transfer Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bd0cd98da852290705c2519857af818a63b3370a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2082393\",\"name\":\"A. Chouldechova\"},{\"authorId\":\"1682008\",\"name\":\"A. Roth\"}],\"doi\":\"10.1145/3376898\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"48336cd6e6aa2e94446e2be6bbbb7ce2c3815c5c\",\"title\":\"A snapshot of the frontiers of fairness in machine learning\",\"url\":\"https://www.semanticscholar.org/paper/48336cd6e6aa2e94446e2be6bbbb7ce2c3815c5c\",\"venue\":\"Commun. ACM\",\"year\":2020},{\"arxivId\":\"1812.08997\",\"authors\":[{\"authorId\":\"2124869\",\"name\":\"K. Lee\"},{\"authorId\":\"2789430\",\"name\":\"J. Choi\"},{\"authorId\":\"9959922\",\"name\":\"Moonsu Cha\"},{\"authorId\":\"8152273\",\"name\":\"J. K. Lee\"},{\"authorId\":\"46760261\",\"name\":\"T. Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e934c882973d27f9fb3257f2970bc56fbbf391f\",\"title\":\"Stochastic Doubly Robust Gradient\",\"url\":\"https://www.semanticscholar.org/paper/7e934c882973d27f9fb3257f2970bc56fbbf391f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.09751\",\"authors\":[{\"authorId\":\"3414700\",\"name\":\"Xinkun Nie\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"},{\"authorId\":\"90481786\",\"name\":\"S. Wager\"}],\"doi\":\"10.1080/01621459.2020.1831925\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc65f1fbf0ac429ffa1104cd8e026df861167331\",\"title\":\"Learning When-to-Treat Policies\",\"url\":\"https://www.semanticscholar.org/paper/bc65f1fbf0ac429ffa1104cd8e026df861167331\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2008.00104\",\"authors\":[{\"authorId\":\"47182790\",\"name\":\"M. Mladenov\"},{\"authorId\":\"3422145\",\"name\":\"Elliot Creager\"},{\"authorId\":\"1388406825\",\"name\":\"Omer Ben-Porat\"},{\"authorId\":\"1754860\",\"name\":\"Kevin Swersky\"},{\"authorId\":\"1804104\",\"name\":\"R. Zemel\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cfce5fe5bcfbecb989e616f966a056cbb6b50dcc\",\"title\":\"Optimizing Long-term Social Welfare in Recommender Systems: A Constrained Matching Approach\",\"url\":\"https://www.semanticscholar.org/paper/cfce5fe5bcfbecb989e616f966a056cbb6b50dcc\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"2002.04083\",\"authors\":[{\"authorId\":\"39965049\",\"name\":\"Ioana Bica\"},{\"authorId\":\"145629795\",\"name\":\"A. Alaa\"},{\"authorId\":\"4690510\",\"name\":\"J. Jordon\"},{\"authorId\":\"1729969\",\"name\":\"M. V. D. Schaar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0cdd98cfdd9fa4b7fa4dd67b561cbac6654d0cb\",\"title\":\"Estimating Counterfactual Treatment Outcomes over Time Through Adversarially Balanced Representations\",\"url\":\"https://www.semanticscholar.org/paper/f0cdd98cfdd9fa4b7fa4dd67b561cbac6654d0cb\",\"venue\":\"ICLR\",\"year\":2020}],\"corpusId\":30003831,\"doi\":\"10.24963/ijcai.2018/729\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"a351b0c02424c5cd6158dc4c375624115c3e9807\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144137911\",\"name\":\"T. Mandel\"},{\"authorId\":\"38057113\",\"name\":\"Yun-En Liu\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"},{\"authorId\":\"1986848\",\"name\":\"Z. Popovic\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"471e452dc02edcb9c8c0ec446cc2eb22188dd86b\",\"title\":\"Offline policy evaluation across representations with applications to educational games\",\"url\":\"https://www.semanticscholar.org/paper/471e452dc02edcb9c8c0ec446cc2eb22188dd86b\",\"venue\":\"AAMAS\",\"year\":2014},{\"arxivId\":\"1511.03722\",\"authors\":[{\"authorId\":null,\"name\":\"Nan Jiang\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2fdb536da39a014c598ea67b0db88431fcd852a8\",\"title\":\"Doubly Robust Off-policy Value Evaluation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2fdb536da39a014c598ea67b0db88431fcd852a8\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"78020db7e3d968f6e6cc26d18e31e5b668ca7fee\",\"title\":\"Eligibility Traces for Off-Policy Policy Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/78020db7e3d968f6e6cc26d18e31e5b668ca7fee\",\"venue\":\"ICML\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e36714eabc5bba8f26bfea1d3ee77e50ec5e79c6\",\"title\":\"High-Confidence Off-Policy Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/e36714eabc5bba8f26bfea1d3ee77e50ec5e79c6\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98292699\",\"name\":\"S. Trybu\\u0142a\"}],\"doi\":\"10.4064/AM-8-2-143-156\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"628e0a5d4b01c2f18df6a033997835e18b7ab5cc\",\"title\":\"On the paradox of n random variables\",\"url\":\"https://www.semanticscholar.org/paper/628e0a5d4b01c2f18df6a033997835e18b7ab5cc\",\"venue\":\"\",\"year\":1965},{\"arxivId\":\"1604.00923\",\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ec8a2f6cfe72309f5f1608d22ec28778d3ee976a\",\"title\":\"Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ec8a2f6cfe72309f5f1608d22ec28778d3ee976a\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1611.01224\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a43d91c8d883e3463b358571125fa0ec7298b3a\",\"title\":\"Sample Efficient Actor-Critic with Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/6a43d91c8d883e3463b358571125fa0ec7298b3a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1611.03071\",\"authors\":[{\"authorId\":\"1775136\",\"name\":\"S. Jabbari\"},{\"authorId\":\"144525693\",\"name\":\"Matthew Joseph\"},{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"144848816\",\"name\":\"J. Morgenstern\"},{\"authorId\":\"1682008\",\"name\":\"A. Roth\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2868d78670aca58bb0e6e09eee43e7a2f6ea1333\",\"title\":\"Fairness in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2868d78670aca58bb0e6e09eee43e7a2f6ea1333\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1708.05448\",\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"145471664\",\"name\":\"B. Silva\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"adb7f9475e438c760c904a2e5f6d6366697503b1\",\"title\":\"On Ensuring that Intelligent Machines Are Well-Behaved\",\"url\":\"https://www.semanticscholar.org/paper/adb7f9475e438c760c904a2e5f6d6366697503b1\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Doroudi et al\"},{\"authorId\":null,\"name\":\"2017 Shayan Doroudi\"},{\"authorId\":null,\"name\":\"Philip S. Thomas\"},{\"authorId\":null,\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Importance sampling for fair policy selection\",\"url\":\"\",\"venue\":\"In Uncertainity in Artificial Intelligence. Association of Uncertainty in Artificial Intelligence,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43308a6a0d34b9d33b8f420e1678d54640a25a47\",\"title\":\"On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/43308a6a0d34b9d33b8f420e1678d54640a25a47\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"127cf09abf45c9d3cb4d859a72e6cc67acc2b57b\",\"title\":\"High Confidence Policy Improvement\",\"url\":\"https://www.semanticscholar.org/paper/127cf09abf45c9d3cb4d859a72e6cc67acc2b57b\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2294095\",\"name\":\"M. Powell\"},{\"authorId\":\"40543697\",\"name\":\"J. Swann\"}],\"doi\":\"10.1093/IMAMAT/2.3.228\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6992e1715de5cccd98d2ecea40184a022a6b67df\",\"title\":\"Weighted Uniform Sampling \\u2014 a Monte Carlo Technique for Reducing Variance\",\"url\":\"https://www.semanticscholar.org/paper/6992e1715de5cccd98d2ecea40184a022a6b67df\",\"venue\":\"\",\"year\":1966},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Gardner\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Paradox of nontransitive dice and elusive principle of indifference\",\"url\":\"\",\"venue\":\"Scientific American,\",\"year\":1970},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"244539f454800697ed663326b7cfba337ca0c2ec\",\"title\":\"Guided Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/244539f454800697ed663326b7cfba337ca0c2ec\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":\"1611.03451\",\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2b319e33a31d290f30ea197b3279e04348e4f401\",\"title\":\"Importance Sampling with Unequal Support\",\"url\":\"https://www.semanticscholar.org/paper/2b319e33a31d290f30ea197b3279e04348e4f401\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1103.4601\",\"authors\":[{\"authorId\":\"144652072\",\"name\":\"M. Dud\\u00edk\"},{\"authorId\":\"144162125\",\"name\":\"J. Langford\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5ccf7658018981bf492d0c8d66277d22ebaac815\",\"title\":\"Doubly Robust Policy Evaluation and Learning\",\"url\":\"https://www.semanticscholar.org/paper/5ccf7658018981bf492d0c8d66277d22ebaac815\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98292699\",\"name\":\"S. Trybu\\u0142a\"}],\"doi\":\"10.4064/AM-5-4-321-332\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"963077afd54d58151e6b368ad2f51dc9bc211f25\",\"title\":\"On the paradox of three random variables\",\"url\":\"https://www.semanticscholar.org/paper/963077afd54d58151e6b368ad2f51dc9bc211f25\",\"venue\":\"\",\"year\":1961}],\"title\":\"Importance Sampling for Fair Policy Selection\",\"topics\":[{\"topic\":\"Importance sampling\",\"topicId\":\"96047\",\"url\":\"https://www.semanticscholar.org/topic/96047\"},{\"topic\":\"Sampling (signal processing)\",\"topicId\":\"7839\",\"url\":\"https://www.semanticscholar.org/topic/7839\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Fairness measure\",\"topicId\":\"92367\",\"url\":\"https://www.semanticscholar.org/topic/92367\"},{\"topic\":\"Gibbs sampling\",\"topicId\":\"43651\",\"url\":\"https://www.semanticscholar.org/topic/43651\"}],\"url\":\"https://www.semanticscholar.org/paper/a351b0c02424c5cd6158dc4c375624115c3e9807\",\"venue\":\"UAI\",\"year\":2017}\n"