"{\"abstract\":\"In this paper, we propose a deep reinforcement learning algorithm to learn multiple tasks concurrently. A new network architecture is proposed in the algorithm which reduces the number of parameters needed by more than 75% per task compared to typical single-task deep reinforcement learning algorithms. The proposed algorithm and network fuse images with sensor data and were tested with up to 12 movement-based control tasks on a simulated Pioneer 3AT robot equipped with a camera and range sensors. Results show that the proposed algorithm and network can learn skills that are as good as the skills learned by a comparable single-task learning algorithm. Results also show that learning performance is consistent even when the number of tasks and the number of constraints on the tasks increased.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\",\"url\":\"https://www.semanticscholar.org/author/47087136\"},{\"authorId\":\"1758755\",\"name\":\"Kathryn E. Merrick\",\"url\":\"https://www.semanticscholar.org/author/1758755\"},{\"authorId\":\"1713460\",\"name\":\"H. Abbass\",\"url\":\"https://www.semanticscholar.org/author/1713460\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\",\"url\":\"https://www.semanticscholar.org/author/144838978\"}],\"citationVelocity\":11,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"},{\"authorId\":\"1754204\",\"name\":\"L. Zhang\"},{\"authorId\":\"9236777\",\"name\":\"Jiakai Shen\"},{\"authorId\":\"50621973\",\"name\":\"Qingcai Wang\"},{\"authorId\":\"1721227\",\"name\":\"G. Liu\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207328\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ec26f9d6ecc93e5586b4bb01b23b81baa688c4d\",\"title\":\"Seismic Event Detection via Deep Multi-Task Learning\",\"url\":\"https://www.semanticscholar.org/paper/5ec26f9d6ecc93e5586b4bb01b23b81baa688c4d\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11020126\",\"name\":\"Parijat Dewangan\"},{\"authorId\":\"8954660\",\"name\":\"Abhishek Sarkar\"},{\"authorId\":\"145211574\",\"name\":\"K. Madhava Krishna\"},{\"authorId\":\"4868221\",\"name\":\"Suril Vijaykumar Shah\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1cdcdb407dd09b91f4b637e80b7fc23280c02489\",\"title\":\"Multi-task Reinforcement Learning for shared action spaces in Robotic Systems\",\"url\":\"https://www.semanticscholar.org/paper/1cdcdb407dd09b91f4b637e80b7fc23280c02489\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1908.10255\",\"authors\":[{\"authorId\":\"84635100\",\"name\":\"Andreas L. H. Gerken\"},{\"authorId\":\"145570895\",\"name\":\"Michael Spranger\"}],\"doi\":\"10.1109/ICRA.2019.8794347\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"601e6a3747371c8c7f86bdd21062f53b561ad9cd\",\"title\":\"Continuous Value Iteration (CVI) Reinforcement Learning and Imaginary Experience Replay (IER) For Learning Multi-Goal, Continuous Action and State Space Controllers\",\"url\":\"https://www.semanticscholar.org/paper/601e6a3747371c8c7f86bdd21062f53b561ad9cd\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":\"2009.00249\",\"authors\":[{\"authorId\":\"3717358\",\"name\":\"Tan Tang\"},{\"authorId\":\"47370114\",\"name\":\"Renzhong Li\"},{\"authorId\":\"1699615900\",\"name\":\"Xinke Wu\"},{\"authorId\":\"1500428907\",\"name\":\"Shuhan Liu\"},{\"authorId\":\"3037070\",\"name\":\"Johannes Knittel\"},{\"authorId\":\"34520778\",\"name\":\"S. Koch\"},{\"authorId\":\"144784290\",\"name\":\"T. Ertl\"},{\"authorId\":\"153154036\",\"name\":\"L. Yu\"},{\"authorId\":\"3246404\",\"name\":\"P. Ren\"},{\"authorId\":\"121962020\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/TVCG.2020.3030467\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"392a2b92a0be230f8bb2a26c0e70cebc11ad6ff9\",\"title\":\"PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/392a2b92a0be230f8bb2a26c0e70cebc11ad6ff9\",\"venue\":\"IEEE transactions on visualization and computer graphics\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46895706\",\"name\":\"Wenfeng Song\"},{\"authorId\":\"48830645\",\"name\":\"Shuai Li\"},{\"authorId\":\"103054850\",\"name\":\"Tao Chang\"},{\"authorId\":\"2252725\",\"name\":\"Ai-min Hao\"},{\"authorId\":\"20658737\",\"name\":\"Q. Zhao\"},{\"authorId\":\"100787805\",\"name\":\"Hong Qin\"}],\"doi\":\"10.1109/TIP.2019.2953587\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58a06477685794544e34d03a9190ca20a1a8e505\",\"title\":\"Context-Interactive CNN for Person Re-Identification\",\"url\":\"https://www.semanticscholar.org/paper/58a06477685794544e34d03a9190ca20a1a8e505\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492113723\",\"name\":\"Jing Li\"},{\"authorId\":\"82089761\",\"name\":\"Xing Zhang\"},{\"authorId\":\"1519048272\",\"name\":\"J. Zhang\"},{\"authorId\":\"2017491\",\"name\":\"J. Wu\"},{\"authorId\":\"123555217\",\"name\":\"Q. Sun\"},{\"authorId\":\"47779481\",\"name\":\"Yuxuan Xie\"}],\"doi\":\"10.1109/TCCN.2019.2954396\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3096578fc2504cb543107759f1ac894003d98576\",\"title\":\"Deep Reinforcement Learning-Based Mobility-Aware Robust Proactive Resource Allocation in Heterogeneous Networks\",\"url\":\"https://www.semanticscholar.org/paper/3096578fc2504cb543107759f1ac894003d98576\",\"venue\":\"IEEE Transactions on Cognitive Communications and Networking\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67329496\",\"name\":\"T. Vuong\"},{\"authorId\":\"30436475\",\"name\":\"D. Nguyen\"},{\"authorId\":\"151492445\",\"name\":\"Tai-Long Nguyen\"},{\"authorId\":\"151495838\",\"name\":\"Cong-Minh Bui\"},{\"authorId\":\"67032833\",\"name\":\"Hai-Dang Kieu\"},{\"authorId\":\"3301579\",\"name\":\"Viet-Cuong Ta\"},{\"authorId\":\"3285873\",\"name\":\"Quoc-Long Tran\"},{\"authorId\":\"3188009\",\"name\":\"T. L\\u00ea\"}],\"doi\":\"10.24963/ijcai.2019/505\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"19d9134effebb79799b1ed6189109b5c7bc56e24\",\"title\":\"Sharing Experience in Multitask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/19d9134effebb79799b1ed6189109b5c7bc56e24\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"1758755\",\"name\":\"Kathryn E. Merrick\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"},{\"authorId\":\"1713460\",\"name\":\"H. Abbass\"}],\"doi\":\"10.1109/TNNLS.2018.2805379\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bf8d58faf972ad0a1026c0a7c5577c07996ef3a7\",\"title\":\"Hierarchical Deep Reinforcement Learning for Continuous Action Control\",\"url\":\"https://www.semanticscholar.org/paper/bf8d58faf972ad0a1026c0a7c5577c07996ef3a7\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2018},{\"arxivId\":\"1802.10463\",\"authors\":[{\"authorId\":\"11020126\",\"name\":\"Parijat Dewangan\"},{\"authorId\":\"144210843\",\"name\":\"S. Teja\"},{\"authorId\":\"145211574\",\"name\":\"K. Krishna\"},{\"authorId\":\"8954660\",\"name\":\"Abhishek Sarkar\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9758d2ce1c717123900f319cedb73da1767caa38\",\"title\":\"DiGrad: Multi-Task Reinforcement Learning with Shared Actions\",\"url\":\"https://www.semanticscholar.org/paper/9758d2ce1c717123900f319cedb73da1767caa38\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39251685\",\"name\":\"Siddharth Mysore\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"57cae55e199d441440a5959f0b99fac7b15d3534\",\"title\":\"Reward-guided Curriculum for Learning Robust Action Policies\",\"url\":\"https://www.semanticscholar.org/paper/57cae55e199d441440a5959f0b99fac7b15d3534\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2678283\",\"name\":\"Sim Kuan Goh\"},{\"authorId\":\"1713460\",\"name\":\"H. Abbass\"},{\"authorId\":\"1707078\",\"name\":\"K. Tan\"},{\"authorId\":\"1402101146\",\"name\":\"A. Al-Mamun\"},{\"authorId\":\"145146201\",\"name\":\"N. Thakor\"},{\"authorId\":\"144200616\",\"name\":\"A. Bezerianos\"},{\"authorId\":\"51253294\",\"name\":\"Junhua Li\"}],\"doi\":\"10.1109/TNSRE.2018.2864119\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ba0631517fead04f902c684413f34d5e7200f83\",\"title\":\"Spatio\\u2013Spectral Representation Learning for Electroencephalographic Gait-Pattern Classification\",\"url\":\"https://www.semanticscholar.org/paper/2ba0631517fead04f902c684413f34d5e7200f83\",\"venue\":\"IEEE Transactions on Neural Systems and Rehabilitation Engineering\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8590418\",\"name\":\"Haibo Shi\"},{\"authorId\":\"2908728\",\"name\":\"Y. Sun\"},{\"authorId\":\"47949585\",\"name\":\"Guangyuan Li\"},{\"authorId\":\"46429010\",\"name\":\"F. Wang\"},{\"authorId\":\"2242987\",\"name\":\"D. Wang\"},{\"authorId\":\"47785980\",\"name\":\"J. Li\"}],\"doi\":\"10.1109/ACCESS.2019.2904910\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1773f2f389d41134acd80cac7cc58ccc3c371973\",\"title\":\"Hierarchical Intermittent Motor Control With Deterministic Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/1773f2f389d41134acd80cac7cc58ccc3c371973\",\"venue\":\"IEEE Access\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6426801\",\"name\":\"Mudit Kumar Verma\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"baf89d9cb8dad59edbe165ddb2f50f6710033ee6\",\"title\":\"Diverging Emerging Field of Multi-Task Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/baf89d9cb8dad59edbe165ddb2f50f6710033ee6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1399348619\",\"name\":\"Carlo D'Eramo\"},{\"authorId\":\"35387097\",\"name\":\"Davide Tateo\"},{\"authorId\":\"1729320\",\"name\":\"A. Bonarini\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"144719340\",\"name\":\"J. Peters\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e74aded7d0839af48706c51a7b55af2ea20f0603\",\"title\":\"Sharing Knowledge in Multi-Task Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e74aded7d0839af48706c51a7b55af2ea20f0603\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2002.01088\",\"authors\":[{\"authorId\":\"13526886\",\"name\":\"Thommen George Karimpanal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83795a7b9621e7efd50e5e182ffec71e475710ac\",\"title\":\"Neuro-evolutionary Frameworks for Generalized Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/83795a7b9621e7efd50e5e182ffec71e475710ac\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"1387921048\",\"name\":\"Ron Berenstein\"},{\"authorId\":\"1716557\",\"name\":\"I. Stoica\"},{\"authorId\":\"144344283\",\"name\":\"Ken Goldberg\"}],\"doi\":\"10.1109/COASE.2019.8843293\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7723086a4a09c39a1cd91c62b2234cdf72dc1f63\",\"title\":\"Multi-Task Hierarchical Imitation Learning for Home Automation\",\"url\":\"https://www.semanticscholar.org/paper/7723086a4a09c39a1cd91c62b2234cdf72dc1f63\",\"venue\":\"2019 IEEE 15th International Conference on Automation Science and Engineering (CASE)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1659026745\",\"name\":\"Renzong Lian\"},{\"authorId\":\"1885309109\",\"name\":\"Huachun Tan\"},{\"authorId\":\"97605052\",\"name\":\"Jiankun Peng\"},{\"authorId\":\"9372632\",\"name\":\"Q. Li\"},{\"authorId\":\"2283739\",\"name\":\"Y. Wu\"}],\"doi\":\"10.1109/TVT.2020.2999263\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1373206f36def7313836d88cac1544c62f99f514\",\"title\":\"Cross-Type Transfer for Deep Reinforcement Learning Based Hybrid Electric Vehicle Energy Management\",\"url\":\"https://www.semanticscholar.org/paper/1373206f36def7313836d88cac1544c62f99f514\",\"venue\":\"IEEE Transactions on Vehicular Technology\",\"year\":2020},{\"arxivId\":\"1903.04411\",\"authors\":[{\"authorId\":\"14042304\",\"name\":\"Zhewei Huang\"},{\"authorId\":\"145577184\",\"name\":\"Wen Heng\"},{\"authorId\":\"35132667\",\"name\":\"Shuchang Zhou\"}],\"doi\":\"10.1109/ICCV.2019.00880\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9d302ce1e035d8db85bd0991f0737dbddb273532\",\"title\":\"Learning to Paint With Model-Based Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9d302ce1e035d8db85bd0991f0737dbddb273532\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48559420\",\"name\":\"Zhiyuan Xu\"},{\"authorId\":\"1500391598\",\"name\":\"Kun Wu\"},{\"authorId\":\"1939695\",\"name\":\"Zhengping Che\"},{\"authorId\":\"152226504\",\"name\":\"J. Tang\"},{\"authorId\":\"2778556\",\"name\":\"Jie-ping Ye\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"137ce6d57e8f8bbe6af397ba064eda51c039b1e3\",\"title\":\"Multi-Task Deep Reinforcement Learning with Knowledge Transfer for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/137ce6d57e8f8bbe6af397ba064eda51c039b1e3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46365371\",\"name\":\"Junta Wu\"},{\"authorId\":\"3312590\",\"name\":\"Huiyun Li\"}],\"doi\":\"10.1155/2020/4275623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dcc02065f3f51a6bc4117adc431801e3be8a2362\",\"title\":\"Deep Ensemble Reinforcement Learning with Multiple Deep Deterministic Policy Gradient Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/dcc02065f3f51a6bc4117adc431801e3be8a2362\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1905.05180\",\"authors\":[{\"authorId\":\"3777665\",\"name\":\"L. Xing\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ff6a9d9e9f8dc97141ced4eb9debd396e7344ee0\",\"title\":\"Learning and Exploiting Multiple Subgoals for Fast Exploration in Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ff6a9d9e9f8dc97141ced4eb9debd396e7344ee0\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48017303\",\"name\":\"H. Wang\"},{\"authorId\":null,\"name\":\"Wenjing Yang\"},{\"authorId\":\"3441469\",\"name\":\"Wanrong Huang\"},{\"authorId\":\"49290329\",\"name\":\"Zhipeng Lin\"},{\"authorId\":\"3261878\",\"name\":\"Yuhua Tang\"}],\"doi\":\"10.1007/978-3-030-04239-4_27\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"317953dfe87a892c7275c8d1b816d3bae419ecd0\",\"title\":\"Multi-feature Fusion for Deep Reinforcement Learning: Sequential Control of Mobile Robots\",\"url\":\"https://www.semanticscholar.org/paper/317953dfe87a892c7275c8d1b816d3bae419ecd0\",\"venue\":\"ICONIP\",\"year\":2018},{\"arxivId\":\"2007.07011\",\"authors\":[{\"authorId\":\"118519138\",\"name\":\"Jorge Armando Mendez Mendez\"},{\"authorId\":\"143676076\",\"name\":\"Boyu Wang\"},{\"authorId\":\"144020269\",\"name\":\"E. Eaton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4650f9265eef5e5fd05835860c06b814d0f6db34\",\"title\":\"Lifelong Policy Gradient Learning of Factored Policies for Faster Training Without Forgetting\",\"url\":\"https://www.semanticscholar.org/paper/4650f9265eef5e5fd05835860c06b814d0f6db34\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12116553\",\"name\":\"Y. Wu\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"145592727\",\"name\":\"K. Song\"}],\"doi\":\"10.24963/ijcai.2018/211\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"title\":\"Master-Slave Curriculum Design for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"2007.05694\",\"authors\":[{\"authorId\":\"1810775245\",\"name\":\"Ugurkan Ates\"}],\"doi\":\"10.1109/ASYU50717.2020.9259811\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"df08b6e8cfd38ce430b852fddb9aec796c80ffa3\",\"title\":\"Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones\",\"url\":\"https://www.semanticscholar.org/paper/df08b6e8cfd38ce430b852fddb9aec796c80ffa3\",\"venue\":\"2020 Innovations in Intelligent Systems and Applications Conference (ASYU)\",\"year\":2020},{\"arxivId\":\"1903.00827\",\"authors\":[{\"authorId\":\"39539779\",\"name\":\"Zhizheng Zhang\"},{\"authorId\":\"14584879\",\"name\":\"J. Chen\"},{\"authorId\":\"143912275\",\"name\":\"Zhibo Chen\"},{\"authorId\":\"50135568\",\"name\":\"W. Li\"}],\"doi\":\"10.1109/tcyb.2019.2939174\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"6a9c324a86cfff5998402845e43e1bba4c42d209\",\"title\":\"Asynchronous Episodic Deep Deterministic Policy Gradient: Towards Continuous Control in Computationally Complex Environments\",\"url\":\"https://www.semanticscholar.org/paper/6a9c324a86cfff5998402845e43e1bba4c42d209\",\"venue\":\"IEEE transactions on cybernetics\",\"year\":2019},{\"arxivId\":\"1709.07979\",\"authors\":[{\"authorId\":\"70461341\",\"name\":\"Wenhao Yu\"},{\"authorId\":\"1713189\",\"name\":\"Greg Turk\"},{\"authorId\":\"1688533\",\"name\":\"C. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d672baf56986a3bc5748c25362b2d2b4d65efcb8\",\"title\":\"Multi-task Learning with Gradient Guided Policy Specialization\",\"url\":\"https://www.semanticscholar.org/paper/d672baf56986a3bc5748c25362b2d2b4d65efcb8\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3428549\",\"name\":\"Tingwu Wang\"},{\"authorId\":\"2246396\",\"name\":\"Renjie Liao\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"37895334\",\"name\":\"S. Fidler\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"249408527106d7595d45dd761dd53c83e5a02613\",\"title\":\"NerveNet: Learning Structured Policy with Graph Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/249408527106d7595d45dd761dd53c83e5a02613\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118519138\",\"name\":\"Jorge Armando Mendez Mendez\"},{\"authorId\":\"144020269\",\"name\":\"E. Eaton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b4a1672929438410d9b6bdc23daccd34656bd95\",\"title\":\"Lifelong Learning of Factored Policies via Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/1b4a1672929438410d9b6bdc23daccd34656bd95\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4132833\",\"name\":\"H. Wang\"},{\"authorId\":\"1860840\",\"name\":\"Shuangping Huang\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"}],\"doi\":\"10.1109/ICPR.2018.8545022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c79b14ffb4e1bc7eb5157c6ec9d04298551d1aa4\",\"title\":\"Focus on Scene Text Using Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c79b14ffb4e1bc7eb5157c6ec9d04298551d1aa4\",\"venue\":\"2018 24th International Conference on Pattern Recognition (ICPR)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042697526\",\"name\":\"Haomin Qiu\"},{\"authorId\":\"144238413\",\"name\":\"F. Liu\"}],\"doi\":\"10.1109/ICTAI50040.2020.00107\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6df60ba532272469595d79e2117d735762cb8de2\",\"title\":\"A State Representation Dueling Network for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/6df60ba532272469595d79e2117d735762cb8de2\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152764093\",\"name\":\"Z. Wang\"},{\"authorId\":\"81963977\",\"name\":\"H. Li\"},{\"authorId\":\"4520850\",\"name\":\"Chunlin Chen\"}],\"doi\":\"10.1109/TNNLS.2019.2927320\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eda06f77f146d77c5984db41fcfb7170dc33fad4\",\"title\":\"Incremental Reinforcement Learning in Continuous Spaces via Policy Relaxation and Importance Weighting\",\"url\":\"https://www.semanticscholar.org/paper/eda06f77f146d77c5984db41fcfb7170dc33fad4\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143790841\",\"name\":\"D. Xu\"},{\"authorId\":\"49931645\",\"name\":\"Z. Hui\"},{\"authorId\":\"47908791\",\"name\":\"Yongqi Liu\"},{\"authorId\":\"46965289\",\"name\":\"Gang Chen\"}],\"doi\":\"10.1016/J.AST.2019.05.058\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a811bc38364bc22c4402f4f730ff203458ef195f\",\"title\":\"Morphing control of a new bionic morphing UAV with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/a811bc38364bc22c4402f4f730ff203458ef195f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2010.07494\",\"authors\":[{\"authorId\":\"48559420\",\"name\":\"Zhiyuan Xu\"},{\"authorId\":\"1500391598\",\"name\":\"Kun Wu\"},{\"authorId\":\"1939695\",\"name\":\"Zhengping Che\"},{\"authorId\":\"152226504\",\"name\":\"J. Tang\"},{\"authorId\":\"2778556\",\"name\":\"Jie-ping Ye\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"85bc7bac671da416c46b42903ab831b28c561ec9\",\"title\":\"Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/85bc7bac671da416c46b42903ab831b28c561ec9\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":12727536,\"doi\":\"10.24963/ijcai.2017/461\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":3,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"13ec391c21ded02ed3be31ff19f90d529a431e89\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32858189\",\"name\":\"R. Krishnamurthy\"},{\"authorId\":\"2943530\",\"name\":\"Aravind S. Lakshminarayanan\"},{\"authorId\":\"15045198\",\"name\":\"P. Kumar\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a0316936d335fa74860e1099e97e69c092ee457\",\"title\":\"Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0a0316936d335fa74860e1099e97e69c092ee457\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1603.04467\",\"authors\":[{\"authorId\":\"145832079\",\"name\":\"M. Abadi\"},{\"authorId\":\"145984138\",\"name\":\"A. Agarwal\"},{\"authorId\":\"144758007\",\"name\":\"P. Barham\"},{\"authorId\":\"2445241\",\"name\":\"E. Brevdo\"},{\"authorId\":\"2545358\",\"name\":\"Z. Chen\"},{\"authorId\":\"48738717\",\"name\":\"Craig Citro\"},{\"authorId\":\"32131713\",\"name\":\"G. S. Corrado\"},{\"authorId\":\"36347083\",\"name\":\"Andy Davis\"},{\"authorId\":\"49959210\",\"name\":\"J. Dean\"},{\"authorId\":\"145139947\",\"name\":\"M. Devin\"},{\"authorId\":\"1780892\",\"name\":\"Sanjay Ghemawat\"},{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"3384453\",\"name\":\"A. Harp\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"2090818\",\"name\":\"M. Isard\"},{\"authorId\":\"39978391\",\"name\":\"Y. Jia\"},{\"authorId\":\"1944541\",\"name\":\"R. J\\u00f3zefowicz\"},{\"authorId\":\"40527594\",\"name\":\"L. Kaiser\"},{\"authorId\":\"1942300\",\"name\":\"M. Kudlur\"},{\"authorId\":\"3369421\",\"name\":\"Josh Levenberg\"},{\"authorId\":\"143767989\",\"name\":\"Dan Man\\u00e9\"},{\"authorId\":\"3089272\",\"name\":\"Rajat Monga\"},{\"authorId\":\"144375552\",\"name\":\"Sherry Moore\"},{\"authorId\":\"20154699\",\"name\":\"D. Murray\"},{\"authorId\":\"153301219\",\"name\":\"Chris Olah\"},{\"authorId\":\"144927151\",\"name\":\"Mike Schuster\"},{\"authorId\":\"1789737\",\"name\":\"Jonathon Shlens\"},{\"authorId\":\"32163737\",\"name\":\"B. Steiner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"35210462\",\"name\":\"Kunal Talwar\"},{\"authorId\":\"2080690\",\"name\":\"P. Tucker\"},{\"authorId\":\"2657155\",\"name\":\"V. Vanhoucke\"},{\"authorId\":\"38062095\",\"name\":\"V. Vasudevan\"},{\"authorId\":\"1765169\",\"name\":\"F. Vi\\u00e9gas\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"47941411\",\"name\":\"Pete Warden\"},{\"authorId\":\"145233583\",\"name\":\"M. Wattenberg\"},{\"authorId\":\"35078078\",\"name\":\"Martin Wicke\"},{\"authorId\":\"47112093\",\"name\":\"Y. Yu\"},{\"authorId\":\"2777763\",\"name\":\"X. Zheng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"title\":\"TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\",\"url\":\"https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Diederik P. Kingma\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Matthew Grounds and Daniel Kudenko . Parallel reinforcement learning with linear function approximation\",\"url\":\"\",\"venue\":\"Adaptive Agents and Multi - Agent Systems III . Adaptation and Multi - Agent Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2327581\",\"name\":\"Eirini Kaldeli\"},{\"authorId\":\"1766996\",\"name\":\"A. Lazovik\"},{\"authorId\":\"1747132\",\"name\":\"Marco Aiello\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"533214fbe2ff4d2532b050a9c2dbffaa9f356eec\",\"title\":\"AAAI Conference on Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/533214fbe2ff4d2532b050a9c2dbffaa9f356eec\",\"venue\":\"AAAI 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew Grounds\"},{\"authorId\":null,\"name\":\"Daniel Kudenko. Parallel reinforcement learning with line Agents\"},{\"authorId\":null,\"name\":\"Multi-Agent Systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Adaptation and Multi-Agent Learning\",\"url\":\"\",\"venue\":\"pp.60-74. Springer, Berlin, Heidelberg,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alessandro Lazaric\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh. Bayesian Multi-Task Reinforcement Learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pp.599-606. ACM,\",\"year\":2010},{\"arxivId\":\"1106.3655\",\"authors\":[{\"authorId\":\"1720480\",\"name\":\"Christos Dimitrakakis\"},{\"authorId\":\"3249046\",\"name\":\"C. Rothkopf\"}],\"doi\":\"10.1007/978-3-642-29946-9_27\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2832e4cdb1f927dca50422ff010bbd2a9934292e\",\"title\":\"Bayesian Multitask Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2832e4cdb1f927dca50422ff010bbd2a9934292e\",\"venue\":\"EWRL\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Tejas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kulkarni , Karthik R . Narasimhan , Ardavan Saeedi and Joshua B . Tenenbaum . Hierarchical Deep Reinforcement Learning Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"\",\"venue\":\"Proceedings of Advances in Neural Information Processing Systems\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abdel-rahman Mohamed Alex Graves\"},{\"authorId\":null,\"name\":\"Geoffrey Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Nando de Freitas and Shimon Whiteson . Learning to Communicate with Deep Multi - Agent Reinforcement Learning\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shakir Mohamed\"},{\"authorId\":null,\"name\":\"Danilo J. Rezende. Variational Information Maximisation f Learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of Advances in Neural Information Processing Systems\",\"url\":\"\",\"venue\":\"pp.2125-2133. MIT Press,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alex Krizhevsky\"},{\"authorId\":null,\"name\":\"Ilya Sutskever\"},{\"authorId\":null,\"name\":\"Geoffrey E. Hinton. ImageNet Classification with Deep Convo Network\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of Advances in Neural Information Processing Systems\",\"url\":\"\",\"venue\":\"pp.1097-1105. MIT Press,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98302096\",\"name\":\"Peter Kulchyski\"}],\"doi\":\"10.1080/13688790.2015.1136585\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dfbfcd288ed9b37106564bf6dc95041f6d33b6b2\",\"title\":\"and\",\"url\":\"https://www.semanticscholar.org/paper/dfbfcd288ed9b37106564bf6dc95041f6d33b6b2\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":\"10.1007/978-3-642-27645-3_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"title\":\"Transfer in Reinforcement Learning: A Framework and a Survey\",\"url\":\"https://www.semanticscholar.org/paper/16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.1038/2071238d0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f08bf0e2bdcb107acd7621152c52321acc07b4e3\",\"title\":\"Physical Review\",\"url\":\"https://www.semanticscholar.org/paper/f08bf0e2bdcb107acd7621152c52321acc07b4e3\",\"venue\":\"Nature\",\"year\":1965},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"N. Jakob\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Dimitrakakis and Rothkopf , 2011 ] Christos Dimitrakakis and Constantin A . Rothkopf . Bayesian Multitask Inverse Reinforcement Learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1312.4400\",\"authors\":[{\"authorId\":\"143953684\",\"name\":\"M. Lin\"},{\"authorId\":\"35370244\",\"name\":\"Q. Chen\"},{\"authorId\":\"143653681\",\"name\":\"S. Yan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"5e83ab70d0cbc003471e87ec306d27d9c80ecb16\",\"title\":\"Network In Network\",\"url\":\"https://www.semanticscholar.org/paper/5e83ab70d0cbc003471e87ec306d27d9c80ecb16\",\"venue\":\"ICLR\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2011924\",\"name\":\"A. Farahmand\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"49185899\",\"name\":\"S. Mannor\"},{\"authorId\":\"1776193\",\"name\":\"O. Teytaud\"},{\"authorId\":\"144951839\",\"name\":\"E. Moulines\"},{\"authorId\":\"144119475\",\"name\":\"A. Russo\"},{\"authorId\":\"2528631\",\"name\":\"Peter Vrancx\"},{\"authorId\":\"2136788\",\"name\":\"T. Croonenborghs\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13980f9ba77ca3adb59d6ec04844148e977d0884\",\"title\":\"European Workshop on Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/13980f9ba77ca3adb59d6ec04844148e977d0884\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abdel-rahman Mohamed Alex Graves\"},{\"authorId\":null,\"name\":\"Geoffrey Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Nando de Freitas and Shimon Whiteson . Learning to Communicate with Deep Multi - Agent Reinforcement Learning\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451410\",\"name\":\"Matthew Jon Grounds\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":\"10.1145/1329125.1329179\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"187f6831f2c5026375631a72159c1e96d785d0bd\",\"title\":\"Parallel reinforcement learning with linear function approximation\",\"url\":\"https://www.semanticscholar.org/paper/187f6831f2c5026375631a72159c1e96d785d0bd\",\"venue\":\"AAMAS '07\",\"year\":2007},{\"arxivId\":\"1611.09894\",\"authors\":[{\"authorId\":\"32135299\",\"name\":\"Sai Praveen Bangaru\"},{\"authorId\":\"145035257\",\"name\":\"J. S. Suhas\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"125654a455c46d26763b23ecc2cb20019dd247e6\",\"title\":\"Exploration for Multi-task Reinforcement Learning with Deep Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/125654a455c46d26763b23ecc2cb20019dd247e6\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alex Graves\"},{\"authorId\":null,\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":null,\"name\":\"Geoffrey Hinton. Speech Recognition with Deep Recurrent Neu Acoustics\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Speech and Signal Processing\",\"url\":\"\",\"venue\":\"pp.6645-6649. IEEE,\",\"year\":2013},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50538596\",\"name\":\"G. Uhlenbeck\"},{\"authorId\":\"91175404\",\"name\":\"L. S. Ornstein\"}],\"doi\":\"10.1103/PHYSREV.36.823\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d2a3c57defe48b0c310c561f97c8bd2b81672f14\",\"title\":\"On the Theory of the Brownian Motion\",\"url\":\"https://www.semanticscholar.org/paper/d2a3c57defe48b0c310c561f97c8bd2b81672f14\",\"venue\":\"\",\"year\":1930},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1022633531479\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"title\":\"Learning to Predict by the Methods of Temporal Differences\",\"url\":\"https://www.semanticscholar.org/paper/a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"venue\":\"Machine Learning\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Aja Huang\"},{\"authorId\":null,\"name\":\"Arthur Guez Chris J. Maddison\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Rusu , et al . Human - level Control through Deep reinforcement Learning\",\"url\":\"\",\"venue\":\"Nature Proceedings of Advances in Neural Information Processing Systems\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1712908\",\"name\":\"P. Brazdil\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c0ec50a3a933e9d55efa1a304a76c4cdf9367442\",\"title\":\"Proceedings of the European Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/c0ec50a3a933e9d55efa1a304a76c4cdf9367442\",\"venue\":\"\",\"year\":1993},{\"arxivId\":\"1603.02041\",\"authors\":[{\"authorId\":\"2311858\",\"name\":\"Diana Borsa\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"1404459229\",\"name\":\"J. Shawe-Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dae11b5e333abe7423fc81ee78b9a219f0e48500\",\"title\":\"Learning Shared Representations in Multi-task Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/dae11b5e333abe7423fc81ee78b9a219f0e48500\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1612.05533\",\"authors\":[{\"authorId\":\"8214233\",\"name\":\"J. Zhang\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"145581493\",\"name\":\"J. Boedecker\"},{\"authorId\":\"1725973\",\"name\":\"W. Burgard\"}],\"doi\":\"10.1109/IROS.2017.8206049\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b\",\"title\":\"Deep reinforcement learning with successor features for navigation across similar environments\",\"url\":\"https://www.semanticscholar.org/paper/1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b\",\"venue\":\"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1509.08731\",\"authors\":[{\"authorId\":\"14594344\",\"name\":\"S. Mohamed\"},{\"authorId\":\"1748523\",\"name\":\"Danilo Jimenez Rezende\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060\",\"title\":\"Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea4128e734f6a43841017fbe3e497d8128fc296f\",\"title\":\"Bayesian MultiTask Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ea4128e734f6a43841017fbe3e497d8128fc296f\",\"venue\":\"\",\"year\":2010}],\"title\":\"Multi-Task Deep Reinforcement Learning for Continuous Action Control\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Network architecture\",\"topicId\":\"58473\",\"url\":\"https://www.semanticscholar.org/topic/58473\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Sensor\",\"topicId\":\"1117\",\"url\":\"https://www.semanticscholar.org/topic/1117\"}],\"url\":\"https://www.semanticscholar.org/paper/13ec391c21ded02ed3be31ff19f90d529a431e89\",\"venue\":\"IJCAI\",\"year\":2017}\n"