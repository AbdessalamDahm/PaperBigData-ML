"{\"abstract\":\"We consider the problem of learning from demonstration, where extra side information about the demonstration is encoded as a co-safe linear temporal logic formula. We address two known limitations of existing methods that do not account for such side information. First, the policies that result from existing methods, while matching the expected features or likelihood of the demonstrations, may still be in conflict with high-level objectives not explicit in the demonstration trajectories. Second, existing methods fail to provide a priori guarantees on the out-of-sample generalization performance with respect to such high-level goals. This lack of formal guarantees can prevent the application of learning from demonstration to safetycritical systems, especially when inference to state space regions with poor demonstration coverage is required. In this work, we show that side information, when explicitly taken into account, indeed improves the performance and safety of the learned policy with respect to task implementation. Moreover, we describe an automated procedure to systematically generate the features that encode side information expressed in temporal logic.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"144641675\",\"name\":\"M. Wen\",\"url\":\"https://www.semanticscholar.org/author/144641675\"},{\"authorId\":\"144707690\",\"name\":\"I. Papusha\",\"url\":\"https://www.semanticscholar.org/author/144707690\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\",\"url\":\"https://www.semanticscholar.org/author/3199888\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"50383691\",\"name\":\"T. Xu\"},{\"authorId\":\"151031068\",\"name\":\"Henghui Zhu\"},{\"authorId\":\"1691402\",\"name\":\"I. Paschalidis\"}],\"doi\":\"10.1016/j.ejcon.2020.04.003\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b96c3a84eaf777fdbbe8d7e15dbcc331e87e9189\",\"title\":\"Learning parametric policies and transition probability models of markov decision processes from data\",\"url\":\"https://www.semanticscholar.org/paper/b96c3a84eaf777fdbbe8d7e15dbcc331e87e9189\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144707690\",\"name\":\"I. Papusha\"},{\"authorId\":\"144641675\",\"name\":\"M. Wen\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":\"10.23919/ACC.2018.8431646\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d8c6dad07af53f381495a0e910898d6a7fa96a99\",\"title\":\"Inverse Optimal Control with Regular Language Specifications\",\"url\":\"https://www.semanticscholar.org/paper/d8c6dad07af53f381495a0e910898d6a7fa96a99\",\"venue\":\"2018 Annual American Control Conference (ACC)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81430198\",\"name\":\"Qitong Gao\"},{\"authorId\":\"2072873\",\"name\":\"M. Pajic\"},{\"authorId\":\"1764546\",\"name\":\"Michael M. Zavlanos\"}],\"doi\":\"10.1109/ICRA40945.2020.9197297\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c7eceb02636ec72e41de26f91d404645b3d8963b\",\"title\":\"Deep Imitative Reinforcement Learning for Temporal Logic Robot Motion Planning with Noisy Semantic Observations\",\"url\":\"https://www.semanticscholar.org/paper/c7eceb02636ec72e41de26f91d404645b3d8963b\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":\"1909.04256\",\"authors\":[{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":\"10.24963/IJCAI.2019/557\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5c0645162f4b74895b86a98db86df8af680de77\",\"title\":\"Transfer of Temporal Logic Formulas in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a5c0645162f4b74895b86a98db86df8af680de77\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2002.00784\",\"authors\":[{\"authorId\":\"144810417\",\"name\":\"C. Innes\"},{\"authorId\":\"47172195\",\"name\":\"S. Ramamoorthy\"}],\"doi\":\"10.15607/rss.2020.xvi.004\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2a503de23abe1c3047a58f5a391d393d25c189a\",\"title\":\"Elaborating on Learned Demonstrations with Temporal Logic Specifications\",\"url\":\"https://www.semanticscholar.org/paper/a2a503de23abe1c3047a58f5a391d393d25c189a\",\"venue\":\"RSS 2020\",\"year\":2020},{\"arxivId\":\"2006.15714\",\"authors\":[{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"152365289\",\"name\":\"Bo Wu\"},{\"authorId\":\"1389929290\",\"name\":\"Daniel Neider\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad64998683ae863b0fb7ca80f5cc0da54135e466\",\"title\":\"Active Finite Reward Automaton Inference and Reinforcement Learning Using Queries and Counterexamples\",\"url\":\"https://www.semanticscholar.org/paper/ad64998683ae863b0fb7ca80f5cc0da54135e466\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.01074\",\"authors\":[{\"authorId\":\"90723763\",\"name\":\"Eleanor Quint\"},{\"authorId\":\"145481056\",\"name\":\"D. Xu\"},{\"authorId\":\"40632220\",\"name\":\"Haluk Dogan\"},{\"authorId\":\"8158431\",\"name\":\"Zeynep Hakguder\"},{\"authorId\":\"98356961\",\"name\":\"Stephanie A. Scott\"},{\"authorId\":\"145738070\",\"name\":\"Matthew B. Dwyer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a9c23d911ae5628640068f30e866674c9c9d6475\",\"title\":\"Formal Language Constraints for Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/a9c23d911ae5628640068f30e866674c9c9d6475\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3399824\",\"name\":\"R. Bhattacharyya\"},{\"authorId\":\"2350548\",\"name\":\"S. M. Hazarika\"}],\"doi\":\"10.1515/pjbr-2018-0021\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c91513ba36c1ff5ed4ebf6a4f9e98a2c93f91d3f\",\"title\":\"Object Affordance Driven Inverse Reinforcement Learning Through Conceptual Abstraction and Advice\",\"url\":\"https://www.semanticscholar.org/paper/c91513ba36c1ff5ed4ebf6a4f9e98a2c93f91d3f\",\"venue\":\"Paladyn J. Behav. Robotics\",\"year\":2018},{\"arxivId\":\"1809.06305\",\"authors\":[{\"authorId\":\"48568816\",\"name\":\"X. Li\"},{\"authorId\":\"30380763\",\"name\":\"Y. Ma\"},{\"authorId\":\"1730719\",\"name\":\"C. Belta\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"365ee41de6d92d6557864e183ca104cb677a0122\",\"title\":\"Automata Guided Reinforcement Learning With Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/365ee41de6d92d6557864e183ca104cb677a0122\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153010013\",\"name\":\"Xiao Li\"},{\"authorId\":\"31216448\",\"name\":\"Zachary T. Serlin\"},{\"authorId\":\"145872006\",\"name\":\"G. Yang\"},{\"authorId\":\"1730719\",\"name\":\"C. Belta\"}],\"doi\":\"10.1126/scirobotics.aay6276\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5d8392ad68b5f3849ac7a4d6a24a21a809dba721\",\"title\":\"A formal methods approach to interpretable reinforcement learning for robotic planning\",\"url\":\"https://www.semanticscholar.org/paper/5d8392ad68b5f3849ac7a4d6a24a21a809dba721\",\"venue\":\"Science Robotics\",\"year\":2019},{\"arxivId\":\"2002.06000\",\"authors\":[{\"authorId\":\"150936641\",\"name\":\"Borja G. Leon\"},{\"authorId\":\"3142000\",\"name\":\"F. Belardinelli\"}],\"doi\":\"10.3233/FAIA200086\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1dc50f7e2a347ddc7f722a3a87207893eae28c3\",\"title\":\"Extended Markov Games to Learn Multiple Tasks in Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f1dc50f7e2a347ddc7f722a3a87207893eae28c3\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46811032\",\"name\":\"R. Patel\"},{\"authorId\":\"2949185\",\"name\":\"Ellie Pavlick\"},{\"authorId\":\"2913681\",\"name\":\"Stefanie Tellex\"}],\"doi\":\"10.15607/rss.2020.xvi.016\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f2001dbd83c92be784b67744161d66e2f25a9d2c\",\"title\":\"Grounding Language to Non-Markovian Tasks with No Supervision of Task Specifications\",\"url\":\"https://www.semanticscholar.org/paper/f2001dbd83c92be784b67744161d66e2f25a9d2c\",\"venue\":\"RSS 2020\",\"year\":2020},{\"arxivId\":\"2005.14419\",\"authors\":[{\"authorId\":\"1729517384\",\"name\":\"Olivier Buffer\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"144834415\",\"name\":\"Paul Weng\"}],\"doi\":\"10.1002/9780470512517.ch6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"307beef407d6821eb4fc4e97710b0b038170daa4\",\"title\":\"Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/307beef407d6821eb4fc4e97710b0b038170daa4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48322149\",\"name\":\"Roma Patel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ce98b5de700b51f949f521220634e1b0677d3786\",\"title\":\"Learning to Ground Language to Temporal Logical Form\",\"url\":\"https://www.semanticscholar.org/paper/ce98b5de700b51f949f521220634e1b0677d3786\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1758085\",\"name\":\"Toryn Q. Klassen\"},{\"authorId\":\"2682734\",\"name\":\"R. Valenzano\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"074300c14841d5c29c76ea37a48f38b365a7bf8d\",\"title\":\"Teaching Multiple Tasks to an RL Agent using LTL\",\"url\":\"https://www.semanticscholar.org/paper/074300c14841d5c29c76ea37a48f38b365a7bf8d\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":\"2001.09227\",\"authors\":[{\"authorId\":\"93657967\",\"name\":\"F. Memarian\"},{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"152365289\",\"name\":\"Bo Wu\"},{\"authorId\":\"144641675\",\"name\":\"M. Wen\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"17f23a5fdc202bd208bbc87de2a4824e6c125d5e\",\"title\":\"Active Task-Inference-Guided Deep Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/17f23a5fdc202bd208bbc87de2a4824e6c125d5e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738302\",\"name\":\"S. Tripakis\"}],\"doi\":\"10.1109/ICPHYS.2018.8387644\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ec9cb1b79a388b7d47eeb751b4aaa92f9d3b02e\",\"title\":\"Data-driven and model-based design\",\"url\":\"https://www.semanticscholar.org/paper/4ec9cb1b79a388b7d47eeb751b4aaa92f9d3b02e\",\"venue\":\"2018 IEEE Industrial Cyber-Physical Systems (ICPS)\",\"year\":2018}],\"corpusId\":2325533,\"doi\":\"10.24963/ijcai.2017/426\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"635ed492434411e644cfc81683d897f1b0c2ae39\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1836885\",\"name\":\"Brenna Argall\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"},{\"authorId\":\"1699032\",\"name\":\"B. Browning\"}],\"doi\":\"10.1016/j.robot.2008.10.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"title\":\"A survey of robot learning from demonstration\",\"url\":\"https://www.semanticscholar.org/paper/4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34929449\",\"name\":\"G. John\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3479cfa6c395ad8b2ad77a3d59dc2be5935a50d8\",\"title\":\"When the Best Move Isn't Optimal: Q-learning with Exploration\",\"url\":\"https://www.semanticscholar.org/paper/3479cfa6c395ad8b2ad77a3d59dc2be5935a50d8\",\"venue\":\"AAAI\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405542838\",\"name\":\"Monica Babes-Vroman\"},{\"authorId\":\"1875175\",\"name\":\"Vukosi Marivate\"},{\"authorId\":\"145362925\",\"name\":\"K. Subramanian\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"99455cd5021f02a00927eae9d8bcafe9d6a27f8e\",\"title\":\"Apprenticeship Learning About Multiple Intentions\",\"url\":\"https://www.semanticscholar.org/paper/99455cd5021f02a00927eae9d8bcafe9d6a27f8e\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Krishnamurthy Dvijotham\"},{\"authorId\":null,\"name\":\"Emanuel Todorov. Inverse optimal control with linearly-sol MDPs\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 335\\u2013342,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398671969\",\"name\":\"H. Durrant-Whyte\"},{\"authorId\":\"143724999\",\"name\":\"N. Roy\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":\"10.7551/mitpress/9481.003.0008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3cfb6c03f862c746a9bfc337e3dd8610d91050bd\",\"title\":\"Controlling Wild Bodies Using Linear Temporal Logic\",\"url\":\"https://www.semanticscholar.org/paper/3cfb6c03f862c746a9bfc337e3dd8610d91050bd\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7887362\",\"name\":\"D. Smith\"}],\"doi\":\"10.1057/JORS.1996.103\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f4ec0ce0c3a89cbf385c09594030af8a71504aba\",\"title\":\"Dynamic Programming and Optimal Control. Volume 1\",\"url\":\"https://www.semanticscholar.org/paper/f4ec0ce0c3a89cbf385c09594030af8a71504aba\",\"venue\":\"\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3015827\",\"name\":\"Eric M. Wolff\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"},{\"authorId\":\"144696890\",\"name\":\"R. Murray\"}],\"doi\":\"10.15607/RSS.2012.VIII.057\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c208caafb8f69e0e3db207bd3304a8f373ddf376\",\"title\":\"Optimal Control with Weighted Average Costs and Temporal Logic Specifications\",\"url\":\"https://www.semanticscholar.org/paper/c208caafb8f69e0e3db207bd3304a8f373ddf376\",\"venue\":\"Robotics: Science and Systems\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1729912\",\"name\":\"Krishnamurthy Dvijotham\"},{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"564a96582f0b9da48d0d5142b9f1ea2299bc7b87\",\"title\":\"Inverse Optimal Control with Linearly-Solvable MDPs\",\"url\":\"https://www.semanticscholar.org/paper/564a96582f0b9da48d0d5142b9f1ea2299bc7b87\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13693897\",\"name\":\"Nathan D. Ratliff\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"}],\"doi\":\"10.1145/1143844.1143936\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"117a50fbdfd473e43e550c6103733e6cb4aecb4c\",\"title\":\"Maximum margin planning\",\"url\":\"https://www.semanticscholar.org/paper/117a50fbdfd473e43e550c6103733e6cb4aecb4c\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Georgios E Fainekos\"},{\"authorId\":null,\"name\":\"Hadas Kress-Gazit\"},{\"authorId\":null,\"name\":\"George J Pappas.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Krishnamurthy Dvijotham and Emanuel Todorov . Inverse optimal control with linearlysolvable MDPs\",\"url\":\"\",\"venue\":\"International Conference on Machine Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387890355\",\"name\":\"H. Kress-Gazit\"},{\"authorId\":\"2166716\",\"name\":\"T. Wongpiromsarn\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":\"10.1109/MRA.2011.942116\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fb1f832867488f5c87e79b866d6e425c75a52981\",\"title\":\"Correct, Reactive, High-Level Robot Control\",\"url\":\"https://www.semanticscholar.org/paper/fb1f832867488f5c87e79b866d6e425c75a52981\",\"venue\":\"IEEE Robotics & Automation Magazine\",\"year\":2011},{\"arxivId\":\"1206.5264\",\"authors\":[{\"authorId\":\"1741549\",\"name\":\"Gergely Neu\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4dd0cb932d3da7f97a50842b10f8b0e17fc5012\",\"title\":\"Apprenticeship Learning using Inverse Reinforcement Learning and Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/c4dd0cb932d3da7f97a50842b10f8b0e17fc5012\",\"venue\":\"UAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael Bloem\"},{\"authorId\":null,\"name\":\"Nicholas Bambos. Infinite time horizon maximum causal entro learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Decision and Control (CDC)\",\"url\":\"\",\"venue\":\"2014 IEEE 53rd Annual Conference on, pages 4911\\u20134916. IEEE,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5403194\",\"name\":\"C. Baier\"},{\"authorId\":\"1727089\",\"name\":\"J. Katoen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"2bbf4707b8555fd001b1d3248ca0b9815ac9bd46\",\"title\":\"Principles of model checking\",\"url\":\"https://www.semanticscholar.org/paper/2bbf4707b8555fd001b1d3248ca0b9815ac9bd46\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"James Macglashan\"},{\"authorId\":null,\"name\":\"Michael L Littman. Between imitation\"},{\"authorId\":null,\"name\":\"intention learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 3692\\u20133698,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eric Wolff\"},{\"authorId\":null,\"name\":\"Ufuk Topcu\"},{\"authorId\":null,\"name\":\"Richard Murray. Optimal control with weighted average costs\"},{\"authorId\":null,\"name\":\"temporal logic specifications\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of Robotics: Science and Systems\",\"url\":\"\",\"venue\":\"Sydney, Australia, July\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nathan D Ratliff\"},{\"authorId\":null,\"name\":\"J Andrew Bagnell\"},{\"authorId\":null,\"name\":\"Martin A Zinkevich\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deepak Ramachandran and Eyal Amir . Bayesian inverse reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47157189\",\"name\":\"A. Gupta\"},{\"authorId\":\"144682958\",\"name\":\"S. Malik\"}],\"doi\":\"10.1007/S10703-009-0072-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"700ff75e1c03b84026794c753582660e5376103d\",\"title\":\"Formal Methods in System Design: Preface\",\"url\":\"https://www.semanticscholar.org/paper/700ff75e1c03b84026794c753582660e5376103d\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1739139\",\"name\":\"O. Kupferman\"},{\"authorId\":\"9083969\",\"name\":\"Moshe Y. Vardi\"}],\"doi\":\"10.1023/A:1011254632723\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ada677f6c63cc22853b1d73aacd3d890e898e6e\",\"title\":\"Model Checking of Safety Properties\",\"url\":\"https://www.semanticscholar.org/paper/4ada677f6c63cc22853b1d73aacd3d890e898e6e\",\"venue\":\"Formal Methods Syst. Des.\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2209847\",\"name\":\"Abdeslam Boularias\"},{\"authorId\":\"145739642\",\"name\":\"J. Kober\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4b16ae0b925eb7277cfe62ecac427e1427636f0\",\"title\":\"Relative Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d4b16ae0b925eb7277cfe62ecac427e1427636f0\",\"venue\":\"AISTATS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47750298\",\"name\":\"J. C. Trinkle\"},{\"authorId\":\"1723726\",\"name\":\"Y. Matsuoka\"}],\"doi\":\"10.1609/aimag.v31i1.2237\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"0649a333f64e83dd9cd64b37b401d7e181c22dff\",\"title\":\"Robotics: Science and Systems\",\"url\":\"https://www.semanticscholar.org/paper/0649a333f64e83dd9cd64b37b401d7e181c22dff\",\"venue\":\"AI Mag.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50674616\",\"name\":\"A. Albert\"},{\"authorId\":\"6922032\",\"name\":\"J. Anderson\"}],\"doi\":\"10.1093/BIOMET/71.1.1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9b91028ed553ec7d4c2ee9bbcea47d6b118bdc54\",\"title\":\"On the existence of maximum likelihood estimates in logistic regression models\",\"url\":\"https://www.semanticscholar.org/paper/9b91028ed553ec7d4c2ee9bbcea47d6b118bdc54\",\"venue\":\"\",\"year\":1984},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hadas Kress-Gazit\"},{\"authorId\":null,\"name\":\"Tichakorn Wongpiromsarn\"},{\"authorId\":null,\"name\":\"Ufuk Topcu. Correct\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"reactive\",\"url\":\"\",\"venue\":\"high-level robot control. IEEE Robotics & Automation Magazine, 18(3):65\\u201374,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1682745\",\"name\":\"Georgios Fainekos\"},{\"authorId\":\"1387890355\",\"name\":\"H. Kress-Gazit\"},{\"authorId\":\"143770945\",\"name\":\"George J. Pappas\"}],\"doi\":\"10.1109/ROBOT.2005.1570410\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"95af37d5ceaa0e15b64adabfeae40e53d26ce9f5\",\"title\":\"Temporal Logic Motion Planning for Mobile Robots\",\"url\":\"https://www.semanticscholar.org/paper/95af37d5ceaa0e15b64adabfeae40e53d26ce9f5\",\"venue\":\"Proceedings of the 2005 IEEE International Conference on Robotics and Automation\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1401736799\",\"name\":\"D. Garc\\u00eda-Garc\\u00eda\"},{\"authorId\":\"1728654\",\"name\":\"U. V. Luxburg\"},{\"authorId\":\"1388811589\",\"name\":\"Ra\\u00fal Santos-Rodr\\u00edguez\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ef1e94eb2a277022dde9c9908497d50eed51d5e9\",\"title\":\"Proceedings of the 28th International Conference on Machine Learning, ICML 2011\",\"url\":\"https://www.semanticscholar.org/paper/ef1e94eb2a277022dde9c9908497d50eed51d5e9\",\"venue\":\"ICML 2011\",\"year\":2011},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ce7f136497b2dc1afea4c874b1e735235f04ddc2\",\"title\":\"Ijcai 2007\",\"url\":\"https://www.semanticscholar.org/paper/ce7f136497b2dc1afea4c874b1e735235f04ddc2\",\"venue\":\"K\\u00fcnstliche Intell.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113713833\",\"name\":\"S. J. Crawford\"},{\"authorId\":\"116971860\",\"name\":\"D. F. Kelley\"},{\"authorId\":\"137736247\",\"name\":\"In\\u00e9s Hern\\u00e1ndez \\u00c1vila\"}],\"doi\":\"10.1023/A:1017127612903\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"title\":\"Volume 1\",\"url\":\"https://www.semanticscholar.org/paper/6a8d3d2b99ab48fef1848fb16506a126a2702bde\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"100678123\",\"name\":\"A. Conradi\"},{\"authorId\":\"1450401234\",\"name\":\"K. Per-Johnny\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bb6c1823a868bea657c635be28f433fd7352a53\",\"title\":\"Robotics and Autonomous Systems { Project Report\",\"url\":\"https://www.semanticscholar.org/paper/7bb6c1823a868bea657c635be28f433fd7352a53\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786249\",\"name\":\"D. Bertsekas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a82db864e472b5aa6313596ef9919f64e3363b1f\",\"title\":\"Dynamic Programming and Optimal Control\",\"url\":\"https://www.semanticscholar.org/paper/a82db864e472b5aa6313596ef9919f64e3363b1f\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Georgios E Fainekos\"},{\"authorId\":null,\"name\":\"Hadas KressGazit\"},{\"authorId\":null,\"name\":\"George J Pappas. Temporal logic motion planning for mobil Robotics\"},{\"authorId\":null,\"name\":\"Automation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"2005\",\"url\":\"\",\"venue\":\"ICRA 2005. Proceedings of the 2005 IEEE International Conference on, pages 2020\\u20132025. IEEE,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145175901\",\"name\":\"M. Nivat\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"400a433d9f882f3218b516b4be656ff79e4711fb\",\"title\":\"Theoretical Computer Science Volume 213-214\",\"url\":\"https://www.semanticscholar.org/paper/400a433d9f882f3218b516b4be656ff79e4711fb\",\"venue\":\"\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34968061\",\"name\":\"Deepak Ramachandran\"},{\"authorId\":\"145957401\",\"name\":\"E. Amir\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd62bc380b66500c31a8f1a8b566bcaea25d1652\",\"title\":\"Bayesian Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/fd62bc380b66500c31a8f1a8b566bcaea25d1652\",\"venue\":\"IJCAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"},{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"144021446\",\"name\":\"Anind K. Dey\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"11b6bdfe36c48b11367b27187da11d95892f0361\",\"title\":\"Maximum Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/11b6bdfe36c48b11367b27187da11d95892f0361\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leonardo Bobadilla\"},{\"authorId\":null,\"name\":\"Oscar Sanchez\"},{\"authorId\":null,\"name\":\"Justin Czarnowski\"},{\"authorId\":null,\"name\":\"Katrina Gossman\"},{\"authorId\":null,\"name\":\"Steven M LaValle\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Michael Bloem and Nicholas Bambos . Infinite time horizon maximum causal entropy inverse reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1015330.1015430\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"title\":\"Apprenticeship learning via inverse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"100655694\",\"name\":\"A. Moore\"}],\"doi\":\"10.1145/1143844\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"title\":\"Proceedings of the 23rd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"venue\":\"ICML 2008\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"81373a804c9b32fb30fdafaba0c97b995de3fd4c\",\"title\":\"Between Imitation and Intention Learning\",\"url\":\"https://www.semanticscholar.org/paper/81373a804c9b32fb30fdafaba0c97b995de3fd4c\",\"venue\":\"IJCAI\",\"year\":2015}],\"title\":\"Learning from Demonstrations with High-Level Side Information\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Requirement\",\"topicId\":\"136\",\"url\":\"https://www.semanticscholar.org/topic/136\"},{\"topic\":\"Linear temporal logic\",\"topicId\":\"63286\",\"url\":\"https://www.semanticscholar.org/topic/63286\"},{\"topic\":\"Motion planning\",\"topicId\":\"24367\",\"url\":\"https://www.semanticscholar.org/topic/24367\"},{\"topic\":\"State space\",\"topicId\":\"6115\",\"url\":\"https://www.semanticscholar.org/topic/6115\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"Automaton\",\"topicId\":\"10069\",\"url\":\"https://www.semanticscholar.org/topic/10069\"},{\"topic\":\"ENCODE\",\"topicId\":\"365717\",\"url\":\"https://www.semanticscholar.org/topic/365717\"},{\"topic\":\"Loss function\",\"topicId\":\"3650\",\"url\":\"https://www.semanticscholar.org/topic/3650\"}],\"url\":\"https://www.semanticscholar.org/paper/635ed492434411e644cfc81683d897f1b0c2ae39\",\"venue\":\"IJCAI\",\"year\":2017}\n"