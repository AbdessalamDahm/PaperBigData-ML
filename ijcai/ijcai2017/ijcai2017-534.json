"{\"abstract\":\"Reinforcement Learning (RL) can be extremely effective in solving complex, real-world problems. However, injecting human knowledge into an RL agent may require extensive effort and expertise on the human designer's part. To date, human factors are generally not considered in the development and evaluation of possible RL approaches. In this article, we set out to investigate how different methods for injecting human knowledge are applied, in practice, by human designers of varying levels of knowledge and skill. We perform the first empirical evaluation of several methods, including a newly proposed method named SASS which is based on the notion of similarities in the agent's state-action space. Through this human study, consisting of 51 human participants, we shed new light on the human factors that play a key role in RL. We find that the classical reward shaping technique seems to be the most natural method for most designers, both expert and non-expert, to speed up RL. However, we further find that our proposed method SASS can be effectively and efficiently combined with reward shaping, and provides a beneficial alternative to using only a single speedup method with minimal human designer effort overhead.\",\"arxivId\":\"1805.05769\",\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\",\"url\":\"https://www.semanticscholar.org/author/39890672\"},{\"authorId\":\"1389208435\",\"name\":\"Matthew E. Taylor\",\"url\":\"https://www.semanticscholar.org/author/1389208435\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\",\"url\":\"https://www.semanticscholar.org/author/144992450\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Von Menschlichem\"},{\"authorId\":\"3041209\",\"name\":\"Dorothea Koert\"},{\"authorId\":null,\"name\":\"Tag der Einreichung\"},{\"authorId\":null,\"name\":\"Erkl\\u00e4rung zur Master-Thesis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4494395f9e8550e5516c44ef7ce2cb71b217374f\",\"title\":\"Learning from Human Feedback: A Comparison of Interactive Reinforcement Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/4494395f9e8550e5516c44ef7ce2cb71b217374f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32413884\",\"name\":\"Keita Sugiyama\"},{\"authorId\":\"9358647\",\"name\":\"Naoki Fukuta\"}],\"doi\":\"10.1109/AIT49014.2019.9144766\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a7a13ec4b3de3732028d80131a1994c67d1cc66f\",\"title\":\"A Multiagent Learning Approach for Distributed Control of Address Randomization in Communication Destination Anonymization\",\"url\":\"https://www.semanticscholar.org/paper/a7a13ec4b3de3732028d80131a1994c67d1cc66f\",\"venue\":\"2019 International Congress on Applied Information Technology (AIT)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36736090\",\"name\":\"Christian Arzate Cruz\"},{\"authorId\":\"48264024\",\"name\":\"T. Igarashi\"}],\"doi\":\"10.1145/3357236.3395525\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"1e273fb74ebe3d0cb860a301292cee066d058d12\",\"title\":\"A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges\",\"url\":\"https://www.semanticscholar.org/paper/1e273fb74ebe3d0cb860a301292cee066d058d12\",\"venue\":\"Conference on Designing Interactive Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"144330511\",\"name\":\"M. Cohen\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":\"10.1017/S0269888918000206\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f6af321fc3e3f7ad890c3c793b3f68fe5f2f3d48\",\"title\":\"Leveraging human knowledge in tabular reinforcement learning: a study of human subjects\",\"url\":\"https://www.semanticscholar.org/paper/f6af321fc3e3f7ad890c3c793b3f68fe5f2f3d48\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.24963/ijcai.2018/817\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2769ea248be3ccdcd72c6c1c2bcc55d496c779a0\",\"title\":\"Improving Reinforcement Learning with Human Input\",\"url\":\"https://www.semanticscholar.org/paper/2769ea248be3ccdcd72c6c1c2bcc55d496c779a0\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9a6a59cbb26848e8e6e62a26e3c953e4c939218f\",\"title\":\"Automated Agents for Advice Provision\",\"url\":\"https://www.semanticscholar.org/paper/9a6a59cbb26848e8e6e62a26e3c953e4c939218f\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"146007280\",\"name\":\"Andrew A. Anderson\"},{\"authorId\":\"1519478862\",\"name\":\"Jonathan Dodge\"},{\"authorId\":\"88727643\",\"name\":\"Amrita Sadarangani\"},{\"authorId\":\"88728623\",\"name\":\"Zoe Juozapaitis\"},{\"authorId\":\"145364055\",\"name\":\"Evan Newman\"},{\"authorId\":\"1519978688\",\"name\":\"Jed Irvine\"},{\"authorId\":\"1680063325\",\"name\":\"Souti Chattopadhyay\"},{\"authorId\":\"113923179\",\"name\":\"Matthew H. Olson\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"},{\"authorId\":\"69854669\",\"name\":\"M. Burnett\"}],\"doi\":\"10.1145/3366485\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e654e71b65de3c8b549a553c526f3779042f1e2e\",\"title\":\"Mental Models of Mere Mortals with Explanations of Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e654e71b65de3c8b549a553c526f3779042f1e2e\",\"venue\":\"ACM Trans. Interact. Intell. Syst.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36736090\",\"name\":\"Christian Arzate Cruz\"},{\"authorId\":\"48264024\",\"name\":\"T. Igarashi\"}],\"doi\":\"10.1145/3383668.3419938\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e4a97d0f533de9e0050cd032a50e3d41ae8bf6ce\",\"title\":\"MarioMix: Creating Aligned Playstyles for Bots with Interactive Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e4a97d0f533de9e0050cd032a50e3d41ae8bf6ce\",\"venue\":\"CHI PLAY\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10393074\",\"name\":\"C. Deng\"},{\"authorId\":\"2004262468\",\"name\":\"Xunbi A. Ji\"},{\"authorId\":\"2004335467\",\"name\":\"Colton Rainey\"},{\"authorId\":\"47539412\",\"name\":\"Jian-yu Zhang\"},{\"authorId\":\"1410166574\",\"name\":\"Wei Lu\"}],\"doi\":\"10.1016/j.isci.2020.101656\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b9d8420edb23b03eb43bd5aa11e5c0a7d25e0c18\",\"title\":\"Integrating Machine Learning with Human Knowledge\",\"url\":\"https://www.semanticscholar.org/paper/b9d8420edb23b03eb43bd5aa11e5c0a7d25e0c18\",\"venue\":\"iScience\",\"year\":2020}],\"corpusId\":21674940,\"doi\":\"10.24963/ijcai.2017/534\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"51f4ea9831e23777bf1cbfed165c1cffceb209ec\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"1746466\",\"name\":\"Amos Azaria\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"},{\"authorId\":\"5006412\",\"name\":\"C. Goldman\"},{\"authorId\":\"1723297\",\"name\":\"Omer Tsimhoni\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d675fdc0b218c17e3d9d44bb897a4ed180b28ebf\",\"title\":\"Adaptive Advice in Automobile Climate Control Systems\",\"url\":\"https://www.semanticscholar.org/paper/d675fdc0b218c17e3d9d44bb897a4ed180b28ebf\",\"venue\":\"AAAI Workshop: AI for Transportation\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4562073\",\"name\":\"Chris Watkins\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5c8bb027eb65b6d250a22e9b6db22853a552ac81\",\"title\":\"Learning from delayed rewards\",\"url\":\"https://www.semanticscholar.org/paper/5c8bb027eb65b6d250a22e9b6db22853a552ac81\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R. S. Sutton\"},{\"authorId\":null,\"name\":\"A. G. Barto\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Reinforcement learning: An introduction, MIT press\",\"url\":\"\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ariel Rosenfeld\"},{\"authorId\":null,\"name\":\"Sarit Kraus. Providing arguments in discussions on the b behavior\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"TiiS\",\"url\":\"\",\"venue\":\"6(4):30:1\\u201330:33,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shravan Matthur Narayanamurthy\"},{\"authorId\":null,\"name\":\"Balaraman Ravindran. On the hardness of finding symmetries i processes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 688\\u2013695,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145177632\",\"name\":\"B. Tanner\"},{\"authorId\":\"35367799\",\"name\":\"A. White\"}],\"doi\":\"10.1145/1577069.1755857\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"865af2da99c2cc3b9718522fd337368701f12897\",\"title\":\"RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments\",\"url\":\"https://www.semanticscholar.org/paper/865af2da99c2cc3b9718522fd337368701f12897\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4537947\",\"name\":\"A. W. Landfield\"}],\"doi\":\"10.1007/978-1-4615-9125-2_5\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"03c926f23d30550108025d2b1fc6c20f64a9f6e1\",\"title\":\"Personal Construct Psychology\",\"url\":\"https://www.semanticscholar.org/paper/03c926f23d30550108025d2b1fc6c20f64a9f6e1\",\"venue\":\"\",\"year\":1980},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ariel Rosenfeld\"},{\"authorId\":null,\"name\":\"Joseph Keshet\"},{\"authorId\":null,\"name\":\"Claudia V. Goldman\"},{\"authorId\":null,\"name\":\"Sarit Kraus. Online prediction of exponential decay time application\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ECAI\",\"url\":\"\",\"venue\":\"pages 595\\u2013603,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. J. Mataric\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Reward functions for accelerated learning, in \\u2018Machine Learning\",\"url\":\"\",\"venue\":\"Proceedings of the Eleventh international conference\\u2019,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Zinkevich\"},{\"authorId\":null,\"name\":\"T. Balch\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Symmetry in markov decision processes and its implications for single agent and multi agent learning, in \\u2018ICML\",\"url\":\"\",\"venue\":\"\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Ribeiro\"},{\"authorId\":null,\"name\":\"Csaba Szepesv\\u00e1ri\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Q-learning combined with spreading: Convergence and results\",\"url\":\"\",\"venue\":\"Proc. of Intelligent and Cognitive Systems,\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30710621\",\"name\":\"J. Randl\\u00f8v\"},{\"authorId\":\"48495271\",\"name\":\"P. Alstr\\u00f8m\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9d8f6219fbd2da14d8d55562dcedf43fe671d0e3\",\"title\":\"Learning to Drive a Bicycle Using Reinforcement Learning and Shaping\",\"url\":\"https://www.semanticscholar.org/paper/9d8f6219fbd2da14d8d55562dcedf43fe671d0e3\",\"venue\":\"ICML\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39473200\",\"name\":\"M. Tamassia\"},{\"authorId\":\"1739843\",\"name\":\"Fabio Zambetta\"},{\"authorId\":\"1686864\",\"name\":\"W. Raffe\"},{\"authorId\":\"144769043\",\"name\":\"Florian S. M\\u00fcller\"},{\"authorId\":\"40915593\",\"name\":\"X. Li\"}],\"doi\":\"10.3233/978-1-61499-672-9-46\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"506f779f5c8e635a4c58488baadfb543772cf3fb\",\"title\":\"Dynamic Choice of State Abstraction in Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/506f779f5c8e635a4c58488baadfb543772cf3fb\",\"venue\":\"ECAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1838206.1838208\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"accb4b7a1e670ec1be3d5a2e784b8f524ff8b303\",\"title\":\"Combining manual feedback with subsequent MDP reward signals for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/accb4b7a1e670ec1be3d5a2e784b8f524ff8b303\",\"venue\":\"AAMAS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1742183\",\"name\":\"M. Mataric\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50030-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"563cf7359970015db2f8a05c41e9efef7025dda2\",\"title\":\"Reward Functions for Accelerated Learning\",\"url\":\"https://www.semanticscholar.org/paper/563cf7359970015db2f8a05c41e9efef7025dda2\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32999467\",\"name\":\"Reinaldo A. C. Bianchi\"},{\"authorId\":\"145279513\",\"name\":\"M. F. Martins\"},{\"authorId\":\"3432492\",\"name\":\"C. Ribeiro\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"}],\"doi\":\"10.1109/TCYB.2013.2253094\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ecfc310ae505e603b29ca42a68f827dd5a44e29\",\"title\":\"Heuristically-Accelerated Multiagent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ecfc310ae505e603b29ca42a68f827dd5a44e29\",\"venue\":\"IEEE Transactions on Cybernetics\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744526\",\"name\":\"Pedro Sequeira\"},{\"authorId\":\"145125979\",\"name\":\"Francisco S. Melo\"},{\"authorId\":\"145136631\",\"name\":\"Ana Paiva\"}],\"doi\":\"10.1007/978-3-642-40669-0_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"baa756d70105720bd5c5d88e07ead1f31f3de9b0\",\"title\":\"An Associative State-Space Metric for Learning in Factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/baa756d70105720bd5c5d88e07ead1f31f3de9b0\",\"venue\":\"EPIA\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50027-1\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"title\":\"Markov Games as a Framework for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709638\",\"name\":\"L. Busoniu\"},{\"authorId\":\"1705222\",\"name\":\"Robert Babu\\u0161ka\"},{\"authorId\":\"1724741\",\"name\":\"B. D. Schutter\"},{\"authorId\":\"1751167\",\"name\":\"D. Ernst\"}],\"doi\":\"10.1201/9781439821091\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc22d04377d75c35fd806620687143e7a120db5d\",\"title\":\"Reinforcement Learning and Dynamic Programming Using Function Approximators\",\"url\":\"https://www.semanticscholar.org/paper/cc22d04377d75c35fd806620687143e7a120db5d\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"27049951\",\"name\":\"F. Verloove\"},{\"authorId\":\"3340900\",\"name\":\"E. Robbrecht\"}],\"doi\":\"10.1023/A:1017123022676\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1ee449f7a2c0bf0620109fede6446213407d7fa6\",\"title\":\"Volume 39\",\"url\":\"https://www.semanticscholar.org/paper/1ee449f7a2c0bf0620109fede6446213407d7fa6\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"32182645\",\"name\":\"R. Loftin\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"145630067\",\"name\":\"D. Roberts\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"14371054fc71f64d5c93d8e503e8d813b775b09e\",\"title\":\"A Need for Speed: Adapting Agent Action Speed to Improve Task Learning from Non-Expert Humans\",\"url\":\"https://www.semanticscholar.org/paper/14371054fc71f64d5c93d8e503e8d813b775b09e\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"James Sacra Albus. Brains\"},{\"authorId\":null,\"name\":\"Behavior\"},{\"authorId\":null,\"name\":\"Robotics. McGraw-Hill\"},{\"authorId\":null,\"name\":\"Inc.\"},{\"authorId\":null,\"name\":\"New York\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"NY\",\"url\":\"\",\"venue\":\"USA,\",\"year\":1981},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98241663\",\"name\":\"M. V. Rossum\"}],\"doi\":\"10.1142/9789814360784_0003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"title\":\"Neural Computation\",\"url\":\"https://www.semanticscholar.org/paper/2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35022714\",\"name\":\"S. Girgin\"},{\"authorId\":\"1761620\",\"name\":\"F. Polat\"},{\"authorId\":\"144451975\",\"name\":\"R. Alhajj\"}],\"doi\":\"10.1109/TSMCB.2007.899419\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"af0b43eff6ce36f041418ed33124fa8c101dbbee\",\"title\":\"Positive Impact of State Similarity on Reinforcement Learning Performance\",\"url\":\"https://www.semanticscholar.org/paper/af0b43eff6ce36f041418ed33124fa8c101dbbee\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693696\",\"name\":\"S. Devlin\"},{\"authorId\":\"2186376\",\"name\":\"M. Grzes\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"879ac22bb38dd6dd6c1622b062fc0299965e8d60\",\"title\":\"Multi-agent, reward shaping for RoboCup KeepAway\",\"url\":\"https://www.semanticscholar.org/paper/879ac22bb38dd6dd6c1622b062fc0299965e8d60\",\"venue\":\"AAMAS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"2442305\",\"name\":\"N. Agmon\"},{\"authorId\":\"144390881\",\"name\":\"O. Maksimov\"},{\"authorId\":\"152473727\",\"name\":\"S. Kraus\"}],\"doi\":\"10.1016/j.artint.2017.08.005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a4a0a118e091601d1f1e04a704d71333d940a688\",\"title\":\"Intelligent agent supporting human-multi-robot team collaboration\",\"url\":\"https://www.semanticscholar.org/paper/a4a0a118e091601d1f1e04a704d71333d940a688\",\"venue\":\"Artif. Intell.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"2811287\",\"name\":\"Halit Bener Suay\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f926d3bf410875857effe1b5000a4fbc25397b74\",\"title\":\"Reinforcement Learning from Demonstration through Shaping\",\"url\":\"https://www.semanticscholar.org/paper/f926d3bf410875857effe1b5000a4fbc25397b74\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3005102\",\"name\":\"Shravan M. Narayanamurthy\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":\"10.1145/1390156.1390243\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03f2d1db7b7da30ae249545f6912e9bce5a05030\",\"title\":\"On the hardness of finding symmetries in Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/03f2d1db7b7da30ae249545f6912e9bce5a05030\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Schaul\"},{\"authorId\":null,\"name\":\"J. Bayer\"},{\"authorId\":null,\"name\":\"D. Wierstra\"},{\"authorId\":null,\"name\":\"Y. Sun\"},{\"authorId\":null,\"name\":\"M. Felder\"},{\"authorId\":null,\"name\":\"F. Sehnke\"},{\"authorId\":null,\"name\":\"T. R\\u00fcckstie\\u00df\"},{\"authorId\":null,\"name\":\"J. Schmidhuber\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"PyBrain\\u2019, Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1700606\",\"name\":\"B. Leffler\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"145957225\",\"name\":\"T. Edmunds\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c9282ac55cf2fb6a9123ac30fc108843ca8a44c4\",\"title\":\"Efficient Reinforcement Learning with Relocatable Action Models\",\"url\":\"https://www.semanticscholar.org/paper/c9282ac55cf2fb6a9123ac30fc108843ca8a44c4\",\"venue\":\"AAAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0757c265ef2ccc697d58af06b9be40def0c3eac\",\"title\":\"Speeding up Tabular Reinforcement Learning Using State-Action Similarities\",\"url\":\"https://www.semanticscholar.org/paper/c0757c265ef2ccc697d58af06b9be40def0c3eac\",\"venue\":\"AAMAS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Brockman\"},{\"authorId\":null,\"name\":\"V. Cheung\"},{\"authorId\":null,\"name\":\"L. Pettersson\"},{\"authorId\":null,\"name\":\"J. Schneider\"},{\"authorId\":null,\"name\":\"J. Schulman\"},{\"authorId\":null,\"name\":\"J. Tang\"},{\"authorId\":null,\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Openai gym\\u2019, https://gym.openai.com. [Online; accessed 24-10-2017\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. Ribeiro\"},{\"authorId\":null,\"name\":\"C. Szepesv\\u00e1ri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Q - learning combined with spreading : Convergence and results , in \\u2018 Procs . of the ISRF - IEE International Conf . on Intelligent and Cognitive Systems ( Neural Networks Symposium )\",\"url\":\"\",\"venue\":\"\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145279513\",\"name\":\"M. F. Martins\"},{\"authorId\":\"32999467\",\"name\":\"Reinaldo A. C. Bianchi\"}],\"doi\":\"10.1007/978-3-662-43645-5_2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"618ca05968c2933211399b2599b9c07bf3004769\",\"title\":\"Heuristically-Accelerated Reinforcement Learning: A Comparative Analysis of Performance\",\"url\":\"https://www.semanticscholar.org/paper/618ca05968c2933211399b2599b9c07bf3004769\",\"venue\":\"TAROS\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745716\",\"name\":\"Nicholas K. Jong\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1329125.1329242\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac3a76815a809ab844f34e90c4667488b89fbacd\",\"title\":\"Model-based function approximation in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/ac3a76815a809ab844f34e90c4667488b89fbacd\",\"venue\":\"AAMAS '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144110667\",\"name\":\"S. Hart\"},{\"authorId\":\"70517045\",\"name\":\"L. Staveland\"}],\"doi\":\"10.1016/S0166-4115(08)62386-9\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"966dfdd0b0bb6b50416559d35a2561c0b96d9a9e\",\"title\":\"Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research\",\"url\":\"https://www.semanticscholar.org/paper/966dfdd0b0bb6b50416559d35a2561c0b96d9a9e\",\"venue\":\"\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2811287\",\"name\":\"Halit Bener Suay\"},{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2896f821c991824fc0bc96949e4baedc37fee06a\",\"title\":\"Learning from Demonstration for Shaping through Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2896f821c991824fc0bc96949e4baedc37fee06a\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"145805766\",\"name\":\"G. Kuhlmann\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2394697\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/11780519_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ca0ea624b068042ca34264ddaa5e42620ceee875\",\"title\":\"Keepaway Soccer: From Machine Learning Testbed to Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/ca0ea624b068042ca34264ddaa5e42620ceee875\",\"venue\":\"RoboCup\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1162/089976699300016070\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7888be55fb513c20919f94f0274bd9b78bda44b6\",\"title\":\"A Unified Analysis of Value-Function-Based Reinforcement-Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/7888be55fb513c20919f94f0274bd9b78bda44b6\",\"venue\":\"Neural Computation\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Geramifard\"},{\"authorId\":null,\"name\":\"R. H. Klein\"},{\"authorId\":null,\"name\":\"C. Dann\"},{\"authorId\":null,\"name\":\"W. Dabney\"},{\"authorId\":null,\"name\":\"J. P. How\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"RLPy: The Reinforcement Learning Library for Education and Research\\u2019, http://acl.mit.edu/RLPy\",\"url\":\"\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"},{\"authorId\":\"1826964\",\"name\":\"Tucker R. Balch\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"21972a9989136617f8e9779792faf4e2da12139f\",\"title\":\"Symmetry in Markov Decision Processes and its Implications for Single Agent and Multiagent Learning\",\"url\":\"https://www.semanticscholar.org/paper/21972a9989136617f8e9779792faf4e2da12139f\",\"venue\":\"ICML\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"20355159\",\"name\":\"P. Levy\"},{\"authorId\":\"1707363\",\"name\":\"D. Sarne\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"93754e57b284af281a5d4fce24c649006fc6b68c\",\"title\":\"Intelligent Advice Provisioning for Repeated Interaction\",\"url\":\"https://www.semanticscholar.org/paper/93754e57b284af281a5d4fce24c649006fc6b68c\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Ai Magazine\",\"url\":\"\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nicholas K Jong\"},{\"authorId\":null,\"name\":\"Peter Stone. Model-based function approximation in reinf AAMAS\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"page 95\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Geramifard\"},{\"authorId\":null,\"name\":\"R. H. Klein\"},{\"authorId\":null,\"name\":\"C. Dann\"},{\"authorId\":null,\"name\":\"W. Dabney\"},{\"authorId\":null,\"name\":\"J. P. How\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"\\u2018 RLPy : The Reinforcement Learning Library for Education and Research\",\"url\":\"\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"1771345\",\"name\":\"Joseph Keshet\"},{\"authorId\":\"5006412\",\"name\":\"C. Goldman\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":\"10.3233/978-1-61499-672-9-595\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f4a203dd8cbe7e0cadbecc7b184f2f9c012d2a07\",\"title\":\"Online Prediction of Exponential Decay Time Series with Human-Agent Application\",\"url\":\"https://www.semanticscholar.org/paper/f4a203dd8cbe7e0cadbecc7b184f2f9c012d2a07\",\"venue\":\"ECAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1678200\",\"name\":\"J. Bruner\"}],\"doi\":\"10.4324/9780203088609-8\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b91cab0683737c5c980d1356a1ed07dafddfbd72\",\"title\":\"Going Beyond the Information Given\",\"url\":\"https://www.semanticscholar.org/paper/b91cab0683737c5c980d1356a1ed07dafddfbd72\",\"venue\":\"\",\"year\":1973},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L Littman. Markov games as a framework for multi-a ICML\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 157\",\"url\":\"\",\"venue\":\"pages 157\\u2013163,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Rosenfeld\"},{\"authorId\":null,\"name\":\"M. E. Taylor\"},{\"authorId\":null,\"name\":\"S. Kraus\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Speeding up tabular reinforcement learning using stateaction similarities, in \\u2018AAMAS\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. S. Bruner\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Going beyond the information given\\u2019, Contemporary approaches to cognition\",\"url\":\"\",\"venue\":\"\",\"year\":1957},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bei Peng\"},{\"authorId\":null,\"name\":\"James MacGlashan\"},{\"authorId\":null,\"name\":\"Robert Loftin\"},{\"authorId\":null,\"name\":\"Michael L Littman\"},{\"authorId\":null,\"name\":\"David L Roberts\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Shravan Matthur Narayanamurthy and Balaraman Ravindran . On the hardness of finding symmetries in markov decision processes\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3259367\",\"name\":\"J. Albus\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"b42c83364f2f31ac5c27ccf0bd560f1620583cfb\",\"title\":\"Brains, behavior, and robotics\",\"url\":\"https://www.semanticscholar.org/paper/b42c83364f2f31ac5c27ccf0bd560f1620583cfb\",\"venue\":\"\",\"year\":1981},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398948869\",\"name\":\"Richard S. Sutton\"}],\"doi\":\"10.1145/122344.122377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"831edc3d67457db83da40d260e93bfd7559347ae\",\"title\":\"Dyna, an integrated architecture for learning, planning, and reacting\",\"url\":\"https://www.semanticscholar.org/paper/831edc3d67457db83da40d260e93bfd7559347ae\",\"venue\":\"SGAR\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144371832\",\"name\":\"E. Gibson\"},{\"authorId\":\"1678200\",\"name\":\"J. Bruner\"},{\"authorId\":\"46697628\",\"name\":\"E. Brunswik\"},{\"authorId\":\"5281667\",\"name\":\"L. Festinger\"},{\"authorId\":\"50592305\",\"name\":\"F. Heider\"},{\"authorId\":\"16803507\",\"name\":\"K. F. Muenzinger\"},{\"authorId\":\"70106961\",\"name\":\"C. Osgood\"},{\"authorId\":\"36149833\",\"name\":\"D. Rapaport\"}],\"doi\":\"10.2307/1420352\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e2600a50b66fb924df9bd9b8964c679b368ffc0d\",\"title\":\"Contemporary approaches to cognition\",\"url\":\"https://www.semanticscholar.org/paper/e2600a50b66fb924df9bd9b8964c679b368ffc0d\",\"venue\":\"\",\"year\":1958},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3432492\",\"name\":\"C. Ribeiro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"07f89ac50e638be8ac455299830cbc5d854f9831\",\"title\":\"Attentional Mechanisms as a Strategy for Generalization in the Q-Learning Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/07f89ac50e638be8ac455299830cbc5d854f9831\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ariel Rosenfeld\"},{\"authorId\":null,\"name\":\"Amos Azaria\"},{\"authorId\":null,\"name\":\"Sarit Kraus\"},{\"authorId\":null,\"name\":\"Claudia V. Goldman\"},{\"authorId\":null,\"name\":\"Omer Tsimhoni. Adaptive advice in automobile climate co systems\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 543\\u2013551,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1007/s10994-012-5322-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"055e69a2210fe8db1c8d8be75fbecd894363d6c4\",\"title\":\"TEXPLORE: real-time sample-efficient reinforcement learning for robots\",\"url\":\"https://www.semanticscholar.org/paper/055e69a2210fe8db1c8d8be75fbecd894363d6c4\",\"venue\":\"Machine Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145217663\",\"name\":\"M. Stewart\"}],\"doi\":\"10.1016/J.WEM.2014.12.006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17200cec932975996054db71c83f1d41a9bfa876\",\"title\":\"Ieee Transactions On Cybernetics\",\"url\":\"https://www.semanticscholar.org/paper/17200cec932975996054db71c83f1d41a9bfa876\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pedro Sequeira\"},{\"authorId\":null,\"name\":\"Francisco S Melo\"},{\"authorId\":null,\"name\":\"Ana Paiva. An associative state-space metric for learn Intelligence\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 163\\u2013174\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Halit Bener Suay\"},{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"},{\"authorId\":null,\"name\":\"Sonia Chernova. Learning from demonstration for shaping learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 429\\u2013437,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marco Tamassia\"},{\"authorId\":null,\"name\":\"Fabio Zambetta\"},{\"authorId\":null,\"name\":\"William Raffe\"},{\"authorId\":null,\"name\":\"Florian Mueller\"},{\"authorId\":null,\"name\":\"Xiaodong Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"namic choice of state abstraction in qlearning\",\"url\":\"\",\"venue\":\"Neural computation\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A unified analysis of valuefunctionbased reinforcementlearning algorithms\",\"url\":\"\",\"venue\":\"ACM SIGART Bulletin\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":\"10.1145/2983925\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d4c03523a0628ec2d1e032469ad91f8fa26a8cb\",\"title\":\"Providing Arguments in Discussions on the Basis of the Prediction of Human Argumentative Behavior\",\"url\":\"https://www.semanticscholar.org/paper/8d4c03523a0628ec2d1e032469ad91f8fa26a8cb\",\"venue\":\"TIIS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"144330511\",\"name\":\"M. Cohen\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":\"10.1017/S0269888918000206\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f6af321fc3e3f7ad890c3c793b3f68fe5f2f3d48\",\"title\":\"Leveraging human knowledge in tabular reinforcement learning: a study of human subjects\",\"url\":\"https://www.semanticscholar.org/paper/f6af321fc3e3f7ad890c3c793b3f68fe5f2f3d48\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ariel Rosenfeld\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"},{\"authorId\":null,\"name\":\"Sarit Kraus. Speeding up tabular reinforcement learning similarities\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 1722\\u2013 1724,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Kelly\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Personal construct psychology, New York: Norton\",\"url\":\"\",\"venue\":\"\",\"year\":1955},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1746466\",\"name\":\"Amos Azaria\"},{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"152473727\",\"name\":\"S. Kraus\"},{\"authorId\":\"5006412\",\"name\":\"C. Goldman\"},{\"authorId\":\"1723297\",\"name\":\"Omer Tsimhoni\"}],\"doi\":\"10.1609/aimag.v36i3.2603\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c29b6c91079bd4de540e3747afce2d673cce40c\",\"title\":\"Advice Provision for Energy Saving in Automobile Climate-Control System\",\"url\":\"https://www.semanticscholar.org/paper/1c29b6c91079bd4de540e3747afce2d673cce40c\",\"venue\":\"AI Mag.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71619391\",\"name\":\"\\u0e2d\\u0e19\\u0e34\\u0e23\\u0e38\\u0e18 \\u0e2a\\u0e37\\u0e1a\\u0e2a\\u0e34\\u0e07\\u0e2b\\u0e4c\"}],\"doi\":\"10.1016/c2009-0-19715-5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b42b1bfdc262bf99e9484e2e9df94df216b96374\",\"title\":\"Data Mining Practical Machine Learning Tools and Techniques\",\"url\":\"https://www.semanticscholar.org/paper/b42b1bfdc262bf99e9484e2e9df94df216b96374\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. S. Albus\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Brains, Behavior and Robotics, McGraw-Hill, Inc\",\"url\":\"\",\"venue\":\"\",\"year\":1981},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Anna Harutyunyan\"},{\"authorId\":null,\"name\":\"Halit Bener Suay\"},{\"authorId\":null,\"name\":\"Sonia Chernova\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9. Reinforcement learning from demonstration th shaping\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 3352\\u20133358,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Watkins\"},{\"authorId\":null,\"name\":\"H C.J.C.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Learning from delayed rewards, PhD thesis, University of Cambridge England\",\"url\":\"\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. Ribeiro\"},{\"authorId\":null,\"name\":\"C. Szepesv\\u00e1ri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Q-learning combined with spreading: Convergence and results, in \\u2018Procs\",\"url\":\"\",\"venue\":\"of the ISRF-IEE International Conf. on Intelligent and Cognitive Systems (Neural Networks Symposium)\\u2019,\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54c5bc55e67f4af0e6e097aa8f1f75e6c7d0cac5\",\"title\":\"Combining Multiple Correlated Reward and Shaping Signals by Measuring Confidence\",\"url\":\"https://www.semanticscholar.org/paper/54c5bc55e67f4af0e6e097aa8f1f75e6c7d0cac5\",\"venue\":\"AAAI\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39890672\",\"name\":\"Ariel Rosenfeld\"},{\"authorId\":\"144992450\",\"name\":\"S. Kraus\"}],\"doi\":\"10.2200/S00820ED1V01Y201712AIM036\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5dff6badc55a45ace7ae82bc22cca1af144b12ce\",\"title\":\"Predicting Human Decision-Making: From Prediction to Action\",\"url\":\"https://www.semanticscholar.org/paper/5dff6badc55a45ace7ae82bc22cca1af144b12ce\",\"venue\":\"Predicting Human Decision-Making\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S Sutton. Dyna\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"an integrated architecture for learning\",\"url\":\"\",\"venue\":\"planning, and reacting. ACM SIGART Bulletin, 2(4):160\\u2013163,\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2463017\",\"name\":\"S. Karakovskiy\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/TCIAIG.2012.2188528\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"title\":\"The Mario AI Benchmark and Competitions\",\"url\":\"https://www.semanticscholar.org/paper/6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2707413\",\"name\":\"M. Benda\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c3250315567b52c3a300bde8e7183dadd720bf25\",\"title\":\"On Optimal Cooperation of Knowledge Sources\",\"url\":\"https://www.semanticscholar.org/paper/c3250315567b52c3a300bde8e7183dadd720bf25\",\"venue\":\"\",\"year\":1985},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. H. Ribeiro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Attentional mechanisms as a strategy for generalisation in the q-learning algorithm, in \\u2018Proceedings of ICANN\",\"url\":\"\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9\"},{\"authorId\":null,\"name\":\"Daniel Kudenko\"},{\"authorId\":null,\"name\":\"Matthew E Taylor. Combining multiple correlated reward\"},{\"authorId\":null,\"name\":\"shaping signals by measuring confidence\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 1687\\u20131693,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"B. F. Skinner\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Reinforcement today.\",\"url\":\"\",\"venue\":\"American Psychologist\",\"year\":1958},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ariel Rosenfeld\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"},{\"authorId\":null,\"name\":\"Sarit Kraus\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Goldman , and Sarit Kraus . Online prediction of exponential decay time series with humanagent application\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145528658\",\"name\":\"G. Kendall\"}],\"doi\":\"10.1109/TCIAIG.2015.2409514\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9046f46d088eee7be4af8be5ffea602394a937c0\",\"title\":\"Editorial: IEEE Transactions on Computational Intelligence and AI in Games\",\"url\":\"https://www.semanticscholar.org/paper/9046f46d088eee7be4af8be5ffea602394a937c0\",\"venue\":\"IEEE Trans. Comput. Intell. AI Games\",\"year\":2015}],\"title\":\"Leveraging Human Knowledge in Tabular Reinforcement Learning: A Study of Human Subjects\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Speedup\",\"topicId\":\"4162\",\"url\":\"https://www.semanticscholar.org/topic/4162\"},{\"topic\":\"Noise shaping\",\"topicId\":\"135480\",\"url\":\"https://www.semanticscholar.org/topic/135480\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Sass\",\"topicId\":\"67658\",\"url\":\"https://www.semanticscholar.org/topic/67658\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Table (information)\",\"topicId\":\"200\",\"url\":\"https://www.semanticscholar.org/topic/200\"},{\"topic\":\"Human factors and ergonomics\",\"topicId\":\"4501\",\"url\":\"https://www.semanticscholar.org/topic/4501\"},{\"topic\":\"Linear function\",\"topicId\":\"45804\",\"url\":\"https://www.semanticscholar.org/topic/45804\"},{\"topic\":\"Approximation\",\"topicId\":\"3247\",\"url\":\"https://www.semanticscholar.org/topic/3247\"},{\"topic\":\"Programmer\",\"topicId\":\"23475\",\"url\":\"https://www.semanticscholar.org/topic/23475\"},{\"topic\":\"Overhead (computing)\",\"topicId\":\"4163\",\"url\":\"https://www.semanticscholar.org/topic/4163\"},{\"topic\":\"QR decomposition\",\"topicId\":\"105094\",\"url\":\"https://www.semanticscholar.org/topic/105094\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Software quality assurance\",\"topicId\":\"54373\",\"url\":\"https://www.semanticscholar.org/topic/54373\"}],\"url\":\"https://www.semanticscholar.org/paper/51f4ea9831e23777bf1cbfed165c1cffceb209ec\",\"venue\":\"IJCAI\",\"year\":2017}\n"