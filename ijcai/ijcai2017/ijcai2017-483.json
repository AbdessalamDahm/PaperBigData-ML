"{\"abstract\":null,\"arxivId\":null,\"authors\":[{\"authorId\":\"2079174\",\"name\":\"Zongzhang Zhang\",\"url\":\"https://www.semanticscholar.org/author/2079174\"},{\"authorId\":\"39747083\",\"name\":\"Zhiyuan Pan\",\"url\":\"https://www.semanticscholar.org/author/39747083\"},{\"authorId\":\"2275756\",\"name\":\"Mykel J. Kochenderfer\",\"url\":\"https://www.semanticscholar.org/author/2275756\"}],\"citationVelocity\":7,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"71231791\",\"name\":\"S. Ohnishi\"},{\"authorId\":\"1773761\",\"name\":\"E. Uchibe\"},{\"authorId\":\"1452988350\",\"name\":\"Yotaro Yamaguchi\"},{\"authorId\":\"51516944\",\"name\":\"Kosuke Nakanishi\"},{\"authorId\":\"2048202\",\"name\":\"Yuji Yasui\"},{\"authorId\":\"145516720\",\"name\":\"S. Ishii\"}],\"doi\":\"10.3389/fnbot.2019.00103\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f1ba4d23cbd05b0e62171a95965d1cc0d2a30dca\",\"title\":\"Constrained Deep Q-Learning Gradually Approaching Ordinary Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/f1ba4d23cbd05b0e62171a95965d1cc0d2a30dca\",\"venue\":\"Front. Neurorobot.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2027160854\",\"name\":\"Jiarun Cai\"}],\"doi\":\"10.1007/978-3-030-63833-7_60\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b58de6291b8e5352c98566cd6300e53f244007d\",\"title\":\"WD3-MPER: A Method to Alleviate Approximation Bias in Actor-Critic\",\"url\":\"https://www.semanticscholar.org/paper/4b58de6291b8e5352c98566cd6300e53f244007d\",\"venue\":\"ICONIP\",\"year\":2020},{\"arxivId\":\"2011.14281\",\"authors\":[{\"authorId\":\"119890891\",\"name\":\"Changxi Zhu\"},{\"authorId\":\"1701688\",\"name\":\"Ho-fung Leung\"},{\"authorId\":\"1883634\",\"name\":\"S. Hu\"},{\"authorId\":\"1492164937\",\"name\":\"Yi Cai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3892daf1dfdea672a82aad2578aeec8c9b1bb122\",\"title\":\"A Q-values Sharing Framework for Multiagent Reinforcement Learning under Budget Constraint\",\"url\":\"https://www.semanticscholar.org/paper/3892daf1dfdea672a82aad2578aeec8c9b1bb122\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145514185\",\"name\":\"D. Tennakoon\"},{\"authorId\":\"66677606\",\"name\":\"S. Karunarathna\"},{\"authorId\":\"51120470\",\"name\":\"Brian Udugama\"}],\"doi\":\"10.1109/MERCON.2018.8421895\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"862f83ccc2274fc23cc231398ac837616522b439\",\"title\":\"Q-learning Approach for Load-balancing in Software Defined Networks\",\"url\":\"https://www.semanticscholar.org/paper/862f83ccc2274fc23cc231398ac837616522b439\",\"venue\":\"2018 Moratuwa Engineering Research Conference (MERCon)\",\"year\":2018},{\"arxivId\":\"2002.06487\",\"authors\":[{\"authorId\":\"51305487\",\"name\":\"Qingfeng Lan\"},{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"2655967\",\"name\":\"Alona Fyshe\"},{\"authorId\":\"114860989\",\"name\":\"M. White\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f4a9374772eb29ac55dd7eebb4a3093871092252\",\"title\":\"Maxmin Q-learning: Controlling the Estimation Bias of Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/f4a9374772eb29ac55dd7eebb4a3093871092252\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"103521898\",\"name\":\"X. Zhang\"}],\"doi\":\"10.1007/978-981-15-2770-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"78b8bee1bb67238916b32328a52bfd2e826e7cf2\",\"title\":\"A Matrix Algebra Approach to Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/78b8bee1bb67238916b32328a52bfd2e826e7cf2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.04269\",\"authors\":[{\"authorId\":\"1689648183\",\"name\":\"Arsenii Kuznetsov\"},{\"authorId\":\"51885067\",\"name\":\"Pavel Shvechikov\"},{\"authorId\":\"6328867\",\"name\":\"A. Grishin\"},{\"authorId\":\"2492721\",\"name\":\"D. Vetrov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"76a15540274a9940d22df70a70e7f8cad340d6c0\",\"title\":\"Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics\",\"url\":\"https://www.semanticscholar.org/paper/76a15540274a9940d22df70a70e7f8cad340d6c0\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"1802.08534\",\"authors\":[{\"authorId\":\"145001045\",\"name\":\"Y. Zheng\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"2079174\",\"name\":\"Zongzhang Zhang\"}],\"doi\":\"10.1007/978-3-319-97310-4_48\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cd4bde001cfa381dfa185dcaeb1e8903fe247b80\",\"title\":\"Weighted Double Deep Multiagent Reinforcement Learning in Stochastic Cooperative Environments\",\"url\":\"https://www.semanticscholar.org/paper/cd4bde001cfa381dfa185dcaeb1e8903fe247b80\",\"venue\":\"PRICAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2090968\",\"name\":\"Yao Xiang\"},{\"authorId\":\"7998512\",\"name\":\"Jingling Yuan\"},{\"authorId\":\"1697327\",\"name\":\"Ruiqi Luo\"},{\"authorId\":\"46812609\",\"name\":\"Xian Zhong\"},{\"authorId\":\"153051030\",\"name\":\"T. Li\"}],\"doi\":\"10.1142/S0218001419510091\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"206fb4055934bdfe31d90b244f69a2d8411023fd\",\"title\":\"An Energy Dynamic Control Algorithm Based on Reinforcement Learning for Data Centers\",\"url\":\"https://www.semanticscholar.org/paper/206fb4055934bdfe31d90b244f69a2d8411023fd\",\"venue\":\"Int. J. Pattern Recognit. Artif. Intell.\",\"year\":2019},{\"arxivId\":\"2006.13823\",\"authors\":[{\"authorId\":\"21479306\",\"name\":\"Hassam Sheikh\"},{\"authorId\":\"1701593\",\"name\":\"Ladislau B\\u00f6l\\u00f6ni\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eaed105b446a8cbcf39d4debfd514cf2df2d753e\",\"title\":\"Reducing Overestimation Bias by Increasing Representation Dissimilarity in Ensemble Based Deep Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/eaed105b446a8cbcf39d4debfd514cf2df2d753e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.14257\",\"authors\":[{\"authorId\":\"145106740\",\"name\":\"Huaqing Xiong\"},{\"authorId\":\"145066457\",\"name\":\"Lin Zhao\"},{\"authorId\":\"145097686\",\"name\":\"Y. Liang\"},{\"authorId\":\"145261268\",\"name\":\"W. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"58438a6f0610667a06b141ff9347cd508587a5dc\",\"title\":\"Finite-Time Analysis for Double Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/58438a6f0610667a06b141ff9347cd508587a5dc\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yishen Wang\"},{\"authorId\":\"2079174\",\"name\":\"Zongzhang Zhang\"}],\"doi\":\"10.1109/ICTAI.2019.00123\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b40ce1997fae293b3307b2b5e0ed8d4c193b7cdc\",\"title\":\"Experience Selection in Multi-agent Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b40ce1997fae293b3307b2b5e0ed8d4c193b7cdc\",\"venue\":\"2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49292299\",\"name\":\"B. Wang\"},{\"authorId\":\"50080205\",\"name\":\"X. Li\"},{\"authorId\":\"1476816055\",\"name\":\"Zhiqiang Gao\"},{\"authorId\":\"144051816\",\"name\":\"Yangjun Zhong\"}],\"doi\":\"10.1109/ACCESS.2020.2977400\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69b3c0cefc8721ced99a45d4e2532a23f2ac9441\",\"title\":\"Risk Aversion Operator for Addressing Maximization Bias in Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/69b3c0cefc8721ced99a45d4e2532a23f2ac9441\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143736907\",\"name\":\"Z. Hu\"},{\"authorId\":\"152771577\",\"name\":\"Yubin Jiang\"},{\"authorId\":\"2702285\",\"name\":\"X. Ling\"},{\"authorId\":\"145014498\",\"name\":\"Q. Liu\"}],\"doi\":\"10.1007/978-3-030-04182-3_49\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b32812b5611921f1b41bb6ff5a2a46c87ec1857\",\"title\":\"Accurate Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/4b32812b5611921f1b41bb6ff5a2a46c87ec1857\",\"venue\":\"ICONIP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1499181461\",\"name\":\"Yan Zheng\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"70882195\",\"name\":\"Zongzhang Zhang\"},{\"authorId\":\"2528357\",\"name\":\"Zhao-Peng Meng\"},{\"authorId\":\"1708208678\",\"name\":\"Xiao-Tian Hao\"}],\"doi\":\"10.1007/s11390-020-9967-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cab9ccf5b316c268e863d1206e90831413629d74\",\"title\":\"Efficient Multiagent Policy Optimization Based on Weighted Estimators in Stochastic Cooperative Environments\",\"url\":\"https://www.semanticscholar.org/paper/cab9ccf5b316c268e863d1206e90831413629d74\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144852495\",\"name\":\"M. He\"},{\"authorId\":\"143992318\",\"name\":\"Hongliang Guo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"deeb074d2362c6d8a6c1cbefcd6aaf5fd952f718\",\"title\":\"Interleaved Q-Learning with Partially Coupled Training Process\",\"url\":\"https://www.semanticscholar.org/paper/deeb074d2362c6d8a6c1cbefcd6aaf5fd952f718\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10343823\",\"name\":\"Ignacio Carlucho\"},{\"authorId\":\"145825862\",\"name\":\"M. D. Paula\"},{\"authorId\":\"145095913\",\"name\":\"G. G. Acosta\"}],\"doi\":\"10.1016/J.ESWA.2019.06.066\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"24f9cec3c63d0a37a709bbccdf55c55a7770614d\",\"title\":\"Double Q-PID algorithm for mobile robot control\",\"url\":\"https://www.semanticscholar.org/paper/24f9cec3c63d0a37a709bbccdf55c55a7770614d\",\"venue\":\"Expert Syst. Appl.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47040114\",\"name\":\"Zhanghua Wu\"},{\"authorId\":\"2079174\",\"name\":\"Zongzhang Zhang\"},{\"authorId\":\"49470221\",\"name\":\"Xiaofang Zhang\"}],\"doi\":\"10.1007/978-3-030-63833-7_51\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1153921eb6602194cab512345b5957be1639a427\",\"title\":\"Recency-Weighted Acceleration for Continuous Control Through Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1153921eb6602194cab512345b5957be1639a427\",\"venue\":\"ICONIP\",\"year\":2020},{\"arxivId\":\"2006.07442\",\"authors\":[{\"authorId\":\"11501567\",\"name\":\"Yunhao Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3d064ac519f9f4a5fbc31187a5126de6fe5b728a\",\"title\":\"Self-Imitation Learning via Generalized Lower Bound Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/3d064ac519f9f4a5fbc31187a5126de6fe5b728a\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2003.09280\",\"authors\":[{\"authorId\":\"104787982\",\"name\":\"A. Cini\"},{\"authorId\":\"1399348619\",\"name\":\"Carlo D'Eramo\"},{\"authorId\":\"144719340\",\"name\":\"J. Peters\"},{\"authorId\":\"1785004\",\"name\":\"C. Alippi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9682587ebbdb10380e4b7ebd65ec5ba25dfa719d\",\"title\":\"Deep Reinforcement Learning with Weighted Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/9682587ebbdb10380e4b7ebd65ec5ba25dfa719d\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":44718857,\"doi\":\"10.24963/ijcai.2017/483\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"3b676810327d119689b5f7c11f433575e3e785fa\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2275756\",\"name\":\"Mykel J. Kochenderfer\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1733356\",\"name\":\"G. Chowdhary\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"},{\"authorId\":\"8755646\",\"name\":\"H. Reynolds\"},{\"authorId\":\"47882530\",\"name\":\"J. Thornton\"},{\"authorId\":\"1403025404\",\"name\":\"P. Torres-Carrasquillo\"},{\"authorId\":\"2877453\",\"name\":\"N. K. Ure\"},{\"authorId\":\"153289126\",\"name\":\"J. Vian\"}],\"doi\":\"10.7551/mitpress/10187.001.0001\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"879de295a03b77403d6d08828f792b2e3be9ff51\",\"title\":\"Decision Making Under Uncertainty: Theory and Application\",\"url\":\"https://www.semanticscholar.org/paper/879de295a03b77403d6d08828f792b2e3be9ff51\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735211\",\"name\":\"D. Lee\"},{\"authorId\":\"1852241\",\"name\":\"W. Powell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e4e1aa9063963b0cfd0f6200b51101174a00c88\",\"title\":\"An Intelligent Battery Controller Using Bias-Corrected Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/4e4e1aa9063963b0cfd0f6200b51101174a00c88\",\"venue\":\"AAAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34740554\",\"name\":\"I. Goodfellow\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"}],\"doi\":\"10.1038/nature14539\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a4cec122a08216fe8a3bc19b22e78fbaea096256\",\"title\":\"Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/a4cec122a08216fe8a3bc19b22e78fbaea096256\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"1766844\",\"name\":\"Eric Wiewiora\"},{\"authorId\":\"152677062\",\"name\":\"J. Langford\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1145/1143844.1143955\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"187f3f984e6f375178f41827ab90c4e748773fa7\",\"title\":\"PAC model-free reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/187f3f984e6f375178f41827ab90c4e748773fa7\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1038/nature14540\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bf807c251962a694499b29938bdc54e716ca2dda\",\"title\":\"Reinforcement learning improves behaviour from evaluative feedback\",\"url\":\"https://www.semanticscholar.org/paper/bf807c251962a694499b29938bdc54e716ca2dda\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"644a079073969a92674f69483c4a85679d066545\",\"title\":\"Double Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/644a079073969a92674f69483c4a85679d066545\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4562073\",\"name\":\"Chris Watkins\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5c8bb027eb65b6d250a22e9b6db22853a552ac81\",\"title\":\"Learning from delayed rewards\",\"url\":\"https://www.semanticscholar.org/paper/5c8bb027eb65b6d250a22e9b6db22853a552ac81\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"65b37ae4010e17d1fa7d53094bb6167caa0eb2f5\",\"title\":\"Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/65b37ae4010e17d1fa7d53094bb6167caa0eb2f5\",\"venue\":\"NIPS\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751167\",\"name\":\"D. Ernst\"},{\"authorId\":\"50206577\",\"name\":\"P. Geurts\"},{\"authorId\":\"1695713\",\"name\":\"L. Wehenkel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"41356b8998dd7ddf89429445320d82a269e3ab14\",\"title\":\"Tree-Based Batch Mode Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/41356b8998dd7ddf89429445320d82a269e3ab14\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786249\",\"name\":\"D. Bertsekas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a82db864e472b5aa6313596ef9919f64e3363b1f\",\"title\":\"Dynamic Programming and Optimal Control\",\"url\":\"https://www.semanticscholar.org/paper/a82db864e472b5aa6313596ef9919f64e3363b1f\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2576957\",\"name\":\"R. Bellman\"}],\"doi\":\"10.2307/1909506\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a6b8a2f80436acdcb201144238c7a49735eb1e9\",\"title\":\"Dynamic programming.\",\"url\":\"https://www.semanticscholar.org/paper/7a6b8a2f80436acdcb201144238c7a49735eb1e9\",\"venue\":\"Science\",\"year\":1966},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1399348619\",\"name\":\"Carlo D'Eramo\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"3428870\",\"name\":\"Alessandro Nuara\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7584cbddff33f557aa05df72dec28fd9ab86f08\",\"title\":\"Estimating Maximum Expected Value through Gaussian Approximation\",\"url\":\"https://www.semanticscholar.org/paper/b7584cbddff33f557aa05df72dec28fd9ab86f08\",\"venue\":\"ICML\",\"year\":2016}],\"title\":\"Weighted Double Q-learning\",\"topics\":[{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"}],\"url\":\"https://www.semanticscholar.org/paper/3b676810327d119689b5f7c11f433575e3e785fa\",\"venue\":\"IJCAI\",\"year\":2017}\n"