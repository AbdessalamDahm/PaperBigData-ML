"{\"abstract\":\"No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent's optimisation, reward corruption can be partially managed under some assumptions.\",\"arxivId\":\"1705.08417\",\"authors\":[{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\",\"url\":\"https://www.semanticscholar.org/author/1868196\"},{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\",\"url\":\"https://www.semanticscholar.org/author/2578985\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\",\"url\":\"https://www.semanticscholar.org/author/1749270\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\",\"url\":\"https://www.semanticscholar.org/author/34313265\"}],\"citationVelocity\":14,\"citations\":[{\"arxivId\":\"1810.10369\",\"authors\":[{\"authorId\":\"2641675\",\"name\":\"Vahid Behzadan\"},{\"authorId\":\"1748235\",\"name\":\"A. Munir\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4cb86d8cf26a3f9949d66e9720bf30e6a47afd18\",\"title\":\"The Faults in Our Pi Stars: Security Issues and Open Challenges in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4cb86d8cf26a3f9949d66e9720bf30e6a47afd18\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1808.05770\",\"authors\":[{\"authorId\":\"144546799\",\"name\":\"Yi Han\"},{\"authorId\":\"1868067\",\"name\":\"Benjamin I. P. Rubinstein\"},{\"authorId\":\"144529106\",\"name\":\"Tamas Abraham\"},{\"authorId\":\"1736599\",\"name\":\"T. Alpcan\"},{\"authorId\":\"1790642\",\"name\":\"O. Vel\"},{\"authorId\":\"144757691\",\"name\":\"S. Erfani\"},{\"authorId\":\"51231282\",\"name\":\"David Hubczenko\"},{\"authorId\":\"1688394\",\"name\":\"C. Leckie\"},{\"authorId\":\"144644538\",\"name\":\"P. Montague\"}],\"doi\":\"10.1007/978-3-030-01554-1_9\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c98073f8882dab163d38d18bcacd9fcfc9ef346c\",\"title\":\"Reinforcement Learning for Autonomous Defence in Software-Defined Networking\",\"url\":\"https://www.semanticscholar.org/paper/c98073f8882dab163d38d18bcacd9fcfc9ef346c\",\"venue\":\"GameSec\",\"year\":2018},{\"arxivId\":\"1805.03359\",\"authors\":[{\"authorId\":\"8365320\",\"name\":\"Joshua Romoff\"},{\"authorId\":\"5618859\",\"name\":\"A. Pich\\u00e9\"},{\"authorId\":\"153322220\",\"name\":\"P. Henderson\"},{\"authorId\":\"1389921282\",\"name\":\"Vincent Fran\\u00e7ois-Lavet\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"bff9c3aa0289c57ef1757e37719d0bf159f3903e\",\"title\":\"Reward Estimation for Variance Reduction in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bff9c3aa0289c57ef1757e37719d0bf159f3903e\",\"venue\":\"CoRL\",\"year\":2018},{\"arxivId\":\"2001.06725\",\"authors\":[{\"authorId\":\"145560498\",\"name\":\"Juan F. Vargas\"},{\"authorId\":\"14444019\",\"name\":\"L. Andjelic\"},{\"authorId\":\"3614493\",\"name\":\"A. Farimani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"438c7a8f3602607db1e68beacbc62f7ebb9c3fed\",\"title\":\"Effects of sparse rewards of different magnitudes in the speed of learning of model-based actor critic methods\",\"url\":\"https://www.semanticscholar.org/paper/438c7a8f3602607db1e68beacbc62f7ebb9c3fed\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.01095\",\"authors\":[{\"authorId\":\"1388301316\",\"name\":\"Jonas Schuett\"}],\"doi\":\"10.2139/ssrn.3453632\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a2ee0d1b20f23079b03a20db9b73a02128e843a6\",\"title\":\"A Legal Definition of AI\",\"url\":\"https://www.semanticscholar.org/paper/a2ee0d1b20f23079b03a20db9b73a02128e843a6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1902.09725\",\"authors\":[{\"authorId\":\"49277224\",\"name\":\"A. Turner\"},{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":\"10.1145/3375627.3375851\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ed0b6aa983df3d6f458a6cb0ef53cb89157c4eb\",\"title\":\"Conservative Agency via Attainable Utility Preservation\",\"url\":\"https://www.semanticscholar.org/paper/9ed0b6aa983df3d6f458a6cb0ef53cb89157c4eb\",\"venue\":\"AIES\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3411127\",\"name\":\"Ritesh Noothigattu\"},{\"authorId\":\"12082007\",\"name\":\"Tom Yan\"},{\"authorId\":\"1689184\",\"name\":\"A. Procaccia\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0da646add1af224d36eb16a63687317db97ec982\",\"title\":\"Inverse Reinforcement Learning From Like-Minded Teachers\",\"url\":\"https://www.semanticscholar.org/paper/0da646add1af224d36eb16a63687317db97ec982\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2757194\",\"name\":\"Mark O. Riedl\"},{\"authorId\":\"35066258\",\"name\":\"B. Harrison\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"789dc83eaf50f5a6c3e52a07e65bd0b3f25d1e48\",\"title\":\"Enter the Matrix: Safely Interruptible Autonomous Systems via Virtualization\",\"url\":\"https://www.semanticscholar.org/paper/789dc83eaf50f5a6c3e52a07e65bd0b3f25d1e48\",\"venue\":\"SafeAI@AAAI\",\"year\":2019},{\"arxivId\":\"1907.00452\",\"authors\":[{\"authorId\":\"31748788\",\"name\":\"J. Mancuso\"},{\"authorId\":\"90480405\",\"name\":\"Tomasz Kisielewski\"},{\"authorId\":\"4829248\",\"name\":\"David Lindner\"},{\"authorId\":\"50286441\",\"name\":\"A. Singh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"dc7b085a257eb6a067db9ccdd7f833e661ddaa35\",\"title\":\"Detecting Spiky Corruption in Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/dc7b085a257eb6a067db9ccdd7f833e661ddaa35\",\"venue\":\"AISafety@IJCAI\",\"year\":2019},{\"arxivId\":\"1803.01164\",\"authors\":[{\"authorId\":\"1932404\",\"name\":\"M. Alom\"},{\"authorId\":\"1799779\",\"name\":\"T. Taha\"},{\"authorId\":\"35991974\",\"name\":\"Christopher Yakopcic\"},{\"authorId\":\"40893684\",\"name\":\"Stefan Westberg\"},{\"authorId\":\"2325550\",\"name\":\"P. Sidike\"},{\"authorId\":\"100898809\",\"name\":\"Mst Shamima Nasrin\"},{\"authorId\":\"40895870\",\"name\":\"Brian C. Van Esesn\"},{\"authorId\":\"2490422\",\"name\":\"A. Awwal\"},{\"authorId\":\"2401900\",\"name\":\"V. Asari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b57e6468740d9320f3f14c6079168b8e21366416\",\"title\":\"The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches\",\"url\":\"https://www.semanticscholar.org/paper/b57e6468740d9320f3f14c6079168b8e21366416\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1802.07228\",\"authors\":[{\"authorId\":\"35167962\",\"name\":\"M. Brundage\"},{\"authorId\":\"10995202\",\"name\":\"S. Avin\"},{\"authorId\":\"145468701\",\"name\":\"J. Clark\"},{\"authorId\":\"48625835\",\"name\":\"Helen Toner\"},{\"authorId\":\"2654106\",\"name\":\"P. Eckersley\"},{\"authorId\":\"39928654\",\"name\":\"B. Garfinkel\"},{\"authorId\":\"3198576\",\"name\":\"A. Dafoe\"},{\"authorId\":\"35681920\",\"name\":\"Paul Scharre\"},{\"authorId\":\"46225273\",\"name\":\"T. Zeitzoff\"},{\"authorId\":\"7888676\",\"name\":\"Bobby Filar\"},{\"authorId\":\"2639880\",\"name\":\"H. Anderson\"},{\"authorId\":\"23619311\",\"name\":\"Heather Roff\"},{\"authorId\":\"145265027\",\"name\":\"G. Allen\"},{\"authorId\":\"5164568\",\"name\":\"J. Steinhardt\"},{\"authorId\":\"152629250\",\"name\":\"C. Flynn\"},{\"authorId\":\"35793299\",\"name\":\"Se\\u00e1n \\u00d3 h\\u00c9igeartaigh\"},{\"authorId\":\"38992229\",\"name\":\"S. Beard\"},{\"authorId\":\"36729401\",\"name\":\"Haydn Belfield\"},{\"authorId\":\"33859827\",\"name\":\"S. Farquhar\"},{\"authorId\":\"39439114\",\"name\":\"Clare Lyle\"},{\"authorId\":\"35431817\",\"name\":\"Rebecca Crootof\"},{\"authorId\":\"47107786\",\"name\":\"Owain Evans\"},{\"authorId\":\"144156311\",\"name\":\"Michael Page\"},{\"authorId\":\"145315448\",\"name\":\"J. Bryson\"},{\"authorId\":\"26336155\",\"name\":\"Roman Yampolskiy\"},{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"}],\"doi\":\"10.17863/CAM.22520\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a4d513cfc9d4902ef1a80198582f29b8ba46ac28\",\"title\":\"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation\",\"url\":\"https://www.semanticscholar.org/paper/a4d513cfc9d4902ef1a80198582f29b8ba46ac28\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2008.04071\",\"authors\":[{\"authorId\":\"1976753\",\"name\":\"Roman V Yampolskiy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"236c00be7fe8e7452d9ad0ff16cb9a36a6bd783f\",\"title\":\"On Controllability of AI\",\"url\":\"https://www.semanticscholar.org/paper/236c00be7fe8e7452d9ad0ff16cb9a36a6bd783f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1811.07871\",\"authors\":[{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"145055042\",\"name\":\"David Krueger\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"51965508\",\"name\":\"Vishal Maini\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6f913e4baa7f2c85363c0625c87003ad3b3a14c\",\"title\":\"Scalable agent alignment via reward modeling: a research direction\",\"url\":\"https://www.semanticscholar.org/paper/c6f913e4baa7f2c85363c0625c87003ad3b3a14c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2003.02740\",\"authors\":[{\"authorId\":\"97685390\",\"name\":\"Yongle Luo\"},{\"authorId\":\"47233551\",\"name\":\"Kun Dong\"},{\"authorId\":\"40579783\",\"name\":\"Li-li Zhao\"},{\"authorId\":\"150358051\",\"name\":\"Zhiyong Sun\"},{\"authorId\":\"144623101\",\"name\":\"Chao Zhou\"},{\"authorId\":\"92255644\",\"name\":\"B. Song\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4bf07dc0d993e11aeabdefba4e39ed4af15410cc\",\"title\":\"Balance Between Efficient and Effective Learning: Dense2Sparse Reward Shaping for Robot Manipulation with Environment Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/4bf07dc0d993e11aeabdefba4e39ed4af15410cc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.11210\",\"authors\":[{\"authorId\":\"50134015\",\"name\":\"Y. Yu\"},{\"authorId\":\"1736584\",\"name\":\"S. Liew\"},{\"authorId\":\"46958862\",\"name\":\"T. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83edc7183a67ed8d0ee49bcdbd2e86f2fa11bb8b\",\"title\":\"Multi-Agent Deep Reinforcement Learning Multiple Access for Heterogeneous Wireless Networks with Imperfect Channels\",\"url\":\"https://www.semanticscholar.org/paper/83edc7183a67ed8d0ee49bcdbd2e86f2fa11bb8b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1652697400\",\"name\":\"Julia Haas\"}],\"doi\":\"10.1007/s11023-020-09524-9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"06106f3488e7d5869b868774895cf07613887312\",\"title\":\"Moral Gridworlds: A Theoretical Proposal for Modeling Artificial Moral Cognition\",\"url\":\"https://www.semanticscholar.org/paper/06106f3488e7d5869b868774895cf07613887312\",\"venue\":\"Minds and Machines\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144710847\",\"name\":\"S. Armstrong\"},{\"authorId\":\"32777162\",\"name\":\"S. Mindermann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"11ca969b9c4f02bda7ce42f1bdfa491be4beceb1\",\"title\":\"Impossibility of deducing preferences and rationality from human policy\",\"url\":\"https://www.semanticscholar.org/paper/11ca969b9c4f02bda7ce42f1bdfa491be4beceb1\",\"venue\":\"NIPS 2018\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3448813\",\"name\":\"P. Mallozzi\"},{\"authorId\":\"144068335\",\"name\":\"R. Pardo\"},{\"authorId\":\"40992651\",\"name\":\"Vincent Duplessis\"},{\"authorId\":\"2909166\",\"name\":\"P. Pelliccione\"},{\"authorId\":\"1760975\",\"name\":\"G. Schneider\"}],\"doi\":\"10.1109/IRC.2018.00053\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1da468f0bc6b9133284fdad5857ecaedd756680\",\"title\":\"MoVEMo: A Structured Approach for Engineering Reward Functions\",\"url\":\"https://www.semanticscholar.org/paper/a1da468f0bc6b9133284fdad5857ecaedd756680\",\"venue\":\"2018 Second IEEE International Conference on Robotic Computing (IRC)\",\"year\":2018},{\"arxivId\":\"1810.01032\",\"authors\":[{\"authorId\":\"71563016\",\"name\":\"Jingkang Wang\"},{\"authorId\":\"1681842\",\"name\":\"Y. Liu\"},{\"authorId\":\"2485552\",\"name\":\"B. Li\"}],\"doi\":\"10.1609/AAAI.V34I04.6086\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"baff7613deb1c84d2570bee2212ebb2391261727\",\"title\":\"Reinforcement Learning with Perturbed Rewards\",\"url\":\"https://www.semanticscholar.org/paper/baff7613deb1c84d2570bee2212ebb2391261727\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31748788\",\"name\":\"J. Mancuso\"},{\"authorId\":\"90480405\",\"name\":\"Tomasz Kisielewski\"},{\"authorId\":\"4829248\",\"name\":\"David Lindner\"},{\"authorId\":\"50286441\",\"name\":\"A. Singh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"625c2f8b7d0a5a1ca76244d4e27c60766ad69b25\",\"title\":\"L G ] 3 0 Ju n 20 19 Detecting Spiky Corruption in Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/625c2f8b7d0a5a1ca76244d4e27c60766ad69b25\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2010.01748\",\"authors\":[{\"authorId\":\"71563016\",\"name\":\"Jingkang Wang\"},{\"authorId\":\"15836216\",\"name\":\"Hong-Yi Guo\"},{\"authorId\":\"49658780\",\"name\":\"Zhaowei Zhu\"},{\"authorId\":\"1614035413\",\"name\":\"Yang Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fae0366593f579f76ebb89439330a12ea9b9f291\",\"title\":\"Policy Learning Using Weak Supervision\",\"url\":\"https://www.semanticscholar.org/paper/fae0366593f579f76ebb89439330a12ea9b9f291\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.13559\",\"authors\":[{\"authorId\":\"2538104\",\"name\":\"M. Mladenov\"},{\"authorId\":\"3174612\",\"name\":\"Ofer Meshi\"},{\"authorId\":\"147126120\",\"name\":\"Jayden Ooi\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":\"10.24963/ijcai.2019/439\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"78824341f657af4fd6d92ebfc080d0c34eec296b\",\"title\":\"Advantage Amplification in Slowly Evolving Latent-State Environments\",\"url\":\"https://www.semanticscholar.org/paper/78824341f657af4fd6d92ebfc080d0c34eec296b\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"97693361\",\"name\":\"Yinlong Yuan\"},{\"authorId\":\"153009573\",\"name\":\"Z. Yu\"},{\"authorId\":\"9238882\",\"name\":\"Z. Gu\"},{\"authorId\":\"49077262\",\"name\":\"Xiaoyan Deng\"},{\"authorId\":\"1791693\",\"name\":\"Y. Li\"}],\"doi\":\"10.1007/s10489-019-01417-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd0e3da7d5dbc60c282545be4c0b37363ea2b6c8\",\"title\":\"A novel multi-step reinforcement learning method for solving reward hacking\",\"url\":\"https://www.semanticscholar.org/paper/fd0e3da7d5dbc60c282545be4c0b37363ea2b6c8\",\"venue\":\"Applied Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2641675\",\"name\":\"Vahid Behzadan\"},{\"authorId\":\"1748235\",\"name\":\"A. Munir\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"66cd16655a5571e9d513bb40e632f47c1567cbe1\",\"title\":\"The Faults in Our \\u03c0 \\u2217 s : Security Issues and Open Challenges in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/66cd16655a5571e9d513bb40e632f47c1567cbe1\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1906.10571\",\"authors\":[{\"authorId\":\"4264478\",\"name\":\"Yunhan Huang\"},{\"authorId\":\"1709793\",\"name\":\"Q. Zhu\"}],\"doi\":\"10.1007/978-3-030-32430-8_14\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"14a74243ce18d36c4ee7460f94b7e1d2b45b8b34\",\"title\":\"Deceptive Reinforcement Learning Under Adversarial Manipulations on Cost Signals\",\"url\":\"https://www.semanticscholar.org/paper/14a74243ce18d36c4ee7460f94b7e1d2b45b8b34\",\"venue\":\"GameSec\",\"year\":2019},{\"arxivId\":\"2011.08820\",\"authors\":[{\"authorId\":\"38912219\",\"name\":\"R. Kumar\"},{\"authorId\":\"9960452\",\"name\":\"Jonathan Uesato\"},{\"authorId\":\"33613300\",\"name\":\"R. Ngo\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c309158b64f8b05e74c8e208b22f8f160c1501d9\",\"title\":\"REALab: An Embedded Perspective on Tampering\",\"url\":\"https://www.semanticscholar.org/paper/c309158b64f8b05e74c8e208b22f8f160c1501d9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145528060\",\"name\":\"J. Turner\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"17a760676293cf740da2e5d2ee09df35b39ee3ba\",\"title\":\"Identifying contemplative intensity in cognitive architectures for virtual-agent minds\",\"url\":\"https://www.semanticscholar.org/paper/17a760676293cf740da2e5d2ee09df35b39ee3ba\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2012.00856\",\"authors\":[{\"authorId\":\"3096139\",\"name\":\"Matt Luckcuck\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"09e71b9906f11aa46c29c98b71d694ffe00b448f\",\"title\":\"Another Tool in the Box: Why use Formal Methods for Autonomous Systems?\",\"url\":\"https://www.semanticscholar.org/paper/09e71b9906f11aa46c29c98b71d694ffe00b448f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1901.11184\",\"authors\":[{\"authorId\":\"2757194\",\"name\":\"Mark O. Riedl\"}],\"doi\":\"10.1002/HBE2.117\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1fccba11583dc9e1030713d61bd65e9e9990e39f\",\"title\":\"Human-Centered Artificial Intelligence and Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/1fccba11583dc9e1030713d61bd65e9e9990e39f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1807.08060\",\"authors\":[{\"authorId\":\"3402361\",\"name\":\"Arushi Jain\"},{\"authorId\":\"38562041\",\"name\":\"Khimya Khetarpal\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"24594fd86012b69078ea7283e1334b05a5ce3afb\",\"title\":\"Safe Option-Critic: Learning Safety in the Option-Critic Architecture\",\"url\":\"https://www.semanticscholar.org/paper/24594fd86012b69078ea7283e1334b05a5ce3afb\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2011.08827\",\"authors\":[{\"authorId\":\"9960452\",\"name\":\"Jonathan Uesato\"},{\"authorId\":\"38912219\",\"name\":\"R. Kumar\"},{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"33613300\",\"name\":\"R. Ngo\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1f8763f2d33fc65e7236b82fe96258c99256b5e8\",\"title\":\"Avoiding Tampering Incentives in Deep RL via Decoupled Approval\",\"url\":\"https://www.semanticscholar.org/paper/1f8763f2d33fc65e7236b82fe96258c99256b5e8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1902.09469\",\"authors\":[{\"authorId\":\"2756886\",\"name\":\"Abram Demski\"},{\"authorId\":\"1740494\",\"name\":\"Scott Garrabrant\"}],\"doi\":\"10.1007/978-1-4614-3858-8_100283\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4d5fde0fac476411bb6249f15c12097ddd84c81f\",\"title\":\"Embedded Agency\",\"url\":\"https://www.semanticscholar.org/paper/4d5fde0fac476411bb6249f15c12097ddd84c81f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8383113\",\"name\":\"R. Carey\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5ea079d76dd5b067cebe8037184acd047cff8344\",\"title\":\"HOW USEFUL IS QUANTILIZATION FOR MITIGATING SPECIFICATION-GAMING?\",\"url\":\"https://www.semanticscholar.org/paper/5ea079d76dd5b067cebe8037184acd047cff8344\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2002.03827\",\"authors\":[{\"authorId\":\"4113230\",\"name\":\"Yun-Han Huang\"},{\"authorId\":\"1709793\",\"name\":\"Q. Zhu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b3d8c8966575ad553ed2c08ac330324c58911557\",\"title\":\"Manipulating Reinforcement Learning: Poisoning Attacks on Cost Signals\",\"url\":\"https://www.semanticscholar.org/paper/b3d8c8966575ad553ed2c08ac330324c58911557\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.09577\",\"authors\":[{\"authorId\":\"1755581\",\"name\":\"Feng Tao\"},{\"authorId\":\"153842938\",\"name\":\"Yongcan Cao\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03b00e54c817ff7175ff5d22c829ed40e8dee1ff\",\"title\":\"Learn to Exceed: Stereo Inverse Reinforcement Learning with Concurrent Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/03b00e54c817ff7175ff5d22c829ed40e8dee1ff\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2680008\",\"name\":\"T. K. Faulkner\"},{\"authorId\":\"32775309\",\"name\":\"Elaine Schaertl Short\"},{\"authorId\":\"1682788\",\"name\":\"A. Thomaz\"}],\"doi\":\"10.1109/ICRA40945.2020.9197219\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"57de62ff4f10aa33383ae61153af9fc777869161\",\"title\":\"Interactive Reinforcement Learning with Inaccurate Feedback\",\"url\":\"https://www.semanticscholar.org/paper/57de62ff4f10aa33383ae61153af9fc777869161\",\"venue\":\"2020 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2020},{\"arxivId\":\"2006.16785\",\"authors\":[{\"authorId\":\"51430494\",\"name\":\"L. Blond\\u00e9\"},{\"authorId\":\"152952322\",\"name\":\"Pablo Strasser\"},{\"authorId\":\"1784711\",\"name\":\"Alexandros Kalousis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e10e92fb9f96d32fdac055af29217f5f87ebc3ff\",\"title\":\"Lipschitzness Is All You Need To Tame Off-policy Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/e10e92fb9f96d32fdac055af29217f5f87ebc3ff\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1712.05812\",\"authors\":[{\"authorId\":\"144710847\",\"name\":\"S. Armstrong\"},{\"authorId\":\"32777162\",\"name\":\"S. Mindermann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cdabff298e9467de9babf0baad04c19789cde44b\",\"title\":\"Occam's razor is insufficient to infer the preferences of irrational agents\",\"url\":\"https://www.semanticscholar.org/paper/cdabff298e9467de9babf0baad04c19789cde44b\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1573823921\",\"name\":\"Hamon Ronan\"},{\"authorId\":\"93875904\",\"name\":\"J. Henrik\"},{\"authorId\":\"74459892\",\"name\":\"Sanchez Martin Jose Ignacio\"}],\"doi\":\"10.2760/57493\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"494e89a013256934470c53145c2ad84b3d752280\",\"title\":\"Robustness and Explainability of Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/494e89a013256934470c53145c2ad84b3d752280\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.06547\",\"authors\":[{\"authorId\":\"49277224\",\"name\":\"A. Turner\"},{\"authorId\":\"13002147\",\"name\":\"Neale Ratzlaff\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6f028e8611417d813ed7939f2434e3fb8c1d641\",\"title\":\"Avoiding Side Effects in Complex Environments\",\"url\":\"https://www.semanticscholar.org/paper/b6f028e8611417d813ed7939f2434e3fb8c1d641\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3448813\",\"name\":\"P. Mallozzi\"},{\"authorId\":\"2909166\",\"name\":\"P. Pelliccione\"},{\"authorId\":\"144341567\",\"name\":\"C. Menghi\"}],\"doi\":\"10.1145/3195555.3195558\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5631db30f6f93700ad0d5904768c55e2e4c69f63\",\"title\":\"Keeping Intelligence under Control\",\"url\":\"https://www.semanticscholar.org/paper/5631db30f6f93700ad0d5904768c55e2e4c69f63\",\"venue\":\"2018 IEEE/ACM 1st International Workshop on Software Engineering for Cognitive Services (SE4COG)\",\"year\":2018},{\"arxivId\":\"1912.05109\",\"authors\":[{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"1411436226\",\"name\":\"Raihan Seraj\"},{\"authorId\":\"30625371\",\"name\":\"Samin Yeasar Arnob\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"7a7e1f9bdb51dd86fa474a4e722cb5efc3deb741\",\"title\":\"Doubly Robust Off-Policy Actor-Critic Algorithms for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7a7e1f9bdb51dd86fa474a4e722cb5efc3deb741\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29356191\",\"name\":\"Kanata Suzuki\"},{\"authorId\":\"50527812\",\"name\":\"Tetsuya Ogata\"}],\"doi\":\"10.1007/978-3-030-63833-7_55\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3fd3491ae03ef735b236546c77fca947e0cc0c13\",\"title\":\"Stable Deep Reinforcement Learning Method by Predicting Uncertainty in Rewards as a Subtask\",\"url\":\"https://www.semanticscholar.org/paper/3fd3491ae03ef735b236546c77fca947e0cc0c13\",\"venue\":\"ICONIP\",\"year\":2020}],\"corpusId\":3075935,\"doi\":\"10.24963/ijcai.2017/656\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"e5ba74bd3b6c9bb0287d8835621ddd40dd3ebbf4\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1007/b138233\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f47655fe96e155cfc29860331681e65377ae467a\",\"title\":\"Universal Artificial Intellegence - Sequential Decisions Based on Algorithmic Probability\",\"url\":\"https://www.semanticscholar.org/paper/f47655fe96e155cfc29860331681e65377ae467a\",\"venue\":\"Texts in Theoretical Computer Science. An EATCS Series\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145155782\",\"name\":\"C. Robert\"}],\"doi\":\"10.1080/09332480.2017.1302723\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e0fe9b2f77288bc5e6f778611a49e62e98231f8c\",\"title\":\"Superintelligence: Paths, Dangers, Strategies\",\"url\":\"https://www.semanticscholar.org/paper/e0fe9b2f77288bc5e6f778611a49e62e98231f8c\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1606.03137\",\"authors\":[{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777\",\"title\":\"Cooperative Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e6abd43fcb157fde4d4ddc3ac8787ae45dbf777\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1696678\",\"name\":\"D. Wolpert\"},{\"authorId\":\"2957256\",\"name\":\"W. Macready\"}],\"doi\":\"10.1109/4235.585893\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"8315dff3d304baf47c025f4b33535b9d693350c1\",\"title\":\"No free lunch theorems for optimization\",\"url\":\"https://www.semanticscholar.org/paper/8315dff3d304baf47c025f4b33535b9d693350c1\",\"venue\":\"IEEE Trans. Evol. Comput.\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1976753\",\"name\":\"Roman V Yampolskiy\"}],\"doi\":\"10.1080/0952813X.2014.895114\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9292afd24ff65d4c0368fa9529e08dbf5c0a356b\",\"title\":\"Utility function security in artificially intelligent agents\",\"url\":\"https://www.semanticscholar.org/paper/9292afd24ff65d4c0368fa9529e08dbf5c0a356b\",\"venue\":\"J. Exp. Theor. Artif. Intell.\",\"year\":2014},{\"arxivId\":\"1705.10557\",\"authors\":[{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.24963/ijcai.2017/194\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1627e5dd7c6656aa8c16d861677ac631c5c4301\",\"title\":\"Universal Reinforcement Learning Algorithms: Survey and Experiments\",\"url\":\"https://www.semanticscholar.org/paper/d1627e5dd7c6656aa8c16d861677ac631c5c4301\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144364160\",\"name\":\"Jessica Taylor\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4e8ff3b4069a12a00196d62925bab8add7389742\",\"title\":\"Quantilizers: A Safer Alternative to Maximizers for Limited Optimization\",\"url\":\"https://www.semanticscholar.org/paper/4e8ff3b4069a12a00196d62925bab8add7389742\",\"venue\":\"AAAI Workshop: AI, Ethics, and Society\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50652024\",\"name\":\"Ming Li\"},{\"authorId\":\"46641506\",\"name\":\"P. Vit\\u00e1nyi\"}],\"doi\":\"10.1016/0020-0190(92)90138-L\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c491d521ece54bc174f4eb7199065fe01d516866\",\"title\":\"Average Case Complexity Under the Universal Distribution Equals Worst-Case Complexity\",\"url\":\"https://www.semanticscholar.org/paper/c491d521ece54bc174f4eb7199065fe01d516866\",\"venue\":\"Inf. Process. Lett.\",\"year\":1992},{\"arxivId\":\"1611.08219\",\"authors\":[{\"authorId\":\"1397904824\",\"name\":\"Dylan Hadfield-Menell\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":\"10.24963/ijcai.2017/32\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"808dec0828a74fecab07a497c10cd93e3748a5e2\",\"title\":\"The Off-Switch Game\",\"url\":\"https://www.semanticscholar.org/paper/808dec0828a74fecab07a497c10cd93e3748a5e2\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3113049\",\"name\":\"T. Jaksch\"},{\"authorId\":\"1786887\",\"name\":\"R. Ortner\"},{\"authorId\":\"144543541\",\"name\":\"P. Auer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cafe2903b097fc042782c359cb231ea34ef7ed3\",\"title\":\"Near-optimal Regret Bounds for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0cafe2903b097fc042782c359cb231ea34ef7ed3\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5881933\",\"name\":\"P. W. Jones\"}],\"doi\":\"10.1057/JORS.1987.129\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d4690cd528cae72cc9afbca0506ca595dad9434b\",\"title\":\"Bandit Problems, Sequential Allocation of Experiments\",\"url\":\"https://www.semanticscholar.org/paper/d4690cd528cae72cc9afbca0506ca595dad9434b\",\"venue\":\"\",\"year\":1987},{\"arxivId\":\"1512.05832\",\"authors\":[{\"authorId\":\"47107786\",\"name\":\"Owain Evans\"},{\"authorId\":\"2214496\",\"name\":\"Andreas Stuhlm\\u00fcller\"},{\"authorId\":\"144002017\",\"name\":\"Noah D. Goodman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5260a706305c59c0fa2981bcdd86280c3c4a16b1\",\"title\":\"Learning the Preferences of Ignorant, Inconsistent Agents\",\"url\":\"https://www.semanticscholar.org/paper/5260a706305c59c0fa2981bcdd86280c3c4a16b1\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Aslanides\"},{\"authorId\":null,\"name\":\"Jan Leike\"},{\"authorId\":null,\"name\":\"Marcus Hutter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Universal reinforcement learning algorithms: Survey\",\"url\":\"\",\"venue\":\"Problems in AI Safety. CoRR,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"2453007\",\"name\":\"A. Cassandra\"}],\"doi\":\"10.1016/S0004-3702(98)00023-X\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"116d7798c1123cf7fad4176e98f58fd49de4f8f1\",\"title\":\"Planning and Acting in Partially Observable Stochastic Domains\",\"url\":\"https://www.semanticscholar.org/paper/116d7798c1123cf7fad4176e98f58fd49de4f8f1\",\"venue\":\"Artif. Intell.\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dario Amodei\"},{\"authorId\":null,\"name\":\"Jack Clark\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Faulty Reward Functions in the Wild. https://openai.com/blog/ faulty-reward-functions/, 2016\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"11023955\",\"name\":\"Mark B. Ring\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"}],\"doi\":\"10.1007/978-3-642-22887-2_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d8a200f7e0ef5659d544b4f5306251deaf781d1b\",\"title\":\"Delusion, Survival, and Intelligent Agents\",\"url\":\"https://www.semanticscholar.org/paper/d8a200f7e0ef5659d544b4f5306251deaf781d1b\",\"venue\":\"AGI\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1162/neco.1994.6.6.1185\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2cca494cd58f483547a4bd059b319a915e5751bc\",\"title\":\"On the Convergence of Stochastic Iterative Dynamic Programming Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/2cca494cd58f483547a4bd059b319a915e5751bc\",\"venue\":\"Neural Computation\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2757194\",\"name\":\"Mark O. Riedl\"},{\"authorId\":\"35066258\",\"name\":\"B. Harrison\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33b53abdf2824b2cb0ee083c284000df4343a33e\",\"title\":\"Using Stories to Teach Human Values to Artificial Agents\",\"url\":\"https://www.semanticscholar.org/paper/33b53abdf2824b2cb0ee083c284000df4343a33e\",\"venue\":\"AAAI Workshop: AI, Ethics, and Society\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"2016. Dylan Hadfield-Menell\"},{\"authorId\":null,\"name\":\"Anca Dragan\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Stuart Russell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cooperative Inverse Reinforcement\",\"url\":\"\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tom Everitt\"},{\"authorId\":null,\"name\":\"Victoria Krakovna\"},{\"authorId\":null,\"name\":\"Laurent Orseau\"},{\"authorId\":null,\"name\":\"Marcus Hutter\"},{\"authorId\":null,\"name\":\"Shane Legg\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Reinforcement Learning with Corrupted Reward Channel\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Aslanides\"},{\"authorId\":null,\"name\":\"Jan Leike\"},{\"authorId\":null,\"name\":\"Marcus Hutter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"General reinforcement learning algorithms: Survey\",\"url\":\"\",\"venue\":\"Problems in AI Safety. CoRR,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":\"10.2460/AJVR.67.2.323\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b05b67aca720d0bc39bc9afad02a19f522c7a1bc\",\"title\":\"Algorithms for Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b05b67aca720d0bc39bc9afad02a19f522c7a1bc\",\"venue\":\"ICML\",\"year\":2000}],\"title\":\"Reinforcement Learning with a Corrupted Reward Channel\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Software bug\",\"topicId\":\"37\",\"url\":\"https://www.semanticscholar.org/topic/37\"},{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Stationary process\",\"topicId\":\"21296\",\"url\":\"https://www.semanticscholar.org/topic/21296\"},{\"topic\":\"Randomness\",\"topicId\":\"726\",\"url\":\"https://www.semanticscholar.org/topic/726\"},{\"topic\":\"Fwupd\",\"topicId\":\"1689253\",\"url\":\"https://www.semanticscholar.org/topic/1689253\"},{\"topic\":\"Decision problem\",\"topicId\":\"15220\",\"url\":\"https://www.semanticscholar.org/topic/15220\"},{\"topic\":\"Stemming\",\"topicId\":\"4937\",\"url\":\"https://www.semanticscholar.org/topic/4937\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Regret (decision theory)\",\"topicId\":\"528786\",\"url\":\"https://www.semanticscholar.org/topic/528786\"},{\"topic\":\"Formal language\",\"topicId\":\"43444\",\"url\":\"https://www.semanticscholar.org/topic/43444\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"},{\"topic\":\"RL (complexity)\",\"topicId\":\"3597734\",\"url\":\"https://www.semanticscholar.org/topic/3597734\"},{\"topic\":\"Intelligent agent\",\"topicId\":\"18636\",\"url\":\"https://www.semanticscholar.org/topic/18636\"},{\"topic\":\"Heuristic\",\"topicId\":\"4146\",\"url\":\"https://www.semanticscholar.org/topic/4146\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"},{\"topic\":\"State space\",\"topicId\":\"6115\",\"url\":\"https://www.semanticscholar.org/topic/6115\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Semi-supervised learning\",\"topicId\":\"254497\",\"url\":\"https://www.semanticscholar.org/topic/254497\"},{\"topic\":\"Simulation hypothesis\",\"topicId\":\"1581388\",\"url\":\"https://www.semanticscholar.org/topic/1581388\"},{\"topic\":\"Semiconductor industry\",\"topicId\":\"76540\",\"url\":\"https://www.semanticscholar.org/topic/76540\"},{\"topic\":\"Andrew Barto\",\"topicId\":\"1221498\",\"url\":\"https://www.semanticscholar.org/topic/1221498\"},{\"topic\":\"Nico Habermann\",\"topicId\":\"1690688\",\"url\":\"https://www.semanticscholar.org/topic/1690688\"},{\"topic\":\"Arc (programming language)\",\"topicId\":\"2756459\",\"url\":\"https://www.semanticscholar.org/topic/2756459\"},{\"topic\":\"Memory corruption\",\"topicId\":\"120185\",\"url\":\"https://www.semanticscholar.org/topic/120185\"},{\"topic\":\"Jan Bergstra\",\"topicId\":\"617308\",\"url\":\"https://www.semanticscholar.org/topic/617308\"}],\"url\":\"https://www.semanticscholar.org/paper/e5ba74bd3b6c9bb0287d8835621ddd40dd3ebbf4\",\"venue\":\"IJCAI\",\"year\":2017}\n"