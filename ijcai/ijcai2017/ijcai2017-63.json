"{\"abstract\":\"We propose a novel baseline regret minimization algorithm for multi-agent planning problems modeled as finite-horizon decentralized POMDPs. It guarantees to produce a policy that is provably at least as good as a given baseline policy. We also propose an iterative belief generation algorithm to efficiently minimize the baseline regret, which only requires necessary iterations so as to converge to the policy with minimum baseline regret. Experimental results on common benchmark problems confirm the benefits of the algorithm compared with the state-of-the-art approaches.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\",\"url\":\"https://www.semanticscholar.org/author/144864333\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\",\"url\":\"https://www.semanticscholar.org/author/1707550\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\",\"url\":\"https://www.semanticscholar.org/author/27054809\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1911.07712\",\"authors\":[{\"authorId\":\"9384558\",\"name\":\"Runsheng Yu\"},{\"authorId\":\"46451107\",\"name\":\"Zhenyu Shi\"},{\"authorId\":\"10737491\",\"name\":\"Xinrun Wang\"},{\"authorId\":\"103856029\",\"name\":\"R. Wang\"},{\"authorId\":\"1417262808\",\"name\":\"Buhong Liu\"},{\"authorId\":\"1761961\",\"name\":\"X. Hou\"},{\"authorId\":\"2356867\",\"name\":\"Hanjiang Lai\"},{\"authorId\":\"143706343\",\"name\":\"Bo An\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"409f9b459d5906282d524a8c3d1cc947ed2a8b9d\",\"title\":\"Inducing Cooperation via Team Regret Minimization based Multi-Agent Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/409f9b459d5906282d524a8c3d1cc947ed2a8b9d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cee27111fdd93f27958279bb1065efd457b3292d\",\"title\":\"Privacy-Preserving Policy Iteration for Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/cee27111fdd93f27958279bb1065efd457b3292d\",\"venue\":\"AAAI\",\"year\":2018}],\"corpusId\":13028578,\"doi\":\"10.24963/ijcai.2017/63\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"a96e42377fea44b604b61bb24d07fad51a21207e\",\"references\":[{\"arxivId\":\"1309.1973\",\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"144626042\",\"name\":\"N. Jennings\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fa910371d9581c3ce297e1482e99f33f926b81e2\",\"title\":\"Regret-Based Multi-Agent Coordination with Uncertain Task Rewards\",\"url\":\"https://www.semanticscholar.org/paper/fa910371d9581c3ce297e1482e99f33f926b81e2\",\"venue\":\"AAAI\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bikramjit Banerjee\"},{\"authorId\":null,\"name\":\"Landon Kraemer. Counterfactual regret minimization for de planning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Workshop on Multiagent Sequential Decision Making\",\"url\":\"\",\"venue\":\"pages 32\\u201339,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143719795\",\"name\":\"Huan Xu\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":\"10.1109/CDC.2009.5400796\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1075d2bc76922c8a72a47d33cd836e89cfded95\",\"title\":\"Parametric regret in uncertain Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/a1075d2bc76922c8a72a47d33cd836e89cfded95\",\"venue\":\"Proceedings of the 48h IEEE Conference on Decision and Control (CDC) held jointly with 2009 28th Chinese Control Conference\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Frans A. Oliehoek. Sufficient plan-time statistics for d Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of IJCAI\",\"url\":\"\",\"venue\":\"pages 302\\u2013308,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"},{\"authorId\":\"1681530\",\"name\":\"Michael Bradley Johanson\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"},{\"authorId\":\"3248202\",\"name\":\"C. Piccione\"}],\"doi\":\"10.7939/R3Q23R282\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db20a76a702be3ad4e7cebf7eeb7f1898827cebd\",\"title\":\"Regret Minimization in Games with Incomplete Information\",\"url\":\"https://www.semanticscholar.org/paper/db20a76a702be3ad4e7cebf7eeb7f1898827cebd\",\"venue\":\"NIPS\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Feng Wu\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein\"},{\"authorId\":null,\"name\":\"Xiaoping Chen. Trial-based dynamic programming for multiage Proc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAAI\",\"url\":\"\",\"venue\":\"pages 908\\u2013914,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":\"10.1016/j.artint.2010.09.008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d29d50b5d6a41873b15c8fa92178a03e79e5ecaa\",\"title\":\"Online planning for multi-agent systems with bounded communication\",\"url\":\"https://www.semanticscholar.org/paper/d29d50b5d6a41873b15c8fa92178a03e79e5ecaa\",\"venue\":\"Artif. Intell.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145902797\",\"name\":\"B. Banerjee\"},{\"authorId\":\"47072824\",\"name\":\"Landon Kraemer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"31c417bec4494f8fec748e021604c3c2148ab834\",\"title\":\"Counterfactual Regret Minimization for Decentralized Planning\",\"url\":\"https://www.semanticscholar.org/paper/31c417bec4494f8fec748e021604c3c2148ab834\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f4f6c7ce2b1977734fa9607b2f7b32c546c1db69\",\"title\":\"Planning with macro-actions in decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/f4f6c7ce2b1977734fa9607b2f7b32c546c1db69\",\"venue\":\"AAMAS\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martin Zinkevich\"},{\"authorId\":null,\"name\":\"H. Michael\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Para - metric regret in uncertain Markov decision processes\",\"url\":\"\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"George D. Konidaris\"},{\"authorId\":null,\"name\":\"P. Leslie\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kaelbling . Planning with macroactions in decentralized POMDPs\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144389412\",\"name\":\"Kevin Regan\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58e69a23f1d8a587923415f5a270b893c6e74c7e\",\"title\":\"Robust Policy Computation in Reward-Uncertain MDPs Using Nondominated Policies\",\"url\":\"https://www.semanticscholar.org/paper/58e69a23f1d8a587923415f5a270b893c6e74c7e\",\"venue\":\"AAAI\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Asrar Ahmed\"},{\"authorId\":null,\"name\":\"Pradeep Varakantham\"},{\"authorId\":null,\"name\":\"Yossiri Adulyasak\"},{\"authorId\":null,\"name\":\"Patrick Jaillet. Regret based robust solutions for uncerta Proc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of NIPS\",\"url\":\"\",\"venue\":\"pages 881\\u2013889,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"George D. Konidaris\"},{\"authorId\":null,\"name\":\"P. Leslie\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kaelbling . Planning with macroactions in decentralized POMDPs\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Frans A. Oliehoek\"},{\"authorId\":null,\"name\":\"Matthijs T.J. Spaan\"},{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"Shimon Whiteson. Incremental clustering\"},{\"authorId\":null,\"name\":\"expansion for faster optimal planning in Dec-POMDPs\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"46:449\\u2013509,\",\"year\":2013},{\"arxivId\":\"1402.0566\",\"authors\":[{\"authorId\":\"1799949\",\"name\":\"Frans A. Oliehoek\"},{\"authorId\":\"1723205\",\"name\":\"M. Spaan\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":\"10.1613/jair.3804\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"afa430e0e1f76e52c01c3af94f420eb74b639a64\",\"title\":\"Incremental Clustering and Expansion for Faster Optimal Planning in Dec-POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/afa430e0e1f76e52c01c3af94f420eb74b639a64\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144497046\",\"name\":\"N. Nilsson\"}],\"doi\":\"10.7551/mitpress/11723.003.0006\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b886f2c097b635ee9550ca29fff7dcbbb7727ff7\",\"title\":\"Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b886f2c097b635ee9550ca29fff7dcbbb7727ff7\",\"venue\":\"IFIP Congress\",\"year\":1974},{\"arxivId\":\"1207.1359\",\"authors\":[{\"authorId\":\"1809352\",\"name\":\"D. Szer\"},{\"authorId\":\"1731714\",\"name\":\"F. Charpillet\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c5860809315a4c18abeac7ba95652d642e15fea\",\"title\":\"MAA*: A Heuristic Search Algorithm for Solving Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/6c5860809315a4c18abeac7ba95652d642e15fea\",\"venue\":\"UAI\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marek Petrik\"},{\"authorId\":null,\"name\":\"Yinlam Chow\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh. Safe policy improvement by minimizing Proc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of NIPS\",\"url\":\"\",\"venue\":\"pages 2298\\u20132306,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Frans\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Incre - mental clustering and expansion for faster optimal planning in DecPOMDPs\",\"url\":\"\",\"venue\":\"Journal of Artificial Intelligence Research\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40305195\",\"name\":\"Akshat Kumar\"},{\"authorId\":\"1889251\",\"name\":\"H. Mostafa\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aebec5faa3bac9daadfa5bb849aa61d57ca1ca39\",\"title\":\"Dual Formulations for Optimizing Dec-POMDP Controllers\",\"url\":\"https://www.semanticscholar.org/paper/aebec5faa3bac9daadfa5bb849aa61d57ca1ca39\",\"venue\":\"ICAPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1682133\",\"name\":\"J. Dibangoye\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1776632\",\"name\":\"O. Buffet\"},{\"authorId\":\"1731714\",\"name\":\"F. Charpillet\"}],\"doi\":\"10.1613/jair.4623\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d70a1eb73176741007a427c64cd711aea3fbce9f\",\"title\":\"Optimally Solving Dec-POMDPs as Continuous-State MDPs\",\"url\":\"https://www.semanticscholar.org/paper/d70a1eb73176741007a427c64cd711aea3fbce9f\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Feng Wu\"},{\"authorId\":null,\"name\":\"Nicholas R. Jennings. Regret-based multi-agent coordination Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAAI\",\"url\":\"\",\"venue\":\"pages 1492\\u20131499,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kevin Regan\"},{\"authorId\":null,\"name\":\"Craig Boutilier. Robust policy computation in reward-unc Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAAI\",\"url\":\"\",\"venue\":\"pages 1127\\u20131133,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39891551\",\"name\":\"A. Ahmed\"},{\"authorId\":\"1718824\",\"name\":\"Pradeep Varakantham\"},{\"authorId\":\"2070610\",\"name\":\"Yossiri Adulyasak\"},{\"authorId\":\"1805753\",\"name\":\"Patrick Jaillet\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fb23ab605163534912fd7da535c275aac2897134\",\"title\":\"Regret based Robust Solutions for Uncertain Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/fb23ab605163534912fd7da535c275aac2897134\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1682133\",\"name\":\"J. Dibangoye\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd0a295e4b2d91162c0b8986b4b13d20568206e7\",\"title\":\"Incremental Policy Generation for Finite-Horizon DEC-POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/dd0a295e4b2d91162c0b8986b4b13d20568206e7\",\"venue\":\"ICAPS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40305195\",\"name\":\"Akshat Kumar\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":\"10.1145/1838206.1838378\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4a1230176be61a0fe65843dea82d54a4ff35764\",\"title\":\"Point-based backup for decentralized POMDPs: complexity and new algorithms\",\"url\":\"https://www.semanticscholar.org/paper/b4a1230176be61a0fe65843dea82d54a4ff35764\",\"venue\":\"AAMAS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1697031\",\"name\":\"E. A. Hansen\"},{\"authorId\":\"35176432\",\"name\":\"D. Bernstein\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9764ed9cf14b439235987dfe65d35bb6ce406ef\",\"title\":\"Dynamic Programming for Partially Observable Stochastic Games\",\"url\":\"https://www.semanticscholar.org/paper/b9764ed9cf14b439235987dfe65d35bb6ce406ef\",\"venue\":\"AAAI\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152175853\",\"name\":\"L. Goddard\"}],\"doi\":\"10.1038/222304c0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60ec5ddae05190537c72974b8f93beb46b8db857\",\"title\":\"Operations Research\",\"url\":\"https://www.semanticscholar.org/paper/60ec5ddae05190537c72974b8f93beb46b8db857\",\"venue\":\"Nature\",\"year\":1969},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eric A. Hansen\"},{\"authorId\":null,\"name\":\"Daniel S. Bernstein\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Optimally solving DecPOMDPs as continuousstate MDPs\",\"url\":\"\",\"venue\":\"Journal of Artificial Intelligence Research\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1756878\",\"name\":\"Zinovi Rabinovich\"},{\"authorId\":\"5006412\",\"name\":\"C. Goldman\"},{\"authorId\":\"1735970\",\"name\":\"J. S. Rosenschein\"}],\"doi\":\"10.1145/860575.860816\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a3cd1be403621b5fb188ea102097b95f28e9dbe1\",\"title\":\"The complexity of multiagent systems: the price of silence\",\"url\":\"https://www.semanticscholar.org/paper/a3cd1be403621b5fb188ea102097b95f28e9dbe1\",\"venue\":\"AAMAS '03\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Feng Wu\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein\"},{\"authorId\":null,\"name\":\"Xiaoping Chen. Rollout sampling policy iteration for decent POMDPs\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proc of UAI\",\"url\":\"\",\"venue\":\"pages 666\\u2013673,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Akshat Kumar\"},{\"authorId\":null,\"name\":\"Hala Mostafa\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein. Dual formulations for optimizing Dec- Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of ICAPS\",\"url\":\"\",\"venue\":\"pages 202\\u2013 210,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"144626042\",\"name\":\"N. Jennings\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":\"10.3233/978-1-61499-098-7-858\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b245f177950c6658c9030bd81ae92642c98a6687\",\"title\":\"Sample-Based Policy Iteration for Constrained DEC-POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/b245f177950c6658c9030bd81ae92642c98a6687\",\"venue\":\"ECAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Raghav Aras\"},{\"authorId\":null,\"name\":\"Alain Dutech. An investigation into mathematical program POMDPs\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"37(1):329\\u2013396,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sven Seuken\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein. Memory-bounded dynamic programming fo Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of IJCAI\",\"url\":\"\",\"venue\":\"pages 2009\\u20132015,\",\"year\":2007},{\"arxivId\":\"1301.3836\",\"authors\":[{\"authorId\":\"35176432\",\"name\":\"D. Bernstein\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"1808597\",\"name\":\"N. Immerman\"}],\"doi\":\"10.1287/MOOR.27.4.819.297\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"07d88404f24d61a8ea41ee7f688f57ee8f44ac12\",\"title\":\"The Complexity of Decentralized Control of Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/07d88404f24d61a8ea41ee7f688f57ee8f44ac12\",\"venue\":\"UAI\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eric A. Hansen\"},{\"authorId\":null,\"name\":\"Daniel S. Bernstein\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein. Dynamic programming for partially obs Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAAI\",\"url\":\"\",\"venue\":\"pages 709\\u2013715,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marek Petrik\"},{\"authorId\":null,\"name\":\"Yinlam Chow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Pajarinen and Jaakko Peltonen . Periodic finite state controllers for efficient POMDP and DECPOMDP planning\",\"url\":\"\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":\"10.1145/1838206.1838377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"513983409194130dec9f6356044edd992465f4c8\",\"title\":\"Point-based policy generation for decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/513983409194130dec9f6356044edd992465f4c8\",\"venue\":\"AAMAS\",\"year\":2010},{\"arxivId\":\"1607.03842\",\"authors\":[{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"145630605\",\"name\":\"M. Petrik\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"title\":\"Safe Policy Improvement by Minimizing Robust Baseline Regret\",\"url\":\"https://www.semanticscholar.org/paper/bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chistopher Amato\"},{\"authorId\":null,\"name\":\"Jilles S. Dibangoye\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein. Incremental policy generation for fin Proc\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"of ICAPS\",\"url\":\"\",\"venue\":\"pages 2\\u20139,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martin Zinkevich\"},{\"authorId\":null,\"name\":\"Michael Johanson\"},{\"authorId\":null,\"name\":\"Michael H. Bowling\"},{\"authorId\":null,\"name\":\"Carmelo Piccione. Regret minimization in games with incomp Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of NIPS\",\"url\":\"\",\"venue\":\"pages 1729\\u20131736,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joni K. Pajarinen\"},{\"authorId\":null,\"name\":\"Jaakko Peltonen. Periodic finite state controllers for ef POMDP\"},{\"authorId\":null,\"name\":\"DEC-POMDP planning. In Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of NIPS\",\"url\":\"\",\"venue\":\"pages 2636\\u20132644,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Huan Xu\"},{\"authorId\":null,\"name\":\"Shie Mannor. Parametric regret in uncertain Markov deci Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of CDC\",\"url\":\"\",\"venue\":\"pages 3606\\u20133613,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jilles S. Dibangoye\"},{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"Olivier Buffet\"},{\"authorId\":null,\"name\":\"Fran\\u00e7ois Charpillet. Optimally solving Dec-POMDPs as contin MDPs\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"55:443\\u2013497,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"K. Joni\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Oliehoek . Sufficient plantime statistics for decentralized POMDPs\",\"url\":\"\",\"venue\":\"Proc . of IJCAI\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1884044\",\"name\":\"Sven Seuken\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f76953478b42d79a68c7af6c2737167fdd18c0c\",\"title\":\"Memory-Bounded Dynamic Programming for DEC-POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/1f76953478b42d79a68c7af6c2737167fdd18c0c\",\"venue\":\"IJCAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b95898945c529e00af47d886fda475ec4654c0d\",\"title\":\"Trial-Based Dynamic Programming for Multi-Agent Planning\",\"url\":\"https://www.semanticscholar.org/paper/3b95898945c529e00af47d886fda475ec4654c0d\",\"venue\":\"AAAI\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"144626042\",\"name\":\"N. Jennings\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e0bc5778eb961cfb48c3cf8deda0bda43425c02\",\"title\":\"Monte-Carlo Expectation Maximization for Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/2e0bc5778eb961cfb48c3cf8deda0bda43425c02\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"George D. Konidaris\"},{\"authorId\":null,\"name\":\"Leslie P. Kaelbling. Planning with macroactions in decent Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAMAS\",\"url\":\"\",\"venue\":\"pages 1273\\u20131280,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Feng Wu\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein\"},{\"authorId\":null,\"name\":\"Xiaoping Chen. Point-based policy generation for decentrali Proc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of AAMAS\",\"url\":\"\",\"venue\":\"pages 1307\\u20131314,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Feng Wu\"},{\"authorId\":null,\"name\":\"Shlomo Zilberstein\"},{\"authorId\":null,\"name\":\"Nicholas R. Jennings. Monte-carlo expectation maximization Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of IJCAI\",\"url\":\"\",\"venue\":\"pages 397\\u2013403,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Feng Wu\"},{\"authorId\":null,\"name\":\"Nicholas R. Jennings\"},{\"authorId\":null,\"name\":\"Xiaoping Chen. Sample-based policy iteration for constraine Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"of ECAI\",\"url\":\"\",\"venue\":\"pages 858\\u2013863,\",\"year\":2012},{\"arxivId\":\"1203.3528\",\"authors\":[{\"authorId\":\"144864333\",\"name\":\"Feng Wu\"},{\"authorId\":\"1707550\",\"name\":\"S. Zilberstein\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8dc91cc1931ae67fc59c54e61802e930c338b997\",\"title\":\"Rollout Sampling Policy Iteration for Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/8dc91cc1931ae67fc59c54e61802e930c338b997\",\"venue\":\"UAI\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2691283\",\"name\":\"Ranjit Nair\"},{\"authorId\":\"143736701\",\"name\":\"Milind Tambe\"},{\"authorId\":\"144921751\",\"name\":\"M. Yokoo\"},{\"authorId\":\"1748597\",\"name\":\"David V. Pynadath\"},{\"authorId\":\"1788771\",\"name\":\"S. Marsella\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a1ba74b52f3d481d5f679cb88353633c83fdc443\",\"title\":\"Taming Decentralized POMDPs: Towards Efficient Policy Computation for Multiagent Settings\",\"url\":\"https://www.semanticscholar.org/paper/a1ba74b52f3d481d5f679cb88353633c83fdc443\",\"venue\":\"IJCAI\",\"year\":2003}],\"title\":\"Multi-Agent Planning with Baseline Regret Minimization\",\"topics\":[{\"topic\":\"Regret (decision theory)\",\"topicId\":\"528786\",\"url\":\"https://www.semanticscholar.org/topic/528786\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Intel vPro\",\"topicId\":\"1332982\",\"url\":\"https://www.semanticscholar.org/topic/1332982\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Iteration\",\"topicId\":\"11823\",\"url\":\"https://www.semanticscholar.org/topic/11823\"},{\"topic\":\"Converge\",\"topicId\":\"205534\",\"url\":\"https://www.semanticscholar.org/topic/205534\"},{\"topic\":\"Dynamic programming\",\"topicId\":\"10893\",\"url\":\"https://www.semanticscholar.org/topic/10893\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"}],\"url\":\"https://www.semanticscholar.org/paper/a96e42377fea44b604b61bb24d07fad51a21207e\",\"venue\":\"IJCAI\",\"year\":2017}\n"