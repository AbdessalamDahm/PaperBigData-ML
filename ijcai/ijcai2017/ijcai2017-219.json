"{\"abstract\":\"In platform videogames, players are frequently tasked with solving medium-term navigation problems in order to gather items or powerups. Arti- ficial agents must generally obtain some form of direct experience before they can solve such tasks. Experience is gained either through training runs, or by exploiting knowledge of the game's physics to generate detailed simulations. Human players, on the other hand, seem to look ahead in high-level, abstract steps. Motivated by human play, we introduce an approach that leverages not only abstract \\\"skills\\\", but also knowledge of what those skills can and cannot achieve. We apply this approach to Infinite Mario, where despite facing randomly generated, maze-like levels, our agent is capable of deriving complex plans in real-time, without relying on perfect knowledge of the game's physics.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"143609015\",\"name\":\"Michael Dann\",\"url\":\"https://www.semanticscholar.org/author/143609015\"},{\"authorId\":\"1739843\",\"name\":\"Fabio Zambetta\",\"url\":\"https://www.semanticscholar.org/author/1739843\"},{\"authorId\":\"2156466\",\"name\":\"John Thangarajah\",\"url\":\"https://www.semanticscholar.org/author/2156466\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"143609015\",\"name\":\"Michael Dann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"daf93749171e3575cca5b584a4146b247692c86f\",\"title\":\"Learning and planning in videogames via task decomposition\",\"url\":\"https://www.semanticscholar.org/paper/daf93749171e3575cca5b584a4146b247692c86f\",\"venue\":\"\",\"year\":2019}],\"corpusId\":20225473,\"doi\":\"10.24963/ijcai.2017/219\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"978a0e992a8f0ba3922f333ecd96ace0e28664f7\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2510156\",\"name\":\"\\u00d6. Simsek\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1145/1015330.1015353\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd4de76fadddd1cc9a91cea954200c8d656e1dbb\",\"title\":\"Using relative novelty to identify useful temporal abstractions in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/fd4de76fadddd1cc9a91cea954200c8d656e1dbb\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"},{\"authorId\":\"2463017\",\"name\":\"S. Karakovskiy\"},{\"authorId\":\"46273266\",\"name\":\"R. Baumgarten\"}],\"doi\":\"10.1109/CEC.2010.5586133\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"20c7139595570f080fc85a054c84262d11a488bd\",\"title\":\"The 2009 Mario AI Competition\",\"url\":\"https://www.semanticscholar.org/paper/20c7139595570f080fc85a054c84262d11a488bd\",\"venue\":\"IEEE Congress on Evolutionary Computation\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bdc5a10aa5805808cfca58ac527ddc23e737bee8\",\"title\":\"Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining\",\"url\":\"https://www.semanticscholar.org/paper/bdc5a10aa5805808cfca58ac527ddc23e737bee8\",\"venue\":\"NIPS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7142185fd2e86e922f609562f30820cd7c5af7f4\",\"title\":\"Advances in Neural Information Processing Systems (NIPS)\",\"url\":\"https://www.semanticscholar.org/paper/7142185fd2e86e922f609562f30820cd7c5af7f4\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shie Mannor\"},{\"authorId\":null,\"name\":\"Ishai Menache\"},{\"authorId\":null,\"name\":\"Amit Hoze\"},{\"authorId\":null,\"name\":\"Uri Klein. Dynamic Abstraction in Reinforcement Learni Clustering\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 21st International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"page 71. ACM,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"\\u00d6zg\\u00fcr \\u015eim\\u015fek\"},{\"authorId\":null,\"name\":\"Andrew G Barto. Using Relative Novelty to Identify Useful Learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 21st International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"page 95. ACM,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720466\",\"name\":\"T. Elomaa\"},{\"authorId\":\"1712654\",\"name\":\"H. Mannila\"},{\"authorId\":\"143785973\",\"name\":\"Hannu (TT) Toivonen\"}],\"doi\":\"10.1007/3-540-36755-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1f2b459bb644fcaf61118dbf31db5856f9f991de\",\"title\":\"Machine Learning: ECML 2002\",\"url\":\"https://www.semanticscholar.org/paper/1f2b459bb644fcaf61118dbf31db5856f9f991de\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/CIG.2009.5286507\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a63cca6749d5a6f20e779c315a0b07d91c3c977\",\"title\":\"Mario AI competition\",\"url\":\"https://www.semanticscholar.org/paper/0a63cca6749d5a6f20e779c315a0b07d91c3c977\",\"venue\":\"2009 IEEE Symposium on Computational Intelligence and Games\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1684547\",\"name\":\"I. Menache\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1742179\",\"name\":\"N. Shimkin\"}],\"doi\":\"10.1007/3-540-36755-1_25\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dca9444e1c69eee36c0be04703d71114a762c84a\",\"title\":\"Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/dca9444e1c69eee36c0be04703d71114a762c84a\",\"venue\":\"ECML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bruce L Digney. Learning Hierarchical Control Structures Tasks\"},{\"authorId\":null,\"name\":\"Changing Environments\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 5th International Conference on Simulation of Adaptive Behavior\",\"url\":\"\",\"venue\":\"volume 5, pages 321\\u2013 330,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144851733\",\"name\":\"M. Pickett\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3307e0ff313497b8a1dc3358982cc679125d14cd\",\"title\":\"PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3307e0ff313497b8a1dc3358982cc679125d14cd\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3423072\",\"name\":\"B. Digney\"}],\"doi\":\"10.7551/mitpress/3119.003.0050\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82a80da61c0f1a3a33deab676213794ee1f8bd6d\",\"title\":\"Learning hierarchical control structures for multiple tasks and changing environments\",\"url\":\"https://www.semanticscholar.org/paper/82a80da61c0f1a3a33deab676213794ee1f8bd6d\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478313\",\"name\":\"L. Lin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54c4cf3a8168c1b70f91cf78a3dc98b671935492\",\"title\":\"Reinforcement learning for robots using neural networks\",\"url\":\"https://www.semanticscholar.org/paper/54c4cf3a8168c1b70f91cf78a3dc98b671935492\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2452570\",\"name\":\"Ngo Anh Vien\"},{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e9eac3538af54c1f9e0ab837bfc5fc3ec3a5b09\",\"title\":\"Hierarchical Monte-Carlo Planning\",\"url\":\"https://www.semanticscholar.org/paper/0e9eac3538af54c1f9e0ab837bfc5fc3ec3a5b09\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"52e2ac397f0c8d5f533959905df899bc328d9f85\",\"title\":\"Reinforcement Learning with Hierarchies of Machines\",\"url\":\"https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85\",\"venue\":\"NIPS\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Julian Togelius\"},{\"authorId\":null,\"name\":\"Sergey Karakovskiy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Jan Koutn\\u0131\\u0301k\",\"url\":\"\",\"venue\":\"and J\\u00fcrgen Schmidhuber. Super Mario evolution. In Computational Intelligence and Games, 2009. CIG 2009. IEEE Symposium on, pages 156\\u2013161. IEEE,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ngo Anh Vien\"},{\"authorId\":null,\"name\":\"Marc Toussaint. Hierarchical Monte-Carlo Planning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 29th AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 3613\\u20133619,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas G Dietterich. Hierarchical Reinforcement Learning Decomposition\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research (JAIR)\",\"url\":\"\",\"venue\":\"13:227\\u2013303,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693549\",\"name\":\"S. Dzeroski\"},{\"authorId\":\"1740042\",\"name\":\"L. D. Raedt\"},{\"authorId\":\"145057418\",\"name\":\"S. Wrobel\"}],\"doi\":\"10.1145/1102351\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df976ef18596067c4f06b714edd3e3a70b1a0fd7\",\"title\":\"Proceedings of the 22nd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/df976ef18596067c4f06b714edd3e3a70b1a0fd7\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"16050a256dd6add1e9187e8c4f5c30c85f342fd8\",\"title\":\"Building Portable Options: Skill Transfer in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/16050a256dd6add1e9187e8c4f5c30c85f342fd8\",\"venue\":\"IJCAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2510156\",\"name\":\"\\u00d6. Simsek\"},{\"authorId\":\"1759756\",\"name\":\"Alicia P. Wolfe\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1145/1102351.1102454\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1c59bfa0e8654ebea94277064f82062875cae8b6\",\"title\":\"Identifying useful subgoals in reinforcement learning by local graph partitioning\",\"url\":\"https://www.semanticscholar.org/paper/1c59bfa0e8654ebea94277064f82062875cae8b6\",\"venue\":\"ICML '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Julian Togelius\"},{\"authorId\":null,\"name\":\"Sergey Karakovskiy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Julian Togelius , Sergey Karakovskiy , Jan Koutn\\u0131\\u0301k , and J\\u00fcrgen Schmidhuber . Super Mario evolution . In Computational Intelligence and Games , 2009 . CIG 2009\",\"url\":\"\",\"venue\":\"Finding Structure in Reinforcement Learning . Advances in Neural Information Processing Systems ( NIPS )\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886166\",\"name\":\"E. Jacobsen\"},{\"authorId\":\"34720673\",\"name\":\"R. Greve\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1145/2576768.2598392\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f98716a2a94e64d118ae17ddf1283c0a379fe5ec\",\"title\":\"Monte Mario: platforming with MCTS\",\"url\":\"https://www.semanticscholar.org/paper/f98716a2a94e64d118ae17ddf1283c0a379fe5ec\",\"venue\":\"GECCO\",\"year\":2014},{\"arxivId\":\"1606.04695\",\"authors\":[{\"authorId\":\"2918740\",\"name\":\"A. Vezhnevets\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"70495322\",\"name\":\"J. Agapiou\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ba25cb493ac7a03fc15d3b936257c9a6c689c1d\",\"title\":\"Strategic Attentive Writer for Learning Macro-Actions\",\"url\":\"https://www.semanticscholar.org/paper/4ba25cb493ac7a03fc15d3b936257c9a6c689c1d\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1606.01868\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"143810408\",\"name\":\"D. Saxton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e90fd78e8a3b98af3954aae5209703aa966603e\",\"title\":\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"cs/9905014\",\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1613/jair.639\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c96ca25d889251e20e33d01f24eec175301ab94\",\"title\":\"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/4c96ca25d889251e20e33d01f24eec175301ab94\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2000}],\"title\":\"Real-Time Navigation in Classical Platform Games via Skill Reuse\",\"topics\":[{\"topic\":\"Lattice problem\",\"topicId\":\"256022\",\"url\":\"https://www.semanticscholar.org/topic/256022\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Gauntlet\",\"topicId\":\"811521\",\"url\":\"https://www.semanticscholar.org/topic/811521\"},{\"topic\":\"Exploit (computer security)\",\"topicId\":\"439222\",\"url\":\"https://www.semanticscholar.org/topic/439222\"},{\"topic\":\"Experience\",\"topicId\":\"4221\",\"url\":\"https://www.semanticscholar.org/topic/4221\"},{\"topic\":\"Parsing\",\"topicId\":\"1910\",\"url\":\"https://www.semanticscholar.org/topic/1910\"},{\"topic\":\"Procedural generation\",\"topicId\":\"21963\",\"url\":\"https://www.semanticscholar.org/topic/21963\"},{\"topic\":\"Real-time locating system\",\"topicId\":\"811893\",\"url\":\"https://www.semanticscholar.org/topic/811893\"},{\"topic\":\"Simulation\",\"topicId\":\"194\",\"url\":\"https://www.semanticscholar.org/topic/194\"},{\"topic\":\"Rover (The Prisoner)\",\"topicId\":\"638\",\"url\":\"https://www.semanticscholar.org/topic/638\"},{\"topic\":\"Real-time computing\",\"topicId\":\"172684\",\"url\":\"https://www.semanticscholar.org/topic/172684\"},{\"topic\":\"Real-time clock\",\"topicId\":\"121831\",\"url\":\"https://www.semanticscholar.org/topic/121831\"},{\"topic\":\"Overhead (computing)\",\"topicId\":\"4163\",\"url\":\"https://www.semanticscholar.org/topic/4163\"},{\"topic\":\"Real-time transcription\",\"topicId\":\"763488\",\"url\":\"https://www.semanticscholar.org/topic/763488\"},{\"topic\":\"A* search algorithm\",\"topicId\":\"175017\",\"url\":\"https://www.semanticscholar.org/topic/175017\"}],\"url\":\"https://www.semanticscholar.org/paper/978a0e992a8f0ba3922f333ecd96ace0e28664f7\",\"venue\":\"IJCAI\",\"year\":2017}\n"