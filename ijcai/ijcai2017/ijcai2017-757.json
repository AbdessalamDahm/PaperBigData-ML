"{\"abstract\":null,\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\",\"url\":\"https://www.semanticscholar.org/author/1737999\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"151494070\",\"name\":\"Lior Fuks\"},{\"authorId\":\"3461721\",\"name\":\"Noor Awad\"},{\"authorId\":\"144661829\",\"name\":\"F. Hutter\"},{\"authorId\":\"145963266\",\"name\":\"M. Lindauer\"}],\"doi\":\"10.24963/ijcai.2019/172\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"0dbf2d2c05e1955fbcf3461def336224cc68ce1f\",\"title\":\"An Evolution Strategy with Progressive Episode Lengths for Playing Games\",\"url\":\"https://www.semanticscholar.org/paper/0dbf2d2c05e1955fbcf3461def336224cc68ce1f\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1588174060\",\"name\":\"Bangyu Qin\"},{\"authorId\":\"48146797\",\"name\":\"Yue Gao\"},{\"authorId\":\"144843221\",\"name\":\"Y. Bai\"}],\"doi\":\"10.1109/ICRAE48301.2019.9043822\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1817dacc1c6f87d0969b14b73ac2855fc22dfdc4\",\"title\":\"Sim-to-real: Six-legged Robot Control with Deep Reinforcement Learning and Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/1817dacc1c6f87d0969b14b73ac2855fc22dfdc4\",\"venue\":\"2019 4th International Conference on Robotics and Automation Engineering (ICRAE)\",\"year\":2019},{\"arxivId\":\"2001.01536\",\"authors\":[{\"authorId\":\"83620577\",\"name\":\"Liuyu Xiang\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"}],\"doi\":\"10.1007/978-3-030-58558-7_15\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20d8cb84cd1cf26e803e73291fa43673ef3c9c86\",\"title\":\"Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification\",\"url\":\"https://www.semanticscholar.org/paper/20d8cb84cd1cf26e803e73291fa43673ef3c9c86\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1903.00742\",\"authors\":[{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"145148319\",\"name\":\"E. Hughes\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"260f1ee9b2fdefe885aca9232689fb36c5c2eab2\",\"title\":\"Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research\",\"url\":\"https://www.semanticscholar.org/paper/260f1ee9b2fdefe885aca9232689fb36c5c2eab2\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2008.09377\",\"authors\":[{\"authorId\":\"118608816\",\"name\":\"Binyamin Manela\"},{\"authorId\":\"2924948\",\"name\":\"A. Biess\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a026fb990836c9c36f0d24bc3d079656938f5193\",\"title\":\"Curriculum Learning with Hindsight Experience Replay for Sequential Object Manipulation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/a026fb990836c9c36f0d24bc3d079656938f5193\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50811693\",\"name\":\"Matilde Gargiani\"},{\"authorId\":\"35437368\",\"name\":\"A. Zanelli\"},{\"authorId\":\"1403374767\",\"name\":\"Q. Tran-Dinh\"},{\"authorId\":\"1694268\",\"name\":\"M. Diehl\"},{\"authorId\":\"144661829\",\"name\":\"F. Hutter\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"74a26773ea750739822e3a686be7051ceae9e6f4\",\"title\":\"Transferring Optimality Across Data Distributions via Homotopy Methods\",\"url\":\"https://www.semanticscholar.org/paper/74a26773ea750739822e3a686be7051ceae9e6f4\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9415843\",\"name\":\"C. Dewa\"},{\"authorId\":\"145472435\",\"name\":\"J. Miura\"}],\"doi\":\"10.1109/ACCESS.2020.3033016\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b8ef3ae749ba2225038d35c462395d565a5d6cc9\",\"title\":\"A Framework for DRL Navigation With State Transition Checking and Velocity Increment Scheduling\",\"url\":\"https://www.semanticscholar.org/paper/b8ef3ae749ba2225038d35c462395d565a5d6cc9\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387999164\",\"name\":\"Matthew Reynard\"},{\"authorId\":\"2308553\",\"name\":\"H. Kamper\"},{\"authorId\":\"27378556\",\"name\":\"H. Engelbrecht\"},{\"authorId\":\"2831294\",\"name\":\"Benjamin Rosman\"}],\"doi\":\"10.1109/SAUPEC/RobMech/PRASA48453.2020.9041025\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a30917733db939b004d6a637a5d316373914af4\",\"title\":\"Combining primitive DQNs for improved reinforcement learning in Minecraft\",\"url\":\"https://www.semanticscholar.org/paper/5a30917733db939b004d6a637a5d316373914af4\",\"venue\":\"2020 International SAUPEC/RobMech/PRASA Conference\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39593364\",\"name\":\"Bob McGrew\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed63612ab053b384c01d22e77da727928e652807\",\"title\":\"CURRICULUM LEARNING WITH HINDSIGHT EXPERIENCE REPLAY FOR SEQUENTIAL OBJECT MANIPULATION TASKS\",\"url\":\"https://www.semanticscholar.org/paper/ed63612ab053b384c01d22e77da727928e652807\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2027008507\",\"name\":\"Kuk Jang\"},{\"authorId\":\"2790512\",\"name\":\"Y. Pant\"},{\"authorId\":\"2027008824\",\"name\":\"Al\\u00ebna Rodionova\"},{\"authorId\":\"1785009\",\"name\":\"Rahul Mangharam\"}],\"doi\":\"10.1109/DASC50938.2020.9256710\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0792ce9cc103c2a5b09c783c9e6360e2c8ac6e0e\",\"title\":\"Learning-to-Fly RL: Reinforcement Learning-based Collision Avoidance for Scalable Urban Air Mobility\",\"url\":\"https://www.semanticscholar.org/paper/0792ce9cc103c2a5b09c783c9e6360e2c8ac6e0e\",\"venue\":\"2020 AIAA/IEEE 39th Digital Avionics Systems Conference (DASC)\",\"year\":2020},{\"arxivId\":\"1811.10264\",\"authors\":[{\"authorId\":\"2041152\",\"name\":\"Qihao Liu\"},{\"authorId\":\"48033112\",\"name\":\"X. Liu\"},{\"authorId\":\"11237130\",\"name\":\"Guoping Cai\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9519538f9d4e12acc5045434f2fde7ef3d32e2a4\",\"title\":\"Control with Distributed Deep Reinforcement Learning: Learn a Better Policy\",\"url\":\"https://www.semanticscholar.org/paper/9519538f9d4e12acc5045434f2fde7ef3d32e2a4\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"BUTIONS VIA\"},{\"authorId\":null,\"name\":\"HOMOTOPY METHODS\"},{\"authorId\":\"50811693\",\"name\":\"Matilde Gargiani\"},{\"authorId\":\"35437368\",\"name\":\"A. Zanelli\"},{\"authorId\":\"1403374767\",\"name\":\"Q. Tran-Dinh\"},{\"authorId\":\"1694268\",\"name\":\"M. Diehl\"},{\"authorId\":\"144661829\",\"name\":\"F. Hutter\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b390e3e6d2b586a937e26124da192ac30726a5c\",\"title\":\"TRANSFERRING OPTIMALITY ACROSS DATA DISTRI-\",\"url\":\"https://www.semanticscholar.org/paper/4b390e3e6d2b586a937e26124da192ac30726a5c\",\"venue\":\"\",\"year\":2020}],\"corpusId\":33552496,\"doi\":\"10.24963/ijcai.2017/757\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"8e2c45367820406c65e9ebe26d598054a1996f28\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1756287\",\"name\":\"Vishal Soni\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c069d5777f943875878eeaaa67814d58ebad894e\",\"title\":\"Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains\",\"url\":\"https://www.semanticscholar.org/paper/c069d5777f943875878eeaaa67814d58ebad894e\",\"venue\":\"AAAI\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.24963/ijcai.2017/353\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7708394600130c8c5403178976fcfca1972f3db\",\"title\":\"Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b7708394600130c8c5403178976fcfca1972f3db\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":\"10.1007/978-3-642-27645-3_5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"title\":\"Transfer in Reinforcement Learning: A Framework and a Survey\",\"url\":\"https://www.semanticscholar.org/paper/16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"title\":\"Source Task Creation for Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"1729320\",\"name\":\"A. Bonarini\"}],\"doi\":\"10.1145/1390156.1390225\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"title\":\"Transfer of samples in batch reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"venue\":\"ICML '08\",\"year\":2008}],\"title\":\"Curriculum Learning in Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/8e2c45367820406c65e9ebe26d598054a1996f28\",\"venue\":\"IJCAI\",\"year\":2017}\n"