"{\"abstract\":\"We introduce a new count-based optimistic exploration algorithm for Reinforcement Learning (RL) that is feasible in environments with high-dimensional state-action spaces. The success of RL algorithms in these domains depends crucially on generalisation from limited training experience. Function approximation techniques enable RL agents to generalise in order to estimate the value of unvisited states, but at present few methods enable generalisation regarding uncertainty. This has prevented the combination of scalable RL algorithms with efficient exploration strategies that drive the agent to reduce its uncertainty. We present a new method for computing a generalised state visit-count, which allows the agent to estimate the uncertainty associated with any state. Our \\\\phi-pseudocount achieves generalisation by exploiting same feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The \\\\phi-Exploration-Bonus algorithm rewards the agent for exploring in feature space rather than in the untransformed state space. The method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks.\",\"arxivId\":\"1706.08090\",\"authors\":[{\"authorId\":\"34104547\",\"name\":\"J. Martin\",\"url\":\"https://www.semanticscholar.org/author/34104547\"},{\"authorId\":\"9763678\",\"name\":\"S. N. Sasikumar\",\"url\":\"https://www.semanticscholar.org/author/9763678\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\",\"url\":\"https://www.semanticscholar.org/author/1868196\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\",\"url\":\"https://www.semanticscholar.org/author/144154444\"}],\"citationVelocity\":14,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Lukas Rusch\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"77a31a4601444a3f7aeed15061b08684d0bea92b\",\"title\":\"Exploration-Exploitation Trade-off in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/77a31a4601444a3f7aeed15061b08684d0bea92b\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9937287\",\"name\":\"Ronan Fruit\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e54edd0c55c0cb63e719b501e41790c75a2c73a\",\"title\":\"Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge. (Impact des connaissances a priori sur le compromis exploration-exploitation en apprentissage par renforcement)\",\"url\":\"https://www.semanticscholar.org/paper/0e54edd0c55c0cb63e719b501e41790c75a2c73a\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"932ca5b6a80b91e76dcadd68b7ba2937a67b8376\",\"title\":\"RIDE: REWARDING IMPACT-DRIVEN EXPLORATION\",\"url\":\"https://www.semanticscholar.org/paper/932ca5b6a80b91e76dcadd68b7ba2937a67b8376\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1709.06009\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":\"10.24963/ijcai.2018/787\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3b290ffa1f4f8226e326f00984acecdfbe9e28bf\",\"title\":\"Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents\",\"url\":\"https://www.semanticscholar.org/paper/3b290ffa1f4f8226e326f00984acecdfbe9e28bf\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1906.07865\",\"authors\":[{\"authorId\":\"146391397\",\"name\":\"Cam Linke\"},{\"authorId\":\"89146747\",\"name\":\"Nadia M. Ady\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"},{\"authorId\":\"49491434\",\"name\":\"Thomas Degris\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6b8d35da220d274b561bc0affa12810990e720a\",\"title\":\"Adapting Behaviour via Intrinsic Reward: A Survey and Empirical Study\",\"url\":\"https://www.semanticscholar.org/paper/c6b8d35da220d274b561bc0affa12810990e720a\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387894646\",\"name\":\"A. Aubret\"},{\"authorId\":\"2335305\",\"name\":\"L. Matignon\"},{\"authorId\":\"1730965\",\"name\":\"S. Hassas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c6011b09783150bcda711c8d3176385875020a60\",\"title\":\"\\u00c9tude de la motivation intrins\\u00e8que en apprentissage par renforcement\",\"url\":\"https://www.semanticscholar.org/paper/c6011b09783150bcda711c8d3176385875020a60\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1804.08619\",\"authors\":[{\"authorId\":\"1743225\",\"name\":\"W. Li\"},{\"authorId\":\"50187585\",\"name\":\"Fuxian Huang\"},{\"authorId\":\"50079147\",\"name\":\"X. Li\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"},{\"authorId\":\"39918420\",\"name\":\"F. Wu\"}],\"doi\":\"10.1007/s11063-018-9944-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"427b05d77d56a1047fcc12b66e4aa507a99bafa8\",\"title\":\"State Distribution-Aware Sampling for Deep Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/427b05d77d56a1047fcc12b66e4aa507a99bafa8\",\"venue\":\"Neural Processing Letters\",\"year\":2018},{\"arxivId\":\"1812.07019\",\"authors\":[{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"37591038\",\"name\":\"Edward Hughes\"},{\"authorId\":\"35091903\",\"name\":\"S. Wheelwright\"},{\"authorId\":\"2367822\",\"name\":\"Adam H. Marblestone\"},{\"authorId\":\"1400818648\",\"name\":\"Edgar A. Du\\u00e9\\u00f1ez-Guzm\\u00e1n\"},{\"authorId\":\"1814162\",\"name\":\"Peter Sunehag\"},{\"authorId\":\"2768462\",\"name\":\"Iain Dunning\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf4a4d18da4aa335cd18e095bdfff21290afebb0\",\"title\":\"Malthusian Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/cf4a4d18da4aa335cd18e095bdfff21290afebb0\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":\"1807.01425\",\"authors\":[{\"authorId\":\"143625418\",\"name\":\"Artem Molchanov\"},{\"authorId\":\"1944801\",\"name\":\"Karol Hausman\"},{\"authorId\":\"2238841\",\"name\":\"S. Birchfield\"},{\"authorId\":\"1732493\",\"name\":\"G. Sukhatme\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73abd0733e0125c123e6a0b23472772a9c343e5f\",\"title\":\"Region Growing Curriculum Generation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/73abd0733e0125c123e6a0b23472772a9c343e5f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144111160\",\"name\":\"Youngwoon Lee\"},{\"authorId\":\"1917234\",\"name\":\"S. Sun\"},{\"authorId\":\"144136097\",\"name\":\"S. Somasundaram\"},{\"authorId\":\"153201035\",\"name\":\"Edward S. Hu\"},{\"authorId\":\"35198686\",\"name\":\"Joseph J. Lim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"74e12851de2d542aa2aef7b8a39ef021a5802689\",\"title\":\"Composing Complex Skills by Learning Transition Policies\",\"url\":\"https://www.semanticscholar.org/paper/74e12851de2d542aa2aef7b8a39ef021a5802689\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1905.04640\",\"authors\":[{\"authorId\":\"3383051\",\"name\":\"Yuhang Song\"},{\"authorId\":\"49606288\",\"name\":\"Jianyi Wang\"},{\"authorId\":\"1690572\",\"name\":\"Thomas Lukasiewicz\"},{\"authorId\":\"50070382\",\"name\":\"Zhenghua Xu\"},{\"authorId\":\"2503523\",\"name\":\"S. Zhang\"},{\"authorId\":\"1743773\",\"name\":\"M. Xu\"}],\"doi\":\"10.1609/AAAI.V34I04.6040\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"edcd3a5e8e0e6fd27f95c34348404ea449b6927d\",\"title\":\"Mega-Reward: Achieving Human-Level Play without Extrinsic Rewards\",\"url\":\"https://www.semanticscholar.org/paper/edcd3a5e8e0e6fd27f95c34348404ea449b6927d\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21285272\",\"name\":\"Stephen W. Carden\"},{\"authorId\":\"144590477\",\"name\":\"S. Walker\"}],\"doi\":\"10.3390/MAKE1020041\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d467fefe39413757d04899038105bf23a112307\",\"title\":\"Exploration Using Without-Replacement Sampling of Actions Is Sometimes Inferior\",\"url\":\"https://www.semanticscholar.org/paper/4d467fefe39413757d04899038105bf23a112307\",\"venue\":\"Mach. Learn. Knowl. Extr.\",\"year\":2019},{\"arxivId\":\"1908.06976\",\"authors\":[{\"authorId\":\"1387894646\",\"name\":\"A. Aubret\"},{\"authorId\":\"153442621\",\"name\":\"L. Matignon\"},{\"authorId\":\"1730965\",\"name\":\"S. Hassas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"895735cace0de940aa647dbafc046b7f30316fe5\",\"title\":\"A survey on intrinsic motivation in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/895735cace0de940aa647dbafc046b7f30316fe5\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143609015\",\"name\":\"Michael Dann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"daf93749171e3575cca5b584a4146b247692c86f\",\"title\":\"Learning and planning in videogames via task decomposition\",\"url\":\"https://www.semanticscholar.org/paper/daf93749171e3575cca5b584a4146b247692c86f\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1705.10557\",\"authors\":[{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.24963/ijcai.2017/194\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d1627e5dd7c6656aa8c16d861677ac631c5c4301\",\"title\":\"Universal Reinforcement Learning Algorithms: Survey and Experiments\",\"url\":\"https://www.semanticscholar.org/paper/d1627e5dd7c6656aa8c16d861677ac631c5c4301\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"1806.11525\",\"authors\":[{\"authorId\":\"2854297\",\"name\":\"Xingdi Yuan\"},{\"authorId\":\"40638665\",\"name\":\"Marc-Alexandre C\\u00f4t\\u00e9\"},{\"authorId\":\"2041695\",\"name\":\"Alessandro Sordoni\"},{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"15032777\",\"name\":\"Remi Tachet des Combes\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"3382568\",\"name\":\"Adam Trischler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c52dddfbb4a3af4cc5e72849fe965c62801539e7\",\"title\":\"Counting to Explore and Generalize in Text-based Games\",\"url\":\"https://www.semanticscholar.org/paper/c52dddfbb4a3af4cc5e72849fe965c62801539e7\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Louis Bagot\"},{\"authorId\":\"98922276\",\"name\":\"Kevin Mets\"},{\"authorId\":\"46301036\",\"name\":\"S. Latr\\u00e9\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f4e973204815f10372a47d7ff742b0b01935cca\",\"title\":\"Learning Intrinsically Motivated Options to Stimulate Policy Exploration\",\"url\":\"https://www.semanticscholar.org/paper/5f4e973204815f10372a47d7ff742b0b01935cca\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1811.01483\",\"authors\":[{\"authorId\":\"1899119\",\"name\":\"Jongwook Choi\"},{\"authorId\":\"1857914\",\"name\":\"Yijie Guo\"},{\"authorId\":\"3009779\",\"name\":\"M. Moczulski\"},{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"3104836\",\"name\":\"Neal Wu\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bf604ae3ddd5adec55554921b37f04035b7350a7\",\"title\":\"Contingency-Aware Exploration in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bf604ae3ddd5adec55554921b37f04035b7350a7\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13461630\",\"name\":\"Niko Yasui\"},{\"authorId\":\"2580584\",\"name\":\"Sungsu Lim\"},{\"authorId\":\"146391397\",\"name\":\"Cam Linke\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"114860989\",\"name\":\"Martha White\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e37260307362b869c025d170c5833453e3d1e4ea\",\"title\":\"An Empirical and Conceptual Categorization of Value-based Exploration Methods\",\"url\":\"https://www.semanticscholar.org/paper/e37260307362b869c025d170c5833453e3d1e4ea\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152108465\",\"name\":\"B. Zhou\"},{\"authorId\":\"14093621\",\"name\":\"H. Zeng\"},{\"authorId\":\"73928580\",\"name\":\"F. Wang\"},{\"authorId\":\"7597771\",\"name\":\"Rongzhong Lian\"},{\"authorId\":\"9761982\",\"name\":\"Hao Tian\"}],\"doi\":\"10.1007/978-3-030-29135-8_10\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"11900ac111b5d50fc7a6d5c5f1d8f4fb0c009b22\",\"title\":\"Efficient and Robust Learning on Elaborated Gaits with Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/11900ac111b5d50fc7a6d5c5f1d8f4fb0c009b22\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1806.05898\",\"authors\":[{\"authorId\":\"51003241\",\"name\":\"Miquel Junyent\"},{\"authorId\":\"143808510\",\"name\":\"A. Jonsson\"},{\"authorId\":\"145810673\",\"name\":\"V. G\\u00f3mez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1876d84a53ffc173331d6a4f9f1b3172fea2442c\",\"title\":\"Improving width-based planning with compact policies\",\"url\":\"https://www.semanticscholar.org/paper/1876d84a53ffc173331d6a4f9f1b3172fea2442c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1904.07091\",\"authors\":[{\"authorId\":\"51003241\",\"name\":\"Miquel Junyent\"},{\"authorId\":\"143808510\",\"name\":\"A. Jonsson\"},{\"authorId\":\"145810673\",\"name\":\"V. G\\u00f3mez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"90c41db6420f0c8e7360ae4cba50650504fe9af5\",\"title\":\"Deep Policies for Width-Based Planning in Pixel Domains\",\"url\":\"https://www.semanticscholar.org/paper/90c41db6420f0c8e7360ae4cba50650504fe9af5\",\"venue\":\"ICAPS\",\"year\":2019},{\"arxivId\":\"1810.06530\",\"authors\":[{\"authorId\":\"1397996167\",\"name\":\"David Janz\"},{\"authorId\":\"29785199\",\"name\":\"J. Hron\"},{\"authorId\":\"1388574431\",\"name\":\"Jos\\u00e9 Miguel Hern\\u00e1ndez-Lobato\"},{\"authorId\":\"1380228856\",\"name\":\"Katja Hofmann\"},{\"authorId\":\"3302876\",\"name\":\"Sebastian Tschiatschek\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"72a75aeab2c918394dc9af2408fd8e1076ae39ac\",\"title\":\"Successor Uncertainties: exploration and uncertainty in temporal difference learning\",\"url\":\"https://www.semanticscholar.org/paper/72a75aeab2c918394dc9af2408fd8e1076ae39ac\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35273604\",\"name\":\"C. Sun\"},{\"authorId\":\"15328841\",\"name\":\"Renmin Wang\"},{\"authorId\":\"8140131\",\"name\":\"R. Li\"},{\"authorId\":\"100551849\",\"name\":\"Jiao Wu\"},{\"authorId\":\"47027378\",\"name\":\"X. Hu\"}],\"doi\":\"10.1109/IJCNN.2019.8852234\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"12e9394725a085a89114d2c981ce4a76ec6f37cf\",\"title\":\"Efficient and Scalable Exploration via Estimation-Error\",\"url\":\"https://www.semanticscholar.org/paper/12e9394725a085a89114d2c981ce4a76ec6f37cf\",\"venue\":\"2019 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1976034011\",\"name\":\"He A. Xu\"},{\"authorId\":\"40791701\",\"name\":\"Alireza Modirshanechi\"},{\"authorId\":\"1976664875\",\"name\":\"Marco P. Lehmann\"},{\"authorId\":\"1708945\",\"name\":\"W. Gerstner\"},{\"authorId\":\"2102793\",\"name\":\"M. Herzog\"}],\"doi\":\"10.1101/2020.09.24.311084\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f1988c5399dc501805bed7ecfec491383386b18\",\"title\":\"Novelty is not Surprise: Exploration and learning in human sequential decision-making\",\"url\":\"https://www.semanticscholar.org/paper/3f1988c5399dc501805bed7ecfec491383386b18\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12264663\",\"name\":\"J. Qian\"},{\"authorId\":\"9937287\",\"name\":\"Ronan Fruit\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6222c8b38521e38ebbe6ea4868d4aaafa619a335\",\"title\":\"Exploration Bonus for Regret Minimization in Discrete and Continuous Average Reward MDPs\",\"url\":\"https://www.semanticscholar.org/paper/6222c8b38521e38ebbe6ea4868d4aaafa619a335\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2006.01419\",\"authors\":[{\"authorId\":\"3435623\",\"name\":\"Seungyul Han\"},{\"authorId\":\"9238685\",\"name\":\"Youngchul Sung\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c68715345332a92057e7603271c046d3236c4b64\",\"title\":\"Diversity Actor-Critic: Sample-Aware Entropy Regularization for Sample-Efficient Exploration\",\"url\":\"https://www.semanticscholar.org/paper/c68715345332a92057e7603271c046d3236c4b64\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1707.00524\",\"authors\":[{\"authorId\":\"50086584\",\"name\":\"Haiyan Yin\"},{\"authorId\":\"3359642\",\"name\":\"J. Chen\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"}],\"doi\":\"10.24963/ijcai.2018/420\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ea10749c0ef01294d5beef9a5c901d899a2e1f6e\",\"title\":\"Hashing Over Predicted Future Frames for Informed Exploration of Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ea10749c0ef01294d5beef9a5c901d899a2e1f6e\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1905.11591\",\"authors\":[{\"authorId\":\"47905826\",\"name\":\"Yufei Wang\"},{\"authorId\":\"3006308\",\"name\":\"Q. Ye\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"47231dec371ac92a5caabf64508ed5332cf7e4f8\",\"title\":\"Beyond Exponentially Discounted Sum: Automatic Learning of Return Function\",\"url\":\"https://www.semanticscholar.org/paper/47231dec371ac92a5caabf64508ed5332cf7e4f8\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"146391397\",\"name\":\"Cam Linke\"},{\"authorId\":\"89146747\",\"name\":\"Nadia M. Ady\"},{\"authorId\":\"114860989\",\"name\":\"M. White\"},{\"authorId\":\"49491434\",\"name\":\"T. Degris\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"}],\"doi\":\"10.1613/jair.1.12087\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"376e0853411acb4e5732c587471d0a3910689b20\",\"title\":\"Adapting Behavior via Intrinsic Reward: A Survey and Empirical Study\",\"url\":\"https://www.semanticscholar.org/paper/376e0853411acb4e5732c587471d0a3910689b20\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2020},{\"arxivId\":\"1901.10995\",\"authors\":[{\"authorId\":\"66821245\",\"name\":\"Adrien Ecoffet\"},{\"authorId\":\"39378983\",\"name\":\"J. Huizinga\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c520bf47db3360ae3a52219771390a354ed8a91f\",\"title\":\"Go-Explore: a New Approach for Hard-Exploration Problems\",\"url\":\"https://www.semanticscholar.org/paper/c520bf47db3360ae3a52219771390a354ed8a91f\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1812.04363\",\"authors\":[{\"authorId\":\"145179958\",\"name\":\"Jian Qian\"},{\"authorId\":\"9937287\",\"name\":\"Ronan Fruit\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f06b33e59ff17d891604fb5f0f81831819b9da64\",\"title\":\"Exploration Bonus for Regret Minimization in Undiscounted Discrete and Continuous Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/f06b33e59ff17d891604fb5f0f81831819b9da64\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"}],\"doi\":\"10.25911/5D134A2F8A7D3\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8b1e149ae23ea7839c9e3a2bd063c354ff7075d0\",\"title\":\"Towards Safe Artificial General Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/8b1e149ae23ea7839c9e3a2bd063c354ff7075d0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1901.02478\",\"authors\":[{\"authorId\":\"2689633\",\"name\":\"Andrew Jaegle\"},{\"authorId\":\"66550313\",\"name\":\"Vahid Mehrpour\"},{\"authorId\":\"2913957\",\"name\":\"N. Rust\"}],\"doi\":\"10.1016/j.conb.2019.08.004\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"93ed1bfd12ddd23f7bf9410774cafac204189d56\",\"title\":\"Visual novelty, curiosity, and intrinsic reward in machine learning and the brain\",\"url\":\"https://www.semanticscholar.org/paper/93ed1bfd12ddd23f7bf9410774cafac204189d56\",\"venue\":\"Current Opinion in Neurobiology\",\"year\":2019},{\"arxivId\":\"2005.02880\",\"authors\":[{\"authorId\":\"8519553\",\"name\":\"Eliza Kosoy\"},{\"authorId\":\"39229748\",\"name\":\"Jasmine Collins\"},{\"authorId\":\"144568152\",\"name\":\"David M. Chan\"},{\"authorId\":\"2158860\",\"name\":\"Jessica B. Hamrick\"},{\"authorId\":\"2064588\",\"name\":\"Sandy H. Huang\"},{\"authorId\":\"1392440590\",\"name\":\"A. Gopnik\"},{\"authorId\":\"1729041\",\"name\":\"J. Canny\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32b5cb82214d343aa201b71f784015e37b527b33\",\"title\":\"Exploring Exploration: Comparing Children with RL Agents in Unified Environments\",\"url\":\"https://www.semanticscholar.org/paper/32b5cb82214d343aa201b71f784015e37b527b33\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1807.11622\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":\"10.1609/AAAI.V34I04.5955\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f67b3edc67a35c884bd532a5e73fa3a7f3660d8\",\"title\":\"Count-Based Exploration with the Successor Representation\",\"url\":\"https://www.semanticscholar.org/paper/9f67b3edc67a35c884bd532a5e73fa3a7f3660d8\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2002.12292\",\"authors\":[{\"authorId\":\"48647153\",\"name\":\"Roberta Raileanu\"},{\"authorId\":\"2620211\",\"name\":\"Tim Rockt\\u00e4schel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bebe8ffb0c357ac0c7eea2556f817b03ee22b570\",\"title\":\"RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments\",\"url\":\"https://www.semanticscholar.org/paper/bebe8ffb0c357ac0c7eea2556f817b03ee22b570\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1902.00843\",\"authors\":[{\"authorId\":\"145585382\",\"name\":\"F. Garcia\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8c0071c74e04ea1598ed2a208cbc255656f50b0\",\"title\":\"A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b8c0071c74e04ea1598ed2a208cbc255656f50b0\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":\"2003.04518\",\"authors\":[{\"authorId\":\"48481808\",\"name\":\"Y. Song\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"}],\"doi\":\"10.1109/CoG47356.2020.9231562\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b5132c21a85306af42c180a8bbb7fa0116b50126\",\"title\":\"Exploring Unknown States with Action Balance\",\"url\":\"https://www.semanticscholar.org/paper/b5132c21a85306af42c180a8bbb7fa0116b50126\",\"venue\":\"CoG\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000904823\",\"name\":\"Song Yan\"},{\"authorId\":\"66731876\",\"name\":\"Chen Ying-feng\"},{\"authorId\":\"66085863\",\"name\":\"Hu Yu-jing\"},{\"authorId\":\"2001001047\",\"name\":\"Fan Changjie\"}],\"doi\":\"10.1109/CoG47356.2020.9231562\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"706f1555be9ff1a05efb3eea32eb5a45c1ee577f\",\"title\":\"Exploring Unknown States with Action Balance\",\"url\":\"https://www.semanticscholar.org/paper/706f1555be9ff1a05efb3eea32eb5a45c1ee577f\",\"venue\":\"2020 IEEE Conference on Games (CoG)\",\"year\":2020},{\"arxivId\":\"1905.11583\",\"authors\":[{\"authorId\":null,\"name\":\"Ruihan Yang\"},{\"authorId\":\"3006308\",\"name\":\"Q. Ye\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"81768cdace11e14982d3aba9060059e1133d83fc\",\"title\":\"Learning Efficient and Effective Exploration Policies with Counterfactual Meta Policy\",\"url\":\"https://www.semanticscholar.org/paper/81768cdace11e14982d3aba9060059e1133d83fc\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2007.13729\",\"authors\":[{\"authorId\":\"144158271\",\"name\":\"Chuang Gan\"},{\"authorId\":\"144220896\",\"name\":\"Xiaoyu Chen\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"143805212\",\"name\":\"A. Torralba\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f978b3829acad6ac8b3372d2fa8d38a45b96d3d\",\"title\":\"Noisy Agents: Self-supervised Exploration by Predicting Auditory Events\",\"url\":\"https://www.semanticscholar.org/paper/5f978b3829acad6ac8b3372d2fa8d38a45b96d3d\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":24715002,\"doi\":\"10.24963/ijcai.2017/344\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"0f810eb4777fd05317951ebaa7a3f5835ee84cf4\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Philipp Moritz\"},{\"authorId\":null,\"name\":\"Michael I. Jordan\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1502.05477,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bradly C. Stadie\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Incentivizing exploration in reinforcement models\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1507.00814,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ian Osband\"},{\"authorId\":null,\"name\":\"Benjamin Van Roy\"},{\"authorId\":null,\"name\":\"Zheng Wen. Generalization\"},{\"authorId\":null,\"name\":\"exploration via randomized value functions\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1\\u201326,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc G. Bellemare\"},{\"authorId\":null,\"name\":\"Sriram Srinivasan\"},{\"authorId\":null,\"name\":\"Georg Ostrovski\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"David Saxton\"},{\"authorId\":null,\"name\":\"R\\u00e9mi Munos. Unifying count-based exploration\"},{\"authorId\":null,\"name\":\"intrinsic motivation\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1606.01868,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1145/1577069.1755867\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5d8e1eeeb0e4b0e0846a355532d0f9452249e68a\",\"title\":\"Reinforcement Learning in Finite MDPs: PAC Analysis\",\"url\":\"https://www.semanticscholar.org/paper/5d8e1eeeb0e4b0e0846a355532d0f9452249e68a\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1512.01563\",\"authors\":[{\"authorId\":\"2397352\",\"name\":\"Yitao Liang\"},{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"42b4d4087360228c5e4a8f24a01ebba9aba6d06a\",\"title\":\"State of the Art Control of Atari Games Using Shallow Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/42b4d4087360228c5e4a8f24a01ebba9aba6d06a\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hado van Hasselt\"},{\"authorId\":null,\"name\":\"Arthur Guez\"},{\"authorId\":null,\"name\":\"Matteo Hessel\"},{\"authorId\":null,\"name\":\"David Silver. Learning values across many orders of magnitude\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1602.07714,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1109/ICTAI.2004.28\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"849aaec99f3684505876a6d0f3e9a73621a8daff\",\"title\":\"An empirical evaluation of interval estimation for Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/849aaec99f3684505876a6d0f3e9a73621a8daff\",\"venue\":\"16th IEEE International Conference on Tools with Artificial Intelligence\",\"year\":2004},{\"arxivId\":\"1606.01868\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"143810408\",\"name\":\"D. Saxton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"6e90fd78e8a3b98af3954aae5209703aa966603e\",\"title\":\"Unifying Count-Based Exploration and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/6e90fd78e8a3b98af3954aae5209703aa966603e\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ian Osband\"},{\"authorId\":null,\"name\":\"Charles Blundell\"},{\"authorId\":null,\"name\":\"Alexander Pritzel\"},{\"authorId\":null,\"name\":\"Benjamin Van Roy. Deep exploration via bootstrapped DQN\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1602.04621,\",\"year\":2016},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1602.07714\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4931c91f4b30eb122def1e697abc096f14c48987\",\"title\":\"Learning values across many orders of magnitude\",\"url\":\"https://www.semanticscholar.org/paper/4931c91f4b30eb122def1e697abc096f14c48987\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66063679\",\"name\":\"Elsevier Sdol\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e9a1f7db02328fa945a3215ed047a48bcb06a4f\",\"title\":\"Advances in Applied Mathematics\",\"url\":\"https://www.semanticscholar.org/paper/0e9a1f7db02328fa945a3215ed047a48bcb06a4f\",\"venue\":\"\",\"year\":2009},{\"arxivId\":\"1111.3182\",\"authors\":[{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"34746380\",\"name\":\"K. S. Ng\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1109/DCC.2012.39\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"be4318c4f7fbc821d9b80f569156b7ace81743c9\",\"title\":\"Context Tree Switching\",\"url\":\"https://www.semanticscholar.org/paper/be4318c4f7fbc821d9b80f569156b7ace81743c9\",\"venue\":\"2012 Data Compression Conference\",\"year\":2012},{\"arxivId\":\"1602.04621\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b63e34276aa98d5345efa7fe09bb06d8a9d8f52\",\"title\":\"Deep Exploration via Bootstrapped DQN\",\"url\":\"https://www.semanticscholar.org/paper/4b63e34276aa98d5345efa7fe09bb06d8a9d8f52\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joel Veness\"},{\"authorId\":null,\"name\":\"Kee Siong Ng\"},{\"authorId\":null,\"name\":\"Marcus Hutter\"},{\"authorId\":null,\"name\":\"Michael Bowling. Context tree switching\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IEEE Data Compression Conference\",\"url\":\"\",\"venue\":\"pages 327\\u2013336,\",\"year\":2012},{\"arxivId\":\"1402.0635\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"},{\"authorId\":\"145254492\",\"name\":\"Z. Wen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1389772b8a0f9c7fc43057f9da41a7d0ebf0308b\",\"title\":\"Generalization and Exploration via Randomized Value Functions\",\"url\":\"https://www.semanticscholar.org/paper/1389772b8a0f9c7fc43057f9da41a7d0ebf0308b\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Georg Ostrovski\"},{\"authorId\":null,\"name\":\"Marc G. Bellemare\"},{\"authorId\":null,\"name\":\"A\\u00e4ron van den Oord\"},{\"authorId\":null,\"name\":\"R\\u00e9mi Munos. Countbased exploration with neural density models\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1703.01310,\",\"year\":2017},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6b14af216f6a667c4e03c6964babe828c680a05a\",\"title\":\"On the sample complexity of reinforcement learning.\",\"url\":\"https://www.semanticscholar.org/paper/6b14af216f6a667c4e03c6964babe828c680a05a\",\"venue\":\"\",\"year\":2003},{\"arxivId\":\"1507.00814\",\"authors\":[{\"authorId\":\"3275284\",\"name\":\"Bradly C. Stadie\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2470fcf0f89082de874ac9133ccb3a8667dd89a8\",\"title\":\"Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models\",\"url\":\"https://www.semanticscholar.org/paper/2470fcf0f89082de874ac9133ccb3a8667dd89a8\",\"venue\":\"ArXiv\",\"year\":2015},{\"arxivId\":\"1511.06581\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"title\":\"Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1016/b978-1-55860-141-3.50030-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b5f8a0858fb82ce0e50b55446577a70e40137aaf\",\"title\":\"Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/b5f8a0858fb82ce0e50b55446577a70e40137aaf\",\"venue\":\"ML\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145116464\",\"name\":\"J. Z. Kolter\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1553374.1553441\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4282669947ca340de9f93a9a2a38007fe542195d\",\"title\":\"Near-Bayesian exploration in polynomial time\",\"url\":\"https://www.semanticscholar.org/paper/4282669947ca340de9f93a9a2a38007fe542195d\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36359915\",\"name\":\"M. Panella\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9ff15528cbb9c47bb2324d0299c76bf331994882\",\"title\":\"Associate Editor of the Journal of Computer and System Sciences\",\"url\":\"https://www.semanticscholar.org/paper/9ff15528cbb9c47bb2324d0299c76bf331994882\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/j.jcss.2007.08.009\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"237a1cf18ed83bb3ad852b34f443c6c1ff3336c1\",\"title\":\"An analysis of model-based Interval Estimation for Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/237a1cf18ed83bb3ad852b34f443c6c1ff3336c1\",\"venue\":\"J. Comput. Syst. Sci.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T L Lai Andherbertrobbins\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"572379c1d56e7e19422ae38218ee228c61aefb2f\",\"title\":\"Asymptotically Efficient Adaptive Allocation Rules\",\"url\":\"https://www.semanticscholar.org/paper/572379c1d56e7e19422ae38218ee228c61aefb2f\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1611.04717\",\"authors\":[{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"22966825\",\"name\":\"Davis Foote\"},{\"authorId\":\"47541311\",\"name\":\"Adam Stooke\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf\",\"title\":\"#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741147143\",\"name\":\"Vijay Kumar Kaul\"}],\"doi\":\"10.2307/j.ctt5hh7c0.12\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff62aa2bfd3b4db63307c8a684e3c4edb39d990c\",\"title\":\"Planning\",\"url\":\"https://www.semanticscholar.org/paper/ff62aa2bfd3b4db63307c8a684e3c4edb39d990c\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Zico Kolter\"},{\"authorId\":null,\"name\":\"Andrew Y Ng. Near-Bayesian exploration in polynomial time\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 513\\u2013520,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015}],\"title\":\"Count-Based Exploration in Feature Space for Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Feature vector\",\"topicId\":\"4255\",\"url\":\"https://www.semanticscholar.org/topic/4255\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"State space\",\"topicId\":\"6115\",\"url\":\"https://www.semanticscholar.org/topic/6115\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Approximation\",\"topicId\":\"3247\",\"url\":\"https://www.semanticscholar.org/topic/3247\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Analysis of algorithms\",\"topicId\":\"13372\",\"url\":\"https://www.semanticscholar.org/topic/13372\"},{\"topic\":\"Nonlinear system\",\"topicId\":\"5329\",\"url\":\"https://www.semanticscholar.org/topic/5329\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"},{\"topic\":\"Scalability\",\"topicId\":\"1360\",\"url\":\"https://www.semanticscholar.org/topic/1360\"},{\"topic\":\"Phantasie\",\"topicId\":\"361667\",\"url\":\"https://www.semanticscholar.org/topic/361667\"}],\"url\":\"https://www.semanticscholar.org/paper/0f810eb4777fd05317951ebaa7a3f5835ee84cf4\",\"venue\":\"IJCAI\",\"year\":2017}\n"