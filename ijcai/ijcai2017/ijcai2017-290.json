"{\"abstract\":\"In this paper, we highlight our recent work [9] considering the safe learning scenario where we need to restrict the exploratory behavior of a reinforcement learning agent. Specifically, we treat the problem as a form of Bayesian reinforcement learning (BRL) in an environment that is modeled as a constrained MDP (CMDP) where the cost function penalizes undesirable situations. We propose a model-based BRL algorithm for such an environment, eliciting risk-sensitive exploration in a principled way. Our algorithm efficiently solves the constrained BRL problem by approximate linear programming, and generates a finite state controller in an off-line manner. We provide theoretical guarantees and demonstrate empirically that our approach outperforms the state of the art.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"38726140\",\"name\":\"Jongmin Lee\",\"url\":\"https://www.semanticscholar.org/author/38726140\"},{\"authorId\":\"48595109\",\"name\":\"Youngsoo Jang\",\"url\":\"https://www.semanticscholar.org/author/48595109\"},{\"authorId\":\"1807041\",\"name\":\"P. Poupart\",\"url\":\"https://www.semanticscholar.org/author/1807041\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\",\"url\":\"https://www.semanticscholar.org/author/1741330\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1802.08331\",\"authors\":[{\"authorId\":\"145509577\",\"name\":\"A. Cohen\"},{\"authorId\":\"47785308\",\"name\":\"L. Yu\"},{\"authorId\":\"144766657\",\"name\":\"R. Wright\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a033cd88249be20b1457d5b580375995414c3cfe\",\"title\":\"Diverse Exploration for Fast and Safe Policy Improvement\",\"url\":\"https://www.semanticscholar.org/paper/a033cd88249be20b1457d5b580375995414c3cfe\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145437032\",\"name\":\"Shitian Shen\"},{\"authorId\":\"51039532\",\"name\":\"Markel Sanz Ausin\"},{\"authorId\":\"3159445\",\"name\":\"B. Mostafavi\"},{\"authorId\":\"1731937\",\"name\":\"Min Chi\"}],\"doi\":\"10.1145/3209219.3209232\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e095ed60e0b2e730ea8eb6c472b5d2811983698\",\"title\":\"Improving Learning & Reducing Time: A Constrained Action-Based Reinforcement Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/7e095ed60e0b2e730ea8eb6c472b5d2811983698\",\"venue\":\"UMAP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2863060\",\"name\":\"F. D. Nijs\"}],\"doi\":\"10.4233/UUID:89C0F1A2-D19F-4466-9CC5-52AEB3950E53\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"53675eb969ad590ff59eaa5d7ccbbc950ad2496b\",\"title\":\"Resource-constrained Multi-agent Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/53675eb969ad590ff59eaa5d7ccbbc950ad2496b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49685598\",\"name\":\"JongMin Lee\"},{\"authorId\":\"40379031\",\"name\":\"G. Kim\"},{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"147cd12a7e39d61f9a1bafc0c16fd570b6206002\",\"title\":\"Monte-Carlo Tree Search for Constrained MDPs\",\"url\":\"https://www.semanticscholar.org/paper/147cd12a7e39d61f9a1bafc0c16fd570b6206002\",\"venue\":\"IJCAI 2018\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38726140\",\"name\":\"Jongmin Lee\"},{\"authorId\":\"40379031\",\"name\":\"G. Kim\"},{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"49d4adfdcbfca7aab796a97287f3a00d1988fbea\",\"title\":\"Monte-Carlo Tree Search for Constrained POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/49d4adfdcbfca7aab796a97287f3a00d1988fbea\",\"venue\":\"NeurIPS\",\"year\":2018}],\"corpusId\":20370721,\"doi\":\"10.24963/ijcai.2017/290\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"015012b607e77645d6bfa5894b8cdbdcf62cf260\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"31764087\",\"name\":\"M. Wilkinson\"}],\"doi\":\"10.1038/sj.bdj.4806810\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70f6f070f45afdd58b85cafde3a2e9ea02f3fcdb\",\"title\":\"Management science\",\"url\":\"https://www.semanticscholar.org/paper/70f6f070f45afdd58b85cafde3a2e9ea02f3fcdb\",\"venue\":\"British Dental Journal\",\"year\":1989},{\"arxivId\":\"1402.0560\",\"authors\":[{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"}],\"doi\":\"10.1613/jair.3761\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1bc8a129675fbbb651d0a23f827ab867165088f9\",\"title\":\"Safe Exploration of State and Action Spaces in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1bc8a129675fbbb651d0a23f827ab867165088f9\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2012},{\"arxivId\":\"1311.2097\",\"authors\":[{\"authorId\":\"48234901\",\"name\":\"Y. Shen\"},{\"authorId\":\"2485388\",\"name\":\"Michael J. Tobia\"},{\"authorId\":\"1746788\",\"name\":\"T. Sommer\"},{\"authorId\":\"1743272\",\"name\":\"K. Obermayer\"}],\"doi\":\"10.1162/NECO_a_00600\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0c4b7568f378e652645232e66a1dab4c5b5293f\",\"title\":\"Risk-Sensitive Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f0c4b7568f378e652645232e66a1dab4c5b5293f\",\"venue\":\"Neural Computation\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Dearden\"},{\"authorId\":null,\"name\":\"Nir Friedman\"},{\"authorId\":null,\"name\":\"Stuart Russell. Bayesian Q-learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 15th National Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 761\\u2013768,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dongho Kim\"},{\"authorId\":null,\"name\":\"Jaesong Lee\"},{\"authorId\":null,\"name\":\"Pascal Poupart\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Iyengar . Robust dynamic programming\",\"url\":\"\",\"venue\":\"Mathematics of Operations Research\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143654598\",\"name\":\"R. Dearden\"},{\"authorId\":\"50785579\",\"name\":\"N. Friedman\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b90b5b0cf05bc63f768f55322381f5cfbee6ce1c\",\"title\":\"Bayesian Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/b90b5b0cf05bc63f768f55322381f5cfbee6ce1c\",\"venue\":\"AAAI/IAAI\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948478\",\"name\":\"M. Strens\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"48cce5ee49facf75eeb12832c387452424b645dd\",\"title\":\"A Bayesian Framework for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/48cce5ee49facf75eeb12832c387452424b645dd\",\"venue\":\"ICML\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c0f2c4104ef6e36bb67022001179887e6600d24d\",\"title\":\"A comprehensive survey on safe reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/c0f2c4104ef6e36bb67022001179887e6600d24d\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47191491\",\"name\":\"M. O. Duff\"},{\"authorId\":\"9070053\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6932937c3ac9e6e42c78f0e214445d017486542f\",\"title\":\"Optimal learning: computational procedures for bayes-adaptive markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/6932937c3ac9e6e42c78f0e214445d017486542f\",\"venue\":\"\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Zico Kolter\"},{\"authorId\":null,\"name\":\"Andrew Y. Ng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Oliver Mihatsch and Ralph Neuneier . Risksensitive reinforcement learning\",\"url\":\"\",\"venue\":\"Machine Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40146204\",\"name\":\"Shay B. Cohen\"},{\"authorId\":\"143707114\",\"name\":\"M. Collins\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"242dff90c373e1d1b7884b7ab2596895438d04df\",\"title\":\"Advances in Neural Information Processing Systems 25\",\"url\":\"https://www.semanticscholar.org/paper/242dff90c373e1d1b7884b7ab2596895438d04df\",\"venue\":\"NIPS 2012\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"100655694\",\"name\":\"A. Moore\"}],\"doi\":\"10.1145/1143844\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"title\":\"Proceedings of the 23rd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"venue\":\"ICML 2008\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152175853\",\"name\":\"L. Goddard\"}],\"doi\":\"10.1038/222304c0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60ec5ddae05190537c72974b8f93beb46b8db857\",\"title\":\"Operations Research\",\"url\":\"https://www.semanticscholar.org/paper/60ec5ddae05190537c72974b8f93beb46b8db857\",\"venue\":\"Nature\",\"year\":1969},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722628\",\"name\":\"A. Nilim\"},{\"authorId\":\"1701847\",\"name\":\"L. Ghaoui\"}],\"doi\":\"10.1287/opre.1050.0216\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6db16608fccddef51202af84112b34cfebfbe20a\",\"title\":\"Robust Control of Markov Decision Processes with Uncertain Transition Matrices\",\"url\":\"https://www.semanticscholar.org/paper/6db16608fccddef51202af84112b34cfebfbe20a\",\"venue\":\"Oper. Res.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37202259\",\"name\":\"J. Meigs\"}],\"doi\":\"10.1515/cclm.1994.32.8.631\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c3391bde2bb1b3d737913ee8caa01492a782732\",\"title\":\"WHO Technical Report\",\"url\":\"https://www.semanticscholar.org/paper/5c3391bde2bb1b3d737913ee8caa01492a782732\",\"venue\":\"The Yale Journal of Biology and Medicine\",\"year\":1954},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Dearden\"},{\"authorId\":null,\"name\":\"Nir Friedman\"},{\"authorId\":null,\"name\":\"David Andre. Model based Bayesian exploration\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 150\\u2013159,\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jongmin Lee\"},{\"authorId\":null,\"name\":\"Youngsoo Jang\"},{\"authorId\":null,\"name\":\"Pascal Poupart\"},{\"authorId\":null,\"name\":\"Kee-Eung Kim\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Constrained Bayesian reinforcement learning via approximate linear programming\",\"url\":\"\",\"venue\":\"In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1734775\",\"name\":\"E. Altman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7\",\"title\":\"Constrained Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/3cc2608fd77b9b65f5bd378e8797b2ab1b8acde7\",\"venue\":\"\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alexander Hans\"},{\"authorId\":null,\"name\":\"Daniel Schneega\"},{\"authorId\":null,\"name\":\"M Anton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Sch\\u00e4fer , and Steffen Udluft . Safe exploration for reinforcement learning\",\"url\":\"\",\"venue\":\"Proceedings of the European Symposium on Artificial Neural Network\",\"year\":null},{\"arxivId\":\"1301.6690\",\"authors\":[{\"authorId\":\"143654598\",\"name\":\"R. Dearden\"},{\"authorId\":\"50785579\",\"name\":\"N. Friedman\"},{\"authorId\":\"144980509\",\"name\":\"D. Andre\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"30a8d1e9c37969034e4823332a05de8536e4fded\",\"title\":\"Model based Bayesian Exploration\",\"url\":\"https://www.semanticscholar.org/paper/30a8d1e9c37969034e4823332a05de8536e4fded\",\"venue\":\"UAI\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Dearden\"},{\"authorId\":null,\"name\":\"Nir Friedman\"},{\"authorId\":null,\"name\":\"Stuart Russell.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bayesian Qlearning\",\"url\":\"\",\"venue\":\"Proceedings of the 15 th National Conference on Artificial Intelligence\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Thomas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Minka . Bayesian linear regression\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145570534\",\"name\":\"G. Iyengar\"}],\"doi\":\"10.1287/MOOR.1040.0129\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ab4a1c4dfe23b3a1e3d077df467452cc68f64de8\",\"title\":\"Robust Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/ab4a1c4dfe23b3a1e3d077df467452cc68f64de8\",\"venue\":\"Math. Oper. Res.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dongho Kim\"},{\"authorId\":null,\"name\":\"Jaesong Lee\"},{\"authorId\":null,\"name\":\"Kee-Eung Kim\"},{\"authorId\":null,\"name\":\"Pascal Poupart. Point-based value iteration for constrain pomdps\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 1968\\u20131974,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"N. Garud\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Iyengar . Robust dynamic program\",\"url\":\"\",\"venue\":\"Mathematics of Operations Research\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2972848\",\"name\":\"Alexander Hans\"},{\"authorId\":\"2766932\",\"name\":\"Daniel Schneega\\u00df\"},{\"authorId\":\"34962728\",\"name\":\"Anton Maximilian Sch\\u00e4fer\"},{\"authorId\":\"1699265\",\"name\":\"S. Udluft\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5ee27e9db2ae248d1254107852311117c4cda1c9\",\"title\":\"Safe exploration for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/5ee27e9db2ae248d1254107852311117c4cda1c9\",\"venue\":\"ESANN\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144575699\",\"name\":\"S. Vijayakumar\"},{\"authorId\":\"49250809\",\"name\":\"S. Schaal\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"104e91be65dcf5987e4b3e71d664046cb1a3ed3c\",\"title\":\"ICML '00 Proceedings of the Seventeenth International Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/104e91be65dcf5987e4b3e71d664046cb1a3ed3c\",\"venue\":\"\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"021f6ee86cfeebaf3ee02f52b43288c225d59631\",\"title\":\"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/021f6ee86cfeebaf3ee02f52b43288c225d59631\",\"venue\":\"UAI 2009\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3116807\",\"name\":\"O. Mihatsch\"},{\"authorId\":\"1759183\",\"name\":\"R. Neuneier\"}],\"doi\":\"10.1023/A:1017940631555\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"165f4a4bc8080093c89b8e9916b779aee8b37b81\",\"title\":\"Risk-Sensitive Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/165f4a4bc8080093c89b8e9916b779aee8b37b81\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J. Zico Kolter\"},{\"authorId\":null,\"name\":\"Andrew Y. Ng. Near-Bayesian exploration in polynomial time\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 26th Annual International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 513\\u2013520,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145116464\",\"name\":\"J. Z. Kolter\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1553374.1553441\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4282669947ca340de9f93a9a2a38007fe542195d\",\"title\":\"Near-Bayesian exploration in polynomial time\",\"url\":\"https://www.semanticscholar.org/paper/4282669947ca340de9f93a9a2a38007fe542195d\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144184704\",\"name\":\"Dongho Kim\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"},{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3cb8cb4698a110ac411e74919c84aa53dae46189\",\"title\":\"Cost-Sensitive Exploration in Bayesian Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3cb8cb4698a110ac411e74919c84aa53dae46189\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael O\\u2019Gordon Duff\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Near - optimal BRL using optimistic local transition A Bayesian sampling approach to exploration in reinforcement learning Bayesian Q - learning Model based Bayesian exploration\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Hsu\"},{\"authorId\":null,\"name\":\"Wee Sun Lee\"},{\"authorId\":null,\"name\":\"Nan Rong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Howard and James E . Matheson . Risk - sensitive markov decision processes\",\"url\":\"\",\"venue\":\"Management Science\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35335083\",\"name\":\"R. Thrall\"}],\"doi\":\"10.21236/ada049700\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b4b9507d9ff065cc5a3d4a30d604eb4ec41bfe2\",\"title\":\"Mathematics of Operations Research.\",\"url\":\"https://www.semanticscholar.org/paper/1b4b9507d9ff065cc5a3d4a30d604eb4ec41bfe2\",\"venue\":\"\",\"year\":1978},{\"arxivId\":\"1205.2664\",\"authors\":[{\"authorId\":\"35775902\",\"name\":\"J. Asmuth\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"2758123\",\"name\":\"Ali Nouri\"},{\"authorId\":\"30585164\",\"name\":\"D. Wingate\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"606c3108fe948d9a8a0da8759f88de4df53b5d94\",\"title\":\"A Bayesian Sampling Approach to Exploration in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/606c3108fe948d9a8a0da8759f88de4df53b5d94\",\"venue\":\"UAI\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145463096\",\"name\":\"D. Hsu\"},{\"authorId\":\"1740222\",\"name\":\"Wee Sun Lee\"},{\"authorId\":\"1927140\",\"name\":\"N. Rong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e72c2a9e19d90ab6a8f2c239ef00b97d8233a13\",\"title\":\"What makes some POMDP problems easy to approximate?\",\"url\":\"https://www.semanticscholar.org/paper/7e72c2a9e19d90ab6a8f2c239ef00b97d8233a13\",\"venue\":\"NIPS\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2223282\",\"name\":\"Nikolaos Tziortziotis\"},{\"authorId\":\"1720480\",\"name\":\"Christos Dimitrakakis\"},{\"authorId\":\"3106211\",\"name\":\"K. Blekas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"22e54a6249027425bf7351af922ff94b05c4100f\",\"title\":\"Linear Bayesian Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/22e54a6249027425bf7351af922ff94b05c4100f\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"26662080\",\"name\":\"T. Tang\"},{\"authorId\":\"144423965\",\"name\":\"A. Murray\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a62eca0d34c41de74b52434afa6ce139e7a6452\",\"title\":\"Proceedings of the European Symposium on Artificial Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/0a62eca0d34c41de74b52434afa6ce139e7a6452\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"},{\"authorId\":\"8109317\",\"name\":\"Aarti Malhotra\"},{\"authorId\":\"46905069\",\"name\":\"P. Pei\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"},{\"authorId\":\"2717154\",\"name\":\"Bongseok Goh\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7190d3e96e02e515b2cc5cb67375002a17a0cfa2\",\"title\":\"Approximate Linear Programming for Constrained Partially Observable Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/7190d3e96e02e515b2cc5cb67375002a17a0cfa2\",\"venue\":\"AAAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"33758038\",\"name\":\"S. Marcus\"},{\"authorId\":\"1403242509\",\"name\":\"E. Fern\\u00e1ndez-Gaucherand\"},{\"authorId\":\"83203043\",\"name\":\"D. Hern\\u00e1ndez-Hern\\u00e1ndez\"},{\"authorId\":\"50723844\",\"name\":\"S. Coraluppi\"},{\"authorId\":\"41232425\",\"name\":\"Pedram J. Fard\"}],\"doi\":\"10.1007/978-1-4612-4120-1_14\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"43c3a0a759a530b6206f6ecfe66dfaa3b0a88a19\",\"title\":\"Risk Sensitive Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/43c3a0a759a530b6206f6ecfe66dfaa3b0a88a19\",\"venue\":\"\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007},{\"arxivId\":\"1206.4613\",\"authors\":[{\"authorId\":\"1404121979\",\"name\":\"Mauricio Araya-L\\u00f3pez\"},{\"authorId\":\"1776632\",\"name\":\"O. Buffet\"},{\"authorId\":\"49390231\",\"name\":\"Vincent Thomas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"059db4c99f3bfd526ac070549ee3f2906a9662e0\",\"title\":\"Near-Optimal BRL using Optimistic Local Transitions\",\"url\":\"https://www.semanticscholar.org/paper/059db4c99f3bfd526ac070549ee3f2906a9662e0\",\"venue\":\"ICML\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"},{\"authorId\":\"31651045\",\"name\":\"N. Vlassis\"},{\"authorId\":\"145803385\",\"name\":\"J. Hoey\"},{\"authorId\":\"144389412\",\"name\":\"Kevin Regan\"}],\"doi\":\"10.1145/1143844.1143932\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"40e4d0eaff2ce7538ecff773bda326388a87b515\",\"title\":\"An analytic solution to discrete Bayesian reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/40e4d0eaff2ce7538ecff773bda326388a87b515\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jongmin Lee\"},{\"authorId\":null,\"name\":\"Youngsoo Jang\"},{\"authorId\":null,\"name\":\"Pascal Poupart\"},{\"authorId\":null,\"name\":\"Kee-Eung Kim\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"strained Bayesian reinforcement learning via approximate linear programming\",\"url\":\"\",\"venue\":\"Proceedings of the Twenty - Sixth International Joint Conference on Artificial Intelligence\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70072509\",\"name\":\"G. M. Walter\"},{\"authorId\":\"1843151\",\"name\":\"Thomas Augustin\"}],\"doi\":\"10.5282/UBM/EPUB.11050\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"929adb1693090be52b328dc0a36f81d35de65ee1\",\"title\":\"Bayesian linear regression\",\"url\":\"https://www.semanticscholar.org/paper/929adb1693090be52b328dc0a36f81d35de65ee1\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144184704\",\"name\":\"Dongho Kim\"},{\"authorId\":\"3017896\",\"name\":\"Jaesong Lee\"},{\"authorId\":\"1741330\",\"name\":\"Kee-Eung Kim\"},{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"}],\"doi\":\"10.5591/978-1-57735-516-8/IJCAI11-329\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"20aebf66e68b517c63fc48045324f7a5342f9c2b\",\"title\":\"Point-Based Value Iteration for Constrained POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/20aebf66e68b517c63fc48045324f7a5342f9c2b\",\"venue\":\"IJCAI\",\"year\":2011}],\"title\":\"Constrained Bayesian Reinforcement Learning via Approximate Linear Programming\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Linear programming\",\"topicId\":\"12656\",\"url\":\"https://www.semanticscholar.org/topic/12656\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"},{\"topic\":\"Approximation error\",\"topicId\":\"87641\",\"url\":\"https://www.semanticscholar.org/topic/87641\"},{\"topic\":\"Time complexity\",\"topicId\":\"3448\",\"url\":\"https://www.semanticscholar.org/topic/3448\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"},{\"topic\":\"Scalability\",\"topicId\":\"1360\",\"url\":\"https://www.semanticscholar.org/topic/1360\"},{\"topic\":\"Online and offline\",\"topicId\":\"12094\",\"url\":\"https://www.semanticscholar.org/topic/12094\"},{\"topic\":\"Genetic algorithm\",\"topicId\":\"2069\",\"url\":\"https://www.semanticscholar.org/topic/2069\"},{\"topic\":\"Loss function\",\"topicId\":\"3650\",\"url\":\"https://www.semanticscholar.org/topic/3650\"}],\"url\":\"https://www.semanticscholar.org/paper/015012b607e77645d6bfa5894b8cdbdcf62cf260\",\"venue\":\"IJCAI\",\"year\":2017}\n"