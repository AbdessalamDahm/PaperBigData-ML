"{\"abstract\":\"Reinforcement learning has had many successes, but in practice it often requires significant amounts of data to learn high-performing policies. One common way to improve learning is to allow a trained (source) agent to assist a new (target) agent. The goals in this setting are to 1) improve the target agent\\u2019s performance, relative to learning unaided, and 2) allow the target agent to outperform the source agent. Our approach leverages source agent demonstrations, removing any requirements on the source agent\\u2019s learning algorithm or representation. The target agent then estimates the source agent\\u2019s policy and improves upon it. The key contribution of this work is to show that leveraging the target agent\\u2019s uncertainty in the source agent\\u2019s policy can significantly improve learning in two complex simulated domains, Keepaway and Mario.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2870663\",\"name\":\"Zhaodong Wang\",\"url\":\"https://www.semanticscholar.org/author/2870663\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\",\"url\":\"https://www.semanticscholar.org/author/39286677\"}],\"citationVelocity\":8,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145902797\",\"name\":\"B. Banerjee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd88f5ca3751404e6929b49d65397ab339b00f7f\",\"title\":\"Coordination Confidence based Human-Multi-Agent Transfer Learning for Collaborative Teams\",\"url\":\"https://www.semanticscholar.org/paper/bd88f5ca3751404e6929b49d65397ab339b00f7f\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1907.04957\",\"authors\":[{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"1925969\",\"name\":\"M. Murray\"},{\"authorId\":\"49258625\",\"name\":\"Maya Cakmak\"},{\"authorId\":\"1982950\",\"name\":\"Luke Zettlemoyer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1b9ce27801c077433245ad0f9e43e3c38441cecd\",\"title\":\"Vision-and-Dialog Navigation\",\"url\":\"https://www.semanticscholar.org/paper/1b9ce27801c077433245ad0f9e43e3c38441cecd\",\"venue\":\"CoRL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2557664\",\"name\":\"Yanhai Xiong\"},{\"authorId\":\"39674336\",\"name\":\"Haipeng Chen\"},{\"authorId\":\"3386549\",\"name\":\"Mengchen Zhao\"},{\"authorId\":\"143706345\",\"name\":\"Bo An\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c343e3fed1460e41b9821ce8cf64aa802004458\",\"title\":\"HogRider: Champion Agent of Microsoft Malmo Collaborative AI Challenge\",\"url\":\"https://www.semanticscholar.org/paper/3c343e3fed1460e41b9821ce8cf64aa802004458\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145902797\",\"name\":\"B. Banerjee\"},{\"authorId\":\"148120358\",\"name\":\"Sneha Racharla\"}],\"doi\":\"10.1017/S0269888920000387\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cc2371871eaa5e3a510e588361bf4a4602867ed\",\"title\":\"Human Agent Transfer from Observations\",\"url\":\"https://www.semanticscholar.org/paper/7cc2371871eaa5e3a510e588361bf4a4602867ed\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40963426\",\"name\":\"Sriram Ganapathi Subramanian\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"26c79dff7bbb88f5aba1c43c50daaf9dc3e7c468\",\"title\":\"A Complementary Approach to Improve Wild Fire Prediction Systems\",\"url\":\"https://www.semanticscholar.org/paper/26c79dff7bbb88f5aba1c43c50daaf9dc3e7c468\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119890891\",\"name\":\"Changxi Zhu\"},{\"authorId\":\"1492164937\",\"name\":\"Yi Cai\"},{\"authorId\":\"1701688\",\"name\":\"Ho-fung Leung\"},{\"authorId\":\"1883634\",\"name\":\"S. Hu\"}],\"doi\":\"10.5555/3398761.3398953\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"535a638c549a5938404f6e82a8ac92afae0db2ba\",\"title\":\"Learning by Reusing Previous Advice in Teacher-Student Paradigm\",\"url\":\"https://www.semanticscholar.org/paper/535a638c549a5938404f6e82a8ac92afae0db2ba\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8392935\",\"name\":\"Mengting Li\"},{\"authorId\":\"150352796\",\"name\":\"Yi Wei\"},{\"authorId\":\"2380005\",\"name\":\"Daniel Kudenko\"}],\"doi\":\"10.1017/s0269888919000092\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"98a01d460b2b9d81a4963a22aa6d504258b15d3c\",\"title\":\"Two-level Q-learning: learning from conflict demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/98a01d460b2b9d81a4963a22aa6d504258b15d3c\",\"venue\":\"Knowledge Eng. Review\",\"year\":2019},{\"arxivId\":\"1909.04121\",\"authors\":[{\"authorId\":\"48578738\",\"name\":\"A. Kurenkov\"},{\"authorId\":\"49686756\",\"name\":\"Ajay Mandlekar\"},{\"authorId\":\"34172291\",\"name\":\"R. Martin\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"1873736\",\"name\":\"Animesh Garg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea074efa65609eabd6b345f724ec814579329092\",\"title\":\"AC-Teach: A Bayesian Actor-Critic Method for Policy Learning with an Ensemble of Suboptimal Teachers\",\"url\":\"https://www.semanticscholar.org/paper/ea074efa65609eabd6b345f724ec814579329092\",\"venue\":\"CoRL\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50651825\",\"name\":\"M. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9a972900982ca55cc74c40bf6e0e3ad11faaa242\",\"title\":\"Reinforcement learning from multiple experts demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/9a972900982ca55cc74c40bf6e0e3ad11faaa242\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1709.04083\",\"authors\":[{\"authorId\":\"7547309\",\"name\":\"G. Cruz\"},{\"authorId\":\"24742854\",\"name\":\"Yunshu Du\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d915c26ddce144dd91b2670cc18495c3dcf6207\",\"title\":\"Pre-training Neural Networks with Human Demonstrations for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4d915c26ddce144dd91b2670cc18495c3dcf6207\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1903.05697\",\"authors\":[{\"authorId\":\"145619799\",\"name\":\"Sanjay Thakur\"},{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"2550467\",\"name\":\"J. Higuera\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":\"10.1109/ICRA.2019.8794328\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd278599c2bae09278e63d6473b4fa920fe86c01\",\"title\":\"Uncertainty Aware Learning from Demonstrations in Multiple Contexts using Bayesian Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/bd278599c2bae09278e63d6473b4fa920fe86c01\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":\"2006.05725\",\"authors\":[{\"authorId\":\"52225987\",\"name\":\"Michael Gimelfarb\"},{\"authorId\":\"1732536\",\"name\":\"S. Sanner\"},{\"authorId\":\"2944274\",\"name\":\"Chi-Guhn Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d6033f2f8aed093327307a987ffe1b67dd57651\",\"title\":\"Bayesian Experience Reuse for Learning from Multiple Demonstrators\",\"url\":\"https://www.semanticscholar.org/paper/2d6033f2f8aed093327307a987ffe1b67dd57651\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49177827\",\"name\":\"H. Chen\"}],\"doi\":\"10.32657/10356/74968\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7685fcaa7269f97c1758f6a4565dde4544e043db\",\"title\":\"Large scale strategic decision making in multi-agent systems\",\"url\":\"https://www.semanticscholar.org/paper/7685fcaa7269f97c1758f6a4565dde4544e043db\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1812.08904\",\"authors\":[{\"authorId\":\"7547309\",\"name\":\"G. Cruz\"},{\"authorId\":\"24742854\",\"name\":\"Yunshu Du\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1017/S0269888919000055\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ac1ad3a421c5db0d1f29b815359312a22357d40b\",\"title\":\"Pre-training with Non-expert Human Demonstration for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ac1ad3a421c5db0d1f29b815359312a22357d40b\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1795620\",\"name\":\"Sang Wan Lee\"},{\"authorId\":\"50736267\",\"name\":\"B. Seymour\"}],\"doi\":\"10.1016/j.cobeha.2018.12.012\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"034de0d088956bb8ebd100704ad7ac41933fa3c0\",\"title\":\"Decision-making in brains and robots \\u2014 the case for an interdisciplinary approach\",\"url\":\"https://www.semanticscholar.org/paper/034de0d088956bb8ebd100704ad7ac41933fa3c0\",\"venue\":\"Current Opinion in Behavioral Sciences\",\"year\":2019},{\"arxivId\":\"2006.16498\",\"authors\":[{\"authorId\":\"145548210\",\"name\":\"D. Xu\"},{\"authorId\":\"1776245\",\"name\":\"Mohit Agarwal\"},{\"authorId\":\"1730720\",\"name\":\"F. Fekri\"},{\"authorId\":\"145896661\",\"name\":\"R. Sivakumar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82827af87d95747097cb94dd7e246c8dcdc5efa0\",\"title\":\"Accelerating Reinforcement Learning Agent with EEG-based Implicit Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/82827af87d95747097cb94dd7e246c8dcdc5efa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145902797\",\"name\":\"B. Banerjee\"},{\"authorId\":\"134768818\",\"name\":\"Syamala Nanditha Vittanala\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1017/S0269888919000043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03e78b926213b4f618753e7860c9a71a0090d86c\",\"title\":\"Team learning from human demonstration with coordination confidence\",\"url\":\"https://www.semanticscholar.org/paper/03e78b926213b4f618753e7860c9a71a0090d86c\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2870663\",\"name\":\"Zhaodong Wang\"},{\"authorId\":\"48991910\",\"name\":\"Zhiwei Qin\"},{\"authorId\":\"1959758\",\"name\":\"Xiaocheng Tang\"},{\"authorId\":\"144030870\",\"name\":\"Jieping Ye\"},{\"authorId\":\"3280236\",\"name\":\"H. Zhu\"}],\"doi\":\"10.1109/ICDM.2018.00077\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a52f1d1873197fe220d5c2ee9c0eef29f80bb192\",\"title\":\"Deep Reinforcement Learning with Knowledge Transfer for Online Rides Order Dispatching\",\"url\":\"https://www.semanticscholar.org/paper/a52f1d1873197fe220d5c2ee9c0eef29f80bb192\",\"venue\":\"2018 IEEE International Conference on Data Mining (ICDM)\",\"year\":2018},{\"arxivId\":\"1909.04256\",\"authors\":[{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":\"10.24963/IJCAI.2019/557\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a5c0645162f4b74895b86a98db86df8af680de77\",\"title\":\"Transfer of Temporal Logic Formulas in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a5c0645162f4b74895b86a98db86df8af680de77\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145050960\",\"name\":\"F. Silva\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"}],\"doi\":\"10.1613/jair.1.11396\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3660f76126fe1343c91f065f452845981041206c\",\"title\":\"A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems\",\"url\":\"https://www.semanticscholar.org/paper/3660f76126fe1343c91f065f452845981041206c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"24742854\",\"name\":\"Yunshu Du\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144130066\",\"name\":\"A. Gebremedhin\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b01d3fd9bded0c8c5799eeec30a811569a5fa156\",\"title\":\"Work-in-progress: Corrected Self Imitation Learning via Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/b01d3fd9bded0c8c5799eeec30a811569a5fa156\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2001.06781\",\"authors\":[{\"authorId\":\"2797515\",\"name\":\"Baicen Xiao\"},{\"authorId\":\"153083396\",\"name\":\"Qifan Lu\"},{\"authorId\":\"31306695\",\"name\":\"B. Ramasubramanian\"},{\"authorId\":\"50502989\",\"name\":\"A. Clark\"},{\"authorId\":\"47932194\",\"name\":\"L. Bushnell\"},{\"authorId\":\"144786412\",\"name\":\"R. Poovendran\"}],\"doi\":\"10.5555/3398761.3398935\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"14b7d4a34ba869e106d82d658e2973163a11eb12\",\"title\":\"FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/14b7d4a34ba869e106d82d658e2973163a11eb12\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.24963/ijcai.2018/817\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2769ea248be3ccdcd72c6c1c2bcc55d496c779a0\",\"title\":\"Improving Reinforcement Learning with Human Input\",\"url\":\"https://www.semanticscholar.org/paper/2769ea248be3ccdcd72c6c1c2bcc55d496c779a0\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1904.02206\",\"authors\":[{\"authorId\":\"7547309\",\"name\":\"G. Cruz\"},{\"authorId\":\"24742854\",\"name\":\"Yunshu Du\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"685af16ecd0a9087c8b0871ff46bbc266383d79d\",\"title\":\"Jointly Pre-training with Supervised, Autoencoder, and Value Losses for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/685af16ecd0a9087c8b0871ff46bbc266383d79d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1812.02632\",\"authors\":[{\"authorId\":\"27555505\",\"name\":\"Si-An Chen\"},{\"authorId\":\"1801873\",\"name\":\"Voot Tangkaratt\"},{\"authorId\":\"1798966\",\"name\":\"Hsuan-Tien Lin\"},{\"authorId\":\"67154907\",\"name\":\"Masashi Sugiyama\"}],\"doi\":\"10.1007/s10994-019-05849-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"73b5c8b2e4d291a556c63c29b2033b63c0eeb10a\",\"title\":\"Active deep Q-learning with demonstration\",\"url\":\"https://www.semanticscholar.org/paper/73b5c8b2e4d291a556c63c29b2033b63c0eeb10a\",\"venue\":\"Machine Learning\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48691553\",\"name\":\"Su Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ccf403a5dccb6f32f29f5591602515e055f28f77\",\"title\":\"Enhanced Learning from Multiple Demonstrations with a Flexible Two-level Structure Approach\",\"url\":\"https://www.semanticscholar.org/paper/ccf403a5dccb6f32f29f5591602515e055f28f77\",\"venue\":\"AAMAS\",\"year\":2019}],\"corpusId\":27795769,\"doi\":\"10.24963/ijcai.2017/422\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"585182e93bc33daacfc7cf52744eae569e180c52\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1806591\",\"name\":\"G. Celeux\"},{\"authorId\":\"7663024\",\"name\":\"G. Govaert\"}],\"doi\":\"10.1016/0167-9473(92)90042-E\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8bce728b23956b5fdaa8d70d01ff40ee1f007082\",\"title\":\"A Classification EM algorithm for clustering and two stochastic versions\",\"url\":\"https://www.semanticscholar.org/paper/8bce728b23956b5fdaa8d70d01ff40ee1f007082\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3466704\",\"name\":\"Gavin Adrian Rummery\"},{\"authorId\":\"1697360\",\"name\":\"M. Niranjan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a09464f26e18a25a948baaa736270bfb84b5e12\",\"title\":\"On-line Q-learning using connectionist systems\",\"url\":\"https://www.semanticscholar.org/paper/7a09464f26e18a25a948baaa736270bfb84b5e12\",\"venue\":\"\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2463017\",\"name\":\"S. Karakovskiy\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/TCIAIG.2012.2188528\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"title\":\"The Mario AI Benchmark and Competitions\",\"url\":\"https://www.semanticscholar.org/paper/6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"145805766\",\"name\":\"G. Kuhlmann\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2394697\",\"name\":\"Y. Liu\"}],\"doi\":\"10.1007/11780519_9\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ca0ea624b068042ca34264ddaa5e42620ceee875\",\"title\":\"Keepaway Soccer: From Machine Learning Testbed to Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/ca0ea624b068042ca34264ddaa5e42620ceee875\",\"venue\":\"RoboCup\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1007/BF00992699\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"title\":\"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\"url\":\"https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71619391\",\"name\":\"\\u0e2d\\u0e19\\u0e34\\u0e23\\u0e38\\u0e18 \\u0e2a\\u0e37\\u0e1a\\u0e2a\\u0e34\\u0e07\\u0e2b\\u0e4c\"}],\"doi\":\"10.1016/c2009-0-19715-5\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b42b1bfdc262bf99e9484e2e9df94df216b96374\",\"title\":\"Data Mining Practical Machine Learning Tools and Techniques\",\"url\":\"https://www.semanticscholar.org/paper/b42b1bfdc262bf99e9484e2e9df94df216b96374\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2811287\",\"name\":\"Halit Bener Suay\"},{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2896f821c991824fc0bc96949e4baedc37fee06a\",\"title\":\"Learning from Demonstration for Shaping through Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2896f821c991824fc0bc96949e4baedc37fee06a\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Long-Ji Lin. Self-improving reactive agents based on reinf learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"planning and teaching\",\"url\":\"\",\"venue\":\"Machine learning, 8(3-4):293\\u2013321,\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Halit Bener Suay\"},{\"authorId\":null,\"name\":\"Sonia Chernova. Integrating reinforcement learning with ability\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS)\",\"url\":\"\",\"venue\":\"May\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735414\",\"name\":\"T. Knasel\"}],\"doi\":\"10.1016/0921-8890(88)90002-4\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"edd77f310393f521669b209cbb6828fb45a8485d\",\"title\":\"Robotics and autonomous systems\",\"url\":\"https://www.semanticscholar.org/paper/edd77f310393f521669b209cbb6828fb45a8485d\",\"venue\":\"Robotics Auton. Syst.\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35193947\",\"name\":\"I. Noda\"},{\"authorId\":\"48084745\",\"name\":\"Hitoshi Matsubara\"},{\"authorId\":\"1805464\",\"name\":\"K. Hiraki\"},{\"authorId\":\"144481866\",\"name\":\"I. Frank\"}],\"doi\":\"10.1080/088395198117848\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1c7fbf3abf6c97d2b9d534d22e31452bc5499fa7\",\"title\":\"Soccer Server: A Tool for Research on Multiagent Systems\",\"url\":\"https://www.semanticscholar.org/paper/1c7fbf3abf6c97d2b9d534d22e31452bc5499fa7\",\"venue\":\"Appl. Artif. Intell.\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas G Dietterich. Ensemble methods in machine learning systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 1\\u201315\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145528658\",\"name\":\"G. Kendall\"}],\"doi\":\"10.1109/TCIAIG.2015.2409514\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9046f46d088eee7be4af8be5ffea602394a937c0\",\"title\":\"Editorial: IEEE Transactions on Computational Intelligence and AI in Games\",\"url\":\"https://www.semanticscholar.org/paper/9046f46d088eee7be4af8be5ffea602394a937c0\",\"venue\":\"IEEE Trans. Comput. Intell. AI Games\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sonia Chernova\"},{\"authorId\":null,\"name\":\"Manuela Veloso. Interactive policy learning through confid autonomy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"34(1):1,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"JS Albus\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Brains\",\"url\":\"\",\"venue\":\"behavior. & Robotics. Peterboro, NH: Byte Books\",\"year\":1981},{\"arxivId\":\"1401.3439\",\"authors\":[{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1613/jair.2584\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e40a0969aac73d50485c05de7f1c0ab081d77028\",\"title\":\"Interactive Policy Learning through Confidence-Based Autonomy\",\"url\":\"https://www.semanticscholar.org/paper/e40a0969aac73d50485c05de7f1c0ab081d77028\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52319496\",\"name\":\"ScienceDirect\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"573a8b7cb402f923557f185ae2a7533e7982c342\",\"title\":\"Computational statistics & data analysis\",\"url\":\"https://www.semanticscholar.org/paper/573a8b7cb402f923557f185ae2a7533e7982c342\",\"venue\":\"\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1018012322525\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c21accc28f1ab02404948ca4c315ef4a8596ed1\",\"title\":\"Reinforcement Learning with Replacing Eligibility Traces\",\"url\":\"https://www.semanticscholar.org/paper/4c21accc28f1ab02404948ca4c315ef4a8596ed1\",\"venue\":\"Machine Learning\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007}],\"title\":\"Improving Reinforcement Learning with Confidence-Based Demonstrations\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Instant messaging\",\"topicId\":\"44006\",\"url\":\"https://www.semanticscholar.org/topic/44006\"},{\"topic\":\"RL (complexity)\",\"topicId\":\"3597734\",\"url\":\"https://www.semanticscholar.org/topic/3597734\"},{\"topic\":\"Computer performance\",\"topicId\":\"122225\",\"url\":\"https://www.semanticscholar.org/topic/122225\"},{\"topic\":\"Requirement\",\"topicId\":\"136\",\"url\":\"https://www.semanticscholar.org/topic/136\"},{\"topic\":\"Robot learning\",\"topicId\":\"214893\",\"url\":\"https://www.semanticscholar.org/topic/214893\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Converge\",\"topicId\":\"205534\",\"url\":\"https://www.semanticscholar.org/topic/205534\"},{\"topic\":\"Unbalanced circuit\",\"topicId\":\"4990362\",\"url\":\"https://www.semanticscholar.org/topic/4990362\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"}],\"url\":\"https://www.semanticscholar.org/paper/585182e93bc33daacfc7cf52744eae569e180c52\",\"venue\":\"IJCAI\",\"year\":2017}\n"