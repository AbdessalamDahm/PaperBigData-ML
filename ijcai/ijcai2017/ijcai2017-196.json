"{\"abstract\":\"In the context of hierarchical reinforcement learning, the idea of hierarchies of abstract machines (HAMs) is to write a partial policy as a set of hierarchical finite state machines with unspecified choice states, and use reinforcement learning to learn an optimal completion of this partial policy. Given a HAM with deep hierarchical structure, there often exist many internal transitions where a machine calls another machine with the environment state unchanged. In this paper, we propose a new hierarchical reinforcement learning algorithm that automatically discovers such internal transitions, and shortcircuits them recursively in the computation of Q values. The resulting HAMQ-INT algorithm outperforms the state of the art significantly on the benchmark Taxi domain and a much more complex RoboCup Keepaway domain.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"39934673\",\"name\":\"A. Bai\",\"url\":\"https://www.semanticscholar.org/author/39934673\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\",\"url\":\"https://www.semanticscholar.org/author/145107462\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"31979719\",\"name\":\"Jarrett Holtz\"},{\"authorId\":\"2100712\",\"name\":\"Arjun Guha\"},{\"authorId\":\"3045593\",\"name\":\"Joydeep Biswas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32b560cf2eb8be578313900c0ca4065ca2bbefb3\",\"title\":\"Demo : Interactive Robot Transition Repair Demonstration\",\"url\":\"https://www.semanticscholar.org/paper/32b560cf2eb8be578313900c0ca4065ca2bbefb3\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1802.01706\",\"authors\":[{\"authorId\":\"31979719\",\"name\":\"Jarrett Holtz\"},{\"authorId\":\"2100712\",\"name\":\"Arjun Guha\"},{\"authorId\":\"3045593\",\"name\":\"J. Biswas\"}],\"doi\":\"10.24963/ijcai.2018/681\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"20a4b4c8ea83393f06a17320b2ea0f8d8298dc59\",\"title\":\"Interactive Robot Transition Repair With SMT\",\"url\":\"https://www.semanticscholar.org/paper/20a4b4c8ea83393f06a17320b2ea0f8d8298dc59\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1912.07544\",\"authors\":[{\"authorId\":\"144214684\",\"name\":\"J. Winder\"},{\"authorId\":\"144177520\",\"name\":\"S. Milani\"},{\"authorId\":\"52209217\",\"name\":\"Matthew Landen\"},{\"authorId\":\"1466551011\",\"name\":\"Erebus Oh\"},{\"authorId\":\"1468844871\",\"name\":\"Shane Parr\"},{\"authorId\":\"50460087\",\"name\":\"S. Squire\"},{\"authorId\":\"144980202\",\"name\":\"M. desJardins\"},{\"authorId\":\"2674440\",\"name\":\"Cynthia Matuszek\"}],\"doi\":\"10.1609/AAAI.V34I06.6555\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff50006801efdbcd38163bc87c761210506987f4\",\"title\":\"Planning with Abstract Learned Models While Learning Transferable Subtasks\",\"url\":\"https://www.semanticscholar.org/paper/ff50006801efdbcd38163bc87c761210506987f4\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1709.08233\",\"authors\":[{\"authorId\":\"50341745\",\"name\":\"S. Li\"},{\"authorId\":\"46999267\",\"name\":\"T. Liu\"},{\"authorId\":\"145657495\",\"name\":\"Chi Zhang\"},{\"authorId\":\"1739816\",\"name\":\"D. Yeung\"},{\"authorId\":\"3225993\",\"name\":\"S. Shen\"}],\"doi\":\"10.24963/ijcai.2018/685\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"546d22231322b68d989278d376497fe97504f36d\",\"title\":\"Learning Unmanned Aerial Vehicle Control for Autonomous Target Following\",\"url\":\"https://www.semanticscholar.org/paper/546d22231322b68d989278d376497fe97504f36d\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35346748\",\"name\":\"S. Mazumder\"},{\"authorId\":\"40107096\",\"name\":\"Bing Liu\"},{\"authorId\":\"1717480\",\"name\":\"Shuai Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6dad90f530cb9aa0deec5fa232155dab539d1b49\",\"title\":\"Action Permissibility in Deep Reinforcement Learning and Application to Autonomous Driving\",\"url\":\"https://www.semanticscholar.org/paper/6dad90f530cb9aa0deec5fa232155dab539d1b49\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"91846747\",\"name\":\"V. Kuzmin\"},{\"authorId\":null,\"name\":\"Aleksandr I. Panov\"},{\"authorId\":null,\"name\":\"Aleksandr I. Panov\"}],\"doi\":\"10.1007/978-3-030-01818-4_45\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4d8d9ab77e63bbc693df243ee81335e529d24c3\",\"title\":\"Hierarchical Reinforcement Learning with Options and United Neural Network Approximation\",\"url\":\"https://www.semanticscholar.org/paper/e4d8d9ab77e63bbc693df243ee81335e529d24c3\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"2001.04397\",\"authors\":[{\"authorId\":\"31979719\",\"name\":\"Jarrett Holtz\"},{\"authorId\":\"2100712\",\"name\":\"Arjun Guha\"},{\"authorId\":\"153878307\",\"name\":\"J. Biswas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9e67e5bf3b09ee1f7ca6778835ef91a58ae3839f\",\"title\":\"SMT-based Robot Transition Repair\",\"url\":\"https://www.semanticscholar.org/paper/9e67e5bf3b09ee1f7ca6778835ef91a58ae3839f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152422014\",\"name\":\"David Abel\"},{\"authorId\":\"1768775881\",\"name\":\"Nathan Umbanhowar\"},{\"authorId\":\"38562041\",\"name\":\"Khimya Khetarpal\"},{\"authorId\":\"34544888\",\"name\":\"Dilip Arumugam\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a9d14cf9f22e624cc9f850881a0a924de56a7a9\",\"title\":\"Value Preserving State-Action Abstractions\",\"url\":\"https://www.semanticscholar.org/paper/7a9d14cf9f22e624cc9f850881a0a924de56a7a9\",\"venue\":\"AISTATS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Abel\"},{\"authorId\":\"1768775881\",\"name\":\"Nathan Umbanhowar\"},{\"authorId\":\"38562041\",\"name\":\"Khimya Khetarpal\"},{\"authorId\":\"34544888\",\"name\":\"Dilip Arumugam\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce61de9e26235729d125266867ec3cec4feb5944\",\"title\":\"\\u201c Structure & Priors in Reinforcement Learning \\u201d at ICLR 2019 V ALUE P RESERVING S TATE-A CTION A BSTRACTIONS ( A PPENDIX )\",\"url\":\"https://www.semanticscholar.org/paper/ce61de9e26235729d125266867ec3cec4feb5944\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10805698\",\"name\":\"L. Lu\"},{\"authorId\":\"3273963\",\"name\":\"Xueqiang Gu\"},{\"authorId\":null,\"name\":\"Wanpeng Zhang\"},{\"authorId\":\"49251781\",\"name\":\"J. Chen\"}],\"doi\":\"10.1109/CAC48633.2019.8996271\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d861cab02cbb314fa7f1e14103a238d66e5d8809\",\"title\":\"Research on Learning Method Based on Hierarchical Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/d861cab02cbb314fa7f1e14103a238d66e5d8809\",\"venue\":\"2019 Chinese Automation Congress (CAC)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1992624061\",\"name\":\"Marco Braun\"},{\"authorId\":\"2026477\",\"name\":\"S. Wrede\"}],\"doi\":\"10.1109/ETFA46521.2020.9211917\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ace7687c63f42f3371ee89c401e8de5c0687481f\",\"title\":\"Incorporation of Expert Knowledge for Learning Robotic Assembly Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ace7687c63f42f3371ee89c401e8de5c0687481f\",\"venue\":\"2020 25th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39934673\",\"name\":\"A. Bai\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":\"10.1007/978-3-030-00308-1_16\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"582d95e350bc69744f9c37c35c9e985e21b119ba\",\"title\":\"Concurrent Hierarchical Reinforcement Learning for RoboCup Keepaway\",\"url\":\"https://www.semanticscholar.org/paper/582d95e350bc69744f9c37c35c9e985e21b119ba\",\"venue\":\"RoboCup\",\"year\":2017},{\"arxivId\":\"2011.13577\",\"authors\":[{\"authorId\":\"104345907\",\"name\":\"B. Stapelberg\"},{\"authorId\":\"39260915\",\"name\":\"K. M. Malan\"}],\"doi\":\"10.18489/sacj.v32i2.746\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c80e54ca811ee42c063076ee2542418c10e70195\",\"title\":\"A survey of benchmarking frameworks for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/c80e54ca811ee42c063076ee2542418c10e70195\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":7420124,\"doi\":\"10.24963/ijcai.2017/196\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"9486d64580fcc72869d46407171285c82d3b005c\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144980509\",\"name\":\"D. Andre\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b930f68e73fb06bd70d72e05781cf00d2781bd95\",\"title\":\"Programmable Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/b930f68e73fb06bd70d72e05781cf00d2781bd95\",\"venue\":\"NIPS\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145805766\",\"name\":\"G. Kuhlmann\"}],\"doi\":\"10.1177/105971230501300301\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"39a2cd04d81eeefe8ea8293a29532cf7ad55e34c\",\"title\":\"Reinforcement Learning for RoboCup Soccer Keepaway\",\"url\":\"https://www.semanticscholar.org/paper/39a2cd04d81eeefe8ea8293a29532cf7ad55e34c\",\"venue\":\"Adapt. Behav.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bhaskara Marthi\"},{\"authorId\":null,\"name\":\"Stuart J Russell\"},{\"authorId\":null,\"name\":\"David Latham\"},{\"authorId\":null,\"name\":\"Carlos Guestrin. Concurrent hierarchical reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 779\\u2013785,\",\"year\":2005},{\"arxivId\":\"cs/9905014\",\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1613/jair.639\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c96ca25d889251e20e33d01f24eec175301ab94\",\"title\":\"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\",\"url\":\"https://www.semanticscholar.org/paper/4c96ca25d889251e20e33d01f24eec175301ab94\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1850503\",\"name\":\"S. Mahadevan\"}],\"doi\":\"10.1023/A:1025696116075\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8\",\"title\":\"Recent Advances in Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8\",\"venue\":\"Discret. Event Dyn. Syst.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Bellman. Dynamic Programming. Princeton University Press\"},{\"authorId\":null,\"name\":\"Princeton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"NJ\",\"url\":\"\",\"venue\":\"USA,\",\"year\":1957},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Edward F Moore. Gedanken-experiments on sequential machines\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Automata studies\",\"url\":\"\",\"venue\":\"34:129\\u2013153,\",\"year\":1956},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144980509\",\"name\":\"D. Andre\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"156e32df7f0c149de468f7d5ed4d7c5e17eba7d2\",\"title\":\"State abstraction for programmable reinforcement learning agents\",\"url\":\"https://www.semanticscholar.org/paper/156e32df7f0c149de468f7d5ed4d7c5e17eba7d2\",\"venue\":\"AAAI/IAAI\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1850503\",\"name\":\"S. Mahadevan\"}],\"doi\":\"10.1023/A:1022140919877\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a8149fb5aa8a5684e7d530c264451a5cb9250f5\",\"title\":\"Recent Advances in Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0a8149fb5aa8a5684e7d530c264451a5cb9250f5\",\"venue\":\"Discret. Event Dyn. Syst.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"52e2ac397f0c8d5f533959905df899bc328d9f85\",\"title\":\"Reinforcement Learning with Hierarchies of Machines\",\"url\":\"https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85\",\"venue\":\"NIPS\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas G Dietterich. Hierarchical reinforcement learning Research\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"13(1):63\",\"url\":\"\",\"venue\":\"May\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34166566\",\"name\":\"E. F. Moore\"}],\"doi\":\"10.1515/9781400882618-006\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6021735e8de4f32010c6313396432d99bbe2440\",\"title\":\"Gedanken-Experiments on Sequential Machines\",\"url\":\"https://www.semanticscholar.org/paper/a6021735e8de4f32010c6313396432d99bbe2440\",\"venue\":\"\",\"year\":1956},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1711416\",\"name\":\"Bhaskara Marthi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"165c7cc7c36112bb4215631c0270b5a8a5241dcd\",\"title\":\"Concurrent Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/165c7cc7c36112bb4215631c0270b5a8a5241dcd\",\"venue\":\"IJCAI\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145739642\",\"name\":\"J. Kober\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":\"10.1177/0278364913495721\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"65438e0ba226c1f97bd8a36333ebc3297b1a32fd\",\"title\":\"Reinforcement learning in robotics: A survey\",\"url\":\"https://www.semanticscholar.org/paper/65438e0ba226c1f97bd8a36333ebc3297b1a32fd\",\"venue\":\"Int. J. Robotics Res.\",\"year\":2013}],\"title\":\"Efficient Reinforcement Learning with Hierarchies of Machines by Leveraging Internal Transitions\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Finite-state machine\",\"topicId\":\"4280\",\"url\":\"https://www.semanticscholar.org/topic/4280\"},{\"topic\":\"Hold-And-Modify\",\"topicId\":\"2718150\",\"url\":\"https://www.semanticscholar.org/topic/2718150\"},{\"topic\":\"Recursion\",\"topicId\":\"2417\",\"url\":\"https://www.semanticscholar.org/topic/2417\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"INT (x86 instruction)\",\"topicId\":\"1430569\",\"url\":\"https://www.semanticscholar.org/topic/1430569\"},{\"topic\":\"INT 13H\",\"topicId\":\"1372389\",\"url\":\"https://www.semanticscholar.org/topic/1372389\"}],\"url\":\"https://www.semanticscholar.org/paper/9486d64580fcc72869d46407171285c82d3b005c\",\"venue\":\"IJCAI\",\"year\":2017}\n"