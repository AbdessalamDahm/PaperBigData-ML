"{\"abstract\":\"Transfer learning is a method where an agent reuses knowledge learned in a source task to improve learning on a target task. Recent work has shown that transfer learning can be extended to the idea of curriculum learning, where the agent incrementally accumulates knowledge over a sequence of tasks (i.e. a curriculum). In most existing work, such curricula have been constructed manually. Furthermore, they are fixed ahead of time, and do not adapt to the progress or abilities of the agent. In this paper, we formulate the design of a curriculum as a Markov Decision Process, which directly models the accumulation of knowledge as an agent interacts with tasks, and propose a method that approximates an execution of an optimal policy in this MDP to produce an agent-specific curriculum. We use our approach to automatically sequence tasks for 3 agents with varying sensing and action capabilities in an experimental domain, and show that our method produces curricula customized for each agent that improve performance relative to learning from scratch or using a different agent\\u2019s curriculum.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\",\"url\":\"https://www.semanticscholar.org/author/1737999\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\",\"url\":\"https://www.semanticscholar.org/author/1715858\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\",\"url\":\"https://www.semanticscholar.org/author/144848112\"}],\"citationVelocity\":13,\"citations\":[{\"arxivId\":\"1703.07853\",\"authors\":[{\"authorId\":\"47678623\",\"name\":\"V. Jain\"},{\"authorId\":\"1782469\",\"name\":\"Theja Tulabandhula\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3702585def7a5e6dfd555f86c1f775a29099a278\",\"title\":\"Faster Reinforcement Learning Using Active Simulators\",\"url\":\"https://www.semanticscholar.org/paper/3702585def7a5e6dfd555f86c1f775a29099a278\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1906.06178\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"137124626\",\"name\":\"Christiano Coletto Christakou\"},{\"authorId\":\"138689850\",\"name\":\"R. Gutierrez\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":\"10.24963/ijcai.2019/320\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1988a0134839f36449bbacc4589cc8b37e46a965\",\"title\":\"Curriculum Learning for Cumulative Return Maximization\",\"url\":\"https://www.semanticscholar.org/paper/1988a0134839f36449bbacc4589cc8b37e46a965\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35346748\",\"name\":\"S. Mazumder\"},{\"authorId\":\"40107096\",\"name\":\"Bing Liu\"},{\"authorId\":\"1717480\",\"name\":\"Shuai Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6dad90f530cb9aa0deec5fa232155dab539d1b49\",\"title\":\"Action Permissibility in Deep Reinforcement Learning and Application to Autonomous Driving\",\"url\":\"https://www.semanticscholar.org/paper/6dad90f530cb9aa0deec5fa232155dab539d1b49\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"139a4c936edba87af0717179c1f58bf2975cbac6\",\"title\":\"Generalizing Curricula for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/139a4c936edba87af0717179c1f58bf2975cbac6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.01054\",\"authors\":[{\"authorId\":\"138689850\",\"name\":\"R. Gutierrez\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a95392211ec1e41a8e0a34b6b8e1a750e966fbbd\",\"title\":\"Information-theoretic Task Selection for Meta-Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a95392211ec1e41a8e0a34b6b8e1a750e966fbbd\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151439389\",\"name\":\"Floris den Hengst\"},{\"authorId\":\"41078977\",\"name\":\"Eoin Martino Grua\"},{\"authorId\":\"40571759\",\"name\":\"A. E. Hassouni\"},{\"authorId\":\"7579233\",\"name\":\"Mark Hoogendoorn\"}],\"doi\":\"10.3233/ds-200028\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"39c52e98fb6f04d5a938217fd3ea8164a8888268\",\"title\":\"Reinforcement learning for personalization: A systematic literature review\",\"url\":\"https://www.semanticscholar.org/paper/39c52e98fb6f04d5a938217fd3ea8164a8888268\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1474604732\",\"name\":\"Deepika Kumari\"},{\"authorId\":\"1474613198\",\"name\":\"M. Chaudhary\"},{\"authorId\":\"144057634\",\"name\":\"A. Mishra\"}],\"doi\":\"10.1109/icccnt45670.2019.8944912\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"85af8ad47717aa91f1b8f0e972adbc057ea3c395\",\"title\":\"An Optimal Transfer of Knowledge in Reinforcement Learning through Greedy Approach\",\"url\":\"https://www.semanticscholar.org/paper/85af8ad47717aa91f1b8f0e972adbc057ea3c395\",\"venue\":\"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)\",\"year\":2019},{\"arxivId\":\"2005.09833\",\"authors\":[{\"authorId\":\"51235122\",\"name\":\"Keting Lu\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"49697607\",\"name\":\"Xiaoping Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c674570759cc7553512fdf823513de355aa6c592\",\"title\":\"Learning and Reasoning for Robot Dialog and Navigation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/c674570759cc7553512fdf823513de355aa6c592\",\"venue\":\"SIGdial\",\"year\":2020},{\"arxivId\":\"1802.10567\",\"authors\":[{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"49512734\",\"name\":\"Roland Hafner\"},{\"authorId\":\"46534085\",\"name\":\"T. Lampe\"},{\"authorId\":\"2366050\",\"name\":\"Michael Neunert\"},{\"authorId\":\"3110620\",\"name\":\"J. Degrave\"},{\"authorId\":\"8023592\",\"name\":\"T. Wiele\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cab81775baae7ba2d056ebbc60437f2e03358ca3\",\"title\":\"Learning by Playing - Solving Sparse Reward Tasks from Scratch\",\"url\":\"https://www.semanticscholar.org/paper/cab81775baae7ba2d056ebbc60437f2e03358ca3\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0eeb742c8bca090488621146ee2a2937f2f8a188\",\"title\":\"Learning in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0eeb742c8bca090488621146ee2a2937f2f8a188\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1909.02790\",\"authors\":[{\"authorId\":\"2209431\",\"name\":\"Weixun Wang\"},{\"authorId\":\"3449314\",\"name\":\"Tianpei Yang\"},{\"authorId\":\"93006732\",\"name\":\"Y. Liu\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"2105811\",\"name\":\"Xiaotian Hao\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":null,\"name\":\"Yang Gao\"}],\"doi\":\"10.1609/AAAI.V34I05.6221\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8c8e611be5599a94ef8e0f8758eddd4f21d10093\",\"title\":\"From Few to More: Large-scale Dynamic Multiagent Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/8c8e611be5599a94ef8e0f8758eddd4f21d10093\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1812.00285\",\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"c821db2ca3c26c146f75c90de8cfbd6913013748\",\"title\":\"Learning Curriculum Policies for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c821db2ca3c26c146f75c90de8cfbd6913013748\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":\"2001.01536\",\"authors\":[{\"authorId\":\"83620577\",\"name\":\"Liuyu Xiang\"},{\"authorId\":\"38329336\",\"name\":\"G. Ding\"}],\"doi\":\"10.1007/978-3-030-58558-7_15\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20d8cb84cd1cf26e803e73291fa43673ef3c9c86\",\"title\":\"Learning From Multiple Experts: Self-paced Knowledge Distillation for Long-tailed Classification\",\"url\":\"https://www.semanticscholar.org/paper/20d8cb84cd1cf26e803e73291fa43673ef3c9c86\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145050960\",\"name\":\"F. Silva\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e105d71d384fdfbed12b8eee93c38db4269d01a4\",\"title\":\"Object-Oriented Curriculum Generation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e105d71d384fdfbed12b8eee93c38db4269d01a4\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"}],\"doi\":\"10.24963/ijcai.2017/757\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8e2c45367820406c65e9ebe26d598054a1996f28\",\"title\":\"Curriculum Learning in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8e2c45367820406c65e9ebe26d598054a1996f28\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"2008.09377\",\"authors\":[{\"authorId\":\"118608816\",\"name\":\"Binyamin Manela\"},{\"authorId\":\"2924948\",\"name\":\"A. Biess\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a026fb990836c9c36f0d24bc3d079656938f5193\",\"title\":\"Curriculum Learning with Hindsight Experience Replay for Sequential Object Manipulation Tasks\",\"url\":\"https://www.semanticscholar.org/paper/a026fb990836c9c36f0d24bc3d079656938f5193\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.00511\",\"authors\":[{\"authorId\":\"1850139106\",\"name\":\"Andrea Bassich\"},{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"777ae9b36cacca5deca104541235d19030c19d5e\",\"title\":\"Curriculum Learning with a Progression Function\",\"url\":\"https://www.semanticscholar.org/paper/777ae9b36cacca5deca104541235d19030c19d5e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145050960\",\"name\":\"F. Silva\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1109/BRACIS.2019.00090\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"91eabf58179071b24322b40bf68cab7e4a0adb22\",\"title\":\"Building Self-Play Curricula Online by Playing with Expert Agents in Adversarial Games\",\"url\":\"https://www.semanticscholar.org/paper/91eabf58179071b24322b40bf68cab7e4a0adb22\",\"venue\":\"2019 8th Brazilian Conference on Intelligent Systems (BRACIS)\",\"year\":2019},{\"arxivId\":\"1809.11074\",\"authors\":[{\"authorId\":\"51235122\",\"name\":\"Keting Lu\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"27054809\",\"name\":\"X. Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"052cf374d8902f21de720bd209ebdda39167764c\",\"title\":\"Robot Representing and Reasoning with Knowledge from Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/052cf374d8902f21de720bd209ebdda39167764c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1907.02998\",\"authors\":[{\"authorId\":\"150310213\",\"name\":\"Srinivas Venkattaramanujam\"},{\"authorId\":\"40520520\",\"name\":\"E. Crawford\"},{\"authorId\":\"70270790\",\"name\":\"T. Doan\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"33ae72e9b150877c7fd6b135d5ea7a760807f131\",\"title\":\"Self-supervised Learning of Distance Functions for Goal-Conditioned Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/33ae72e9b150877c7fd6b135d5ea7a760807f131\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"Sanmit Narvekar\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"25c6435d7a840631561475fe553a33151498ca84\",\"title\":\"Policies for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/25c6435d7a840631561475fe553a33151498ca84\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12116553\",\"name\":\"Y. Wu\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"145592727\",\"name\":\"K. Song\"}],\"doi\":\"10.24963/ijcai.2018/211\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"title\":\"Master-Slave Curriculum Design for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46760252\",\"name\":\"Taewoo Kim\"},{\"authorId\":\"1520033283\",\"name\":\"Joo-Haeng Lee\"}],\"doi\":\"10.1109/ACCESS.2020.3015245\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6807a86f1e60948362ccb126b3db78fde04e2dd1\",\"title\":\"Reinforcement Learning-Based Path Generation Using Sequential Pattern Reduction and Self-Directed Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/6807a86f1e60948362ccb126b3db78fde04e2dd1\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9582955\",\"name\":\"S. Chow\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b586ec17e4644129550a53d23ba5f70f0391a0db\",\"title\":\"Policy Progress Score for Automatic Task Selection in Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/b586ec17e4644129550a53d23ba5f70f0391a0db\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1809.06146\",\"authors\":[{\"authorId\":\"2236890\",\"name\":\"Manfred Eppe\"},{\"authorId\":\"2632932\",\"name\":\"Sven Magg\"},{\"authorId\":\"1736513\",\"name\":\"S. Wermter\"}],\"doi\":\"10.1109/DEVLRN.2019.8850721\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9907ff0cde37e9b508478dbf9609e73bb70d4398\",\"title\":\"Curriculum goal masking for continuous deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/9907ff0cde37e9b508478dbf9609e73bb70d4398\",\"venue\":\"2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)\",\"year\":2019},{\"arxivId\":\"2001.04418\",\"authors\":[{\"authorId\":\"40418436\",\"name\":\"M. V. D. Meer\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"2552871\",\"name\":\"Elia Bruni\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4e91c1fde4030c9518932c7a0b46358bb93ba91a\",\"title\":\"Exploiting Language Instructions for Interpretable and Compositional Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4e91c1fde4030c9518932c7a0b46358bb93ba91a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1707.09183\",\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"1689073\",\"name\":\"M. Kaisers\"},{\"authorId\":\"1738050\",\"name\":\"T. Baarslag\"},{\"authorId\":\"2643564\",\"name\":\"E. Cote\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0717e64a010a8cc45da64127b5bce10afc9d6d01\",\"title\":\"A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity\",\"url\":\"https://www.semanticscholar.org/paper/0717e64a010a8cc45da64127b5bce10afc9d6d01\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1580334928\",\"name\":\"T.S.C. Pollack\"},{\"authorId\":\"2939347\",\"name\":\"E. Kampen\"}],\"doi\":\"10.2514/6.2020-2100\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ad1366e86aa90631da3d982f77b51c54b184985\",\"title\":\"Safe Curriculum Learning for Optimal Flight Control of Unmanned Aerial Vehicles with Uncertain System Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/7ad1366e86aa90631da3d982f77b51c54b184985\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2007.00350\",\"authors\":[{\"authorId\":\"145213709\",\"name\":\"Kuan Fang\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"885d22a6ed0c4a00f2eabf05d617c5e41554fd08\",\"title\":\"Adaptive Procedural Task Generation for Hard-Exploration Problems\",\"url\":\"https://www.semanticscholar.org/paper/885d22a6ed0c4a00f2eabf05d617c5e41554fd08\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1791707\",\"name\":\"Meng Fang\"},{\"authorId\":\"144545221\",\"name\":\"Cheng Zhou\"},{\"authorId\":\"40206014\",\"name\":\"Boqing Gong\"},{\"authorId\":\"1723338\",\"name\":\"J. Xu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e29687df9e9a465c1b74f7487ccab23aec6f45d6\",\"title\":\"DHER: HINDSIGHT EXPERIENCE REPLAY\",\"url\":\"https://www.semanticscholar.org/paper/e29687df9e9a465c1b74f7487ccab23aec6f45d6\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1912.00444\",\"authors\":[{\"authorId\":\"48180698\",\"name\":\"A. Srinivasan\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"1400415653\",\"name\":\"Maxime Chevalier-Boisvert\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8112e04e71b83f996f6098bc6ae2b4856128fbce\",\"title\":\"Automated curriculum generation for Policy Gradients from Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/8112e04e71b83f996f6098bc6ae2b4856128fbce\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39593364\",\"name\":\"Bob McGrew\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ed63612ab053b384c01d22e77da727928e652807\",\"title\":\"CURRICULUM LEARNING WITH HINDSIGHT EXPERIENCE REPLAY FOR SEQUENTIAL OBJECT MANIPULATION TASKS\",\"url\":\"https://www.semanticscholar.org/paper/ed63612ab053b384c01d22e77da727928e652807\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.15427\",\"authors\":[{\"authorId\":\"3319957\",\"name\":\"Hailan Ma\"},{\"authorId\":\"103338302\",\"name\":\"Daoyi Dong\"},{\"authorId\":\"144558270\",\"name\":\"S. Ding\"},{\"authorId\":\"4520850\",\"name\":\"Chunlin Chen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d958c6deeed12244bb457fcf8a0876402b655c67\",\"title\":\"Curriculum-based Deep Reinforcement Learning for Quantum Control\",\"url\":\"https://www.semanticscholar.org/paper/d958c6deeed12244bb457fcf8a0876402b655c67\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3431333\",\"name\":\"David Isele\"},{\"authorId\":\"144020269\",\"name\":\"E. Eaton\"},{\"authorId\":\"144200581\",\"name\":\"M. Roberts\"},{\"authorId\":\"1969847\",\"name\":\"D. Aha\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a3cdafc7c9eba0971815da03f1d8031366b1c8d\",\"title\":\"Modeling Consecutive Task Learning with Task Graph Agendas\",\"url\":\"https://www.semanticscholar.org/paper/7a3cdafc7c9eba0971815da03f1d8031366b1c8d\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":\"1901.11478\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":\"10.1109/DEVLRN.2019.8850690\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"1e658c39cf8695c3363457903f5bc898231611dd\",\"title\":\"An Optimization Framework for Task Sequencing in Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e658c39cf8695c3363457903f5bc898231611dd\",\"venue\":\"2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)\",\"year\":2019},{\"arxivId\":\"2007.01544\",\"authors\":[{\"authorId\":\"36267914\",\"name\":\"Adam Bignold\"},{\"authorId\":\"144465583\",\"name\":\"F. Cruz\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"3327913\",\"name\":\"R. Dazeley\"},{\"authorId\":\"1990124\",\"name\":\"P. Vamplew\"},{\"authorId\":\"2763108\",\"name\":\"C. Foale\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b8c5e12a508df6e980bfd59106186aa897a8ee2\",\"title\":\"A Conceptual Framework for Externally-influenced Agents: An Assisted Reinforcement Learning Review\",\"url\":\"https://www.semanticscholar.org/paper/4b8c5e12a508df6e980bfd59106186aa897a8ee2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.03877\",\"authors\":[{\"authorId\":\"118608816\",\"name\":\"Binyamin Manela\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b5853151f56a2b9779d123904bc719e8c549fac3\",\"title\":\"Deep Reinforcement Learning for Complex Manipulation Tasks with Sparse Feedback\",\"url\":\"https://www.semanticscholar.org/paper/b5853151f56a2b9779d123904bc719e8c549fac3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.11468\",\"authors\":[{\"authorId\":\"2105811\",\"name\":\"Xiaotian Hao\"},{\"authorId\":\"2209431\",\"name\":\"Weixun Wang\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"672eef3b5aa82d7cd64b5fbaeecf8172cd191f9d\",\"title\":\"Independent Generative Adversarial Self-Imitation Learning in Cooperative Multiagent Systems\",\"url\":\"https://www.semanticscholar.org/paper/672eef3b5aa82d7cd64b5fbaeecf8172cd191f9d\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51111354\",\"name\":\"Jan W\\u00f6hlke\"},{\"authorId\":\"48604701\",\"name\":\"F. Schmitt\"},{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"}],\"doi\":\"10.5555/3398761.3398934\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"97258b89e7d30498f5449efb5be11a518af517d6\",\"title\":\"A Performance-Based Start State Curriculum Framework for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/97258b89e7d30498f5449efb5be11a518af517d6\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9415843\",\"name\":\"C. Dewa\"},{\"authorId\":\"145472435\",\"name\":\"J. Miura\"}],\"doi\":\"10.1109/ACCESS.2020.3033016\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8ef3ae749ba2225038d35c462395d565a5d6cc9\",\"title\":\"A Framework for DRL Navigation With State Transition Checking and Velocity Increment Scheduling\",\"url\":\"https://www.semanticscholar.org/paper/b8ef3ae749ba2225038d35c462395d565a5d6cc9\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145050960\",\"name\":\"F. Silva\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"}],\"doi\":\"10.1613/jair.1.11396\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3660f76126fe1343c91f065f452845981041206c\",\"title\":\"A Survey on Transfer Learning for Multiagent Reinforcement Learning Systems\",\"url\":\"https://www.semanticscholar.org/paper/3660f76126fe1343c91f065f452845981041206c\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2019},{\"arxivId\":\"2003.04960\",\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"21022150\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fb0bfc9c1518d5c7c2fbdb7bc9e254eff0ab6238\",\"title\":\"Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey\",\"url\":\"https://www.semanticscholar.org/paper/fb0bfc9c1518d5c7c2fbdb7bc9e254eff0ab6238\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2020}],\"corpusId\":31069197,\"doi\":\"10.24963/ijcai.2017/353\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":3,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"b7708394600130c8c5403178976fcfca1972f3db\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"3171506\",\"name\":\"S. Mahmoud\"},{\"authorId\":\"145116379\",\"name\":\"S. Miles\"},{\"authorId\":\"1721762\",\"name\":\"Michael Luck\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"7ba8d84592115f825da99c2ebb18db5ba66cc232\",\"title\":\"Proceedings of the 15th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2016\",\"url\":\"https://www.semanticscholar.org/paper/7ba8d84592115f825da99c2ebb18db5ba66cc232\",\"venue\":\"AAMAS 2016\",\"year\":2016},{\"arxivId\":\"1206.6836\",\"authors\":[{\"authorId\":\"1757489\",\"name\":\"N. Ferns\"},{\"authorId\":\"39163115\",\"name\":\"P. S. Castro\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1784317\",\"name\":\"P. Panangaden\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2f8aaf6d0654e729e47224435aec83d733685cc3\",\"title\":\"Methods for Computing State Similarity in Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/2f8aaf6d0654e729e47224435aec83d733685cc3\",\"venue\":\"UAI\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1411490468\",\"name\":\"Fernando Fern\\u00e1ndez-Rebollo\"},{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1016/j.robot.2010.03.007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e108bab11c7dffa0d6532c79dcfc04d3ff1e3155\",\"title\":\"Probabilistic Policy Reuse for inter-task transfer learning\",\"url\":\"https://www.semanticscholar.org/paper/e108bab11c7dffa0d6532c79dcfc04d3ff1e3155\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"2373952\",\"name\":\"J. Louradour\"},{\"authorId\":\"2939803\",\"name\":\"Ronan Collobert\"},{\"authorId\":\"145183709\",\"name\":\"J. Weston\"}],\"doi\":\"10.1145/1553374.1553380\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8de174ab5419b9d3127695405efd079808e956e8\",\"title\":\"Curriculum learning\",\"url\":\"https://www.semanticscholar.org/paper/8de174ab5419b9d3127695405efd079808e956e8\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"title\":\"Source Task Creation for Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Norm Ferns\"},{\"authorId\":null,\"name\":\"Prakash Panangaden\"},{\"authorId\":null,\"name\":\"Doina Precup. Bisimulation metrics for continuous markov processes\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"SIAM Journal on Computing\",\"url\":\"\",\"venue\":\"40(6):1662\\u20131714,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoshua Bengio\"},{\"authorId\":null,\"name\":\"J\\u00e9r\\u00f4me Louradour\"},{\"authorId\":null,\"name\":\"Ronan Collobert\"},{\"authorId\":null,\"name\":\"Jason Weston. Curriculum learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 26th Annual International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 41\\u201348. ACM,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Maxwell Svetlik\"},{\"authorId\":null,\"name\":\"Matteo Leonetti\"},{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Rishi Shah\"},{\"authorId\":null,\"name\":\"Nick Walker\"},{\"authorId\":null,\"name\":\"Peter Stone. Automatic curriculum graph generation for r agents\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI)\",\"url\":\"\",\"venue\":\"February\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"},{\"authorId\":\"1729320\",\"name\":\"A. Bonarini\"}],\"doi\":\"10.1145/1390156.1390225\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"title\":\"Transfer of samples in batch reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/117d0903a0dc0d78aacc8cbc84e6cd86f4530ef2\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Peter Stone.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Behavior transfer for valuefunctionbased reinforcement learning\",\"url\":\"\",\"venue\":\"The Fourth International Joint Conference on Autonomous Agents and Multiagent Systems\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47346762\",\"name\":\"M. Svetlik\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"9578995\",\"name\":\"Rishi Shah\"},{\"authorId\":\"145314605\",\"name\":\"Nick Walker\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"42750855b95449ba74921bfecc84e2257a2c5b19\",\"title\":\"Automatic Curriculum Graph Generation for Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/42750855b95449ba74921bfecc84e2257a2c5b19\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1082473.1082482\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0416ef638e83622caea7a636ead67c2dc9763ab7\",\"title\":\"Behavior transfer for value-function-based reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/0416ef638e83622caea7a636ead67c2dc9763ab7\",\"venue\":\"AAMAS '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d69be75b44153086b51a7ca53b863f151e3e89a1\",\"title\":\"An Empirical Study of Non-Expert Curriculum Design for Machine Learners\",\"url\":\"https://www.semanticscholar.org/paper/d69be75b44153086b51a7ca53b863f151e3e89a1\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"2394697\",\"name\":\"Y. Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3412ded0375f8fe7336e82dc534eed994cac088\",\"title\":\"Transfer Learning via Inter-Task Mappings for Temporal Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/b3412ded0375f8fe7336e82dc534eed994cac088\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Sanmit Narvekar\"},{\"authorId\":null,\"name\":\"Matteo Leonetti\"},{\"authorId\":null,\"name\":\"Peter Stone. Learning inter-task transferability in the samples\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 2015 ACM Conference on Autonomous Agents and Multi-Agent Systems (AAMAS)\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Anestis Fachantidis\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Transfer - ring task models in reinforcement learning agents\",\"url\":\"\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Peter Stone\"},{\"authorId\":null,\"name\":\"Yaxin Liu. Transfer learning via inter-task mappings for learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"8(1):2125\\u20132167,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Peter Stone. Behavior transfer for value-function-based Agents\"},{\"authorId\":null,\"name\":\"Multiagent Systems\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 53\\u201359\",\"url\":\"\",\"venue\":\"New York, NY, July\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Lazaric\"},{\"authorId\":null,\"name\":\"M. Restelli\"},{\"authorId\":null,\"name\":\"A. Bonarini. Transfer of samples in batch reinforceme learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the Twenty-Fifth Annual International Conference on Machine Learning (ICML-2008)\",\"url\":\"\",\"venue\":\"pages 544\\u2013551, Helsinki, Finalnd, July\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"05758c5681ea342031bb13db2e1ba66a989baf13\",\"title\":\"Learning Inter-Task Transferability in the Absence of Target Task Samples\",\"url\":\"https://www.semanticscholar.org/paper/05758c5681ea342031bb13db2e1ba66a989baf13\",\"venue\":\"AAMAS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1756287\",\"name\":\"Vishal Soni\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c069d5777f943875878eeaaa67814d58ebad894e\",\"title\":\"Using Homomorphisms to Transfer Options across Continuous Reinforcement Learning Domains\",\"url\":\"https://www.semanticscholar.org/paper/c069d5777f943875878eeaaa67814d58ebad894e\",\"venue\":\"AAAI\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1757489\",\"name\":\"N. Ferns\"},{\"authorId\":\"1784317\",\"name\":\"P. Panangaden\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":\"10.1137/10080484X\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"842345bc690a42432f6510efe894c801a1740bda\",\"title\":\"Bisimulation Metrics for Continuous Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/842345bc690a42432f6510efe894c801a1740bda\",\"venue\":\"SIAM J. Comput.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66921351\",\"name\":\"Natalia Criado Pacheco\"},{\"authorId\":\"144622087\",\"name\":\"J. M. Such\"}],\"doi\":\"10.1016/0004-3702(72)90052-5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0c711a90bf4768599e022511c545f9cfddbf3b35\",\"title\":\"International Joint Conference on Artificial Intelligence (IJCAI)\",\"url\":\"https://www.semanticscholar.org/paper/0c711a90bf4768599e022511c545f9cfddbf3b35\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"},{\"authorId\":\"144020269\",\"name\":\"E. Eaton\"},{\"authorId\":\"12114845\",\"name\":\"P. Ruvolo\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3cdcfe28827bf5bd6f0717ba24af991746e6050e\",\"title\":\"Online Multi-Task Learning for Policy Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/3cdcfe28827bf5bd6f0717ba24af991746e6050e\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":\"10.1007/978-3-642-27645-3_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"title\":\"Transfer in Reinforcement Learning: A Framework and a Survey\",\"url\":\"https://www.semanticscholar.org/paper/16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Haitham Bou Ammar\"},{\"authorId\":null,\"name\":\"Eric Eaton\"},{\"authorId\":null,\"name\":\"Paul Ruvolo\"},{\"authorId\":null,\"name\":\"Matthew Taylor. Online multi-task learning for policy grad methods\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 31st International Conference on Machine Learning (ICML14)\",\"url\":\"\",\"venue\":\"pages 1206\\u20131214,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Anestis Fachantidis\"},{\"authorId\":null,\"name\":\"Ioannis Partalas\"},{\"authorId\":null,\"name\":\"Grigorios Tsoumakas\"},{\"authorId\":null,\"name\":\"Ioannis Vlahavas. Transferring task models in reinforcemen agents\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Neurocomputing\",\"url\":\"\",\"venue\":\"107:23\\u201332,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Fernando Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Javier Garc\\u0131\\u0301a\",\"url\":\"\",\"venue\":\"and Manuela Veloso. Probabilistic policy reuse for intertask transfer learning. Robotics and Autonomous Systems, 58(7):866 \\u2013 871,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144056302\",\"name\":\"A. Wilson\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"},{\"authorId\":\"145527877\",\"name\":\"Soumya Ray\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":\"10.1145/1273496.1273624\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"title\":\"Multi-task reinforcement learning: a hierarchical Bayesian approach\",\"url\":\"https://www.semanticscholar.org/paper/ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"venue\":\"ICML '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"16050a256dd6add1e9187e8c4f5c30c85f342fd8\",\"title\":\"Building Portable Options: Skill Transfer in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/16050a256dd6add1e9187e8c4f5c30c85f342fd8\",\"venue\":\"IJCAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46257744\",\"name\":\"H. Ammar\"},{\"authorId\":\"144020269\",\"name\":\"E. Eaton\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2571038\",\"name\":\"D. Mocanu\"},{\"authorId\":\"1695114\",\"name\":\"K. Driessens\"},{\"authorId\":\"144799183\",\"name\":\"G. Wei\\u00df\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"fa56ea61131a42afd1b2413c3d39576092b678db\",\"title\":\"An automated measure of MDP similarity for transfer in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/fa56ea61131a42afd1b2413c3d39576092b678db\",\"venue\":\"AAAI 2014\",\"year\":2014}],\"title\":\"Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning\",\"topics\":[{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"AP Computer Science A\",\"topicId\":\"1293197\",\"url\":\"https://www.semanticscholar.org/topic/1293197\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"},{\"topic\":\"Tree accumulation\",\"topicId\":\"2611275\",\"url\":\"https://www.semanticscholar.org/topic/2611275\"},{\"topic\":\"Objectivity/DB\",\"topicId\":\"1392905\",\"url\":\"https://www.semanticscholar.org/topic/1392905\"},{\"topic\":\"UT-VPN\",\"topicId\":\"4390941\",\"url\":\"https://www.semanticscholar.org/topic/4390941\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"}],\"url\":\"https://www.semanticscholar.org/paper/b7708394600130c8c5403178976fcfca1972f3db\",\"venue\":\"IJCAI\",\"year\":2017}\n"