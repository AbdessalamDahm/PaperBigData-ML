"{\"abstract\":\"Gradient-based methods are often used for policy optimization in deep reinforcement learning, despite being vulnerable to local optima and saddle points. Although gradient-free methods (e.g., genetic algorithms or evolution strategies) help mitigate these issues, poor initialization and local optima are still concerns in highly nonconvex spaces. This paper presents a method for policy optimization based on Monte-Carlo tree search and gradient-free optimization. Our method, called Monte-Carlo tree search for policy optimization (MCTSPO), provides a better exploration-exploitation trade-off through the use of the upper confidence bound heuristic. We demonstrate improved performance on reinforcement learning tasks with deceptive or sparse reward functions compared to popular gradient-based and deep genetic algorithm baselines.\",\"arxivId\":\"1912.10648\",\"authors\":[{\"authorId\":\"30681342\",\"name\":\"X. Ma\",\"url\":\"https://www.semanticscholar.org/author/30681342\"},{\"authorId\":\"1404112858\",\"name\":\"K. Driggs-Campbell\",\"url\":\"https://www.semanticscholar.org/author/1404112858\"},{\"authorId\":\"2079174\",\"name\":\"Zongzhang Zhang\",\"url\":\"https://www.semanticscholar.org/author/2079174\"},{\"authorId\":\"2275756\",\"name\":\"Mykel J. Kochenderfer\",\"url\":\"https://www.semanticscholar.org/author/2275756\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2010.05545\",\"authors\":[{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"1599360864\",\"name\":\"Nicolas Heess\"},{\"authorId\":\"3187297\",\"name\":\"Daniel J. Mankowitz\"},{\"authorId\":\"1599377224\",\"name\":\"Josh Merel\"},{\"authorId\":\"2631257\",\"name\":\"Arunkumar Byravan\"},{\"authorId\":\"2799799\",\"name\":\"Abbas Abdolmaleki\"},{\"authorId\":\"50979962\",\"name\":\"J. Kay\"},{\"authorId\":\"3110620\",\"name\":\"J. Degrave\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"1741293\",\"name\":\"J. Buchli\"},{\"authorId\":\"143813532\",\"name\":\"D. Belov\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6cf28553e91f14422941b9289b3281a4d09a072f\",\"title\":\"Local Search for Policy Iteration in Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/6cf28553e91f14422941b9289b3281a4d09a072f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2027160854\",\"name\":\"Jiarun Cai\"}],\"doi\":\"10.1007/978-3-030-63833-7_60\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b58de6291b8e5352c98566cd6300e53f244007d\",\"title\":\"WD3-MPER: A Method to Alleviate Approximation Bias in Actor-Critic\",\"url\":\"https://www.semanticscholar.org/paper/4b58de6291b8e5352c98566cd6300e53f244007d\",\"venue\":\"ICONIP\",\"year\":2020}],\"corpusId\":199465937,\"doi\":\"10.24963/ijcai.2019/432\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"b1948d623c2dae0aa9427a909f452d52cc31b1cc\",\"references\":[{\"arxivId\":\"1712.06563\",\"authors\":[{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"144910178\",\"name\":\"J. Chen\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"}],\"doi\":\"10.1145/3205455.3205473\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"0a830e847f46425f21e34239ba75f026280c0706\",\"title\":\"Safe mutations for deep and recurrent neural networks through output gradients\",\"url\":\"https://www.semanticscholar.org/paper/0a830e847f46425f21e34239ba75f026280c0706\",\"venue\":\"GECCO\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36313424\",\"name\":\"S. S. Murthy\"},{\"authorId\":\"2083404\",\"name\":\"M. Raibert\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7334fac8662ccdfc76438d23e10b76f3ca2726f\",\"title\":\"3-D balance in legged locomotion: modeling and simulation for the one-legged case\",\"url\":\"https://www.semanticscholar.org/paper/f7334fac8662ccdfc76438d23e10b76f3ca2726f\",\"venue\":\"SIGGRAPH 1986\",\"year\":1986},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1604.06778\",\"authors\":[{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"title\":\"Benchmarking Deep Reinforcement Learning for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Guillaume M.J.B. Chaslot\"},{\"authorId\":null,\"name\":\"Mark H.M. Winands\"},{\"authorId\":null,\"name\":\"H. Jaap Van Den Herik. Parallel Monte-Carlo tree search\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Computers and Games\",\"url\":\"\",\"venue\":\"pages 60\\u201371,\",\"year\":2008},{\"arxivId\":\"1703.03864\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ee802a58d32aa049d549d06be440ac947b53987\",\"title\":\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1710.02298\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"title\":\"Rainbow: Combining Improvements in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1506.02438\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"title\":\"High-Dimensional Continuous Control Using Generalized Advantage Estimation\",\"url\":\"https://www.semanticscholar.org/paper/d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1712.06567\",\"authors\":[{\"authorId\":\"9927844\",\"name\":\"Felipe Petroski Such\"},{\"authorId\":\"8309711\",\"name\":\"V. Madhavan\"},{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"},{\"authorId\":\"2552141\",\"name\":\"J. Clune\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"ba3ace39f1f1afb6651ef4c0e4b8317fd9d48fcf\",\"title\":\"Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ba3ace39f1f1afb6651ef4c0e4b8317fd9d48fcf\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143865718\",\"name\":\"V. Ferrari\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7142185fd2e86e922f609562f30820cd7c5af7f4\",\"title\":\"Advances in Neural Information Processing Systems (NIPS)\",\"url\":\"https://www.semanticscholar.org/paper/7142185fd2e86e922f609562f30820cd7c5af7f4\",\"venue\":\"\",\"year\":2007},{\"arxivId\":\"1805.07917\",\"authors\":[{\"authorId\":\"3440874\",\"name\":\"S. Khadka\"},{\"authorId\":\"1711099\",\"name\":\"Kagan Tumer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5805a80b63ed0a605e5469e321a7e3c42eaf324\",\"title\":\"Evolution-Guided Policy Gradient in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d5805a80b63ed0a605e5469e321a7e3c42eaf324\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1979505\",\"name\":\"A. Geramifard\"},{\"authorId\":\"2160071\",\"name\":\"C. Dann\"},{\"authorId\":\"144714602\",\"name\":\"R. H. Klein\"},{\"authorId\":\"143929091\",\"name\":\"W. Dabney\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"f8f03a2b287aaa712b8dbd14024b452da6b04956\",\"title\":\"RLPy: a value-function-based reinforcement learning framework for education and research\",\"url\":\"https://www.semanticscholar.org/paper/f8f03a2b287aaa712b8dbd14024b452da6b04956\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2015},{\"arxivId\":\"1708.05144\",\"authors\":[{\"authorId\":\"3374063\",\"name\":\"Yuhuai Wu\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"1785346\",\"name\":\"Roger B. Grosse\"},{\"authorId\":\"143997138\",\"name\":\"Shun Liao\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b6f2b163372e3417b687cc43313f2a630e7bca7\",\"title\":\"Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation\",\"url\":\"https://www.semanticscholar.org/paper/2b6f2b163372e3417b687cc43313f2a630e7bca7\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A. Rusu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Joel Veness , Marc G . Bellemare , Alex Graves , Martin Riedmiller , Andreas K . Fidje - land , Georg Ostrovski , et al . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Genetic and Evolutionary Computation Conference\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marek Grzes\"},{\"authorId\":null,\"name\":\"Daniel Kudenko. Theoretical\"},{\"authorId\":null,\"name\":\"empirical analysis of reward shaping in reinforcement learni Learning\"},{\"authorId\":null,\"name\":\"Applications\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 337\\u2013344\",\"url\":\"\",\"venue\":\"IEEE,\",\"year\":2009},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2668005\",\"name\":\"P. Wawrzynski\"}],\"doi\":\"10.1109/EURCON.2007.4400335\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"77f3cd7165ecf2c60c1b3e29eba1b0623a91d00a\",\"title\":\"Learning to Control a 6-Degree-of-Freedom Walking Robot\",\"url\":\"https://www.semanticscholar.org/paper/77f3cd7165ecf2c60c1b3e29eba1b0623a91d00a\",\"venue\":\"EUROCON 2007 - The International Conference on \\\"Computer as a Tool\\\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joel Lehman\"},{\"authorId\":null,\"name\":\"Jay Chen\"},{\"authorId\":null,\"name\":\"Jeff Clune\"},{\"authorId\":null,\"name\":\"Kenneth O. Stanley. Safe mutations for deep\"},{\"authorId\":null,\"name\":\"recurrent neural networks through output gradients. In Genetic\"},{\"authorId\":null,\"name\":\"Evolutionary Computation Conference\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 117\\u2013124\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2018},{\"arxivId\":\"1802.08842\",\"authors\":[{\"authorId\":\"22259392\",\"name\":\"P. Chrabaszcz\"},{\"authorId\":\"1678656\",\"name\":\"I. Loshchilov\"},{\"authorId\":\"144661829\",\"name\":\"F. Hutter\"}],\"doi\":\"10.24963/ijcai.2018/197\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"19af387c0ee32929b54ff45656129a721a84c192\",\"title\":\"Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari\",\"url\":\"https://www.semanticscholar.org/paper/19af387c0ee32929b54ff45656129a721a84c192\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yan Duan\"},{\"authorId\":null,\"name\":\"Xi Chen\"},{\"authorId\":null,\"name\":\"Rein Houthooft\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Benchmarking deep reinforcement learning f control\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning (ICML)\",\"url\":\"\",\"venue\":\"pages 1329\\u20131338,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2275756\",\"name\":\"Mykel J. Kochenderfer\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1733356\",\"name\":\"G. Chowdhary\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"},{\"authorId\":\"8755646\",\"name\":\"H. Reynolds\"},{\"authorId\":\"47882530\",\"name\":\"J. Thornton\"},{\"authorId\":\"1403025404\",\"name\":\"P. Torres-Carrasquillo\"},{\"authorId\":\"2877453\",\"name\":\"N. K. Ure\"},{\"authorId\":\"153289126\",\"name\":\"J. Vian\"}],\"doi\":\"10.7551/mitpress/10187.001.0001\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"879de295a03b77403d6d08828f792b2e3be9ff51\",\"title\":\"Decision Making Under Uncertainty: Theory and Application\",\"url\":\"https://www.semanticscholar.org/paper/879de295a03b77403d6d08828f792b2e3be9ff51\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2502026\",\"name\":\"H. Baier\"},{\"authorId\":\"1714467\",\"name\":\"P. Cowling\"}],\"doi\":\"10.1109/CIG.2018.8490403\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff036887bb872c65fb41130044125493e7856f5e\",\"title\":\"Evolutionary MCTS for Multi-Action Adversarial Games\",\"url\":\"https://www.semanticscholar.org/paper/ff036887bb872c65fb41130044125493e7856f5e\",\"venue\":\"2018 IEEE Conference on Computational Intelligence and Games (CIG)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2186376\",\"name\":\"M. Grzes\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":\"10.1109/ICMLA.2009.33\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fc305a94648ce083799ff1e5e1e9e339b2ae316b\",\"title\":\"Theoretical and Empirical Analysis of Reward Shaping in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/fc305a94648ce083799ff1e5e1e9e339b2ae316b\",\"venue\":\"2009 International Conference on Machine Learning and Applications\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning (ICML)\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Levente Kocsis\"},{\"authorId\":null,\"name\":\"Csaba Szepesv\\u00e1ri. Bandit based Monte-Carlo planning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In European Conference on Machine Learning (ECML)\",\"url\":\"\",\"venue\":\"pages 282\\u2013293,\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Adrien Cou\\u00ebtoux\"},{\"authorId\":null,\"name\":\"Jean-Baptiste Hoock\"},{\"authorId\":null,\"name\":\"Nataliya Sokolovska\"},{\"authorId\":null,\"name\":\"Olivier Teytaud\"},{\"authorId\":null,\"name\":\"Nicolas Bonnard. Continuous upper confidence trees\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Learning and Intelligent Optimization (LION)\",\"url\":\"\",\"venue\":\"pages 433\\u2013445,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hendrik Baier\"},{\"authorId\":null,\"name\":\"Peter I. Cowling. Evolutionary MCTS for multi-action adv games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IEEE Conference on Computational Intelligence and Games (CIG)\",\"url\":\"\",\"venue\":\"pages 1\\u20138. IEEE,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas Keller\"},{\"authorId\":null,\"name\":\"Malte Helmert. Trial-based heuristic tree search for fin MDPs\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Automated Planning and Scheduling (ICAPS)\",\"url\":\"\",\"venue\":\"pages 135\\u2013143,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1876322\",\"name\":\"Guillaume Chaslot\"},{\"authorId\":\"1997644\",\"name\":\"Mark H. M. Winands\"},{\"authorId\":\"1685622\",\"name\":\"H. J. V. Herik\"},{\"authorId\":\"145495343\",\"name\":\"J. W. Uiterwijk\"},{\"authorId\":\"1759257\",\"name\":\"B. Bouzy\"}],\"doi\":\"10.1142/S1793005708001094\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"966b8c5b01f1eb4b7bb8f4a83ba3f1f1879f5250\",\"title\":\"Progressive Strategies for Monte-Carlo Tree Search\",\"url\":\"https://www.semanticscholar.org/paper/966b8c5b01f1eb4b7bb8f4a83ba3f1f1879f5250\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Robert H. Klein\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", William Dabney , and Jonathan P . How . RLPy : A value - function - based reinforcement learning framework for education and research\",\"url\":\"\",\"venue\":\"Journal of Machine Learning Research AAAI Conference on Artificial Intelligence ( AAAI ) International Conference on Automated Planning and Scheduling ( ICAPS )\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Guillaume M.J.B Chaslot\"},{\"authorId\":null,\"name\":\"Mark H.M. Winands\"},{\"authorId\":null,\"name\":\"H. Jaap Van Den Herick\"},{\"authorId\":null,\"name\":\"Jos W.H.M Uiterwijk\"},{\"authorId\":null,\"name\":\"Bruno Bouzy. Progressive strategies for Monte-Carlo tree search\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"New Mathematics and Natural Computation\",\"url\":\"\",\"venue\":\"4(03):343\\u2013357,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988}],\"title\":\"Monte Carlo Tree Search for Policy Optimization\",\"topics\":[{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Monte Carlo tree search\",\"topicId\":\"18928\",\"url\":\"https://www.semanticscholar.org/topic/18928\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Genetic algorithm\",\"topicId\":\"2069\",\"url\":\"https://www.semanticscholar.org/topic/2069\"},{\"topic\":\"Local optimum\",\"topicId\":\"4165\",\"url\":\"https://www.semanticscholar.org/topic/4165\"},{\"topic\":\"Robotics\",\"topicId\":\"2759\",\"url\":\"https://www.semanticscholar.org/topic/2759\"},{\"topic\":\"Evolution strategy\",\"topicId\":\"64343\",\"url\":\"https://www.semanticscholar.org/topic/64343\"},{\"topic\":\"Sparse matrix\",\"topicId\":\"126\",\"url\":\"https://www.semanticscholar.org/topic/126\"},{\"topic\":\"Heuristic\",\"topicId\":\"4146\",\"url\":\"https://www.semanticscholar.org/topic/4146\"},{\"topic\":\"Iteration\",\"topicId\":\"11823\",\"url\":\"https://www.semanticscholar.org/topic/11823\"},{\"topic\":\"Sampling (signal processing)\",\"topicId\":\"7839\",\"url\":\"https://www.semanticscholar.org/topic/7839\"},{\"topic\":\"Hopper\",\"topicId\":\"161126\",\"url\":\"https://www.semanticscholar.org/topic/161126\"},{\"topic\":\"Driven right leg circuit\",\"topicId\":\"427743\",\"url\":\"https://www.semanticscholar.org/topic/427743\"},{\"topic\":\"Software release life cycle\",\"topicId\":\"105937\",\"url\":\"https://www.semanticscholar.org/topic/105937\"}],\"url\":\"https://www.semanticscholar.org/paper/b1948d623c2dae0aa9427a909f452d52cc31b1cc\",\"venue\":\"IJCAI\",\"year\":2019}\n"