"{\"abstract\":\"Deep Reinforcement Learning (DRL) has been applied to address a variety of cooperative multi-agent problems with either discrete action spaces or continuous action spaces. However, to the best of our knowledge, no previous work has ever succeeded in applying DRL to multi-agent problems with discrete-continuous hybrid (or parameterized) action spaces which is very common in practice. Our work fills this gap by proposing two novel algorithms: Deep Multi-Agent Parameterized Q-Networks (Deep MAPQN) and Deep Multi-Agent Hierarchical Hybrid Q-Networks (Deep MAHHQN). We follow the centralized training but decentralized execution paradigm: different levels of communication between different agents are used to facilitate the training process, while each agent executes its policy independently based on local observations during execution. Our empirical results on several challenging tasks (simulated RoboCup Soccer and game Ghost Story) show that both Deep MAPQN and Deep MAHHQN are effective and significantly outperform existing independent deep parameterized Q-learning method.\",\"arxivId\":\"1903.04959\",\"authors\":[{\"authorId\":\"81875788\",\"name\":\"Haotian Fu\",\"url\":\"https://www.semanticscholar.org/author/81875788\"},{\"authorId\":\"31190626\",\"name\":\"Hongyao Tang\",\"url\":\"https://www.semanticscholar.org/author/31190626\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\",\"url\":\"https://www.semanticscholar.org/author/40513470\"},{\"authorId\":\"2349753\",\"name\":\"Zihan Lei\",\"url\":\"https://www.semanticscholar.org/author/2349753\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\",\"url\":\"https://www.semanticscholar.org/author/2519427\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\",\"url\":\"https://www.semanticscholar.org/author/3120655\"}],\"citationVelocity\":5,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"49480086\",\"name\":\"Leilei Liu\"},{\"authorId\":\"50032052\",\"name\":\"Y. Ma\"},{\"authorId\":\"15318113\",\"name\":\"Xianglei Zhu\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"1491630925\",\"name\":\"Xiaotian Hao\"},{\"authorId\":\"91243129\",\"name\":\"L. Wang\"},{\"authorId\":\"3255510\",\"name\":\"Jiajie Peng\"}],\"doi\":\"10.1109/BIBM47256.2019.8983330\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e77accf9d594cc2d20abade59ca07d3ed8bbd734\",\"title\":\"Integrating Sequence and Network Information to Enhance Protein-Protein Interaction Prediction Using Graph Convolutional Networks\",\"url\":\"https://www.semanticscholar.org/paper/e77accf9d594cc2d20abade59ca07d3ed8bbd734\",\"venue\":\"2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"90467999\",\"name\":\"Stefano Massaroli\"},{\"authorId\":\"40585370\",\"name\":\"Michael Poli\"},{\"authorId\":\"1750884384\",\"name\":\"Sanzhar Bakhtiyarov\"},{\"authorId\":\"48490286\",\"name\":\"Jinkyoo Park\"},{\"authorId\":\"152521159\",\"name\":\"A. Yamashita\"},{\"authorId\":\"50631807\",\"name\":\"H. Asama\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cdb6ddb3c393001a79fb3c978251cd55a6cff6b7\",\"title\":\"CONTINUOUS\\u2013DEPTH VALUE NETWORKS FOR PARAMETRIZED ACTIONS\",\"url\":\"https://www.semanticscholar.org/paper/cdb6ddb3c393001a79fb3c978251cd55a6cff6b7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1491631991\",\"name\":\"Xiaojie Zhang\"},{\"authorId\":\"34831223\",\"name\":\"A. Pal\"},{\"authorId\":\"31829691\",\"name\":\"S. Debroy\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fe82f65bca456d871620e01e8e53ef36c469c350\",\"title\":\"Deep Reinforcement Learning Based Energy-efficient Task Offloading for Secondary Mobile Edge Systems\",\"url\":\"https://www.semanticscholar.org/paper/fe82f65bca456d871620e01e8e53ef36c469c350\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1519286161\",\"name\":\"Junjie Wang\"},{\"authorId\":\"1803456\",\"name\":\"Xiaohong Su\"},{\"authorId\":\"152665058\",\"name\":\"Lingling Zhao\"},{\"authorId\":\"1519064904\",\"name\":\"Jun Zhang\"}],\"doi\":\"10.3389/fbioe.2020.00298\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f47304c0c56b19f865a30c65d062c59d27a4084f\",\"title\":\"Deep Reinforcement Learning for Data Association in Cell Tracking\",\"url\":\"https://www.semanticscholar.org/paper/f47304c0c56b19f865a30c65d062c59d27a4084f\",\"venue\":\"Frontiers in Bioengineering and Biotechnology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145982842\",\"name\":\"Wei Du\"},{\"authorId\":\"49099486\",\"name\":\"Shifei Ding\"},{\"authorId\":\"49099486\",\"name\":\"Shifei Ding\"}],\"doi\":\"10.1007/s10462-020-09938-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3002d4f76f36bd78bf9e2118fbc830648cc5ea08\",\"title\":\"A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications\",\"url\":\"https://www.semanticscholar.org/paper/3002d4f76f36bd78bf9e2118fbc830648cc5ea08\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2037732324\",\"name\":\"Leilei Liu\"},{\"authorId\":\"15318113\",\"name\":\"Xianglei Zhu\"},{\"authorId\":\"50032052\",\"name\":\"Y. Ma\"},{\"authorId\":\"66711267\",\"name\":\"Haiyin Piao\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"1491630925\",\"name\":\"Xiaotian Hao\"},{\"authorId\":\"2037761803\",\"name\":\"Yue Fu\"},{\"authorId\":\"91243129\",\"name\":\"L. Wang\"},{\"authorId\":\"3255510\",\"name\":\"Jiajie Peng\"}],\"doi\":\"10.1186/s12859-020-03896-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"75870489a0f2029365e30728a2c699ee21d799b5\",\"title\":\"Combining sequence and network information to enhance protein\\u2013protein interaction prediction\",\"url\":\"https://www.semanticscholar.org/paper/75870489a0f2029365e30728a2c699ee21d799b5\",\"venue\":\"BMC Bioinform.\",\"year\":2020},{\"arxivId\":\"2003.08839\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"1412004702\",\"name\":\"Jakob Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3f5f13c6f5c659754086a7faa51d6bc60f577cd1\",\"title\":\"Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3f5f13c6f5c659754086a7faa51d6bc60f577cd1\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1780680011\",\"name\":\"S. Kelly\"},{\"authorId\":\"1780674129\",\"name\":\"Jacob Newsted\"},{\"authorId\":\"2507766\",\"name\":\"W. Banzhaf\"},{\"authorId\":\"2092891\",\"name\":\"C. Gondro\"}],\"doi\":\"10.1145/3377930.3390216\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c8ae6eff4062cd7e2f2cbb03935e9f5b8d88b03\",\"title\":\"A modular memory framework for time series prediction\",\"url\":\"https://www.semanticscholar.org/paper/5c8ae6eff4062cd7e2f2cbb03935e9f5b8d88b03\",\"venue\":\"GECCO\",\"year\":2020},{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"104012169\",\"name\":\"J. Guo\"},{\"authorId\":\"34824727\",\"name\":\"I. Harmati\"}],\"doi\":\"10.1016/j.conengprac.2020.104525\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2e376914e93ea10e581d6d53542be4406b393f48\",\"title\":\"Evaluating semi-cooperative Nash/Stackelberg Q-learning for traffic routes plan in a single intersection\",\"url\":\"https://www.semanticscholar.org/paper/2e376914e93ea10e581d6d53542be4406b393f48\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1909.13794\",\"authors\":[{\"authorId\":null,\"name\":\"Yunkai Wang\"},{\"authorId\":\"153224592\",\"name\":\"Shenhan Jia\"},{\"authorId\":\"3766266\",\"name\":\"Zexi Chen\"},{\"authorId\":\"48783268\",\"name\":\"Zheyuan Huang\"},{\"authorId\":\"51130379\",\"name\":\"R. Xiong\"}],\"doi\":\"10.1109/robio49542.2019.8961725\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b4c1352fc1718fc6a30f9623d5499c49f77cedd2\",\"title\":\"Multi-agent Collaboration for Feasible Collaborative Behavior Construction and Evaluation*\",\"url\":\"https://www.semanticscholar.org/paper/b4c1352fc1718fc6a30f9623d5499c49f77cedd2\",\"venue\":\"2019 IEEE International Conference on Robotics and Biomimetics (ROBIO)\",\"year\":2019},{\"arxivId\":\"2011.04405\",\"authors\":[{\"authorId\":\"1737840870\",\"name\":\"Jiajing Ling\"},{\"authorId\":\"151071357\",\"name\":\"Kushagra Chandak\"},{\"authorId\":\"40305195\",\"name\":\"Akshat Kumar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"30e6fff174645d090f12fe51b9285101a0db4194\",\"title\":\"Combining Propositional Logic Based Decision Diagrams with Decision Making in Urban Systems\",\"url\":\"https://www.semanticscholar.org/paper/30e6fff174645d090f12fe51b9285101a0db4194\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.00127\",\"authors\":[{\"authorId\":\"143613433\",\"name\":\"K. Jiang\"},{\"authorId\":\"12836918\",\"name\":\"X. Qin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3406d877b7b9faed2c791a7d5b869d8f2b927e91\",\"title\":\"Reinforcement Learning with Goal-Distance Gradient\",\"url\":\"https://www.semanticscholar.org/paper/3406d877b7b9faed2c791a7d5b869d8f2b927e91\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144239399\",\"name\":\"Z. Tong\"},{\"authorId\":\"50688677\",\"name\":\"Hongjian Chen\"},{\"authorId\":\"50587937\",\"name\":\"Xiao-mei Deng\"},{\"authorId\":\"39893222\",\"name\":\"KenLi Li\"},{\"authorId\":\"153141874\",\"name\":\"Keqin Li\"}],\"doi\":\"10.1016/j.ins.2019.10.035\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9ad02c629d295d269d9509e69c4093fb3b260fbb\",\"title\":\"A scheduling scheme in the cloud computing environment using deep Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/9ad02c629d295d269d9509e69c4093fb3b260fbb\",\"venue\":\"Inf. Sci.\",\"year\":2020}],\"corpusId\":75135102,\"doi\":\"10.24963/ijcai.2019/323\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"ec80965b72de5076a183026ba182068f6e0a928a\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2617208\",\"name\":\"Dayong Ye\"},{\"authorId\":\"34994053\",\"name\":\"M. Zhang\"},{\"authorId\":\"46285618\",\"name\":\"Yun Yang\"}],\"doi\":\"10.3390/s150510026\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abec9b327b60179138af0855431f5953f48bcbe2\",\"title\":\"A Multi-Agent Framework for Packet Routing in Wireless Sensor Networks\",\"url\":\"https://www.semanticscholar.org/paper/abec9b327b60179138af0855431f5953f48bcbe2\",\"venue\":\"Sensors\",\"year\":2015},{\"arxivId\":\"1810.06394\",\"authors\":[{\"authorId\":\"3081531\",\"name\":\"Jiechao Xiong\"},{\"authorId\":\"47598805\",\"name\":\"Qing Wang\"},{\"authorId\":\"3069462\",\"name\":\"Zhuoran Yang\"},{\"authorId\":\"145551072\",\"name\":\"Peng Sun\"},{\"authorId\":\"39943719\",\"name\":\"L. Han\"},{\"authorId\":\"40344025\",\"name\":\"Yang Zheng\"},{\"authorId\":\"2268469\",\"name\":\"Haobo Fu\"},{\"authorId\":\"38144094\",\"name\":\"T. Zhang\"},{\"authorId\":\"1691762\",\"name\":\"J. Liu\"},{\"authorId\":\"89749055\",\"name\":\"H. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"328233806be18fe15176ce71debaecd95e771fcf\",\"title\":\"Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space\",\"url\":\"https://www.semanticscholar.org/paper/328233806be18fe15176ce71debaecd95e771fcf\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peng Peng\"},{\"authorId\":null,\"name\":\"Quan Yuan\"},{\"authorId\":null,\"name\":\"Ying Wen\"},{\"authorId\":null,\"name\":\"Yaodong Yang\"},{\"authorId\":null,\"name\":\"Zhenkun Tang\"},{\"authorId\":null,\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang. Multiagent bidirectionally-coordinated nets games\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1703.10069,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32630148\",\"name\":\"H. S. Wolff\"}],\"doi\":\"10.1136/pgmj.46.536.343\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8d9b10cf54629364523ec065e6307ab87f7d4f0\",\"title\":\"Sensors.\",\"url\":\"https://www.semanticscholar.org/paper/b8d9b10cf54629364523ec065e6307ab87f7d4f0\",\"venue\":\"\",\"year\":1970},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Timothy P. Lillicrap\"},{\"authorId\":null,\"name\":\"Jonathan J. Hunt\"},{\"authorId\":null,\"name\":\"Alexander Pritzel\"},{\"authorId\":null,\"name\":\"Nicolas Heess\"},{\"authorId\":null,\"name\":\"Tom Erez\"},{\"authorId\":null,\"name\":\"Yuval Tassa\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Daan Wierstra. Continuous control with deep reinforceme learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Proceedings of ICLR\",\"url\":\"\",\"venue\":\"pages 1052\\u20131059,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46636184\",\"name\":\"P. Peng\"},{\"authorId\":\"145001851\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Alex Graves\"},{\"authorId\":null,\"name\":\"Ioannis Antonoglou\"},{\"authorId\":null,\"name\":\"Daan Wierstra\"},{\"authorId\":null,\"name\":\"Martin A. Riedmiller. Playing atari with deep reinforceme learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1312.5602,\",\"year\":2013},{\"arxivId\":\"1706.02275\",\"authors\":[{\"authorId\":\"2054294\",\"name\":\"Ryan Lowe\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2080746\",\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"title\":\"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\"url\":\"https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ermo Wei\"},{\"authorId\":null,\"name\":\"Drew Wicke\"},{\"authorId\":null,\"name\":\"Sean Luke. Hierarchical approaches for reinforcement le space\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of AAAI\",\"url\":\"\",\"venue\":\"pages 3211\\u20133218,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ryan Lowe\"},{\"authorId\":null,\"name\":\"Yi Wu\"},{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Igor Mordatch. Multi-agent actor-critic for mixed coope environments\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of NeurIPS\",\"url\":\"\",\"venue\":\"pages 6382\\u20136393,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"113887627\",\"name\":\"M. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c260a51d57c4adff52838a39ee474864696d0e40\",\"title\":\"Half Field Offense: An Environment for Multiagent Learning and Ad Hoc Teamwork\",\"url\":\"https://www.semanticscholar.org/paper/c260a51d57c4adff52838a39ee474864696d0e40\",\"venue\":\"\",\"year\":2016},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1207.3231\",\"authors\":[{\"authorId\":\"33403228\",\"name\":\"Yongcan Cao\"},{\"authorId\":\"32981010\",\"name\":\"W. Yu\"},{\"authorId\":\"145805552\",\"name\":\"W. Ren\"},{\"authorId\":\"145205046\",\"name\":\"G. Chen\"}],\"doi\":\"10.1109/TII.2012.2219061\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b4eceb166de3f0c96ceae657975b8394c890a4f4\",\"title\":\"An Overview of Recent Progress in the Study of Distributed Multi-Agent Coordination\",\"url\":\"https://www.semanticscholar.org/paper/b4eceb166de3f0c96ceae657975b8394c890a4f4\",\"venue\":\"IEEE Transactions on Industrial Informatics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47598805\",\"name\":\"Qing Wang\"},{\"authorId\":\"3081531\",\"name\":\"Jiechao Xiong\"},{\"authorId\":\"39943719\",\"name\":\"L. Han\"},{\"authorId\":\"145551072\",\"name\":\"Peng Sun\"},{\"authorId\":\"49957601\",\"name\":\"Han Liu\"},{\"authorId\":\"47593026\",\"name\":\"T. Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"caeb088b19b0829551ef4bf3da8b1a5c98bf8e73\",\"title\":\"Exponentially Weighted Imitation Learning for Batched Historical Data\",\"url\":\"https://www.semanticscholar.org/paper/caeb088b19b0829551ef4bf3da8b1a5c98bf8e73\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Qing Wang\"},{\"authorId\":null,\"name\":\"Jiechao Xiong\"},{\"authorId\":null,\"name\":\"Lei Han\"},{\"authorId\":null,\"name\":\"Peng Sun\"},{\"authorId\":null,\"name\":\"Han Liu\"},{\"authorId\":null,\"name\":\"Tong Zhang. Exponentially weighted imitation learning f data\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of NeurIPS\",\"url\":\"\",\"venue\":\"pages 6291\\u20136300,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"69424391\",\"name\":\"Ieee Xplore\"}],\"doi\":\"10.1109/tii.9424\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"031993ba37949b18ce2c3a93690dc1f00105eca7\",\"title\":\"IEEE transactions on industrial informatics\",\"url\":\"https://www.semanticscholar.org/paper/031993ba37949b18ce2c3a93690dc1f00105eca7\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ermo Wei\"},{\"authorId\":null,\"name\":\"Drew Wicke\"},{\"authorId\":null,\"name\":\"David Freelan\"},{\"authorId\":null,\"name\":\"Sean Luke. Multiagent soft q-learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of AAAI\",\"url\":\"\",\"venue\":\"pages 1729\\u20131736,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Warwick Masson\"},{\"authorId\":null,\"name\":\"Pravesh Ranchod\"},{\"authorId\":null,\"name\":\"George Konidaris. Reinforcement learning with parameteriz actions\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of AAAI\",\"url\":\"\",\"venue\":\"pages 1934\\u2013 1940,\",\"year\":2016},{\"arxivId\":\"1804.09817\",\"authors\":[{\"authorId\":\"34765120\",\"name\":\"E. Wei\"},{\"authorId\":\"40617392\",\"name\":\"D. Wicke\"},{\"authorId\":\"2434084\",\"name\":\"David Freelan\"},{\"authorId\":\"1706276\",\"name\":\"S. Luke\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cfbdca4d494fc2a50c25d69486c15b195b42b989\",\"title\":\"Multiagent Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/cfbdca4d494fc2a50c25d69486c15b195b42b989\",\"venue\":\"AAAI Spring Symposia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D. Tejas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kulkarni , Karthik Narasimhan , Ardavan Saeedi , and Josh Tenenbaum . Hierarchical deep reinforcement learning : Integrating temporal abstraction and intrinsic motivation\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":null,\"name\":\"Peter Stone. Deep reinforcement learning in parameterize space\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of ICLR\",\"url\":\"\",\"venue\":\"pages 861\\u2013868,\",\"year\":2016},{\"arxivId\":\"1810.09656\",\"authors\":[{\"authorId\":\"34765120\",\"name\":\"E. Wei\"},{\"authorId\":\"40617392\",\"name\":\"D. Wicke\"},{\"authorId\":\"1706276\",\"name\":\"S. Luke\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f9160fb21d80498842f137181ab12f1beeaa1f4\",\"title\":\"Hierarchical Approaches for Reinforcement Learning in Parameterized Action Space\",\"url\":\"https://www.semanticscholar.org/paper/1f9160fb21d80498842f137181ab12f1beeaa1f4\",\"venue\":\"AAAI Spring Symposia\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hongyao Tang\"},{\"authorId\":null,\"name\":\"Jianye Hao\"},{\"authorId\":null,\"name\":\"Tangjie Lv\"},{\"authorId\":null,\"name\":\"Yingfeng Chen\"},{\"authorId\":null,\"name\":\"Zongzhang Zhang\"},{\"authorId\":null,\"name\":\"Hangtian Jia\"},{\"authorId\":null,\"name\":\"Chunxu Ren\"},{\"authorId\":null,\"name\":\"Yan Zheng\"},{\"authorId\":null,\"name\":\"Changjie Fan\"},{\"authorId\":null,\"name\":\"Li Wang. Hierarchical deep multiagent reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1809.09332,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mikayel Samvelyan Tabish Rashid\"},{\"authorId\":null,\"name\":\"Christian Schr\\u00f6der de Witt\"},{\"authorId\":null,\"name\":\"Gregory Farquhar\"},{\"authorId\":null,\"name\":\"N Jakob\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Foerster , and Shimon Whiteson . Qmix : Monotonic value function factorisation for deep multi - agent reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jakob N. Foerster\"},{\"authorId\":null,\"name\":\"Yannis M. Assael\"},{\"authorId\":null,\"name\":\"Nando de Freitas\"},{\"authorId\":null,\"name\":\"Shimon Whiteson. Learning to communicate with deep multi- learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of NeurIPS\",\"url\":\"\",\"venue\":\"pages 2137\\u20132145,\",\"year\":2016}],\"title\":\"Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Testbed\",\"topicId\":\"1705\",\"url\":\"https://www.semanticscholar.org/topic/1705\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Centralized computing\",\"topicId\":\"18260\",\"url\":\"https://www.semanticscholar.org/topic/18260\"},{\"topic\":\"Programming paradigm\",\"topicId\":\"29522\",\"url\":\"https://www.semanticscholar.org/topic/29522\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Driven right leg circuit\",\"topicId\":\"427743\",\"url\":\"https://www.semanticscholar.org/topic/427743\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Massively multiplayer online role-playing game\",\"topicId\":\"336834\",\"url\":\"https://www.semanticscholar.org/topic/336834\"},{\"topic\":\"Spaces\",\"topicId\":\"124\",\"url\":\"https://www.semanticscholar.org/topic/124\"},{\"topic\":\"Hybrid fibre-optic\",\"topicId\":\"3705959\",\"url\":\"https://www.semanticscholar.org/topic/3705959\"},{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"}],\"url\":\"https://www.semanticscholar.org/paper/ec80965b72de5076a183026ba182068f6e0a928a\",\"venue\":\"IJCAI\",\"year\":2019}\n"