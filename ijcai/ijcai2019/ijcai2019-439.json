"{\"abstract\":\"Latent-state environments with long horizons, such as those faced by recommender systems, pose significant challenges for reinforcement learning (RL). In this work, we identify and analyze several key hurdles for RL in such environments, including belief state error and small action advantage. We develop a general principle of advantage amplification that can overcome these hurdles through the use of temporal abstraction. We propose several aggregation methods and prove they induce amplification in certain settings. We also bound the loss in optimality incurred by our methods in environments where latent state evolves slowly and demonstrate their performance empirically in a stylized user-modeling task.\",\"arxivId\":\"1905.13559\",\"authors\":[{\"authorId\":\"2538104\",\"name\":\"M. Mladenov\",\"url\":\"https://www.semanticscholar.org/author/2538104\"},{\"authorId\":\"3174612\",\"name\":\"Ofer Meshi\",\"url\":\"https://www.semanticscholar.org/author/3174612\"},{\"authorId\":\"147126120\",\"name\":\"Jayden Ooi\",\"url\":\"https://www.semanticscholar.org/author/147126120\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\",\"url\":\"https://www.semanticscholar.org/author/1714772\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\",\"url\":\"https://www.semanticscholar.org/author/145646162\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1904.03821\",\"authors\":[{\"authorId\":\"3247303\",\"name\":\"Inseok Oh\"},{\"authorId\":\"93702774\",\"name\":\"Seungeun Rho\"},{\"authorId\":\"144804090\",\"name\":\"Sangbin Moon\"},{\"authorId\":\"66876351\",\"name\":\"Seongho Son\"},{\"authorId\":\"66406708\",\"name\":\"H. Lee\"},{\"authorId\":\"50622284\",\"name\":\"Jinyun Chung\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aa0a37fba8f5f9327ab43c222fc1e8fb3901a837\",\"title\":\"Creating Pro-Level AI for Real-Time Fighting Game with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/aa0a37fba8f5f9327ab43c222fc1e8fb3901a837\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1909.04847\",\"authors\":[{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"145344473\",\"name\":\"C. Hsu\"},{\"authorId\":\"47182790\",\"name\":\"M. Mladenov\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1734069\",\"name\":\"J. Wang\"},{\"authorId\":\"144265846\",\"name\":\"R. Wu\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f81372f1c489c0ffa695ba6f623bb71dc2dc60ed\",\"title\":\"RecSim: A Configurable Simulation Platform for Recommender Systems\",\"url\":\"https://www.semanticscholar.org/paper/f81372f1c489c0ffa695ba6f623bb71dc2dc60ed\",\"venue\":\"ArXiv\",\"year\":2019}],\"corpusId\":173188241,\"doi\":\"10.24963/ijcai.2019/439\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"78824341f657af4fd6d92ebfc080d0c34eec296b\",\"references\":[{\"arxivId\":\"1702.06054\",\"authors\":[{\"authorId\":\"48703799\",\"name\":\"S. Sharma\"},{\"authorId\":\"2943530\",\"name\":\"Aravind S. Lakshminarayanan\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2ad53229b33ddfd3447045ea28c4a0687747b6b0\",\"title\":\"Learning to Repeat: Fine Grained Action Repetition for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2ad53229b33ddfd3447045ea28c4a0687747b6b0\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Ramanathan\"},{\"authorId\":null,\"name\":\"A. Bonomo\"},{\"authorId\":null,\"name\":\"S. Jain\"},{\"authorId\":null,\"name\":\"E. H. Chi\"},{\"authorId\":null,\"name\":\"J. Gillenwater\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Temporal regularization for Markov decision process The learning curve equation\",\"url\":\"\",\"venue\":\"Psychological Monographs\",\"year\":1919},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145427893\",\"name\":\"A. Silvestrini\"},{\"authorId\":\"2695223\",\"name\":\"David Veredas\"}],\"doi\":\"10.1111/j.1467-6419.2007.00538.x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"149745f3a0b8d4f0dabdf213cbb2484f04f4f205\",\"title\":\"Temporal Aggregation of Univariate and Multivariate Time Series Models: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/149745f3a0b8d4f0dabdf213cbb2484f04f4f205\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34763783\",\"name\":\"J. W. Roberts\"},{\"authorId\":\"1726802\",\"name\":\"Russ Tedrake\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0bcdc0143a1bff5a1cbea7bc38157bb821ae1b9c\",\"title\":\"Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/0bcdc0143a1bff5a1cbea7bc38157bb821ae1b9c\",\"venue\":\"NIPS\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"X. Zhao\"},{\"authorId\":null,\"name\":\"L. Xia\"},{\"authorId\":null,\"name\":\"L. Zhang\"},{\"authorId\":null,\"name\":\"Z. Ding\"},{\"authorId\":null,\"name\":\"D. Yin\"},{\"authorId\":null,\"name\":\"J. Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep reinforcement learning for pagewise recommendations\",\"url\":\"\",\"venue\":\"RecSys-18, pp.95\\u2013103\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1623397502\",\"name\":\"Saskia Bonjour\"},{\"authorId\":\"1623405226\",\"name\":\"Doutje Lettinga\"},{\"authorId\":\"1623397535\",\"name\":\"Christian Joppke\"}],\"doi\":\"10.1515/9783111576855-009\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"776f6d45b38cf048ff30cea41f0cd3ceeefed622\",\"title\":\"D\",\"url\":\"https://www.semanticscholar.org/paper/776f6d45b38cf048ff30cea41f0cd3ceeefed622\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"X. Zhao\"},{\"authorId\":null,\"name\":\"L. Zhang\"},{\"authorId\":null,\"name\":\"Z. Ding\"},{\"authorId\":null,\"name\":\"D. Yin\"},{\"authorId\":null,\"name\":\"Y. Zhao\"},{\"authorId\":null,\"name\":\"J. Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep reinforcement learning for listwise recommendations\",\"url\":\"\",\"venue\":\"arXiv:1801.00209\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47753224\",\"name\":\"R. Smallwood\"},{\"authorId\":\"3395263\",\"name\":\"E. Sondik\"}],\"doi\":\"10.1287/opre.21.5.1071\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"450b0047c7314cb47b1b1bf1d081d9472acaa0e4\",\"title\":\"The Optimal Control of Partially Observable Markov Processes over a Finite Horizon\",\"url\":\"https://www.semanticscholar.org/paper/450b0047c7314cb47b1b1bf1d081d9472acaa0e4\",\"venue\":\"Oper. Res.\",\"year\":1973},{\"arxivId\":\"1705.09353\",\"authors\":[{\"authorId\":\"35526495\",\"name\":\"C. Downey\"},{\"authorId\":\"145253835\",\"name\":\"A. Hefny\"},{\"authorId\":\"3288815\",\"name\":\"B. Boots\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"29892904\",\"name\":\"Boyue Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9c92c6729ce0ecc52476d0a606769e2adb0de59a\",\"title\":\"Predictive State Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/9c92c6729ce0ecc52476d0a606769e2adb0de59a\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1301.0600\",\"authors\":[{\"authorId\":\"1719532\",\"name\":\"Guy Shani\"},{\"authorId\":\"46589296\",\"name\":\"D. Heckerman\"},{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9b739b106b585c963cca70a10f38e564cc9d98cc\",\"title\":\"An MDP-Based Recommender System\",\"url\":\"https://www.semanticscholar.org/paper/9b739b106b585c963cca70a10f38e564cc9d98cc\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2005},{\"arxivId\":\"1512.08562\",\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"3314041\",\"name\":\"Ari Pakman\"},{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4a026fd65af4ba3575e64174de56fee093fa3330\",\"title\":\"Taming the Noise in Reinforcement Learning via Soft Updates\",\"url\":\"https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":\"1811.00429\",\"authors\":[{\"authorId\":\"3451632\",\"name\":\"Pierre Thodoroff\"},{\"authorId\":\"1777414\",\"name\":\"A. Durand\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb5cd5ef44509cb15c011428c031056081b0535e\",\"title\":\"Temporal Regularization in Markov Decision Process\",\"url\":\"https://www.semanticscholar.org/paper/bb5cd5ef44509cb15c011428c031056081b0535e\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4876b758f66311325291686d64c285e2e01d0dd2\",\"title\":\"Between MDPs and Semi-MDPs : Learning , Planning , and Representing Knowledge at Multiple Temporal Scales\",\"url\":\"https://www.semanticscholar.org/paper/4876b758f66311325291686d64c285e2e01d0dd2\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1844179\",\"name\":\"L. Baird\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"18f4e6b5b5b226a47e4aed700ea8b9ae6f121308\",\"title\":\"Reinforcement Learning Through Gradient Descent\",\"url\":\"https://www.semanticscholar.org/paper/18f4e6b5b5b226a47e4aed700ea8b9ae6f121308\",\"venue\":\"\",\"year\":1999},{\"arxivId\":\"1512.04860\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f88a6f6fd6611543220482e6b3a5f379b7bf5049\",\"title\":\"Increasing the Action Gap: New Operators for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f88a6f6fd6611543220482e6b3a5f379b7bf5049\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1301.7381\",\"authors\":[{\"authorId\":\"1731761\",\"name\":\"M. Hauskrecht\"},{\"authorId\":\"1735986\",\"name\":\"N. Meuleau\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"},{\"authorId\":\"39971338\",\"name\":\"T. Dean\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"036373f17e5e47bcadc289e6c57d61cf5e08fe3d\",\"title\":\"Hierarchical Solution of Markov Decision Processes using Macro-actions\",\"url\":\"https://www.semanticscholar.org/paper/036373f17e5e47bcadc289e6c57d61cf5e08fe3d\",\"venue\":\"UAI\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2538104\",\"name\":\"M. Mladenov\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"},{\"authorId\":\"3174612\",\"name\":\"Ofer Meshi\"},{\"authorId\":\"1684677\",\"name\":\"G. Elidan\"},{\"authorId\":\"2409297\",\"name\":\"Tyler Lu\"}],\"doi\":\"10.24963/ijcai.2017/346\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eeb9f3dce2a79f548f7e6c8f7157ae68233ccb2c\",\"title\":\"Logistic Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/eeb9f3dce2a79f548f7e6c8f7157ae68233ccb2c\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37755468\",\"name\":\"C. Daniel\"},{\"authorId\":\"70211944\",\"name\":\"G. Neumann\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8101ec9a994551edfdc7c79ebc89ed939cd07eb3\",\"title\":\"Hierarchical Relative Entropy Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/8101ec9a994551edfdc7c79ebc89ed939cd07eb3\",\"venue\":\"AISTATS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2042413\",\"name\":\"E. Ie\"},{\"authorId\":\"20048351\",\"name\":\"Vihan Jain\"},{\"authorId\":null,\"name\":\"Jing Wang\"},{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"3065516\",\"name\":\"R. Agarwal\"},{\"authorId\":\"144693688\",\"name\":\"R. Wu\"},{\"authorId\":\"2061550\",\"name\":\"Heng-Tze Cheng\"},{\"authorId\":\"2158476\",\"name\":\"T. Chandra\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":\"10.24963/IJCAI.2019/360\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ecba7d8d3981b88f6ae29ba8905c9229e1c8779a\",\"title\":\"SlateQ: A Tractable Decomposition for Reinforcement Learning with Recommendation Sets\",\"url\":\"https://www.semanticscholar.org/paper/ecba7d8d3981b88f6ae29ba8905c9229e1c8779a\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"439294e13425d60f1141eadb695bfd7281821f95\",\"title\":\"Between MOPs and Semi-MOP: Learning, Planning & Representing Knowledge at Multiple Temporal Scales\",\"url\":\"https://www.semanticscholar.org/paper/439294e13425d60f1141eadb695bfd7281821f95\",\"venue\":\"\",\"year\":1998},{\"arxivId\":\"1801.05532\",\"authors\":[{\"authorId\":\"3261484\",\"name\":\"Sungwoon Choi\"},{\"authorId\":\"3419011\",\"name\":\"Heonseok Ha\"},{\"authorId\":\"27642187\",\"name\":\"Uiwon Hwang\"},{\"authorId\":\"15797750\",\"name\":\"Chanju Kim\"},{\"authorId\":\"2577039\",\"name\":\"Jung-Woo Ha\"},{\"authorId\":\"2999019\",\"name\":\"S. Yoon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"50f9c36cca2cf8f4fab4776c92fa1521ddbf68b1\",\"title\":\"Reinforcement Learning based Recommender System using Biclustering Technique\",\"url\":\"https://www.semanticscholar.org/paper/50f9c36cca2cf8f4fab4776c92fa1521ddbf68b1\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1301.7405\",\"authors\":[{\"authorId\":\"145726861\",\"name\":\"R. Parr\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f7e986884c7fe0a8427bc656151feae4e9c03fe5\",\"title\":\"Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems\",\"url\":\"https://www.semanticscholar.org/paper/f7e986884c7fe0a8427bc656151feae4e9c03fe5\",\"venue\":\"UAI\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152784304\",\"name\":\"A. Kumar\"},{\"authorId\":\"151506072\",\"name\":\"A. Sharma\"},{\"authorId\":\"91318330\",\"name\":\"B. Torre\"},{\"authorId\":\"3976878\",\"name\":\"F. Albericio\"}],\"doi\":\"10.5040/9781474284028.0024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"02f36224042ba7fccb20af534eb8a764f72f3d6d\",\"title\":\"S\",\"url\":\"https://www.semanticscholar.org/paper/02f36224042ba7fccb20af534eb8a764f72f3d6d\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Avraham Adler\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"aecb95717d82d547bcc11ee4b74f714b9d656091\",\"title\":\"Lambert-W Function\",\"url\":\"https://www.semanticscholar.org/paper/aecb95717d82d547bcc11ee4b74f714b9d656091\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3061599\",\"name\":\"Nima Taghipour\"},{\"authorId\":\"1695547\",\"name\":\"A. Kardan\"},{\"authorId\":\"3354354\",\"name\":\"S. S. Ghidary\"}],\"doi\":\"10.1145/1297231.1297250\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b361fc8e42111a25946b4cf6fe04a857accda4ce\",\"title\":\"Usage-based web recommendations: a reinforcement learning approach\",\"url\":\"https://www.semanticscholar.org/paper/b361fc8e42111a25946b4cf6fe04a857accda4ce\",\"venue\":\"RecSys '07\",\"year\":2007},{\"arxivId\":\"1709.07796\",\"authors\":[{\"authorId\":\"1389921282\",\"name\":\"Vincent Fran\\u00e7ois-Lavet\"},{\"authorId\":\"1751167\",\"name\":\"D. Ernst\"},{\"authorId\":\"120448038\",\"name\":\"Raphael Fonteneau\"}],\"doi\":\"10.1613/jair.1.11478\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"de1d820a959e45f27e13b63ee00770df9cde4538\",\"title\":\"On overfitting and asymptotic bias in batch reinforcement learning with partial observability\",\"url\":\"https://www.semanticscholar.org/paper/de1d820a959e45f27e13b63ee00770df9cde4538\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1709005\",\"name\":\"Georgios Theocharous\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0c11db37dd24ca3a866481303bebfc37a93a37d0\",\"title\":\"Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees\",\"url\":\"https://www.semanticscholar.org/paper/0c11db37dd24ca3a866481303bebfc37a93a37d0\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":\"1301.7362\",\"authors\":[{\"authorId\":\"1681881\",\"name\":\"Xavier Boyen\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"92aea50331c19fe9716d3a9a02e26704afe24d88\",\"title\":\"Tractable Inference for Complex Stochastic Processes\",\"url\":\"https://www.semanticscholar.org/paper/92aea50331c19fe9716d3a9a02e26704afe24d88\",\"venue\":\"UAI\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"C. Downey\"},{\"authorId\":null,\"name\":\"A. Hefny\"},{\"authorId\":null,\"name\":\"B. Boots\"},{\"authorId\":null,\"name\":\"G. J. Gordon\"},{\"authorId\":null,\"name\":\"B. Li. Predictive state recurrent neural networks\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"NIPS-17\",\"url\":\"\",\"venue\":\"pp.6053\\u20136064.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2011924\",\"name\":\"A. Farahmand\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e094bf04c932faf37059c16d5085280b1012446b\",\"title\":\"Action-Gap Phenomenon in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e094bf04c932faf37059c16d5085280b1012446b\",\"venue\":\"NIPS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Theocharous\"},{\"authorId\":null,\"name\":\"P. S. Thomas\"},{\"authorId\":null,\"name\":\"M. Ghavamzadeh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Safe policy search\",\"url\":\"\",\"venue\":\"ICML-2014 Workshop on Customer Life-Time Value Optimization in Digital Marketing, pp.1806\\u20131812\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Francois-Lavet\"},{\"authorId\":null,\"name\":\"G. Rabusseau\"},{\"authorId\":null,\"name\":\"J. Pineau\"},{\"authorId\":null,\"name\":\"D. Ernst\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and R\",\"url\":\"\",\"venue\":\"Fonteneau. On overfitting and asymptotic bias in batch reinforcement learning with partial observability,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"A. Durand P. Thodoroff\"},{\"authorId\":null,\"name\":\"D. Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Temporal regularization for Markov decision process The learning curve equation\",\"url\":\"\",\"venue\":\"Psychological Monographs\",\"year\":1919},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1705.08417\",\"authors\":[{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":\"10.24963/ijcai.2017/656\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5ba74bd3b6c9bb0287d8835621ddd40dd3ebbf4\",\"title\":\"Reinforcement Learning with a Corrupted Reward Channel\",\"url\":\"https://www.semanticscholar.org/paper/e5ba74bd3b6c9bb0287d8835621ddd40dd3ebbf4\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"1801.00209\",\"authors\":[{\"authorId\":\"2733057\",\"name\":\"Xiangyu Zhao\"},{\"authorId\":\"48570713\",\"name\":\"L. Zhang\"},{\"authorId\":\"1775738\",\"name\":\"Zhuoye Ding\"},{\"authorId\":\"50559722\",\"name\":\"D. Yin\"},{\"authorId\":\"46317126\",\"name\":\"Yihong Zhao\"},{\"authorId\":\"1736632\",\"name\":\"Jiliang Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"63bce48a68c027f8f7df7ed5cd6e99d9e3ac14e9\",\"title\":\"Deep Reinforcement Learning for List-wise Recommendations\",\"url\":\"https://www.semanticscholar.org/paper/63bce48a68c027f8f7df7ed5cd6e99d9e3ac14e9\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31815234\",\"name\":\"M. Wilhelm\"},{\"authorId\":\"27606811\",\"name\":\"A. Ramanathan\"},{\"authorId\":\"51478782\",\"name\":\"A. Bonomo\"},{\"authorId\":\"2219642\",\"name\":\"S. Jain\"},{\"authorId\":\"2226805\",\"name\":\"Ed Huai-hsin Chi\"},{\"authorId\":\"144340937\",\"name\":\"J. Gillenwater\"}],\"doi\":\"10.1145/3269206.3272018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c09b090a664674ba528957a28330ba07d1004dcc\",\"title\":\"Practical Diversified Recommendations on YouTube with Determinantal Point Processes\",\"url\":\"https://www.semanticscholar.org/paper/c09b090a664674ba528957a28330ba07d1004dcc\",\"venue\":\"CIKM\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50042-8\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a579d06ac278e14948f67748cd651e4eb617ae4e\",\"title\":\"Learning Without State-Estimation in Partially Observable Markovian Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/a579d06ac278e14948f67748cd651e4eb617ae4e\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730590\",\"name\":\"A. Barto\"},{\"authorId\":\"1850503\",\"name\":\"S. Mahadevan\"}],\"doi\":\"10.1023/A:1022140919877\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0a8149fb5aa8a5684e7d530c264451a5cb9250f5\",\"title\":\"Recent Advances in Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0a8149fb5aa8a5684e7d530c264451a5cb9250f5\",\"venue\":\"Discret. Event Dyn. Syst.\",\"year\":2003},{\"arxivId\":\"1702.08892\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"title\":\"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4a7de0669fd835b2efcab97c7d3dc28ea7a1e6a3\",\"title\":\"Predictive Representations of State\",\"url\":\"https://www.semanticscholar.org/paper/4a7de0669fd835b2efcab97c7d3dc28ea7a1e6a3\",\"venue\":\"NIPS\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2815948\",\"name\":\"Henning Hohnhold\"},{\"authorId\":\"1400961380\",\"name\":\"Deirdre O'Brien\"},{\"authorId\":\"144575353\",\"name\":\"Diane Tang\"}],\"doi\":\"10.1145/2783258.2788583\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"085df6a5790aae5c455388350b0b0f422bc4da14\",\"title\":\"Focusing on the Long-term: It's Good for Users and Business\",\"url\":\"https://www.semanticscholar.org/paper/085df6a5790aae5c455388350b0b0f422bc4da14\",\"venue\":\"KDD\",\"year\":2015},{\"arxivId\":\"1811.00260\",\"authors\":[{\"authorId\":\"144363634\",\"name\":\"Jason Gauci\"},{\"authorId\":\"32577240\",\"name\":\"Edoardo Conti\"},{\"authorId\":\"2397352\",\"name\":\"Yitao Liang\"},{\"authorId\":\"1894596\",\"name\":\"Kittipat Virochsiri\"},{\"authorId\":\"144047307\",\"name\":\"Y. He\"},{\"authorId\":\"51010350\",\"name\":\"Zachary Kaden\"},{\"authorId\":\"49540857\",\"name\":\"V. Narayanan\"},{\"authorId\":\"3202244\",\"name\":\"Xiaohui Ye\"},{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"82ef41d2065e366ce1130d36b53881e06f1af2ed\",\"title\":\"Horizon: Facebook's Open Source Applied Reinforcement Learning Platform\",\"url\":\"https://www.semanticscholar.org/paper/82ef41d2065e366ce1130d36b53881e06f1af2ed\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34609940\",\"name\":\"M. Jaber\"}],\"doi\":\"10.1201/9781420038347.ch30\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c741157af5e00bd27d29660e0d6349d5e797981a\",\"title\":\"Learning and forgetting models and their applications\",\"url\":\"https://www.semanticscholar.org/paper/c741157af5e00bd27d29660e0d6349d5e797981a\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1762737\",\"name\":\"Nikolay Archak\"},{\"authorId\":\"1728881\",\"name\":\"V. Mirrokni\"},{\"authorId\":\"144963537\",\"name\":\"S. Muthukrishnan\"}],\"doi\":\"10.1007/978-3-642-35311-6_7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d04e6ed536d72c2354416fb5e078aea7f7bf17f5\",\"title\":\"Budget Optimization for Online Campaigns with Positive Carryover Effects\",\"url\":\"https://www.semanticscholar.org/paper/d04e6ed536d72c2354416fb5e078aea7f7bf17f5\",\"venue\":\"WINE\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70604473\",\"name\":\"L. L. Thurstone\"}],\"doi\":\"10.1037/h0093187\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b08ce3d11f6c1dbefa2f69a724b30200aaa87274\",\"title\":\"The Learning Curve Equation\",\"url\":\"https://www.semanticscholar.org/paper/b08ce3d11f6c1dbefa2f69a724b30200aaa87274\",\"venue\":\"\",\"year\":1919},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Mladenov\"},{\"authorId\":null,\"name\":\"O. Meshi\"},{\"authorId\":null,\"name\":\"J. Ooi\"},{\"authorId\":null,\"name\":\"D. Schuurmans\"},{\"authorId\":null,\"name\":\"C. Boutilier\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Advantage amplification in slowly evolving latent-state environments\",\"url\":\"\",\"venue\":\"arXiv preprint\",\"year\":2019}],\"title\":\"Advantage Amplification in Slowly Evolving Latent-State Environments\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Recommender system\",\"topicId\":\"12428\",\"url\":\"https://www.semanticscholar.org/topic/12428\"},{\"topic\":\"User modeling\",\"topicId\":\"14899\",\"url\":\"https://www.semanticscholar.org/topic/14899\"}],\"url\":\"https://www.semanticscholar.org/paper/78824341f657af4fd6d92ebfc080d0c34eec296b\",\"venue\":\"IJCAI\",\"year\":2019}\n"