"{\"abstract\":\"In the context of learning deterministic policies in continuous domains, we revisit an approach, which was first proposed in Continuous Actor Critic Learning Automaton (CACLA) and later extended in Neural Fitted Actor Critic (NFAC). This approach is based on a policy update different from that of deterministic policy gradient (DPG). Previous work has observed its excellent performance empirically, but a theoretical justification is lacking. To fill this gap, we provide a theoretical explanation to motivate this unorthodox policy update by relating it to another update and making explicit the objective function of the latter. We furthermore discuss in depth the properties of these updates to get a deeper understanding of the overall approach. In addition, we extend it and propose a new trust region algorithm, Penalized NFAC (PeNFAC). Finally, we experimentally demonstrate in several classic control problems that it surpasses the state-of-the-art algorithms to learn deterministic policies.\",\"arxivId\":\"1906.04556\",\"authors\":[{\"authorId\":\"30207605\",\"name\":\"Matthieu Zimmer\",\"url\":\"https://www.semanticscholar.org/author/30207605\"},{\"authorId\":\"144834415\",\"name\":\"Paul Weng\",\"url\":\"https://www.semanticscholar.org/author/144834415\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a4633f06205a7ba4db98494a5ad671b6ac75e06\",\"title\":\"URES IN DETERMINISTIC ENVIRONMENTS WITH SPARSE REWARDS\",\"url\":\"https://www.semanticscholar.org/paper/7a4633f06205a7ba4db98494a5ad671b6ac75e06\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1911.11679\",\"authors\":[{\"authorId\":\"51208779\",\"name\":\"Guillaume Matheron\"},{\"authorId\":\"144249526\",\"name\":\"N. Perrin\"},{\"authorId\":\"97009622\",\"name\":\"Olivier Sigaud\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"84eb3a26622d359650bfd467055482f3a953db33\",\"title\":\"The problem with DDPG: understanding failures in deterministic environments with sparse rewards\",\"url\":\"https://www.semanticscholar.org/paper/84eb3a26622d359650bfd467055482f3a953db33\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153785126\",\"name\":\"M. Zimmer\"},{\"authorId\":\"144834415\",\"name\":\"Paul Weng\"}],\"doi\":\"10.1145/3356464.3357704\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6b78d15a2e14d2caa42e94350280330a210e6eb9\",\"title\":\"An efficient reinforcement learning algorithm for learning deterministic policies in continuous domains\",\"url\":\"https://www.semanticscholar.org/paper/6b78d15a2e14d2caa42e94350280330a210e6eb9\",\"venue\":\"DAI\",\"year\":2019}],\"corpusId\":184487065,\"doi\":\"10.24963/ijcai.2019/625\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"4342598da04242cb0f8e2307d47ca50909ae0717\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Trust Region Policy Optimization\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"page 16,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50739685\",\"name\":\"A. Albrecht\"},{\"authorId\":\"144574741\",\"name\":\"M. Loomes\"},{\"authorId\":\"1685870\",\"name\":\"Kathleen Steinh\\u00f6fel\"},{\"authorId\":\"2954496\",\"name\":\"M. Taupitz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bb738ee9fab0919f50a6a8e9b183fc261b095b07\",\"title\":\"European Symposium on Artificial Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/bb738ee9fab0919f50a6a8e9b183fc261b095b07\",\"venue\":\"\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wierstra\"},{\"authorId\":null,\"name\":\"Shane Legg\"},{\"authorId\":null,\"name\":\"Demis Hassabis\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Humanlevel control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Nature, 518(7540):529\\u2013533,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Danil V. Prokhorov\"},{\"authorId\":null,\"name\":\"Donald C. Wunsch\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Adaptive critic designs\",\"url\":\"\",\"venue\":\"\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4682880\",\"name\":\"M. F. Balcan\"},{\"authorId\":\"7446832\",\"name\":\"Kilian Q. Weinberger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a0442c88be8f16e180d8e365235950d1f7dff436\",\"title\":\"Proceedings of the 33rd International Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/a0442c88be8f16e180d8e365235950d1f7dff436\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"69038308\",\"name\":\"\\u5c71\\u7530 \\u7950\"},{\"authorId\":\"52244440\",\"name\":\"\\u5e73\\u7530 \\u9686\\u5e78\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b1f50f4c33f55d41f710c72980dc53fd8ce12a52\",\"title\":\"Open Dynamics Engine \\u3092\\u7528\\u3044\\u305f\\u30b9\\u30ce\\u30fc\\u30dc\\u30fc\\u30c9\\u30ed\\u30dc\\u30c3\\u30c8\\u30b7\\u30df\\u30e5\\u30ec\\u30fc\\u30bf\\u306e\\u958b\\u767a\",\"url\":\"https://www.semanticscholar.org/paper/b1f50f4c33f55d41f710c72980dc53fd8ce12a52\",\"venue\":\"\",\"year\":2007},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"},{\"authorId\":\"153059475\",\"name\":\"S. Becker\"},{\"authorId\":\"1405497839\",\"name\":\"Z. G. Eds\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"89558a43b3b0a24cd0fdb6d5c2283112493af3a0\",\"title\":\"In Advances in Neural Information Processing Systems 15\",\"url\":\"https://www.semanticscholar.org/paper/89558a43b3b0a24cd0fdb6d5c2283112493af3a0\",\"venue\":\"NIPS 1991\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50844636\",\"name\":\"Vijay R. Konda\"},{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"64d6b75e14987f6e5716468037caf7e2ce84954f\",\"title\":\"Actor-Critic Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/64d6b75e14987f6e5716468037caf7e2ce84954f\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"},{\"authorId\":\"144162125\",\"name\":\"J. Langford\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"523b4ce1c2a1336962444abc1dec215756c2f3e6\",\"title\":\"Approximately Optimal Approximate Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/523b4ce1c2a1336962444abc1dec215756c2f3e6\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hado Van Hasselt\"},{\"authorId\":null,\"name\":\"Marco A. Wiering. Reinforcement learning in continuous a spaces\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning\",\"url\":\"\",\"venue\":\"pages 272\\u2013279,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":\"1802.05054\",\"authors\":[{\"authorId\":\"102281182\",\"name\":\"C\\u00e9dric Colas\"},{\"authorId\":\"3211142\",\"name\":\"Olivier Sigaud\"},{\"authorId\":\"1720664\",\"name\":\"Pierre-Yves Oudeyer\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1fdf0319b4a1db62c611980351e5f4c2f08958cd\",\"title\":\"GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/1fdf0319b4a1db62c611980351e5f4c2f08958cd\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"title\":\"Deterministic Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1022633531479\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"title\":\"Learning to Predict by the Methods of Temporal Differences\",\"url\":\"https://www.semanticscholar.org/paper/a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"venue\":\"Machine Learning\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30207605\",\"name\":\"Matthieu Zimmer\"},{\"authorId\":\"1711682\",\"name\":\"Y. Boniface\"},{\"authorId\":\"1785337\",\"name\":\"A. Dutech\"}],\"doi\":\"10.1109/DEVLRN.2018.8761021\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7877ec383bd3201cd85956f3692ec69aa03ca8f5\",\"title\":\"Developmental Reinforcement Learning through Sensorimotor Space Enlargement\",\"url\":\"https://www.semanticscholar.org/paper/7877ec383bd3201cd85956f3692ec69aa03ca8f5\",\"venue\":\"2018 Joint IEEE 8th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Josef Maatyas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Random optimization\",\"url\":\"\",\"venue\":\"Neural Information Processing Systems\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R Vijay\"},{\"authorId\":null,\"name\":\"John N Konda\"},{\"authorId\":null,\"name\":\"Tsitsiklis\"},{\"authorId\":null,\"name\":\"Lillicrap\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Josef Maatyas. Random optimization. Automation and Remote control\",\"url\":\"\",\"venue\":\"Continuous control with deep reinforcement learning. International Conference on Learning Representations\",\"year\":1965},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30207605\",\"name\":\"Matthieu Zimmer\"},{\"authorId\":\"1711682\",\"name\":\"Y. Boniface\"},{\"authorId\":\"1785337\",\"name\":\"A. Dutech\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"73fc283de6a012ade3492536d7f3c6713086267d\",\"title\":\"Neural fitted actor-critic\",\"url\":\"https://www.semanticscholar.org/paper/73fc283de6a012ade3492536d7f3c6713086267d\",\"venue\":\"ESANN\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145803549\",\"name\":\"V. Vapnik\"},{\"authorId\":\"35224059\",\"name\":\"S. Golowich\"},{\"authorId\":\"46234526\",\"name\":\"Alex Smola\"}],\"doi\":\"10.7551/mitpress/11474.003.0014\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"18c8c66e14d5276e5435376d67b9c9d16d7b58b7\",\"title\":\"Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/18c8c66e14d5276e5435376d67b9c9d16d7b58b7\",\"venue\":\"NIPS 1997\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthieu Zimmer\"},{\"authorId\":null,\"name\":\"Yann Boniface\"},{\"authorId\":null,\"name\":\"Alain Dutech. Developmental reinforcement learning throu enlargement\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In The 8th Joint IEEE International Conference on Development and Learning and on Epigenetic Robotics\",\"url\":\"\",\"venue\":\"September\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Matyas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Random optimization\",\"url\":\"\",\"venue\":\"Automation and Remote control, 26(2):246\\u2013253\",\"year\":1965},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Filip Wolski\"},{\"authorId\":null,\"name\":\"Prafulla Dhariwal\"},{\"authorId\":null,\"name\":\"Alec Radford\"},{\"authorId\":null,\"name\":\"Oleg Klimov. Proximal policy optimization algorithms\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1707.06347,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Vijay R. Konda\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and John N . Tsitsiklis . Actor - Critic Algorithms\",\"url\":\"\",\"venue\":\"Kingma and Jimmy L . Ba . Adam : a Method for Stochastic Optimization . International Conference on Learning Representations , pages 1 \\u2013 13 , 2015 . [ Konda and Tsitsiklis\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Danil V. Prokhorov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Donald C . Wunsch . Adaptive critic designs\",\"url\":\"\",\"venue\":\"Nature\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1446962550\",\"name\":\"Dock Bumpers\"},{\"authorId\":\"1446961266\",\"name\":\"Support Ledgers\"}],\"doi\":\"10.1023/A:1017189329742\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"title\":\"Volume 2\",\"url\":\"https://www.semanticscholar.org/paper/e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"venue\":\"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143741534\",\"name\":\"L. Weinberg\"}],\"doi\":\"10.1134/10513.1608-3032\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"196e88bbaea12aa5bb5be82703e1749b51efabdf\",\"title\":\"Automation and Remote Control\",\"url\":\"https://www.semanticscholar.org/paper/196e88bbaea12aa5bb5be82703e1749b51efabdf\",\"venue\":\"\",\"year\":1957},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70359327\",\"name\":\"Vector Machines\"},{\"authorId\":\"4023505\",\"name\":\"P. Ti\\u00f1o\"}],\"doi\":\"10.1109/tnn.72\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e84b31f3822c9eb9bc22eccfee4acc96b32f8eea\",\"title\":\"IEEE Transactions on Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/e84b31f3822c9eb9bc22eccfee4acc96b32f8eea\",\"venue\":\"\",\"year\":2009},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"16947120\",\"name\":\"H. van Hasselt\"},{\"authorId\":\"122294617\",\"name\":\"M.A. Wiering\"}],\"doi\":\"10.1109/ADPRL.2007.368199\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7617db33f278d185f8455bedcf0b4899fb76105a\",\"title\":\"Reinforcement Learning in Continuous Action Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7617db33f278d185f8455bedcf0b4899fb76105a\",\"venue\":\"2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning\",\"year\":2007},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1709.06560\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"33690ff21ef1efb576410e656f2e60c89d0307d6\",\"title\":\"Deep Reinforcement Learning that Matters\",\"url\":\"https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6\",\"venue\":\"AAAI\",\"year\":2018}],\"title\":\"Exploiting the Sign of the Advantage Function to Learn Deterministic Policies in Continuous Domains\",\"topics\":[{\"topic\":\"Trust region\",\"topicId\":\"149255\",\"url\":\"https://www.semanticscholar.org/topic/149255\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Smoothing spline\",\"topicId\":\"74597\",\"url\":\"https://www.semanticscholar.org/topic/74597\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"Automaton\",\"topicId\":\"10069\",\"url\":\"https://www.semanticscholar.org/topic/10069\"},{\"topic\":\"Loss function\",\"topicId\":\"3650\",\"url\":\"https://www.semanticscholar.org/topic/3650\"}],\"url\":\"https://www.semanticscholar.org/paper/4342598da04242cb0f8e2307d47ca50909ae0717\",\"venue\":\"IJCAI\",\"year\":2019}\n"