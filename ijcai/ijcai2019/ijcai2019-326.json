"{\"abstract\":\"Document summarisation can be formulated as a sequential decision-making problem, which can be solved by Reinforcement Learning (RL) algorithms. The predominant RL paradigm for summarisation learns a cross-input policy, which requires considerable time, data and parameter tuning due to the huge search spaces and the delayed rewards. Learning input-specific RL policies is a more efficient alternative but so far depends on handcrafted rewards, which are difficult to design and yield poor performance. We propose RELIS, a novel RL paradigm that learns a reward function with Learning-to-Rank (L2R) algorithms at training time and uses this reward function to train an input-specific RL policy at test time. We prove that RELIS guarantees to generate near-optimal summaries with appropriate L2R and RL algorithms. Empirically, we evaluate our approach on extractive multi-document summarisation. We show that RELIS reduces the training time by two orders of magnitude compared to the state-of-the-art models while performing on par with them.\",\"arxivId\":\"1907.12894\",\"authors\":[{\"authorId\":\"152128108\",\"name\":\"Yang Gao\",\"url\":\"https://www.semanticscholar.org/author/152128108\"},{\"authorId\":\"49658113\",\"name\":\"Yang Gao\",\"url\":\"https://www.semanticscholar.org/author/49658113\"},{\"authorId\":\"145575695\",\"name\":\"C. Meyer\",\"url\":\"https://www.semanticscholar.org/author/145575695\"},{\"authorId\":\"2568628\",\"name\":\"M. Mesgar\",\"url\":\"https://www.semanticscholar.org/author/2568628\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\",\"url\":\"https://www.semanticscholar.org/author/1730400\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1909.08593\",\"authors\":[{\"authorId\":\"40179323\",\"name\":\"D. Ziegler\"},{\"authorId\":\"1387983862\",\"name\":\"Nisan Stiennon\"},{\"authorId\":\"119837982\",\"name\":\"Jeffrey Wu\"},{\"authorId\":\"31035595\",\"name\":\"T. Brown\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"},{\"authorId\":\"145370786\",\"name\":\"Paul Christiano\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"85172f97aedb9cc10358e6da31e4adc0e0ba71ae\",\"title\":\"Fine-Tuning Language Models from Human Preferences\",\"url\":\"https://www.semanticscholar.org/paper/85172f97aedb9cc10358e6da31e4adc0e0ba71ae\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1909.01214\",\"authors\":[{\"authorId\":\"33899215\",\"name\":\"F. B\\u00f6hm\"},{\"authorId\":\"152128108\",\"name\":\"Yang Gao\"},{\"authorId\":\"145575695\",\"name\":\"C. Meyer\"},{\"authorId\":\"15392843\",\"name\":\"Ori Shapira\"},{\"authorId\":\"7465342\",\"name\":\"I. Dagan\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.18653/v1/D19-1307\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"90d8f96e2cd71a50b40992020cb65bc75f352ea1\",\"title\":\"Better Rewards Yield Better Summaries: Learning to Summarise Without References\",\"url\":\"https://www.semanticscholar.org/paper/90d8f96e2cd71a50b40992020cb65bc75f352ea1\",\"venue\":\"EMNLP/IJCNLP\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2921990\",\"name\":\"Margot Mieskes\"},{\"authorId\":\"1764420\",\"name\":\"Eneldo Loza Menc\\u00eda\"},{\"authorId\":\"1644087861\",\"name\":\"Tim Kronsbein\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e1fd4ee3c107786b70e714f3b7057c8d53d829d1\",\"title\":\"A Data Set for the Analysis of Text Quality Dimensions in Summarization Evaluation\",\"url\":\"https://www.semanticscholar.org/paper/e1fd4ee3c107786b70e714f3b7057c8d53d829d1\",\"venue\":\"LREC\",\"year\":2020},{\"arxivId\":\"1906.02923\",\"authors\":[{\"authorId\":\"152128108\",\"name\":\"Yang Gao\"},{\"authorId\":\"145644807\",\"name\":\"Y. Gao\"},{\"authorId\":\"145575695\",\"name\":\"C. Meyer\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.1007/s10791-019-09367-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d5390f4db61ada45a32bb255bcf14265e120dc2b\",\"title\":\"Preference-based interactive multi-document summarisation\",\"url\":\"https://www.semanticscholar.org/paper/d5390f4db61ada45a32bb255bcf14265e120dc2b\",\"venue\":\"Information Retrieval Journal\",\"year\":2019},{\"arxivId\":\"1908.08486\",\"authors\":[{\"authorId\":\"2568628\",\"name\":\"M. Mesgar\"},{\"authorId\":\"1753296844\",\"name\":\"Sebastian Bucker\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.18653/v1/2020.acl-main.133\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"33168a640106bbb2d48cf2064a61e69f354ac5e7\",\"title\":\"Dialogue Coherence Assessment Without Explicit Dialogue Act Labels\",\"url\":\"https://www.semanticscholar.org/paper/33168a640106bbb2d48cf2064a61e69f354ac5e7\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2009.01325\",\"authors\":[{\"authorId\":\"1387983862\",\"name\":\"Nisan Stiennon\"},{\"authorId\":\"31793034\",\"name\":\"L. Ouyang\"},{\"authorId\":\"49387725\",\"name\":\"Jeff Wu\"},{\"authorId\":\"40179323\",\"name\":\"D. Ziegler\"},{\"authorId\":\"49407415\",\"name\":\"Ryan J. Lowe\"},{\"authorId\":\"153387869\",\"name\":\"Chelsea Voss\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"},{\"authorId\":\"145370786\",\"name\":\"Paul Christiano\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7d9fc80073d86238db87dc465daa965ca48506ae\",\"title\":\"Learning to summarize from human feedback\",\"url\":\"https://www.semanticscholar.org/paper/7d9fc80073d86238db87dc465daa965ca48506ae\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2568628\",\"name\":\"M. Mesgar\"},{\"authorId\":\"152490487\",\"name\":\"Sebastian B\\u00fccker\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ad69680fa67d123cdef40a1b51864a567570ffb\",\"title\":\"A Neural Model for Dialogue Coherence Assessment\",\"url\":\"https://www.semanticscholar.org/paper/7ad69680fa67d123cdef40a1b51864a567570ffb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2011.04264\",\"authors\":[{\"authorId\":\"32763968\",\"name\":\"A. Fisch\"},{\"authorId\":\"2544107\",\"name\":\"Kenton Lee\"},{\"authorId\":\"1744179\",\"name\":\"Ming-Wei Chang\"},{\"authorId\":\"144797264\",\"name\":\"J. Clark\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8ca71f61139c69131ab200368a30a3dc72fa6785\",\"title\":\"CapWAP: Captioning with a Purpose\",\"url\":\"https://www.semanticscholar.org/paper/8ca71f61139c69131ab200368a30a3dc72fa6785\",\"venue\":\"EMNLP\",\"year\":2020}],\"corpusId\":198985659,\"doi\":\"10.24963/ijcai.2019/326\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"be9aa06811f1438e8c6e2fcbed2f8aed06469f2c\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zeyu Zheng\"},{\"authorId\":null,\"name\":\"Junhyuk Oh\"},{\"authorId\":null,\"name\":\"Satinder Singh. On learning intrinsic rewards for policy gr methods\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 4649\\u20134659,\",\"year\":2018},{\"arxivId\":\"1408.5882\",\"authors\":[{\"authorId\":\"38367242\",\"name\":\"Yoon Kim\"}],\"doi\":\"10.3115/v1/D14-1181\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba\",\"title\":\"Convolutional Neural Networks for Sentence Classification\",\"url\":\"https://www.semanticscholar.org/paper/1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hang Li. A short introduction to learning to rank\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"IEICE Transactions\",\"url\":\"\",\"venue\":\"94-D(10):1854\\u20131862,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ziqiang Cao\"},{\"authorId\":null,\"name\":\"Furu Wei\"},{\"authorId\":null,\"name\":\"Sujian Li\"},{\"authorId\":null,\"name\":\"Wenjie Li\"},{\"authorId\":null,\"name\":\"Ming Zhou\"},{\"authorId\":null,\"name\":\"Houfeng Wang. Learning summary prior representation for ex summarization\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In ACL\",\"url\":\"\",\"venue\":\"pages 829\\u2013833,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kaichun Yao\"},{\"authorId\":null,\"name\":\"Libo Zhang\"},{\"authorId\":null,\"name\":\"Tiejian Luo\"},{\"authorId\":null,\"name\":\"Yanjun Wu. Deep reinforcement learning for extractive doc summarization\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Neurocomputing\",\"url\":\"\",\"venue\":\"284:52\\u2013 62,\",\"year\":2018},{\"arxivId\":\"1511.06732\",\"authors\":[{\"authorId\":\"1706809\",\"name\":\"Marc'Aurelio Ranzato\"},{\"authorId\":\"3295092\",\"name\":\"S. Chopra\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"35c1668dc64d24a28c6041978e5fcca754eb2f4b\",\"title\":\"Sequence Level Training with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/35c1668dc64d24a28c6041978e5fcca754eb2f4b\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1749477\",\"name\":\"Pengjie Ren\"},{\"authorId\":\"1721165\",\"name\":\"Zhumin Chen\"},{\"authorId\":\"2780667\",\"name\":\"Z. Ren\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"143982887\",\"name\":\"L. Nie\"},{\"authorId\":\"35729719\",\"name\":\"J. Ma\"},{\"authorId\":\"1696030\",\"name\":\"M. Rijke\"}],\"doi\":\"10.1145/3200864\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"301d56f6de408f6328baa4f4f5586f7269bb9a6b\",\"title\":\"Sentence Relations for Extractive Summarization with Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/301d56f6de408f6328baa4f4f5586f7269bb9a6b\",\"venue\":\"ACM Trans. Inf. Syst.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shivani Agarwal\"},{\"authorId\":null,\"name\":\"Michael Collins. Maximum margin ranking algorithms for inf retrieval\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ECIR\",\"url\":\"\",\"venue\":\"pages 332\\u2013343,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2797063\",\"name\":\"S. Agarwal\"},{\"authorId\":\"143707112\",\"name\":\"M. Collins\"}],\"doi\":\"10.1007/978-3-642-12275-0_30\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"118c0c734add3b986ab0eafa0647a0594a2bf436\",\"title\":\"Maximum Margin Ranking Algorithms for Information Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/118c0c734add3b986ab0eafa0647a0594a2bf436\",\"venue\":\"ECIR\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xingxing Zhang\"},{\"authorId\":null,\"name\":\"Mirella Lapata. Sentence simplification with deep reinforc learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In EMNLP\",\"url\":\"\",\"venue\":\"pages 584\\u2013594,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32244268\",\"name\":\"D. Gillick\"},{\"authorId\":\"144155256\",\"name\":\"B. Favre\"}],\"doi\":\"10.3115/1611638.1611640\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8e2cd5369db9242574740e0d2739c755f8f61c92\",\"title\":\"A Scalable Global Model for Summarization\",\"url\":\"https://www.semanticscholar.org/paper/8e2cd5369db9242574740e0d2739c755f8f61c92\",\"venue\":\"ILP 2009\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yuxiang Wu\"},{\"authorId\":null,\"name\":\"Baotian Hu. Learning to extract coherent summary via deep learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 5602\\u20135609,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shisha Narayan\"},{\"authorId\":null,\"name\":\"Shay B. Cohen\"},{\"authorId\":null,\"name\":\"Mirella Lapata. Ranking sentences for extractive summariza learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NAACL-HLT\",\"url\":\"\",\"venue\":\"pages 1747\\u20131759,\",\"year\":2018},{\"arxivId\":\"1805.10413\",\"authors\":[{\"authorId\":\"1978613\",\"name\":\"Ching-An Cheng\"},{\"authorId\":\"8073235\",\"name\":\"Xinyan Yan\"},{\"authorId\":\"40234261\",\"name\":\"Nolan Wagener\"},{\"authorId\":\"3288815\",\"name\":\"B. Boots\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"58a44c2fcbe282a0b8fe354e33703458640b4c28\",\"title\":\"Fast Policy Learning through Imitation and Reinforcement\",\"url\":\"https://www.semanticscholar.org/paper/58a44c2fcbe282a0b8fe354e33703458640b4c28\",\"venue\":\"UAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wojciech Kryscinski\"},{\"authorId\":null,\"name\":\"Romain Paulus\"},{\"authorId\":null,\"name\":\"Caiming Xiong\"},{\"authorId\":null,\"name\":\"Richard Socher. Improving abstraction in text summarization\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In EMNLP\",\"url\":\"\",\"venue\":\"pages 1808\\u20131817,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Antoine Bosselut\"},{\"authorId\":null,\"name\":\"Asli \\u00c7elikyilmaz\"},{\"authorId\":null,\"name\":\"Xiaodong He\"},{\"authorId\":null,\"name\":\"Jianfeng Gao\"},{\"authorId\":null,\"name\":\"Po-Sen Huang\"},{\"authorId\":null,\"name\":\"Yejin Choi. Discourse-aware neural rewards for coherent generation\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NAACL-HLT\",\"url\":\"\",\"venue\":\"pages 173\\u2013184,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1015330.1015430\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"title\":\"Apprenticeship learning via inverse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":\"1802.08636\",\"authors\":[{\"authorId\":\"143790510\",\"name\":\"Shashi Narayan\"},{\"authorId\":\"40146204\",\"name\":\"Shay B. Cohen\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":\"10.18653/v1/N18-1158\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"59562be2cf8e01e8b7bb7560cef56158ea171227\",\"title\":\"Ranking Sentences for Extractive Summarization with Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/59562be2cf8e01e8b7bb7560cef56158ea171227\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":\"1707.07402\",\"authors\":[{\"authorId\":\"144867624\",\"name\":\"Khanh Nguyen\"},{\"authorId\":\"1722360\",\"name\":\"Hal Daum\\u00e9\"},{\"authorId\":\"1409929082\",\"name\":\"Jordan L. Boyd-Graber\"}],\"doi\":\"10.18653/v1/D17-1153\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"abcbfc9742e8f4825cfc536091fd414e08d03998\",\"title\":\"Reinforcement Learning for Bandit Neural Machine Translation with Simulated Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/abcbfc9742e8f4825cfc536091fd414e08d03998\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35512303\",\"name\":\"Maxime Peyrard\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.18653/v1/N18-2103\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7ed8dd8a10ebf1508947c29c1e82f8a0197f9f78\",\"title\":\"Objective Function Learning to Match Human Judgements for Optimization-Based Summarization\",\"url\":\"https://www.semanticscholar.org/paper/7ed8dd8a10ebf1508947c29c1e82f8a0197f9f78\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S. Sutton\"},{\"authorId\":null,\"name\":\"Hamid Reza Maei\"},{\"authorId\":null,\"name\":\"Doina Precup\"},{\"authorId\":null,\"name\":\"Shalabh Bhatnagar\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Csaba Szepesv\\u00e1ri\"},{\"authorId\":null,\"name\":\"Eric Wiewiora. Fast gradient-descent methods for tempor approximation\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 993\\u20131000,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Maryam Fazel\"},{\"authorId\":null,\"name\":\"Rong Ge\"},{\"authorId\":null,\"name\":\"Sham Kakade\"},{\"authorId\":null,\"name\":\"Mehran Mesbahi. Global convergence of policy gradient met regulator\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 1466\\u20131475,\",\"year\":2018},{\"arxivId\":\"1805.03766\",\"authors\":[{\"authorId\":\"2691021\",\"name\":\"Antoine Bosselut\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"144137069\",\"name\":\"X. He\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"2421691\",\"name\":\"Po-Sen Huang\"},{\"authorId\":\"1699545\",\"name\":\"Yejin Choi\"}],\"doi\":\"10.18653/v1/N18-1016\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"680bfa179c33d56f524c6adf9b7f7f5a62e5ef46\",\"title\":\"Discourse-Aware Neural Rewards for Coherent Text Generation\",\"url\":\"https://www.semanticscholar.org/paper/680bfa179c33d56f524c6adf9b7f7f5a62e5ef46\",\"venue\":\"NAACL-HLT\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Arun Tejasvi Chaganty\"},{\"authorId\":null,\"name\":\"Stephen Mussmann\"},{\"authorId\":null,\"name\":\"Percy Liang. The price of debiasing automatic metrics in evalaution\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ACL\",\"url\":\"\",\"venue\":\"pages 643\\u2013653,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S. Sutton\"},{\"authorId\":null,\"name\":\"David A. McAllester\"},{\"authorId\":null,\"name\":\"Satinder P. Singh\"},{\"authorId\":null,\"name\":\"Yishay Mansour. Policy gradient methods for reinforcement approximation\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NIPS\",\"url\":\"\",\"venue\":\"pages 1057\\u20131063,\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S. Sutton\"},{\"authorId\":null,\"name\":\"David A. McAllester\"},{\"authorId\":null,\"name\":\"Satinder P. Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hasan , and Yl - lias Chali . Fear the REAPER : A system for automatic multi - document summarization with reinforcement learn\",\"url\":\"\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1703.10931\",\"authors\":[{\"authorId\":\"37409910\",\"name\":\"Xingxing Zhang\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":\"10.18653/v1/D17-1062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"31f5864ada5fb08b69da74b6d5ad99e385dcc737\",\"title\":\"Sentence Simplification with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/31f5864ada5fb08b69da74b6d5ad99e385dcc737\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410302\",\"name\":\"Cody Rioux\"},{\"authorId\":\"37187331\",\"name\":\"Sadid A. Hasan\"},{\"authorId\":\"1735475\",\"name\":\"Yllias Chali\"}],\"doi\":\"10.3115/v1/D14-1075\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c5f24dd170079f1624575e51c4b650acc39942f7\",\"title\":\"Fear the REAPER: A System for Automatic Multi-Document Summarization with Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5f24dd170079f1624575e51c4b650acc39942f7\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dan Gillick\"},{\"authorId\":null,\"name\":\"Benoit Favre. A scalable global model for summarization. ILP\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 10\\u201318\",\"url\":\"\",\"venue\":\"Association for Computational Linguistics,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2314396\",\"name\":\"Ziqiang Cao\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"1695451\",\"name\":\"Sujian Li\"},{\"authorId\":\"50135831\",\"name\":\"W. Li\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"},{\"authorId\":\"1781885\",\"name\":\"Houfeng Wang\"}],\"doi\":\"10.3115/v1/P15-2136\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0f6d8e5d9b243eaf1d3968d8cd272db64e536beb\",\"title\":\"Learning Summary Prior Representation for Extractive Summarization\",\"url\":\"https://www.semanticscholar.org/paper/0f6d8e5d9b243eaf1d3968d8cd272db64e536beb\",\"venue\":\"ACL\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1781574\",\"name\":\"Chin-Yew Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b05f32c32519a809f21642ef1eb3eaf3848008\",\"title\":\"ROUGE: A Package for Automatic Evaluation of Summaries\",\"url\":\"https://www.semanticscholar.org/paper/60b05f32c32519a809f21642ef1eb3eaf3848008\",\"venue\":\"ACL 2004\",\"year\":2004},{\"arxivId\":\"1805.10627\",\"authors\":[{\"authorId\":\"3422710\",\"name\":\"Julia Kreutzer\"},{\"authorId\":\"46172696\",\"name\":\"Joshua Uyheng\"},{\"authorId\":\"3289329\",\"name\":\"S. Riezler\"}],\"doi\":\"10.18653/v1/P18-1165\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"15919637566348de8ca1e054151c24cc864b0f0e\",\"title\":\"Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/15919637566348de8ca1e054151c24cc864b0f0e\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35512303\",\"name\":\"Maxime Peyrard\"},{\"authorId\":\"1403764997\",\"name\":\"Judith Eckle-Kohler\"}],\"doi\":\"10.18653/v1/P17-2005\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4ace76465d43752d80925c06037f3832a857a6a6\",\"title\":\"A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments\",\"url\":\"https://www.semanticscholar.org/paper/4ace76465d43752d80925c06037f3832a857a6a6\",\"venue\":\"ACL\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ching-An Cheng\"},{\"authorId\":null,\"name\":\"Xinyan Yan\"},{\"authorId\":null,\"name\":\"Nolan Wagener\"},{\"authorId\":null,\"name\":\"Byron Boots. Fast policy learning through imitation\"},{\"authorId\":null,\"name\":\"reinforcement\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In UAI\",\"url\":\"\",\"venue\":\"pages 845\\u2013855,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3122129\",\"name\":\"L. Maystre\"},{\"authorId\":\"2052174\",\"name\":\"M. Grossglauser\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"90a0657536806c1cdc19d757b76e7b6a1b15dd4d\",\"title\":\"Just Sort It! A Simple and Effective Approach to Active Preference Learning\",\"url\":\"https://www.semanticscholar.org/paper/90a0657536806c1cdc19d757b76e7b6a1b15dd4d\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1808.09658\",\"authors\":[{\"authorId\":\"48146895\",\"name\":\"Yang Gao\"},{\"authorId\":\"145575695\",\"name\":\"C. Meyer\"},{\"authorId\":\"1730400\",\"name\":\"Iryna Gurevych\"}],\"doi\":\"10.18653/v1/D18-1445\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cc7d5cf6e37cfec9e100418d54c2deb89a05b53c\",\"title\":\"APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/cc7d5cf6e37cfec9e100418d54c2deb89a05b53c\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49404233\",\"name\":\"Hang Li\"}],\"doi\":\"10.1587/TRANSINF.E94.D.1854\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d74a1419d75e8743eb7e3da2bb425340c7753342\",\"title\":\"A Short Introduction to Learning to Rank\",\"url\":\"https://www.semanticscholar.org/paper/d74a1419d75e8743eb7e3da2bb425340c7753342\",\"venue\":\"IEICE Trans. Inf. Syst.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Borja Ibarz\"},{\"authorId\":null,\"name\":\"Jan Leike\"},{\"authorId\":null,\"name\":\"Tobias Pohlen\"},{\"authorId\":null,\"name\":\"Geoffrey Irving\"},{\"authorId\":null,\"name\":\"Shane Legg\"},{\"authorId\":null,\"name\":\"Dario Amodei. Reward learning from human preferences\"},{\"authorId\":null,\"name\":\"demonstrations in atari\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 8022\\u20138034,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":\"1807.02202\",\"authors\":[{\"authorId\":\"2719924\",\"name\":\"A. Chaganty\"},{\"authorId\":\"1776721\",\"name\":\"Stephen Mussmann\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"}],\"doi\":\"10.18653/v1/P18-1060\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d0c3adbee8098d47c7c0704e0841fa7daf8d161\",\"title\":\"The price of debiasing automatic metrics in natural language evalaution\",\"url\":\"https://www.semanticscholar.org/paper/9d0c3adbee8098d47c7c0704e0841fa7daf8d161\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ziqiang Cao\"},{\"authorId\":null,\"name\":\"Wenjie Li\"},{\"authorId\":null,\"name\":\"Sujian Li\"},{\"authorId\":null,\"name\":\"Furu Wei. Improving multi-document summarization via te classification\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 3053\\u20133059,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Khanh Nguyen\"},{\"authorId\":null,\"name\":\"Hal Daum\\u00e9\"},{\"authorId\":null,\"name\":\"Jordan L. Boyd-Graber. Reinforcement learning for bandit feedback\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In EMNLP\",\"url\":\"\",\"venue\":\"pages 1465\\u20131475,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Seonggi Ryang\"},{\"authorId\":null,\"name\":\"Takeshi Abekawa. Framework of automatic text summarization learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In EMNLP/CoNLL\",\"url\":\"\",\"venue\":\"pages 256\\u2013265,\",\"year\":2012},{\"arxivId\":\"1811.06521\",\"authors\":[{\"authorId\":\"6675568\",\"name\":\"Borja Ibarz\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"3408089\",\"name\":\"Tobias Pohlen\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"325e1b7cec684c22bb3c2cf65205c77eaf55114f\",\"title\":\"Reward learning from human preferences and demonstrations in Atari\",\"url\":\"https://www.semanticscholar.org/paper/325e1b7cec684c22bb3c2cf65205c77eaf55114f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1804.07036\",\"authors\":[{\"authorId\":\"39417610\",\"name\":\"Yuxiang Wu\"},{\"authorId\":\"33968873\",\"name\":\"Baotian Hu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c72582122ff631117a05deb2aefa04b01362e3fa\",\"title\":\"Learning to Extract Coherent Summary via Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c72582122ff631117a05deb2aefa04b01362e3fa\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1804.06459\",\"authors\":[{\"authorId\":\"34418171\",\"name\":\"Zeyu Zheng\"},{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"145537841\",\"name\":\"S. Singh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35271d36cb20bf8d716e79c9dd15d738d955a931\",\"title\":\"On Learning Intrinsic Rewards for Policy Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/35271d36cb20bf8d716e79c9dd15d738d955a931\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Maxime Peyrard\"},{\"authorId\":null,\"name\":\"Iryna Gurevych. Objective function learning to match hum summarization\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NAACL-HLT\",\"url\":\"\",\"venue\":\"pages 654\\u2013660,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pengjie Ren\"},{\"authorId\":null,\"name\":\"Zhumin Chen\"},{\"authorId\":null,\"name\":\"Zhaochun Ren\"},{\"authorId\":null,\"name\":\"Furu Wei\"},{\"authorId\":null,\"name\":\"Liqiang Nie\"},{\"authorId\":null,\"name\":\"Jun Ma\"},{\"authorId\":null,\"name\":\"Maarten de Rijke. Sentence relations for extractive summar Trans\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Inf\",\"url\":\"\",\"venue\":\"Syst., 36(4):39:1\\u2013 39:32,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoon Kim. Convolutional neural networks for sentence classification\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In EMNLP\",\"url\":\"\",\"venue\":\"pages 1746\\u20131751,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Julia Kreutzer\"},{\"authorId\":null,\"name\":\"Joshua Uyheng\"},{\"authorId\":null,\"name\":\"Stefan Riezler. Reliability\"},{\"authorId\":null,\"name\":\"learnability of human bandit feedback for sequence-to-sequence learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ACL\",\"url\":\"\",\"venue\":\"pages 1777\\u20131788,\",\"year\":2018}],\"title\":\"Reward Learning for Efficient Reinforcement Learning in Extractive Document Summarisation\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Learning to rank\",\"topicId\":\"135827\",\"url\":\"https://www.semanticscholar.org/topic/135827\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Programming paradigm\",\"topicId\":\"29522\",\"url\":\"https://www.semanticscholar.org/topic/29522\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"QA & UX Manager\",\"topicId\":\"9716554\",\"url\":\"https://www.semanticscholar.org/topic/9716554\"},{\"topic\":\"Natural language generation\",\"topicId\":\"6196\",\"url\":\"https://www.semanticscholar.org/topic/6196\"}],\"url\":\"https://www.semanticscholar.org/paper/be9aa06811f1438e8c6e2fcbed2f8aed06469f2c\",\"venue\":\"IJCAI\",\"year\":2019}\n"