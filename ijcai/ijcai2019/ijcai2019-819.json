"{\"abstract\":\"We introduce a novel apprenticeship learning algorithm to learn an expert's underlying reward structure in off-policy model-free \\\\emph{batch} settings. Unlike existing methods that require a dynamics model or additional data acquisition for on-policy evaluation, our algorithm requires only the batch data of observed expert behavior. Such settings are common in real-world tasks---health care, finance or industrial processes ---where accurate simulators do not exist or data acquisition is costly. To address challenges in batch settings, we introduce Deep Successor Feature Networks(DSFN) that estimate feature expectations in an off-policy setting and a transition-regularized imitation network that produces a near-expert initial policy and an efficient feature representation. Our algorithm achieves superior results in batch settings on both control benchmarks and a vital clinical task of sepsis management in the Intensive Care Unit.\",\"arxivId\":\"1903.10077\",\"authors\":[{\"authorId\":\"90414572\",\"name\":\"Donghun Lee\",\"url\":\"https://www.semanticscholar.org/author/90414572\"},{\"authorId\":\"10736408\",\"name\":\"S. Srinivasan\",\"url\":\"https://www.semanticscholar.org/author/10736408\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\",\"url\":\"https://www.semanticscholar.org/author/1388372395\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"10736408\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d0b0d003fc6e2a71ba12bde3ce190a0c15ea8e71\",\"title\":\"Interpretable Batch IRL to Extract Clinician Goals in ICU Hypotension Management.\",\"url\":\"https://www.semanticscholar.org/paper/d0b0d003fc6e2a71ba12bde3ce190a0c15ea8e71\",\"venue\":\"AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science\",\"year\":2020},{\"arxivId\":\"1905.09710\",\"authors\":[{\"authorId\":\"121982230\",\"name\":\"Philip Korsunsky\"},{\"authorId\":\"120866863\",\"name\":\"Stav Belogolovsky\"},{\"authorId\":\"3331540\",\"name\":\"T. Zahavy\"},{\"authorId\":null,\"name\":\"Chen Tessler\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"abbde872971f558b14a6a3cd8f2880fcbc0a73b0\",\"title\":\"Inverse Reinforcement Learning in Contextual MDPs\",\"url\":\"https://www.semanticscholar.org/paper/abbde872971f558b14a6a3cd8f2880fcbc0a73b0\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51101308\",\"name\":\"Siqi Liu\"},{\"authorId\":\"4822484\",\"name\":\"K. See\"},{\"authorId\":\"7452993\",\"name\":\"K. Y. Ngiam\"},{\"authorId\":\"143605744\",\"name\":\"L. Celi\"},{\"authorId\":\"2549273\",\"name\":\"X. Sun\"},{\"authorId\":\"2773476\",\"name\":\"M. Feng\"}],\"doi\":\"10.2196/18477\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"58f75ed69d10e0e1fad28794963c87fc81f7c1d9\",\"title\":\"Reinforcement Learning for Clinical Decision Support in Critical Care: Comprehensive Review\",\"url\":\"https://www.semanticscholar.org/paper/58f75ed69d10e0e1fad28794963c87fc81f7c1d9\",\"venue\":\"Journal of medical Internet research\",\"year\":2020},{\"arxivId\":\"2010.02974\",\"authors\":[{\"authorId\":\"49857577\",\"name\":\"Tarun Gupta\"},{\"authorId\":\"2827015\",\"name\":\"Anuj Mahajan\"},{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"1722871\",\"name\":\"Wendelin Boehmer\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e5dbe2dfb221032d7191abe2539cc4f801b044e1\",\"title\":\"UneVEn: Universal Value Exploration for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e5dbe2dfb221032d7191abe2539cc4f801b044e1\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.14154\",\"authors\":[{\"authorId\":\"123723354\",\"name\":\"Daniel Jarrett\"},{\"authorId\":\"1751623812\",\"name\":\"Ioana Bica\"},{\"authorId\":\"1729969\",\"name\":\"M. V. D. Schaar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa935f1753db47e899e1305d7fa0d0de6af81e51\",\"title\":\"Strictly Batch Imitation Learning by Energy-based Distribution Matching\",\"url\":\"https://www.semanticscholar.org/paper/aa935f1753db47e899e1305d7fa0d0de6af81e51\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2007.13531\",\"authors\":[{\"authorId\":\"1751623812\",\"name\":\"Ioana Bica\"},{\"authorId\":\"123723354\",\"name\":\"Daniel Jarrett\"},{\"authorId\":\"83246796\",\"name\":\"Alihan H\\u00fcy\\u00fck\"},{\"authorId\":\"1729969\",\"name\":\"M. V. D. Schaar\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d67e101152e9b27fe0aff0faf2ae229cc9471fbc\",\"title\":\"Batch Inverse Reinforcement Learning Using Counterfactuals for Understanding Decision Making\",\"url\":\"https://www.semanticscholar.org/paper/d67e101152e9b27fe0aff0faf2ae229cc9471fbc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.09710\",\"authors\":[{\"authorId\":\"120866863\",\"name\":\"Stav Belogolovsky\"},{\"authorId\":\"121982230\",\"name\":\"Philip Korsunsky\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"3393407\",\"name\":\"Chen Tessler\"},{\"authorId\":\"121459457\",\"name\":\"Tom Zahavy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb8da600e0a2c98056e800c062dbfaddcc6315cc\",\"title\":\"Learning personalized treatments via IRL.\",\"url\":\"https://www.semanticscholar.org/paper/eb8da600e0a2c98056e800c062dbfaddcc6315cc\",\"venue\":\"\",\"year\":2020}],\"corpusId\":85499320,\"doi\":\"10.24963/ijcai.2019/819\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"fdc99b79b1355fc5f0ef5b81703021344c2ff0ac\",\"references\":[{\"arxivId\":\"1711.09602\",\"authors\":[{\"authorId\":\"31411877\",\"name\":\"Aniruddh Raghu\"},{\"authorId\":\"6204450\",\"name\":\"M. Komorowski\"},{\"authorId\":\"144168380\",\"name\":\"I. Ahmed\"},{\"authorId\":\"1827828\",\"name\":\"L. A. Celi\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"},{\"authorId\":\"145348788\",\"name\":\"M. Ghassemi\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a729073d3ec9a159b86fd438e06f3dff2735beb5\",\"title\":\"Deep Reinforcement Learning for Sepsis Treatment\",\"url\":\"https://www.semanticscholar.org/paper/a729073d3ec9a159b86fd438e06f3dff2735beb5\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yan Duan\"},{\"authorId\":null,\"name\":\"Xi Chen\"},{\"authorId\":null,\"name\":\"Rein Houthooft\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Benchmarking deep reinforcement learning f control\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1329\\u2013 1338,\",\"year\":2016},{\"arxivId\":\"1606.02396\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"3422751\",\"name\":\"Simanta Gautam\"},{\"authorId\":\"1831199\",\"name\":\"S. Gershman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10a4992ece5baea79326a8878a6244eeacbc6af5\",\"title\":\"Deep Successor Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/10a4992ece5baea79326a8878a6244eeacbc6af5\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"},{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"144021446\",\"name\":\"Anind K. Dey\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"11b6bdfe36c48b11367b27187da11d95892f0361\",\"title\":\"Maximum Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/11b6bdfe36c48b11367b27187da11d95892f0361\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"St\\u00e9phane Ross\"},{\"authorId\":null,\"name\":\"Geoffrey Gordon\"},{\"authorId\":null,\"name\":\"Drew Bagnell. A reduction of imitation learning\"},{\"authorId\":null,\"name\":\"structured prediction to no-regret online learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 14 AISTATS\",\"url\":\"\",\"venue\":\"pages 627\\u2013635,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28972636\",\"name\":\"Alistair E. W. Johnson\"},{\"authorId\":\"40541154\",\"name\":\"Tom J. Pollard\"},{\"authorId\":\"50002304\",\"name\":\"Lu Shen\"},{\"authorId\":\"2771337\",\"name\":\"L. Lehman\"},{\"authorId\":\"2773476\",\"name\":\"M. Feng\"},{\"authorId\":\"143811844\",\"name\":\"M. Ghassemi\"},{\"authorId\":\"39934542\",\"name\":\"Benjamin Moody\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"},{\"authorId\":\"143605744\",\"name\":\"L. Celi\"},{\"authorId\":\"1978710\",\"name\":\"R. Mark\"}],\"doi\":\"10.1038/sdata.2016.35\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"95cd83603a0d2b6918a8e34a5637a8f382da96f5\",\"title\":\"MIMIC-III, a freely accessible critical care database\",\"url\":\"https://www.semanticscholar.org/paper/95cd83603a0d2b6918a8e34a5637a8f382da96f5\",\"venue\":\"Scientific data\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.1007/978-3-642-40988-2_2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb074e9adbcdb722da1c65a4b2db63e3a87d8765\",\"title\":\"Learning from Demonstrations: Is It Worth Estimating a Reward Function?\",\"url\":\"https://www.semanticscholar.org/paper/bb074e9adbcdb722da1c65a4b2db63e3a87d8765\",\"venue\":\"ECML/PKDD\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Justin Fu\"},{\"authorId\":null,\"name\":\"Katie Luo\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and Sergey Levine\",\"url\":\"\",\"venue\":\"Learning robust rewards with adversarial inverse reinforcement learning.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abdeslam Boularias\"},{\"authorId\":null,\"name\":\"Jens Kober\"},{\"authorId\":null,\"name\":\"Jan Peters. Relative entropy inverse reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 14 AISTATS\",\"url\":\"\",\"venue\":\"pages 182\\u2013189,\",\"year\":2011},{\"arxivId\":\"1604.00923\",\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ec8a2f6cfe72309f5f1608d22ec28778d3ee976a\",\"title\":\"Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ec8a2f6cfe72309f5f1608d22ec28778d3ee976a\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1767216\",\"name\":\"E. Klein\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.1007/978-3-642-29946-9_28\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"747ac0521b20f23638ff39b96b0cd75ead430a25\",\"title\":\"Batch, Off-Policy and Model-Free Apprenticeship Learning\",\"url\":\"https://www.semanticscholar.org/paper/747ac0521b20f23638ff39b96b0cd75ead430a25\",\"venue\":\"EWRL\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nathan D Ratliff\"},{\"authorId\":null,\"name\":\"J Andrew Bagnell\"},{\"authorId\":null,\"name\":\"Martin A Zinkevich. Maximum margin planning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In 23 ICML\",\"url\":\"\",\"venue\":\"pages 729\\u2013736,\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492009633\",\"name\":\"Patrick J. Roa\"}],\"doi\":\"10.1023/A:1017153816538\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8e6d789ee714d29c9b5156ba9d61b2170d7a315f\",\"title\":\"Volume 8\",\"url\":\"https://www.semanticscholar.org/paper/8e6d789ee714d29c9b5156ba9d61b2170d7a315f\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"13693897\",\"name\":\"Nathan D. Ratliff\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"}],\"doi\":\"10.1145/1143844.1143936\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"117a50fbdfd473e43e550c6103733e6cb4aecb4c\",\"title\":\"Maximum margin planning\",\"url\":\"https://www.semanticscholar.org/paper/117a50fbdfd473e43e550c6103733e6cb4aecb4c\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yan Duan\"},{\"authorId\":null,\"name\":\"Xi Chen\"},{\"authorId\":null,\"name\":\"Rein Houthooft\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Benchmarking deep reinforcement learning f control\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 33 ICML\",\"url\":\"\",\"venue\":\"pages 1329\\u20131338,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"Antonoglou Ioannis Quan\"},{\"authorId\":null,\"name\":\"John\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and David Silver\",\"url\":\"\",\"venue\":\"Prioritized experience replay.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"14212205\",\"name\":\"D. Liu\"},{\"authorId\":\"1398217037\",\"name\":\"M. Abu-Khalaf\"},{\"authorId\":\"1683616\",\"name\":\"A. M. Alimi\"},{\"authorId\":\"144826602\",\"name\":\"C. Anderson\"},{\"authorId\":\"71018001\",\"name\":\"A. Fausto\"},{\"authorId\":\"3836512\",\"name\":\"A. T. Azar\"},{\"authorId\":\"115675135\",\"name\":\"B. Baesens\"},{\"authorId\":\"1764788\",\"name\":\"G. Battistelli\"},{\"authorId\":\"1398734187\",\"name\":\"E. Bayro-Corrochano\"},{\"authorId\":\"1929254\",\"name\":\"S. Boht\\u00e9\"},{\"authorId\":\"2613481\",\"name\":\"P. Bouboulis\"},{\"authorId\":\"1409071182\",\"name\":\"Padua Braga\"},{\"authorId\":\"2123613\",\"name\":\"C. Cervellera\"},{\"authorId\":\"1703849\",\"name\":\"B. Chen\"},{\"authorId\":\"1781965\",\"name\":\"S. Cruces\"},{\"authorId\":\"39293304\",\"name\":\"Qiong-Hai Dai\"},{\"authorId\":\"7416642\",\"name\":\"S. Damelin\"},{\"authorId\":\"103338302\",\"name\":\"Daoyi Dong\"},{\"authorId\":\"1384339708\",\"name\":\"E. El-Alfy\"},{\"authorId\":\"48225830\",\"name\":\"K. Fahd\"},{\"authorId\":\"145319527\",\"name\":\"S. Arabia\"},{\"authorId\":\"143845181\",\"name\":\"D. Elizondo\"},{\"authorId\":\"3138895\",\"name\":\"M. Filippone\"},{\"authorId\":\"145692771\",\"name\":\"Y. Fu\"},{\"authorId\":\"2264182\",\"name\":\"Giorgio Gnecco\"},{\"authorId\":\"2198278\",\"name\":\"Haibo He\"},{\"authorId\":\"1743600\",\"name\":\"S. Ji\"},{\"authorId\":\"1815835\",\"name\":\"P. Kidmose\"},{\"authorId\":\"1779619\",\"name\":\"R. M. Kil\"},{\"authorId\":\"2073142\",\"name\":\"R. Legenstein\"},{\"authorId\":\"47892555\",\"name\":\"Hongyi Li\"},{\"authorId\":\"46946977\",\"name\":\"Zhijun Li\"},{\"authorId\":\"47663266\",\"name\":\"Jinling Liang\"},{\"authorId\":\"150152476\",\"name\":\"Juwei Lu\"},{\"authorId\":\"1723706\",\"name\":\"Wenlian Lu\"},{\"authorId\":\"9074167\",\"name\":\"Jiancheng Lv\"},{\"authorId\":\"144893748\",\"name\":\"A. Madureira\"},{\"authorId\":\"152748915\",\"name\":\"M. Panella\"},{\"authorId\":\"1780024\",\"name\":\"R. Polikar\"},{\"authorId\":\"2353770\",\"name\":\"D. Prokhorov\"},{\"authorId\":\"1744102\",\"name\":\"M. Roveri\"},{\"authorId\":\"145411696\",\"name\":\"B. Schuller\"},{\"authorId\":\"46581758\",\"name\":\"Madhusudana Shashanka\"},{\"authorId\":\"12459603\",\"name\":\"Chunhua Shen\"},{\"authorId\":\"1721547\",\"name\":\"I. \\u0160krjanc\"},{\"authorId\":\"40560020\",\"name\":\"Y. Song\"},{\"authorId\":\"1771979\",\"name\":\"S. Squartini\"},{\"authorId\":\"1776000\",\"name\":\"Changyin Sun\"},{\"authorId\":\"144732378\",\"name\":\"T. Tanaka\"},{\"authorId\":\"3134548\",\"name\":\"H. Tang\"},{\"authorId\":\"143719918\",\"name\":\"D. Tao\"},{\"authorId\":\"4023505\",\"name\":\"P. Ti\\u00f1o\"},{\"authorId\":\"2706487\",\"name\":\"D. Wang\"},{\"authorId\":\"48325417\",\"name\":\"M. J. Watts\"},{\"authorId\":\"1717781\",\"name\":\"Qinglai Wei\"},{\"authorId\":\"50313481\",\"name\":\"Stefan Wermter\"},{\"authorId\":\"32239759\",\"name\":\"M. Wiering\"},{\"authorId\":\"33455548\",\"name\":\"Jonathan Wu\"},{\"authorId\":\"145382103\",\"name\":\"S. Xie\"},{\"authorId\":\"144016434\",\"name\":\"D. Xu\"}],\"doi\":\"10.1109/tnnls.2013.2286276\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"41eb6f471abaa7d111baefda111e488f8ffb39a4\",\"title\":\"IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS\",\"url\":\"https://www.semanticscholar.org/paper/41eb6f471abaa7d111baefda111e488f8ffb39a4\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Edouard Klein\"},{\"authorId\":null,\"name\":\"Bilal Piot\"},{\"authorId\":null,\"name\":\"Matthieu Geist\"},{\"authorId\":null,\"name\":\"Olivier Pietquin. A cascaded supervised learning approach Learning\"},{\"authorId\":null,\"name\":\"Knowledge Discovery in Databases\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 1\\u201316\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2013},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1606.05312\",\"authors\":[{\"authorId\":\"143999673\",\"name\":\"Andr\\u00e9 Barreto\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d8686b657b61a37da351af2952aabd8b281de408\",\"title\":\"Successor Features for Transfer in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d8686b657b61a37da351af2952aabd8b281de408\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Singer Mervyn\"},{\"authorId\":null,\"name\":\"S. DeutschmanClifford\"},{\"authorId\":null,\"name\":\"Seymour Cristopher\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"The third international consensus definitions for sepsis and septic shock (sepsis3). JAMA, 315(8):801\\u2013810,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhao Song\"},{\"authorId\":null,\"name\":\"Ronald Parr\"},{\"authorId\":null,\"name\":\"Xuejun Liao\"},{\"authorId\":null,\"name\":\"Lawrence Carin. Linear feature encoding for reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 30 NeurIPS\",\"url\":\"\",\"venue\":\"pages 4224\\u20134232,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3794143\",\"name\":\"M. Singer\"},{\"authorId\":\"5854751\",\"name\":\"C. Deutschman\"},{\"authorId\":\"1721318\",\"name\":\"C. Seymour\"},{\"authorId\":\"1384301779\",\"name\":\"M. Shankar-Hari\"},{\"authorId\":\"3805671\",\"name\":\"D. Annane\"},{\"authorId\":\"113517702\",\"name\":\"M. Bauer\"},{\"authorId\":\"3604318\",\"name\":\"R. Bellomo\"},{\"authorId\":\"2105497\",\"name\":\"G. Bernard\"},{\"authorId\":\"98236774\",\"name\":\"J. Chiche\"},{\"authorId\":\"3721143\",\"name\":\"C. Coopersmith\"},{\"authorId\":\"3006319\",\"name\":\"R. Hotchkiss\"},{\"authorId\":\"22488411\",\"name\":\"M. Levy\"},{\"authorId\":\"144050532\",\"name\":\"J. Marshall\"},{\"authorId\":\"145617958\",\"name\":\"G. Martin\"},{\"authorId\":\"4143816\",\"name\":\"S. Opal\"},{\"authorId\":\"5742786\",\"name\":\"G. Rubenfeld\"},{\"authorId\":\"47499613\",\"name\":\"T. van der Poll\"},{\"authorId\":\"50577961\",\"name\":\"J. Vincent\"},{\"authorId\":\"1834539\",\"name\":\"D. Angus\"}],\"doi\":\"10.1001/jama.2016.0287\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2eef3e5addd1efa355e7536590984ecd1656c79e\",\"title\":\"The Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-3).\",\"url\":\"https://www.semanticscholar.org/paper/2eef3e5addd1efa355e7536590984ecd1656c79e\",\"venue\":\"JAMA\",\"year\":2016},{\"arxivId\":\"1709.06560\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"33690ff21ef1efb576410e656f2e60c89d0307d6\",\"title\":\"Deep Reinforcement Learning that Matters\",\"url\":\"https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1700433\",\"name\":\"S. Ross\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"70e10a5459c6f1aaf346ee4f2dcc837151fbe75c\",\"title\":\"Efficient Reductions for Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/70e10a5459c6f1aaf346ee4f2dcc837151fbe75c\",\"venue\":\"AISTATS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peter Henderson\"},{\"authorId\":null,\"name\":\"Risashat Islam\"},{\"authorId\":null,\"name\":\"Philip BAchman\"},{\"authorId\":null,\"name\":\"Joelle Pineau\"},{\"authorId\":null,\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and David Meger\",\"url\":\"\",\"venue\":\"Deep reinforcement learning that matters.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sirs Elixhauser\"},{\"authorId\":null,\"name\":\"Gender\"},{\"authorId\":null,\"name\":\"Gcs -Glasgow Coma Re-Admission\"},{\"authorId\":null,\"name\":\"Scale\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Age 2. Lab Values -Albumin, Arterial pH, Calcium, Glucose\",\"url\":\"\",\"venue\":\"Hemoglobin, Magnesium, PTT -Partial Thromboplastin Time, Potassium, SGPT -Serum Glutamic-Pyruvic Transaminase, Arterial Blood Gas, BUN -Blood Urea Nitrogen, Chloride, Bicarbonate, INR -International Normalized Ratio\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jan Leike\"},{\"authorId\":null,\"name\":\"Miljan Martic\"},{\"authorId\":null,\"name\":\"Victoria Krakovna\"},{\"authorId\":null,\"name\":\"Pedro A Ortega\"},{\"authorId\":null,\"name\":\"Tom Everitt\"},{\"authorId\":null,\"name\":\"Andrew Lefrancq\"},{\"authorId\":null,\"name\":\"Laurent Orseau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Shane Legg\",\"url\":\"\",\"venue\":\"Ai safety gridworlds.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1767216\",\"name\":\"E. Klein\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7a1409b35c4138b5c1e23f77ad70488b888a66f7\",\"title\":\"Inverse Reinforcement Learning through Structured Classification\",\"url\":\"https://www.semanticscholar.org/paper/7a1409b35c4138b5c1e23f77ad70488b888a66f7\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":\"1712.03779\",\"authors\":[{\"authorId\":\"144923780\",\"name\":\"B. Yu\"},{\"authorId\":\"19225295\",\"name\":\"Karl Kumbier\"}],\"doi\":\"10.1631/FITEE.1700813\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"747f5ea1b986fb15a20b902407f48ef47c80bbcb\",\"title\":\"Artificial intelligence and statistics\",\"url\":\"https://www.semanticscholar.org/paper/747f5ea1b986fb15a20b902407f48ef47c80bbcb\",\"venue\":\"Frontiers of Information Technology & Electronic Engineering\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1784072\",\"name\":\"M. Lagoudakis\"},{\"authorId\":\"145726861\",\"name\":\"R. Parr\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b750a17921d32936425e05f8b00b96569e2fc5a6\",\"title\":\"Least-Squares Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/b750a17921d32936425e05f8b00b96569e2fc5a6\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Jin\"},{\"authorId\":null,\"name\":\"Andreas Damianou\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Costas Spanos\",\"url\":\"\",\"venue\":\"Inverse reinforcement learning via deep gaussian process.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Philip Thomas\"},{\"authorId\":null,\"name\":\"Emma Brunskill. Data-efficient off-policy policy evalua rl\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 33 ICML\",\"url\":\"\",\"venue\":\"pages 2139\\u20132148,\",\"year\":2016},{\"arxivId\":\"1507.08750\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4257bc131c36504a04382290cbc27ca8bb27813\",\"title\":\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Philip Thomas\"},{\"authorId\":null,\"name\":\"Emma Brunskill. Data-efficient off-policy policy evalua learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 2139\\u20132148,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bilal Piot\"},{\"authorId\":null,\"name\":\"Matthieu Geist\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Algo - rithms for inverse reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1710.11248\",\"authors\":[{\"authorId\":\"2550764\",\"name\":\"Justin Fu\"},{\"authorId\":\"27649809\",\"name\":\"Katie Luo\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"5e2c4e7b3302549b3718601c44d9af6c7554efef\",\"title\":\"Learning Robust Rewards with Adversarial Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5e2c4e7b3302549b3718601c44d9af6c7554efef\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1011.0686\",\"authors\":[{\"authorId\":\"1700433\",\"name\":\"S. Ross\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"title\":\"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\",\"url\":\"https://www.semanticscholar.org/paper/79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"venue\":\"AISTATS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Brian D Ziebart\"},{\"authorId\":null,\"name\":\"Andrew L Maas\"},{\"authorId\":null,\"name\":\"J Andrew Bagnell\"},{\"authorId\":null,\"name\":\"Anind K Dey. Maximum entropy inverse reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In 23 AAAI\",\"url\":\"\",\"venue\":\"pages 1433\\u2013 1438,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Barreto et al\"},{\"authorId\":null,\"name\":\"2017 Andr\\u00e9 Barreto\"},{\"authorId\":null,\"name\":\"Will Dabney\"},{\"authorId\":null,\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":null,\"name\":\"Jonathan J Hunt\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"Hado P van Hasselt\"},{\"authorId\":null,\"name\":\"David Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Successor features for transfer\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michail G Lagoudakis\"},{\"authorId\":null,\"name\":\"Ronald Parr. Least-squares policy iteration\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JMLR\",\"url\":\"\",\"venue\":\"4:1107\\u20131149,\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1986848\",\"name\":\"Z. Popovic\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"66ac3d7d8e75a64766fc59747d580bfa6d9e4031\",\"title\":\"Feature Construction for Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/66ac3d7d8e75a64766fc59747d580bfa6d9e4031\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1767216\",\"name\":\"E. Klein\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.1007/978-3-642-40988-2_1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c39e6fbefee9e3a367d762b2c8443999a0adbf5\",\"title\":\"A Cascaded Supervised Learning Approach to Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3c39e6fbefee9e3a367d762b2c8443999a0adbf5\",\"venue\":\"ECML/PKDD\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"}],\"doi\":\"10.1145/1143844.1143963\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"65f048e0420d11d099d7130fc16e4bd6e3ad88b1\",\"title\":\"Probabilistic inference for solving discrete and continuous state Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/65f048e0420d11d099d7130fc16e4bd6e3ad88b1\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Andrew Y Ng. Apprenticeship learning via inverse reinforc learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In 21ICML\",\"url\":\"\",\"venue\":\"page 1,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Benjamin Burchfiel\"},{\"authorId\":null,\"name\":\"Carlo Tomasi\"},{\"authorId\":null,\"name\":\"Ronald Parr. Distance minimization for reward learning fr trajectories\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 30 AAAI\",\"url\":\"\",\"venue\":\"pages 3330\\u2013 3336,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143825455\",\"name\":\"Z. Song\"},{\"authorId\":\"1806178\",\"name\":\"R. Parr\"},{\"authorId\":\"2585822\",\"name\":\"X. Liao\"},{\"authorId\":\"145006560\",\"name\":\"L. Carin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"aae163c9ee7c0ff4dc974fcf6890bd23f62d8f13\",\"title\":\"Linear Feature Encoding for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/aae163c9ee7c0ff4dc974fcf6890bd23f62d8f13\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1604.06778\",\"authors\":[{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"title\":\"Benchmarking Deep Reinforcement Learning for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/1464776f20e2bccb6182f183b5ff2e15b0ae5e56\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Edouard Klein\"},{\"authorId\":null,\"name\":\"Matthieu Geist\"},{\"authorId\":null,\"name\":\"Olivier Pietquin. In European Workshop on Reinforcement Learning\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 285\\u2013296\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2011},{\"arxivId\":\"1606.03476\",\"authors\":[{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"title\":\"Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Output Intake\"},{\"authorId\":null,\"name\":\"Events\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Fluid Output -4 hourly period, Total Fluid Output, Mechanical Ventilation, IV Fluids\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Edouard Klein\"},{\"authorId\":null,\"name\":\"Matthieu Geist\"},{\"authorId\":null,\"name\":\"Bilal Piot\"},{\"authorId\":null,\"name\":\"Olivier Pietquin. Inverse reinforcement learning through s classification\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In 26 NIPS\",\"url\":\"\",\"venue\":\"pages 1007\\u20131015,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"100655694\",\"name\":\"A. Moore\"}],\"doi\":\"10.1145/1143844\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"title\":\"Proceedings of the 23rd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"venue\":\"ICML 2008\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jonathan Ho\"},{\"authorId\":null,\"name\":\"Stefano Ermon. Generative adversarial imitation learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 30 NeurIPS\",\"url\":\"\",\"venue\":\"pages 4565\\u20134573,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Andrew Y Ng. Apprenticeship learning via inverse reinforc learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"page 1\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aniruddh Raghu\"},{\"authorId\":null,\"name\":\"Matthieu Komorowski\"},{\"authorId\":null,\"name\":\"Leo Celi Ahmed\"},{\"authorId\":null,\"name\":\"Imran\"},{\"authorId\":null,\"name\":\"Peter Szolovits\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Marzyeh Ghassemi\",\"url\":\"\",\"venue\":\"Deep reinforcement learning for sepsis treatment.\",\"year\":2017},{\"arxivId\":\"1604.03912\",\"authors\":[{\"authorId\":\"47205733\",\"name\":\"S. Herman\"},{\"authorId\":\"2729321\",\"name\":\"T. Gindele\"},{\"authorId\":\"102693700\",\"name\":\"J\\u00f6rg Wagner\"},{\"authorId\":\"48604701\",\"name\":\"F. Schmitt\"},{\"authorId\":\"1725973\",\"name\":\"W. Burgard\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ad9a450d07e23b91da660beca0e63365f50012a\",\"title\":\"Inverse Reinforcement Learning with Simultaneous Estimation of Rewards and Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/2ad9a450d07e23b91da660beca0e63365f50012a\",\"venue\":\"AISTATS\",\"year\":2016},{\"arxivId\":\"1711.09883\",\"authors\":[{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"2578985\",\"name\":\"Victoria Krakovna\"},{\"authorId\":\"145981974\",\"name\":\"Pedro A. Ortega\"},{\"authorId\":\"1868196\",\"name\":\"Tom Everitt\"},{\"authorId\":\"8455031\",\"name\":\"Andrew Lefrancq\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d09bec5af4eef5038e48b26b6c14098f95997114\",\"title\":\"AI Safety Gridworlds\",\"url\":\"https://www.semanticscholar.org/paper/d09bec5af4eef5038e48b26b6c14098f95997114\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"St\\u00e9phane Ross\"},{\"authorId\":null,\"name\":\"Drew Bagnell. Efficient reductions for imitation learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the thirteenth international conference on artificial intelligence and statistics\",\"url\":\"\",\"venue\":\"pages 661\\u2013668,\",\"year\":2010},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.1023/A:1017193430650\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"70850393a5efea8b407f2ba5cdf4eabe88827143\",\"title\":\"Volume 4\",\"url\":\"https://www.semanticscholar.org/paper/70850393a5efea8b407f2ba5cdf4eabe88827143\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"857d4a07e3ac77f37e4442a3e38987d8bdc0d7fa\",\"title\":\"Boosted and reward-regularized classification for apprenticeship learning\",\"url\":\"https://www.semanticscholar.org/paper/857d4a07e3ac77f37e4442a3e38987d8bdc0d7fa\",\"venue\":\"AAMAS\",\"year\":2014}],\"title\":\"Truly Batch Apprenticeship Learning with Deep Successor Features\",\"topics\":[{\"topic\":\"Apprenticeship learning\",\"topicId\":\"1269616\",\"url\":\"https://www.semanticscholar.org/topic/1269616\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Data acquisition\",\"topicId\":\"7391\",\"url\":\"https://www.semanticscholar.org/topic/7391\"},{\"topic\":\"International Components for Unicode\",\"topicId\":\"345728\",\"url\":\"https://www.semanticscholar.org/topic/345728\"},{\"topic\":\"Simulation\",\"topicId\":\"194\",\"url\":\"https://www.semanticscholar.org/topic/194\"},{\"topic\":\"ATi Radeon R300 Series\",\"topicId\":\"828773\",\"url\":\"https://www.semanticscholar.org/topic/828773\"}],\"url\":\"https://www.semanticscholar.org/paper/fdc99b79b1355fc5f0ef5b81703021344c2ff0ac\",\"venue\":\"IJCAI\",\"year\":2019}\n"