"{\"abstract\":\"Reinforcement Learning agents are expected to eventually perform well. Typically, this takes the form of a guarantee about the asymptotic behavior of an algorithm given some assumptions about the environment. We present an algorithm for a policy whose value approaches the optimal value with probability 1 in all computable probabilistic environments, provided the agent has a bounded horizon. This is known as strong asymptotic optimality, and it was previously unknown whether it was possible for a policy to be strongly asymptotically optimal in the class of all computable probabilistic environments. Our agent, Inquisitive Reinforcement Learner (Inq), is more likely to explore the more it expects an exploratory action to reduce its uncertainty about which environment it is in, hence the term inquisitive. Exploring inquisitively is a strategy that can be applied generally; for more manageable environment classes, inquisitiveness is tractable. We conducted experiments in \\\"grid-worlds\\\" to compare the Inquisitive Reinforcement Learner to other weakly asymptotically optimal agents.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"11983844\",\"name\":\"Michael K. Cohen\",\"url\":\"https://www.semanticscholar.org/author/11983844\"},{\"authorId\":\"22574075\",\"name\":\"Elliot Catt\",\"url\":\"https://www.semanticscholar.org/author/22574075\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\",\"url\":\"https://www.semanticscholar.org/author/144154444\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2006.03357\",\"authors\":[{\"authorId\":\"11983844\",\"name\":\"Michael K. Cohen\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9c1e73c0df0ac1d5a29ebf562a81988f5551b78c\",\"title\":\"Curiosity Killed the Cat and the Asymptotically Optimal Agent\",\"url\":\"https://www.semanticscholar.org/paper/9c1e73c0df0ac1d5a29ebf562a81988f5551b78c\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.09153\",\"authors\":[{\"authorId\":\"34936272\",\"name\":\"D. Krueger\"},{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"138d367a9352685faebf33128461a58a0f38060b\",\"title\":\"Hidden Incentives for Auto-Induced Distributional Shift\",\"url\":\"https://www.semanticscholar.org/paper/138d367a9352685faebf33128461a58a0f38060b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34936272\",\"name\":\"D. Krueger\"},{\"authorId\":\"3422058\",\"name\":\"Tegan Maharaj\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d61f385b0a3a324acf5c6c3726efd64c6f739268\",\"title\":\"Hidden incentives for self-induced distributional shift\",\"url\":\"https://www.semanticscholar.org/paper/d61f385b0a3a324acf5c6c3726efd64c6f739268\",\"venue\":\"\",\"year\":2019}],\"corpusId\":166227856,\"doi\":\"10.24963/ijcai.2019/302\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"1430ff8b67a5a221dd719eff6e1c9dee6adbc595\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peter Whittle\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Discussion of Dr Gittins\\u2019 paper\",\"url\":\"\",\"venue\":\"Journal of the Royal Statistical Society, 41:164\\u2013177,\",\"year\":1979},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1109/DevLrn.2012.6400802\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0ccc40cdbd81ef83cdf99f4bed486c848e1f090\",\"title\":\"Intrinsically motivated model learning for a developing curious agent\",\"url\":\"https://www.semanticscholar.org/paper/e0ccc40cdbd81ef83cdf99f4bed486c848e1f090\",\"venue\":\"2012 IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"2989692\",\"name\":\"Tor Lattimore\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1007/978-3-642-40935-6_12\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e8dd477ec5d34ba46b8fe654537d509560c91ca3\",\"title\":\"Universal Knowledge-Seeking Agents for Stochastic Environments\",\"url\":\"https://www.semanticscholar.org/paper/e8dd477ec5d34ba46b8fe654537d509560c91ca3\",\"venue\":\"ALT\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143817739\",\"name\":\"J. Leeuwen\"},{\"authorId\":\"1714183\",\"name\":\"C. Blundo\"},{\"authorId\":\"1749870\",\"name\":\"C. Laneve\"},{\"authorId\":\"143817739\",\"name\":\"J. Leeuwen\"}],\"doi\":\"10.1007/b13810\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"524f2ec5811aa0e9a6aea053c75488d9b83bb170\",\"title\":\"Theoretical Computer Science\",\"url\":\"https://www.semanticscholar.org/paper/524f2ec5811aa0e9a6aea053c75488d9b83bb170\",\"venue\":\"Lecture Notes in Computer Science\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jan Leike\"},{\"authorId\":null,\"name\":\"Tor Lattimore\"},{\"authorId\":null,\"name\":\"Laurent Orseau\"},{\"authorId\":null,\"name\":\"Marcus Hutter. Thompson sampling is asymptotically optima Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"32nd International Conf\",\"url\":\"\",\"venue\":\"on Uncertainty in Artificial Intelligence (UAI\\u201916), pages 417\\u2013426, New Jersey, USA,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Susanne Still. Information-theoretic approach to interacti learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"EPL (Europhysics Letters)\",\"url\":\"\",\"venue\":\"85(2):28005,\",\"year\":2009},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hessel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning\",\"url\":\"\",\"venue\":\"Proc. of AAAI Conference on Artificial Intelligence\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Todd Hester\"},{\"authorId\":null,\"name\":\"Peter Stone. Intrinsically motivated model learning for agent\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 2012 IEEE international conference on development and learning and epigenetic robotics (ICDL)\",\"url\":\"\",\"venue\":\"pages 1\\u20136. IEEE,\",\"year\":2012},{\"arxivId\":\"1705.07615\",\"authors\":[{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1234cd20688261084f6223909dc910c935235f7a\",\"title\":\"AIXIjs: A Software Demo for General Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1234cd20688261084f6223909dc910c935235f7a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2989692\",\"name\":\"Tor Lattimore\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1016/j.tcs.2013.09.022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a84a2239f29d1e72be2d381374f11b2a49afd32\",\"title\":\"General time consistent discounting\",\"url\":\"https://www.semanticscholar.org/paper/2a84a2239f29d1e72be2d381374f11b2a49afd32\",\"venue\":\"Theor. Comput. Sci.\",\"year\":2014},{\"arxivId\":\"1712.01815\",\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2449382\",\"name\":\"T. Hubert\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"40227832\",\"name\":\"Matthew Lai\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"38fb1902c6a2ab4f767d4532b28a92473ea737aa\",\"title\":\"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/38fb1902c6a2ab4f767d4532b28a92473ea737aa\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Laurent Orseau\"},{\"authorId\":null,\"name\":\"Tor Lattimore\"},{\"authorId\":null,\"name\":\"Marcus Hutter. Universal knowledge-seeking agents for sto Proc\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"24th International Conf\",\"url\":\"\",\"venue\":\"on Algorithmic Learning Theory (ALT\\u201913), volume 8139 of LNAI, pages 158\\u2013172, Singapore,\",\"year\":2013},{\"arxivId\":\"1710.02298\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"title\":\"Rainbow: Combining Improvements in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1107.5537\",\"authors\":[{\"authorId\":\"2989692\",\"name\":\"Tor Lattimore\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1007/978-3-642-24412-4_29\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"acfe9ca6d6b7599c536390a3df1c25284a45f429\",\"title\":\"Asymptotically Optimal Agents\",\"url\":\"https://www.semanticscholar.org/paper/acfe9ca6d6b7599c536390a3df1c25284a45f429\",\"venue\":\"ALT\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1007/b138233\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f47655fe96e155cfc29860331681e65377ae467a\",\"title\":\"Universal Artificial Intellegence - Sequential Decisions Based on Algorithmic Probability\",\"url\":\"https://www.semanticscholar.org/paper/f47655fe96e155cfc29860331681e65377ae467a\",\"venue\":\"Texts in Theoretical Computer Science. An EATCS Series\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jan Leike\"},{\"authorId\":null,\"name\":\"Tor Lattimore\"},{\"authorId\":null,\"name\":\"Laurent Orseau\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"andMarcus Hutter\",\"url\":\"\",\"venue\":\"Thompson sampling is asymptotically optimal in general environments. In Proc. 32nd International Conf. on Uncertainty in Artificial Intelligence (UAI\\u201916), pages 417\\u2013426, New Jersey, USA,\",\"year\":2016},{\"arxivId\":\"1705.10557\",\"authors\":[{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.24963/ijcai.2017/194\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d1627e5dd7c6656aa8c16d861677ac631c5c4301\",\"title\":\"Universal Reinforcement Learning Algorithms: Survey and Experiments\",\"url\":\"https://www.semanticscholar.org/paper/d1627e5dd7c6656aa8c16d861677ac631c5c4301\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2989692\",\"name\":\"Tor Lattimore\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":\"10.1007/978-3-319-11662-4_13\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3415c56de8c6ae142a9eebe54192322ae12d078e\",\"title\":\"Bayesian Reinforcement Learning with Exploration\",\"url\":\"https://www.semanticscholar.org/paper/3415c56de8c6ae142a9eebe54192322ae12d078e\",\"venue\":\"ALT\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tor Lattimore\"},{\"authorId\":null,\"name\":\"Marcus Hutter. Asymptotically optimal agents. In Proc\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"22nd International Conf\",\"url\":\"\",\"venue\":\"on Algorithmic Learning Theory (ALT\\u201911), volume 6925 of LNAI, pages 368\\u2013382, Espoo, Finland,\",\"year\":2011},{\"arxivId\":\"1602.07905\",\"authors\":[{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"2989692\",\"name\":\"Tor Lattimore\"},{\"authorId\":\"1749270\",\"name\":\"Laurent Orseau\"},{\"authorId\":\"144154444\",\"name\":\"Marcus Hutter\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a81f1155f847b268ae4bed160540c2ee3acb5a9\",\"title\":\"Thompson Sampling is Asymptotically Optimal in General Environments\",\"url\":\"https://www.semanticscholar.org/paper/1a81f1155f847b268ae4bed160540c2ee3acb5a9\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1605.09674\",\"authors\":[{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1715957\",\"name\":\"F. Turck\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fb3c6456708b0e143f545d77dc8ec804eb947395\",\"title\":\"Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/fb3c6456708b0e143f545d77dc8ec804eb947395\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tor Lattimore\"},{\"authorId\":null,\"name\":\"Marcus Hutter. Bayesian reinforcement learning with explo Theory\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 170\\u2013184\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2014}],\"title\":\"A Strongly Asymptotically Optimal Agent in General Environments\",\"topics\":[{\"topic\":\"Asymptotically optimal algorithm\",\"topicId\":\"26547\",\"url\":\"https://www.semanticscholar.org/topic/26547\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Computable function\",\"topicId\":\"39979\",\"url\":\"https://www.semanticscholar.org/topic/39979\"},{\"topic\":\"Bayesian network\",\"topicId\":\"14005\",\"url\":\"https://www.semanticscholar.org/topic/14005\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Cobham's thesis\",\"topicId\":\"266433\",\"url\":\"https://www.semanticscholar.org/topic/266433\"},{\"topic\":\"Optimal control\",\"topicId\":\"3646\",\"url\":\"https://www.semanticscholar.org/topic/3646\"},{\"topic\":\"The Australian\",\"topicId\":\"38542\",\"url\":\"https://www.semanticscholar.org/topic/38542\"},{\"topic\":\"Optimization problem\",\"topicId\":\"12682\",\"url\":\"https://www.semanticscholar.org/topic/12682\"},{\"topic\":\"Entropy maximization\",\"topicId\":\"182676\",\"url\":\"https://www.semanticscholar.org/topic/182676\"}],\"url\":\"https://www.semanticscholar.org/paper/1430ff8b67a5a221dd719eff6e1c9dee6adbc595\",\"venue\":\"IJCAI\",\"year\":2019}\n"