"{\"abstract\":\"Gradient sparsification is a promising technique to significantly reduce the communication overhead in decentralized synchronous stochastic gradient descent (S-SGD) algorithms. Yet, many existing gradient sparsification schemes (e.g., Top-k sparsification) have a communication complexity ofO(kP ), where k is the number of selected gradients by each worker and P is the number of workers. Recently, the gTop-k sparsification scheme has been proposed to reduce the communication complexity from O(kP ) to O(k logP ), which significantly boosts the system scalability. However, it remains unclear whether the gTop-k sparsification scheme can converge in theory. In this paper, we first provide theoretical proofs on the convergence of the gTop-k scheme for non-convex objective functions under certain analytic assumptions. We then derive the convergence rate of gTop-k SSGD, which is at the same order as the vanilla minibatch SGD. Finally, we conduct extensive experiments on different machine learning models and data sets to verify the soundness of the assumptions and theoretical results, and discuss the impact of the compression ratio on the convergence performance.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\",\"url\":\"https://www.semanticscholar.org/author/2268704\"},{\"authorId\":\"116747823\",\"name\":\"Kaiyong Zhao\",\"url\":\"https://www.semanticscholar.org/author/116747823\"},{\"authorId\":\"39453853\",\"name\":\"Q. Wang\",\"url\":\"https://www.semanticscholar.org/author/39453853\"},{\"authorId\":\"66873962\",\"name\":\"Z. Tang\",\"url\":\"https://www.semanticscholar.org/author/66873962\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\",\"url\":\"https://www.semanticscholar.org/author/51502693\"}],\"citationVelocity\":6,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1878321362\",\"name\":\"Yuetong Yang\"},{\"authorId\":\"3050140\",\"name\":\"Zhiquan Lai\"},{\"authorId\":\"39102104\",\"name\":\"L. Cai\"},{\"authorId\":\"144032853\",\"name\":\"Dong-sheng Li\"}],\"doi\":\"10.1109/infocomwkshps50562.2020.9162748\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"36630a3b605da02e2aac36ea56d2a9ba88710c2a\",\"title\":\"Poster Abstract: Model Average-based Distributed Training for Sparse Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/36630a3b605da02e2aac36ea56d2a9ba88710c2a\",\"venue\":\"IEEE INFOCOM 2020 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"39453853\",\"name\":\"Q. Wang\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"},{\"authorId\":\"71788673\",\"name\":\"Bo Li\"},{\"authorId\":\"38253418\",\"name\":\"Y. Qin\"},{\"authorId\":\"50268171\",\"name\":\"Ruihao Liu\"},{\"authorId\":\"9272990\",\"name\":\"Xinxiao Zhao\"}],\"doi\":\"10.1109/infocom41043.2020.9155269\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9222062c20aa6a7fad8899da0453269fb804d6ac\",\"title\":\"Communication-Efficient Distributed Deep Learning with Merged Gradient Sparsification on GPUs\",\"url\":\"https://www.semanticscholar.org/paper/9222062c20aa6a7fad8899da0453269fb804d6ac\",\"venue\":\"IEEE INFOCOM 2020 - IEEE Conference on Computer Communications\",\"year\":2020},{\"arxivId\":\"1912.09268\",\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"},{\"authorId\":\"46708584\",\"name\":\"B. Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"63582943e96bacbfed81f70b1ee3b91543ed01f6\",\"title\":\"MG-WFBP: Merging Gradients Wisely for Efficient Communication in Distributed Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/63582943e96bacbfed81f70b1ee3b91543ed01f6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.10458\",\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"48667448\",\"name\":\"Xianhao Zhou\"},{\"authorId\":\"11719148\",\"name\":\"Shutao Song\"},{\"authorId\":\"1932054959\",\"name\":\"Xingyao Wang\"},{\"authorId\":\"71663028\",\"name\":\"Zilin Zhu\"},{\"authorId\":\"145495862\",\"name\":\"Xue Huang\"},{\"authorId\":\"87514554\",\"name\":\"Xin-An Jiang\"},{\"authorId\":\"144165017\",\"name\":\"F. Zhou\"},{\"authorId\":\"145181674\",\"name\":\"Zhenyu Guo\"},{\"authorId\":\"49352631\",\"name\":\"L. Xie\"},{\"authorId\":\"122211571\",\"name\":\"R. Lan\"},{\"authorId\":\"1657698999\",\"name\":\"Xianbin Ouyang\"},{\"authorId\":null,\"name\":\"Yan Zhang\"},{\"authorId\":\"1999567013\",\"name\":\"Jieqian Wei\"},{\"authorId\":\"145116756\",\"name\":\"Jing Gong\"},{\"authorId\":\"2298843\",\"name\":\"Wei-Liang Lin\"},{\"authorId\":\"1557300469\",\"name\":\"Ping Gao\"},{\"authorId\":\"2709055\",\"name\":\"P. Meng\"},{\"authorId\":\"1500399723\",\"name\":\"Xiaomin Xu\"},{\"authorId\":\"3723105\",\"name\":\"Chenyang Guo\"},{\"authorId\":\"143787140\",\"name\":\"B. Yang\"},{\"authorId\":\"143912275\",\"name\":\"Zhibo Chen\"},{\"authorId\":\"47096329\",\"name\":\"Yongjian Wu\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f57db358459390590bc838663025dae0f8d51ebf\",\"title\":\"Towards Scalable Distributed Training of Deep Learning on Public Cloud Clusters\",\"url\":\"https://www.semanticscholar.org/paper/f57db358459390590bc838663025dae0f8d51ebf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.06307\",\"authors\":[{\"authorId\":\"66873962\",\"name\":\"Z. Tang\"},{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"},{\"authorId\":\"86955124\",\"name\":\"Wei Wang\"},{\"authorId\":\"143771567\",\"name\":\"Bo Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69170faf00279bdcdfd91290a3756d539f9fb6e6\",\"title\":\"Communication-Efficient Distributed Deep Learning: A Comprehensive Survey\",\"url\":\"https://www.semanticscholar.org/paper/69170faf00279bdcdfd91290a3756d539f9fb6e6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.06377\",\"authors\":[{\"authorId\":\"35342555\",\"name\":\"Sarit Khirirat\"},{\"authorId\":\"31783616\",\"name\":\"S. Magn\\u00fasson\"},{\"authorId\":\"1686881\",\"name\":\"A. Aytekin\"},{\"authorId\":\"153374846\",\"name\":\"M. Johansson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"239a5598deb4fa3c99c62a3a4dceec95bce69cb4\",\"title\":\"A flexible framework for communication-efficient machine learning: from HPC to IoT\",\"url\":\"https://www.semanticscholar.org/paper/239a5598deb4fa3c99c62a3a4dceec95bce69cb4\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.08772\",\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"},{\"authorId\":\"2264187\",\"name\":\"K. Cheung\"},{\"authorId\":\"144308998\",\"name\":\"S. See\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"9b07ecad383a426ec5f5d110329f0c04d4e9cf0b\",\"title\":\"Understanding Top-k Sparsification in Distributed Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/9b07ecad383a426ec5f5d110329f0c04d4e9cf0b\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2001.04756\",\"authors\":[{\"authorId\":\"2598318\",\"name\":\"P. Han\"},{\"authorId\":\"50695457\",\"name\":\"Shiqiang Wang\"},{\"authorId\":\"145353889\",\"name\":\"K. Leung\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bed26e79b800bc4a6e2fc46342a84d270ab38174\",\"title\":\"Adaptive Gradient Sparsification for Efficient Federated Learning: An Online Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/bed26e79b800bc4a6e2fc46342a84d270ab38174\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.04436\",\"authors\":[{\"authorId\":\"50980344\",\"name\":\"Yi Liu\"},{\"authorId\":\"2567863\",\"name\":\"Ruihui Zhao\"},{\"authorId\":\"145993626\",\"name\":\"Jiawen Kang\"},{\"authorId\":\"146224202\",\"name\":\"Abdulsalam Yassine\"},{\"authorId\":\"123252833\",\"name\":\"Dusit Niyato\"},{\"authorId\":\"2715913\",\"name\":\"Jialiang Peng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1474a3710ef47777761c4ee054186c933d5724c5\",\"title\":\"Towards Communication-efficient and Attack-Resistant Federated Edge Learning for Industrial Internet of Things\",\"url\":\"https://www.semanticscholar.org/paper/1474a3710ef47777761c4ee054186c933d5724c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.08727\",\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"66873962\",\"name\":\"Z. Tang\"},{\"authorId\":\"145805403\",\"name\":\"Q. Wang\"},{\"authorId\":\"116747823\",\"name\":\"Kaiyong Zhao\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"}],\"doi\":\"10.3233/FAIA200253\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7e084a5d054c29402e591e6cefedba6cf5978822\",\"title\":\"Layer-wise Adaptive Gradient Sparsification for Distributed Deep Learning with Convergence Guarantees\",\"url\":\"https://www.semanticscholar.org/paper/7e084a5d054c29402e591e6cefedba6cf5978822\",\"venue\":\"ECAI\",\"year\":2020},{\"arxivId\":\"2002.09692\",\"authors\":[{\"authorId\":\"66873962\",\"name\":\"Z. Tang\"},{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"eaeaba781c2fed352dacdf8e8d93423393a1d9d4\",\"title\":\"Communication-Efficient Decentralized Learning with Sparsification and Adaptive Peer Selection\",\"url\":\"https://www.semanticscholar.org/paper/eaeaba781c2fed352dacdf8e8d93423393a1d9d4\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.07405\",\"authors\":[{\"authorId\":\"48018729\",\"name\":\"Subhadeep Bhattacharya\"},{\"authorId\":\"1709886\",\"name\":\"Weikuan Yu\"},{\"authorId\":\"1749348289\",\"name\":\"Fahim Tahmid Chowdhury\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"639695a5cec7f872bc6648d4a269d7477ff66567\",\"title\":\"O(1) Communication for Distributed SGD through Two-Level Gradient Averaging\",\"url\":\"https://www.semanticscholar.org/paper/639695a5cec7f872bc6648d4a269d7477ff66567\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199466136,\"doi\":\"10.24963/ijcai.2019/473\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"966995e660c1b760ccce6e4995bd70020e7484c7\",\"references\":[{\"arxivId\":\"1802.08021\",\"authors\":[{\"authorId\":\"37833666\",\"name\":\"C\\u00e9dric Renggli\"},{\"authorId\":\"3311387\",\"name\":\"Dan Alistarh\"},{\"authorId\":\"1713648\",\"name\":\"Torsten Hoefler\"}],\"doi\":\"10.1145/3295500.3356222\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"e0c0043c43c03de6e2c26a605dc8b8e08872a8a0\",\"title\":\"SparCML: high-performance sparse communication for machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e0c0043c43c03de6e2c26a605dc8b8e08872a8a0\",\"venue\":\"SC\",\"year\":2019},{\"arxivId\":\"1710.09854\",\"authors\":[{\"authorId\":\"3383269\",\"name\":\"Jianqiao Wangni\"},{\"authorId\":\"1809306\",\"name\":\"Jialei Wang\"},{\"authorId\":\"1691762\",\"name\":\"J. Liu\"},{\"authorId\":\"49104973\",\"name\":\"Tong Zhang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1caff87770b1cbddcf94edc0a9b1bce029324765\",\"title\":\"Gradient Sparsification for Communication-Efficient Distributed Optimization\",\"url\":\"https://www.semanticscholar.org/paper/1caff87770b1cbddcf94edc0a9b1bce029324765\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sebastian U Stich\"},{\"authorId\":null,\"name\":\"Jean-Baptiste Cordonnier\"},{\"authorId\":null,\"name\":\"Martin Jaggi. Sparsified SGD with memory\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 4452\\u20134463,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peng Jiang\"},{\"authorId\":null,\"name\":\"Gagan Agrawal. A linear speedup analysis of distributed sparse\"},{\"authorId\":null,\"name\":\"quantized communication\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 2530\\u20132541,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alham Fikri Aji\"},{\"authorId\":null,\"name\":\"Kenneth Heafield. Sparse communication for distributed gra descent\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In EMNLP\",\"url\":\"\",\"venue\":\"pages 440\\u2013445,\",\"year\":2017},{\"arxivId\":\"1712.01887\",\"authors\":[{\"authorId\":\"49417466\",\"name\":\"Yujun Lin\"},{\"authorId\":\"153114506\",\"name\":\"S. Han\"},{\"authorId\":\"3123774\",\"name\":\"Huizi Mao\"},{\"authorId\":null,\"name\":\"Yu Wang\"},{\"authorId\":\"80724002\",\"name\":\"W. Dally\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"92495abbac86394cb759bec15a763dbf49a8e590\",\"title\":\"Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training\",\"url\":\"https://www.semanticscholar.org/paper/92495abbac86394cb759bec15a763dbf49a8e590\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shaohuai Shi\"},{\"authorId\":null,\"name\":\"Wang Qiang\"},{\"authorId\":null,\"name\":\"Xiaowen Chu. Performance modeling\"},{\"authorId\":null,\"name\":\"evaluation of distributed deep learning frameworks on GPUs\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IEEE DataCom\",\"url\":\"\",\"venue\":\"pages 949\\u2013957,\",\"year\":2018},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1811.11141\",\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"51502693\",\"name\":\"Xiaowen Chu\"}],\"doi\":\"10.1109/INFOCOM.2019.8737367\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bfd6a7e5a26acbd2569fcb29595a46e3ef2f7755\",\"title\":\"MG-WFBP: Efficient Data Communication for Distributed Synchronous SGD Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/bfd6a7e5a26acbd2569fcb29595a46e3ef2f7755\",\"venue\":\"IEEE INFOCOM 2019 - IEEE Conference on Computer Communications\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1712.02679\",\"authors\":[{\"authorId\":\"48240124\",\"name\":\"Chia-Yu Chen\"},{\"authorId\":\"2506452\",\"name\":\"Jungwook Choi\"},{\"authorId\":\"143915650\",\"name\":\"D. Brand\"},{\"authorId\":\"143624709\",\"name\":\"A. Agrawal\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"33678523\",\"name\":\"K. Gopalakrishnan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3193c82ae0598f7df7527b6f1a3836eee345d122\",\"title\":\"AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training\",\"url\":\"https://www.semanticscholar.org/paper/3193c82ae0598f7df7527b6f1a3836eee345d122\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1711.05979\",\"authors\":[{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"1680596\",\"name\":\"Xiaowen Chu\"}],\"doi\":\"10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.000-4\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f5f0b1be231321b6b2099de69c00eb8d9892ed2a\",\"title\":\"Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs\",\"url\":\"https://www.semanticscholar.org/paper/f5f0b1be231321b6b2099de69c00eb8d9892ed2a\",\"venue\":\"2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hongyi Wang\"},{\"authorId\":null,\"name\":\"Scott Sievert\"},{\"authorId\":null,\"name\":\"Shengchao Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Jean - Baptiste Cordon - nier , and Martin Jaggi . Sparsified SGD with memory\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"67213297\",\"name\":\"A. Acero\"}],\"doi\":\"10.1007/978-1-4615-3122-7\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5bab523889f820a92043cd43662393018ea546b\",\"title\":\"Acoustical and environmental robustness in automatic speech recognition\",\"url\":\"https://www.semanticscholar.org/paper/e5bab523889f820a92043cd43662393018ea546b\",\"venue\":\"\",\"year\":1991},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1807.11205\",\"authors\":[{\"authorId\":\"3440541\",\"name\":\"X. Jia\"},{\"authorId\":\"51150373\",\"name\":\"S. Song\"},{\"authorId\":\"48692318\",\"name\":\"W. He\"},{\"authorId\":\"1772360\",\"name\":\"Yangzihao Wang\"},{\"authorId\":\"51151596\",\"name\":\"Haidong Rong\"},{\"authorId\":\"144165017\",\"name\":\"F. Zhou\"},{\"authorId\":\"49352631\",\"name\":\"L. Xie\"},{\"authorId\":\"1850900\",\"name\":\"Zhenyu Guo\"},{\"authorId\":\"1819338\",\"name\":\"Yuanzhou Yang\"},{\"authorId\":\"47785024\",\"name\":\"L. Yu\"},{\"authorId\":\"51150012\",\"name\":\"Tiegang Chen\"},{\"authorId\":\"51149410\",\"name\":\"G. Hu\"},{\"authorId\":\"2268704\",\"name\":\"Shaohuai Shi\"},{\"authorId\":\"1680596\",\"name\":\"Xiaowen Chu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a82fc0115c1802d48d352b35595204738fad84f0\",\"title\":\"Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes\",\"url\":\"https://www.semanticscholar.org/paper/a82fc0115c1802d48d352b35595204738fad84f0\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1806.04090\",\"authors\":[{\"authorId\":\"3148801\",\"name\":\"H. Wang\"},{\"authorId\":\"34953991\",\"name\":\"S. Sievert\"},{\"authorId\":\"40929370\",\"name\":\"S. Liu\"},{\"authorId\":\"143676545\",\"name\":\"Zachary B. Charles\"},{\"authorId\":\"1740595\",\"name\":\"Dimitris Papailiopoulos\"},{\"authorId\":\"144731792\",\"name\":\"S. Wright\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b611636f3cfe7b9aa41a606bec1d9fa72e1359ae\",\"title\":\"ATOMO: Communication-efficient Learning via Atomic Sparsification\",\"url\":\"https://www.semanticscholar.org/paper/b611636f3cfe7b9aa41a606bec1d9fa72e1359ae\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"L\\u00e9on Bottou\"},{\"authorId\":null,\"name\":\"Frank E Curtis\"},{\"authorId\":null,\"name\":\"Jorge Nocedal. Optimization methods for large-scale mach learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Siam Review\",\"url\":\"\",\"venue\":\"60(2):223\\u2013311,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1734174\",\"name\":\"M. Marcus\"},{\"authorId\":\"2424234\",\"name\":\"Beatrice Santorini\"},{\"authorId\":\"2063206\",\"name\":\"Mary Ann Marcinkiewicz\"}],\"doi\":\"10.21236/ada273556\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0b44fcbeea9415d400c5f5789d6b892b6f98daff\",\"title\":\"Building a Large Annotated Corpus of English: The Penn Treebank\",\"url\":\"https://www.semanticscholar.org/paper/0b44fcbeea9415d400c5f5789d6b892b6f98daff\",\"venue\":\"Comput. Linguistics\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40262099\",\"name\":\"C. Chen\"},{\"authorId\":\"49337214\",\"name\":\"W. Wang\"},{\"authorId\":\"38513543\",\"name\":\"B. Li\"}],\"doi\":\"10.1109/INFOCOM.2019.8737587\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"48dec1ce3cfdef6667208cee20eee8a64c9d3302\",\"title\":\"Round-Robin Synchronization: Mitigating Communication Bottlenecks in Parameter Servers\",\"url\":\"https://www.semanticscholar.org/paper/48dec1ce3cfdef6667208cee20eee8a64c9d3302\",\"venue\":\"IEEE INFOCOM 2019 - IEEE Conference on Computer Communications\",\"year\":2019}],\"title\":\"A Convergence Analysis of Distributed SGD with Communication-Efficient Gradient Sparsification\",\"topics\":[{\"topic\":\"Stochastic gradient descent\",\"topicId\":\"202796\",\"url\":\"https://www.semanticscholar.org/topic/202796\"}],\"url\":\"https://www.semanticscholar.org/paper/966995e660c1b760ccce6e4995bd70020e7484c7\",\"venue\":\"IJCAI\",\"year\":2019}\n"