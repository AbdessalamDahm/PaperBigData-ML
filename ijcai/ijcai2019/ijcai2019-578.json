"{\"abstract\":\"It is arguably believed that flatter minima can generalize better. However, it has been pointed out that the usual definitions of sharpness, which consider either the maxima or the integral of loss over a \\u03b4 ball of parameters around minima, cannot give consistent measurement for scale invariant neural networks, e.g., networks with batch normalization layer. In this paper, we first propose a measure of sharpness, BN-Sharpness, which gives consistent value for equivalent networks under BN. It achieves the property of scale invariance by connecting the integral diameter with the scale of parameter. Then we present a computation-efficient way to calculate the BN-sharpness approximately i.e., one dimensional integral along the \\u201dsharpest\\u201d direction. Furthermore, we use the BN-sharpness to regularize the training and design an algorithm to minimize the new regularized objective. Our algorithm achieves considerably better performance than vanilla SGD over various experiment settings.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"3541183\",\"name\":\"M. Yi\",\"url\":\"https://www.semanticscholar.org/author/3541183\"},{\"authorId\":\"2973831\",\"name\":\"Huishuai Zhang\",\"url\":\"https://www.semanticscholar.org/author/2973831\"},{\"authorId\":\"103160264\",\"name\":\"W. Chen\",\"url\":\"https://www.semanticscholar.org/author/103160264\"},{\"authorId\":\"2249674\",\"name\":\"Z. Ma\",\"url\":\"https://www.semanticscholar.org/author/2249674\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\",\"url\":\"https://www.semanticscholar.org/author/152998017\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":199466150,\"doi\":\"10.24963/ijcai.2019/578\",\"fieldsOfStudy\":[\"Mathematics\",\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"5b9c93df5fc8db6b709f195bcecd8e6e1018d558\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Elad Hoffer\"},{\"authorId\":null,\"name\":\"Itay Hubara\"},{\"authorId\":null,\"name\":\"Daniel Soudryn. Train longer\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"generalize better: closing the generalization gap in large batch training of neural networks\",\"url\":\"\",\"venue\":\"Advances in Neural Information Processing Systems 30 (NIPS 2017),\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1794034\",\"name\":\"Manfred K. Warmuth\"}],\"doi\":\"10.1145/180139\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2dc6315533aaec1741e6348c826b0dbaf869cad5\",\"title\":\"Proceedings of the seventh annual conference on Computational learning theory\",\"url\":\"https://www.semanticscholar.org/paper/2dc6315533aaec1741e6348c826b0dbaf869cad5\",\"venue\":\"COLT 1994\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50356391\",\"name\":\"Pierre-Antoine Absil\"},{\"authorId\":\"1686222\",\"name\":\"K. Gallivan\"}],\"doi\":\"10.1109/ICASSP.2006.1661433\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"95be6cb9ba7d9eae5f2930153823cb37200305df\",\"title\":\"Joint Diagonalization on the Oblique Manifold for Independent Component Analysis\",\"url\":\"https://www.semanticscholar.org/paper/95be6cb9ba7d9eae5f2930153823cb37200305df\",\"venue\":\"2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings\",\"year\":2006},{\"arxivId\":\"1805.07898\",\"authors\":[{\"authorId\":\"152611136\",\"name\":\"Wei Wen\"},{\"authorId\":null,\"name\":\"Yandan Wang\"},{\"authorId\":\"48754100\",\"name\":\"Feng Yan\"},{\"authorId\":\"97932781\",\"name\":\"C. Xu\"},{\"authorId\":\"3207491\",\"name\":\"Chunpeng Wu\"},{\"authorId\":\"50579965\",\"name\":\"Yiran Chen\"},{\"authorId\":\"51208601\",\"name\":\"Hongbing Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a20efbe0bf1e5af0af29cd8966c8bc983a8ac6d0\",\"title\":\"SmoothOut: Smoothing Out Sharp Minima to Improve Generalization in Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/a20efbe0bf1e5af0af29cd8966c8bc983a8ac6d0\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1502.03167\",\"authors\":[{\"authorId\":\"144147316\",\"name\":\"S. Ioffe\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d376d6978dad0374edfa6709c9556b42d3594d3\",\"title\":\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\",\"url\":\"https://www.semanticscholar.org/paper/4d376d6978dad0374edfa6709c9556b42d3594d3\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David A. McAllesterl. Pac-bayesian model averaging\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the eleventh annual conference on Computational learning theory\",\"url\":\"\",\"venue\":\"pages 164\\u2013 170,\",\"year\":1999},{\"arxivId\":\"1709.09603\",\"authors\":[{\"authorId\":\"1853319\",\"name\":\"Minhyung Cho\"},{\"authorId\":\"5352673\",\"name\":\"Jaehyung Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"64764484de6b2ce5099a7a20b5a926c63f01c8e9\",\"title\":\"Riemannian approach to batch normalization\",\"url\":\"https://www.semanticscholar.org/paper/64764484de6b2ce5099a7a20b5a926c63f01c8e9\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David A. McAllesterl. Simplified pacbayesian margin bounds\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lecture notes in computer science\",\"url\":\"\",\"venue\":\"pages 203\\u2013215,\",\"year\":2003},{\"arxivId\":\"1709.06079\",\"authors\":[{\"authorId\":\"48544864\",\"name\":\"Lei Huang\"},{\"authorId\":\"6820648\",\"name\":\"Xianglong Liu\"},{\"authorId\":\"1727601\",\"name\":\"B. Lang\"},{\"authorId\":\"40625240\",\"name\":\"Adams Wei Yu\"},{\"authorId\":\"143771567\",\"name\":\"Bo Li\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6cc8fe49b353c77846af7437d8f98d3d7e3b56c3\",\"title\":\"Orthogonal Weight Normalization: Solution to Optimization over Multiple Dependent Stiefel Manifolds in Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/6cc8fe49b353c77846af7437d8f98d3d7e3b56c3\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1609.04836\",\"authors\":[{\"authorId\":\"2844898\",\"name\":\"N. Keskar\"},{\"authorId\":\"2205699\",\"name\":\"D. Mudigere\"},{\"authorId\":\"2784955\",\"name\":\"J. Nocedal\"},{\"authorId\":\"1711231\",\"name\":\"M. Smelyanskiy\"},{\"authorId\":\"144669504\",\"name\":\"P. Tang\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8ec5896b4490c6e127d1718ffc36a3439d84cb81\",\"title\":\"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima\",\"url\":\"https://www.semanticscholar.org/paper/8ec5896b4490c6e127d1718ffc36a3439d84cb81\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Elad Hoffer\"},{\"authorId\":null,\"name\":\"Itay Hubara\"},{\"authorId\":null,\"name\":\"Daniel Soudryn. Train longer\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"generalize better: closing the generalization gap in large batch training of neural networks\",\"url\":\"\",\"venue\":\"Advances in Neural Information Processing Systems 30 (NIPS 2017),\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3252772\",\"name\":\"Soham De\"},{\"authorId\":\"27372090\",\"name\":\"A. Yadav\"},{\"authorId\":\"34734622\",\"name\":\"D. Jacobs\"},{\"authorId\":\"1962083\",\"name\":\"T. Goldstein\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aff6a21f5fadc3f240b4275f6c229ac00d89f420\",\"title\":\"Automated Inference with Adaptive Batches\",\"url\":\"https://www.semanticscholar.org/paper/aff6a21f5fadc3f240b4275f6c229ac00d89f420\",\"venue\":\"AISTATS\",\"year\":2017},{\"arxivId\":\"1605.08101\",\"authors\":[{\"authorId\":\"2418520\",\"name\":\"Nicolas Boumal\"},{\"authorId\":\"101407532\",\"name\":\"P.-A. Absil\"},{\"authorId\":\"2929000\",\"name\":\"C. Cartis\"}],\"doi\":\"10.1093/imanum/drx080\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c26cc911ec12923af30ad485b4c93691a534960d\",\"title\":\"Global rates of convergence for nonconvex optimization on manifolds\",\"url\":\"https://www.semanticscholar.org/paper/c26cc911ec12923af30ad485b4c93691a534960d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50356391\",\"name\":\"Pierre-Antoine Absil\"},{\"authorId\":\"144814747\",\"name\":\"R. Mahony\"},{\"authorId\":\"1707966\",\"name\":\"R. Sepulchre\"}],\"doi\":\"10.1515/9781400830244\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"238176f85df700e0679ad3bacc8b2c5b1114cc58\",\"title\":\"Optimization Algorithms on Matrix Manifolds\",\"url\":\"https://www.semanticscholar.org/paper/238176f85df700e0679ad3bacc8b2c5b1114cc58\",\"venue\":\"\",\"year\":2007},{\"arxivId\":\"1706.08947\",\"authors\":[{\"authorId\":\"3007442\",\"name\":\"Behnam Neyshabur\"},{\"authorId\":\"1798880\",\"name\":\"Srinadh Bhojanapalli\"},{\"authorId\":\"93591671\",\"name\":\"D. McAllester\"},{\"authorId\":\"1706280\",\"name\":\"Nathan Srebro\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d53fb3feeeab07a0d70bf466dd473ec6052ecc07\",\"title\":\"Exploring Generalization in Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/d53fb3feeeab07a0d70bf466dd473ec6052ecc07\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1611.03530\",\"authors\":[{\"authorId\":\"40313479\",\"name\":\"C. Zhang\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1775622\",\"name\":\"M. Hardt\"},{\"authorId\":\"9229182\",\"name\":\"B. Recht\"},{\"authorId\":\"1689108\",\"name\":\"Oriol Vinyals\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54ddb00fa691728944fd8becea90a373d21597cf\",\"title\":\"Understanding deep learning requires rethinking generalization\",\"url\":\"https://www.semanticscholar.org/paper/54ddb00fa691728944fd8becea90a373d21597cf\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145233522\",\"name\":\"L. Wu\"},{\"authorId\":\"144905350\",\"name\":\"Chao Ma\"},{\"authorId\":\"1789499\",\"name\":\"E. Weinan\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"c77c38a773fd2a1e04c034b4f835c375886e9fd0\",\"title\":\"How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c77c38a773fd2a1e04c034b4f835c375886e9fd0\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1703.04933\",\"authors\":[{\"authorId\":\"46573521\",\"name\":\"Laurent Dinh\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"1751569\",\"name\":\"S. Bengio\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58123025178256279bb060ca5da971b62bc329ee\",\"title\":\"Sharp Minima Can Generalize For Deep Nets\",\"url\":\"https://www.semanticscholar.org/paper/58123025178256279bb060ca5da971b62bc329ee\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1705.08741\",\"authors\":[{\"authorId\":\"40555034\",\"name\":\"E. Hoffer\"},{\"authorId\":\"2477463\",\"name\":\"Itay Hubara\"},{\"authorId\":\"1912398\",\"name\":\"Daniel Soudry\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8501e330d78391f4e690886a8eb8fac867704ea6\",\"title\":\"Train longer, generalize better: closing the generalization gap in large batch training of neural networks\",\"url\":\"https://www.semanticscholar.org/paper/8501e330d78391f4e690886a8eb8fac867704ea6\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1412.0233\",\"authors\":[{\"authorId\":\"3216141\",\"name\":\"A. Choromanska\"},{\"authorId\":\"39713408\",\"name\":\"Mikael Henaff\"},{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"3317660\",\"name\":\"G. B. Arous\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba\",\"title\":\"The Loss Surfaces of Multilayer Networks\",\"url\":\"https://www.semanticscholar.org/paper/ad8a12a19e74d9788f8fe92f5c0dfea7b6a52aba\",\"venue\":\"AISTATS\",\"year\":2015},{\"arxivId\":\"1812.03271\",\"authors\":[{\"authorId\":\"40625427\",\"name\":\"X. Yuan\"},{\"authorId\":\"143646284\",\"name\":\"Zheng Feng\"},{\"authorId\":\"143959867\",\"name\":\"M. Norton\"},{\"authorId\":null,\"name\":\"Xiaolin Li\"}],\"doi\":\"10.1609/aaai.v33i01.33011682\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"817e04a56448152e25c0f3742796035e2e7f2f16\",\"title\":\"Generalized Batch Normalization: Towards Accelerating Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/817e04a56448152e25c0f3742796035e2e7f2f16\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yann LeCun\"},{\"authorId\":null,\"name\":\"Leon Bottou\"},{\"authorId\":null,\"name\":\"Yoshua Bengio\"},{\"authorId\":null,\"name\":\"Patrick Haffner. Gradient-based learning applied to docume recognition\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Proceedings of the IEEE\",\"url\":\"\",\"venue\":\"86(11):2278 \\u2013 2324,\",\"year\":1998},{\"arxivId\":\"1712.09913\",\"authors\":[{\"authorId\":\"79482877\",\"name\":\"Hao Li\"},{\"authorId\":\"144897102\",\"name\":\"Zheng Xu\"},{\"authorId\":\"2189083\",\"name\":\"G. Taylor\"},{\"authorId\":\"1962083\",\"name\":\"T. Goldstein\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"6baca6351dc55baac44f0416e74a7e0ba2bfd03e\",\"title\":\"Visualizing the Loss Landscape of Neural Nets\",\"url\":\"https://www.semanticscholar.org/paper/6baca6351dc55baac44f0416e74a7e0ba2bfd03e\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1409.1556\",\"authors\":[{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb42cf88027de515750f230b23b1a057dc782108\",\"title\":\"Very Deep Convolutional Networks for Large-Scale Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/eb42cf88027de515750f230b23b1a057dc782108\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1811.03962\",\"authors\":[{\"authorId\":\"1388725932\",\"name\":\"Zeyuan Allen-Zhu\"},{\"authorId\":\"2205248\",\"name\":\"Y. Li\"},{\"authorId\":\"145328196\",\"name\":\"Zhao Song\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"42ec3db12a2e4628885451b13035c2e975220a25\",\"title\":\"A Convergence Theory for Deep Learning via Over-Parameterization\",\"url\":\"https://www.semanticscholar.org/paper/42ec3db12a2e4628885451b13035c2e975220a25\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1811.03804\",\"authors\":[{\"authorId\":\"145697585\",\"name\":\"S. Du\"},{\"authorId\":\"2421201\",\"name\":\"J. Lee\"},{\"authorId\":\"46382587\",\"name\":\"H. Li\"},{\"authorId\":\"49680905\",\"name\":\"L. Wang\"},{\"authorId\":\"22226408\",\"name\":\"Xiyu Zhai\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"03e7e8663c69e691be6b6403b1eb1bbf593d31f2\",\"title\":\"Gradient Descent Finds Global Minima of Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/03e7e8663c69e691be6b6403b1eb1bbf593d31f2\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1809.07402\",\"authors\":[{\"authorId\":\"49527968\",\"name\":\"H. Wang\"},{\"authorId\":\"2844898\",\"name\":\"N. Keskar\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2169acce9014fd4ce462da494b71a3d2ef1c8191\",\"title\":\"Identifying Generalization Properties in Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/2169acce9014fd4ce462da494b71a3d2ef1c8191\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1611.01838\",\"authors\":[{\"authorId\":\"143976382\",\"name\":\"Pratik Chaudhari\"},{\"authorId\":\"3216141\",\"name\":\"A. Choromanska\"},{\"authorId\":\"1715959\",\"name\":\"Stefano Soatto\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"3066793\",\"name\":\"Carlo Baldassi\"},{\"authorId\":\"1721812\",\"name\":\"C. Borgs\"},{\"authorId\":\"1695997\",\"name\":\"J. Chayes\"},{\"authorId\":\"2246570\",\"name\":\"Levent Sagun\"},{\"authorId\":\"1719010\",\"name\":\"R. Zecchina\"}],\"doi\":\"10.1088/1742-5468/AB39D9\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6583fe9c9dc52bb129aff4cefc60519349f3b4c\",\"title\":\"Entropy-SGD: Biasing Gradient Descent Into Wide Valleys\",\"url\":\"https://www.semanticscholar.org/paper/b6583fe9c9dc52bb129aff4cefc60519349f3b4c\",\"venue\":\"ICLR\",\"year\":2017}],\"title\":\"BN-invariant Sharpness Regularizes the Training Model to Better Generalization\",\"topics\":[{\"topic\":\"Acutance\",\"topicId\":\"70745\",\"url\":\"https://www.semanticscholar.org/topic/70745\"}],\"url\":\"https://www.semanticscholar.org/paper/5b9c93df5fc8db6b709f195bcecd8e6e1018d558\",\"venue\":\"IJCAI\",\"year\":2019}\n"