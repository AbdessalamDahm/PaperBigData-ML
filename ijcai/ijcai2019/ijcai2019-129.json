"{\"abstract\":\"Given an arbitrary face image and an arbitrary speech clip, the proposed work attempts to generating the talking face video with accurate lip synchronization while maintaining smooth transition of both lip and facial movement over the entire video clip. Existing works either do not consider temporal dependency on face images across different video frames thus easily yielding noticeable/abrupt facial and lip movement or are only limited to the generation of talking face video for a specific person thus lacking generalization capacity. We propose a novel conditional video generation network where the audio input is treated as a condition for the recurrent adversarial network such that temporal dependency is incorporated to realize smooth transition for the lip and facial movement. In addition, we deploy a multi-task adversarial training scheme in the context of video generation to improve both photo-realism and the accuracy for lip synchronization. Finally, based on the phoneme distribution information extracted from the audio clip, we develop a sample selection method that effectively reduces the size of the training dataset without sacrificing the quality of the generated video. Extensive experiments on both controlled and uncontrolled datasets demonstrate the superiority of the proposed approach in terms of visual quality, lip sync accuracy, and smooth transition of lip and facial movement, as compared to the state-of-the-art.\",\"arxivId\":\"1804.04786\",\"authors\":[{\"authorId\":\"115504645\",\"name\":\"Y. Song\",\"url\":\"https://www.semanticscholar.org/author/115504645\"},{\"authorId\":\"145532978\",\"name\":\"Jingwen Zhu\",\"url\":\"https://www.semanticscholar.org/author/145532978\"},{\"authorId\":\"49620929\",\"name\":\"Dawei Li\",\"url\":\"https://www.semanticscholar.org/author/49620929\"},{\"authorId\":\"39635018\",\"name\":\"Xiaolong Wang\",\"url\":\"https://www.semanticscholar.org/author/39635018\"},{\"authorId\":\"144117139\",\"name\":\"Hairong Qi\",\"url\":\"https://www.semanticscholar.org/author/144117139\"}],\"citationVelocity\":20,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"144746444\",\"name\":\"H. Bischof\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"},{\"authorId\":\"40454588\",\"name\":\"J. Frahm\"}],\"doi\":\"10.1007/978-3-030-58577-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"title\":\"Computer Vision \\u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\\u201328, 2020, Proceedings, Part XXX\",\"url\":\"https://www.semanticscholar.org/paper/08a578d7f7f3d0edf46470e33f92e2335d19b70b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145798291\",\"name\":\"Hang Zhou\"},{\"authorId\":\"40457380\",\"name\":\"Yu Liu\"},{\"authorId\":\"3243969\",\"name\":\"Ziwei Liu\"},{\"authorId\":\"144389949\",\"name\":\"Ping Luo\"},{\"authorId\":\"31843833\",\"name\":\"Xiaogang Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9c79a853f7beb5726506c358b5fd1a1b7ce12aac\",\"title\":\"Word ID Person ID pid information audio wid information visual wid information wid information wid adversarial against pid pid adversarial against wid wid\",\"url\":\"https://www.semanticscholar.org/paper/9c79a853f7beb5726506c358b5fd1a1b7ce12aac\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1905.03820\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1109/CVPR.2019.00802\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a0852cd9a026bc90168fa85fa422cb0e48f98394\",\"title\":\"Hierarchical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss\",\"url\":\"https://www.semanticscholar.org/paper/a0852cd9a026bc90168fa85fa422cb0e48f98394\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1905.06860\",\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"},{\"authorId\":\"2785748\",\"name\":\"B. Theobald\"},{\"authorId\":\"9967657\",\"name\":\"Justin F. Binder\"},{\"authorId\":\"48095899\",\"name\":\"Gabriele Fanelli\"},{\"authorId\":\"113131955\",\"name\":\"P. Dixon\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"},{\"authorId\":\"2246174\",\"name\":\"T. Weise\"},{\"authorId\":\"123576773\",\"name\":\"Sachin Kajareker\"}],\"doi\":\"10.1145/3340555.3353745\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d599e1b5deb2c0a95438960ce0fe7b62723cb16\",\"title\":\"Speaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models\",\"url\":\"https://www.semanticscholar.org/paper/0d599e1b5deb2c0a95438960ce0fe7b62723cb16\",\"venue\":\"ICMI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7572778\",\"name\":\"Gizem Orta\\u00e7\"},{\"authorId\":\"119926208\",\"name\":\"Zeliha Do\\u011fan\"},{\"authorId\":\"1391845226\",\"name\":\"Zeynep Orman\"},{\"authorId\":\"1401957451\",\"name\":\"R\\u00fcya \\u015eaml\\u0131\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"60c99c78d7496f1d80128e7d804439040dd4bd7f\",\"title\":\"Baby Face Generation with Generative Adversarial Neural Networks: A Case Study\",\"url\":\"https://www.semanticscholar.org/paper/60c99c78d7496f1d80128e7d804439040dd4bd7f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2002.10137\",\"authors\":[{\"authorId\":\"50488872\",\"name\":\"R. Yi\"},{\"authorId\":\"7818093\",\"name\":\"Zipeng Ye\"},{\"authorId\":\"2938279\",\"name\":\"J. Zhang\"},{\"authorId\":\"1679542\",\"name\":\"H. Bao\"},{\"authorId\":\"46398687\",\"name\":\"Yongjin Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"bec253e82076dc363b8fd72d5c8fadf8f5b7e475\",\"title\":\"Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose\",\"url\":\"https://www.semanticscholar.org/paper/bec253e82076dc363b8fd72d5c8fadf8f5b7e475\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39906330\",\"name\":\"Sefik Emre Eskimez\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"},{\"authorId\":\"72028302\",\"name\":\"Zhiyao Duan\"}],\"doi\":\"10.1109/ICASSP40776.2020.9054103\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"db15b6799d7a09fcd41388d9e55d9c267e454965\",\"title\":\"End-To-End Generation of Talking Faces from Noisy Speech\",\"url\":\"https://www.semanticscholar.org/paper/db15b6799d7a09fcd41388d9e55d9c267e454965\",\"venue\":\"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2020},{\"arxivId\":\"2005.12318\",\"authors\":[{\"authorId\":\"8524712\",\"name\":\"Sanjana Sinha\"},{\"authorId\":\"18961663\",\"name\":\"S. Biswas\"},{\"authorId\":\"3262263\",\"name\":\"B. Bhowmick\"}],\"doi\":\"10.1109/IJCNN48605.2020.9206665\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"24b3701937bd05ef6377f14682f4f67b40d09b50\",\"title\":\"Identity-Preserving Realistic Talking Face Generation\",\"url\":\"https://www.semanticscholar.org/paper/24b3701937bd05ef6377f14682f4f67b40d09b50\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":\"1809.08927\",\"authors\":[{\"authorId\":\"144610333\",\"name\":\"Jing Han\"},{\"authorId\":\"1742291\",\"name\":\"Zixing Zhang\"},{\"authorId\":\"49249279\",\"name\":\"Nicholas Cummins\"},{\"authorId\":\"9460079\",\"name\":\"B. Schuller\"}],\"doi\":\"10.1109/MCI.2019.2901088\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19ea8cbb219795f985b60fc6e6fd9ae76f5ccbc0\",\"title\":\"Adversarial Training in Affective Computing and Sentiment Analysis: Recent Advances and Perspectives [Review Article]\",\"url\":\"https://www.semanticscholar.org/paper/19ea8cbb219795f985b60fc6e6fd9ae76f5ccbc0\",\"venue\":\"IEEE Computational Intelligence Magazine\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49107901\",\"name\":\"Albert Pumarola\"},{\"authorId\":\"144003484\",\"name\":\"A. Agudo\"},{\"authorId\":\"145358414\",\"name\":\"A. Mart\\u00ednez\"},{\"authorId\":\"1791054\",\"name\":\"A. Sanfeliu\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"}],\"doi\":\"10.1007/s11263-019-01210-3\",\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"9c41d7cd81c3af37ac68a4f348122aead4f53962\",\"title\":\"GANimation: One-Shot Anatomically Consistent Facial Animation\",\"url\":\"https://www.semanticscholar.org/paper/9c41d7cd81c3af37ac68a4f348122aead4f53962\",\"venue\":\"International Journal of Computer Vision\",\"year\":2019},{\"arxivId\":\"2004.09169\",\"authors\":[{\"authorId\":\"1643682503\",\"name\":\"Andrei-Timotei Ardelean\"},{\"authorId\":\"1567393666\",\"name\":\"Lucian M. Sasu\"}],\"doi\":\"10.15837/ijccc.2020.2.3862\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"51c853c4e11365cf2f796da16424fca32904ccce\",\"title\":\"Pose Manipulation with Identity Preservation\",\"url\":\"https://www.semanticscholar.org/paper/51c853c4e11365cf2f796da16424fca32904ccce\",\"venue\":\"Int. J. Comput. Commun. Control\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39128291\",\"name\":\"Teng Zhang\"},{\"authorId\":\"51492180\",\"name\":\"Lirui Deng\"},{\"authorId\":\"47059067\",\"name\":\"L. Zhang\"},{\"authorId\":\"3203437\",\"name\":\"Xianglei Dang\"}],\"doi\":\"10.1109/CCET50901.2020.9213159\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"42693caf4e6c7480295ad88094fdc8b30509305b\",\"title\":\"Deep Learning in Face Synthesis: A Survey on Deepfakes\",\"url\":\"https://www.semanticscholar.org/paper/42693caf4e6c7480295ad88094fdc8b30509305b\",\"venue\":\"2020 IEEE 3rd International Conference on Computer and Communication Engineering Technology (CCET)\",\"year\":2020},{\"arxivId\":\"2011.15126\",\"authors\":[{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"},{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"88f8974f6f9bc2febfb88ed3ffe12dfc9c441808\",\"title\":\"One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing\",\"url\":\"https://www.semanticscholar.org/paper/88f8974f6f9bc2febfb88ed3ffe12dfc9c441808\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39906330\",\"name\":\"Sefik Emre Eskimez\"},{\"authorId\":\"93506431\",\"name\":\"Y. Zhang\"},{\"authorId\":\"72028302\",\"name\":\"Zhiyao Duan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"8fc195090bcfe8de6d075681227a677cd64d2e0b\",\"title\":\"Speech Driven Talking Face Generation from a Single Image and an Emotion Condition\",\"url\":\"https://www.semanticscholar.org/paper/8fc195090bcfe8de6d075681227a677cd64d2e0b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.11138\",\"authors\":[{\"authorId\":\"2934352\",\"name\":\"Yisroel Mirsky\"},{\"authorId\":\"49627181\",\"name\":\"W. Lee\"}],\"doi\":\"10.1145/3425780\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"92b4c8deecee703569b9e909dfb88aa70e691219\",\"title\":\"The Creation and Detection of Deepfakes: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/92b4c8deecee703569b9e909dfb88aa70e691219\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47932716\",\"name\":\"X. Huang\"},{\"authorId\":\"151474565\",\"name\":\"Mingjie Wang\"},{\"authorId\":\"1473876432\",\"name\":\"M. Gong\"}],\"doi\":\"10.1007/S00371-020-01982-7\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ef6467ddba06210c7cd0677955234e2e850274f0\",\"title\":\"Fine-grained talking face generation with video reinterpretation\",\"url\":\"https://www.semanticscholar.org/paper/ef6467ddba06210c7cd0677955234e2e850274f0\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47785129\",\"name\":\"Lingyun Yu\"},{\"authorId\":\"119883542\",\"name\":\"J. Yu\"},{\"authorId\":\"40177644\",\"name\":\"Q. Ling\"}],\"doi\":\"10.1109/ICDM.2019.00089\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c93fb94cbc6e5cc67851c9436127f56ab7e8725\",\"title\":\"Mining Audio, Text and Visual Information for Talking Face Generation\",\"url\":\"https://www.semanticscholar.org/paper/6c93fb94cbc6e5cc67851c9436127f56ab7e8725\",\"venue\":\"2019 IEEE International Conference on Data Mining (ICDM)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50878990\",\"name\":\"Joni O. Salminen\"},{\"authorId\":\"2734912\",\"name\":\"Joni Salminen\"},{\"authorId\":\"122155850\",\"name\":\"Soon-Gyo Jung\"},{\"authorId\":\"2004621406\",\"name\":\"A. Kamel\"},{\"authorId\":\"152527658\",\"name\":\"Jo\\u00e3o Santos\"},{\"authorId\":\"145604861\",\"name\":\"Bernard J. Jansen\"}],\"doi\":\"10.1080/0144929x.2020.1838610\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e3f8e62510d8adcb9c1bc88227342028b6dfa7\",\"title\":\"Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas\",\"url\":\"https://www.semanticscholar.org/paper/69e3f8e62510d8adcb9c1bc88227342028b6dfa7\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1452343340\",\"name\":\"Talha Iqbal\"},{\"authorId\":\"47318412\",\"name\":\"Hazrat Ali\"}],\"doi\":\"10.1007/978-3-030-40977-7_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ed54c7dd0c81deebbe41691ebe7d74edb8dd6fe2\",\"title\":\"Generative Adversarial Network and Retinal Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/ed54c7dd0c81deebbe41691ebe7d74edb8dd6fe2\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1516225025\",\"name\":\"Na Liu\"},{\"authorId\":\"153295714\",\"name\":\"Tao Zhou\"},{\"authorId\":\"152994876\",\"name\":\"Yunfeng Ji\"},{\"authorId\":\"47122744\",\"name\":\"Ziyi Zhao\"},{\"authorId\":\"9402234\",\"name\":\"L. Wan\"}],\"doi\":\"10.1016/j.patcog.2020.107231\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"52b706ad10b77532f8d2b3ca2e3e6ab5601f15b5\",\"title\":\"Synthesizing Talking Faces from Text and Audio: An Autoencoder and Sequence-to-Sequence Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/52b706ad10b77532f8d2b3ca2e3e6ab5601f15b5\",\"venue\":\"Pattern Recognit.\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143790066\",\"name\":\"Dipanjan Das\"},{\"authorId\":\"18961663\",\"name\":\"S. Biswas\"},{\"authorId\":\"8524712\",\"name\":\"Sanjana Sinha\"},{\"authorId\":\"3262263\",\"name\":\"B. Bhowmick\"}],\"doi\":\"10.1007/978-3-030-58577-8_25\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c748ae0688a7ba0b6dbc33436a0fe76fa30786c2\",\"title\":\"Speech-Driven Facial Animation Using Cascaded GANs for Learning of Motion and Texture\",\"url\":\"https://www.semanticscholar.org/paper/c748ae0688a7ba0b6dbc33436a0fe76fa30786c2\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2009.05784\",\"authors\":[{\"authorId\":\"8790824\",\"name\":\"Weicong Chen\"},{\"authorId\":\"1391138969\",\"name\":\"Xu Tan\"},{\"authorId\":\"2794096\",\"name\":\"Yingce Xia\"},{\"authorId\":\"82620854\",\"name\":\"Tao Qin\"},{\"authorId\":null,\"name\":\"Yu Wang\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":\"10.1145/3394171.3413623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4dabcac75e2a6e52b2ec915eef1fd74bd5f677f\",\"title\":\"DualLip: A System for Joint Lip Reading and Generation\",\"url\":\"https://www.semanticscholar.org/paper/a4dabcac75e2a6e52b2ec915eef1fd74bd5f677f\",\"venue\":\"ACM Multimedia\",\"year\":2020},{\"arxivId\":\"2011.03530\",\"authors\":[{\"authorId\":\"7179962\",\"name\":\"Yan-qi Yang\"},{\"authorId\":\"3144580\",\"name\":\"Brendan Shillingford\"},{\"authorId\":\"3365565\",\"name\":\"Yannis M. Assael\"},{\"authorId\":\"5116578\",\"name\":\"Miaosen Wang\"},{\"authorId\":\"2000911990\",\"name\":\"Wendi Liu\"},{\"authorId\":\"1816749492\",\"name\":\"Yutian Chen\"},{\"authorId\":\"80266998\",\"name\":\"Y. Zhang\"},{\"authorId\":\"1413718981\",\"name\":\"Eren Sezener\"},{\"authorId\":\"38712164\",\"name\":\"Luis C. Cobo\"},{\"authorId\":\"1715051\",\"name\":\"Misha Denil\"},{\"authorId\":\"3152281\",\"name\":\"Y. Aytar\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d99611d2b8d1147c72e1e311b50246f0a2d3f0eb\",\"title\":\"Large-scale multilingual audio visual dubbing\",\"url\":\"https://www.semanticscholar.org/paper/d99611d2b8d1147c72e1e311b50246f0a2d3f0eb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.08547\",\"authors\":[{\"authorId\":\"152875073\",\"name\":\"Lele Chen\"},{\"authorId\":\"4942685\",\"name\":\"G. Cui\"},{\"authorId\":\"9585133\",\"name\":\"Celong Liu\"},{\"authorId\":\"49969600\",\"name\":\"Z. Li\"},{\"authorId\":\"16132631\",\"name\":\"Z. Kou\"},{\"authorId\":\"92413558\",\"name\":\"Yi Xu\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-58545-7_3\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"f7e40f631825034e474be5640e6ac9ed7a6917c7\",\"title\":\"Talking-head Generation with Rhythmic Head Motion\",\"url\":\"https://www.semanticscholar.org/paper/f7e40f631825034e474be5640e6ac9ed7a6917c7\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"1909.12780\",\"authors\":[{\"authorId\":\"41016678\",\"name\":\"Givi Meishvili\"},{\"authorId\":\"5641221\",\"name\":\"S. Jenni\"},{\"authorId\":\"144707901\",\"name\":\"P. Favaro\"}],\"doi\":\"10.1109/cvpr42600.2020.00144\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b073e91f84ab59e482c2f1e22918f46ef606a531\",\"title\":\"Learning to Have an Ear for Face Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/b073e91f84ab59e482c2f1e22918f46ef606a531\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2034345781\",\"name\":\"Jichen Wu\"},{\"authorId\":\"15396571\",\"name\":\"M. Lamers\"},{\"authorId\":\"3018705\",\"name\":\"W. Kowalczyk\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bd0721ecfa54e22d0a8ea7c9e8a993cb183b9f0b\",\"title\":\"Being Creative: A Cross-Domain Mapping Network\",\"url\":\"https://www.semanticscholar.org/paper/bd0721ecfa54e22d0a8ea7c9e8a993cb183b9f0b\",\"venue\":\"ICCC\",\"year\":2020},{\"arxivId\":\"2008.02793\",\"authors\":[{\"authorId\":\"1596793949\",\"name\":\"M. Liu\"},{\"authorId\":\"47932904\",\"name\":\"Xun Huang\"},{\"authorId\":\"150167366\",\"name\":\"Jiahui Yu\"},{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"36508529\",\"name\":\"Arun Mallya\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"555e7029e8fd3b759300edfd2a4b7dfa2fd34a48\",\"title\":\"Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications\",\"url\":\"https://www.semanticscholar.org/paper/555e7029e8fd3b759300edfd2a4b7dfa2fd34a48\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.10688\",\"authors\":[{\"authorId\":\"8073409\",\"name\":\"Xin-Wei Yao\"},{\"authorId\":\"2416503\",\"name\":\"Ohad Fried\"},{\"authorId\":\"1399047905\",\"name\":\"Kayvon Fatahalian\"},{\"authorId\":\"1820412\",\"name\":\"M. Agrawala\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7984d3ee29eeb4ef9b0dc5fa6ad780bdca14e4b\",\"title\":\"Iterative Text-based Editing of Talking-heads Using Neural Retargeting\",\"url\":\"https://www.semanticscholar.org/paper/f7984d3ee29eeb4ef9b0dc5fa6ad780bdca14e4b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2009454032\",\"name\":\"Kaisiyuan Wang\"},{\"authorId\":\"123182910\",\"name\":\"Qianyi Wu\"},{\"authorId\":\"8785343\",\"name\":\"Linsen Song\"},{\"authorId\":\"51126032\",\"name\":\"Zhuoqian Yang\"},{\"authorId\":\"3096434\",\"name\":\"W. Wu\"},{\"authorId\":\"2005026904\",\"name\":\"Chen Qian\"},{\"authorId\":\"50361997\",\"name\":\"R. He\"},{\"authorId\":\"145858545\",\"name\":\"Y. Qiao\"},{\"authorId\":\"1717179\",\"name\":\"Chen Change Loy\"}],\"doi\":\"10.1007/978-3-030-58589-1_42\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ce752f69e167db27d4c186f5dfcd064e9b6e7f3b\",\"title\":\"MEAD: A Large-Scale Audio-Visual Dataset for Emotional Talking-Face Generation\",\"url\":\"https://www.semanticscholar.org/paper/ce752f69e167db27d4c186f5dfcd064e9b6e7f3b\",\"venue\":\"ECCV\",\"year\":2020},{\"arxivId\":\"2001.00179\",\"authors\":[{\"authorId\":\"1708502\",\"name\":\"R. Tolosana\"},{\"authorId\":\"1402712530\",\"name\":\"R. Vera-Rodr\\u00edguez\"},{\"authorId\":\"1701431\",\"name\":\"Julian Fierrez\"},{\"authorId\":\"144083995\",\"name\":\"A. Morales\"},{\"authorId\":\"1397258551\",\"name\":\"J. Ortega-Garcia\"}],\"doi\":\"10.1016/j.inffus.2020.06.014\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"96bcd003424aa4b2f9d6e8ade013a3a4293fecf5\",\"title\":\"DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection\",\"url\":\"https://www.semanticscholar.org/paper/96bcd003424aa4b2f9d6e8ade013a3a4293fecf5\",\"venue\":\"Inf. Fusion\",\"year\":2020},{\"arxivId\":\"1904.04571\",\"authors\":[{\"authorId\":\"49107901\",\"name\":\"Albert Pumarola\"},{\"authorId\":\"145120381\",\"name\":\"Jordi Sanchez\"},{\"authorId\":\"10682704\",\"name\":\"Gary P. T. Choi\"},{\"authorId\":\"49743313\",\"name\":\"A. Sanfeliu\"},{\"authorId\":\"1397181875\",\"name\":\"F. Moreno-Noguer\"}],\"doi\":\"10.1109/ICCV.2019.00233\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"08a14770d60d502020732e43cdf348b0a27bfb21\",\"title\":\"3DPeople: Modeling the Geometry of Dressed Humans\",\"url\":\"https://www.semanticscholar.org/paper/08a14770d60d502020732e43cdf348b0a27bfb21\",\"venue\":\"2019 IEEE/CVF International Conference on Computer Vision (ICCV)\",\"year\":2019},{\"arxivId\":\"2011.10727\",\"authors\":[{\"authorId\":\"49375545\",\"name\":\"R. Yadav\"},{\"authorId\":\"50847752\",\"name\":\"Ashish Sardana\"},{\"authorId\":\"145460361\",\"name\":\"Vinay P. Namboodiri\"},{\"authorId\":\"1893306\",\"name\":\"Rajesh M. Hegde\"}],\"doi\":\"10.21437/interspeech.2020-1823\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"411cb36c01d780524bf022d342f41b2db96b2bca\",\"title\":\"Stochastic Talking Face Generation Using Latent Distribution Matching\",\"url\":\"https://www.semanticscholar.org/paper/411cb36c01d780524bf022d342f41b2db96b2bca\",\"venue\":\"INTERSPEECH\",\"year\":2020},{\"arxivId\":\"2008.05023\",\"authors\":[{\"authorId\":\"32774629\",\"name\":\"A. Richard\"},{\"authorId\":\"98658818\",\"name\":\"C. Lea\"},{\"authorId\":\"2863531\",\"name\":\"Shugao Ma\"},{\"authorId\":\"145689714\",\"name\":\"Juergen Gall\"},{\"authorId\":\"143867160\",\"name\":\"F. Torre\"},{\"authorId\":\"1774867\",\"name\":\"Yaser Sheikh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66d8edc6dc19550eff7a0ea930cdd431d6ed1c6a\",\"title\":\"Audio- and Gaze-driven Facial Animation of Codec Avatars\",\"url\":\"https://www.semanticscholar.org/paper/66d8edc6dc19550eff7a0ea930cdd431d6ed1c6a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.03201\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"4942685\",\"name\":\"G. Cui\"},{\"authorId\":\"16132631\",\"name\":\"Z. Kou\"},{\"authorId\":\"2743695\",\"name\":\"Haitian Zheng\"},{\"authorId\":\"100887531\",\"name\":\"Chenliang Xu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b63a9dc18baf645b4766b2b2ec8461c2c843275a\",\"title\":\"What comprises a good talking-head video generation?: A Survey and Benchmark\",\"url\":\"https://www.semanticscholar.org/paper/b63a9dc18baf645b4766b2b2ec8461c2c843275a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1807.07860\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"40457380\",\"name\":\"Y. Liu\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33019299\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1816f98e2a4dd54690c2689cf529699d8843e847\",\"title\":\"Talking Face Generation by Adversarially Disentangled Audio-Visual Representation\",\"url\":\"https://www.semanticscholar.org/paper/1816f98e2a4dd54690c2689cf529699d8843e847\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"2011.01114\",\"authors\":[{\"authorId\":\"73771369\",\"name\":\"Prateek Manocha\"},{\"authorId\":\"46401518\",\"name\":\"Prithwijit Guha\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"6873eb283ff1349db303dd79cc4bbb20fea99296\",\"title\":\"Facial Keypoint Sequence Generation from Audio\",\"url\":\"https://www.semanticscholar.org/paper/6873eb283ff1349db303dd79cc4bbb20fea99296\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50488872\",\"name\":\"R. Yi\"},{\"authorId\":\"7818093\",\"name\":\"Zipeng Ye\"},{\"authorId\":\"2938279\",\"name\":\"J. Zhang\"},{\"authorId\":\"1679542\",\"name\":\"H. Bao\"},{\"authorId\":\"46398687\",\"name\":\"Yongjin Liu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d76d7102edb5668cf425af1e806375b5a01dab33\",\"title\":\"Audio-driven Talking Face Video Generation with Natural Head Pose\",\"url\":\"https://www.semanticscholar.org/paper/d76d7102edb5668cf425af1e806375b5a01dab33\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.08261\",\"authors\":[{\"authorId\":\"50383147\",\"name\":\"M. Doukas\"},{\"authorId\":\"1379747201\",\"name\":\"S. Zafeiriou\"},{\"authorId\":\"1790503\",\"name\":\"V. Sharmanska\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7b9302212440e9be73808d0d3b802c523b4ca1d1\",\"title\":\"HeadGAN: Video-and-Audio-Driven Talking Head Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/7b9302212440e9be73808d0d3b802c523b4ca1d1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2005.13616\",\"authors\":[{\"authorId\":\"1694508\",\"name\":\"Ahmed Hussen Abdelaziz\"},{\"authorId\":\"115268571\",\"name\":\"Barry-John Theobald\"},{\"authorId\":\"145251024\",\"name\":\"P. Dixon\"},{\"authorId\":\"2497993\",\"name\":\"Reinhard Knothe\"},{\"authorId\":\"3301859\",\"name\":\"N. Apostoloff\"},{\"authorId\":\"123576773\",\"name\":\"Sachin Kajareker\"}],\"doi\":\"10.1145/3382507.3418840\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"title\":\"Modality Dropout for Improved Performance-driven Talking Faces\",\"url\":\"https://www.semanticscholar.org/paper/917385fe54184bf03d9fa170f8b79a7c12fe76c8\",\"venue\":\"ICMI\",\"year\":2020},{\"arxivId\":\"2006.15327\",\"authors\":[{\"authorId\":\"48319922\",\"name\":\"A. Bar\"},{\"authorId\":\"46796686\",\"name\":\"Roei Herzig\"},{\"authorId\":\"122024152\",\"name\":\"Xiaolong Wang\"},{\"authorId\":\"1732280\",\"name\":\"Gal Chechik\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1786843\",\"name\":\"A. Globerson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cdc4ae0adcff94e9ea4591f25b81f88c93cb4fbe\",\"title\":\"Compositional Video Synthesis with Action Graphs\",\"url\":\"https://www.semanticscholar.org/paper/cdc4ae0adcff94e9ea4591f25b81f88c93cb4fbe\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.12992\",\"authors\":[{\"authorId\":\"32025363\",\"name\":\"Yang Zhou\"},{\"authorId\":\"40580714\",\"name\":\"Dingzeyu Li\"},{\"authorId\":\"1399909799\",\"name\":\"Xintong Han\"},{\"authorId\":\"2808670\",\"name\":\"E. Kalogerakis\"},{\"authorId\":\"2177801\",\"name\":\"E. Shechtman\"},{\"authorId\":\"80452718\",\"name\":\"J. Echevarria\"}],\"doi\":\"10.1145/3414685.3417774\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"739497ec657d2c1e6ed7eb424951e0affe117be4\",\"title\":\"MakeItTalk: Speaker-Aware Talking Head Animation\",\"url\":\"https://www.semanticscholar.org/paper/739497ec657d2c1e6ed7eb424951e0affe117be4\",\"venue\":\"ACM Trans. Graph.\",\"year\":2020}],\"corpusId\":4867611,\"doi\":\"10.24963/ijcai.2019/129\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":8,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"7c223baccf679fc212dd4d9ecbebf30b7a8616af\",\"references\":[{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143636342\",\"name\":\"Sarah L. Taylor\"},{\"authorId\":\"2066626\",\"name\":\"T. Kim\"},{\"authorId\":\"1740159\",\"name\":\"Yisong Yue\"},{\"authorId\":\"30303590\",\"name\":\"M. Mahler\"},{\"authorId\":\"1988242\",\"name\":\"James Krahe\"},{\"authorId\":\"36969558\",\"name\":\"Anastasio Garcia Rodriguez\"},{\"authorId\":\"1788773\",\"name\":\"J. Hodgins\"},{\"authorId\":\"1711695\",\"name\":\"I. Matthews\"}],\"doi\":\"10.1145/3072959.3073699\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"title\":\"A deep learning approach for generalized speech animation\",\"url\":\"https://www.semanticscholar.org/paper/cc63b9cf84b1fb0b3eca84372919f74a40b7c132\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":\"1707.04993\",\"authors\":[{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00165\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"title\":\"MoCoGAN: Decomposing Motion and Content for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":\"1605.05396\",\"authors\":[{\"authorId\":\"144828948\",\"name\":\"S. Reed\"},{\"authorId\":\"2893664\",\"name\":\"Zeynep Akata\"},{\"authorId\":\"3084614\",\"name\":\"Xinchen Yan\"},{\"authorId\":\"2876316\",\"name\":\"L. Logeswaran\"},{\"authorId\":\"48920094\",\"name\":\"B. Schiele\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"6c7f040a150abf21dbcefe1f22e0f98fa184f41a\",\"title\":\"Generative Adversarial Text to Image Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/6c7f040a150abf21dbcefe1f22e0f98fa184f41a\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2976930\",\"name\":\"Tero Karras\"},{\"authorId\":\"1761103\",\"name\":\"Timo Aila\"},{\"authorId\":\"36436218\",\"name\":\"S. Laine\"},{\"authorId\":\"3468872\",\"name\":\"Antti Herva\"},{\"authorId\":\"49244945\",\"name\":\"J. Lehtinen\"}],\"doi\":\"10.1145/3072959.3073658\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"title\":\"Audio-driven facial animation by joint end-to-end learning of pose and emotion\",\"url\":\"https://www.semanticscholar.org/paper/95b803d07c37e8349bd7b1318367d8237c76cbc0\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tero Karras\"},{\"authorId\":null,\"name\":\"Timo Aila\"},{\"authorId\":null,\"name\":\"Samuli Laine\"},{\"authorId\":null,\"name\":\"Antti Herva\"},{\"authorId\":null,\"name\":\"Jaakko Lehtinen. Audio-driven facial animation by joint e pose\"},{\"authorId\":null,\"name\":\"emotion\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ACM Transactions on Graphics (TOG)\",\"url\":\"\",\"venue\":\"36(4):94,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1803.10404\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"48458657\",\"name\":\"Zhiheng Li\"},{\"authorId\":\"4053196\",\"name\":\"Ross K. Maddox\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1007/978-3-030-01234-2_32\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"title\":\"Lip Movements Generation at a Glance\",\"url\":\"https://www.semanticscholar.org/paper/d7e12f93339dc9a97cc325a4a3e9a13bdffb4988\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153440022\",\"name\":\"Ian J. Goodfellow\"},{\"authorId\":\"1403025868\",\"name\":\"Jean Pouget-Abadie\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"144738865\",\"name\":\"Bing Xu\"},{\"authorId\":\"1393680089\",\"name\":\"David Warde-Farley\"},{\"authorId\":\"1955694\",\"name\":\"Sherjil Ozair\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"title\":\"Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/54e325aee6b2d476bbbb88615ac15e251c6e8214\",\"venue\":\"NIPS\",\"year\":2014},{\"arxivId\":\"1706.00556\",\"authors\":[{\"authorId\":\"144404428\",\"name\":\"Yang Song\"},{\"authorId\":\"47295086\",\"name\":\"Zhifei Zhang\"},{\"authorId\":\"1698645\",\"name\":\"H. Qi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4589d6bbb3186fc001ea2a42ae1ea2718edba915\",\"title\":\"Recursive Cross-Domain Face/Sketch Generation from Limited Facial Parts\",\"url\":\"https://www.semanticscholar.org/paper/4589d6bbb3186fc001ea2a42ae1ea2718edba915\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1703.10593\",\"authors\":[{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"145599603\",\"name\":\"T. Park\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/ICCV.2017.244\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c43d954cf8133e6254499f3d68e45218067e4941\",\"title\":\"Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/c43d954cf8133e6254499f3d68e45218067e4941\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1609.04802\",\"authors\":[{\"authorId\":\"1779917\",\"name\":\"C. Ledig\"},{\"authorId\":\"2073063\",\"name\":\"L. Theis\"},{\"authorId\":\"3108066\",\"name\":\"Ferenc Husz\\u00e1r\"},{\"authorId\":\"79382929\",\"name\":\"J. Caballero\"},{\"authorId\":\"83015038\",\"name\":\"Andrew Aitken\"},{\"authorId\":\"41203992\",\"name\":\"Alykhan Tejani\"},{\"authorId\":\"1853456\",\"name\":\"J. Totz\"},{\"authorId\":\"34627233\",\"name\":\"Zehan Wang\"},{\"authorId\":\"152554375\",\"name\":\"W. Shi\"}],\"doi\":\"10.1109/CVPR.2017.19\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3\",\"title\":\"Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\",\"url\":\"https://www.semanticscholar.org/paper/df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1612.03242\",\"authors\":[{\"authorId\":\"120811666\",\"name\":\"Han Zhang\"},{\"authorId\":\"145017761\",\"name\":\"Tao Xu\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1109/ICCV.2017.629\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921\",\"title\":\"StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1801.01442\",\"authors\":[{\"authorId\":\"39458024\",\"name\":\"Rithesh Kumar\"},{\"authorId\":\"143778281\",\"name\":\"J. Sotelo\"},{\"authorId\":\"145411463\",\"name\":\"K. Kumar\"},{\"authorId\":\"2346028\",\"name\":\"A. D. Br\\u00e9bisson\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e119ce794f45069df77946cce94fb12c15b4e44c\",\"title\":\"ObamaNet: Photo-realistic lip-sync from text\",\"url\":\"https://www.semanticscholar.org/paper/e119ce794f45069df77946cce94fb12c15b4e44c\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1702.08423\",\"authors\":[{\"authorId\":\"1786391\",\"name\":\"Zhifei Zhang\"},{\"authorId\":\"144404428\",\"name\":\"Yang Song\"},{\"authorId\":\"1698645\",\"name\":\"H. Qi\"}],\"doi\":\"10.1109/CVPR.2017.463\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7bcebd481bb1161843efdd135e4ba59dfac4b61c\",\"title\":\"Age Progression/Regression by Conditional Adversarial Autoencoder\",\"url\":\"https://www.semanticscholar.org/paper/7bcebd481bb1161843efdd135e4ba59dfac4b61c\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":\"1704.08292\",\"authors\":[{\"authorId\":\"1753356\",\"name\":\"Lele Chen\"},{\"authorId\":\"4037274\",\"name\":\"Sudhanshu Srivastava\"},{\"authorId\":\"3270912\",\"name\":\"Z. Duan\"},{\"authorId\":\"2026123\",\"name\":\"Chenliang Xu\"}],\"doi\":\"10.1145/3126686.3126723\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"27e8965cc9c166e9afee46e611039f0ce8263e51\",\"title\":\"Deep Cross-Modal Audio-Visual Generation\",\"url\":\"https://www.semanticscholar.org/paper/27e8965cc9c166e9afee46e611039f0ce8263e51\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1507.08750\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4257bc131c36504a04382290cbc27ca8bb27813\",\"title\":\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1611.07004\",\"authors\":[{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"},{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"1822702\",\"name\":\"Tinghui Zhou\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2017.632\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8acbe90d5b852dadea7810345451a99608ee54c7\",\"title\":\"Image-to-Image Translation with Conditional Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/8acbe90d5b852dadea7810345451a99608ee54c7\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48241398\",\"name\":\"Shuang Wei\"}],\"doi\":\"10.18260/p.23725\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"af9b88f2d962541f01805e5478b758a9fd5b56d8\",\"title\":\"Computer vision aided lip movement correction to improve English pronunciation\",\"url\":\"https://www.semanticscholar.org/paper/af9b88f2d962541f01805e5478b758a9fd5b56d8\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nitish Srivastava\"},{\"authorId\":null,\"name\":\"Elman Mansimov\"},{\"authorId\":null,\"name\":\"Ruslan Salakhudinov. Unsupervised learning of video repre lstms\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International conference on machine learning\",\"url\":\"\",\"venue\":\"pages 843\\u2013852,\",\"year\":2015},{\"arxivId\":\"1502.04681\",\"authors\":[{\"authorId\":\"2897313\",\"name\":\"Nitish Srivastava\"},{\"authorId\":\"2711409\",\"name\":\"Elman Mansimov\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"title\":\"Unsupervised Learning of Video Representations using LSTMs\",\"url\":\"https://www.semanticscholar.org/paper/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1705.02966\",\"authors\":[{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"2727313\",\"name\":\"A. Jamaludin\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.31.109\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a8632cf6c1ef4319966564328d187876d3bef363\",\"title\":\"You said that?\",\"url\":\"https://www.semanticscholar.org/paper/a8632cf6c1ef4319966564328d187876d3bef363\",\"venue\":\"BMVC\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joon Son Chung\"},{\"authorId\":null,\"name\":\"Andrew Zisserman. Lip reading in the wild. In Asian Confe Vision\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 87\\u2013103\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144404428\",\"name\":\"Yang Song\"},{\"authorId\":\"1786391\",\"name\":\"Zhifei Zhang\"},{\"authorId\":\"1698645\",\"name\":\"H. Qi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7fc22f1edf4fbe318a3f8795c702665bf3707b55\",\"title\":\"r-BTN: Cross-Domain Face Composite and Synthesis From Limited Facial Patches\",\"url\":\"https://www.semanticscholar.org/paper/7fc22f1edf4fbe318a3f8795c702665bf3707b55\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1801.06790\",\"authors\":[{\"authorId\":\"1786391\",\"name\":\"Zhifei Zhang\"},{\"authorId\":\"144404428\",\"name\":\"Yang Song\"},{\"authorId\":\"1698645\",\"name\":\"H. Qi\"}],\"doi\":\"10.1109/WACV.2018.00082\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"458ce461ab28ea6daa87cb9a3ba8313df6933b1a\",\"title\":\"Decoupled Learning for Conditional Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/458ce461ab28ea6daa87cb9a3ba8313df6933b1a\",\"venue\":\"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)\",\"year\":2018},{\"arxivId\":\"1708.00284\",\"authors\":[{\"authorId\":\"40250403\",\"name\":\"Xiaodan Liang\"},{\"authorId\":\"87068304\",\"name\":\"L. Lee\"},{\"authorId\":\"143716171\",\"name\":\"Wei Dai\"},{\"authorId\":\"143977260\",\"name\":\"E. Xing\"}],\"doi\":\"10.1109/ICCV.2017.194\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eb911796464b1e01e98c2b3d007a4ef9310272e2\",\"title\":\"Dual Motion GAN for Future-Flow Embedded Video Prediction\",\"url\":\"https://www.semanticscholar.org/paper/eb911796464b1e01e98c2b3d007a4ef9310272e2\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1706.08612\",\"authors\":[{\"authorId\":\"19263506\",\"name\":\"Arsha Nagrani\"},{\"authorId\":\"2863890\",\"name\":\"Joon Son Chung\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.21437/Interspeech.2017-950\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0\",\"title\":\"VoxCeleb: A Large-Scale Speaker Identification Dataset\",\"url\":\"https://www.semanticscholar.org/paper/8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0\",\"venue\":\"INTERSPEECH\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3188342\",\"name\":\"Omkar M. Parkhi\"},{\"authorId\":\"1687524\",\"name\":\"A. Vedaldi\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":\"10.5244/C.29.41\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"162ea969d1929ed180cc6de9f0bf116993ff6e06\",\"title\":\"Deep Face Recognition\",\"url\":\"https://www.semanticscholar.org/paper/162ea969d1929ed180cc6de9f0bf116993ff6e06\",\"venue\":\"BMVC\",\"year\":2015},{\"arxivId\":\"1808.06601\",\"authors\":[{\"authorId\":\"2195314\",\"name\":\"T. Wang\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"47062069\",\"name\":\"Guilin Liu\"},{\"authorId\":\"29955511\",\"name\":\"A. Tao\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"},{\"authorId\":\"2301680\",\"name\":\"Bryan Catanzaro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c5b55f410365bb889c25a9f0354f2b86ec61c4f0\",\"title\":\"Video-to-Video Synthesis\",\"url\":\"https://www.semanticscholar.org/paper/c5b55f410365bb889c25a9f0354f2b86ec61c4f0\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1807.07860\",\"authors\":[{\"authorId\":\"145798292\",\"name\":\"Hang Zhou\"},{\"authorId\":\"40457380\",\"name\":\"Y. Liu\"},{\"authorId\":\"3243969\",\"name\":\"Z. Liu\"},{\"authorId\":\"47571885\",\"name\":\"Ping Luo\"},{\"authorId\":\"31843833\",\"name\":\"X. Wang\"}],\"doi\":\"10.1609/aaai.v33i01.33019299\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1816f98e2a4dd54690c2689cf529699d8843e847\",\"title\":\"Talking Face Generation by Adversarially Disentangled Audio-Visual Representation\",\"url\":\"https://www.semanticscholar.org/paper/1816f98e2a4dd54690c2689cf529699d8843e847\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joon Son Chung\"},{\"authorId\":null,\"name\":\"Amir Jamaludin\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and Andrew Zisserman\",\"url\":\"\",\"venue\":\"You said that? British Machine Vision Conference (BMVC),\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37016781\",\"name\":\"Supasorn Suwajanakorn\"},{\"authorId\":\"1396612598\",\"name\":\"Steven M. Seitz\"},{\"authorId\":\"1397689071\",\"name\":\"Ira Kemelmacher-Shlizerman\"}],\"doi\":\"10.1145/3072959.3073640\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"title\":\"Synthesizing Obama\",\"url\":\"https://www.semanticscholar.org/paper/7aa88dafb5d5fd5645c0ada2539e9eaf5b2fe949\",\"venue\":\"ACM Trans. Graph.\",\"year\":2017},{\"arxivId\":\"1903.00834\",\"authors\":[{\"authorId\":\"50317087\",\"name\":\"Zhifei Zhang\"},{\"authorId\":\"50218411\",\"name\":\"Z. Wang\"},{\"authorId\":\"145527707\",\"name\":\"Zhe L. Lin\"},{\"authorId\":\"1698645\",\"name\":\"H. Qi\"}],\"doi\":\"10.1109/CVPR.2019.00817\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0268917fdb8422b8c8b3f579ef5b1f53a3a054e1\",\"title\":\"Image Super-Resolution by Neural Texture Transfer\",\"url\":\"https://www.semanticscholar.org/paper/0268917fdb8422b8c8b3f579ef5b1f53a3a054e1\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1505.04597\",\"authors\":[{\"authorId\":\"1737326\",\"name\":\"O. Ronneberger\"},{\"authorId\":\"152702479\",\"name\":\"P. Fischer\"},{\"authorId\":\"1710872\",\"name\":\"T. Brox\"}],\"doi\":\"10.1007/978-3-319-24574-4_28\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6364fdaa0a0eccd823a779fcdd489173f938e91a\",\"title\":\"U-Net: Convolutional Networks for Biomedical Image Segmentation\",\"url\":\"https://www.semanticscholar.org/paper/6364fdaa0a0eccd823a779fcdd489173f938e91a\",\"venue\":\"MICCAI\",\"year\":2015},{\"arxivId\":\"1511.05440\",\"authors\":[{\"authorId\":\"143949035\",\"name\":\"Micha\\u00ebl Mathieu\"},{\"authorId\":\"2341378\",\"name\":\"C. Couprie\"},{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"title\":\"Deep multi-scale video prediction beyond mean square error\",\"url\":\"https://www.semanticscholar.org/paper/17fa1c2a24ba8f731c8b21f1244463bc4b465681\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1706.08033\",\"authors\":[{\"authorId\":\"144543406\",\"name\":\"R. Villegas\"},{\"authorId\":\"1768964\",\"name\":\"Jimei Yang\"},{\"authorId\":\"2241528\",\"name\":\"Seunghoon Hong\"},{\"authorId\":\"10668384\",\"name\":\"Xunyu Lin\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8375ff50b8a6f1a10dd809129a18df96888ac8b\",\"title\":\"Decomposing Motion and Content for Natural Video Sequence Prediction\",\"url\":\"https://www.semanticscholar.org/paper/b8375ff50b8a6f1a10dd809129a18df96888ac8b\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144686633\",\"name\":\"N. Harte\"},{\"authorId\":\"23710772\",\"name\":\"E. Gillen\"}],\"doi\":\"10.1109/TMM.2015.2407694\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d6829332d0596659272451920d9ff778b0b400af\",\"title\":\"TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech\",\"url\":\"https://www.semanticscholar.org/paper/d6829332d0596659272451920d9ff778b0b400af\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2015},{\"arxivId\":\"1609.02612\",\"authors\":[{\"authorId\":\"1856025\",\"name\":\"Carl Vondrick\"},{\"authorId\":\"2367683\",\"name\":\"H. Pirsiavash\"},{\"authorId\":\"143805211\",\"name\":\"A. Torralba\"}],\"doi\":\"10.13016/M26GIH-TNYZ\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"title\":\"Generating Videos with Scene Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2436356\",\"name\":\"Jun-Yan Zhu\"},{\"authorId\":\"150426657\",\"name\":\"\\u62d3\\u6d77 \\u6749\\u5c71\"},{\"authorId\":\"2094770\",\"name\":\"Phillip Isola\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e8a5f27e7805f8de84ea008d59452ff864271696\",\"title\":\"\\u201cUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\\u201d\\u306e\\u5b66\\u7fd2\\u5831\\u544a\",\"url\":\"https://www.semanticscholar.org/paper/e8a5f27e7805f8de84ea008d59452ff864271696\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sarah Taylor\"},{\"authorId\":null,\"name\":\"Taehwan Kim\"},{\"authorId\":null,\"name\":\"Yisong Yue\"},{\"authorId\":null,\"name\":\"Moshe Mahler\"},{\"authorId\":null,\"name\":\"James Krahe\"},{\"authorId\":null,\"name\":\"Anastasio Garcia Rodriguez\"},{\"authorId\":null,\"name\":\"Jessica Hodgins\"},{\"authorId\":null,\"name\":\"Iain Matthews. A deep learning approach for generalized animation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ACM Transactions on Graphics (TOG)\",\"url\":\"\",\"venue\":\"36(4):93,\",\"year\":2017},{\"arxivId\":\"1603.08155\",\"authors\":[{\"authorId\":\"122867187\",\"name\":\"J. Johnson\"},{\"authorId\":\"3304525\",\"name\":\"Alexandre Alahi\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/978-3-319-46475-6_43\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9fa3720371e78d04973ce9752781bc337480b68f\",\"title\":\"Perceptual Losses for Real-Time Style Transfer and Super-Resolution\",\"url\":\"https://www.semanticscholar.org/paper/9fa3720371e78d04973ce9752781bc337480b68f\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":\"1604.07379\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"2562966\",\"name\":\"Philipp Kr\\u00e4henb\\u00fchl\"},{\"authorId\":\"7408951\",\"name\":\"J. Donahue\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"}],\"doi\":\"10.1109/CVPR.2016.278\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d0effebfa4bed19b6ba41f3af5b7e5b6890de87\",\"title\":\"Context Encoders: Feature Learning by Inpainting\",\"url\":\"https://www.semanticscholar.org/paper/7d0effebfa4bed19b6ba41f3af5b7e5b6890de87\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016}],\"title\":\"Talking Face Generation by Conditional Recurrent Adversarial Network\",\"topics\":[{\"topic\":\"Uncontrolled format string\",\"topicId\":\"753476\",\"url\":\"https://www.semanticscholar.org/topic/753476\"},{\"topic\":\"Video clip\",\"topicId\":\"30493\",\"url\":\"https://www.semanticscholar.org/topic/30493\"},{\"topic\":\"End-to-end principle\",\"topicId\":\"299633\",\"url\":\"https://www.semanticscholar.org/topic/299633\"},{\"topic\":\"Computer multitasking\",\"topicId\":\"6968\",\"url\":\"https://www.semanticscholar.org/topic/6968\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Encoder\",\"topicId\":\"16744\",\"url\":\"https://www.semanticscholar.org/topic/16744\"},{\"topic\":\"Randomness extractor\",\"topicId\":\"653542\",\"url\":\"https://www.semanticscholar.org/topic/653542\"},{\"topic\":\"Image resolution\",\"topicId\":\"881\",\"url\":\"https://www.semanticscholar.org/topic/881\"}],\"url\":\"https://www.semanticscholar.org/paper/7c223baccf679fc212dd4d9ecbebf30b7a8616af\",\"venue\":\"IJCAI\",\"year\":2019}\n"