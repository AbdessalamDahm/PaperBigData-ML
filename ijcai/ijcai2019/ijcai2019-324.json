"{\"abstract\":\"Reinforcement learning has played an important role in decision making related applications, e.g., robotics motion, self-driving, recommendation, etc. The reward function, as a crucial component, affects the efficiency and effectiveness of reinforcement learning to a large extent. In this paper, we focus on the investigation of reinforcement learning with more than one auxiliary reward. It is found that different auxiliary rewards can boost up the learning rate and effectiveness in different stages, and consequently we propose the Automatic Successive Reinforcement Learning (ASR) for auxiliary rewards grading selection for efficient reinforcement learning by stages. Experiments and simulations have shown the superiority of our proposed ASR on a range of environments, including OpenAI classical control domains and video games; Freeway and Catcher.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"46905137\",\"name\":\"Zhao-Yang Fu\",\"url\":\"https://www.semanticscholar.org/author/46905137\"},{\"authorId\":\"1721819\",\"name\":\"D. Zhan\",\"url\":\"https://www.semanticscholar.org/author/1721819\"},{\"authorId\":\"50078941\",\"name\":\"Xin-Chun Li\",\"url\":\"https://www.semanticscholar.org/author/50078941\"},{\"authorId\":\"151480533\",\"name\":\"Yi-Xing Lu\",\"url\":\"https://www.semanticscholar.org/author/151480533\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2011.02669\",\"authors\":[{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2209431\",\"name\":\"Weixun Wang\"},{\"authorId\":\"50982491\",\"name\":\"Hangtian Jia\"},{\"authorId\":null,\"name\":\"Yixiang Wang\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"1684705122\",\"name\":\"Feng Wu\"},{\"authorId\":\"153645633\",\"name\":\"Changjie Fan\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"d12ab6fc45f953a4bec640da267a913d7ad5289b\",\"title\":\"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/d12ab6fc45f953a4bec640da267a913d7ad5289b\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":199465928,\"doi\":\"10.24963/ijcai.2019/324\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"0cca8777ef570d915a1fa0b472421aee8f637dcc\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"98241663\",\"name\":\"M. V. Rossum\"}],\"doi\":\"10.1142/9789814360784_0003\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"title\":\"Neural Computation\",\"url\":\"https://www.semanticscholar.org/paper/2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"venue\":\"\",\"year\":1989},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9165544\",\"name\":\"Kristof Van Moffaert\"},{\"authorId\":\"1751980\",\"name\":\"Madalina M. Drugan\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":\"10.1109/ADPRL.2013.6615007\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f51265b88ce01e7e3c12fa9b8dc84dfd0a73975c\",\"title\":\"Scalarized multi-objective reinforcement learning: Novel design techniques\",\"url\":\"https://www.semanticscholar.org/paper/f51265b88ce01e7e3c12fa9b8dc84dfd0a73975c\",\"venue\":\"2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Pirotta\"},{\"authorId\":null,\"name\":\"Simone Parisi\"},{\"authorId\":null,\"name\":\"Marcello Restelli. Multi-objective reinforcement learning w approximation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 29th AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 2928\\u20132934, Austin, TX.,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrej Karpathy\"},{\"authorId\":null,\"name\":\"Michiel van de Panne. Curriculum learning for motor skills Intelligence\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 325\\u2013330\",\"url\":\"\",\"venue\":\"Heidelberg, Germany,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Anna Harutyunyan\"},{\"authorId\":null,\"name\":\"Peter Vrancx\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor. Multiobjectivization\"},{\"authorId\":null,\"name\":\"ensembles of shapings in reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Neurocomputing\",\"url\":\"\",\"venue\":\"263:48\\u201359,\",\"year\":2017},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.1111/j.1365-2044.1969.tb02820.x\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0a4c1ec24c5f107aaaccf167400874c2ee3576de\",\"title\":\"Cambridge\",\"url\":\"https://www.semanticscholar.org/paper/0a4c1ec24c5f107aaaccf167400874c2ee3576de\",\"venue\":\"\",\"year\":1969},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693549\",\"name\":\"S. Dzeroski\"},{\"authorId\":\"1740042\",\"name\":\"L. D. Raedt\"},{\"authorId\":\"145057418\",\"name\":\"S. Wrobel\"}],\"doi\":\"10.1145/1102351\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"df976ef18596067c4f06b714edd3e3a70b1a0fd7\",\"title\":\"Proceedings of the 22nd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/df976ef18596067c4f06b714edd3e3a70b1a0fd7\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sam Devlin\"},{\"authorId\":null,\"name\":\"Daniel Kudenko\"},{\"authorId\":null,\"name\":\"Marek Grzes. An empirical study of potential-based rewar shaping\"},{\"authorId\":null,\"name\":\"advice in complex\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"multi-agent systems\",\"url\":\"\",\"venue\":\"Advances in Complex Systems, 14(2):251\\u2013278,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Anna Harutyunyan\"},{\"authorId\":null,\"name\":\"Peter Vrancx\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor. Multiobjectivization\"},{\"authorId\":null,\"name\":\"ensembles of shapings in reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Neurocomputing\",\"url\":\"\",\"venue\":\"263:48\\u201359,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49596591\",\"name\":\"I. Kim\"},{\"authorId\":\"1688842\",\"name\":\"O. Weck\"}],\"doi\":\"10.1007/S00158-005-0557-6\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d784b1488e82ce49c6d4baf0bb6d603a5f279b73\",\"title\":\"Adaptive weighted sum method for multiobjective optimization: a new method for Pareto front generation\",\"url\":\"https://www.semanticscholar.org/paper/d784b1488e82ce49c6d4baf0bb6d603a5f279b73\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9\"},{\"authorId\":null,\"name\":\"Daniel Kudenko\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor. Combining multiple correlated reward\"},{\"authorId\":null,\"name\":\"shaping signals by measuring confidence\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 28th AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 1687\\u20131693, Qu\\u00e9bec, Canada,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2018087\",\"name\":\"A. Srivastava\"},{\"authorId\":\"37210858\",\"name\":\"Charles Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2cd8ae29075d7b9b916377ead43b84e485f1399b\",\"title\":\"Proceedings for the 5th International Conference on Learning Representations\",\"url\":\"https://www.semanticscholar.org/paper/2cd8ae29075d7b9b916377ead43b84e485f1399b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Plappert et al\"},{\"authorId\":null,\"name\":\"2018 Matthias Plappert\"},{\"authorId\":null,\"name\":\"Marcin Andrychowicz\"},{\"authorId\":null,\"name\":\"Alex Ray\"},{\"authorId\":null,\"name\":\"Bob McGrew\"},{\"authorId\":null,\"name\":\"Bowen Baker\"},{\"authorId\":null,\"name\":\"Glenn Powell\"},{\"authorId\":null,\"name\":\"Jonas Schneider\"},{\"authorId\":null,\"name\":\"Josh Tobin\"},{\"authorId\":null,\"name\":\"Maciek Chociej\"},{\"authorId\":null,\"name\":\"Peter Welinder\"},{\"authorId\":null,\"name\":\"Vikash Kumar\"},{\"authorId\":null,\"name\":\"Wojciech Zaremba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Multi-goal reinforcement learning: Challenging\",\"url\":\"\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"2528631\",\"name\":\"Peter Vrancx\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":\"10.1109/IJCNN.2014.6889732\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b596cbf733fec66e02035bc7b2ef9177e455f232\",\"title\":\"Multi-objectivization of reinforcement learning problems by reward shaping\",\"url\":\"https://www.semanticscholar.org/paper/b596cbf733fec66e02035bc7b2ef9177e455f232\",\"venue\":\"2014 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145039030\",\"name\":\"John Platt\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"53fcc056f79e04daf11eb798a7238e93699665aa\",\"title\":\"Sequential Minimal Optimization : A Fast Algorithm for Training Support Vector Machines\",\"url\":\"https://www.semanticscholar.org/paper/53fcc056f79e04daf11eb798a7238e93699665aa\",\"venue\":\"\",\"year\":1998},{\"arxivId\":\"1112.5309\",\"authors\":[{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.3389/fpsyg.2013.00313\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"acc552c96fcb49d81e4d38aeb343b8096d96990a\",\"title\":\"PowerPlay: Training an Increasingly General Problem Solver by Continually Searching for the Simplest Still Unsolvable Problem\",\"url\":\"https://www.semanticscholar.org/paper/acc552c96fcb49d81e4d38aeb343b8096d96990a\",\"venue\":\"Front. Psychol.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2630665\",\"name\":\"S. Parisi\"},{\"authorId\":\"6234609\",\"name\":\"M. Pirotta\"},{\"authorId\":\"2983394\",\"name\":\"Nicola Smacchia\"},{\"authorId\":\"1944485\",\"name\":\"L. Bascetta\"},{\"authorId\":\"1792167\",\"name\":\"Marcello Restelli\"}],\"doi\":\"10.1109/IJCNN.2014.6889738\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0f84df5d5f6cbe8b691a260f20d80aa82ab46b6\",\"title\":\"Policy gradient approaches for multi-objective sequential decision making\",\"url\":\"https://www.semanticscholar.org/paper/c0f84df5d5f6cbe8b691a260f20d80aa82ab46b6\",\"venue\":\"2014 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2741488\",\"name\":\"Kyriakos Efthymiadis\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":\"10.1109/CIG.2013.6633622\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2356e4a0080195b1b1632c806bacb9f42cb5d82e\",\"title\":\"Using plan-based reward shaping to learn strategies in StarCraft: Broodwar\",\"url\":\"https://www.semanticscholar.org/paper/2356e4a0080195b1b1632c806bacb9f42cb5d82e\",\"venue\":\"2013 IEEE Conference on Computational Inteligence in Games (CIG)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1932759\",\"name\":\"M. J. Todd\"}],\"doi\":\"10.1201/9781420035315.ch46\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"322647135bab6d777cddf77794499bc6372c242f\",\"title\":\"Mathematical programming\",\"url\":\"https://www.semanticscholar.org/paper/322647135bab6d777cddf77794499bc6372c242f\",\"venue\":\"Handbook of Discrete and Computational Geometry, 2nd Ed.\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1392087129\",\"name\":\"Alberto RibesAbstract\"}],\"doi\":\"10.1007/978-3-319-17885-1_100819\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"550f6163040f1dd580f5cfa3ec007460b3e1cbe9\",\"title\":\"Multi agent systems\",\"url\":\"https://www.semanticscholar.org/paper/550f6163040f1dd580f5cfa3ec007460b3e1cbe9\",\"venue\":\"Proceedings of the 2005 International Conference on Active Media Technology, 2005. (AMT 2005).\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Brys\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9\"},{\"authorId\":null,\"name\":\"Daniel Kudenko\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor. Combining multiple correlated reward\"},{\"authorId\":null,\"name\":\"shaping signals by measuring confidence\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 28th AAAI Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 1687\\u20131693, Qu\\u00e9bec, Canada,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoshua Bengio\"},{\"authorId\":null,\"name\":\"J\\u00e9r\\u00f4me Louradour\"},{\"authorId\":null,\"name\":\"Ronan Collobert\"},{\"authorId\":null,\"name\":\"Jason Weston. Curriculum learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 26th Annual International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 41\\u201348, Montr\\u00e9al, Canada,\",\"year\":2009}],\"title\":\"Automatic Successive Reinforcement Learning with Multiple Auxiliary Rewards\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/0cca8777ef570d915a1fa0b472421aee8f637dcc\",\"venue\":\"IJCAI\",\"year\":2019}\n"