"{\"abstract\":\"With the evolution of various advanced driver assistance system (ADAS) platforms, the design of autonomous driving system is becoming more complex and safety-critical. The autonomous driving system simultaneously activates multiple ADAS functions; and thus it is essential to coordinate various ADAS functions. This paper proposes a randomized adversarial imitation learning (RAIL) method that imitates the coordination of autonomous vehicle equipped with advanced sensors. The RAIL policies are trained through derivative-free optimization for the decision maker that coordinates the proper ADAS functions, e.g., smart cruise control and lane keeping system. Especially, the proposed method is also able to deal with the LIDAR data and makes decisions in complex multi-lane highways and multi-agent environments.\",\"arxivId\":\"1905.05637\",\"authors\":[{\"authorId\":\"40986341\",\"name\":\"Myungjae Shin\",\"url\":\"https://www.semanticscholar.org/author/40986341\"},{\"authorId\":\"1839845\",\"name\":\"J. Kim\",\"url\":\"https://www.semanticscholar.org/author/1839845\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145864982\",\"name\":\"Soohyun Park\"},{\"authorId\":\"51046647\",\"name\":\"Yeongeun Kang\"},{\"authorId\":\"1690849\",\"name\":\"Y. Tian\"},{\"authorId\":\"46454449\",\"name\":\"Joongheon Kim\"}],\"doi\":\"10.1109/ICOIN48656.2020.9016591\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f9d17a926cb9c93cef28fadaa60963e5fd1ceb68\",\"title\":\"Fast and Reliable Offloading via Deep Reinforcement Learning for Mobile Edge Video Computing\",\"url\":\"https://www.semanticscholar.org/paper/f9d17a926cb9c93cef28fadaa60963e5fd1ceb68\",\"venue\":\"2020 International Conference on Information Networking (ICOIN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"87237060\",\"name\":\"S. Park\"},{\"authorId\":\"97649786\",\"name\":\"Junhui Kim\"},{\"authorId\":\"49289411\",\"name\":\"Dohyun Kwon\"},{\"authorId\":\"40986341\",\"name\":\"Myungjae Shin\"},{\"authorId\":\"46454449\",\"name\":\"Joongheon Kim\"}],\"doi\":\"10.1109/VTS-APWCS.2019.8851667\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6793058ae35c963631e5491e82cb166f0f5804b\",\"title\":\"Joint Offloading and Streaming in Mobile Edges: A Deep Reinforcement Learning Approach\",\"url\":\"https://www.semanticscholar.org/paper/b6793058ae35c963631e5491e82cb166f0f5804b\",\"venue\":\"2019 IEEE VTS Asia Pacific Wireless Communications Symposium (APWCS)\",\"year\":2019},{\"arxivId\":\"2009.09647\",\"authors\":[{\"authorId\":\"49591606\",\"name\":\"Soo-Hyun Park\"},{\"authorId\":\"19243159\",\"name\":\"Jeman Park\"},{\"authorId\":\"1438306099\",\"name\":\"David A. Mohaisen\"},{\"authorId\":\"46454449\",\"name\":\"Joongheon Kim\"}],\"doi\":\"10.1109/ICTC49870.2020.9289219\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4d9eed48e1d84b1f83ca272212a1e2f3e9f62565\",\"title\":\"Reinforced Edge Selection using Deep Learning for Robust Surveillance in Unmanned Aerial Vehicles\",\"url\":\"https://www.semanticscholar.org/paper/4d9eed48e1d84b1f83ca272212a1e2f3e9f62565\",\"venue\":\"2020 International Conference on Information and Communication Technology Convergence (ICTC)\",\"year\":2020}],\"corpusId\":153312899,\"doi\":\"10.24963/ijcai.2019/638\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"ee01d43cdc73f083d1cb06bc3f9a11d1d0ec1efb\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"73752411\",\"name\":\"K. I. Ahmed\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16a5decd57a42ba326251e2e38bb85359798b664\",\"title\":\"Modeling drivers' acceleration and lane changing behavior\",\"url\":\"https://www.semanticscholar.org/paper/16a5decd57a42ba326251e2e38bb85359798b664\",\"venue\":\"\",\"year\":1999},{\"arxivId\":\"1809.02925\",\"authors\":[{\"authorId\":\"2000906\",\"name\":\"Ilya Kostrikov\"},{\"authorId\":\"6565766\",\"name\":\"Kumar Krishna Agrawal\"},{\"authorId\":\"2420123\",\"name\":\"D. Dwibedi\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"2704494\",\"name\":\"J. Tompson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1de1e749668a65cf6b88b8138389581108bb129a\",\"title\":\"Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/1de1e749668a65cf6b88b8138389581108bb129a\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1709.06683\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"24064144\",\"name\":\"Wei-Di Chang\"},{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"90356d78db11e14645a96d8483268ce585dbb0c3\",\"title\":\"OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/90356d78db11e14645a96d8483268ce585dbb0c3\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144243658\",\"name\":\"A. Fischer\"}],\"doi\":\"10.1007/springerreference_179129\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f2d5039b55d626ef5466a80e950bddd5cc1819d4\",\"title\":\"Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f2d5039b55d626ef5466a80e950bddd5cc1819d4\",\"venue\":\"\",\"year\":2012},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1603.00448\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"04162cb8cfaa0f7e37586823ff4ad0bff09ed21d\",\"title\":\"Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/04162cb8cfaa0f7e37586823ff4ad0bff09ed21d\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1709.06560\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33690ff21ef1efb576410e656f2e60c89d0307d6\",\"title\":\"Deep Reinforcement Learning that Matters\",\"url\":\"https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1812.00950\",\"authors\":[{\"authorId\":\"1857914\",\"name\":\"Yijie Guo\"},{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"145537841\",\"name\":\"S. Singh\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8f6fff9ef91d42ed02a0fb2002a716876baa8db\",\"title\":\"Generative Adversarial Self-Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/b8f6fff9ef91d42ed02a0fb2002a716876baa8db\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"},{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"144021446\",\"name\":\"Anind K. Dey\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"11b6bdfe36c48b11367b27187da11d95892f0361\",\"title\":\"Maximum Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/11b6bdfe36c48b11367b27187da11d95892f0361\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":\"1611.03852\",\"authors\":[{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"29848635\",\"name\":\"Paul F. Christiano\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a31d0e5668b311b1ec74c5607a9f96c35b395fa8\",\"title\":\"A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models\",\"url\":\"https://www.semanticscholar.org/paper/a31d0e5668b311b1ec74c5607a9f96c35b395fa8\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":\"1708.02596\",\"authors\":[{\"authorId\":\"3195183\",\"name\":\"Anusha Nagabandi\"},{\"authorId\":\"46292812\",\"name\":\"G. Kahn\"},{\"authorId\":\"98107250\",\"name\":\"Ronald S. Fearing\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.1109/ICRA.2018.8463189\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cce22bf6405042a965a86557684c46a441f2a736\",\"title\":\"Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning\",\"url\":\"https://www.semanticscholar.org/paper/cce22bf6405042a965a86557684c46a441f2a736\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2874057\",\"name\":\"Mustafa Mukadam\"},{\"authorId\":\"3216610\",\"name\":\"Akansel Cosgun\"},{\"authorId\":\"2148583\",\"name\":\"A. Nakhaei\"},{\"authorId\":\"35047644\",\"name\":\"K. Fujimura\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a15b2393c3ce3473b859f9fd1892daec67588b66\",\"title\":\"Tactical Decision Making for Lane Changing with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a15b2393c3ce3473b859f9fd1892daec67588b66\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145847467\",\"name\":\"D. Pomerleau\"}],\"doi\":\"10.1184/R1/6603146.V1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3\",\"title\":\"ALVINN: An Autonomous Land Vehicle in a Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3\",\"venue\":\"NIPS\",\"year\":1988},{\"arxivId\":\"1612.04340\",\"authors\":[{\"authorId\":\"7353741\",\"name\":\"Ahmad El Sallab\"},{\"authorId\":\"144026300\",\"name\":\"M. Abdou\"},{\"authorId\":\"8211183\",\"name\":\"E. Perot\"},{\"authorId\":\"2601522\",\"name\":\"S. Yogamani\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cf0930b6dddb437185c7d0c956cd64666b5f1d34\",\"title\":\"End-to-End Deep Reinforcement Learning for Lane Keeping Assist\",\"url\":\"https://www.semanticscholar.org/paper/cf0930b6dddb437185c7d0c956cd64666b5f1d34\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1611.04076\",\"authors\":[{\"authorId\":\"34443348\",\"name\":\"Xudong Mao\"},{\"authorId\":\"153082769\",\"name\":\"Q. Li\"},{\"authorId\":\"49838750\",\"name\":\"Haoran Xie\"},{\"authorId\":\"144031692\",\"name\":\"Raymond Y. K. Lau\"},{\"authorId\":\"50218420\",\"name\":\"Z. Wang\"},{\"authorId\":\"32309056\",\"name\":\"Stephen Paul Smolley\"}],\"doi\":\"10.1109/ICCV.2017.304\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"74ff6d48f9c62e937023106629d27ef2d2ddf8bc\",\"title\":\"Least Squares Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/74ff6d48f9c62e937023106629d27ef2d2ddf8bc\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":\"1708.04133\",\"authors\":[{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"2956733\",\"name\":\"M. Gomrokchi\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dcc2941b5d7a70acf8bd609570e4b6f106ecabc4\",\"title\":\"Reproducibility of Benchmarked Deep Reinforcement Learning Tasks for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/dcc2941b5d7a70acf8bd609570e4b6f106ecabc4\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32981185\",\"name\":\"Yunpeng Pan\"},{\"authorId\":\"1978613\",\"name\":\"Ching-An Cheng\"},{\"authorId\":\"19253835\",\"name\":\"Kamil Saigol\"},{\"authorId\":\"26365079\",\"name\":\"Keuntaek Lee\"},{\"authorId\":\"8073235\",\"name\":\"Xinyan Yan\"},{\"authorId\":\"1751063\",\"name\":\"E. Theodorou\"},{\"authorId\":\"3288815\",\"name\":\"B. Boots\"}],\"doi\":\"10.15607/RSS.2018.XIV.056\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1d3aa6e84299f8d36629e147679614ae09e55d2e\",\"title\":\"Agile Autonomous Driving using End-to-End Deep Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/1d3aa6e84299f8d36629e147679614ae09e55d2e\",\"venue\":\"Robotics: Science and Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1803.10056\",\"authors\":[{\"authorId\":\"32167181\",\"name\":\"C. Hoel\"},{\"authorId\":\"36610543\",\"name\":\"K. Wolff\"},{\"authorId\":\"3238021\",\"name\":\"L. Laine\"}],\"doi\":\"10.1109/ITSC.2018.8569568\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e34b3c2e620aa1c09c234067d9400c8fd53ce6e3\",\"title\":\"Automated Speed and Lane Change Decision Making using Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e34b3c2e620aa1c09c234067d9400c8fd53ce6e3\",\"venue\":\"2018 21st International Conference on Intelligent Transportation Systems (ITSC)\",\"year\":2018},{\"arxivId\":\"1703.02660\",\"authors\":[{\"authorId\":\"19275599\",\"name\":\"A. Rajeswaran\"},{\"authorId\":\"33557393\",\"name\":\"Kendall Lowrey\"},{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"},{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cadb96f49f3c13c86dfe285b5c75c655391ad1c3\",\"title\":\"Towards Generalization and Simplicity in Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/cadb96f49f3c13c86dfe285b5c75c655391ad1c3\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66590033\",\"name\":\"P. Gipps\"}],\"doi\":\"10.1016/0191-2615(86)90012-3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"28a82ce391e29ff11c75e467c8a4c09cc7e86a99\",\"title\":\"A MODEL FOR THE STRUCTURE OF LANE-CHANGING DECISIONS\",\"url\":\"https://www.semanticscholar.org/paper/28a82ce391e29ff11c75e467c8a4c09cc7e86a99\",\"venue\":\"\",\"year\":1986},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1606.03476\",\"authors\":[{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"title\":\"Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145847467\",\"name\":\"D. Pomerleau\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"23cc7024cbcfe3d56d01c1cd4c8d4583e866b270\",\"title\":\"Rapidly Adapting Artificial Neural Networks for Autonomous Navigation\",\"url\":\"https://www.semanticscholar.org/paper/23cc7024cbcfe3d56d01c1cd4c8d4583e866b270\",\"venue\":\"NIPS\",\"year\":1990},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Akansel Cosgun, Alireza Nakhaei, and Kikuo Fujimura. Tactical decision making for lane changing with deep reinforcement learning\",\"url\":\"\",\"venue\":\"NIPS Workshop MLITS\",\"year\":1989},{\"arxivId\":\"1803.07055\",\"authors\":[{\"authorId\":\"2595993\",\"name\":\"Horia Mania\"},{\"authorId\":\"40895205\",\"name\":\"Aurelia Guy\"},{\"authorId\":\"9229182\",\"name\":\"B. Recht\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"abc8415a73e9056fdd8a7bf529b2c86898f29501\",\"title\":\"Simple random search provides a competitive approach to reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/abc8415a73e9056fdd8a7bf529b2c86898f29501\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1007/978-1-4899-7687-1_142\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97661bb6cf6bc388dced1bb6e6519cbf64f1404c\",\"title\":\"Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/97661bb6cf6bc388dced1bb6e6519cbf64f1404c\",\"venue\":\"Encyclopedia of Machine Learning and Data Mining\",\"year\":2017},{\"arxivId\":\"1703.03864\",\"authors\":[{\"authorId\":\"2887364\",\"name\":\"Tim Salimans\"},{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4ee802a58d32aa049d549d06be440ac947b53987\",\"title\":\"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ee802a58d32aa049d549d06be440ac947b53987\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"J Matyas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Random optimization\",\"url\":\"\",\"venue\":\"Automation and Remote Control, 26(2):246\\u2013253\",\"year\":1965},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35641778\",\"name\":\"T. Korssen\"},{\"authorId\":\"3310744\",\"name\":\"V. Dolk\"},{\"authorId\":\"1421704738\",\"name\":\"Joanna van de Mortel-Fronczak\"},{\"authorId\":\"1741581\",\"name\":\"M. Reniers\"},{\"authorId\":\"48523640\",\"name\":\"M. Heemels\"}],\"doi\":\"10.1109/TITS.2017.2776354\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bc3855f2f6b5db4ddb59af6c55f9a7b67460244\",\"title\":\"Systematic Model-Based Design and Implementation of Supervisors for Advanced Driver Assistance Systems\",\"url\":\"https://www.semanticscholar.org/paper/3bc3855f2f6b5db4ddb59af6c55f9a7b67460244\",\"venue\":\"IEEE Transactions on Intelligent Transportation Systems\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51478767\",\"name\":\"Kyushik Min\"},{\"authorId\":\"2519773\",\"name\":\"Hayoung Kim\"}],\"doi\":\"10.1109/IVS.2018.8500645\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ba913f8b6ff5828deb4ae5fe2b7211770f6fdf2d\",\"title\":\"Deep Q Learning Based High Level Driving Policy Determination\",\"url\":\"https://www.semanticscholar.org/paper/ba913f8b6ff5828deb4ae5fe2b7211770f6fdf2d\",\"venue\":\"2018 IEEE Intelligent Vehicles Symposium (IV)\",\"year\":2018}],\"title\":\"Randomized Adversarial Imitation Learning for Autonomous Driving\",\"topics\":[{\"topic\":\"Autonomous car\",\"topicId\":\"642\",\"url\":\"https://www.semanticscholar.org/topic/642\"},{\"topic\":\"Randomized algorithm\",\"topicId\":\"3442\",\"url\":\"https://www.semanticscholar.org/topic/3442\"},{\"topic\":\"Learning Disorders\",\"topicId\":\"196018\",\"url\":\"https://www.semanticscholar.org/topic/196018\"},{\"topic\":\"Architecture Design and Assessment System\",\"topicId\":\"1715942\",\"url\":\"https://www.semanticscholar.org/topic/1715942\"},{\"topic\":\"Autonomous robot\",\"topicId\":\"1175\",\"url\":\"https://www.semanticscholar.org/topic/1175\"},{\"topic\":\"Derivative-free optimization\",\"topicId\":\"266751\",\"url\":\"https://www.semanticscholar.org/topic/266751\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Numerous\",\"topicId\":\"16861\",\"url\":\"https://www.semanticscholar.org/topic/16861\"},{\"topic\":\"CNS disorder\",\"topicId\":\"33389\",\"url\":\"https://www.semanticscholar.org/topic/33389\"},{\"topic\":\"Policy\",\"topicId\":\"60001\",\"url\":\"https://www.semanticscholar.org/topic/60001\"},{\"topic\":\"sensor (device)\",\"topicId\":\"149745\",\"url\":\"https://www.semanticscholar.org/topic/149745\"}],\"url\":\"https://www.semanticscholar.org/paper/ee01d43cdc73f083d1cb06bc3f9a11d1d0ec1efb\",\"venue\":\"IJCAI\",\"year\":2019}\n"