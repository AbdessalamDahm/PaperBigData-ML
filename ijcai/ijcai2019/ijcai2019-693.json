"{\"abstract\":null,\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\",\"url\":\"https://www.semanticscholar.org/author/49319111\"},{\"authorId\":\"49528341\",\"name\":\"H. Wang\",\"url\":\"https://www.semanticscholar.org/author/49528341\"},{\"authorId\":\"73160450\",\"name\":\"Meng Wang\",\"url\":\"https://www.semanticscholar.org/author/73160450\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"40195882\",\"name\":\"M. Firdaus\"},{\"authorId\":\"2008241757\",\"name\":\"Arunav Pratap Shandeelya\"},{\"authorId\":\"1734904\",\"name\":\"Asif Ekbal\"}],\"doi\":\"10.1371/journal.pone.0241271\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9f76b2bf0de083c77f8730809b2c01c28c388c41\",\"title\":\"More to diverse: Generating diversified responses in a task oriented multimodal dialog system\",\"url\":\"https://www.semanticscholar.org/paper/9f76b2bf0de083c77f8730809b2c01c28c388c41\",\"venue\":\"PloS one\",\"year\":2020},{\"arxivId\":\"2004.02194\",\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"46507139\",\"name\":\"Haibo Wang\"},{\"authorId\":\"5462268\",\"name\":\"Hanwang Zhang\"},{\"authorId\":\"143962510\",\"name\":\"Z. Zha\"},{\"authorId\":\"152808542\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/CVPR42600.2020.01007\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a0d2ea210c9bd21676605682a76cec1a4004320a\",\"title\":\"Iterative Context-Aware Graph Inference for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/a0d2ea210c9bd21676605682a76cec1a4004320a\",\"venue\":\"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49319111\",\"name\":\"Dan Guo\"},{\"authorId\":\"39698901\",\"name\":\"H. Wang\"},{\"authorId\":\"47672591\",\"name\":\"Shuhui Wang\"},{\"authorId\":\"48957872\",\"name\":\"Meng Wang\"}],\"doi\":\"10.1109/TIP.2020.2992888\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"de847c41c0b6c52e5fc770f364a19fed04a7b3ae\",\"title\":\"Textual-Visual Reference-Aware Attention Network for Visual Dialog\",\"url\":\"https://www.semanticscholar.org/paper/de847c41c0b6c52e5fc770f364a19fed04a7b3ae\",\"venue\":\"IEEE Transactions on Image Processing\",\"year\":2020},{\"arxivId\":\"2012.15015\",\"authors\":[{\"authorId\":\"65844131\",\"name\":\"Yuxian Meng\"},{\"authorId\":\"1845298604\",\"name\":\"Shuhe Wang\"},{\"authorId\":\"5439717\",\"name\":\"Qinghong Han\"},{\"authorId\":\"48304805\",\"name\":\"Xiaofei Sun\"},{\"authorId\":\"93192602\",\"name\":\"Fei Wu\"},{\"authorId\":null,\"name\":\"Rui Yan\"},{\"authorId\":\"5183779\",\"name\":\"J. Li\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3470e15ae52baae8b8560dd59c616da9820cf43a\",\"title\":\"OpenViDial: A Large-Scale, Open-Domain Dialogue Dataset with Visual Contexts\",\"url\":\"https://www.semanticscholar.org/paper/3470e15ae52baae8b8560dd59c616da9820cf43a\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.03322\",\"authors\":[{\"authorId\":\"2345018\",\"name\":\"Shen Gao\"},{\"authorId\":\"46772896\",\"name\":\"Xiuying Chen\"},{\"authorId\":\"87109212\",\"name\":\"Li Liu\"},{\"authorId\":\"9072379\",\"name\":\"Dongyan Zhao\"},{\"authorId\":\"1845885740\",\"name\":\"Rui Yan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"336cad600d15832243f4228b351c638630d64cb7\",\"title\":\"Learning to Respond with Your Favorite Stickers: A Framework of Unifying Multi-Modality and User Preference in Multi-Turn Dialog\",\"url\":\"https://www.semanticscholar.org/paper/336cad600d15832243f4228b351c638630d64cb7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.13278\",\"authors\":[{\"authorId\":null,\"name\":\"Yue Wang\"},{\"authorId\":\"2708940\",\"name\":\"Shafiq R. Joty\"},{\"authorId\":\"1785083\",\"name\":\"Michael R. Lyu\"},{\"authorId\":\"145310663\",\"name\":\"Irwin King\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"1741126\",\"name\":\"S. Hoi\"}],\"doi\":\"10.18653/v1/2020.emnlp-main.269\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"06c7269c10125589d2599f684b751b1640f7a0cc\",\"title\":\"VD-BERT: A Unified Vision and Dialog Transformer with BERT\",\"url\":\"https://www.semanticscholar.org/paper/06c7269c10125589d2599f684b751b1640f7a0cc\",\"venue\":\"EMNLP\",\"year\":2020}],\"corpusId\":199465890,\"doi\":\"10.24963/ijcai.2019/693\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"e59f84b64f2bc26a8839d47067077780ea4c4806\",\"references\":[],\"title\":\"Dual Visual Attention Network for Visual Dialog\",\"topics\":[{\"topic\":\"dialog\",\"topicId\":\"14876\",\"url\":\"https://www.semanticscholar.org/topic/14876\"}],\"url\":\"https://www.semanticscholar.org/paper/e59f84b64f2bc26a8839d47067077780ea4c4806\",\"venue\":\"IJCAI\",\"year\":2019}\n"