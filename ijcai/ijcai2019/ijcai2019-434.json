"{\"abstract\":\"In this paper, we investigate Exploratory Conservative Policy Optimization (ECPO), a policy optimization strategy that improves exploration behavior while assuring monotonic progress in a principled objective. ECPO conducts maximum entropy exploration within a mirror descent framework, but updates policies using reversed KL projection. This formulation bypasses undesirable mode seeking behavior and avoids premature convergence to sub-optimal policies, while still supporting strong theoretical properties such as guaranteed policy improvement. Experimental evaluations demonstrate that the proposed method significantly improves practical exploration and surpasses the empirical performance of state-of-the art policy optimization methods in a set of benchmark tasks.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\",\"url\":\"https://www.semanticscholar.org/author/3288319\"},{\"authorId\":\"3363788\",\"name\":\"Chenjun Xiao\",\"url\":\"https://www.semanticscholar.org/author/3363788\"},{\"authorId\":\"2136577\",\"name\":\"Ruitong Huang\",\"url\":\"https://www.semanticscholar.org/author/2136577\"},{\"authorId\":\"50319359\",\"name\":\"D. Schuurmans\",\"url\":\"https://www.semanticscholar.org/author/50319359\"},{\"authorId\":\"48588291\",\"name\":\"M. M\\u00fcller\",\"url\":\"https://www.semanticscholar.org/author/48588291\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.09456\",\"authors\":[{\"authorId\":\"15042674\",\"name\":\"Yaozhong Gan\"},{\"authorId\":\"1812651\",\"name\":\"Zhengqian Zhang\"},{\"authorId\":\"2248421\",\"name\":\"Xiaoyang Tan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"bf591da059e692cb5c2f1d6130e22fcb2fde6f54\",\"title\":\"Stabilizing Q Learning Via Soft Mellowmax Operator\",\"url\":\"https://www.semanticscholar.org/paper/bf591da059e692cb5c2f1d6130e22fcb2fde6f54\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2003.04518\",\"authors\":[{\"authorId\":\"48481808\",\"name\":\"Y. Song\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"}],\"doi\":\"10.1109/CoG47356.2020.9231562\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b5132c21a85306af42c180a8bbb7fa0116b50126\",\"title\":\"Exploring Unknown States with Action Balance\",\"url\":\"https://www.semanticscholar.org/paper/b5132c21a85306af42c180a8bbb7fa0116b50126\",\"venue\":\"CoG\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2000904823\",\"name\":\"Song Yan\"},{\"authorId\":\"66731876\",\"name\":\"Chen Ying-feng\"},{\"authorId\":\"66085863\",\"name\":\"Hu Yu-jing\"},{\"authorId\":\"2001001047\",\"name\":\"Fan Changjie\"}],\"doi\":\"10.1109/CoG47356.2020.9231562\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"706f1555be9ff1a05efb3eea32eb5a45c1ee577f\",\"title\":\"Exploring Unknown States with Action Balance\",\"url\":\"https://www.semanticscholar.org/paper/706f1555be9ff1a05efb3eea32eb5a45c1ee577f\",\"venue\":\"2020 IEEE Conference on Games (CoG)\",\"year\":2020},{\"arxivId\":\"2005.09814\",\"authors\":[{\"authorId\":\"119439163\",\"name\":\"M. Tomar\"},{\"authorId\":\"38274824\",\"name\":\"Lior Shani\"},{\"authorId\":\"27098848\",\"name\":\"Yonathan Efroni\"},{\"authorId\":\"103809454\",\"name\":\"Mohammad Ghavamzadeh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b6fd0df8ff8290dc82f54e6fc04f418ae84be008\",\"title\":\"Mirror Descent Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/b6fd0df8ff8290dc82f54e6fc04f418ae84be008\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.06392\",\"authors\":[{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"},{\"authorId\":\"3363788\",\"name\":\"Chenjun Xiao\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"50319359\",\"name\":\"D. Schuurmans\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ef907098e26a9a14037d0f4e988b39aed2f38136\",\"title\":\"On the Global Convergence Rates of Softmax Policy Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/ef907098e26a9a14037d0f4e988b39aed2f38136\",\"venue\":\"ICML\",\"year\":2020}],\"corpusId\":199465859,\"doi\":\"10.24963/ijcai.2019/434\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"729002fdd2f891ef2c44f414edfa19595b9c9646\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"98045118\",\"name\":\"J. Darzentas\"}],\"doi\":\"10.1057/jors.1984.92\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"29f8b7112fb4660a64df8ac34a5d85c119019efe\",\"title\":\"Problem Complexity and Method Efficiency in Optimization\",\"url\":\"https://www.semanticscholar.org/paper/29f8b7112fb4660a64df8ac34a5d85c119019efe\",\"venue\":\"\",\"year\":1983},{\"arxivId\":\"1705.07606\",\"authors\":[{\"authorId\":\"1801873\",\"name\":\"Voot Tangkaratt\"},{\"authorId\":\"2799799\",\"name\":\"Abbas Abdolmaleki\"},{\"authorId\":\"67154907\",\"name\":\"Masashi Sugiyama\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"27bc1680936e27cfc808923443b39c00fd12959b\",\"title\":\"Guide Actor-Critic for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/27bc1680936e27cfc808923443b39c00fd12959b\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"08883c7ca6a500342975e01cc5efc4b45e43ebec\",\"title\":\"Learning of Non-Parametric Control Policies with High-Dimensional State Features\",\"url\":\"https://www.semanticscholar.org/paper/08883c7ca6a500342975e01cc5efc4b45e43ebec\",\"venue\":\"AISTATS\",\"year\":2015},{\"arxivId\":\"1805.00909\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba\",\"title\":\"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\",\"url\":\"https://www.semanticscholar.org/paper/6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrew G Barto\"},{\"authorId\":null,\"name\":\"Jan Peters Herke Van Hoof\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hierarchical relative entropy policy search Guided policy search via approximate mirror descent Bridging the gap between value and policy based reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2261881\",\"name\":\"M. Deisenroth\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":\"10.1561/2300000021\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6bfae6efa1110a57a4d8362721d152d78aae358\",\"title\":\"A Survey on Policy Search for Robotics\",\"url\":\"https://www.semanticscholar.org/paper/b6bfae6efa1110a57a4d8362721d152d78aae358\",\"venue\":\"Found. Trends Robotics\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"References\"},{\"authorId\":null,\"name\":\"Abdolmaleki\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation\",\"url\":\"\",\"venue\":\"In ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40204991\",\"name\":\"A. Beck\"},{\"authorId\":\"1727609\",\"name\":\"M. Teboulle\"}],\"doi\":\"10.1016/S0167-6377(02)00231-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4f0df7ed89d9a5bb5bafa5727ccdc0b6e2fb463d\",\"title\":\"Mirror descent and nonlinear projected subgradient methods for convex optimization\",\"url\":\"https://www.semanticscholar.org/paper/4f0df7ed89d9a5bb5bafa5727ccdc0b6e2fb463d\",\"venue\":\"Oper. Res. Lett.\",\"year\":2003},{\"arxivId\":\"1802.09477\",\"authors\":[{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"},{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"51174612\",\"name\":\"David Meger\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4debb99c0c63bfaa97dd433bc2828e4dac81c48b\",\"title\":\"Addressing Function Approximation Error in Actor-Critic Methods\",\"url\":\"https://www.semanticscholar.org/paper/4debb99c0c63bfaa97dd433bc2828e4dac81c48b\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ronald J Williams\"},{\"authorId\":null,\"name\":\"Jing Peng. Function optimization using connectionist re algorithms\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Connection Science\",\"url\":\"\",\"venue\":\"3(3):241\\u2013268,\",\"year\":1991},{\"arxivId\":\"1611.09321\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"9caf76734a611286513b478ce10557bda477e8ce\",\"title\":\"Improving Policy Gradient by Exploring Under-appreciated Rewards\",\"url\":\"https://www.semanticscholar.org/paper/9caf76734a611286513b478ce10557bda477e8ce\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49312774\",\"name\":\"K. Murphy\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"25badc676197a70aaf9911865eb03469e402ba57\",\"title\":\"Machine learning - a probabilistic perspective\",\"url\":\"https://www.semanticscholar.org/paper/25badc676197a70aaf9911865eb03469e402ba57\",\"venue\":\"Adaptive computation and machine learning series\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1997069\",\"name\":\"J. K. Lenstra\"},{\"authorId\":\"1729250\",\"name\":\"G. Woeginger\"},{\"authorId\":\"69053687\",\"name\":\"F. Spieksma\"},{\"authorId\":\"144683424\",\"name\":\"P. Marcotte\"},{\"authorId\":\"1800675\",\"name\":\"Steven Kou\"},{\"authorId\":\"1706816\",\"name\":\"N. Megiddo\"},{\"authorId\":\"153539869\",\"name\":\"B. Shepherd\"},{\"authorId\":\"1782241\",\"name\":\"S. Seshadri\"},{\"authorId\":\"145157154\",\"name\":\"R. Schultz\"},{\"authorId\":\"35211064\",\"name\":\"T. Kok\"},{\"authorId\":\"1716078\",\"name\":\"C. Helmberg\"},{\"authorId\":\"143857244\",\"name\":\"A. Martin\"},{\"authorId\":\"17107966\",\"name\":\"S. V. D. Velde\"},{\"authorId\":\"1797627\",\"name\":\"S. Andrad\\u00f3ttir\"},{\"authorId\":\"1731931\",\"name\":\"S. Borst\"},{\"authorId\":\"46719267\",\"name\":\"T. Takine\"},{\"authorId\":\"1705564\",\"name\":\"G. Nemhauser\"},{\"authorId\":\"2229828\",\"name\":\"K. Aardal\"},{\"authorId\":\"1823941\",\"name\":\"D. Klatte\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"19cd6c0e6bcf4cf6613e4ab6a20d6cc80833827a\",\"title\":\"Operations Research Letters\",\"url\":\"https://www.semanticscholar.org/paper/19cd6c0e6bcf4cf6613e4ab6a20d6cc80833827a\",\"venue\":\"\",\"year\":2011},{\"arxivId\":\"1704.06440\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"d0352057e2b99f65f8b5244a0b912026c86d7b21\",\"title\":\"Equivalence Between Policy Gradients and Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/d0352057e2b99f65f8b5244a0b912026c86d7b21\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1512.08562\",\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"3314041\",\"name\":\"Ari Pakman\"},{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"4a026fd65af4ba3575e64174de56fee093fa3330\",\"title\":\"Taming the Noise in Reinforcement Learning via Soft Updates\",\"url\":\"https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":\"1712.03779\",\"authors\":[{\"authorId\":\"144923780\",\"name\":\"B. Yu\"},{\"authorId\":\"19225295\",\"name\":\"Karl Kumbier\"}],\"doi\":\"10.1631/FITEE.1700813\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"747f5ea1b986fb15a20b902407f48ef47c80bbcb\",\"title\":\"Artificial intelligence and statistics\",\"url\":\"https://www.semanticscholar.org/paper/747f5ea1b986fb15a20b902407f48ef47c80bbcb\",\"venue\":\"Frontiers of Information Technology & Electronic Engineering\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":\"10.1109/IROS.2012.6386109\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"title\":\"MuJoCo: A physics engine for model-based control\",\"url\":\"https://www.semanticscholar.org/paper/b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"venue\":\"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"},{\"authorId\":\"47918185\",\"name\":\"Jing Peng\"}],\"doi\":\"10.1080/09540099108946587\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6bc8db0c7444d9c07aad440393b2fd300fb3595c\",\"title\":\"Function Optimization using Connectionist Reinforcement Learning Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/6bc8db0c7444d9c07aad440393b2fd300fb3595c\",\"venue\":\"\",\"year\":1991},{\"arxivId\":\"1707.01891\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"bc8eb0c66f8977d3b23dedb247607a8af2360859\",\"title\":\"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/bc8eb0c66f8977d3b23dedb247607a8af2360859\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1806.06920\",\"authors\":[{\"authorId\":\"2799799\",\"name\":\"Abbas Abdolmaleki\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a8ef08940341381390d9a5672546354d0ce51328\",\"title\":\"Maximum a Posteriori Policy Optimisation\",\"url\":\"https://www.semanticscholar.org/paper/a8ef08940341381390d9a5672546354d0ce51328\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1705.07798\",\"authors\":[{\"authorId\":\"1741549\",\"name\":\"Gergely Neu\"},{\"authorId\":\"143808510\",\"name\":\"A. Jonsson\"},{\"authorId\":\"145810673\",\"name\":\"V. G\\u00f3mez\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2e7d1e21409a90e66106722506aeb434ee7a18f3\",\"title\":\"A unified view of entropy-regularized Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/2e7d1e21409a90e66106722506aeb434ee7a18f3\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Art B. Owen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Monte Carlo theory\",\"url\":\"\",\"venue\":\"methods and examples.\",\"year\":2013},{\"arxivId\":\"1702.08165\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"title\":\"Reinforcement Learning with Deep Energy-Based Policies\",\"url\":\"https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jan Peters\"},{\"authorId\":null,\"name\":\"Katharina M\\u00fclling\"},{\"authorId\":null,\"name\":\"Yasemin Altun. Relative entropy policy search. In AAAI\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 1607\\u20131612\",\"url\":\"\",\"venue\":\"Atlanta,\",\"year\":2010}],\"title\":\"On Principled Entropy Exploration in Policy Optimization\",\"topics\":[{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Premature convergence\",\"topicId\":\"123787\",\"url\":\"https://www.semanticscholar.org/topic/123787\"},{\"topic\":\"Program optimization\",\"topicId\":\"31682\",\"url\":\"https://www.semanticscholar.org/topic/31682\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Exploratory testing\",\"topicId\":\"275997\",\"url\":\"https://www.semanticscholar.org/topic/275997\"}],\"url\":\"https://www.semanticscholar.org/paper/729002fdd2f891ef2c44f414edfa19595b9c9646\",\"venue\":\"IJCAI\",\"year\":2019}\n"