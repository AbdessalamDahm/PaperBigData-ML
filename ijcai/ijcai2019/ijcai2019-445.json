"{\"abstract\":\"Dyna is an architecture for model-based reinforcement learning (RL), where simulated experience from a model is used to update policies or value functions. A key component of Dyna is search-control, the mechanism to generate the state and action from which the agent queries the model, which remains largely unexplored. In this work, we propose to generate such states by using the trajectory obtained from Hill Climbing (HC) the current estimate of the value function. This has the effect of propagating value from high-value regions and of preemptively updating value estimates of the regions that the agent is likely to visit next. We derive a noisy projected natural gradient algorithm for hill climbing, and highlight a connection to Langevin dynamics. We provide an empirical demonstration on four classical domains that our algorithm, HC-Dyna, can obtain significant sample efficiency improvements. We study the properties of different sampling distributions for search-control, and find that there appears to be a benefit specifically from using the samples generated by climbing on current value estimates from low-value to high-value region.\",\"arxivId\":\"1906.07791\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\",\"url\":\"https://www.semanticscholar.org/author/7303313\"},{\"authorId\":\"40609469\",\"name\":\"Hengshuai Yao\",\"url\":\"https://www.semanticscholar.org/author/40609469\"},{\"authorId\":\"5689899\",\"name\":\"Amir-massoud Farahmand\",\"url\":\"https://www.semanticscholar.org/author/5689899\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\",\"url\":\"https://www.semanticscholar.org/author/144542337\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"eb5f4f0cb8503beefdf8a5efd5061f61cc64764e\",\"title\":\"F REQUENCY-BASED S EARCH-CONTROL IN D YNA\",\"url\":\"https://www.semanticscholar.org/paper/eb5f4f0cb8503beefdf8a5efd5061f61cc64764e\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2007.09569\",\"authors\":[{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"},{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"114860989\",\"name\":\"M. White\"},{\"authorId\":\"5689899\",\"name\":\"Amir-massoud Farahmand\"},{\"authorId\":\"40609469\",\"name\":\"Hengshuai Yao\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2dd22a920c224a130979e0ad4ceec7f28b4e7c03\",\"title\":\"Beyond Prioritized Replay: Sampling States in Model-Based RL via Simulated Priorities\",\"url\":\"https://www.semanticscholar.org/paper/2dd22a920c224a130979e0ad4ceec7f28b4e7c03\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.05822\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"3288319\",\"name\":\"Jincheng Mei\"},{\"authorId\":\"5689899\",\"name\":\"Amir-massoud Farahmand\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"fe1b8c54eb4a53a4893acd8779cf17fadefde9e5\",\"title\":\"Frequency-based Search-control in Dyna\",\"url\":\"https://www.semanticscholar.org/paper/fe1b8c54eb4a53a4893acd8779cf17fadefde9e5\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"2001.00102\",\"authors\":[{\"authorId\":\"47780630\",\"name\":\"Baoxiang Wang\"},{\"authorId\":\"47319689\",\"name\":\"Shuai Li\"},{\"authorId\":\"10297509\",\"name\":\"J. Li\"},{\"authorId\":\"35207380\",\"name\":\"Siu On Chan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0bfc9a965aa4429afa92000404d61cb535734ec\",\"title\":\"The Gambler's Problem and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/a0bfc9a965aa4429afa92000404d61cb535734ec\",\"venue\":\"ICLR\",\"year\":2020}],\"corpusId\":195069200,\"doi\":\"10.24963/ijcai.2019/445\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"d6a114f9a96e6f893bf6b368f18a944b3e423b74\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sander Adam\"},{\"authorId\":null,\"name\":\"Lucian Busoniu\"},{\"authorId\":null,\"name\":\"Robert Babuska. Experience replay for real-time reinforce Systems\"},{\"authorId\":null,\"name\":\"Man\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Cybernetics\",\"url\":\"\",\"venue\":\"pages 201\\u2013212,\",\"year\":2012},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"},{\"authorId\":\"145471664\",\"name\":\"B. Silva\"},{\"authorId\":\"2160071\",\"name\":\"C. Dann\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6318b1950d21add48aff0b3e70ea4841d5ec130c\",\"title\":\"Energetic Natural Gradient Descent\",\"url\":\"https://www.semanticscholar.org/paper/6318b1950d21add48aff0b3e70ea4841d5ec130c\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":\"1802.04325\",\"authors\":[{\"authorId\":\"10724534\",\"name\":\"Dane S. Corneil\"},{\"authorId\":\"1708945\",\"name\":\"W. Gerstner\"},{\"authorId\":\"2804426\",\"name\":\"J. Brea\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5572169a9adeedb99a6c20f0ede09b320d3fec61\",\"title\":\"Efficient Model-Based Deep Reinforcement Learning with Variational State Tabulation\",\"url\":\"https://www.semanticscholar.org/paper/5572169a9adeedb99a6c20f0ede09b320d3fec61\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard L. Roberts\"},{\"authorId\":null,\"name\":\"O. Gareth\"},{\"authorId\":null,\"name\":\"Tweedie. Exponential convergence of langevin distributions\"},{\"authorId\":null,\"name\":\"their discrete approximations\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bernoulli\",\"url\":\"\",\"venue\":\"pages 341\\u2013363,\",\"year\":1996},{\"arxivId\":\"1806.01825\",\"authors\":[{\"authorId\":\"34454457\",\"name\":\"G. Holland\"},{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a1bde6e920ec93d1529a8d60d334c0606a0d79f\",\"title\":\"The Effect of Planning Shape on Dyna-style Planning in High-dimensional State Spaces\",\"url\":\"https://www.semanticscholar.org/paper/7a1bde6e920ec93d1529a8d60d334c0606a0d79f\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1603.00748\",\"authors\":[{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d358d41c69450b171327ebd99462b6afef687269\",\"title\":\"Continuous Deep Q-Learning with Model-based Acceleration\",\"url\":\"https://www.semanticscholar.org/paper/d358d41c69450b171327ebd99462b6afef687269\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abadi et al\"},{\"authorId\":null,\"name\":\"2015 Mart\\u0131\\u0301n Abadi\"},{\"authorId\":null,\"name\":\"Ashish Agarwal\"},{\"authorId\":null,\"name\":\"Paul Barham\"},{\"authorId\":null,\"name\":\"Eugene Brevdo\"},{\"authorId\":null,\"name\":\"Zhifeng Chen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"TensorFlow: Large-scale machine learning on heterogeneous systems\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"A. Andrei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Rusu , and et al . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102735693\",\"name\":\"Shun-ichi Amari\"}],\"doi\":\"10.1162/089976698300017746\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5a767a341364de1f75bea85e0b12ba7d3586a461\",\"title\":\"Natural Gradient Works Efficiently in Learning\",\"url\":\"https://www.semanticscholar.org/paper/5a767a341364de1f75bea85e0b12ba7d3586a461\",\"venue\":\"Neural Computation\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A. Rusu\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and et al\",\"url\":\"\",\"venue\":\"Human-level control through deep reinforcement learning. Nature,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dane S. Corneil\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Wulfram Gerstner , and Johanni Brea . Efficient model - based deep reinforcement learning with variational state tabulation Nonasymptotic convergence analysis for the unadjusted Langevin algorithm\",\"url\":\"\",\"venue\":\"The Annals of Applied Probability\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":\"10.1023/A:1022689125041\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"621c03cd67b0b7bac665f3c7887481b4b42f269c\",\"title\":\"Asynchronous Stochastic Approximation and Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/621c03cd67b0b7bac665f3c7887481b4b42f269c\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30402784\",\"name\":\"Sander Adam\"},{\"authorId\":\"1709638\",\"name\":\"L. Busoniu\"},{\"authorId\":\"1705222\",\"name\":\"Robert Babu\\u0161ka\"}],\"doi\":\"10.1109/TSMCC.2011.2106494\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b1d730ad00d877b5ac6a1abb720483945a1904fd\",\"title\":\"Experience Replay for Real-Time Reinforcement Learning Control\",\"url\":\"https://www.semanticscholar.org/paper/b1d730ad00d877b5ac6a1abb720483945a1904fd\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Zacharias Holland\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Erik Talvitie\",\"url\":\"\",\"venue\":\"and Michael Bowling. The effect of planning shape on dyna-style planning in high-dimensional state spaces. CoRR, abs/1806.01825\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"title\":\"Deterministic Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398948869\",\"name\":\"Richard S. Sutton\"}],\"doi\":\"10.1145/122344.122377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"831edc3d67457db83da40d260e93bfd7559347ae\",\"title\":\"Dyna, an integrated architecture for learning, planning, and reacting\",\"url\":\"https://www.semanticscholar.org/paper/831edc3d67457db83da40d260e93bfd7559347ae\",\"venue\":\"SGAR\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Max Welling\"},{\"authorId\":null,\"name\":\"Yee Whye Teh. Bayesian learning via stochastic gradien dynamics\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 681\\u2013688,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3161430\",\"name\":\"M. Barbehenn\"},{\"authorId\":\"144193296\",\"name\":\"S. Hutchinson\"}],\"doi\":\"10.1145/122344.122348\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1055230bbec058919c9f22622884f93efa28e3ef\",\"title\":\"An integrated architecture for learning and planning in robotic domains\",\"url\":\"https://www.semanticscholar.org/paper/1055230bbec058919c9f22622884f93efa28e3ef\",\"venue\":\"SGAR\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1206.3285\",\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"1979505\",\"name\":\"A. Geramifard\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eaf557c627d441deb0b4b5e033a3a4f5518f4715\",\"title\":\"Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping\",\"url\":\"https://www.semanticscholar.org/paper/eaf557c627d441deb0b4b5e033a3a4f5518f4715\",\"venue\":\"UAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dane S. Corneil\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Wulfram Gerstner , and Johanni Brea . Efficient model - based deep reinforcement learning with variational state tabulation Nonasymptotic convergence analysis for the unadjusted Langevin algorithm\",\"url\":\"\",\"venue\":\"The Annals of Applied Probability\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tzuu-Shuh Chiang\"},{\"authorId\":null,\"name\":\"Chii-Ruey Hwang\"},{\"authorId\":null,\"name\":\"R ShuennJyiSheu.Diffusionforglobaloptimizationin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"SIAM Journal on Control and Optimization\",\"url\":\"\",\"venue\":\"pages 737\\u2013753,\",\"year\":1987},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66856590\",\"name\":\"Jstor\"},{\"authorId\":\"69381319\",\"name\":\"Euclid\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d4acfde6f428885fb59b0aa313dec6e0a3c4492\",\"title\":\"The annals of applied probability : an official journal of the Institute of Mathematical Statistics.\",\"url\":\"https://www.semanticscholar.org/paper/1d4acfde6f428885fb59b0aa313dec6e0a3c4492\",\"venue\":\"\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741147143\",\"name\":\"Vijay Kumar Kaul\"}],\"doi\":\"10.2307/j.ctt5hh7c0.12\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff62aa2bfd3b4db63307c8a684e3c4edb39d990c\",\"title\":\"Planning\",\"url\":\"https://www.semanticscholar.org/paper/ff62aa2bfd3b4db63307c8a684e3c4edb39d990c\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98241663\",\"name\":\"M. V. Rossum\"}],\"doi\":\"10.1142/9789814360784_0003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"title\":\"Neural Computation\",\"url\":\"https://www.semanticscholar.org/paper/2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2592414\",\"name\":\"Harm Vanseijen\"},{\"authorId\":\"47549459\",\"name\":\"Rich Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"22dc2f3a4afea29ce76fb02a156943afadd6cbd7\",\"title\":\"A Deeper Look at Planning as Learning from Replay\",\"url\":\"https://www.semanticscholar.org/paper/22dc2f3a4afea29ce76fb02a156943afadd6cbd7\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shixiang Gu\"},{\"authorId\":null,\"name\":\"Timothy P. Lillicrap\"},{\"authorId\":null,\"name\":\"Ilya Sutskever\"},{\"authorId\":null,\"name\":\"Sergey Levine. Continuous Deep QLearning with Model-based Acceleration\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 2829\\u20132838,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Guy Lever\"},{\"authorId\":null,\"name\":\"Nicolas Heess\"},{\"authorId\":null,\"name\":\"Thomas Degris\"},{\"authorId\":null,\"name\":\"Daan Wierstra\"},{\"authorId\":null,\"name\":\"Martin Riedmiller. Deterministic policy gradient algorithms\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages I\\u2013387\\u2013I\\u2013395,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Harm van Seijen\"},{\"authorId\":null,\"name\":\"Richard S. Sutton. A deeper look at planning as learning f replay\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 2314\\u20132322,\",\"year\":2015},{\"arxivId\":\"1101.4003\",\"authors\":[{\"authorId\":\"1757449\",\"name\":\"M. Pe\\u00f1as\"},{\"authorId\":\"8503739\",\"name\":\"H. Jos\\u00e9AntonioMart\\u00edn\"},{\"authorId\":\"145477812\",\"name\":\"V. L\\u00f3pez\"},{\"authorId\":\"7736653\",\"name\":\"G. Juan\"}],\"doi\":\"10.1016/j.knosys.2011.09.008\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"257acab3703552c26aeb42623486b428e480c8ea\",\"title\":\"Dyna-H: A heuristic planning reinforcement learning algorithm applied to role-playing game strategy decision systems\",\"url\":\"https://www.semanticscholar.org/paper/257acab3703552c26aeb42623486b428e480c8ea\",\"venue\":\"Knowl. Based Syst.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145615520\",\"name\":\"J. Peng\"},{\"authorId\":\"122440816\",\"name\":\"R. Williams\"}],\"doi\":\"10.1109/ICNN.1993.298551\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c980493673bad9bcb888dd18788d2b4b3ecb7a2e\",\"title\":\"Efficient learning and planning within the Dyna framework\",\"url\":\"https://www.semanticscholar.org/paper/c980493673bad9bcb888dd18788d2b4b3ecb7a2e\",\"venue\":\"IEEE International Conference on Neural Networks\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1016/b978-1-55860-141-3.50030-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b5f8a0858fb82ce0e50b55446577a70e40137aaf\",\"title\":\"Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/b5f8a0858fb82ce0e50b55446577a70e40137aaf\",\"venue\":\"ML\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3119801\",\"name\":\"Xavier Glorot\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b71ac1e9fb49420d13e084ac67254a0bbd40f83f\",\"title\":\"Understanding the difficulty of training deep feedforward neural networks\",\"url\":\"https://www.semanticscholar.org/paper/b71ac1e9fb49420d13e084ac67254a0bbd40f83f\",\"venue\":\"AISTATS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144362425\",\"name\":\"S. Amari\"},{\"authorId\":\"48988892\",\"name\":\"S. Douglas\"}],\"doi\":\"10.1109/ICASSP.1998.675489\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"64eecad63f8256c60388f39c31b30cf5a802d6fa\",\"title\":\"Why natural gradient?\",\"url\":\"https://www.semanticscholar.org/paper/64eecad63f8256c60388f39c31b30cf5a802d6fa\",\"venue\":\"Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1678311\",\"name\":\"M. Welling\"},{\"authorId\":\"1725303\",\"name\":\"Y. Teh\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"aeed631d6a84100b5e9a021ec1914095c66de415\",\"title\":\"Bayesian Learning via Stochastic Gradient Langevin Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/aeed631d6a84100b5e9a021ec1914095c66de415\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"A Andrei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Rusu , and et al . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Self - Improving Reactive Agents Based On Reinforcement Learning , Planning and Teaching . Machine Learning ,\",\"year\":1992},{\"arxivId\":\"1806.04624\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"145690938\",\"name\":\"Andrew Patterson\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":\"10.24963/ijcai.2018/666\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"title\":\"Organizing Experience: a Deeper Look at Replay Mechanisms for Sample-Based Planning in Continuous State Domains\",\"url\":\"https://www.semanticscholar.org/paper/ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Long-Ji Lin. Self-Improving Reactive Agents Based On Reinf Learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Planning and Teaching\",\"url\":\"\",\"venue\":\"Machine Learning,\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Nicolas Heess Guy Lever\"},{\"authorId\":null,\"name\":\"Daan Wierstra Thomas Degris\"},{\"authorId\":null,\"name\":\"Martin Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"terministic policy gradient algorithms\",\"url\":\"\",\"venue\":\"Reinforcement Learning : An Introduction\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tom Schaul\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", John Quan , Ioannis Antonoglou , and David Silver . Prioritized Experience Replay\",\"url\":\"\",\"venue\":\"International Conference on Machine Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sander Adam\"},{\"authorId\":null,\"name\":\"Lucian Busoniu\"},{\"authorId\":null,\"name\":\"Robert Babuska. Experience replay for real-time reinforce Systems\"},{\"authorId\":null,\"name\":\"Man\"},{\"authorId\":null,\"name\":\"Cybernetics\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Part C (Applications and Reviews)\",\"url\":\"\",\"venue\":\"42(2):201\\u2013212, March\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dane S. Corneil\"},{\"authorId\":null,\"name\":\"Wulfram Gerstner\"},{\"authorId\":null,\"name\":\"Johanni Brea. Efficient model-based deep reinforcement lea tabulation\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 1049\\u20131058,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114788213\",\"name\":\"D. Signorini\"},{\"authorId\":\"4137433\",\"name\":\"J. Slattery\"},{\"authorId\":\"12113526\",\"name\":\"S. Dodds\"},{\"authorId\":\"26151757\",\"name\":\"V. Lane\"},{\"authorId\":\"143842868\",\"name\":\"P. Littlejohns\"}],\"doi\":\"10.1016/S0140-6736(95)92525-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"20b844e395355b40fa5940c61362ec40e56027aa\",\"title\":\"Neural networks\",\"url\":\"https://www.semanticscholar.org/paper/20b844e395355b40fa5940c61362ec40e56027aa\",\"venue\":\"The Lancet\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S. Sutton\"},{\"authorId\":null,\"name\":\"Csaba Szepesv\\u00e1ri\"},{\"authorId\":null,\"name\":\"Alborz Geramifard\"},{\"authorId\":null,\"name\":\"Michael Bowling. Dyna-style planning with linear function approximation\"},{\"authorId\":null,\"name\":\"prioritized sweeping\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In UAI\",\"url\":\"\",\"venue\":\"pages 528\\u2013536,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dane S. Corneil\"},{\"authorId\":null,\"name\":\"Wulfram Gerstner\"},{\"authorId\":null,\"name\":\"Johanni Brea. Efficient model-based deep reinforcement lea tabulation\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 1049\\u20131058,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"73355331\",\"name\":\"I. Sato\"},{\"authorId\":\"145153059\",\"name\":\"H. Nakagawa\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5cb36159a1bda5bd82ff4d04f1bd33a8b290d3eb\",\"title\":\"Approximation Analysis of Stochastic Gradient Langevin Dynamics by using Fokker-Planck Equation and Ito Process\",\"url\":\"https://www.semanticscholar.org/paper/5cb36159a1bda5bd82ff4d04f1bd33a8b290d3eb\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1007/BF00992699\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"title\":\"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\"url\":\"https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32174584\",\"name\":\"T. Chiang\"},{\"authorId\":\"1955900\",\"name\":\"Chii-Ruey Hwang\"}],\"doi\":\"10.1137/0325042\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"50f920c6d837d0ace68a4f67adb11edfd9555082\",\"title\":\"Diffusion for global optimization in R n\",\"url\":\"https://www.semanticscholar.org/paper/50f920c6d837d0ace68a4f67adb11edfd9555082\",\"venue\":\"\",\"year\":1987},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760402\",\"name\":\"A. Moore\"},{\"authorId\":\"8483722\",\"name\":\"C. Atkeson\"}],\"doi\":\"10.1023/A:1022635613229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e21956fdbc06204db7984aacea09db7eda6355ad\",\"title\":\"Prioritized Sweeping: Reinforcement Learning with Less Data and Less Time\",\"url\":\"https://www.semanticscholar.org/paper/e21956fdbc06204db7984aacea09db7eda6355ad\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1801.06176\",\"authors\":[{\"authorId\":\"1780690\",\"name\":\"Baolin Peng\"},{\"authorId\":\"40286474\",\"name\":\"Xiujun Li\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"1726477\",\"name\":\"J. Liu\"},{\"authorId\":\"1784988\",\"name\":\"K. Wong\"}],\"doi\":\"10.18653/v1/P18-1203\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"829e45910ca48c9b9d19492d43ef3c035345c380\",\"title\":\"Integrating planning for task-completion dialogue policy learning\",\"url\":\"https://www.semanticscholar.org/paper/829e45910ca48c9b9d19492d43ef3c035345c380\",\"venue\":\"ACL\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Philip Thomas\"},{\"authorId\":null,\"name\":\"Bruno Castro Silva\"},{\"authorId\":null,\"name\":\"Christoph Dann\"},{\"authorId\":null,\"name\":\"Emma Brunskill. Energetic natural gradient descent\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 2887\\u20132895,\",\"year\":2016},{\"arxivId\":\"1507.05021\",\"authors\":[{\"authorId\":\"35501788\",\"name\":\"Alain Durmus\"},{\"authorId\":\"48139965\",\"name\":\"\\u00c9. Moulines\"}],\"doi\":\"10.1214/16-AAP1238\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"644adda108d0a4c61247f8c7a9c5fac353d18b94\",\"title\":\"Non-asymptotic convergence analysis for the Unadjusted Langevin Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/644adda108d0a4c61247f8c7a9c5fac353d18b94\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3264000\",\"name\":\"G. Roberts\"},{\"authorId\":\"3371359\",\"name\":\"R. Tweedie\"}],\"doi\":\"10.2307/3318418\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0dffd5c04f5830deadd4fba16ee1575abc5ee051\",\"title\":\"Exponential convergence of Langevin distributions and their discrete approximations\",\"url\":\"https://www.semanticscholar.org/paper/0dffd5c04f5830deadd4fba16ee1575abc5ee051\",\"venue\":\"\",\"year\":1996}],\"title\":\"Hill Climbing on Value Estimates for Search-control in Dyna\",\"topics\":[{\"topic\":\"Hill climbing\",\"topicId\":\"51492\",\"url\":\"https://www.semanticscholar.org/topic/51492\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Gradient descent\",\"topicId\":\"24880\",\"url\":\"https://www.semanticscholar.org/topic/24880\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Sampling (signal processing)\",\"topicId\":\"7839\",\"url\":\"https://www.semanticscholar.org/topic/7839\"},{\"topic\":\"Times Ascent\",\"topicId\":\"8454609\",\"url\":\"https://www.semanticscholar.org/topic/8454609\"},{\"topic\":\"Rise time\",\"topicId\":\"112472\",\"url\":\"https://www.semanticscholar.org/topic/112472\"}],\"url\":\"https://www.semanticscholar.org/paper/d6a114f9a96e6f893bf6b368f18a944b3e423b74\",\"venue\":\"IJCAI\",\"year\":2019}\n"