"{\"abstract\":\"Video captioning aims at generating a proper sentence to describe the video content. As a video often includes rich visual content and semantic details, different people may be interested in different views. Thus the generated sentence always fails to meet the ad hoc expectations. In this paper, we make a new attempt that, we launch a round of interaction between a human and a captioning agent. After generating an initial caption, the agent asks for a short prompt from the human as a clue of his expectation. Then, based on the prompt, the agent could generate a more accurate caption. We name this process a new task of video interactive captioning (ViCap). Taking a video and an initial caption as input, we devise the ViCap agent which consists of a video encoder, an initial caption encoder, and a refined caption generator. We show that the ViCap can be trained via a full supervision (with ground-truth) way or a weak supervision (with only prompts) way. For the evaluation of ViCap, we first extend the MSRVTT with interaction ground-truth. Experimental results not only show the prompts can help generate more accurate captions, but also demonstrate the good performance of the proposed method.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"48352212\",\"name\":\"Aming Wu\",\"url\":\"https://www.semanticscholar.org/author/48352212\"},{\"authorId\":\"144622313\",\"name\":\"Yahong Han\",\"url\":\"https://www.semanticscholar.org/author/144622313\"},{\"authorId\":\"65747622\",\"name\":\"Yi Yang\",\"url\":\"https://www.semanticscholar.org/author/65747622\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2001.11782\",\"authors\":[{\"authorId\":\"152584142\",\"name\":\"Zhengxiong Jia\"},{\"authorId\":\"9931285\",\"name\":\"Xirong Li\"}],\"doi\":\"10.1145/3372278.3390697\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"87c7ec86e37178720686eba4d00ea53b2aca93d7\",\"title\":\"iCap: Interactive Image Captioning with Predictive Text\",\"url\":\"https://www.semanticscholar.org/paper/87c7ec86e37178720686eba4d00ea53b2aca93d7\",\"venue\":\"ICMR\",\"year\":2020}],\"corpusId\":199466054,\"doi\":\"10.24963/ijcai.2019/135\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"c38ae47ae73d9287e181c3f7693c6ca69aa0432e\",\"references\":[{\"arxivId\":\"1511.03476\",\"authors\":[{\"authorId\":\"1991108\",\"name\":\"P. Pan\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"39033919\",\"name\":\"Y. Yang\"},{\"authorId\":\"144894849\",\"name\":\"Fei Wu\"},{\"authorId\":\"143749205\",\"name\":\"Y. Zhuang\"}],\"doi\":\"10.1109/CVPR.2016.117\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e9a66904559011d48245bba01e55f72246927e77\",\"title\":\"Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e9a66904559011d48245bba01e55f72246927e77\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yuting Zhang\"},{\"authorId\":null,\"name\":\"Kibok Lee\"},{\"authorId\":null,\"name\":\"Honglak Lee. Augmenting supervised neural networks with un classification\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 612\\u2013621,\",\"year\":2016},{\"arxivId\":\"1805.09461\",\"authors\":[{\"authorId\":\"2180949\",\"name\":\"Yaser Keneshloo\"},{\"authorId\":\"145531789\",\"name\":\"Tian Shi\"},{\"authorId\":\"1755938\",\"name\":\"Naren Ramakrishnan\"},{\"authorId\":\"144417522\",\"name\":\"C. Reddy\"}],\"doi\":\"10.1109/TNNLS.2019.2929141\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"15a06d8601539b5eb6df5baf6bc4c3bdefb34855\",\"title\":\"Deep Reinforcement Learning for Sequence-to-Sequence Models\",\"url\":\"https://www.semanticscholar.org/paper/15a06d8601539b5eb6df5baf6bc4c3bdefb34855\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Subhashini Venugopalan\"},{\"authorId\":null,\"name\":\"Marcus Rohrbach\"},{\"authorId\":null,\"name\":\"Jeffrey Donahue\"},{\"authorId\":null,\"name\":\"Raymond Mooney\"},{\"authorId\":null,\"name\":\"Trevor Darrell\"},{\"authorId\":null,\"name\":\"Kate Saenko. Sequence to sequencevideo to text\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICCV\",\"url\":\"\",\"venue\":\"pages 4534\\u20134542,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pingbo Pan\"},{\"authorId\":null,\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":null,\"name\":\"Fei Wu\"},{\"authorId\":null,\"name\":\"Yueting Zhuang. Hierarchical recurrent neural encoder for  captioning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In CVPR\",\"url\":\"\",\"venue\":\"pages 1029\\u20131038,\",\"year\":2016},{\"arxivId\":\"1611.05592\",\"authors\":[{\"authorId\":\"5482750\",\"name\":\"J. Wang\"},{\"authorId\":null,\"name\":\"Wei Wang\"},{\"authorId\":\"49867037\",\"name\":\"Y. Huang\"},{\"authorId\":null,\"name\":\"Liang Wang\"},{\"authorId\":\"143874948\",\"name\":\"T. Tan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c687986256ce206c93fb78303565bacffb09efe\",\"title\":\"Multimodal Memory Modelling for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/0c687986256ce206c93fb78303565bacffb09efe\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pingbo Pan\"},{\"authorId\":null,\"name\":\"Zhongwen Xu\"},{\"authorId\":null,\"name\":\"Yi Yang\"},{\"authorId\":null,\"name\":\"Fei Wu\"},{\"authorId\":null,\"name\":\"Yueting Zhuang. Hierarchical recurrent neural encoder for captioning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In CVPR\",\"url\":\"\",\"venue\":\"pages 1029\\u20131038,\",\"year\":2016},{\"arxivId\":\"1803.01457\",\"authors\":[{\"authorId\":\"40702813\",\"name\":\"Yangyu Chen\"},{\"authorId\":\"2538306\",\"name\":\"S. Wang\"},{\"authorId\":\"47527850\",\"name\":\"W. Zhang\"},{\"authorId\":\"1689702\",\"name\":\"Q. Huang\"}],\"doi\":\"10.1007/978-3-030-01261-8_22\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"title\":\"Less Is More: Picking Informative Frames for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/d5ff7a4580fbfdecc1d912746eee36980f29278b\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yuting Zhang\"},{\"authorId\":null,\"name\":\"Kibok Lee\"},{\"authorId\":null,\"name\":\"Honglak Lee. Augmenting supervised neural networks with un classification\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 612\\u2013621,\",\"year\":2016},{\"arxivId\":\"1705.03122\",\"authors\":[{\"authorId\":\"2401865\",\"name\":\"Jonas Gehring\"},{\"authorId\":\"2325985\",\"name\":\"M. Auli\"},{\"authorId\":\"2529182\",\"name\":\"David Grangier\"},{\"authorId\":\"13759615\",\"name\":\"Denis Yarats\"},{\"authorId\":\"2921469\",\"name\":\"Yann Dauphin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43428880d75b3a14257c3ee9bda054e61eb869c0\",\"title\":\"Convolutional Sequence to Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/43428880d75b3a14257c3ee9bda054e61eb869c0\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1809.04560\",\"authors\":[{\"authorId\":\"10721120\",\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":\"143977268\",\"name\":\"Mohit Bansal\"}],\"doi\":\"10.18653/v1/D18-1012\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"667a6eea4c3039d4d1bde2ebf4f2fe8bcfa4af23\",\"title\":\"Game-Based Video-Context Dialogue\",\"url\":\"https://www.semanticscholar.org/paper/667a6eea4c3039d4d1bde2ebf4f2fe8bcfa4af23\",\"venue\":\"EMNLP\",\"year\":2018},{\"arxivId\":\"1809.04094\",\"authors\":[{\"authorId\":\"1403953272\",\"name\":\"Giorgos Kordopatis-Zilos\"},{\"authorId\":\"144178604\",\"name\":\"S. Papadopoulos\"},{\"authorId\":\"50058816\",\"name\":\"I. Patras\"},{\"authorId\":\"119661806\",\"name\":\"I. Kompatsiaris\"}],\"doi\":\"10.1109/TMM.2019.2905741\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"820db37272e0d76f554f16a3a9cbbf1ebad6539d\",\"title\":\"FIVR: Fine-Grained Incident Video Retrieval\",\"url\":\"https://www.semanticscholar.org/paper/820db37272e0d76f554f16a3a9cbbf1ebad6539d\",\"venue\":\"IEEE Transactions on Multimedia\",\"year\":2019},{\"arxivId\":\"1611.09312\",\"authors\":[{\"authorId\":\"1843795\",\"name\":\"L. Baraldi\"},{\"authorId\":\"153925540\",\"name\":\"C. Grana\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/CVPR.2017.339\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"title\":\"Hierarchical Boundary-Aware Neural Encoder for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/726b1ade8b3d0023f0b4a9f86b7c2c3004885e37\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":null,\"name\":\"Wenhu Chen\"},{\"authorId\":null,\"name\":\"Jiawei Wu\"},{\"authorId\":null,\"name\":\"Yuan-Fang Wang\"},{\"authorId\":null,\"name\":\"William Yang Wang. Video captioning via hierarchical reinf learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In CVPR\",\"url\":\"\",\"venue\":\"pages 4213\\u20134222,\",\"year\":2018},{\"arxivId\":\"1411.5726\",\"authors\":[{\"authorId\":\"8137017\",\"name\":\"Ramakrishna Vedantam\"},{\"authorId\":\"1699161\",\"name\":\"C. L. Zitnick\"},{\"authorId\":\"153432684\",\"name\":\"D. Parikh\"}],\"doi\":\"10.1109/CVPR.2015.7299087\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"258986132bf17755fe8263e42429fe73218c1534\",\"title\":\"CIDEr: Consensus-based image description evaluation\",\"url\":\"https://www.semanticscholar.org/paper/258986132bf17755fe8263e42429fe73218c1534\",\"venue\":\"2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2015},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1620412960\",\"name\":\"Aravaipa Canyon Basin\"}],\"doi\":\"10.1023/A:1017141413812\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b25cc9ad99bd18c68d09065904326ce1d062bffe\",\"title\":\"Volume 3\",\"url\":\"https://www.semanticscholar.org/paper/b25cc9ad99bd18c68d09065904326ce1d062bffe\",\"venue\":\"\",\"year\":1998},{\"arxivId\":\"1601.06759\",\"authors\":[{\"authorId\":\"3422336\",\"name\":\"A. Oord\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"41f1d50c85d3180476c4c7b3eea121278b0d8474\",\"title\":\"Pixel Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/41f1d50c85d3180476c4c7b3eea121278b0d8474\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1720243\",\"name\":\"X. Li\"},{\"authorId\":\"143946808\",\"name\":\"Bin Zhao\"},{\"authorId\":\"7828998\",\"name\":\"Xiaoqiang Lu\"}],\"doi\":\"10.24963/ijcai.2017/307\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e33bc5c83f2cea403a5521385ee8e2794b311275\",\"title\":\"MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning\",\"url\":\"https://www.semanticscholar.org/paper/e33bc5c83f2cea403a5521385ee8e2794b311275\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":\"1606.06582\",\"authors\":[{\"authorId\":\"145489055\",\"name\":\"Y. Zhang\"},{\"authorId\":\"2208511\",\"name\":\"Kibok Lee\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"883eef83e407434dfbda09a2276120b28628429a\",\"title\":\"Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-scale Image Classification\",\"url\":\"https://www.semanticscholar.org/paper/883eef83e407434dfbda09a2276120b28628429a\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1797808\",\"name\":\"G. Salton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"939d409a615d4b88c0a84e1f9b99ed67a9053208\",\"title\":\"The SMART Retrieval System\\u2014Experiments in Automatic Document Processing\",\"url\":\"https://www.semanticscholar.org/paper/939d409a615d4b88c0a84e1f9b99ed67a9053208\",\"venue\":\"\",\"year\":1971},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aming Wu\"},{\"authorId\":null,\"name\":\"Yahong Han. Multimodal circulant fusion for video-to-language\"},{\"authorId\":null,\"name\":\"backward. In IJCAI\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 3\",\"url\":\"\",\"venue\":\"page 8,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xin Wang\"},{\"authorId\":null,\"name\":\"Wenhu Chen\"},{\"authorId\":null,\"name\":\"Jiawei Wu\"},{\"authorId\":null,\"name\":\"Yuan-Fang Wang\"},{\"authorId\":null,\"name\":\"William Yang Wang. Video captioning via hierarchical reinf learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In CVPR\",\"url\":\"\",\"venue\":\"pages 4213\\u20134222,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145971173\",\"name\":\"J. Xu\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"145459057\",\"name\":\"Y. Rui\"}],\"doi\":\"10.1109/CVPR.2016.571\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"title\":\"MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\",\"url\":\"https://www.semanticscholar.org/paper/b8e2e9f3ba008e28257195ec69a00e07f260131d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":\"1511.07122\",\"authors\":[{\"authorId\":\"1807197\",\"name\":\"F. Yu\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7f5fc84819c0cf94b771fe15141f65b123f7b8ec\",\"title\":\"Multi-Scale Context Aggregation by Dilated Convolutions\",\"url\":\"https://www.semanticscholar.org/paper/7f5fc84819c0cf94b771fe15141f65b123f7b8ec\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3323275\",\"name\":\"Kishore Papineni\"},{\"authorId\":\"1781292\",\"name\":\"S. Roukos\"},{\"authorId\":\"144582029\",\"name\":\"T. Ward\"},{\"authorId\":\"2587983\",\"name\":\"Wei-Jing Zhu\"}],\"doi\":\"10.3115/1073083.1073135\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d7da009f457917aa381619facfa5ffae9329a6e9\",\"title\":\"Bleu: a Method for Automatic Evaluation of Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/d7da009f457917aa381619facfa5ffae9329a6e9\",\"venue\":\"ACL\",\"year\":2002},{\"arxivId\":\"1611.07675\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"2053452\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1109/CVPR.2017.111\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"title\":\"Video Captioning with Transferred Semantic Attributes\",\"url\":\"https://www.semanticscholar.org/paper/0d3b5ffff118326fea73341a86a7c29423eb95f0\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sang Phan\"},{\"authorId\":null,\"name\":\"Gustav Eje Henter\"},{\"authorId\":null,\"name\":\"Yusuke Miyao\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Shin\\u2019ichi Satoh\",\"url\":\"\",\"venue\":\"Consensus-based sequence training for video captioning. arXiv preprint arXiv:1712.09532,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"145467703\",\"name\":\"P. Vincent\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f8c8619ea7d68e604e40b814b40c72888a755e95\",\"title\":\"Unsupervised Feature Learning and Deep Learning: A Review and New Perspectives\",\"url\":\"https://www.semanticscholar.org/paper/f8c8619ea7d68e604e40b814b40c72888a755e95\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2157958\",\"name\":\"Michael J. Denkowski\"},{\"authorId\":\"1784914\",\"name\":\"A. Lavie\"}],\"doi\":\"10.3115/v1/W14-3348\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26adb749fc5d80502a6d889966e50b31391560d3\",\"title\":\"Meteor Universal: Language Specific Translation Evaluation for Any Target Language\",\"url\":\"https://www.semanticscholar.org/paper/26adb749fc5d80502a6d889966e50b31391560d3\",\"venue\":\"WMT@ACL\",\"year\":2014},{\"arxivId\":\"1610.09038\",\"authors\":[{\"authorId\":\"1996705\",\"name\":\"Anirudh Goyal\"},{\"authorId\":\"49071560\",\"name\":\"Alex Lamb\"},{\"authorId\":\"1774002\",\"name\":\"Y. Zhang\"},{\"authorId\":\"35097114\",\"name\":\"Saizheng Zhang\"},{\"authorId\":\"1760871\",\"name\":\"Aaron C. Courville\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"db38edba294b7d2fd8ca3aad65721bd9dce32619\",\"title\":\"Professor Forcing: A New Algorithm for Training Recurrent Networks\",\"url\":\"https://www.semanticscholar.org/paper/db38edba294b7d2fd8ca3aad65721bd9dce32619\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Li Yao\"},{\"authorId\":null,\"name\":\"Atousa Torabi\"},{\"authorId\":null,\"name\":\"Kyunghyun Cho\"},{\"authorId\":null,\"name\":\"Nicolas Ballas\"},{\"authorId\":null,\"name\":\"Christopher Pal\"},{\"authorId\":null,\"name\":\"Hugo Larochelle\"},{\"authorId\":null,\"name\":\"Aaron Courville. Describing videos by exploiting tempora structure\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICCV\",\"url\":\"\",\"venue\":\"pages 4507\\u20134515,\",\"year\":2015}],\"title\":\"Video Interactive Captioning with Human Prompts\",\"topics\":[{\"topic\":\"Digital video\",\"topicId\":\"44670\",\"url\":\"https://www.semanticscholar.org/topic/44670\"},{\"topic\":\"Encoder\",\"topicId\":\"16744\",\"url\":\"https://www.semanticscholar.org/topic/16744\"},{\"topic\":\"Hoc (programming language)\",\"topicId\":\"3446\",\"url\":\"https://www.semanticscholar.org/topic/3446\"},{\"topic\":\"Ground truth\",\"topicId\":\"33313\",\"url\":\"https://www.semanticscholar.org/topic/33313\"}],\"url\":\"https://www.semanticscholar.org/paper/c38ae47ae73d9287e181c3f7693c6ca69aa0432e\",\"venue\":\"IJCAI\",\"year\":2019}\n"