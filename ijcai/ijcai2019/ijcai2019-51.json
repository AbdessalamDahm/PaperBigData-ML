"{\"abstract\":\"Coordination on an optimal policy between independent learners in fully cooperative stochastic games is difficult due to problems such as relative overgeneralization and miscoordination. Most state-of-the-art algorithms apply fusion heuristics on agents\\u2019 optimistic and average rewards, by which coordination between agents can be achieved implicitly. However, such implicit coordination faces practical issues such as tedious parametertuning in real world applications. The lack of an explicit coordination mechanism may also lead to a low likelihood of coordination in problems with multiple optimal policies. Based on the necessary conditions of an optimal policy, we propose the explicitly coordinated policy iteration (EXCEL) algorithm which always forces agents to coordinate by comparing the agents\\u2019 separated optimistic and average value functions. We also propose three solutions for deep reinforcement learning extensions of EXCEL. Extensive experiments in matrix games (from 2-agent 2-action games to 5-agent 20-action games) and stochastic games (from 2-agent games to 5-agent games) show that EXCEL has better performance than the state-of-the-art algorithms (such as faster convergence and better coordination).\",\"arxivId\":null,\"authors\":[{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\",\"url\":\"https://www.semanticscholar.org/author/1776850\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\",\"url\":\"https://www.semanticscholar.org/author/2519427\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\",\"url\":\"https://www.semanticscholar.org/author/3120655\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\",\"url\":\"https://www.semanticscholar.org/author/40513470\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.03204\",\"authors\":[{\"authorId\":\"50982491\",\"name\":\"Hangtian Jia\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"81185915\",\"name\":\"Chunxu Ren\"},{\"authorId\":\"80892810\",\"name\":\"Tangjie Lv\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8922d5cd990903e594bcd4de96c6251361919c70\",\"title\":\"Fever Basketball: A Complex, Flexible, and Asynchronized Sports Game Environment for Multi-agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8922d5cd990903e594bcd4de96c6251361919c70\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199466037,\"doi\":\"10.24963/ijcai.2019/51\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"3349ef4018356f874cb2d8d34435a1fbc12de9c3\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gregory Palmer\"},{\"authorId\":null,\"name\":\"Karl Tuyls\"},{\"authorId\":null,\"name\":\"Daan Bloembergen\"},{\"authorId\":null,\"name\":\"Rahul Savani. Lenient multi-agent deep reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS\\u201918)\",\"url\":\"\",\"venue\":\"pages 443\\u2013451,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"La\\u00ebtitia Matignon\"},{\"authorId\":null,\"name\":\"Guillaume J Laurent\"},{\"authorId\":null,\"name\":\"Nadine Le Fort-Piat. Coordination of independent learners report\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Institut FEMTO-ST\",\"url\":\"\",\"venue\":\"UniversitP\\u00e9 de Franche-Comt\\u00e9,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144497046\",\"name\":\"N. Nilsson\"}],\"doi\":\"10.7551/mitpress/11723.003.0006\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b886f2c097b635ee9550ca29fff7dcbbb7727ff7\",\"title\":\"Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b886f2c097b635ee9550ca29fff7dcbbb7727ff7\",\"venue\":\"IFIP Congress\",\"year\":1974},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71101785\",\"name\":\"La\\u00ebtitia Matignon\"},{\"authorId\":\"20710580\",\"name\":\"G. Laurent\"},{\"authorId\":\"1400816398\",\"name\":\"N. L. Fort-Piat\"}],\"doi\":\"10.1109/IROS.2007.4399095\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce8ad8b3dd21ff33c79f249ce85ac5856592a913\",\"title\":\"Hysteretic q-learning :an algorithm for decentralized reinforcement learning in cooperative multi-agent teams\",\"url\":\"https://www.semanticscholar.org/paper/ce8ad8b3dd21ff33c79f249ce85ac5856592a913\",\"venue\":\"2007 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jakob N. Foerster\"},{\"authorId\":null,\"name\":\"Gregory Farquhar\"},{\"authorId\":null,\"name\":\"Triantafyllos Afouras\"},{\"authorId\":null,\"name\":\"Nantas Nardelli\"},{\"authorId\":null,\"name\":\"Shimon Whiteson. Counterfactual multi-agent policy gradients\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (AAAI\\u201918)\",\"url\":\"\",\"venue\":\"pages 2974\\u20132982,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ming Tan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Multi-agent reinforcement learning: Independent vs\",\"url\":\"\",\"venue\":\"cooperative agents. In Proceedings of the 10th International Conference on Machine Learning, volume 337, pages 330\\u2013337,\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144773003\",\"name\":\"R. Wiegand\"},{\"authorId\":\"145502027\",\"name\":\"K. Jong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4146b8baeb01d734e40f3be7ddf74bf98d2f930c\",\"title\":\"An analysis of cooperative coevolutionary algorithms\",\"url\":\"https://www.semanticscholar.org/paper/4146b8baeb01d734e40f3be7ddf74bf98d2f930c\",\"venue\":\"\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Junling Hu\"},{\"authorId\":null,\"name\":\"Michael P. Wellman. Nash Q-learning for general-sum stocha games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"4:1039\\u20131069,\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144771028\",\"name\":\"C. Claus\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"38d35d5581e58dca4e9458501e65c1f85ca754d5\",\"title\":\"The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems\",\"url\":\"https://www.semanticscholar.org/paper/38d35d5581e58dca4e9458501e65c1f85ca754d5\",\"venue\":\"AAAI/IAAI\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":null,\"name\":\"Jason Pazis\"},{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"P Jonathan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Joel Veness , Marc G . Bellemare , Alex Graves , Martin Riedmiller , Andreas K . Fidjeland , Georg Ostrovski , et al . Human - Level Control Through Deep Reinforcement Learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71101785\",\"name\":\"La\\u00ebtitia Matignon\"},{\"authorId\":\"20710580\",\"name\":\"G. Laurent\"},{\"authorId\":\"1400816398\",\"name\":\"N. L. Fort-Piat\"}],\"doi\":\"10.1017/S0269888912000057\",\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"9c6933244bcf31ce8a05a1e4ee0ec6d015416616\",\"title\":\"Independent reinforcement learners in cooperative Markov games: a survey regarding coordination problems\",\"url\":\"https://www.semanticscholar.org/paper/9c6933244bcf31ce8a05a1e4ee0ec6d015416616\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martin Lauer\"},{\"authorId\":null,\"name\":\"Martin Ried-miller\"},{\"authorId\":null,\"name\":\"L Michael\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Kapetanakis and Kudenko , 2002 ] Spiros Kapetanakis and Daniel Kudenko . Reinforcement learning of coordination in cooperative multi - agent systems\",\"url\":\"\",\"venue\":\"Proceedings of the 18 th National Conference on Artificial Intelligence ( AAAI \\u2019 02 )\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ermo Wei\"},{\"authorId\":null,\"name\":\"Sean Luke. Lenient learning in independent-learner stoc games\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):2914\\u20132955,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christopher J. C. H. Watkins\"},{\"authorId\":null,\"name\":\"Ermo Wei\"},{\"authorId\":null,\"name\":\"Sean Luke\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Multi - agent reinforcement learning : Independent vs . cooperative agents\",\"url\":\"\",\"venue\":\"Proceedings of the 10 th International Conference on Machine Learning\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Liviu Panait\"},{\"authorId\":null,\"name\":\"Keith Sullivan\"},{\"authorId\":null,\"name\":\"Sean Luke. Lenient learners in cooperative multiagent systems\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 15th International Conference on Autonomous Agents and Multi-Agent Systems (AAMAS\\u201906)\",\"url\":\"\",\"venue\":\"pages 801\\u2013803,\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1703826\",\"name\":\"Liviu Panait\"},{\"authorId\":\"153850291\",\"name\":\"K. Sullivan\"},{\"authorId\":\"1706276\",\"name\":\"S. Luke\"}],\"doi\":\"10.1145/1160633.1160776\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1fd14e5e41c19bab693c80ef52f9720692a5e348\",\"title\":\"Lenient learners in cooperative multiagent systems\",\"url\":\"https://www.semanticscholar.org/paper/1fd14e5e41c19bab693c80ef52f9720692a5e348\",\"venue\":\"AAMAS '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gregory Palmer\"},{\"authorId\":null,\"name\":\"Karl Tuyls\"},{\"authorId\":null,\"name\":\"Daan Bloembergen\"},{\"authorId\":null,\"name\":\"Rahul Savani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"How , and John Vian . Deep Decentralized Multitask Multi - Agent Reinforcement Learning Under Partial Observability\",\"url\":\"\",\"venue\":\"Proceedings of the 34 th International Conference on Machine Learning ( ICML \\u2019 17 )\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701353\",\"name\":\"Y. Shoham\"},{\"authorId\":\"38076874\",\"name\":\"R. Powers\"},{\"authorId\":\"3050250\",\"name\":\"Trond Grenager\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a69e33ce29c451655d917bf9387ed57e115fcfc7\",\"title\":\"Multi-Agent Reinforcement Learning:a critical survey\",\"url\":\"https://www.semanticscholar.org/paper/a69e33ce29c451655d917bf9387ed57e115fcfc7\",\"venue\":\"\",\"year\":2003},{\"arxivId\":\"1703.06182\",\"authors\":[{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"3140253\",\"name\":\"Jason Pazis\"},{\"authorId\":\"34903901\",\"name\":\"Chris Amato\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"},{\"authorId\":\"46956290\",\"name\":\"J. Vian\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b68673a166f9c620e13152f63d358fb8fce7850d\",\"title\":\"Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability\",\"url\":\"https://www.semanticscholar.org/paper/b68673a166f9c620e13152f63d358fb8fce7850d\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matignon et al\"},{\"authorId\":null,\"name\":\"2008 La\\u00ebtitia Matignon\"},{\"authorId\":null,\"name\":\"Guillaume Laurent\"},{\"authorId\":null,\"name\":\"Nadine Le Fort-Piat\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A study of FMQ heuristic\",\"url\":\"\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":null,\"name\":\"Jason Pazis\"},{\"authorId\":null,\"name\":\"Christopher Amato\"},{\"authorId\":null,\"name\":\"P Jonathan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Joel Veness , Marc G . Bellemare , Alex Graves , Martin Riedmiller , Andreas K . Fidjeland , Georg Ostrovski , et al . Human - Level Control Through Deep Reinforcement Learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":null},{\"arxivId\":\"1707.04402\",\"authors\":[{\"authorId\":\"145411234\",\"name\":\"Gregory Palmer\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"},{\"authorId\":\"2539968\",\"name\":\"D. Bloembergen\"},{\"authorId\":\"2377870\",\"name\":\"Rahul Savani\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4de770c8569893f5757c056390ffe1ae8fb49b17\",\"title\":\"Lenient Multi-Agent Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4de770c8569893f5757c056390ffe1ae8fb49b17\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ryan Lowe\"},{\"authorId\":null,\"name\":\"Yi Wu\"},{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"OpenAI Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Littman . Markov games as a framework for multi - agent reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Spiros Kapetanakis\"},{\"authorId\":null,\"name\":\"Daniel Kudenko. Reinforcement learning of coordination in systems\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 18th National Conference on Artificial Intelligence (AAAI\\u201902)\",\"url\":\"\",\"venue\":\"pages 326\\u2013331. York,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Lucian Bu\\u015foniu\"},{\"authorId\":null,\"name\":\"Robert Babu\\u0161ka\"},{\"authorId\":null,\"name\":\"Bart De Schutter. A comprehensive survey of multiagent Systems\"},{\"authorId\":null,\"name\":\"Man\"},{\"authorId\":null,\"name\":\"Cybernetics\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Part C: Applications and Reviews\",\"url\":\"\",\"venue\":\"38(2):156\\u2013172,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2993306\",\"name\":\"K. Woodsend\"},{\"authorId\":\"1747893\",\"name\":\"Mirella Lapata\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cadf27667b0b37baaf96dd3b05d7559df2945cdd\",\"title\":\"Proceedings of the National Conference on Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/cadf27667b0b37baaf96dd3b05d7559df2945cdd\",\"venue\":\"\",\"year\":2011}],\"title\":\"Explicitly Coordinated Policy Iteration\",\"topics\":[{\"topic\":\"Iteration\",\"topicId\":\"11823\",\"url\":\"https://www.semanticscholar.org/topic/11823\"},{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"}],\"url\":\"https://www.semanticscholar.org/paper/3349ef4018356f874cb2d8d34435a1fbc12de9c3\",\"venue\":\"IJCAI\",\"year\":2019}\n"