"{\"abstract\":\"Curriculum learning has been successfully used in reinforcement learning to accelerate the learning process, through knowledge transfer between tasks of increasing complexity. Critical tasks, in which suboptimal exploratory actions must be minimized, can benefit from curriculum learning, and its ability to shape exploration through transfer. We propose a task sequencing algorithm maximizing the cumulative return, that is, the return obtained by the agent across all the learning episodes. By maximizing the cumulative return, the agent not only aims at achieving high rewards as fast as possible, but also at doing so while limiting suboptimal actions. We experimentally compare our task sequencing algorithm to several popular metaheuristic algorithms for combinatorial optimization, and show that it achieves significantly better performance on the problem of cumulative return maximization. Furthermore, we validate our algorithm on a critical task, optimizing a home controller for a micro energy grid.\",\"arxivId\":\"1906.06178\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\",\"url\":\"https://www.semanticscholar.org/author/52620280\"},{\"authorId\":\"137124626\",\"name\":\"Christiano Coletto Christakou\",\"url\":\"https://www.semanticscholar.org/author/137124626\"},{\"authorId\":\"138689850\",\"name\":\"R. Gutierrez\",\"url\":\"https://www.semanticscholar.org/author/138689850\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\",\"url\":\"https://www.semanticscholar.org/author/1696726\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2008.00511\",\"authors\":[{\"authorId\":\"1850139106\",\"name\":\"Andrea Bassich\"},{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"777ae9b36cacca5deca104541235d19030c19d5e\",\"title\":\"Curriculum Learning with a Progression Function\",\"url\":\"https://www.semanticscholar.org/paper/777ae9b36cacca5deca104541235d19030c19d5e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.01054\",\"authors\":[{\"authorId\":\"138689850\",\"name\":\"R. Gutierrez\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a95392211ec1e41a8e0a34b6b8e1a750e966fbbd\",\"title\":\"Information-theoretic Task Selection for Meta-Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a95392211ec1e41a8e0a34b6b8e1a750e966fbbd\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"1901.11478\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":\"10.1109/DEVLRN.2019.8850690\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"1e658c39cf8695c3363457903f5bc898231611dd\",\"title\":\"An Optimization Framework for Task Sequencing in Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e658c39cf8695c3363457903f5bc898231611dd\",\"venue\":\"2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)\",\"year\":2019},{\"arxivId\":\"2003.04960\",\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"21022150\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fb0bfc9c1518d5c7c2fbdb7bc9e254eff0ab6238\",\"title\":\"Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey\",\"url\":\"https://www.semanticscholar.org/paper/fb0bfc9c1518d5c7c2fbdb7bc9e254eff0ab6238\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2020}],\"corpusId\":189897626,\"doi\":\"10.24963/ijcai.2019/320\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"1988a0134839f36449bbacc4589cc8b37e46a965\",\"references\":[{\"arxivId\":\"1901.11478\",\"authors\":[{\"authorId\":\"52620280\",\"name\":\"Francesco Foglino\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"}],\"doi\":\"10.1109/DEVLRN.2019.8850690\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1e658c39cf8695c3363457903f5bc898231611dd\",\"title\":\"An Optimization Framework for Task Sequencing in Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e658c39cf8695c3363457903f5bc898231611dd\",\"venue\":\"2019 Joint IEEE 9th International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)\",\"year\":2019},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98264506\",\"name\":\"Yuxin Wu\"},{\"authorId\":\"39402399\",\"name\":\"Yuandong Tian\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"add7b8b65355d5408a1ffb93a94b0ae688806bc4\",\"title\":\"Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/add7b8b65355d5408a1ffb93a94b0ae688806bc4\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1802.10567\",\"authors\":[{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"49512734\",\"name\":\"Roland Hafner\"},{\"authorId\":\"46534085\",\"name\":\"T. Lampe\"},{\"authorId\":\"2366050\",\"name\":\"Michael Neunert\"},{\"authorId\":\"3110620\",\"name\":\"J. Degrave\"},{\"authorId\":\"8023592\",\"name\":\"T. Wiele\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cab81775baae7ba2d056ebbc60437f2e03358ca3\",\"title\":\"Learning by Playing - Solving Sparse Reward Tasks from Scratch\",\"url\":\"https://www.semanticscholar.org/paper/cab81775baae7ba2d056ebbc60437f2e03358ca3\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47346762\",\"name\":\"M. Svetlik\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"9578995\",\"name\":\"Rishi Shah\"},{\"authorId\":\"145314605\",\"name\":\"Nick Walker\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"42750855b95449ba74921bfecc84e2257a2c5b19\",\"title\":\"Automatic Curriculum Graph Generation for Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/42750855b95449ba74921bfecc84e2257a2c5b19\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48463134\",\"name\":\"F. Glover\"}],\"doi\":\"10.1287/ijoc.2.1.4\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9a3203a26112ec33d34233ba8b38e5509490f2a0\",\"title\":\"Tabu Search - Part II\",\"url\":\"https://www.semanticscholar.org/paper/9a3203a26112ec33d34233ba8b38e5509490f2a0\",\"venue\":\"INFORMS J. Comput.\",\"year\":1990},{\"arxivId\":\"1705.06366\",\"authors\":[{\"authorId\":\"145641013\",\"name\":\"David Held\"},{\"authorId\":\"3468192\",\"name\":\"Xinyang Geng\"},{\"authorId\":\"10104623\",\"name\":\"Carlos Florensa\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"471f9742b4e32d8ee68f9ee493768ff0466a231d\",\"title\":\"Automatic Goal Generation for Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/471f9742b4e32d8ee68f9ee493768ff0466a231d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.24963/ijcai.2017/353\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b7708394600130c8c5403178976fcfca1972f3db\",\"title\":\"Autonomous Task Sequencing for Customized Curriculum Design in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b7708394600130c8c5403178976fcfca1972f3db\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12116553\",\"name\":\"Y. Wu\"},{\"authorId\":null,\"name\":\"Wei Zhang\"},{\"authorId\":\"145592727\",\"name\":\"K. Song\"}],\"doi\":\"10.24963/ijcai.2018/211\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"title\":\"Master-Slave Curriculum Design for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b03f2cba4038324fb0a3474bf86069e3fc8dc59f\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1606.04671\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"3422052\",\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"title\":\"Progressive Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12114845\",\"name\":\"P. Ruvolo\"},{\"authorId\":\"144020269\",\"name\":\"E. Eaton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8d49d34fff05285cb9a148261caff57775eb4453\",\"title\":\"ELLA: An Efficient Lifelong Learning Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/8d49d34fff05285cb9a148261caff57775eb4453\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1715339\",\"name\":\"D. Goldberg\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"fc2806a373aefe9ec9fd02e6369fdf9b6a6ff8e4\",\"title\":\"Genetic Algorithms in Search\",\"url\":\"https://www.semanticscholar.org/paper/fc2806a373aefe9ec9fd02e6369fdf9b6a6ff8e4\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":\"10.1007/978-3-642-27645-3_5\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"title\":\"Transfer in Reinforcement Learning: A Framework and a Survey\",\"url\":\"https://www.semanticscholar.org/paper/16c97a8a29b0d63fdb119daefabc47df92ff6c24\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marco Dorigo\"},{\"authorId\":null,\"name\":\"Vittorio Maniezzo\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and Alberto Colorni\",\"url\":\"\",\"venue\":\"The ant system: An autocatalytic optimizing process,\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144056302\",\"name\":\"A. Wilson\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"},{\"authorId\":\"145527877\",\"name\":\"Soumya Ray\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":\"10.1145/1273496.1273624\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"title\":\"Multi-task reinforcement learning: a hierarchical Bayesian approach\",\"url\":\"https://www.semanticscholar.org/paper/ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"venue\":\"ICML '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas Jaksch\"},{\"authorId\":null,\"name\":\"Ronald Ortner\"},{\"authorId\":null,\"name\":\"Peter Auer. Near-optimal regret bounds for reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"11(Apr):1563\\u20131600,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":\"1703.05407\",\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"2000906\",\"name\":\"Ilya Kostrikov\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8499a250422a3c66357367c8d5fa504de5424c59\",\"title\":\"Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play\",\"url\":\"https://www.semanticscholar.org/paper/8499a250422a3c66357367c8d5fa504de5424c59\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3113049\",\"name\":\"T. Jaksch\"},{\"authorId\":\"1786887\",\"name\":\"R. Ortner\"},{\"authorId\":\"144543541\",\"name\":\"P. Auer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0cafe2903b097fc042782c359cb231ea34ef7ed3\",\"title\":\"Near-optimal Regret Bounds for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0cafe2903b097fc042782c359cb231ea34ef7ed3\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145740789\",\"name\":\"Fred W. Glover\"}],\"doi\":\"10.1287/ijoc.1.3.190\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d115adcbf63e989804912db1e656ec8debd1f040\",\"title\":\"Tabu Search - Part I\",\"url\":\"https://www.semanticscholar.org/paper/d115adcbf63e989804912db1e656ec8debd1f040\",\"venue\":\"INFORMS J. Comput.\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735414\",\"name\":\"T. Knasel\"}],\"doi\":\"10.1016/0921-8890(88)90002-4\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"edd77f310393f521669b209cbb6828fb45a8485d\",\"title\":\"Robotics and autonomous systems\",\"url\":\"https://www.semanticscholar.org/paper/edd77f310393f521669b209cbb6828fb45a8485d\",\"venue\":\"Robotics Auton. Syst.\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1737999\",\"name\":\"S. Narvekar\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"1696726\",\"name\":\"Matteo Leonetti\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"title\":\"Source Task Creation for Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/8738c213f8b4a1d93b88008ffa2d4ff42bd64a68\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":\"1804.00810\",\"authors\":[{\"authorId\":\"2428563\",\"name\":\"Kun Shao\"},{\"authorId\":\"3124762\",\"name\":\"Yuanheng Zhu\"},{\"authorId\":\"1699234\",\"name\":\"D. Zhao\"}],\"doi\":\"10.1109/TETCI.2018.2823329\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26b90ccf7541fd2cd4235118493e1e49d358c351\",\"title\":\"StarCraft Micromanagement With Reinforcement Learning and Curriculum Transfer Learning\",\"url\":\"https://www.semanticscholar.org/paper/26b90ccf7541fd2cd4235118493e1e49d358c351\",\"venue\":\"IEEE Transactions on Emerging Topics in Computational Intelligence\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sanmit Narvekar\"},{\"authorId\":null,\"name\":\"Jivko Sinapov\"},{\"authorId\":null,\"name\":\"Peter Stone. Autonomous task sequencing for customized c learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In (IJCAI)\",\"url\":\"\",\"venue\":\"The 2017 International Joint Conference on Artificial Intelligence,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145050960\",\"name\":\"F. Silva\"},{\"authorId\":\"2209202\",\"name\":\"A. Costa\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e105d71d384fdfbed12b8eee93c38db4269d01a4\",\"title\":\"Object-Oriented Curriculum Generation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e105d71d384fdfbed12b8eee93c38db4269d01a4\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145518405\",\"name\":\"R. Steele\"}],\"doi\":\"10.1198/jasa.2005.s47\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"de135bd9e74f54877cd1fc536437fe0864c04614\",\"title\":\"Optimization\",\"url\":\"https://www.semanticscholar.org/paper/de135bd9e74f54877cd1fc536437fe0864c04614\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Felipe Leno Da Silva\"},{\"authorId\":null,\"name\":\"Anna Helena Reali Costa. A survey on transfer learning systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"64:645\\u2013703,\",\"year\":2019}],\"title\":\"Curriculum Learning for Cumulative Return Maximization\",\"topics\":[{\"topic\":\"Metaheuristic\",\"topicId\":\"61661\",\"url\":\"https://www.semanticscholar.org/topic/61661\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Expectation\\u2013maximization algorithm\",\"topicId\":\"52938\",\"url\":\"https://www.semanticscholar.org/topic/52938\"},{\"topic\":\"Combinatorial optimization\",\"topicId\":\"30344\",\"url\":\"https://www.semanticscholar.org/topic/30344\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Controllers\",\"topicId\":\"433\",\"url\":\"https://www.semanticscholar.org/topic/433\"}],\"url\":\"https://www.semanticscholar.org/paper/1988a0134839f36449bbacc4589cc8b37e46a965\",\"venue\":\"IJCAI\",\"year\":2019}\n"