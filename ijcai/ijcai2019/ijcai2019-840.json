"{\"abstract\":\"In Reinforcement Learning (RL), an agent is guided by the rewards it receives from the reward function. Unfortunately, it may take many interactions with the environment to learn from sparse rewards, and it can be challenging to specify reward functions that reflect complex reward-worthy behavior. We propose using reward machines (RMs), which are automata-based representations that expose reward function structure, as a normal form representation for reward functions. We show how specifications of reward in various formal languages, including LTL and other regular languages, can be automatically translated into RMs, easing the burden of complex reward function specification. We then show how the exposed structure of the reward function can be exploited by tailored q-learning algorithms and automated reward shaping techniques in order to improve the sample efficiency of reinforcement learning methods. Experiments show that these RM-tailored techniques significantly outperform state-of-the-art (deep) RL algorithms, solving problems that otherwise cannot reasonably be solved by existing approaches.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"47505744\",\"name\":\"Alberto Camacho\",\"url\":\"https://www.semanticscholar.org/author/47505744\"},{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\",\"url\":\"https://www.semanticscholar.org/author/15316342\"},{\"authorId\":\"1758085\",\"name\":\"Toryn Q. Klassen\",\"url\":\"https://www.semanticscholar.org/author/1758085\"},{\"authorId\":\"2682734\",\"name\":\"R. Valenzano\",\"url\":\"https://www.semanticscholar.org/author/2682734\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\",\"url\":\"https://www.semanticscholar.org/author/1683896\"}],\"citationVelocity\":14,\"citations\":[{\"arxivId\":\"1911.13152\",\"authors\":[{\"authorId\":\"1405134785\",\"name\":\"Daniel Furelos-Blanco\"},{\"authorId\":\"2489413\",\"name\":\"M. Law\"},{\"authorId\":\"144119475\",\"name\":\"A. Russo\"},{\"authorId\":\"1731914\",\"name\":\"K. Broda\"},{\"authorId\":\"143808507\",\"name\":\"A. Jonsson\"}],\"doi\":\"10.1609/AAAI.V34I04.5802\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c62d84f3d4753d7e3c4747b966cb050d4b667280\",\"title\":\"Induction of Subgoal Automata for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c62d84f3d4753d7e3c4747b966cb050d4b667280\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144331830\",\"name\":\"Li Qi-an\"},{\"authorId\":null,\"name\":\"Jing Liu\"}],\"doi\":\"10.1109/IJCNN48605.2020.9207384\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0593f15a6a475639d2e85cf9f63bce00b3efdd4\",\"title\":\"Safe Reinforcement Learning via Probabilistic Timed Computation Tree Logic\",\"url\":\"https://www.semanticscholar.org/paper/a0593f15a6a475639d2e85cf9f63bce00b3efdd4\",\"venue\":\"2020 International Joint Conference on Neural Networks (IJCNN)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3437308\",\"name\":\"Ruosong Wang\"},{\"authorId\":\"1980148\",\"name\":\"Peilin Zhong\"},{\"authorId\":\"145697585\",\"name\":\"S. Du\"},{\"authorId\":\"2031418850\",\"name\":\"Russ R. Salakhutdinov\"},{\"authorId\":\"40577530\",\"name\":\"Lin F. Yang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6c5e1376e8b497dc01266f31d24abf49aadf7204\",\"title\":\"Planning with General Objective Functions: Going Beyond Total Rewards\",\"url\":\"https://www.semanticscholar.org/paper/6c5e1376e8b497dc01266f31d24abf49aadf7204\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2005.11737\",\"authors\":[{\"authorId\":\"49478657\",\"name\":\"Kun Xie\"},{\"authorId\":\"145559637\",\"name\":\"Sylvain Hall\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e17f74271c263e8ca8740b2289a41c4fcff55c16\",\"title\":\"Efficient Offline Monitoring of Linear Temporal Logic with Bit Vectors\",\"url\":\"https://www.semanticscholar.org/paper/e17f74271c263e8ca8740b2289a41c4fcff55c16\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.05912\",\"authors\":[{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"2202693\",\"name\":\"Ivan Gavran\"},{\"authorId\":\"143782246\",\"name\":\"Y. Ahmad\"},{\"authorId\":\"144029582\",\"name\":\"R. Majumdar\"},{\"authorId\":\"1779795\",\"name\":\"D. Neider\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"},{\"authorId\":\"143847264\",\"name\":\"B. Wu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6d9cfcc8588ed9c513816874708ac55b5901e697\",\"title\":\"Joint Inference of Reward Machines and Policies for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/6d9cfcc8588ed9c513816874708ac55b5901e697\",\"venue\":\"ICAPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1719219\",\"name\":\"Giuseppe De Giacomo\"},{\"authorId\":\"1712013\",\"name\":\"L. Iocchi\"},{\"authorId\":\"1698994\",\"name\":\"F. Patrizi\"},{\"authorId\":\"3253281\",\"name\":\"Alessandro Ronca\"}],\"doi\":\"10.24963/kr.2020/89\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"38bcf084d0706a992e21e3461c0d41e5f8a3d914\",\"title\":\"Temporal Logic Monitoring Rewards via Transducers\",\"url\":\"https://www.semanticscholar.org/paper/38bcf084d0706a992e21e3461c0d41e5f8a3d914\",\"venue\":\"KR\",\"year\":2020},{\"arxivId\":\"1906.03218\",\"authors\":[{\"authorId\":\"47287735\",\"name\":\"A. Shah\"},{\"authorId\":\"48830658\",\"name\":\"Shen Li\"},{\"authorId\":\"143873973\",\"name\":\"Julie Shah\"}],\"doi\":\"10.1109/LRA.2020.2977217\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6b19ce5b88089725b61ff490bef564b9744e6d28\",\"title\":\"Planning With Uncertain Specifications (PUnS)\",\"url\":\"https://www.semanticscholar.org/paper/6b19ce5b88089725b61ff490bef564b9744e6d28\",\"venue\":\"IEEE Robotics and Automation Letters\",\"year\":2020},{\"arxivId\":\"2006.08767\",\"authors\":[{\"authorId\":\"150936641\",\"name\":\"Borja G. Leon\"},{\"authorId\":\"1757629\",\"name\":\"M. Shanahan\"},{\"authorId\":\"3142000\",\"name\":\"F. Belardinelli\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e863cc9d62741677514ac94e81b4f6d8f82fb9ed\",\"title\":\"Systematic Generalisation through Task Temporal Logic and Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e863cc9d62741677514ac94e81b4f6d8f82fb9ed\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1396422950\",\"name\":\"Ethan Waldie\"},{\"authorId\":\"1758085\",\"name\":\"Toryn Q. Klassen\"},{\"authorId\":\"1396422966\",\"name\":\"Rick Valenzano\"},{\"authorId\":\"41036745\",\"name\":\"Margarita P. Castro\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a03472a07ac2ef5b5b03358d074d2891c8fba144\",\"title\":\"Learning Reward Machines for Partially Observable Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a03472a07ac2ef5b5b03358d074d2891c8fba144\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2008.08548\",\"authors\":[{\"authorId\":\"1628280424\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"1388270562\",\"name\":\"Mohan Sridharan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce808e43af42c095e3dce2dff340e8d6f65f17c2\",\"title\":\"A Survey of Knowledge-based Sequential Decision Making under Uncertainty\",\"url\":\"https://www.semanticscholar.org/paper/ce808e43af42c095e3dce2dff340e8d6f65f17c2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b15a16ac6a2983461289571fd197cf8f415d359e\",\"title\":\"Synthesizing best-effort strategies under expected and exceptional environment behaviors\",\"url\":\"https://www.semanticscholar.org/paper/b15a16ac6a2983461289571fd197cf8f415d359e\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153468821\",\"name\":\"G. Gomes\"},{\"authorId\":\"2029361307\",\"name\":\"C. A. Vidal\"},{\"authorId\":\"1401380999\",\"name\":\"J. B. Cavalcante-Neto\"},{\"authorId\":\"2003016742\",\"name\":\"Yuri L. B. Nogueira\"}],\"doi\":\"10.1109/SBGames51465.2020.00014\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"581584837c0d3f1c79987cae26387f373a098a7a\",\"title\":\"AI4U: A Tool for Game Reinforcement Learning Experiments\",\"url\":\"https://www.semanticscholar.org/paper/581584837c0d3f1c79987cae26387f373a098a7a\",\"venue\":\"2020 19th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames)\",\"year\":2020},{\"arxivId\":\"2009.12600\",\"authors\":[{\"authorId\":\"2704296\",\"name\":\"G. Rens\"},{\"authorId\":\"118006876\",\"name\":\"Jean-Franccois Raskin\"},{\"authorId\":\"1972334007\",\"name\":\"Raphael Reynouad\"},{\"authorId\":\"144748254\",\"name\":\"G. Marra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6c9c80aa47b64a0d9859b380811cffe1fb39eb1b\",\"title\":\"Online Learning of Non-Markovian Reward Models\",\"url\":\"https://www.semanticscholar.org/paper/6c9c80aa47b64a0d9859b380811cffe1fb39eb1b\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1393291635\",\"name\":\"Le\\u00f3n Illanes\"},{\"authorId\":\"1441353365\",\"name\":\"X. Yan\"},{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"418a2d5a6c6027e079357d872a0596ec5b344289\",\"title\":\"Symbolic Plans as High-Level Instructions for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/418a2d5a6c6027e079357d872a0596ec5b344289\",\"venue\":\"ICAPS\",\"year\":2020},{\"arxivId\":\"2003.02232\",\"authors\":[{\"authorId\":\"150031904\",\"name\":\"Ankit Shah\"},{\"authorId\":\"143873973\",\"name\":\"Julie Shah\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2ba247be3bd58da3e0d48f80648485f61cccfa50\",\"title\":\"Interactive Robot Training for Non-Markov Tasks\",\"url\":\"https://www.semanticscholar.org/paper/2ba247be3bd58da3e0d48f80648485f61cccfa50\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.01962\",\"authors\":[{\"authorId\":\"1796254983\",\"name\":\"Cyrus Neary\"},{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"152365289\",\"name\":\"Bo Wu\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c60eb0d8452779a95c93c725851aeb9cbada39eb\",\"title\":\"Reward Machines for Cooperative Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c60eb0d8452779a95c93c725851aeb9cbada39eb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.01008\",\"authors\":[{\"authorId\":\"1515590905\",\"name\":\"E. Abadi\"},{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"}],\"doi\":\"10.24963/ijcai.2020/266\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a1c20b41be5a771e7c191b23bbb6ce4a28721f0\",\"title\":\"Learning and Solving Regular Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/1a1c20b41be5a771e7c191b23bbb6ce4a28721f0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.03855\",\"authors\":[{\"authorId\":\"1405134785\",\"name\":\"Daniel Furelos-Blanco\"},{\"authorId\":\"2489413\",\"name\":\"M. Law\"},{\"authorId\":\"143808507\",\"name\":\"A. Jonsson\"},{\"authorId\":\"1731914\",\"name\":\"K. Broda\"},{\"authorId\":\"144119475\",\"name\":\"A. Russo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5a14b037a5d73e48fc723c40328769d5561fd372\",\"title\":\"Induction and Exploitation of Subgoal Automata for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5a14b037a5d73e48fc723c40328769d5561fd372\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807298\",\"name\":\"Benjamin Aminof\"},{\"authorId\":\"1719219\",\"name\":\"Giuseppe De Giacomo\"},{\"authorId\":\"1390031030\",\"name\":\"A. Lomuscio\"},{\"authorId\":\"1728487\",\"name\":\"A. Murano\"},{\"authorId\":\"1876227\",\"name\":\"S. Rubin\"}],\"doi\":\"10.24963/ijcai.2020/232\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6567f088a3118eb188a84868f306ca7d4e000e0f\",\"title\":\"Synthesizing strategies under expected and exceptional environment behaviors\",\"url\":\"https://www.semanticscholar.org/paper/6567f088a3118eb188a84868f306ca7d4e000e0f\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2001.09293\",\"authors\":[{\"authorId\":\"2704296\",\"name\":\"G. Rens\"},{\"authorId\":\"118006876\",\"name\":\"Jean-Franccois Raskin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4383f115a1e4790ba9b285edb1f1c7c76a5928e0\",\"title\":\"Learning Non-Markovian Reward Models in MDPs\",\"url\":\"https://www.semanticscholar.org/paper/4383f115a1e4790ba9b285edb1f1c7c76a5928e0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1719219\",\"name\":\"Giuseppe De Giacomo\"},{\"authorId\":\"47717839\",\"name\":\"Antonio Di Stasio\"},{\"authorId\":\"1632969642\",\"name\":\"Francesco Fuggitti\"},{\"authorId\":\"1876227\",\"name\":\"S. Rubin\"}],\"doi\":\"10.24963/ijcai.2020/679\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"823dbab690b96cd624facb7b6f9c5db05096af80\",\"title\":\"Pure-Past Linear Temporal and Dynamic Logic on Finite Traces\",\"url\":\"https://www.semanticscholar.org/paper/823dbab690b96cd624facb7b6f9c5db05096af80\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.03950\",\"authors\":[{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1758085\",\"name\":\"Toryn Q. Klassen\"},{\"authorId\":\"2682734\",\"name\":\"R. Valenzano\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"bcd294a5b1ba87d5d730460891f563b5eedd68b6\",\"title\":\"Reward Machines: Exploiting Reward Function Structure in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bcd294a5b1ba87d5d730460891f563b5eedd68b6\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.03277\",\"authors\":[{\"authorId\":\"50844584\",\"name\":\"Christopher Wang\"},{\"authorId\":\"51519704\",\"name\":\"Candace Ross\"},{\"authorId\":\"143912599\",\"name\":\"B. Katz\"},{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f397cde61a2ab402c373e3212048ca858885087f\",\"title\":\"Learning a natural-language to LTL executable semantic parser for grounded robotics.\",\"url\":\"https://www.semanticscholar.org/paper/f397cde61a2ab402c373e3212048ca858885087f\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.14464\",\"authors\":[{\"authorId\":\"47505744\",\"name\":\"Alberto Camacho\"},{\"authorId\":null,\"name\":\"Jacob Varley\"},{\"authorId\":\"145950587\",\"name\":\"Deepali Jain\"},{\"authorId\":null,\"name\":\"Atil Iscen\"},{\"authorId\":null,\"name\":\"Dmitry Kalashnikov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e9420bd6621bf8c09dde9baa28c6b7cd49fe65d1\",\"title\":\"Disentangled Planning and Control in Vision Based Robotics via Reward Machines\",\"url\":\"https://www.semanticscholar.org/paper/e9420bd6621bf8c09dde9baa28c6b7cd49fe65d1\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2006.01110\",\"authors\":[{\"authorId\":\"1947473\",\"name\":\"Yen-Ling Kuo\"},{\"authorId\":\"143912599\",\"name\":\"B. Katz\"},{\"authorId\":\"21570451\",\"name\":\"A. Barbu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"025d142e712c5ddf97157e62765ff518da6ef9b2\",\"title\":\"Encoding formulas as deep networks: Reinforcement learning for zero-shot execution of LTL formulas\",\"url\":\"https://www.semanticscholar.org/paper/025d142e712c5ddf97157e62765ff518da6ef9b2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727062\",\"name\":\"E. Hahn\"},{\"authorId\":\"145304742\",\"name\":\"Mateo Perez\"},{\"authorId\":\"2701127\",\"name\":\"S. Schewe\"},{\"authorId\":\"1693050\",\"name\":\"F. Somenzi\"},{\"authorId\":\"47577211\",\"name\":\"A. Trivedi\"},{\"authorId\":\"3038359\",\"name\":\"D. Wojtczak\"}],\"doi\":\"10.1007/978-3-030-59152-6_6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f757058c0042facca65ba1a1e0d137f392d75181\",\"title\":\"Faithful and Effective Reward Schemes for Model-Free Reinforcement Learning of Omega-Regular Objectives\",\"url\":\"https://www.semanticscholar.org/paper/f757058c0042facca65ba1a1e0d137f392d75181\",\"venue\":\"ATVA\",\"year\":2020},{\"arxivId\":\"2001.09227\",\"authors\":[{\"authorId\":\"93657967\",\"name\":\"F. Memarian\"},{\"authorId\":\"50070268\",\"name\":\"Zhe Xu\"},{\"authorId\":\"152365289\",\"name\":\"Bo Wu\"},{\"authorId\":\"144641675\",\"name\":\"M. Wen\"},{\"authorId\":\"3199888\",\"name\":\"U. Topcu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"17f23a5fdc202bd208bbc87de2a4824e6c125d5e\",\"title\":\"Active Task-Inference-Guided Deep Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/17f23a5fdc202bd208bbc87de2a4824e6c125d5e\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199465884,\"doi\":\"10.24963/ijcai.2019/840\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"c3d0e3c5ca9fa56cf2ff7303a2f67bf44694e6d4\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Giuseppe De Giacomo\"},{\"authorId\":null,\"name\":\"Moshe Y. Vardi. Linear temporal logic\"},{\"authorId\":null,\"name\":\"linear dynamic logic on finite traces\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 854\\u2013860,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":null,\"name\":\"Toryn Q. Klassen\"},{\"authorId\":null,\"name\":\"Richard Anthony Valenzano\"},{\"authorId\":null,\"name\":\"Sheila A. McIlraith. Teaching multiple tasks to an RL age LTL\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAMAS\",\"url\":\"\",\"venue\":\"pages 452\\u2013461,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47157189\",\"name\":\"A. Gupta\"},{\"authorId\":\"144682958\",\"name\":\"S. Malik\"}],\"doi\":\"10.1007/S10703-009-0072-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"700ff75e1c03b84026794c753582660e5376103d\",\"title\":\"Formal Methods in System Design: Preface\",\"url\":\"https://www.semanticscholar.org/paper/700ff75e1c03b84026794c753582660e5376103d\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jorge A. Baier\"},{\"authorId\":null,\"name\":\"Sheila A. McIlraith. Planning with temporally extended go search\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICAPS\",\"url\":\"\",\"venue\":\"pages 342\\u2013345,\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"George H Mealy. A method for synthesizing sequential circ Journal\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"34(5):1045\\u20131079\",\"url\":\"\",\"venue\":\"Sep.\",\"year\":1955},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hado Van Hasselt\"},{\"authorId\":null,\"name\":\"Arthur Guez\"},{\"authorId\":null,\"name\":\"David Silver. Deep reinforcement learning with double Qlearning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 2094\\u20132100,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1719219\",\"name\":\"Giuseppe De Giacomo\"},{\"authorId\":\"9083969\",\"name\":\"Moshe Y. Vardi\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"23e52e7c75cdd9afc70e7ba4432484f994dd44cc\",\"title\":\"Linear Temporal Logic and Linear Dynamic Logic on Finite Traces\",\"url\":\"https://www.semanticscholar.org/paper/23e52e7c75cdd9afc70e7ba4432484f994dd44cc\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christopher J. C. H. Watkins\"},{\"authorId\":null,\"name\":\"Shufang Zhu\"},{\"authorId\":null,\"name\":\"M. Lucas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Tabajara , Jianwen Li , Geguang Pu , and Moshe Y . Vardi . Symbolic LTLf synthesis\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":null,\"name\":\"Toryn Q. Klassen\"},{\"authorId\":null,\"name\":\"Richard Anthony Valenzano\"},{\"authorId\":null,\"name\":\"Sheila A. McIlraith. Using reward machines for high-level specification\"},{\"authorId\":null,\"name\":\"decomposition in reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 2112\\u20132121,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1758085\",\"name\":\"Toryn Q. Klassen\"},{\"authorId\":\"2682734\",\"name\":\"R. Valenzano\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"00ec8123dd2ba03afab7c1fa02f774062f769181\",\"title\":\"Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/00ec8123dd2ba03afab7c1fa02f774062f769181\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47505744\",\"name\":\"Alberto Camacho\"},{\"authorId\":\"47190129\",\"name\":\"Oscar Chen\"},{\"authorId\":\"1732536\",\"name\":\"S. Sanner\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b30ec35f46161c4184be1af364e2afd79f876755\",\"title\":\"Non-Markovian Rewards Expressed in LTL: Guiding Search Via Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/b30ec35f46161c4184be1af364e2afd79f876755\",\"venue\":\"SOCS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jacob Andreas\"},{\"authorId\":null,\"name\":\"Dan Klein\"},{\"authorId\":null,\"name\":\"Sergey Levine. Modular multitask reinforcement learning w sketches\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 166\\u2013175,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leslie Lamport. Specifying Systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The TLA+ Language and Tools for Hardware and Software Engineers\",\"url\":\"\",\"venue\":\"Addison-Wesley,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alberto Camacho\"},{\"authorId\":null,\"name\":\"Oscar Chen\"},{\"authorId\":null,\"name\":\"Scott Sanner\"},{\"authorId\":null,\"name\":\"A Sheila\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Brafman , Giuseppe De Giacomo , and Fabio Patrizi . LTLf / LDLf non - Markovian rewards\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shufang Zhu\"},{\"authorId\":null,\"name\":\"Lucas M. Tabajara\"},{\"authorId\":null,\"name\":\"Jianwen Li\"},{\"authorId\":null,\"name\":\"Geguang Pu\"},{\"authorId\":null,\"name\":\"Moshe Y. Vardi. Symbolic LTLf synthesis\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 1362\\u20131369,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2409297\",\"name\":\"Tyler Lu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"59c0931562668b1ebb7c5cc8e9c236d43308722d\",\"title\":\"Non-delusional Q-learning and value-iteration\",\"url\":\"https://www.semanticscholar.org/paper/59c0931562668b1ebb7c5cc8e9c236d43308722d\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1886681\",\"name\":\"Thorsten Altenkirch\"},{\"authorId\":\"1748428\",\"name\":\"Venanzio Capretta\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c1f80d29040d534523d2ff266137045fc2617432\",\"title\":\"Languages and Computation ( G 52 LAC ) Lecture notes Spring 2018\",\"url\":\"https://www.semanticscholar.org/paper/c1f80d29040d534523d2ff266137045fc2617432\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52264719\",\"name\":\"Bell Telephone\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac4cc434e7e7ae3d148616d33a42dcc456e04722\",\"title\":\"Regular Expression Search Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/ac4cc434e7e7ae3d148616d33a42dcc456e04722\",\"venue\":\"\",\"year\":1968},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736882\",\"name\":\"F. Bacchus\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"},{\"authorId\":\"2187850\",\"name\":\"A. Grove\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5758bec4560070a78b665eb4fa6e2c12dbb5157\",\"title\":\"Structured Solution Methods for Non-Markovian Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/c5758bec4560070a78b665eb4fa6e2c12dbb5157\",\"venue\":\"AAAI/IAAI\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Fahiem Bacchus\"},{\"authorId\":null,\"name\":\"Craig Boutilier\"},{\"authorId\":null,\"name\":\"Adam J. Grove. Rewarding behaviors\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 1160\\u2013 1167,\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jorge A. Baier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Baier , Christian Fritz , and Sheila A . McIlraith . Exploiting procedural domain control knowledge in state - ofthe - art planners\",\"url\":\"\",\"venue\":\"In ICAPS\",\"year\":2007},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"16988294\",\"name\":\"G. Mealy\"}],\"doi\":\"10.1002/J.1538-7305.1955.TB03788.X\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"844b266e71263913873dededf77b275f1cbfce1e\",\"title\":\"A method for synthesizing sequential circuits\",\"url\":\"https://www.semanticscholar.org/paper/844b266e71263913873dededf77b275f1cbfce1e\",\"venue\":\"\",\"year\":1955},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145459273\",\"name\":\"E. Emerson\"}],\"doi\":\"10.1016/b978-0-444-88074-1.50021-4\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5c8cc309444a2d7d49e7ee2e40eeb6d5dc1f7412\",\"title\":\"Temporal and Modal Logic\",\"url\":\"https://www.semanticscholar.org/paper/5c8cc309444a2d7d49e7ee2e40eeb6d5dc1f7412\",\"venue\":\"Handbook of Theoretical Computer Science, Volume B: Formal Models and Sematics\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10213745\",\"name\":\"M. Pollack\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"27c593d8b1a3b6d51d33e13a0fc75052dd921775\",\"title\":\"Journal of Artificial Intelligence Research: Preface\",\"url\":\"https://www.semanticscholar.org/paper/27c593d8b1a3b6d51d33e13a0fc75052dd921775\",\"venue\":\"\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1821852\",\"name\":\"Shirin Sohrabi\"},{\"authorId\":\"2170801\",\"name\":\"Jorge A. Baier\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0bb68802902a8fcaa2087a85cfe203eca7c0c7da\",\"title\":\"Preferred Explanations: Theory and Generation via Planning\",\"url\":\"https://www.semanticscholar.org/paper/0bb68802902a8fcaa2087a85cfe203eca7c0c7da\",\"venue\":\"AAAI\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2170801\",\"name\":\"Jorge A. Baier\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a1e4eb7fb42944985a8d7f6ed22000f1af46665\",\"title\":\"Planning with Temporally Extended Goals Using Heuristic Search\",\"url\":\"https://www.semanticscholar.org/paper/4a1e4eb7fb42944985a8d7f6ed22000f1af46665\",\"venue\":\"ICAPS\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alexandre Duret-Lutz\"},{\"authorId\":null,\"name\":\"Alexandre Lewkowicz\"},{\"authorId\":null,\"name\":\"Amaury Fauchille\"},{\"authorId\":null,\"name\":\"Thibaud Michaud\"},{\"authorId\":null,\"name\":\"Etienne Renault\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Laurent Xu\",\"url\":\"\",\"venue\":\"Spot 2.0 \\u2014 a framework for LTL and \\u03c9-automata manipulation. In ATVA, volume 9938 of LNCS, pages 122\\u2013129. Springer, October\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tyler Lu\"},{\"authorId\":null,\"name\":\"Dale Schuurmans\"},{\"authorId\":null,\"name\":\"Craig Boutilier. Non-delusional Q-learning\"},{\"authorId\":null,\"name\":\"value-iteration\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 9971\\u20139981,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2170801\",\"name\":\"Jorge A. Baier\"},{\"authorId\":\"144748581\",\"name\":\"Christian Fritz\"},{\"authorId\":\"1764889\",\"name\":\"Meghyn Bienvenu\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"720c9f34d70712899b0ea4c56fd5f245e5186e87\",\"title\":\"Beyond Classical Planning: Procedural Control Knowledge and Preferences in State-of-the-Art Planners\",\"url\":\"https://www.semanticscholar.org/paper/720c9f34d70712899b0ea4c56fd5f245e5186e87\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":\"1109.2355\",\"authors\":[{\"authorId\":\"1685896\",\"name\":\"S. Thi\\u00e9baux\"},{\"authorId\":\"1723510\",\"name\":\"Charles Gretton\"},{\"authorId\":\"1814690\",\"name\":\"J. Slaney\"},{\"authorId\":\"144811966\",\"name\":\"D. Price\"},{\"authorId\":\"2132916\",\"name\":\"F. Kabanza\"}],\"doi\":\"10.1613/jair.1676\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2cb4f3c2480de39abc7322cc019c364195e7d334\",\"title\":\"Decision-Theoretic Planning with non-Markovian Rewards\",\"url\":\"https://www.semanticscholar.org/paper/2cb4f3c2480de39abc7322cc019c364195e7d334\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2006},{\"arxivId\":\"1611.01796\",\"authors\":[{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"38666915\",\"name\":\"D. Klein\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7\",\"title\":\"Modular Multitask Reinforcement Learning with Policy Sketches\",\"url\":\"https://www.semanticscholar.org/paper/3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrej Karpathy\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"REINFORCEjs: WaterWorld demo\",\"url\":\"\",\"venue\":\"Retrieved from http://cs.stanford.edu/people/ karpathy/reinforcejs/waterworld.html,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"I Ronen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Christian Fritz , Meghyn Bienvenu , and Sheila McIlraith . Beyond classical planning : Procedural control knowledge and preferences in state - ofthe - art planners\",\"url\":\"\",\"venue\":\"AAAI , Nectar Track\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1719219\",\"name\":\"Giuseppe De Giacomo\"},{\"authorId\":\"1698994\",\"name\":\"F. Patrizi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"914f279742a9f509934a0959c080b08e9fad771e\",\"title\":\"LTLf/LDLf Non-Markovian Rewards\",\"url\":\"https://www.semanticscholar.org/paper/914f279742a9f509934a0959c080b08e9fad771e\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"15316342\",\"name\":\"Rodrigo Toro Icarte\"},{\"authorId\":\"1758085\",\"name\":\"Toryn Q. Klassen\"},{\"authorId\":\"2682734\",\"name\":\"R. Valenzano\"},{\"authorId\":\"1683896\",\"name\":\"Sheila A. McIlraith\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"074300c14841d5c29c76ea37a48f38b365a7bf8d\",\"title\":\"Teaching Multiple Tasks to an RL Agent using LTL\",\"url\":\"https://www.semanticscholar.org/paper/074300c14841d5c29c76ea37a48f38b365a7bf8d\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Fahiem Bacchus\"},{\"authorId\":null,\"name\":\"Craig Boutilier\"},{\"authorId\":null,\"name\":\"Adam J. Grove. Structured solution methods for nonMarko processes\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 112\\u2013117,\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144524233\",\"name\":\"D. Gooch\"}],\"doi\":\"10.1145/2043236.2043240\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"951e4467820029305bd729887d7ab4b9a0772c4a\",\"title\":\"Communications of the ACM\",\"url\":\"https://www.semanticscholar.org/paper/951e4467820029305bd729887d7ab4b9a0772c4a\",\"venue\":\"XRDS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Arthur Guez\"},{\"authorId\":null,\"name\":\"David Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Deep reinforcement learning with double Q - learning Tabajara , Jianwen Li , Geguang Pu , and Moshe Y . Vardi . Symbolic LTLf synthesis\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jorge A. Baier\"},{\"authorId\":null,\"name\":\"Christian Fritz\"},{\"authorId\":null,\"name\":\"Sheila A. McIlraith. Exploiting procedural domain control planners\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICAPS\",\"url\":\"\",\"venue\":\"pages 26\\u2013 33,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eleni Triantafillou\"},{\"authorId\":null,\"name\":\"Jorge A. Baier\"},{\"authorId\":null,\"name\":\"Sheila A. McIlraith. A unifying framework for planning wi LTL\"},{\"authorId\":null,\"name\":\"regular expressions. In MOCHAP\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"a workshop collocated with ICAPS\",\"url\":\"\",\"venue\":\"pages 23\\u201331,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Eleni Triantafillou\"},{\"authorId\":null,\"name\":\"Jorge A. Baier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", and Sheila A . McIlraith . A unifying framework for planning with LTL and regular expressions\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3050437\",\"name\":\"Alexandre Duret-Lutz\"},{\"authorId\":\"35007786\",\"name\":\"Alexandre Lewkowicz\"},{\"authorId\":\"3471339\",\"name\":\"Amaury Fauchille\"},{\"authorId\":\"122296993\",\"name\":\"Thibaud Michaud\"},{\"authorId\":\"34733510\",\"name\":\"Etienne Renault\"},{\"authorId\":\"3471053\",\"name\":\"Laurent Xu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1e90953131b400ee78801580df5503fdf763226d\",\"title\":\"Spot 2 . 0 \\u2014 a framework for LTL and \\u03c9-automata manipulation\",\"url\":\"https://www.semanticscholar.org/paper/1e90953131b400ee78801580df5503fdf763226d\",\"venue\":\"\",\"year\":2016}],\"title\":\"LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Formal language\",\"topicId\":\"43444\",\"url\":\"https://www.semanticscholar.org/topic/43444\"}],\"url\":\"https://www.semanticscholar.org/paper/c3d0e3c5ca9fa56cf2ff7303a2f67bf44694e6d4\",\"venue\":\"IJCAI\",\"year\":2019}\n"