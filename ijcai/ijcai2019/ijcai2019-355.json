"{\"abstract\":\"First-order optimization algorithms have been proven prominent in deep learning. In particular, algorithms such as RMSProp and Adam are extremely popular. However, recent works have pointed out the lack of ``long-term memory\\\" in Adam-like algorithms, which could hamper their performance and lead to divergence. In our study, we observe that there are benefits of weighting more of the past gradients when designing the adaptive learning rate. We therefore propose an algorithm called the Nostalgic Adam (NosAdam) with theoretically guaranteed convergence at the best known convergence rate. NosAdam can be regarded as a fix to the non-convergence issue of Adam in alternative to the recent work of [Reddi et al., 2018]. Our preliminary numerical experiments show that NosAdam is a promising alternative algorithm to Adam. The proofs, code and other supplementary materials can be found in an anonymously shared link.\",\"arxivId\":\"1805.07557\",\"authors\":[{\"authorId\":\"11637809\",\"name\":\"H. Huang\",\"url\":\"https://www.semanticscholar.org/author/11637809\"},{\"authorId\":\"144121913\",\"name\":\"C. Wang\",\"url\":\"https://www.semanticscholar.org/author/144121913\"},{\"authorId\":\"39131579\",\"name\":\"Bin Dong\",\"url\":\"https://www.semanticscholar.org/author/39131579\"}],\"citationVelocity\":10,\"citations\":[{\"arxivId\":\"1910.12249\",\"authors\":[{\"authorId\":\"3270175\",\"name\":\"Jianbang Ding\"},{\"authorId\":\"19169659\",\"name\":\"Xuancheng Ren\"},{\"authorId\":\"51152129\",\"name\":\"Ruixuan Luo\"},{\"authorId\":\"11774802\",\"name\":\"X. Sun\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c30f39f0bd346248b64ea3de89b4ea7db1145cc7\",\"title\":\"An Adaptive and Momental Bound Method for Stochastic Learning\",\"url\":\"https://www.semanticscholar.org/paper/c30f39f0bd346248b64ea3de89b4ea7db1145cc7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1912.03194\",\"authors\":[{\"authorId\":\"50561307\",\"name\":\"J. Zhang\"},{\"authorId\":\"40911632\",\"name\":\"Sai Praneeth Karimireddy\"},{\"authorId\":\"2799898\",\"name\":\"Andreas Veit\"},{\"authorId\":\"5076305\",\"name\":\"Seungyeon Kim\"},{\"authorId\":\"1981186\",\"name\":\"S. Reddi\"},{\"authorId\":\"46574622\",\"name\":\"S. Kumar\"},{\"authorId\":\"48880404\",\"name\":\"S. Sra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d59f87506326970f676d3293c99d8080a3b724f3\",\"title\":\"Why are Adaptive Methods Good for Attention Models?\",\"url\":\"https://www.semanticscholar.org/paper/d59f87506326970f676d3293c99d8080a3b724f3\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2010.11041\",\"authors\":[{\"authorId\":\"1790510695\",\"name\":\"Jie Liu\"},{\"authorId\":\"5739094\",\"name\":\"Chen Lin\"},{\"authorId\":\"114282346\",\"name\":\"Chuming Li\"},{\"authorId\":\"84200540\",\"name\":\"Lu Sheng\"},{\"authorId\":\"152669122\",\"name\":\"Ming Sun\"},{\"authorId\":\"1721677\",\"name\":\"J. Yan\"},{\"authorId\":\"3001348\",\"name\":\"Wanli Ouyang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"bc84178c6b4b19490fb31815c2c15439337a2c89\",\"title\":\"Adaptive Gradient Method with Resilience and Momentum\",\"url\":\"https://www.semanticscholar.org/paper/bc84178c6b4b19490fb31815c2c15439337a2c89\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1907.10258\",\"authors\":[{\"authorId\":\"46252834\",\"name\":\"Qingyi Zhou\"},{\"authorId\":\"1789656\",\"name\":\"F. Zhang\"},{\"authorId\":\"3114824\",\"name\":\"Chuanchuan Yang\"}],\"doi\":\"10.1109/JLT.2020.2991028\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5dc87099ed98cd08496fb9f837b3dc4f0b566cd3\",\"title\":\"AdaNN: Adaptive Neural Network-Based Equalizer via Online Semi-Supervised Learning\",\"url\":\"https://www.semanticscholar.org/paper/5dc87099ed98cd08496fb9f837b3dc4f0b566cd3\",\"venue\":\"Journal of Lightwave Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51292578\",\"name\":\"A. Winnicka\"},{\"authorId\":\"35619046\",\"name\":\"K. Kesik\"},{\"authorId\":\"1734496\",\"name\":\"Dawid Po\\u0142ap\"}],\"doi\":\"10.15439/2019F28\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"a0facf4000f0a3b28823ce8be819b59e9f52448d\",\"title\":\"Signature analysis system using a convolutional neural network\",\"url\":\"https://www.semanticscholar.org/paper/a0facf4000f0a3b28823ce8be819b59e9f52448d\",\"venue\":\"2019 Federated Conference on Computer Science and Information Systems (FedCSIS)\",\"year\":2019},{\"arxivId\":\"2006.04429\",\"authors\":[{\"authorId\":\"50561307\",\"name\":\"J. Zhang\"},{\"authorId\":\"3376305\",\"name\":\"Hongzhou Lin\"},{\"authorId\":\"3225635\",\"name\":\"Subhro Das\"},{\"authorId\":\"3072326\",\"name\":\"S. Sra\"},{\"authorId\":\"1688304\",\"name\":\"A. Jadbabaie\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ccc4c8ccb5dbf6df2c384c276bc701f42bf6b83e\",\"title\":\"Stochastic Optimization with Non-stationary Noise\",\"url\":\"https://www.semanticscholar.org/paper/ccc4c8ccb5dbf6df2c384c276bc701f42bf6b83e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.11015\",\"authors\":[{\"authorId\":\"34992579\",\"name\":\"S. Dubey\"},{\"authorId\":\"2791192\",\"name\":\"Soumendu Chakraborty\"},{\"authorId\":\"1842981\",\"name\":\"S. K. Roy\"},{\"authorId\":\"34356161\",\"name\":\"Snehasis Mukherjee\"},{\"authorId\":\"49551038\",\"name\":\"S. Singh\"},{\"authorId\":\"153733744\",\"name\":\"Bidyut. B. Chaudhuri\"}],\"doi\":\"10.1109/TNNLS.2019.2955777\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e7ff2e0454039b8a3605d16cdf73a747f1e0628c\",\"title\":\"diffGrad: An Optimization Method for Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/e7ff2e0454039b8a3605d16cdf73a747f1e0628c\",\"venue\":\"IEEE Transactions on Neural Networks and Learning Systems\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10785490\",\"name\":\"H. Zhong\"},{\"authorId\":\"2424252\",\"name\":\"Zaiyi Chen\"},{\"authorId\":\"48753416\",\"name\":\"C. Qin\"},{\"authorId\":\"145863758\",\"name\":\"Zai Huang\"},{\"authorId\":\"3113725\",\"name\":\"V. Zheng\"},{\"authorId\":\"41157498\",\"name\":\"T. Xu\"},{\"authorId\":\"144378760\",\"name\":\"E. Chen\"}],\"doi\":\"10.1007/s11704-019-8457-x\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"ce7d06bcf254f1b5c28b867cd55165f94db09135\",\"title\":\"Adam revisited: a weighted past gradients perspective\",\"url\":\"https://www.semanticscholar.org/paper/ce7d06bcf254f1b5c28b867cd55165f94db09135\",\"venue\":\"Frontiers of Computer Science\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2008191597\",\"name\":\"Nazmus Saqib\"},{\"authorId\":\"9145061\",\"name\":\"G. M. Rafiquzzaman\"}],\"doi\":\"10.1109/TENSYMP50017.2020.9230585\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"55d53171b6ade2146ae59ca64a347999ffad25a7\",\"title\":\"Image Classification using DNN with an Improved Optimizer\",\"url\":\"https://www.semanticscholar.org/paper/55d53171b6ade2146ae59ca64a347999ffad25a7\",\"venue\":\"2020 IEEE Region 10 Symposium (TENSYMP)\",\"year\":2020},{\"arxivId\":\"2004.09740\",\"authors\":[{\"authorId\":\"50135338\",\"name\":\"Wen-Jie Li\"},{\"authorId\":\"38122172\",\"name\":\"Zhaoyang Zhang\"},{\"authorId\":\"48630464\",\"name\":\"Xinjiang Wang\"},{\"authorId\":\"145287455\",\"name\":\"Ping Luo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c7ab6660706c2010df1f1a45780ae2871ab97149\",\"title\":\"AdaX: Adaptive Gradient Descent with Exponential Long Term Memory\",\"url\":\"https://www.semanticscholar.org/paper/c7ab6660706c2010df1f1a45780ae2871ab97149\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1808.02941\",\"authors\":[{\"authorId\":\"2347508\",\"name\":\"Xiangyi Chen\"},{\"authorId\":\"143743061\",\"name\":\"Sijia Liu\"},{\"authorId\":\"39447572\",\"name\":\"Ruoyu Sun\"},{\"authorId\":\"1793717\",\"name\":\"M. Hong\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e60dd237328bc92941e0559ab358e6186cdd41de\",\"title\":\"On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization\",\"url\":\"https://www.semanticscholar.org/paper/e60dd237328bc92941e0559ab358e6186cdd41de\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1988888746\",\"name\":\"Liu Yang\"},{\"authorId\":\"1724421\",\"name\":\"Deng Cai\"}],\"doi\":\"10.1016/j.neucom.2020.07.070\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"572abb5eca6b71d8edf8847c3f6336780cd28a35\",\"title\":\"AdaDB: An adaptive gradient method with data-dependent bound\",\"url\":\"https://www.semanticscholar.org/paper/572abb5eca6b71d8edf8847c3f6336780cd28a35\",\"venue\":\"Neurocomputing\",\"year\":2021},{\"arxivId\":\"2007.14166\",\"authors\":[{\"authorId\":\"1409173278\",\"name\":\"Derya Soydaner\"}],\"doi\":\"10.1142/S0218001420520138\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"983def2226650eb14a3dedf5f3fb4f3b8e5c7249\",\"title\":\"A Comparison of Optimization Algorithms for Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/983def2226650eb14a3dedf5f3fb4f3b8e5c7249\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34992579\",\"name\":\"S. Dubey\"},{\"authorId\":\"50592169\",\"name\":\"S. Chakraborty\"},{\"authorId\":\"1842981\",\"name\":\"S. K. Roy\"},{\"authorId\":\"1397741521\",\"name\":\"Snehasis\"},{\"authorId\":\"122556934\",\"name\":\"Mukherjee\"},{\"authorId\":\"49551038\",\"name\":\"S. Singh\"},{\"authorId\":\"1759420\",\"name\":\"B. Chaudhuri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8167acafcf3d045f29ef4b9087fa199e34ec2f85\",\"title\":\"DUBEY et al.: IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS diffGrad: An Optimization Method for Convolutional Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/8167acafcf3d045f29ef4b9087fa199e34ec2f85\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2007.01547\",\"authors\":[{\"authorId\":\"1456064291\",\"name\":\"Robin M. Schmidt\"},{\"authorId\":\"144223390\",\"name\":\"Frank Schneider\"},{\"authorId\":\"2517795\",\"name\":\"Philipp Hennig\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f8ba361f54e66546dfb09165cb3e6d5655b6400\",\"title\":\"Descending through a Crowded Valley - Benchmarking Deep Learning Optimizers\",\"url\":\"https://www.semanticscholar.org/paper/7f8ba361f54e66546dfb09165cb3e6d5655b6400\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.09729\",\"authors\":[{\"authorId\":\"29879678\",\"name\":\"Ahmet Alacaoglu\"},{\"authorId\":\"41158157\",\"name\":\"Yura Malitsky\"},{\"authorId\":\"1844925\",\"name\":\"P. Mertikopoulos\"},{\"authorId\":\"1678641\",\"name\":\"V. Cevher\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"59600463ca4f6b5d02900022b29b6de278154c6b\",\"title\":\"A new regret analysis for Adam-type algorithms\",\"url\":\"https://www.semanticscholar.org/paper/59600463ca4f6b5d02900022b29b6de278154c6b\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"2009.08328\",\"authors\":[{\"authorId\":\"51149606\",\"name\":\"Jeffrey M. Ede\"}],\"doi\":\"10.1088/2632-2153/abd614\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b00e89b4ede90904704dff6b64eed34384dfc083\",\"title\":\"Review: Deep Learning in Electron Microscopy\",\"url\":\"https://www.semanticscholar.org/paper/b00e89b4ede90904704dff6b64eed34384dfc083\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1382569226\",\"name\":\"Jiajie Tan\"},{\"authorId\":\"39774973\",\"name\":\"N. Li\"}],\"doi\":\"10.23919/ChiCC.2019.8865681\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bb90e0d69847df16b4a46cd9596fba9adf18049b\",\"title\":\"Ensemble Learning Based Multi-Color Space in Convolutional Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/bb90e0d69847df16b4a46cd9596fba9adf18049b\",\"venue\":\"2019 Chinese Control Conference (CCC)\",\"year\":2019},{\"arxivId\":\"1811.09358\",\"authors\":[{\"authorId\":\"145085265\",\"name\":\"Fangyu Zou\"},{\"authorId\":\"144779869\",\"name\":\"Li Shen\"},{\"authorId\":\"2750647\",\"name\":\"Zequn Jie\"},{\"authorId\":\"8030859\",\"name\":\"W. Zhang\"},{\"authorId\":null,\"name\":\"Wei Liu\"}],\"doi\":\"10.1109/CVPR.2019.01138\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"27f197e0401b854d14a41829e09209e38fe920b6\",\"title\":\"A Sufficient Condition for Convergences of Adam and RMSProp\",\"url\":\"https://www.semanticscholar.org/paper/27f197e0401b854d14a41829e09209e38fe920b6\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"2010.07468\",\"authors\":[{\"authorId\":\"46251934\",\"name\":\"Juntang Zhuang\"},{\"authorId\":\"1895919\",\"name\":\"T. Tang\"},{\"authorId\":\"1571173988\",\"name\":\"Yifan Ding\"},{\"authorId\":\"1688323\",\"name\":\"S. Tatikonda\"},{\"authorId\":\"5507046\",\"name\":\"N. Dvornek\"},{\"authorId\":\"1932911\",\"name\":\"X. Papademetris\"},{\"authorId\":\"145947161\",\"name\":\"J. Duncan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7dfa2c8025982ab10f8031e6b79c6f9d24d825ee\",\"title\":\"AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients\",\"url\":\"https://www.semanticscholar.org/paper/7dfa2c8025982ab10f8031e6b79c6f9d24d825ee\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":29169707,\"doi\":\"10.24963/ijcai.2019/355\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":4,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"092e84c89dae16d99cd70648a781479c34006bf4\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"143676697\",\"name\":\"Y. Nesterov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"ed910d96802212c9e45d956adaa27d915f5d7469\",\"title\":\"A method for unconstrained convex minimization problem with the rate of convergence o(1/k^2)\",\"url\":\"https://www.semanticscholar.org/paper/ed910d96802212c9e45d956adaa27d915f5d7469\",\"venue\":\"\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martin Zinkevich. Online convex programming\"},{\"authorId\":null,\"name\":\"generalized infinitesimal gradient ascent\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\\u201903\",\"url\":\"\",\"venue\":\"pages 928\\u2013935. AAAI Press,\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nitish Shirish Keskar\"},{\"authorId\":null,\"name\":\"Richard Socher. Improving generalization performance by sw SGD\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1712.07628,\",\"year\":2017},{\"arxivId\":\"1705.08292\",\"authors\":[{\"authorId\":\"144102853\",\"name\":\"A. Wilson\"},{\"authorId\":\"40458654\",\"name\":\"Rebecca Roelofs\"},{\"authorId\":\"144872294\",\"name\":\"Mitchell Stern\"},{\"authorId\":\"1706280\",\"name\":\"Nathan Srebro\"},{\"authorId\":\"9229182\",\"name\":\"B. Recht\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c\",\"title\":\"The Marginal Value of Adaptive Gradient Methods in Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alex Krizhevsky\"},{\"authorId\":null,\"name\":\"Vinod Nair\"},{\"authorId\":null,\"name\":\"Geoffrey Hinton\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cifar-10 (canadian institute for advanced research\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1512.03385\",\"authors\":[{\"authorId\":\"39353098\",\"name\":\"Kaiming He\"},{\"authorId\":\"1771551\",\"name\":\"X. Zhang\"},{\"authorId\":\"3080683\",\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun\"}],\"doi\":\"10.1109/cvpr.2016.90\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"title\":\"Deep Residual Learning for Image Recognition\",\"url\":\"https://www.semanticscholar.org/paper/2c03df8b48bf3fa39054345bafabfeff15bfd11d\",\"venue\":\"2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"145704247\",\"name\":\"J. Martens\"},{\"authorId\":\"35188630\",\"name\":\"G. Dahl\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa7bfd2304201afbb19971ebde87b17e40242e91\",\"title\":\"On the importance of initialization and momentum in deep learning\",\"url\":\"https://www.semanticscholar.org/paper/aa7bfd2304201afbb19971ebde87b17e40242e91\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":\"1705.07774\",\"authors\":[{\"authorId\":\"8604693\",\"name\":\"Lukas Balles\"},{\"authorId\":\"2517795\",\"name\":\"Philipp Hennig\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4da4fb310df2bfa5b9aab217982723634bda4bc\",\"title\":\"Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients\",\"url\":\"https://www.semanticscholar.org/paper/e4da4fb310df2bfa5b9aab217982723634bda4bc\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"38253717\",\"name\":\"H. Braun\"}],\"doi\":\"10.1109/ICNN.1993.298623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"916ceefae4b11dadc3ee754ce590381c568c90de\",\"title\":\"A direct adaptive method for faster backpropagation learning: the RPROP algorithm\",\"url\":\"https://www.semanticscholar.org/paper/916ceefae4b11dadc3ee754ce590381c568c90de\",\"venue\":\"IEEE International Conference on Neural Networks\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1734693\",\"name\":\"John C. Duchi\"},{\"authorId\":\"34840427\",\"name\":\"Elad Hazan\"},{\"authorId\":\"1740765\",\"name\":\"Y. Singer\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"413c1142de9d91804d6d11c67ff3fed59c9fc279\",\"title\":\"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/413c1142de9d91804d6d11c67ff3fed59c9fc279\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergey Zagoruyko\"},{\"authorId\":null,\"name\":\"Nikos Komodakis. Wide residual networks\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1605.07146,\",\"year\":2016},{\"arxivId\":\"1806.06763\",\"authors\":[{\"authorId\":\"7557913\",\"name\":\"J. Chen\"},{\"authorId\":\"9937103\",\"name\":\"Quanquan Gu\"}],\"doi\":\"10.24963/ijcai.2020/448\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a7f661d157cfb689bb35969e1a0fccccf8ba698\",\"title\":\"Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/3a7f661d157cfb689bb35969e1a0fccccf8ba698\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1212.5701\",\"authors\":[{\"authorId\":\"48799969\",\"name\":\"Matthew D. Zeiler\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8729441d734782c3ed532a7d2d9611b438c0a09a\",\"title\":\"ADADELTA: An Adaptive Learning Rate Method\",\"url\":\"https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a\",\"venue\":\"ArXiv\",\"year\":2012},{\"arxivId\":\"1904.09237\",\"authors\":[{\"authorId\":\"1981186\",\"name\":\"S. Reddi\"},{\"authorId\":\"144055676\",\"name\":\"S. Kale\"},{\"authorId\":\"2794322\",\"name\":\"S. Kumar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"43b854bb89f212e2d69a4b64ef0c28ff4d09f666\",\"title\":\"On the Convergence of Adam and Beyond\",\"url\":\"https://www.semanticscholar.org/paper/43b854bb89f212e2d69a4b64ef0c28ff4d09f666\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Duchi\"},{\"authorId\":null,\"name\":\"Elad Hazan\"},{\"authorId\":null,\"name\":\"Yoram Singer. Adaptive subgradient methods for online learning\"},{\"authorId\":null,\"name\":\"stochastic optimization\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Technical Report UCB/EECS-2010-24\",\"url\":\"\",\"venue\":\"EECS Department, University of California, Berkeley, Mar\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chen\"},{\"authorId\":null,\"name\":\"Gu\"},{\"authorId\":null,\"name\":\"2019 Jinghui Chen\"},{\"authorId\":null,\"name\":\"Quanquan Gu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Padam: Closing the generalization gap of adaptive gradient methods in training deep neural networks, 2019\",\"url\":\"\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2018087\",\"name\":\"A. Srivastava\"},{\"authorId\":\"37210858\",\"name\":\"Charles Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2cd8ae29075d7b9b916377ead43b84e485f1399b\",\"title\":\"Proceedings for the 5th International Conference on Learning Representations\",\"url\":\"https://www.semanticscholar.org/paper/2cd8ae29075d7b9b916377ead43b84e485f1399b\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yann Lecun\"},{\"authorId\":null,\"name\":\"Corinna Cortes\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"MNIST handwritten digit database\",\"url\":\"\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kaiming He\"},{\"authorId\":null,\"name\":\"Xiangyu Zhang\"},{\"authorId\":null,\"name\":\"Shaoqing Ren\"},{\"authorId\":null,\"name\":\"Jian Sun. Deep residual learning for image recognition\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1512.03385,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Boris Polyak. Some methods of speeding up the convergenc methods\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"4:1\\u201317\",\"url\":\"\",\"venue\":\"12\",\"year\":1964},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e1f153c6df86d1ca8ecb9561daddfe7a54f901e7\",\"title\":\"Online Convex Programming and Generalized Infinitesimal Gradient Ascent\",\"url\":\"https://www.semanticscholar.org/paper/e1f153c6df86d1ca8ecb9561daddfe7a54f901e7\",\"venue\":\"ICML\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39435526\",\"name\":\"B. Polyak\"}],\"doi\":\"10.1016/0041-5553(64)90137-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4b53e3f719ff983eef867c6d8deac5dbe38aecb4\",\"title\":\"Some methods of speeding up the convergence of iteration methods\",\"url\":\"https://www.semanticscholar.org/paper/4b53e3f719ff983eef867c6d8deac5dbe38aecb4\",\"venue\":\"\",\"year\":1964},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"T. Tieleman\"},{\"authorId\":null,\"name\":\"G. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lecture 6.5\\u2014RmsProp: Divide the gradient by a running average of its recent magnitude\",\"url\":\"\",\"venue\":\"COURSERA: Neural Networks for Machine Learning,\",\"year\":2012},{\"arxivId\":\"1810.00143\",\"authors\":[{\"authorId\":\"145385776\",\"name\":\"Zhiming Zhou\"},{\"authorId\":\"153441799\",\"name\":\"Q. Zhang\"},{\"authorId\":\"46182317\",\"name\":\"Guansong Lu\"},{\"authorId\":\"49527724\",\"name\":\"Hongwei Wang\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"1811427\",\"name\":\"Y. Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb\",\"title\":\"AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods\",\"url\":\"https://www.semanticscholar.org/paper/9f88722cbc4107e3c3d0e1c7934cc7f1d5ae4fdb\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1712.07628\",\"authors\":[{\"authorId\":\"2844898\",\"name\":\"N. Keskar\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f253d759d99e92888bf9eb595c59cf962fd9069\",\"title\":\"Improving Generalization Performance by Switching from Adam to SGD\",\"url\":\"https://www.semanticscholar.org/paper/8f253d759d99e92888bf9eb595c59cf962fd9069\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1688882\",\"name\":\"Y. LeCun\"},{\"authorId\":\"145115014\",\"name\":\"Corinna Cortes\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2\",\"title\":\"The mnist database of handwritten digits\",\"url\":\"https://www.semanticscholar.org/paper/dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2\",\"venue\":\"\",\"year\":2005},{\"arxivId\":\"1712.09913\",\"authors\":[{\"authorId\":\"79482877\",\"name\":\"Hao Li\"},{\"authorId\":\"144897102\",\"name\":\"Zheng Xu\"},{\"authorId\":\"2189083\",\"name\":\"G. Taylor\"},{\"authorId\":\"1962083\",\"name\":\"T. Goldstein\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"6baca6351dc55baac44f0416e74a7e0ba2bfd03e\",\"title\":\"Visualizing the Loss Landscape of Neural Nets\",\"url\":\"https://www.semanticscholar.org/paper/6baca6351dc55baac44f0416e74a7e0ba2bfd03e\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G T. Tieleman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hinton\",\"url\":\"\",\"venue\":\"Lecture 6.5\\u2014RmsProp: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yann LeCun\"},{\"authorId\":null,\"name\":\"Corinna Cortes. MNIST handwritten digit database.\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"URL http://yann\",\"url\":\"\",\"venue\":\"lecun.com/exdb/mnist/.\",\"year\":2010}],\"title\":\"Nostalgic Adam: Weighing more of the past gradients when designing the adaptive learning rate\",\"topics\":[{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"},{\"topic\":\"Rate of convergence\",\"topicId\":\"45663\",\"url\":\"https://www.semanticscholar.org/topic/45663\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Flaw hypothesis methodology\",\"topicId\":\"2163842\",\"url\":\"https://www.semanticscholar.org/topic/2163842\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Perturbation theory\",\"topicId\":\"5972\",\"url\":\"https://www.semanticscholar.org/topic/5972\"}],\"url\":\"https://www.semanticscholar.org/paper/092e84c89dae16d99cd70648a781479c34006bf4\",\"venue\":\"IJCAI\",\"year\":2019}\n"