"{\"abstract\":\"The soundness and optimality of a plan depends on the correctness of the domain model. Specifying complete domain models can be difficult when interactions between an agent and its environment are complex. We propose a model-based reinforcement learning (MBRL) approach to solve planning problems with unknown models. The model is learned incrementally over episodes using only experiences from the current episode which suits non-stationary environments. We introduce the novel concept of reliability as an intrinsic motivation for MBRL, and a method to learn from failure to prevent repeated instances of similar failures. Our motivation is to improve the learning efficiency and goaldirectedness of MBRL. We evaluate our work with experimental results for three planning domains.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"100485748\",\"name\":\"J. H. Ng\",\"url\":\"https://www.semanticscholar.org/author/100485748\"},{\"authorId\":\"35407423\",\"name\":\"Ronald P. A. Petrick\",\"url\":\"https://www.semanticscholar.org/author/35407423\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"50469255\",\"name\":\"P. Verma\"},{\"authorId\":\"1641679453\",\"name\":\"Shashank Rao Marpally\"},{\"authorId\":\"152735970\",\"name\":\"S. Srivastava\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3772cdcdbaccd8c118fc0870557d1bd9ee01a743\",\"title\":\"Asking the Right Questions: Learning Interpretable Action Models Through Query Answering\",\"url\":\"https://www.semanticscholar.org/paper/3772cdcdbaccd8c118fc0870557d1bd9ee01a743\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.13037\",\"authors\":[{\"authorId\":\"3379438\",\"name\":\"Vasanth Sarathy\"},{\"authorId\":\"19185302\",\"name\":\"Daniel Kasenberg\"},{\"authorId\":\"1808903\",\"name\":\"Shivam Goel\"},{\"authorId\":\"1715858\",\"name\":\"J. Sinapov\"},{\"authorId\":\"121848090\",\"name\":\"matthias. scheutz\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"9090c0d302eb6f310c55ae95b360c6606717a366\",\"title\":\"SPOTTER: Extending Symbolic Planning Operators through Targeted Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9090c0d302eb6f310c55ae95b360c6606717a366\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1912.12613\",\"authors\":[{\"authorId\":\"39765564\",\"name\":\"Pulkit Verma\"},{\"authorId\":\"145305731\",\"name\":\"Siddharth Srivastava\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"22c0ea7a5c2fa8c3d752e7d2f7338091de088a34\",\"title\":\"Learning Interpretable Models for Black-Box Agents\",\"url\":\"https://www.semanticscholar.org/paper/22c0ea7a5c2fa8c3d752e7d2f7338091de088a34\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2004.11456\",\"authors\":[{\"authorId\":\"1656712399\",\"name\":\"Yohei Hayamizu\"},{\"authorId\":\"145306763\",\"name\":\"S. Amiri\"},{\"authorId\":\"1383234530\",\"name\":\"Kishan Chandan\"},{\"authorId\":\"2601786\",\"name\":\"Shiqi Zhang\"},{\"authorId\":\"2093424\",\"name\":\"K. Takadama\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9037d81b7cb4fcf9823b7f49ee9924a7df27b21\",\"title\":\"Guided Dyna-Q for Mobile Robot Exploration and Navigation\",\"url\":\"https://www.semanticscholar.org/paper/b9037d81b7cb4fcf9823b7f49ee9924a7df27b21\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.08299\",\"authors\":[{\"authorId\":\"1810686\",\"name\":\"Rohan Chitnis\"},{\"authorId\":\"39047272\",\"name\":\"T. Silver\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"},{\"authorId\":\"1388700951\",\"name\":\"Tomas Lozano-Perez\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"2192fcfbd5040d2bb8d7c7ca7c2138d18534ac9b\",\"title\":\"GLIB: Efficient Exploration for Relational Model-Based Reinforcement Learning via Goal-Literal Babbling\",\"url\":\"https://www.semanticscholar.org/paper/2192fcfbd5040d2bb8d7c7ca7c2138d18534ac9b\",\"venue\":\"\",\"year\":2020}],\"corpusId\":199466378,\"doi\":\"10.24963/ijcai.2019/443\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"7deb60e912c3f904fd5dcbd0aa2ec150c5defb53\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kira Mour\\u00e3o\"},{\"authorId\":null,\"name\":\"Luke S Zettlemoyer\"},{\"authorId\":null,\"name\":\"Ronald P.A. Petrick\"},{\"authorId\":null,\"name\":\"Mark Steedman. Learning STRIPS operators from noisy\"},{\"authorId\":null,\"name\":\"incomplete observations. In Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"UAI\",\"url\":\"\",\"venue\":\"pages 614\\u2013623,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145526918\",\"name\":\"Y. Gil\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50019-2\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"13143dcdd980697b4ed16e7e99dc100495731ac6\",\"title\":\"Learning by Experimentation: Incremental Refinement of Incomplete Planning Domains\",\"url\":\"https://www.semanticscholar.org/paper/13143dcdd980697b4ed16e7e99dc100495731ac6\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730156\",\"name\":\"Carlos Guestrin\"},{\"authorId\":\"2228643\",\"name\":\"Relu Patrascu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8678d49a82020773dfe1e8b5fa789aa927ffcdcb\",\"title\":\"Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/8678d49a82020773dfe1e8b5fa789aa927ffcdcb\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrey Kolobov\"},{\"authorId\":null,\"name\":\"Mausam\"},{\"authorId\":null,\"name\":\"Daniel S Weld. LRTDP vs. UCT for online probabilistic pla Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AAAI\",\"url\":\"\",\"venue\":\"page 1786\\u20131792,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681946\",\"name\":\"Carlos Diuk\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"1700606\",\"name\":\"B. Leffler\"}],\"doi\":\"10.1145/1553374.1553406\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6316daea76f71804a0f3cd1e150d4df77a1bf517\",\"title\":\"The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/6316daea76f71804a0f3cd1e150d4df77a1bf517\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144926179\",\"name\":\"T. Walsh\"},{\"authorId\":\"145362925\",\"name\":\"K. Subramanian\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"1681946\",\"name\":\"Carlos Diuk\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"55ae7e298df17b53ecbadca7d301e1d392a0d9a4\",\"title\":\"Generalizing Apprenticeship Learning across Hypothesis Classes\",\"url\":\"https://www.semanticscholar.org/paper/55ae7e298df17b53ecbadca7d301e1d392a0d9a4\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Todd Hester\"},{\"authorId\":null,\"name\":\"Peter Stone. Intrinsically motivated model learning for robots\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AIJ\",\"url\":\"\",\"venue\":\"247:170\\u2013186,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tobias Lang\"},{\"authorId\":null,\"name\":\"Marc Toussaint\"},{\"authorId\":null,\"name\":\"Kristian Kersting. Exploration in relational domains for mo learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"JMLR\",\"url\":\"\",\"venue\":\"13:3725\\u20133768,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Iain Little\"},{\"authorId\":null,\"name\":\"Sylvie Thiebaux. Probabilistic planning vs. replanning. I Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICAPS Workshop on the International Planning Competition: Past\",\"url\":\"\",\"venue\":\"Present and Future,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Nuttapong Chentanez\"},{\"authorId\":null,\"name\":\"Andrew G Barto\"},{\"authorId\":null,\"name\":\"Satinder P Singh. Intrinsically motivated reinforcement lea Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"NIPS\",\"url\":\"\",\"venue\":\"pages 1281\\u20131288,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas J Walsh\"},{\"authorId\":null,\"name\":\"Kaushik Subramanian\"},{\"authorId\":null,\"name\":\"Michael L Littman\"},{\"authorId\":null,\"name\":\"Carlos Diuk. Generalizing apprenticeship learning across Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 1119\\u20131126,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael Kearns\"},{\"authorId\":null,\"name\":\"Daphne Koller. Efficient reinforcement learning in factor Proc\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"IJCAI\",\"url\":\"\",\"venue\":\"pages 740\\u2013747,\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1829191\",\"name\":\"H. Younes\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"145695954\",\"name\":\"D. Weissman\"},{\"authorId\":\"35775902\",\"name\":\"J. Asmuth\"}],\"doi\":\"10.1613/jair.1880\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05051f38f0110f2cee1307e14145f0628a4441a8\",\"title\":\"The First Probabilistic Track of the International Planning Competition\",\"url\":\"https://www.semanticscholar.org/paper/05051f38f0110f2cee1307e14145f0628a4441a8\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H\\u00e5kan LS Younes\"},{\"authorId\":null,\"name\":\"Michael L Littman\"},{\"authorId\":null,\"name\":\"David Weissman\"},{\"authorId\":null,\"name\":\"John Asmuth. The first probabilistic track of the Inter Competition\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"24:851\\u2013887,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Diuk\"},{\"authorId\":null,\"name\":\"Lihong Li\"},{\"authorId\":null,\"name\":\"Bethany R Leffler. The adaptive k-meteorologists problem\"},{\"authorId\":null,\"name\":\"its application to structure learning\"},{\"authorId\":null,\"name\":\"feature selection in reinforcement learning. In Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 249\\u2013256,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H\\u00e5kan LS Younes\"},{\"authorId\":null,\"name\":\"Michael L Littman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"PPDDL1\",\"url\":\"\",\"venue\":\"0: An extension to PDDL for expressing planning domains with probabilistic effects. Technical Report CMU-CS-04-162,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christophe Rodrigues\"},{\"authorId\":null,\"name\":\"Pierre G\\u00e9rard\"},{\"authorId\":null,\"name\":\"C\\u00e9line Rouveirol. Incremental learning of relational acti Proc\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ILP\",\"url\":\"\",\"venue\":\"pages 206\\u2013213,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31710455\",\"name\":\"D. M. Mart\\u00ednez\"},{\"authorId\":\"40910923\",\"name\":\"G. Aleny\\u00e0\"},{\"authorId\":\"1735976\",\"name\":\"C. Torras\"},{\"authorId\":\"143681572\",\"name\":\"T. Ribeiro\"},{\"authorId\":\"1728869\",\"name\":\"Katsumi Inoue\"}],\"doi\":\"10.13039/501100003176\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2c652e97d951bb101cd2597a4a846591803d9b22\",\"title\":\"Learning Relational Dynamics of Stochastic Domains for Planning\",\"url\":\"https://www.semanticscholar.org/paper/2c652e97d951bb101cd2597a4a846591803d9b22\",\"venue\":\"ICAPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H\\u00e5kan LS Younes\"},{\"authorId\":null,\"name\":\"Michael L Littman\"},{\"authorId\":null,\"name\":\"David Weissman\"},{\"authorId\":null,\"name\":\"John Asmuth. The first probabilistic track of the Inter Competition\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"24:851\\u2013887,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hankz Hankui Zhuo\"},{\"authorId\":null,\"name\":\"Tuan Anh Nguyen\"},{\"authorId\":null,\"name\":\"Subbarao Kambhampati. Refining incomplete planning domain m Proc\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"IJCAI\",\"url\":\"\",\"venue\":\"pages 2451\\u20132458,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1717495\",\"name\":\"X. Wang\"}],\"doi\":\"10.1016/b978-1-55860-377-6.50074-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b779417dce58b1a6ef9ac6cad42850d87ef79f05\",\"title\":\"Learning by Observation and Practice: An Incremental Approach for Planning Operator Acquisition\",\"url\":\"https://www.semanticscholar.org/paper/b779417dce58b1a6ef9ac6cad42850d87ef79f05\",\"venue\":\"ICML\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hanna M Pasula\"},{\"authorId\":null,\"name\":\"Luke S Zettlemoyer\"},{\"authorId\":null,\"name\":\"Leslie Pack Kaelbling. Learning symbolic models of stocha domains\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"29:309\\u2013352,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Guestrin\"},{\"authorId\":null,\"name\":\"Relu Patrascu\"},{\"authorId\":null,\"name\":\"Dale Schuurmans. Algorithm-directed exploration for mod Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 235\\u2013242,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2348257\",\"name\":\"C. Rodrigues\"},{\"authorId\":\"90951635\",\"name\":\"P. G\\u00e9rard\"},{\"authorId\":\"2321708\",\"name\":\"C. Rouveirol\"}],\"doi\":\"10.1007/978-3-642-21295-6_24\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9e06ab9603512ff0abdc1920b81bbafc8be8d703\",\"title\":\"Incremental Learning of Relational Action Models in Noisy Environments\",\"url\":\"https://www.semanticscholar.org/paper/9e06ab9603512ff0abdc1920b81bbafc8be8d703\",\"venue\":\"ILP\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoshua Bengio\"},{\"authorId\":null,\"name\":\"J\\u00e9r\\u00f4me Louradour\"},{\"authorId\":null,\"name\":\"Ronan Collobert\"},{\"authorId\":null,\"name\":\"Jason Weston. Curriculum learning. In Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 41\\u201348,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ronen I Brafman\"},{\"authorId\":null,\"name\":\"Moshe Tennenholtz. R-MAX - A general polynomial time alg learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"JMLR\",\"url\":\"\",\"venue\":\"3:213\\u2013231,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48999805\",\"name\":\"J. Cullen\"},{\"authorId\":\"6503751\",\"name\":\"A. Bryman\"}],\"doi\":\"10.1111/J.1468-0394.1988.TB00065.X\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d79ba65f8560c25dae5a58ab2072076fe3a88e24\",\"title\":\"The Knowledge Acquisition Bottleneck: Time for Reassessment?\",\"url\":\"https://www.semanticscholar.org/paper/d79ba65f8560c25dae5a58ab2072076fe3a88e24\",\"venue\":\"\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143979239\",\"name\":\"T. Oates\"},{\"authorId\":\"144580830\",\"name\":\"P. Cohen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b8d0f572fddfa743b011bda77eb60a0727916af\",\"title\":\"Learning Planning Operators with Conditional and Probabilistic Effects\",\"url\":\"https://www.semanticscholar.org/paper/0b8d0f572fddfa743b011bda77eb60a0727916af\",\"venue\":\"\",\"year\":1996}],\"title\":\"Incremental Learning of Planning Actions in Model-Based Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/7deb60e912c3f904fd5dcbd0aa2ec150c5defb53\",\"venue\":\"IJCAI\",\"year\":2019}\n"