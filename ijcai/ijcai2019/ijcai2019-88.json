"{\"abstract\":\"Multiagent algorithms often aim to accurately predict the behaviors of other agents and find a best response accordingly. Previous works usually assume an opponent uses a stationary strategy or randomly switches among several stationary ones. However, an opponent may exhibit more sophisticated behaviors by adopting more advanced reasoning strategies, e.g., using a Bayesian reasoning strategy. This paper proposes a novel approach called Bayes-ToMoP which can efficiently detect the strategy of opponents using either stationary or higher-level reasoning strategies. Bayes-ToMoP also supports the detection of previously unseen policies and learning a best-response policy accordingly. We provide a theoretical guarantee of the optimality on detecting the opponent's strategies. We also propose a deep version of Bayes-ToMoP by extending Bayes-ToMoP with DRL techniques. Experimental results show both Bayes-ToMoP and deep Bayes-ToMoP outperform the state-of-the-art approaches when faced with different types of opponents in two-agent competitive games.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"3449314\",\"name\":\"Tianpei Yang\",\"url\":\"https://www.semanticscholar.org/author/3449314\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\",\"url\":\"https://www.semanticscholar.org/author/40513470\"},{\"authorId\":\"1889014\",\"name\":\"Zhaopeng Meng\",\"url\":\"https://www.semanticscholar.org/author/1889014\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\",\"url\":\"https://www.semanticscholar.org/author/1797369\"},{\"authorId\":\"46323461\",\"name\":\"Y. Zheng\",\"url\":\"https://www.semanticscholar.org/author/46323461\"},{\"authorId\":\"144638094\",\"name\":\"Ze Zheng\",\"url\":\"https://www.semanticscholar.org/author/144638094\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"21341074\",\"name\":\"C. Wang\"},{\"authorId\":\"48444479\",\"name\":\"Hao Chen\"},{\"authorId\":\"102168409\",\"name\":\"C. Yan\"},{\"authorId\":\"48147720\",\"name\":\"Xiaojia Xiang\"}],\"doi\":\"10.1109/AGENTS.2019.8929148\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d78ce91567375833c4bbc7a3c81524037b36e723\",\"title\":\"Reinforcement Learning with an Extended Classifier System in Zero-sum Markov Games\",\"url\":\"https://www.semanticscholar.org/paper/d78ce91567375833c4bbc7a3c81524037b36e723\",\"venue\":\"2019 IEEE International Conference on Agents (ICA)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1418499676\",\"name\":\"Yan Zheng\"},{\"authorId\":\"153645633\",\"name\":\"Changjie Fan\"},{\"authorId\":\"145564459\",\"name\":\"X. Xie\"},{\"authorId\":\"144121517\",\"name\":\"Ting Su\"},{\"authorId\":\"143828252\",\"name\":\"L. Ma\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"2528357\",\"name\":\"Zhao-Peng Meng\"},{\"authorId\":null,\"name\":\"Yang Liu\"},{\"authorId\":\"2613907\",\"name\":\"R. Shen\"},{\"authorId\":\"152829349\",\"name\":\"Yingfeng Chen\"}],\"doi\":\"10.1109/ASE.2019.00077\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"70af259ee51166292f485093a1ff0abf3bd6f118\",\"title\":\"Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/70af259ee51166292f485093a1ff0abf3bd6f118\",\"venue\":\"2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145982842\",\"name\":\"Wei Du\"},{\"authorId\":\"49099486\",\"name\":\"Shifei Ding\"},{\"authorId\":\"49099486\",\"name\":\"Shifei Ding\"}],\"doi\":\"10.1007/s10462-020-09938-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3002d4f76f36bd78bf9e2118fbc830648cc5ea08\",\"title\":\"A survey on multi-agent deep reinforcement learning: from the perspective of challenges and applications\",\"url\":\"https://www.semanticscholar.org/paper/3002d4f76f36bd78bf9e2118fbc830648cc5ea08\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1810.05587\",\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1007/s10458-019-09421-1\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b\",\"title\":\"Is multiagent deep reinforcement learning the answer or the question? A brief survey\",\"url\":\"https://www.semanticscholar.org/paper/3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2009.14180\",\"authors\":[{\"authorId\":\"51142124\",\"name\":\"M. O. Smith\"},{\"authorId\":\"52442941\",\"name\":\"T. Anthony\"},{\"authorId\":null,\"name\":\"Yongzhao Wang\"},{\"authorId\":\"1796536\",\"name\":\"Michael P. Wellman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"067a389ad4129d4ace4bb9a0bbb3a33f935bffde\",\"title\":\"Learning to Play against Any Mixture of Opponents\",\"url\":\"https://www.semanticscholar.org/paper/067a389ad4129d4ace4bb9a0bbb3a33f935bffde\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.06508\",\"authors\":[{\"authorId\":\"21704901\",\"name\":\"Connor Brooks\"},{\"authorId\":\"35809186\",\"name\":\"D. Szafir\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d1200e127cb7e71fc677dcb54dd3ea33362de874\",\"title\":\"Building Second-Order Mental Models for Human-Robot Interaction\",\"url\":\"https://www.semanticscholar.org/paper/d1200e127cb7e71fc677dcb54dd3ea33362de874\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2005.12553\",\"authors\":[{\"authorId\":\"48444479\",\"name\":\"Hao Chen\"},{\"authorId\":\"21341074\",\"name\":\"C. Wang\"},{\"authorId\":\"3893930\",\"name\":\"J. Huang\"},{\"authorId\":\"9072289\",\"name\":\"Jian-xing Gong\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d77d6efcda514fe15b971de0d99e537d2958499a\",\"title\":\"Efficient Use of heuristics for accelerating XCS-based Policy Learning in Markov Games\",\"url\":\"https://www.semanticscholar.org/paper/d77d6efcda514fe15b971de0d99e537d2958499a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48444479\",\"name\":\"Hao Chen\"},{\"authorId\":\"117578357\",\"name\":\"J. Huang\"},{\"authorId\":\"145014498\",\"name\":\"Q. Liu\"},{\"authorId\":\"21341074\",\"name\":\"C. Wang\"},{\"authorId\":\"2013743481\",\"name\":\"Hanqiang Deng\"}],\"doi\":\"10.1109/ICIEA48937.2020.9248178\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"59d5a857d4fa414ad29d849f64210c6e9f68937b\",\"title\":\"Detecting and Tracing Multi-Strategic Agents with Opponent Modelling and Bayesian Policy Reuse\",\"url\":\"https://www.semanticscholar.org/paper/59d5a857d4fa414ad29d849f64210c6e9f68937b\",\"venue\":\"2020 15th IEEE Conference on Industrial Electronics and Applications (ICIEA)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1499181461\",\"name\":\"Yan Zheng\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"70882195\",\"name\":\"Zongzhang Zhang\"},{\"authorId\":\"2528357\",\"name\":\"Zhao-Peng Meng\"},{\"authorId\":\"1708208678\",\"name\":\"Xiao-Tian Hao\"}],\"doi\":\"10.1007/s11390-020-9967-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"cab9ccf5b316c268e863d1206e90831413629d74\",\"title\":\"Efficient Multiagent Policy Optimization Based on Weighted Estimators in Stochastic Cooperative Environments\",\"url\":\"https://www.semanticscholar.org/paper/cab9ccf5b316c268e863d1206e90831413629d74\",\"venue\":\"Journal of Computer Science and Technology\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1007/s10458-019-09421-1\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"390364c3986d05ad71a40a967b9cc12aa30e4305\",\"title\":\"A survey and critique of multiagent deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/390364c3986d05ad71a40a967b9cc12aa30e4305\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2019}],\"corpusId\":168169789,\"doi\":\"10.24963/ijcai.2019/88\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":2,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"fb2993399b894db86972106aab363559e2b9729e\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1779856\",\"name\":\"J. Crandall\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"7fd833e579b6148b29db1ca9dfc21d22adf47eea\",\"title\":\"Just add Pepper: extending learning algorithms for repeated matrix games to repeated Markov games\",\"url\":\"https://www.semanticscholar.org/paper/7fd833e579b6148b29db1ca9dfc21d22adf47eea\",\"venue\":\"AAMAS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"1689073\",\"name\":\"M. Kaisers\"}],\"doi\":\"10.1007/978-3-319-71682-4_15\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2ad5aaf0dbb2f4f101ac679b78278be0289f8c95\",\"title\":\"Towards a Fast Detection of Opponents in Repeated Stochastic Games\",\"url\":\"https://www.semanticscholar.org/paper/2ad5aaf0dbb2f4f101ac679b78278be0289f8c95\",\"venue\":\"AAMAS Workshops\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38076874\",\"name\":\"R. Powers\"},{\"authorId\":\"1701353\",\"name\":\"Y. Shoham\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d6f5acc8442b4e6dedd3daf8983a70c496c6fb51\",\"title\":\"Learning against opponents with bounded memory\",\"url\":\"https://www.semanticscholar.org/paper/d6f5acc8442b4e6dedd3daf8983a70c496c6fb51\",\"venue\":\"IJCAI\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"2360436\",\"name\":\"Yusen Zhan\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144763689\",\"name\":\"L. Sucar\"},{\"authorId\":\"2643564\",\"name\":\"E. Cote\"}],\"doi\":\"10.1007/s10458-016-9352-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4034c3f86a987efa3db441b6795eb6a9c978720c\",\"title\":\"Efficiently detecting switches against non-stationary opponents\",\"url\":\"https://www.semanticscholar.org/paper/4034c3f86a987efa3db441b6795eb6a9c978720c\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"70056582\",\"name\":\"J. V. Neumann\"}],\"doi\":\"10.1007/BF01448847\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"90d88e38b1fc555012394824d7e9a36171fc0d23\",\"title\":\"Zur Theorie der Gesellschaftsspiele\",\"url\":\"https://www.semanticscholar.org/paper/90d88e38b1fc555012394824d7e9a36171fc0d23\",\"venue\":\"\",\"year\":1928},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21161348\",\"name\":\"Chris L. Baker\"},{\"authorId\":\"2276622\",\"name\":\"R. Saxe\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58eee36a6e0c59622d800ae4131c29b96f1595c8\",\"title\":\"Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution\",\"url\":\"https://www.semanticscholar.org/paper/58eee36a6e0c59622d800ae4131c29b96f1595c8\",\"venue\":\"CogSci\",\"year\":2011},{\"arxivId\":\"1709.08071\",\"authors\":[{\"authorId\":\"1961238\",\"name\":\"Stefano V. Albrecht\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1016/j.artint.2018.01.002\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"9bf2d6526b480f95fed9b0e0a1125cf07d11e01d\",\"title\":\"Autonomous agents modelling other agents: A comprehensive survey and open problems\",\"url\":\"https://www.semanticscholar.org/paper/9bf2d6526b480f95fed9b0e0a1125cf07d11e01d\",\"venue\":\"Artif. Intell.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50027-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"title\":\"Markov Games as a Framework for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7fbf55baccbc5fdc7ded1ba18330605909aef5e5\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"2831294\",\"name\":\"Benjamin Rosman\"},{\"authorId\":\"144763689\",\"name\":\"L. Sucar\"},{\"authorId\":\"2643564\",\"name\":\"E. Cote\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"ea9e4a8441df47692353d728bb2d8d1217a27233\",\"title\":\"Identifying and Tracking Switching, Non-Stationary Opponents: A Bayesian Approach\",\"url\":\"https://www.semanticscholar.org/paper/ea9e4a8441df47692353d728bb2d8d1217a27233\",\"venue\":\"AAAI Workshop: Multiagent Interaction without Prior Coordination\",\"year\":2016},{\"arxivId\":\"1707.09183\",\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"1689073\",\"name\":\"M. Kaisers\"},{\"authorId\":\"1738050\",\"name\":\"T. Baarslag\"},{\"authorId\":\"2643564\",\"name\":\"E. Cote\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0717e64a010a8cc45da64127b5bce10afc9d6d01\",\"title\":\"A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity\",\"url\":\"https://www.semanticscholar.org/paper/0717e64a010a8cc45da64127b5bce10afc9d6d01\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1809.04240\",\"authors\":[{\"authorId\":\"3449314\",\"name\":\"Tianpei Yang\"},{\"authorId\":\"1889014\",\"name\":\"Zhaopeng Meng\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"},{\"authorId\":\"1391188261\",\"name\":\"Yan Zheng\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"27bf2cba9d7115863cf3a0be65be188014600279\",\"title\":\"Bayes-ToMoP: A Fast Detection and Best Response Algorithm Towards Sophisticated Opponents\",\"url\":\"https://www.semanticscholar.org/paper/27bf2cba9d7115863cf3a0be65be188014600279\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"101370407\",\"name\":\"J. Bourguignon\"}],\"doi\":\"10.1007/BF03017618\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"92a258cd97c3b95bb606b84a54ce24b3f2ec037e\",\"title\":\"Mathematische Annalen\",\"url\":\"https://www.semanticscholar.org/paper/92a258cd97c3b95bb606b84a54ce24b3f2ec037e\",\"venue\":\"\",\"year\":1893},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144626042\",\"name\":\"N. Jennings\"},{\"authorId\":\"9076478\",\"name\":\"K. Sycara\"},{\"authorId\":\"144885648\",\"name\":\"M. Wooldridge\"}],\"doi\":\"10.2174/9781608058242114010003\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d4e52bef2d8656d6732107393d67b1c371123574\",\"title\":\"Autonomous Agents and Multi-Agent Systems\",\"url\":\"https://www.semanticscholar.org/paper/d4e52bef2d8656d6732107393d67b1c371123574\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L. Littman. Markov games as a framework for multi- learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 157\\u2013163,\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1411490468\",\"name\":\"Fernando Fern\\u00e1ndez-Rebollo\"},{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1016/j.robot.2010.03.007\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e108bab11c7dffa0d6532c79dcfc04d3ff1e3155\",\"title\":\"Probabilistic Policy Reuse for inter-task transfer learning\",\"url\":\"https://www.semanticscholar.org/paper/e108bab11c7dffa0d6532c79dcfc04d3ff1e3155\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"He He\"},{\"authorId\":null,\"name\":\"Jordan L. BoydGraber. Opponent modeling in deep reinforce learning\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1804\\u20131813,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ronen I. Brafman\"},{\"authorId\":null,\"name\":\"Moshe Tennenholtz. R-MAX - A general polynomial time alg learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"3:213\\u2013231,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7221959\",\"name\":\"H. D. Weerd\"},{\"authorId\":\"2082305\",\"name\":\"R. Verbrugge\"},{\"authorId\":\"1697177\",\"name\":\"Bart Verheij\"}],\"doi\":\"10.1016/j.artint.2013.05.004\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c1454544b71a99dae16fda4658f3783eee4bceb0\",\"title\":\"How much does it help to know what she knows you know? An agent-based simulation study\",\"url\":\"https://www.semanticscholar.org/paper/c1454544b71a99dae16fda4658f3783eee4bceb0\",\"venue\":\"Artif. Intell.\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"L Michael\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Littman . Markov games as a framework for multi - agent reinforcement learning\",\"url\":\"\",\"venue\":\"Autonomous Agents and Multi - Agent Systems\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yoav Shoham\"},{\"authorId\":null,\"name\":\"Kevin Leyton-Brown. Multiagent Systems - Algorithmic\"},{\"authorId\":null,\"name\":\"Game-Theoretic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Logical Foundations\",\"url\":\"\",\"venue\":\"Cambridge University Press,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7410831\",\"name\":\"M. A. Goodrich\"},{\"authorId\":\"1779856\",\"name\":\"J. Crandall\"},{\"authorId\":\"114547297\",\"name\":\"J. L. Stimpson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2f4d3e4a0c047ed4a1fc2adf7c401c5dedb5bb27\",\"title\":\"Neglect Tolerant Teaming: Issues and Dilemmas\",\"url\":\"https://www.semanticscholar.org/paper/2f4d3e4a0c047ed4a1fc2adf7c401c5dedb5bb27\",\"venue\":\"\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Rob Powers\"},{\"authorId\":null,\"name\":\"Yoav Shoham. Learning against opponents with bounded memory\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Joint Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 817\\u2013822,\",\"year\":2005},{\"arxivId\":\"1709.04326\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"2896187\",\"name\":\"Richard Y. Chen\"},{\"authorId\":\"1401178735\",\"name\":\"Maruan Al-Shedivat\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2080746\",\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1d92cf3f0c86bfde10238fcaf31182a245bc920b\",\"title\":\"Learning with Opponent-Learning Awareness\",\"url\":\"https://www.semanticscholar.org/paper/1d92cf3f0c86bfde10238fcaf31182a245bc920b\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":null,\"name\":\"Michael Kaisers. Towards a fast detection of opponents in games\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of International Conference on Autonomous Agents and Multiagent Systems\",\"url\":\"\",\"venue\":\"pages 239\\u2013257,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Benjamin Rosman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Majd Hawasly , and Subramanian Ramamoorthy . Bayesian policy reuse\",\"url\":\"\",\"venue\":\"Machine Learning Multiagent Systems - Algorithmic , Game - Theoretic , and Logical Foundations\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50012345\",\"name\":\"W. Rudin\"}],\"doi\":\"10.2307/3608793\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88263b99ae7d88ae8719ad463e171058686349f0\",\"title\":\"Principles of mathematical analysis\",\"url\":\"https://www.semanticscholar.org/paper/88263b99ae7d88ae8719ad463e171058686349f0\",\"venue\":\"\",\"year\":1964},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Piotr J. Gmytrasiewicz\"},{\"authorId\":null,\"name\":\"Prashant Doshi. A framework for sequential planning in mult settings\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"24:49\\u201379,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Fernando Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Javier Garc\\u0131\\u0301a\",\"url\":\"\",\"venue\":\"and Manuela M. Veloso. Probabilistic policy reuse for inter-task transfer learning. Robotics and Autonomous Systems, 58(7):866\\u2013871,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48332895\",\"name\":\"M. Wunder\"},{\"authorId\":\"1724247\",\"name\":\"John Robert Yaros\"},{\"authorId\":\"1689073\",\"name\":\"M. Kaisers\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8d012ace5730369ea7f7a96cdc022a64a9834774\",\"title\":\"A framework for modeling population strategies by depth of reasoning\",\"url\":\"https://www.semanticscholar.org/paper/8d012ace5730369ea7f7a96cdc022a64a9834774\",\"venue\":\"AAMAS\",\"year\":2012}],\"title\":\"Towards Efficient Detection and Optimal Response against Sophisticated Opponents\",\"topics\":[{\"topic\":\"Stationary process\",\"topicId\":\"21296\",\"url\":\"https://www.semanticscholar.org/topic/21296\"},{\"topic\":\"Sensor\",\"topicId\":\"1117\",\"url\":\"https://www.semanticscholar.org/topic/1117\"},{\"topic\":\"Randomness\",\"topicId\":\"726\",\"url\":\"https://www.semanticscholar.org/topic/726\"},{\"topic\":\"Network switch\",\"topicId\":\"7961\",\"url\":\"https://www.semanticscholar.org/topic/7961\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"},{\"topic\":\"Driven right leg circuit\",\"topicId\":\"427743\",\"url\":\"https://www.semanticscholar.org/topic/427743\"},{\"topic\":\"Behavior\",\"topicId\":\"3332\",\"url\":\"https://www.semanticscholar.org/topic/3332\"},{\"topic\":\"Bayes Theorem\",\"topicId\":\"114168\",\"url\":\"https://www.semanticscholar.org/topic/114168\"},{\"topic\":\"Reasoning\",\"topicId\":\"928\",\"url\":\"https://www.semanticscholar.org/topic/928\"},{\"topic\":\"Policy\",\"topicId\":\"60001\",\"url\":\"https://www.semanticscholar.org/topic/60001\"}],\"url\":\"https://www.semanticscholar.org/paper/fb2993399b894db86972106aab363559e2b9729e\",\"venue\":\"IJCAI\",\"year\":2019}\n"