"{\"abstract\":\"Reinforcement learning (RL) has had many successes in both \\\"deep\\\" and \\\"shallow\\\" settings. In both cases, significant hyperparameter tuning is often required to achieve good performance. Furthermore, when nonlinear function approximation is used, non-stationarity in the state representation can lead to learning instability. A variety of techniques exist to combat this --- most notably large experience replay buffers or the use of multiple parallel actors. These techniques come at the cost of moving away from the online RL problem as it is traditionally formulated (i.e., a single agent learning online without maintaining a large database of training examples). Meta-learning can potentially help with both these issues by tuning hyperparameters online and allowing the algorithm to more robustly adjust to non-stationarity in a problem. This paper applies meta-gradient descent to derive a set of step-size tuning algorithms specifically for online RL control with eligibility traces. Our novel technique, Metatrace, makes use of an eligibility trace analogous to methods like $TD(\\\\lambda)$. We explore tuning both a single scalar step-size and a separate step-size for each learned parameter. We evaluate Metatrace first for control with linear function approximation in the classic mountain car problem and then in a noisy, non-stationary version. Finally, we apply Metatrace for control with nonlinear function approximation in 5 games in the Arcade Learning Environment where we explore how it impacts learning speed and robustness to initial step-size choice. Results show that the meta-step-size parameter of Metatrace is easy to set, Metatrace can speed learning, and Metatrace can allow an RL algorithm to deal with non-stationarity in the learning task.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"145991379\",\"name\":\"K. Young\",\"url\":\"https://www.semanticscholar.org/author/145991379\"},{\"authorId\":\"47780630\",\"name\":\"Baoxiang Wang\",\"url\":\"https://www.semanticscholar.org/author/47780630\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\",\"url\":\"https://www.semanticscholar.org/author/39286677\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145991379\",\"name\":\"K. Young\"},{\"authorId\":\"47440731\",\"name\":\"T. Tian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"05d94cfe5768ebdcc1ecd8cb81de694e0d3e2f5d\",\"title\":\"MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments\",\"url\":\"https://www.semanticscholar.org/paper/05d94cfe5768ebdcc1ecd8cb81de694e0d3e2f5d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"1903.03176\",\"authors\":[{\"authorId\":\"145991379\",\"name\":\"K. Young\"},{\"authorId\":\"36603881\",\"name\":\"T. Tian\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c77b96419feb2025cfd47b6c76b9e5cb598c5846\",\"title\":\"MinAtar: An Atari-inspired Testbed for More Efficient Reinforcement Learning Experiments\",\"url\":\"https://www.semanticscholar.org/paper/c77b96419feb2025cfd47b6c76b9e5cb598c5846\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1903.03252\",\"authors\":[{\"authorId\":\"39414179\",\"name\":\"Alexandra Kearney\"},{\"authorId\":\"2300921\",\"name\":\"Vivek Veeriah\"},{\"authorId\":\"28327411\",\"name\":\"Jaden B. Travnik\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9719da12c7ef618b5a005bd3c704cc2d6891ab97\",\"title\":\"Learning Feature Relevance Through Step Size Adaptation in Temporal-Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/9719da12c7ef618b5a005bd3c704cc2d6891ab97\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.06324\",\"authors\":[{\"authorId\":\"2792016\",\"name\":\"D. A. Calian\"},{\"authorId\":\"145188928\",\"name\":\"D. J. Mankowitz\"},{\"authorId\":\"121459457\",\"name\":\"Tom Zahavy\"},{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"153898744\",\"name\":\"N. Levine\"},{\"authorId\":\"2554720\",\"name\":\"Timothy A. Mann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"eecc04e4751ef623ecd9f9e69e9601c9431152d2\",\"title\":\"Balancing Constraints and Rewards with Meta-Gradient D4PG\",\"url\":\"https://www.semanticscholar.org/paper/eecc04e4751ef623ecd9f9e69e9601c9431152d2\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1905.13551\",\"authors\":[{\"authorId\":\"47780630\",\"name\":\"Baoxiang Wang\"}],\"doi\":\"10.24963/ijcai.2019/507\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4b405743430fd72849bd0b9856c1b7f9722d51d2\",\"title\":\"Recurrent Existence Determination Through Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/4b405743430fd72849bd0b9856c1b7f9722d51d2\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2004.05439\",\"authors\":[{\"authorId\":\"1697755\",\"name\":\"Timothy M. Hospedales\"},{\"authorId\":\"19597437\",\"name\":\"A. Antoniou\"},{\"authorId\":\"51028255\",\"name\":\"P. Micaelli\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0abe0249028016ff8f958c23141c1ff18a1c0064\",\"title\":\"Meta-Learning in Neural Networks: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/0abe0249028016ff8f958c23141c1ff18a1c0064\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":181707506,\"doi\":\"10.24963/ijcai.2019/581\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"d8594f921efce4ee9e4cf196570b8abd7289c105\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1109/ACC.2012.6315022\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa\",\"title\":\"Model-Free reinforcement learning with continuous action in practice\",\"url\":\"https://www.semanticscholar.org/paper/a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa\",\"venue\":\"2012 American Control Conference (ACC)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fdee7d59ab0e02638a5749740c2a88fd460e22c1\",\"title\":\"ADAPTIVE STEP-SIZES FOR REINFORCEMENT LEARNING\",\"url\":\"https://www.semanticscholar.org/paper/fdee7d59ab0e02638a5749740c2a88fd460e22c1\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1506.02438\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"title\":\"High-Dimensional Continuous Control Using Generalized Advantage Estimation\",\"url\":\"https://www.semanticscholar.org/paper/d316c82c12cf4c45f9e85211ef3d1fa62497bff8\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1305.6646\",\"authors\":[{\"authorId\":\"1700433\",\"name\":\"S. Ross\"},{\"authorId\":\"3040175\",\"name\":\"P. Mineiro\"},{\"authorId\":\"144162125\",\"name\":\"J. Langford\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1d127af1174a3f0f36e9181348eaa731d3cca67b\",\"title\":\"Normalized Online Learning\",\"url\":\"https://www.semanticscholar.org/paper/1d127af1174a3f0f36e9181348eaa731d3cca67b\",\"venue\":\"UAI\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1759633\",\"name\":\"A. R. Mahmood\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"}],\"doi\":\"10.1109/ICASSP.2012.6288330\",\"intent\":[\"methodology\",\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"3d0e07b38df2459cb229c71a62495813c720bdae\",\"title\":\"Tuning-free step-size adaptation\",\"url\":\"https://www.semanticscholar.org/paper/3d0e07b38df2459cb229c71a62495813c720bdae\",\"venue\":\"2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"3308897\",\"name\":\"Matthew J. Hausknecht\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":\"10.1613/jair.5699\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"214799f90dcf59c22fccec2e68c20cf9bf6ab37a\",\"title\":\"Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents\",\"url\":\"https://www.semanticscholar.org/paper/214799f90dcf59c22fccec2e68c20cf9bf6ab37a\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2018},{\"arxivId\":\"1702.03118\",\"authors\":[{\"authorId\":\"2169762\",\"name\":\"Stefan Elfwing\"},{\"authorId\":\"1773761\",\"name\":\"E. Uchibe\"},{\"authorId\":\"1714997\",\"name\":\"K. Doya\"}],\"doi\":\"10.1016/j.neunet.2017.12.012\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b587ee7c802a5bd222a69090f59285e0dfdb29f1\",\"title\":\"Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b587ee7c802a5bd222a69090f59285e0dfdb29f1\",\"venue\":\"Neural Networks\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1804.03334\",\"authors\":[{\"authorId\":\"39414179\",\"name\":\"Alexandra Kearney\"},{\"authorId\":\"2300921\",\"name\":\"Vivek Veeriah\"},{\"authorId\":\"28327411\",\"name\":\"Jaden B. Travnik\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b7dd1960262b03d1ed2efe7975e339cc0928f603\",\"title\":\"TIDBD: Adapting Temporal-difference Step-sizes Through Stochastic Meta-descent\",\"url\":\"https://www.semanticscholar.org/paper/b7dd1960262b03d1ed2efe7975e339cc0928f603\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016}],\"title\":\"Metatrace Actor-Critic: Online Step-Size Tuning by Meta-gradient Descent for Reinforcement Learning Control\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Gradient descent\",\"topicId\":\"24880\",\"url\":\"https://www.semanticscholar.org/topic/24880\"},{\"topic\":\"Approximation\",\"topicId\":\"3247\",\"url\":\"https://www.semanticscholar.org/topic/3247\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Mountain Car\",\"topicId\":\"1165615\",\"url\":\"https://www.semanticscholar.org/topic/1165615\"},{\"topic\":\"Nonlinear system\",\"topicId\":\"5329\",\"url\":\"https://www.semanticscholar.org/topic/5329\"},{\"topic\":\"Stationary process\",\"topicId\":\"21296\",\"url\":\"https://www.semanticscholar.org/topic/21296\"},{\"topic\":\"Linear function\",\"topicId\":\"45804\",\"url\":\"https://www.semanticscholar.org/topic/45804\"},{\"topic\":\"Instability\",\"topicId\":\"4779\",\"url\":\"https://www.semanticscholar.org/topic/4779\"},{\"topic\":\"Performance tuning\",\"topicId\":\"234192\",\"url\":\"https://www.semanticscholar.org/topic/234192\"},{\"topic\":\"Tracing (software)\",\"topicId\":\"2918\",\"url\":\"https://www.semanticscholar.org/topic/2918\"}],\"url\":\"https://www.semanticscholar.org/paper/d8594f921efce4ee9e4cf196570b8abd7289c105\",\"venue\":\"IJCAI\",\"year\":2019}\n"