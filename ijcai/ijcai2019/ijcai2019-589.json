"{\"abstract\":\"Experience replay enables reinforcement learning agents to memorize and reuse past experiences, just as humans replay memories for the situation at hand. Contemporary off-policy algorithms either replay past experiences uniformly or utilize a rule-based replay strategy, which may be sub-optimal. In this work, we consider learning a replay policy to optimize the cumulative reward. Replay learning is challenging because the replay memory is noisy and large, and the cumulative reward is unstable. To address these issues, we propose a novel experience replay optimization (ERO) framework which alternately updates two policies: the agent policy, and the replay policy. The agent is updated to maximize the cumulative reward based on the replayed data, while the replay policy is updated to provide the agent with the most useful experiences. The conducted experiments on various continuous control tasks demonstrate the effectiveness of ERO, empirically showing promise in experience replay learning to improve the performance of off-policy reinforcement learning algorithms.\",\"arxivId\":\"1906.08387\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\",\"url\":\"https://www.semanticscholar.org/author/1759658\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\",\"url\":\"https://www.semanticscholar.org/author/51238382\"},{\"authorId\":\"3364022\",\"name\":\"K. Zhou\",\"url\":\"https://www.semanticscholar.org/author/3364022\"},{\"authorId\":\"121774683\",\"name\":\"X. Hu\",\"url\":\"https://www.semanticscholar.org/author/121774683\"}],\"citationVelocity\":10,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"29351303\",\"name\":\"Songyi Huang\"},{\"authorId\":\"153842970\",\"name\":\"Y. Cao\"},{\"authorId\":\"40149802\",\"name\":\"K. Reddy\"},{\"authorId\":\"39827246\",\"name\":\"J. Vargas\"},{\"authorId\":\"144426013\",\"name\":\"Alex Nguyen\"},{\"authorId\":\"1381629357\",\"name\":\"Ruzhe Wei\"},{\"authorId\":\"50115480\",\"name\":\"Junyu Guo\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\"}],\"doi\":\"10.24963/ijcai.2020/764\",\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a13fdbb96a1c3429b1b79b15000e561e27ce0719\",\"title\":\"RLCard: A Platform for Reinforcement Learning in Card Games\",\"url\":\"https://www.semanticscholar.org/paper/a13fdbb96a1c3429b1b79b15000e561e27ce0719\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2011.05525\",\"authors\":[{\"authorId\":\"1519065124\",\"name\":\"Junwei Zhang\"},{\"authorId\":\"1739296776\",\"name\":\"Zhenghao Zhang\"},{\"authorId\":\"152299607\",\"name\":\"Shuai Han\"},{\"authorId\":\"66446739\",\"name\":\"Shuai L\\u00fc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f6dc3e1582f7d04138ff62bc5f57f4bff65c09f\",\"title\":\"Proximal Policy Optimization via Enhanced Exploration Efficiency\",\"url\":\"https://www.semanticscholar.org/paper/4f6dc3e1582f7d04138ff62bc5f57f4bff65c09f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.07415\",\"authors\":[{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"101486331\",\"name\":\"Mingyang Wan\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"258d7bd6a30dc8ce187fc9486ae7d858c0149d19\",\"title\":\"Meta-AAD: Active Anomaly Detection with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/258d7bd6a30dc8ce187fc9486ae7d858c0149d19\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.07358\",\"authors\":[{\"authorId\":\"1564629398\",\"name\":\"Youngmin Oh\"},{\"authorId\":\"3436470\",\"name\":\"Kimin Lee\"},{\"authorId\":\"143720148\",\"name\":\"Jinwoo Shin\"},{\"authorId\":\"1720494\",\"name\":\"E. Yang\"},{\"authorId\":\"35788904\",\"name\":\"Sung Ju Hwang\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"49f54e261633cd53034d85f777b393f41001c289\",\"title\":\"Learning to Sample with Local and Global Contexts in Experience Replay Buffer\",\"url\":\"https://www.semanticscholar.org/paper/49f54e261633cd53034d85f777b393f41001c289\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.04061\",\"authors\":[{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"48513905\",\"name\":\"Yuening Li\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\"}],\"doi\":\"10.24963/ijcai.2020/431\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"f87cd3cb63ee30a9598af86349389708e859589e\",\"title\":\"Dual Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/f87cd3cb63ee30a9598af86349389708e859589e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1912.06910\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2311858\",\"name\":\"Diana Borsa\"},{\"authorId\":\"1399191196\",\"name\":\"David Ding\"},{\"authorId\":\"7635903\",\"name\":\"David Szepesvari\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"63508a5af094298a05de70eabeeb116ded649cb6\",\"title\":\"Adapting Behaviour for Learning Progress\",\"url\":\"https://www.semanticscholar.org/paper/63508a5af094298a05de70eabeeb116ded649cb6\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1519288324\",\"name\":\"Jianshu Wang\"},{\"authorId\":\"50141014\",\"name\":\"Xinzhi Wang\"},{\"authorId\":\"2167614\",\"name\":\"X. Luo\"},{\"authorId\":\"30512167\",\"name\":\"Z. Zhang\"},{\"authorId\":\"41173173\",\"name\":\"Wei Wang\"},{\"authorId\":\"1962986770\",\"name\":\"Yang Li\"}],\"doi\":\"10.1109/ICTAI50040.2020.00166\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"058b90fa38c6dab27e1d50303d4f25f6562c6f92\",\"title\":\"SEM: Adaptive Staged Experience Access Mechanism for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/058b90fa38c6dab27e1d50303d4f25f6562c6f92\",\"venue\":\"2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI)\",\"year\":2020},{\"arxivId\":\"2011.13093\",\"authors\":[{\"authorId\":\"98258868\",\"name\":\"Sang-Hwa Lee\"},{\"authorId\":\"102523405\",\"name\":\"J. Lee\"},{\"authorId\":\"1688962\",\"name\":\"I. Hasuo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66f5ff2e794bb6e29c3047941a979bea6fb571c5\",\"title\":\"Predictive PER: Balancing Priority and Diversity towards Stable Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/66f5ff2e794bb6e29c3047941a979bea6fb571c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.15097\",\"authors\":[{\"authorId\":\"51238382\",\"name\":\"Kwei-Herng Lai\"},{\"authorId\":\"1759658\",\"name\":\"Daochen Zha\"},{\"authorId\":\"3364022\",\"name\":\"K. Zhou\"},{\"authorId\":\"48539382\",\"name\":\"Xia Hu\"}],\"doi\":\"10.1145/3394486.3403088\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"058fa6b41bab3dccfc6ba07a83a99a65fc41d1bc\",\"title\":\"Policy-GNN: Aggregation Optimization for Graph Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/058fa6b41bab3dccfc6ba07a83a99a65fc41d1bc\",\"venue\":\"KDD\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8606501\",\"name\":\"A. Sharma\"},{\"authorId\":\"153852211\",\"name\":\"M. Pal\"},{\"authorId\":\"143660674\",\"name\":\"S. Anand\"},{\"authorId\":\"25374912\",\"name\":\"Sanjit K. Kaul\"}],\"doi\":\"10.1109/BigMM50055.2020.00029\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"8d0e8b26a69af477622db792e7d4b99fd55aa142\",\"title\":\"Stratified Sampling Based Experience Replay for Efficient Camera Selection Decisions\",\"url\":\"https://www.semanticscholar.org/paper/8d0e8b26a69af477622db792e7d4b99fd55aa142\",\"venue\":\"2020 IEEE Sixth International Conference on Multimedia Big Data (BigMM)\",\"year\":2020},{\"arxivId\":\"2003.00779\",\"authors\":[{\"authorId\":\"1515590651\",\"name\":\"Alexandros Tanzanakis\"},{\"authorId\":\"1681053\",\"name\":\"J. Lygeros\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b37189467912ae614950ab39d860403f13a5e5b0\",\"title\":\"Data-Driven Control of Unknown Systems: A Linear Programming Approach\",\"url\":\"https://www.semanticscholar.org/paper/b37189467912ae614950ab39d860403f13a5e5b0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.04664\",\"authors\":[{\"authorId\":\"1379927403\",\"name\":\"R\\u00e9my Portelas\"},{\"authorId\":\"102281182\",\"name\":\"C\\u00e9dric Colas\"},{\"authorId\":\"2617057\",\"name\":\"Lilian Weng\"},{\"authorId\":\"1380228856\",\"name\":\"Katja Hofmann\"},{\"authorId\":\"1720664\",\"name\":\"Pierre-Yves Oudeyer\"}],\"doi\":\"10.24963/ijcai.2020/663\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60a5be13ba59e21e4b8f335b3bcc341d93d8deea\",\"title\":\"Automatic Curriculum Learning For Deep RL: A Short Survey\",\"url\":\"https://www.semanticscholar.org/paper/60a5be13ba59e21e4b8f335b3bcc341d93d8deea\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"88310292\",\"name\":\"L. Vecchietti\"},{\"authorId\":\"143728503\",\"name\":\"T. Kim\"},{\"authorId\":\"92418902\",\"name\":\"K. Choi\"},{\"authorId\":\"71089286\",\"name\":\"Junhee Hong\"},{\"authorId\":\"119892840\",\"name\":\"Dongsoo Har\"}],\"doi\":\"10.1109/ACCESS.2020.3012204\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"24138fe78c47e12dd6c7734600210040a49be418\",\"title\":\"Batch Prioritization in Multigoal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/24138fe78c47e12dd6c7734600210040a49be418\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2009.13736\",\"authors\":[{\"authorId\":\"24742854\",\"name\":\"Yunshu Du\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144130066\",\"name\":\"A. Gebremedhin\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d88f3805086cd65a0834125cc331b1c31e700ce8\",\"title\":\"Lucid Dreaming for Experience Replay: Refreshing Past States with the Current Policy\",\"url\":\"https://www.semanticscholar.org/paper/d88f3805086cd65a0834125cc331b1c31e700ce8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.06946\",\"authors\":[{\"authorId\":\"2072202\",\"name\":\"Saad Mohamad\"},{\"authorId\":\"1443789324\",\"name\":\"Giovanni Montana\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"db20e1ee747d239d48849a98ffeec99ac4f8fe67\",\"title\":\"Adaptive Experience Selection for Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/db20e1ee747d239d48849a98ffeec99ac4f8fe67\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2011.09607\",\"authors\":[{\"authorId\":\"1962592199\",\"name\":\"Xiaoyang Liu\"},{\"authorId\":\"1504453822\",\"name\":\"Hongyang Yang\"},{\"authorId\":\"48772353\",\"name\":\"Q. Chen\"},{\"authorId\":\"1387888389\",\"name\":\"R. Zhang\"},{\"authorId\":\"49575959\",\"name\":\"Liuqing Yang\"},{\"authorId\":\"152659122\",\"name\":\"B. Xiao\"},{\"authorId\":\"48586423\",\"name\":\"C. D. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cd67fe7daed643cabbb2c6360031c7e5bd7b633f\",\"title\":\"FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance\",\"url\":\"https://www.semanticscholar.org/paper/cd67fe7daed643cabbb2c6360031c7e5bd7b633f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.08328\",\"authors\":[{\"authorId\":\"51149606\",\"name\":\"Jeffrey M. Ede\"}],\"doi\":\"10.1088/2632-2153/abd614\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b00e89b4ede90904704dff6b64eed34384dfc083\",\"title\":\"Review: Deep Learning in Electron Microscopy\",\"url\":\"https://www.semanticscholar.org/paper/b00e89b4ede90904704dff6b64eed34384dfc083\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2007.06049\",\"authors\":[{\"authorId\":\"14637819\",\"name\":\"Scott Fujimoto\"},{\"authorId\":\"51174612\",\"name\":\"David Meger\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a425c6d74273cb9d2ad958273429cd0e8b0f13ba\",\"title\":\"An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/a425c6d74273cb9d2ad958273429cd0e8b0f13ba\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2007.06700\",\"authors\":[{\"authorId\":\"26958176\",\"name\":\"W. Fedus\"},{\"authorId\":\"3377142\",\"name\":\"Prajit Ramachandran\"},{\"authorId\":\"147866847\",\"name\":\"Rishabh Agarwal\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"},{\"authorId\":\"1777528\",\"name\":\"H. Larochelle\"},{\"authorId\":\"144845452\",\"name\":\"M. Rowland\"},{\"authorId\":\"1484070054\",\"name\":\"Will Dabney\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"517498898024107a75f5a4ea84f4129e300af603\",\"title\":\"Revisiting Fundamentals of Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/517498898024107a75f5a4ea84f4129e300af603\",\"venue\":\"ICML\",\"year\":2020}],\"corpusId\":195218647,\"doi\":\"10.24963/ijcai.2019/589\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":3,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"references\":[{\"arxivId\":\"1803.05044\",\"authors\":[{\"authorId\":\"35533477\",\"name\":\"T. Xu\"},{\"authorId\":\"47362455\",\"name\":\"Q. Liu\"},{\"authorId\":\"145927744\",\"name\":\"Liang Zhao\"},{\"authorId\":\"144439558\",\"name\":\"J. Peng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7f567f1e8972ff31a7ced59c329e7d75da645baf\",\"title\":\"Learning to Explore with Meta-Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/7f567f1e8972ff31a7ced59c329e7d75da645baf\",\"venue\":\"ICML 2018\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37202259\",\"name\":\"J. Meigs\"}],\"doi\":\"10.1515/cclm.1994.32.8.631\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5c3391bde2bb1b3d737913ee8caa01492a782732\",\"title\":\"WHO Technical Report\",\"url\":\"https://www.semanticscholar.org/paper/5c3391bde2bb1b3d737913ee8caa01492a782732\",\"venue\":\"The Yale Journal of Biology and Medicine\",\"year\":1954},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2151279\",\"name\":\"M. Mattar\"},{\"authorId\":\"1784997\",\"name\":\"N. Daw\"}],\"doi\":\"10.1038/s41593-018-0232-z\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"453cdd8430f33d6b48ea56b145c937612c09179f\",\"title\":\"Prioritized memory access explains planning and hippocampal replay\",\"url\":\"https://www.semanticscholar.org/paper/453cdd8430f33d6b48ea56b145c937612c09179f\",\"venue\":\"Nature Neuroscience\",\"year\":2018},{\"arxivId\":\"1710.02298\",\"authors\":[{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"title\":\"Rainbow: Combining Improvements in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0ab3f7ecbdc5a33565a234215604a6ca9d155a33\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32376567\",\"name\":\"L. J. Lin\"}],\"doi\":\"10.1007/BF00992699\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"title\":\"Self-improving reactive agents based on reinforcement learning, planning and teaching\",\"url\":\"https://www.semanticscholar.org/paper/9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1710.06574\",\"authors\":[{\"authorId\":\"2059283\",\"name\":\"Ruishan Liu\"},{\"authorId\":\"145085305\",\"name\":\"J. Zou\"}],\"doi\":\"10.1109/ALLERTON.2018.8636075\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15b1661cbc70236140cfe221fe09c51eaf9fadd0\",\"title\":\"The Effects of Memory Replay in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/15b1661cbc70236140cfe221fe09c51eaf9fadd0\",\"venue\":\"2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)\",\"year\":2018},{\"arxivId\":\"1611.01224\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6a43d91c8d883e3463b358571125fa0ec7298b3a\",\"title\":\"Sample Efficient Actor-Critic with Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/6a43d91c8d883e3463b358571125fa0ec7298b3a\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1806.04624\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"145690938\",\"name\":\"Andrew Patterson\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":\"10.24963/ijcai.2018/666\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"title\":\"Organizing Experience: a Deeper Look at Replay Mechanisms for Sample-Based Planning in Continuous State Domains\",\"url\":\"https://www.semanticscholar.org/paper/ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Daphna Shohamy\"},{\"authorId\":null,\"name\":\"Nathaniel D Daw. Integrating memories to guide decisions\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Current Opinion in Behavioral Sciences\",\"url\":\"\",\"venue\":\"5:85\\u201390,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":\"1807.05827\",\"authors\":[{\"authorId\":\"2082029\",\"name\":\"G. Novati\"},{\"authorId\":\"1802604\",\"name\":\"P. Koumoutsakos\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"0eefcf9b702b50b7c47300736c5ee0080a1639db\",\"title\":\"Remember and Forget for Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/0eefcf9b702b50b7c47300736c5ee0080a1639db\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":\"1712.01275\",\"authors\":[{\"authorId\":\"2503523\",\"name\":\"S. Zhang\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a447809933556602c06c1c423f5d622a8c57169a\",\"title\":\"A Deeper Look at Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/a447809933556602c06c1c423f5d622a8c57169a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1697573\",\"name\":\"D. Shohamy\"},{\"authorId\":\"1784997\",\"name\":\"N. Daw\"}],\"doi\":\"10.1016/J.COBEHA.2015.08.010\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bf0229ba596b441c6225beff5074a01b4a15fc4\",\"title\":\"Integrating memories to guide decisions\",\"url\":\"https://www.semanticscholar.org/paper/3bf0229ba596b441c6225beff5074a01b4a15fc4\",\"venue\":\"Current Opinion in Behavioral Sciences\",\"year\":2015},{\"arxivId\":\"1805.09801\",\"authors\":[{\"authorId\":\"2351434\",\"name\":\"Zhongwen Xu\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a49a71c9d40051a03c4445fe49025bc75d9eeb6\",\"title\":\"Meta-Gradient Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2a49a71c9d40051a03c4445fe49025bc75d9eeb6\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1804.06459\",\"authors\":[{\"authorId\":\"34418171\",\"name\":\"Zeyu Zheng\"},{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"145537841\",\"name\":\"S. Singh\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"35271d36cb20bf8d716e79c9dd15d738d955a931\",\"title\":\"On Learning Intrinsic Rewards for Policy Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/35271d36cb20bf8d716e79c9dd15d738d955a931\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1810.12081\",\"authors\":[{\"authorId\":\"47767550\",\"name\":\"Lijun Wu\"},{\"authorId\":\"144054173\",\"name\":\"Fei Tian\"},{\"authorId\":\"2794096\",\"name\":\"Yingce Xia\"},{\"authorId\":\"144566102\",\"name\":\"Yang Fan\"},{\"authorId\":\"143826491\",\"name\":\"T. Qin\"},{\"authorId\":\"66117656\",\"name\":\"J. Lai\"},{\"authorId\":\"152998017\",\"name\":\"T. Liu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab561713a71da567d315c09da693060e32ee3470\",\"title\":\"Learning to Teach with Dynamic Loss Functions\",\"url\":\"https://www.semanticscholar.org/paper/ab561713a71da567d315c09da693060e32ee3470\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Long-Ji Lin. Self-improving reactive agents based on reinf learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"planning and teaching\",\"url\":\"\",\"venue\":\"Machine learning, 8(3-4):293\\u2013321,\",\"year\":1992},{\"arxivId\":\"1707.01495\",\"authors\":[{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"},{\"authorId\":\"150074096\",\"name\":\"Dwight Crow\"},{\"authorId\":\"6672240\",\"name\":\"Alex Ray\"},{\"authorId\":\"145540305\",\"name\":\"J. Schneider\"},{\"authorId\":\"39492797\",\"name\":\"Rachel H Fong\"},{\"authorId\":\"2930640\",\"name\":\"P. Welinder\"},{\"authorId\":\"39593364\",\"name\":\"Bob McGrew\"},{\"authorId\":\"48104547\",\"name\":\"Josh Tobin\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"429ed4c9845d0abd1f8204e1d7705919559bc2a2\",\"title\":\"Hindsight Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/429ed4c9845d0abd1f8204e1d7705919559bc2a2\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":\"10.1109/IROS.2012.6386109\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"title\":\"MuJoCo: A physics engine for model-based control\",\"url\":\"https://www.semanticscholar.org/paper/b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"venue\":\"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50086584\",\"name\":\"Haiyan Yin\"},{\"authorId\":\"1746914\",\"name\":\"Sinno Jialin Pan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"title\":\"Knowledge Transfer for Deep Reinforcement Learning with Hierarchical Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/1ad2bbae449362f0cfe02caedb5c2e117e4b1470\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478313\",\"name\":\"L. Lin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"54c4cf3a8168c1b70f91cf78a3dc98b671935492\",\"title\":\"Reinforcement learning for robots using neural networks\",\"url\":\"https://www.semanticscholar.org/paper/54c4cf3a8168c1b70f91cf78a3dc98b671935492\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marcelo Gomes Mattar\"},{\"authorId\":null,\"name\":\"Nathaniel D Daw. Prioritized memory access explains planning\"},{\"authorId\":null,\"name\":\"hippocampal replay\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"bioRxiv\",\"url\":\"\",\"venue\":\"page 225664,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39524286\",\"name\":\"J. H. Stenton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"717282ceb08d38e6fb374270ce85d3bf2416dade\",\"title\":\"Learning how to teach.\",\"url\":\"https://www.semanticscholar.org/paper/717282ceb08d38e6fb374270ce85d3bf2416dade\",\"venue\":\"Nursing mirror and midwives journal\",\"year\":1973},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"title\":\"Deterministic Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":\"1802.10269\",\"authors\":[{\"authorId\":\"3431333\",\"name\":\"David Isele\"},{\"authorId\":\"3216610\",\"name\":\"Akansel Cosgun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c1650cb7c313ca9134edff68952c3defd793d04\",\"title\":\"Selective Experience Replay for Lifelong Learning\",\"url\":\"https://www.semanticscholar.org/paper/8c1650cb7c313ca9134edff68952c3defd793d04\",\"venue\":\"AAAI\",\"year\":2018}],\"title\":\"Experience Replay Optimization\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Edge recombination operator\",\"topicId\":\"2172542\",\"url\":\"https://www.semanticscholar.org/topic/2172542\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Logic programming\",\"topicId\":\"6032\",\"url\":\"https://www.semanticscholar.org/topic/6032\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Experience\",\"topicId\":\"4221\",\"url\":\"https://www.semanticscholar.org/topic/4221\"},{\"topic\":\"Control theory\",\"topicId\":\"3317\",\"url\":\"https://www.semanticscholar.org/topic/3317\"},{\"topic\":\"Humans\",\"topicId\":\"732\",\"url\":\"https://www.semanticscholar.org/topic/732\"},{\"topic\":\"Replay attack\",\"topicId\":\"308862\",\"url\":\"https://www.semanticscholar.org/topic/308862\"},{\"topic\":\"IBM Notes\",\"topicId\":\"82564\",\"url\":\"https://www.semanticscholar.org/topic/82564\"}],\"url\":\"https://www.semanticscholar.org/paper/2ce156d315ca3d7b46d46ea5df27d0ccc02721a4\",\"venue\":\"IJCAI\",\"year\":2019}\n"