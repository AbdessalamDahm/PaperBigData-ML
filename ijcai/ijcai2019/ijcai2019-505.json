"{\"abstract\":\"In multitask reinforcement learning, tasks often have sub-tasks that share the same solution, even though the overall tasks are different. If the sharedportions could be effectively identified, then the learning process could be improved since all the samples between tasks in the shared space could be used. In this paper, we propose a Sharing Experience Framework (SEF) for simultaneously training of multiple tasks. In SEF, a confidence sharing agent uses task-specific rewards from the environment to identify similar parts that should be shared across tasks and defines those parts as shared-regions between tasks. The shared-regions are expected to guide task-policies sharing their experience during the learning process. The experiments highlight that our framework improves the performance and the stability of learning taskpolicies, and is possible to help task-policies avoid local optimums.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"67329496\",\"name\":\"T. Vuong\",\"url\":\"https://www.semanticscholar.org/author/67329496\"},{\"authorId\":\"30436475\",\"name\":\"D. Nguyen\",\"url\":\"https://www.semanticscholar.org/author/30436475\"},{\"authorId\":\"151492445\",\"name\":\"Tai-Long Nguyen\",\"url\":\"https://www.semanticscholar.org/author/151492445\"},{\"authorId\":\"151495838\",\"name\":\"Cong-Minh Bui\",\"url\":\"https://www.semanticscholar.org/author/151495838\"},{\"authorId\":\"67032833\",\"name\":\"Hai-Dang Kieu\",\"url\":\"https://www.semanticscholar.org/author/67032833\"},{\"authorId\":\"3301579\",\"name\":\"Viet-Cuong Ta\",\"url\":\"https://www.semanticscholar.org/author/3301579\"},{\"authorId\":\"3285873\",\"name\":\"Quoc-Long Tran\",\"url\":\"https://www.semanticscholar.org/author/3285873\"},{\"authorId\":\"3188009\",\"name\":\"T. L\\u00ea\",\"url\":\"https://www.semanticscholar.org/author/3188009\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2007.05694\",\"authors\":[{\"authorId\":\"1810775245\",\"name\":\"Ugurkan Ates\"}],\"doi\":\"10.1109/ASYU50717.2020.9259811\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"df08b6e8cfd38ce430b852fddb9aec796c80ffa3\",\"title\":\"Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones\",\"url\":\"https://www.semanticscholar.org/paper/df08b6e8cfd38ce430b852fddb9aec796c80ffa3\",\"venue\":\"2020 Innovations in Intelligent Systems and Applications Conference (ASYU)\",\"year\":2020}],\"corpusId\":199465978,\"doi\":\"10.24963/ijcai.2019/505\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"19d9134effebb79799b1ed6189109b5c7bc56e24\",\"references\":[{\"arxivId\":\"1606.04671\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"3422052\",\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"title\":\"Progressive Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38637273\",\"name\":\"Hsiu-Chin Lin\"},{\"authorId\":\"20930108\",\"name\":\"P. Ray\"},{\"authorId\":\"39600588\",\"name\":\"M. Howard\"}],\"doi\":\"10.1109/icra33291.2017\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0aba478ab135e3b72b38dc9dd3e4608b213ba53\",\"title\":\"The 2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"url\":\"https://www.semanticscholar.org/paper/e0aba478ab135e3b72b38dc9dd3e4608b213ba53\",\"venue\":\"\",\"year\":2017},{\"arxivId\":\"1612.05533\",\"authors\":[{\"authorId\":\"8214233\",\"name\":\"J. Zhang\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"145581493\",\"name\":\"J. Boedecker\"},{\"authorId\":\"1725973\",\"name\":\"W. Burgard\"}],\"doi\":\"10.1109/IROS.2017.8206049\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b\",\"title\":\"Deep reinforcement learning with successor features for navigation across similar environments\",\"url\":\"https://www.semanticscholar.org/paper/1a0d50fd4a3e52b25c0a662b687daeb8ea963b4b\",\"venue\":\"2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143794407\",\"name\":\"X. Hao\"},{\"authorId\":\"8273966\",\"name\":\"Guigang Zhang\"},{\"authorId\":\"144153753\",\"name\":\"Shang Ma\"}],\"doi\":\"10.1142/S1793351X16500045\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f8d648c52edf74e41b0996128aa536e13cc7e82\",\"title\":\"Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/4f8d648c52edf74e41b0996128aa536e13cc7e82\",\"venue\":\"Int. J. Semantic Comput.\",\"year\":2016},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1757489\",\"name\":\"N. Ferns\"},{\"authorId\":\"1784317\",\"name\":\"P. Panangaden\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":\"10.1137/10080484X\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"842345bc690a42432f6510efe894c801a1740bda\",\"title\":\"Bisimulation Metrics for Continuous Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/842345bc690a42432f6510efe894c801a1740bda\",\"venue\":\"SIAM J. Comput.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alex Graves\"},{\"authorId\":null,\"name\":\"Abdel-rahman Mohamed\"},{\"authorId\":null,\"name\":\"Geoffrey Hinton. Speech recognition with deep recurrent neu Acoustics\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"speech and signal processing (icassp)\",\"url\":\"\",\"venue\":\"2013 ieee international conference on, pages 6645\\u20136649. IEEE,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Diana Borsa\"},{\"authorId\":null,\"name\":\"Thore Graepel\"},{\"authorId\":null,\"name\":\"John Shawe-Taylor. Learning shared representations in m learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1603.02041,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sham Kakade\"},{\"authorId\":null,\"name\":\"John Langford. Approximately optimal approximate reinfo ICML\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 2\",\"url\":\"\",\"venue\":\"pages 267\\u2013274,\",\"year\":2002},{\"arxivId\":\"1707.04175\",\"authors\":[{\"authorId\":\"1725303\",\"name\":\"Y. Teh\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"title\":\"Distral: Robust multitask reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1511.06342\",\"authors\":[{\"authorId\":\"3166516\",\"name\":\"Emilio Parisotto\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"title\":\"Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47087136\",\"name\":\"Zhaoyang Yang\"},{\"authorId\":\"1758755\",\"name\":\"Kathryn E. Merrick\"},{\"authorId\":\"1713460\",\"name\":\"H. Abbass\"},{\"authorId\":\"144838978\",\"name\":\"Lianwen Jin\"}],\"doi\":\"10.24963/ijcai.2017/461\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"13ec391c21ded02ed3be31ff19f90d529a431e89\",\"title\":\"Multi-Task Deep Reinforcement Learning for Continuous Action Control\",\"url\":\"https://www.semanticscholar.org/paper/13ec391c21ded02ed3be31ff19f90d529a431e89\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2064160\",\"name\":\"A. Krizhevsky\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":\"10.1145/3065386\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"title\":\"ImageNet classification with deep convolutional neural networks\",\"url\":\"https://www.semanticscholar.org/paper/abd1c342495432171beb7ca8fd9551ef13cbd0ff\",\"venue\":\"Commun. ACM\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144056302\",\"name\":\"A. Wilson\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"},{\"authorId\":\"145527877\",\"name\":\"Soumya Ray\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":\"10.1145/1273496.1273624\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"title\":\"Multi-task reinforcement learning: a hierarchical Bayesian approach\",\"url\":\"https://www.semanticscholar.org/paper/ab19a482195f4299f96b98e4eb15cb3ad4753f3b\",\"venue\":\"ICML '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abhishek Gupta\"},{\"authorId\":null,\"name\":\"Coline Devin\"},{\"authorId\":null,\"name\":\"Yuxuan Liu\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Sergey Levine. Learning invariant feature spaces to trans learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1703.02949,\",\"year\":2017},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3451410\",\"name\":\"Matthew Jon Grounds\"},{\"authorId\":\"2380005\",\"name\":\"D. Kudenko\"}],\"doi\":\"10.1007/978-3-540-77949-0_5\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c35bdf77935c32acb3998365856a6b5955e4cf20\",\"title\":\"Parallel Reinforcement Learning with Linear Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/c35bdf77935c32acb3998365856a6b5955e4cf20\",\"venue\":\"Adaptive Agents and Multi-Agents Systems\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1944801\",\"name\":\"Karol Hausman\"},{\"authorId\":\"2060551\",\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"565af8f2ef461b1d7368f3e9899e0f576e4f0a24\",\"title\":\"Learning an Embedding Space for Transferable Robot Skills\",\"url\":\"https://www.semanticscholar.org/paper/565af8f2ef461b1d7368f3e9899e0f576e4f0a24\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1795401\",\"name\":\"Sarah Osentoski\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c29b7798b6839b84b87b4910f6263ee5a89f9279\",\"title\":\"Value Function Approximation in Reinforcement Learning Using the Fourier Basis\",\"url\":\"https://www.semanticscholar.org/paper/c29b7798b6839b84b87b4910f6263ee5a89f9279\",\"venue\":\"AAAI\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard S Sutton\"},{\"authorId\":null,\"name\":\"Andrew G Barto. Introduction to reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 135\",\"url\":\"\",\"venue\":\"MIT press Cambridge,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Mnih\"},{\"authorId\":null,\"name\":\"A. P. Badia\"},{\"authorId\":null,\"name\":\"M. Mirza\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Lillicrap T\",\"url\":\"\",\"venue\":\"Graves, A., T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\\u20131937\",\"year\":2016},{\"arxivId\":\"1511.06295\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2016840\",\"name\":\"Sergio Gomez Colmenarejo\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"143959037\",\"name\":\"J. Kirkpatrick\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"title\":\"Policy Distillation\",\"url\":\"https://www.semanticscholar.org/paper/1c4927af526d5c28f7c2cfa492ece192d80a61d4\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"V. Mnih\"},{\"authorId\":null,\"name\":\"A. P. Badia\"},{\"authorId\":null,\"name\":\"M. Mirza\"}],\"doi\":null,\"intent\":[],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Lillicrap T\",\"url\":\"\",\"venue\":\"Graves, A., T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928\\u20131937\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael I Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization. In Icml\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 37\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1446962550\",\"name\":\"Dock Bumpers\"},{\"authorId\":\"1446961266\",\"name\":\"Support Ledgers\"}],\"doi\":\"10.1023/A:1017189329742\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"title\":\"Volume 2\",\"url\":\"https://www.semanticscholar.org/paper/e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"venue\":\"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1022633531479\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"title\":\"Learning to Predict by the Methods of Temporal Differences\",\"url\":\"https://www.semanticscholar.org/paper/a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"venue\":\"Machine Learning\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jacob Andreas\"},{\"authorId\":null,\"name\":\"Dan Klein\"},{\"authorId\":null,\"name\":\"Sergey Levine. Modular multitask reinforcement learning w sketches\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1611.01796,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015}],\"title\":\"Sharing Experience in Multitask Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Computer multitasking\",\"topicId\":\"6968\",\"url\":\"https://www.semanticscholar.org/topic/6968\"}],\"url\":\"https://www.semanticscholar.org/paper/19d9134effebb79799b1ed6189109b5c7bc56e24\",\"venue\":\"IJCAI\",\"year\":2019}\n"