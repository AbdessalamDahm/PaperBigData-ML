"{\"abstract\":\"Reinforcement learning has enjoyed multiple impressive successes in recent years. However, these successes typically require very large amounts of data before an agent achieves acceptable performance. This paper focuses on a novel way of combating such requirements by leveraging existing (human or agent) knowledge. In particular, this paper leverages demonstrations, allowing an agent to quickly achieve high performance. This paper introduces the Dynamic Reuse of Prior (DRoP) algorithm, which combines the offline knowledge (demonstrations recorded before learning) with an online confidence-based performance analysis. DRoP leverages the demonstrator\\u2019s knowledge by automatically balancing between reusing the prior knowledge and the current learned policy, allowing the agent to outperform the original demonstrations. We compare with multiple state-of-theart learning algorithms and empirically show that DRoP can achieve superior performance in two domains. Additionally, we show that this confidence measure can be used to selectively request additional demonstrations, significantly improving the learning performance of the agent.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2870663\",\"name\":\"Zhaodong Wang\",\"url\":\"https://www.semanticscholar.org/author/2870663\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\",\"url\":\"https://www.semanticscholar.org/author/39286677\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145902797\",\"name\":\"B. Banerjee\"},{\"authorId\":\"148120358\",\"name\":\"Sneha Racharla\"}],\"doi\":\"10.1017/S0269888920000387\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7cc2371871eaa5e3a510e588361bf4a4602867ed\",\"title\":\"Human Agent Transfer from Observations\",\"url\":\"https://www.semanticscholar.org/paper/7cc2371871eaa5e3a510e588361bf4a4602867ed\",\"venue\":\"\",\"year\":2021},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145902797\",\"name\":\"B. Banerjee\"},{\"authorId\":\"134768818\",\"name\":\"Syamala Nanditha Vittanala\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1017/S0269888919000043\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"03e78b926213b4f618753e7860c9a71a0090d86c\",\"title\":\"Team learning from human demonstration with coordination confidence\",\"url\":\"https://www.semanticscholar.org/paper/03e78b926213b4f618753e7860c9a71a0090d86c\",\"venue\":\"Knowl. Eng. Rev.\",\"year\":2019},{\"arxivId\":\"2011.01298\",\"authors\":[{\"authorId\":\"151481077\",\"name\":\"Yuchen Wu\"},{\"authorId\":\"31816691\",\"name\":\"Melissa Mozifian\"},{\"authorId\":\"2162768\",\"name\":\"Florian Shkurti\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a66df6a55c25e8f131502c922000b9197debe14a\",\"title\":\"Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models\",\"url\":\"https://www.semanticscholar.org/paper/a66df6a55c25e8f131502c922000b9197debe14a\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199465940,\"doi\":\"10.24963/ijcai.2019/530\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"d12ac04b7ba4b03841daa97a5d4d339002a32359\",\"references\":[{\"arxivId\":\"1511.06342\",\"authors\":[{\"authorId\":\"3166516\",\"name\":\"Emilio Parisotto\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"},{\"authorId\":\"145124475\",\"name\":\"R. Salakhutdinov\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"title\":\"Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1def5d3711ebd1d86787b1ed57c91832c5ddc90b\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2122942\",\"name\":\"B. Ripley\"},{\"authorId\":\"67307668\",\"name\":\"C. C. Taylor\"}],\"doi\":\"10.1038/219441b0\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ca23e7a71ace53d6a5b2a553ff37c63365d22b8a\",\"title\":\"Pattern Recognition\",\"url\":\"https://www.semanticscholar.org/paper/ca23e7a71ace53d6a5b2a553ff37c63365d22b8a\",\"venue\":\"Nature\",\"year\":1968},{\"arxivId\":null,\"authors\":[{\"authorId\":\"66180619\",\"name\":\"OctoMiao\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"223af46e6193561b406dd4956b6df3087c502349\",\"title\":\"Overcoming catastrophic forgetting in neural networks\",\"url\":\"https://www.semanticscholar.org/paper/223af46e6193561b406dd4956b6df3087c502349\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2811287\",\"name\":\"Halit Bener Suay\"},{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2896f821c991824fc0bc96949e4baedc37fee06a\",\"title\":\"Learning from Demonstration for Shaping through Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2896f821c991824fc0bc96949e4baedc37fee06a\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1606.04671\",\"authors\":[{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"3422052\",\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":\"2755582\",\"name\":\"G. Desjardins\"},{\"authorId\":\"2794457\",\"name\":\"Hubert Soyer\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"title\":\"Progressive Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/53c9443e4e667170acc60ca1b31a0ec7151fe753\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1577069.1755839\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"title\":\"Transfer Learning for Reinforcement Learning Domains: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2009},{\"arxivId\":\"1712.00123\",\"authors\":[{\"authorId\":\"3378742\",\"name\":\"Zelun Luo\"},{\"authorId\":\"8299168\",\"name\":\"Y. Zou\"},{\"authorId\":\"50196944\",\"name\":\"Judy Hoffman\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ca2f3a290bc24bc1c602389207ee86f9e01779e7\",\"title\":\"Label Efficient Learning of Transferable Representations acrosss Domains and Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ca2f3a290bc24bc1c602389207ee86f9e01779e7\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1145/1329125.1329407\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4340c8b92704ec9a7df746b04b5451d9d6355cf2\",\"title\":\"Confidence-based policy learning from demonstration using Gaussian mixture models\",\"url\":\"https://www.semanticscholar.org/paper/4340c8b92704ec9a7df746b04b5451d9d6355cf2\",\"venue\":\"AAMAS '07\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1836885\",\"name\":\"Brenna Argall\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"},{\"authorId\":\"1699032\",\"name\":\"B. Browning\"}],\"doi\":\"10.1016/j.robot.2008.10.024\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"title\":\"A survey of robot learning from demonstration\",\"url\":\"https://www.semanticscholar.org/paper/4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Li Mao\"},{\"authorId\":null,\"name\":\"Wei Yi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Kudenko Daniel\",\"url\":\"\",\"venue\":\"Adaptive and Learning Agents Workshop at AAMAS-18,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kirkpatrick et al\"},{\"authorId\":null,\"name\":\"2017 James Kirkpatrick\"},{\"authorId\":null,\"name\":\"Razvan Pascanu\"},{\"authorId\":null,\"name\":\"Neil Rabinowitz\"},{\"authorId\":null,\"name\":\"Joel Veness\"},{\"authorId\":null,\"name\":\"Guillaume Desjardins\"},{\"authorId\":null,\"name\":\"Andrei A Rusu\"},{\"authorId\":null,\"name\":\"Kieran Milan\"},{\"authorId\":null,\"name\":\"John Quan\"},{\"authorId\":null,\"name\":\"Tiago Ramalho\"},{\"authorId\":null,\"name\":\"Agnieszka Grabska-Barwinska\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Overcoming catastrophic forgetting\",\"url\":\"\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andreas Maurer\"},{\"authorId\":null,\"name\":\"Massimiliano Pontil\"},{\"authorId\":null,\"name\":\"Bernardino Romera-Paredes. The benefit of multitask represent learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):2853\\u20132884,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50881207\",\"name\":\"Barteld Kooi\"},{\"authorId\":\"2512398\",\"name\":\"H. V. Ditmarsch\"},{\"authorId\":\"1706100\",\"name\":\"W. Hoek\"}],\"doi\":\"10.5555/3306127\",\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"682646e296ad9e559edf75e7f222b0a825273d54\",\"title\":\"Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems\",\"url\":\"https://www.semanticscholar.org/paper/682646e296ad9e559edf75e7f222b0a825273d54\",\"venue\":\"AAMAS 2011\",\"year\":2011},{\"arxivId\":\"1707.04175\",\"authors\":[{\"authorId\":\"1725303\",\"name\":\"Y. Teh\"},{\"authorId\":\"2603033\",\"name\":\"V. Bapst\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"143959037\",\"name\":\"James Kirkpatrick\"},{\"authorId\":\"2315504\",\"name\":\"Raia Hadsell\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"title\":\"Distral: Robust multitask reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/cf90552b5d2e992e93ab838fd615e1c36618e31c\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2463017\",\"name\":\"S. Karakovskiy\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/TCIAIG.2012.2188528\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"title\":\"The Mario AI Benchmark and Competitions\",\"url\":\"https://www.semanticscholar.org/paper/6eaa8e1622f37f4a6cf3deeeee78abd3f86c711e\",\"venue\":\"IEEE Transactions on Computational Intelligence and AI in Games\",\"year\":2012},{\"arxivId\":\"1011.0686\",\"authors\":[{\"authorId\":\"1700433\",\"name\":\"S. Ross\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"title\":\"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\",\"url\":\"https://www.semanticscholar.org/paper/79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"venue\":\"AISTATS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Fernando Fern\\u00e1ndez\"},{\"authorId\":null,\"name\":\"Manuela Veloso. Probabilistic policy reuse in a reinforcem Agents\"},{\"authorId\":null,\"name\":\"Multiagent Systems\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 720\\u2013727\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Zhaodong Wang\"},{\"authorId\":null,\"name\":\"Matthew E. Taylor. Improving Reinforcement Learning with C Demonstrations\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the 26th International Conference on Artificial Intelligence (IJCAI)\",\"url\":\"\",\"venue\":\"August\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew E. Taylor\"},{\"authorId\":null,\"name\":\"Halit Bener Suay\"},{\"authorId\":null,\"name\":\"Sonia Chernova. Integrating reinforcement learning with ability\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the International Conference on Autonomous Agents and Multiagent Systems\",\"url\":\"\",\"venue\":\"May\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735414\",\"name\":\"T. Knasel\"}],\"doi\":\"10.1016/0921-8890(88)90002-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edd77f310393f521669b209cbb6828fb45a8485d\",\"title\":\"Robotics and autonomous systems\",\"url\":\"https://www.semanticscholar.org/paper/edd77f310393f521669b209cbb6828fb45a8485d\",\"venue\":\"Robotics Auton. Syst.\",\"year\":1988},{\"arxivId\":\"1505.06279\",\"authors\":[{\"authorId\":\"144009046\",\"name\":\"Andreas Maurer\"},{\"authorId\":\"1704699\",\"name\":\"M. Pontil\"},{\"authorId\":\"1403031665\",\"name\":\"B. Romera-Paredes\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a122dbde40c9cd976badd99fdc820156908b7ee8\",\"title\":\"The Benefit of Multitask Representation Learning\",\"url\":\"https://www.semanticscholar.org/paper/a122dbde40c9cd976badd99fdc820156908b7ee8\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"JS Albus\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Brains\",\"url\":\"\",\"venue\":\"behavior. & Robotics. Peterboro, NH: Byte Books\",\"year\":1981},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2837869\",\"name\":\"T. Brys\"},{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"2528631\",\"name\":\"Peter Vrancx\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1016/j.neucom.2017.02.096\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"58bcb746db9ae3bb0f605e391d685d7ad830ca9c\",\"title\":\"Multi-objectivization and ensembles of shapings in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/58bcb746db9ae3bb0f605e391d685d7ad830ca9c\",\"venue\":\"Neurocomputing\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"St\\u00e9phane Ross\"},{\"authorId\":null,\"name\":\"Geoffrey Gordon\"},{\"authorId\":null,\"name\":\"Drew Bagnell. A reduction of imitation learning\"},{\"authorId\":null,\"name\":\"structured prediction to no-regret online learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In Proceedings of the fourteenth International Conference on Artificial Intelligence and Statistics\",\"url\":\"\",\"venue\":\"pages 627\\u2013635,\",\"year\":2011},{\"arxivId\":\"1707.08475\",\"authors\":[{\"authorId\":\"39051054\",\"name\":\"I. Higgins\"},{\"authorId\":\"3422676\",\"name\":\"A. Pal\"},{\"authorId\":\"2228824\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"2367480\",\"name\":\"Lo\\u00efc Matthey\"},{\"authorId\":\"143641906\",\"name\":\"C. Burgess\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"46378362\",\"name\":\"M. Botvinick\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"2289726\",\"name\":\"Alexander Lerchner\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2141a5ec0c65ea0a9861ae562f4c9fb8020d197\",\"title\":\"DARLA: Improving Zero-Shot Transfer in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a2141a5ec0c65ea0a9861ae562f4c9fb8020d197\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145528658\",\"name\":\"G. Kendall\"}],\"doi\":\"10.1109/TCIAIG.2015.2409514\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9046f46d088eee7be4af8be5ffea602394a937c0\",\"title\":\"Editorial: IEEE Transactions on Computational Intelligence and AI in Games\",\"url\":\"https://www.semanticscholar.org/paper/9046f46d088eee7be4af8be5ffea602394a937c0\",\"venue\":\"IEEE Trans. Comput. Intell. AI Games\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3202088\",\"name\":\"J. Chung\"},{\"authorId\":null,\"name\":\"Damjan Mikli\\u0107\"},{\"authorId\":null,\"name\":\"Lorenzo Sabattini\"},{\"authorId\":null,\"name\":\"Kagan Tumer\"},{\"authorId\":null,\"name\":\"Roland Siegwart\"}],\"doi\":\"10.5555/3398761\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8f73900ac53537d01cf63ce76b77a377667c7a7b\",\"title\":\"Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems\",\"url\":\"https://www.semanticscholar.org/paper/8f73900ac53537d01cf63ce76b77a377667c7a7b\",\"venue\":\"AAMAS\",\"year\":2020}],\"title\":\"Interactive Reinforcement Learning with Dynamic Reuse of Prior Knowledge from Human and Agent Demonstrations\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Interactivity\",\"topicId\":\"192\",\"url\":\"https://www.semanticscholar.org/topic/192\"}],\"url\":\"https://www.semanticscholar.org/paper/d12ac04b7ba4b03841daa97a5d4d339002a32359\",\"venue\":\"IJCAI\",\"year\":2019}\n"