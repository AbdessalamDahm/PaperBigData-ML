"{\"abstract\":\"Learning optimal policies in real-world domains with delayed rewards is a major challenge in Reinforcement Learning. We address the credit assignment problem by proposing a Gaussian Process (GP)-based immediate reward approximation algorithm and evaluate its effectiveness in 4 contexts where rewards can be delayed for long trajectories. In one GridWorld game and 8 Atari games, where immediate rewards are available, our results showed that on 7 out 9 games, the proposed GPinferred reward policy performed at least as well as the immediate reward policy and significantly outperformed the corresponding delayed reward policy. In e-learning and healthcare applications, we combined GP-inferred immediate rewards with offline Deep Q-Network (DQN) policy induction and showed that the GP-inferred reward policies outperformed the policies induced using delayed rewards in both real-world contexts.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"51216654\",\"name\":\"H. Azizsoltani\",\"url\":\"https://www.semanticscholar.org/author/51216654\"},{\"authorId\":\"49170904\",\"name\":\"Yeo-Jin Kim\",\"url\":\"https://www.semanticscholar.org/author/49170904\"},{\"authorId\":\"51039532\",\"name\":\"Markel Sanz Ausin\",\"url\":\"https://www.semanticscholar.org/author/51039532\"},{\"authorId\":\"1734603\",\"name\":\"T. Barnes\",\"url\":\"https://www.semanticscholar.org/author/1734603\"},{\"authorId\":\"1731937\",\"name\":\"Min Chi\",\"url\":\"https://www.semanticscholar.org/author/1731937\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"31287901\",\"name\":\"Mehak Maniktala\"},{\"authorId\":\"152553911\",\"name\":\"T. Barnes\"},{\"authorId\":\"1423680515\",\"name\":\"Min Chi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ceb101297ef23acf585dd713992ba41af128262\",\"title\":\"Extending the Hint Factory: Towards Modelling Productivity for Open-ended Problem-solving\",\"url\":\"https://www.semanticscholar.org/paper/6ceb101297ef23acf585dd713992ba41af128262\",\"venue\":\"EDM\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3349939\",\"name\":\"Guojing Zhou\"},{\"authorId\":\"51216654\",\"name\":\"H. Azizsoltani\"},{\"authorId\":\"51039532\",\"name\":\"Markel Sanz Ausin\"},{\"authorId\":\"152553911\",\"name\":\"T. Barnes\"},{\"authorId\":\"1423680515\",\"name\":\"Min Chi\"}],\"doi\":\"10.24963/ijcai.2020/647\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6a729366768abd5d85a0efd666e6f7ab40ca27d2\",\"title\":\"Hierarchical Reinforcement Learning for Pedagogical Policy Induction (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/6a729366768abd5d85a0efd666e6f7ab40ca27d2\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2006.05725\",\"authors\":[{\"authorId\":\"52225987\",\"name\":\"Michael Gimelfarb\"},{\"authorId\":\"1732536\",\"name\":\"S. Sanner\"},{\"authorId\":\"2944274\",\"name\":\"Chi-Guhn Lee\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d6033f2f8aed093327307a987ffe1b67dd57651\",\"title\":\"Bayesian Experience Reuse for Learning from Multiple Demonstrators\",\"url\":\"https://www.semanticscholar.org/paper/2d6033f2f8aed093327307a987ffe1b67dd57651\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51039532\",\"name\":\"Markel Sanz Ausin\"},{\"authorId\":\"1508776152\",\"name\":\"Mehak Maniktala\"},{\"authorId\":\"152553911\",\"name\":\"T. Barnes\"},{\"authorId\":\"1423680515\",\"name\":\"Min Chi\"}],\"doi\":\"10.1007/978-3-030-52237-7_38\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6f8aa8c2e003086fb3d5acfcb0821927ee526863\",\"title\":\"Exploring the Impact of Simple Explanations and Agency on Batch Deep Reinforcement Learning Induced Pedagogical Policies\",\"url\":\"https://www.semanticscholar.org/paper/6f8aa8c2e003086fb3d5acfcb0821927ee526863\",\"venue\":\"AIED\",\"year\":2020}],\"corpusId\":199466375,\"doi\":\"10.24963/ijcai.2019/273\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"22433eaf7dc3fecb2e7e2e3560bc383288c98041\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2057050\",\"name\":\"Y. Engel\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1766683\",\"name\":\"R. Meir\"}],\"doi\":\"10.1145/1102351.1102377\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba4bcd8c4ebb1427d335c23d50249d370778ae49\",\"title\":\"Reinforcement learning with Gaussian processes\",\"url\":\"https://www.semanticscholar.org/paper/ba4bcd8c4ebb1427d335c23d50249d370778ae49\",\"venue\":\"ICML '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2681621\",\"name\":\"R. Schwonke\"},{\"authorId\":\"1747907\",\"name\":\"A. Renkl\"},{\"authorId\":\"25708572\",\"name\":\"Carmen Krieg\"},{\"authorId\":\"1838237\",\"name\":\"J. Wittwer\"},{\"authorId\":\"1779915\",\"name\":\"V. Aleven\"},{\"authorId\":\"145433768\",\"name\":\"Ron Salden\"}],\"doi\":\"10.1016/j.chb.2008.12.011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7564a4ca6b23ed9b62877e442c77dd62bb028718\",\"title\":\"The worked-example effect: Not an artefact of lousy control conditions\",\"url\":\"https://www.semanticscholar.org/paper/7564a4ca6b23ed9b62877e442c77dd62bb028718\",\"venue\":\"Comput. Hum. Behav.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3472959\",\"name\":\"C. Rasmussen\"},{\"authorId\":\"8231010\",\"name\":\"M. Kuss\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3cc0095b47615c2f47beaca9bd8f675811fb118a\",\"title\":\"Gaussian Processes in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3cc0095b47615c2f47beaca9bd8f675811fb118a\",\"venue\":\"NIPS\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1790783\",\"name\":\"R. Maitra\"},{\"authorId\":\"3176189\",\"name\":\"I. Ramler\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7a510d21257156ad3dd7eedc7d699b4cefc7bd1f\",\"title\":\"Supplement to \\u201c A k-mean-directions Algorithm for Fast Clustering of Data on the Sphere \\u201d published in the Journal of Computational and Graphical Statistics\",\"url\":\"https://www.semanticscholar.org/paper/7a510d21257156ad3dd7eedc7d699b4cefc7bd1f\",\"venue\":\"\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2057050\",\"name\":\"Y. Engel\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1766683\",\"name\":\"R. Meir\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5f7d2efca150cc63ea4e6d25035c8f2430c6d803\",\"title\":\"Bayes Meets Bellman: The Gaussian Process Approach to Temporal Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/5f7d2efca150cc63ea4e6d25035c8f2430c6d803\",\"venue\":\"ICML\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1766844\",\"name\":\"Eric Wiewiora\"},{\"authorId\":\"48524582\",\"name\":\"G. Cottrell\"},{\"authorId\":\"1722831\",\"name\":\"C. Elkan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8b6b33f750b93b3fb0b90ef219f7e0d0acc66aa\",\"title\":\"Principled Methods for Advising Reinforcement Learning Agents\",\"url\":\"https://www.semanticscholar.org/paper/b8b6b33f750b93b3fb0b90ef219f7e0d0acc66aa\",\"venue\":\"ICML\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrew G Barto. Intrinsic motivation\"},{\"authorId\":null,\"name\":\"reinforcement learning. In Intrinsically motivated learning in natural\"},{\"authorId\":null,\"name\":\"artificial systems\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 17\\u201347\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Noel Cressie. Statistics for spatial data\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Terra Nova\",\"url\":\"\",\"venue\":\"4(5):613\\u2013617,\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Trauma Brewin\"},{\"authorId\":\"145384892\",\"name\":\"C. Andrews\"},{\"authorId\":\"143667139\",\"name\":\"B. Valentine\"},{\"authorId\":\"21463991\",\"name\":\"J. D\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3f886df4d48ee02f6ec7c8a08629b92bae0bdba0\",\"title\":\"Journal of Consulting and Clinical Psychology\",\"url\":\"https://www.semanticscholar.org/paper/3f886df4d48ee02f6ec7c8a08629b92bae0bdba0\",\"venue\":\"\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152128108\",\"name\":\"Yang Gao\"},{\"authorId\":\"49973505\",\"name\":\"Francesca Toni\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9249e04823904b5d8cd77a23f6835b63f284aa7c\",\"title\":\"Potential Based Reward Shaping for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9249e04823904b5d8cd77a23f6835b63f284aa7c\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98241663\",\"name\":\"M. V. Rossum\"}],\"doi\":\"10.1142/9789814360784_0003\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"title\":\"Neural Computation\",\"url\":\"https://www.semanticscholar.org/paper/2d5af1ab6368f20a4a9bb2afae23663e5b08b9c6\",\"venue\":\"\",\"year\":1989},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carl Edward Rasmussen. Gaussian processes in machine le Learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 63\\u201371\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Rolf Schwonke\"},{\"authorId\":null,\"name\":\"Alexander Renkl\"},{\"authorId\":null,\"name\":\"Carmen Krieg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Learning agents for uncertain environments\",\"url\":\"\",\"venue\":\"The Journal of Experimental Education\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49170904\",\"name\":\"Yeo-Jin Kim\"},{\"authorId\":\"1731937\",\"name\":\"Min Chi\"}],\"doi\":\"10.24963/ijcai.2018/322\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c11d0bd680b9c26d60a1c9f78b4bc90e9d0929ab\",\"title\":\"Temporal Belief Memory: Imputing Missing Data during RNN Training\",\"url\":\"https://www.semanticscholar.org/paper/c11d0bd680b9c26d60a1c9f78b4bc90e9d0929ab\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hamoon Azizsoltani\"},{\"authorId\":null,\"name\":\"Achintya Haldar. Reliability analysis of lead-free solders model\"},{\"authorId\":null,\"name\":\"kriging concept\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Electronic Packaging\",\"url\":\"\",\"venue\":\"140(4):041003,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50494837\",\"name\":\"L. Cronbach\"},{\"authorId\":\"50257036\",\"name\":\"R. Snow\"}],\"doi\":\"10.2307/748778\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a6fcf2f9e49c12ecc0e4872783160747a854db3e\",\"title\":\"Aptitudes and instructional methods: A handbook for research on interactions\",\"url\":\"https://www.semanticscholar.org/paper/a6fcf2f9e49c12ecc0e4872783160747a854db3e\",\"venue\":\"\",\"year\":1977},{\"arxivId\":null,\"authors\":[{\"authorId\":\"102963457\",\"name\":\"R. E. Snow\"}],\"doi\":\"10.1037/0022-006X.59.2.205\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a3854085004c436b2ca5c36746bf0546a5f5b7ec\",\"title\":\"Aptitude-treatment interaction as a framework for research on individual differences in psychotherapy.\",\"url\":\"https://www.semanticscholar.org/paper/a3854085004c436b2ca5c36746bf0546a5f5b7ec\",\"venue\":\"Journal of consulting and clinical psychology\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52225987\",\"name\":\"Michael Gimelfarb\"},{\"authorId\":\"1732536\",\"name\":\"S. Sanner\"},{\"authorId\":\"2944274\",\"name\":\"Chi-Guhn Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"90db37691418162f58937652cbc6c109471747a8\",\"title\":\"Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach\",\"url\":\"https://www.semanticscholar.org/paper/90db37691418162f58937652cbc6c109471747a8\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114788213\",\"name\":\"D. Signorini\"},{\"authorId\":\"4137433\",\"name\":\"J. Slattery\"},{\"authorId\":\"12113526\",\"name\":\"S. Dodds\"},{\"authorId\":\"26151757\",\"name\":\"V. Lane\"},{\"authorId\":\"143842868\",\"name\":\"P. Littlejohns\"}],\"doi\":\"10.1016/S0140-6736(95)92525-2\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"20b844e395355b40fa5940c61362ec40e56027aa\",\"title\":\"Neural networks\",\"url\":\"https://www.semanticscholar.org/paper/20b844e395355b40fa5940c61362ec40e56027aa\",\"venue\":\"The Lancet\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"29984691\",\"name\":\"A. Renkl\"},{\"authorId\":\"1737845\",\"name\":\"R. K. Atkinson\"},{\"authorId\":\"30080689\",\"name\":\"U. Maier\"},{\"authorId\":\"40368267\",\"name\":\"R. Staley\"}],\"doi\":\"10.1080/00220970209599510\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bda25348e038bfae46336058940efa7e0b03beb8\",\"title\":\"From Example Study to Problem Solving: Smooth Transitions Help Learning\",\"url\":\"https://www.semanticscholar.org/paper/bda25348e038bfae46336058940efa7e0b03beb8\",\"venue\":\"\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"12475600\",\"name\":\"C. Colaianni\"}],\"doi\":\"10.1056/NEJMp1808753\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7247510016636638b4957922501ca31cbb47aca\",\"title\":\"Terra Nova.\",\"url\":\"https://www.semanticscholar.org/paper/f7247510016636638b4957922501ca31cbb47aca\",\"venue\":\"The New England journal of medicine\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yang Gao\"},{\"authorId\":null,\"name\":\"Francesca Toni\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"inforcement learning with gaussian processes\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":\"1606.01540\",\"authors\":[{\"authorId\":\"49508975\",\"name\":\"G. Brockman\"},{\"authorId\":\"34415167\",\"name\":\"Vicki Cheung\"},{\"authorId\":\"152877508\",\"name\":\"Ludwig Pettersson\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143805717\",\"name\":\"Jie Tang\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"title\":\"OpenAI Gym\",\"url\":\"https://www.semanticscholar.org/paper/ff7f3277c6fa759e84e1ab7664efdac1c1cec76b\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1751657\",\"name\":\"D. Nychka\"},{\"authorId\":\"1746762\",\"name\":\"Soutir Bandyopadhyay\"},{\"authorId\":\"3420744\",\"name\":\"D. Hammerling\"},{\"authorId\":\"35324548\",\"name\":\"F. Lindgren\"},{\"authorId\":\"2246552\",\"name\":\"S. Sain\"}],\"doi\":\"10.1080/10618600.2014.914946\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"609805c8591edf6ecae767214e07378dbb60419c\",\"title\":\"A Multiresolution Gaussian Process Model for the Analysis of Large Spatial Datasets\",\"url\":\"https://www.semanticscholar.org/paper/609805c8591edf6ecae767214e07378dbb60419c\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Andrew Y Ng. Apprenticeship learning via inverse reinforc learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"page 1\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2004},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1711.09602\",\"authors\":[{\"authorId\":\"31411877\",\"name\":\"Aniruddh Raghu\"},{\"authorId\":\"6204450\",\"name\":\"M. Komorowski\"},{\"authorId\":\"144168380\",\"name\":\"I. Ahmed\"},{\"authorId\":\"1827828\",\"name\":\"L. A. Celi\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"},{\"authorId\":\"145348788\",\"name\":\"M. Ghassemi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a729073d3ec9a159b86fd438e06f3dff2735beb5\",\"title\":\"Deep Reinforcement Learning for Sepsis Treatment\",\"url\":\"https://www.semanticscholar.org/paper/a729073d3ec9a159b86fd438e06f3dff2735beb5\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144115104\",\"name\":\"A. D. Carvalho\"},{\"authorId\":\"68983551\",\"name\":\"M. G. Romay\"},{\"authorId\":\"1996441\",\"name\":\"K. Guelton\"},{\"authorId\":\"1712282\",\"name\":\"A. Kaklauskas\"},{\"authorId\":\"144833640\",\"name\":\"B. K. Panigrahi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0af5e2266b8bee31096c219bc99d49d0ca51e004\",\"title\":\"ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE\",\"url\":\"https://www.semanticscholar.org/paper/0af5e2266b8bee31096c219bc99d49d0ca51e004\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrew Ng\"},{\"authorId\":null,\"name\":\"Stuart J Russell\"},{\"authorId\":null,\"name\":\"Soutir Bandyopad-hyay Douglas Nychka\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Dorit Hammerling , Finn Lindgren , and Stephan Sain . A multiresolution gaussian process model for the analysis of large spatial datasets\",\"url\":\"\",\"venue\":\"Journal of Computational and Graphical Statistics\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Stuart J Russell. Learning agents for uncertain environments\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In COLT\",\"url\":\"\",\"venue\":\"pages 101\\u2013103,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yang Gao\"},{\"authorId\":null,\"name\":\"Francesca Toni\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"inforcement learning with gaussian processes\",\"url\":\"\",\"venue\":\"\",\"year\":null}],\"title\":\"Unobserved Is Not Equal to Non-existent: Using Gaussian Processes to Infer Immediate Rewards Across Contexts\",\"topics\":[{\"topic\":\"Gaussian process\",\"topicId\":\"8748\",\"url\":\"https://www.semanticscholar.org/topic/8748\"}],\"url\":\"https://www.semanticscholar.org/paper/22433eaf7dc3fecb2e7e2e3560bc383288c98041\",\"venue\":\"IJCAI\",\"year\":2019}\n"