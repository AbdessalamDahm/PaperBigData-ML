"{\"abstract\":\"Recent reinforcement learning (RL) approaches have shown strong performance in complex domains such as Atari games, but are often highly sample inefficient. A common approach to reduce interaction time with the environment is to use reward shaping, which involves carefully designing reward functions that provide the agent intermediate rewards for progress towards the goal. However, designing appropriate shaping rewards is known to be difficult as well as time-consuming. In this work, we address this problem by using natural language instructions to perform reward shaping. We propose the LanguagE-Action Reward Network (LEARN), a framework that maps free-form natural language instructions to intermediate rewards based on actions taken by the agent. These intermediate language-based rewards can seamlessly be integrated into any standard reinforcement learning algorithm. We experiment with Montezuma's Revenge from the Atari Learning Environment, a popular benchmark in RL. Our experiments on a diverse set of 15 tasks demonstrate that, for the same number of interactions with the environment, language-based rewards lead to successful completion of the task 60% more often on average, compared to learning without language.\",\"arxivId\":\"1903.02020\",\"authors\":[{\"authorId\":\"38774604\",\"name\":\"Prasoon Goyal\",\"url\":\"https://www.semanticscholar.org/author/38774604\"},{\"authorId\":\"2791038\",\"name\":\"S. Niekum\",\"url\":\"https://www.semanticscholar.org/author/2791038\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\",\"url\":\"https://www.semanticscholar.org/author/1797655\"}],\"citationVelocity\":14,\"citations\":[{\"arxivId\":\"1910.04040\",\"authors\":[{\"authorId\":\"1415696218\",\"name\":\"Matthias Hutsebaut-Buysse\"},{\"authorId\":\"98922276\",\"name\":\"Kevin Mets\"},{\"authorId\":\"46301036\",\"name\":\"S. Latr\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0873aac6e84597e3b5d2730783f6f6fb0e361400\",\"title\":\"Fast Task-Adaptation for Tasks Labeled Using Natural Language in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0873aac6e84597e3b5d2730783f6f6fb0e361400\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1911.02683\",\"authors\":[{\"authorId\":\"24835910\",\"name\":\"Jesse Mu\"},{\"authorId\":\"145419642\",\"name\":\"Percy Liang\"},{\"authorId\":\"144002017\",\"name\":\"Noah D. Goodman\"}],\"doi\":\"10.18653/V1/2020.ACL-MAIN.436\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"17c0df95e608f91c2385e42c629a65f095988c10\",\"title\":\"Shaping Visual Representations with Language for Few-shot Classification\",\"url\":\"https://www.semanticscholar.org/paper/17c0df95e608f91c2385e42c629a65f095988c10\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":\"2010.12639\",\"authors\":[{\"authorId\":\"30970136\",\"name\":\"Shurjo Banerjee\"},{\"authorId\":\"2665873\",\"name\":\"Jesse Thomason\"},{\"authorId\":\"3587688\",\"name\":\"Jason J. Corso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"191f2a22c3cd2bcebdb3c2f5ac354e2a464e0931\",\"title\":\"The RobotSlang Benchmark: Dialog-guided Robot Localization and Navigation\",\"url\":\"https://www.semanticscholar.org/paper/191f2a22c3cd2bcebdb3c2f5ac354e2a464e0931\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.02316\",\"authors\":[{\"authorId\":\"33341943\",\"name\":\"Ameet Deshpande\"},{\"authorId\":\"1988380666\",\"name\":\"Eve Fleisig\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8c1fbc0e6cf0ae9eafca5e34a245dcbddb206850\",\"title\":\"Sentiment Analysis for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8c1fbc0e6cf0ae9eafca5e34a245dcbddb206850\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.14767\",\"authors\":[{\"authorId\":\"1389885246\",\"name\":\"Aishwarya Padmakumar\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"87673d658e906a9791d5e7075ed51055b3ebb041\",\"title\":\"Dialog as a Vehicle for Lifelong Learning\",\"url\":\"https://www.semanticscholar.org/paper/87673d658e906a9791d5e7075ed51055b3ebb041\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.09253\",\"authors\":[{\"authorId\":\"102281182\",\"name\":\"C\\u00e9dric Colas\"},{\"authorId\":\"1500721464\",\"name\":\"Tristan Karch\"},{\"authorId\":\"1400348617\",\"name\":\"Nicolas Lair\"},{\"authorId\":\"1400348565\",\"name\":\"Jean-Michel Dussoux\"},{\"authorId\":\"1402792211\",\"name\":\"Cl\\u00e9ment Moulin-Frier\"},{\"authorId\":\"9094493\",\"name\":\"Peter Ford Dominey\"},{\"authorId\":\"1720664\",\"name\":\"Pierre-Yves Oudeyer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bc9071509d7539518e10b87df3a26fc67312b1b0\",\"title\":\"Language as a Cognitive Tool to Imagine Goals in Curiosity-Driven Exploration\",\"url\":\"https://www.semanticscholar.org/paper/bc9071509d7539518e10b87df3a26fc67312b1b0\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2406799\",\"name\":\"Kathryn Mazaitis\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"43fff3c4270204102225c1aade5701b0cdb197e8\",\"title\":\"Conversational Learning\",\"url\":\"https://www.semanticscholar.org/paper/43fff3c4270204102225c1aade5701b0cdb197e8\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Antony Yun\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"648832bc487121147890cc9f10b18f8b05c1d70b\",\"title\":\"Evaluating the Robustness of Natural Language Reward Shaping Models to Spatial Relations\",\"url\":\"https://www.semanticscholar.org/paper/648832bc487121147890cc9f10b18f8b05c1d70b\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1819291575\",\"name\":\"Jinying Lin\"},{\"authorId\":\"98132363\",\"name\":\"Z. Ma\"},{\"authorId\":\"48489626\",\"name\":\"R. Gomez\"},{\"authorId\":\"48570224\",\"name\":\"K. Nakamura\"},{\"authorId\":\"40145946\",\"name\":\"Bo He\"},{\"authorId\":\"46438968\",\"name\":\"Guangliang Li\"}],\"doi\":\"10.1109/ACCESS.2020.3006254\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"008b5c04ce9023b7c10a5c8709ca5f367c4b58b3\",\"title\":\"A Review on Interactive Reinforcement Learning From Human Social Feedback\",\"url\":\"https://www.semanticscholar.org/paper/008b5c04ce9023b7c10a5c8709ca5f367c4b58b3\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":\"2005.00730\",\"authors\":[{\"authorId\":\"8937909\",\"name\":\"Nazneen Rajani\"},{\"authorId\":\"80083020\",\"name\":\"Rui Zhang\"},{\"authorId\":\"144787248\",\"name\":\"Y. Tan\"},{\"authorId\":\"3393918\",\"name\":\"Stephan Zheng\"},{\"authorId\":\"50041481\",\"name\":\"Jeremy Weiss\"},{\"authorId\":\"152945101\",\"name\":\"Aadit Vyas\"},{\"authorId\":\"40696782\",\"name\":\"A. Gupta\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"1405531452\",\"name\":\"D. Radev\"}],\"doi\":\"10.18653/v1/2020.acl-main.706\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d1340b56e158eecce8432b9595fa9dd2c014c54b\",\"title\":\"ESPRIT: Explaining Solutions to Physical Reasoning Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d1340b56e158eecce8432b9595fa9dd2c014c54b\",\"venue\":\"ACL\",\"year\":2020},{\"arxivId\":\"2007.09774\",\"authors\":[{\"authorId\":\"1823518201\",\"name\":\"Brielen Madureira\"},{\"authorId\":\"69056125\",\"name\":\"D. Schlangen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a4069dd677b205fba61b4dea75e26c148dee99c5\",\"title\":\"An Overview of Natural Language State Representation for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a4069dd677b205fba61b4dea75e26c148dee99c5\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2008.12095\",\"authors\":[{\"authorId\":\"1660812666\",\"name\":\"Katya Kudashkina\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"860b1ee24c1b372d89d601d4d1aee28b5735948e\",\"title\":\"Document-editing Assistants and Model-based Reinforcement Learning as a Path to Conversational AI\",\"url\":\"https://www.semanticscholar.org/paper/860b1ee24c1b372d89d601d4d1aee28b5735948e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1500721464\",\"name\":\"Tristan Karch\"},{\"authorId\":\"1400348617\",\"name\":\"Nicolas Lair\"},{\"authorId\":\"102281182\",\"name\":\"C\\u00e9dric Colas\"},{\"authorId\":\"1400348565\",\"name\":\"Jean-Michel Dussoux\"},{\"authorId\":\"1402792211\",\"name\":\"Cl\\u00e9ment Moulin-Frier\"},{\"authorId\":\"9094493\",\"name\":\"Peter Ford Dominey\"},{\"authorId\":\"1720664\",\"name\":\"Pierre-Yves Oudeyer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c0c5a461adebc6c0b3e29153f0bff0f53376d243\",\"title\":\"Language-Goal Imagination to Foster Creative Exploration in Deep RL\",\"url\":\"https://www.semanticscholar.org/paper/c0c5a461adebc6c0b3e29153f0bff0f53376d243\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1911.03219\",\"authors\":[{\"authorId\":\"1400348617\",\"name\":\"Nicolas Lair\"},{\"authorId\":\"102281182\",\"name\":\"C\\u00e9dric Colas\"},{\"authorId\":\"1379927403\",\"name\":\"R\\u00e9my Portelas\"},{\"authorId\":\"1400348565\",\"name\":\"Jean-Michel Dussoux\"},{\"authorId\":\"9094493\",\"name\":\"Peter Ford Dominey\"},{\"authorId\":\"1720664\",\"name\":\"Pierre-Yves Oudeyer\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"c67c46765ac4a084129bdf84b4be93bdd988322f\",\"title\":\"Language Grounding through Social Interactions and Curiosity-Driven Multi-Goal Learning\",\"url\":\"https://www.semanticscholar.org/paper/c67c46765ac4a084129bdf84b4be93bdd988322f\",\"venue\":\"ViGIL@NeurIPS\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144214530\",\"name\":\"B. Prakash\"},{\"authorId\":\"3436871\",\"name\":\"Nicholas R. Waytowich\"},{\"authorId\":\"2116290\",\"name\":\"Ashwinkumar Ganesan\"},{\"authorId\":\"143979239\",\"name\":\"T. Oates\"},{\"authorId\":\"2393902\",\"name\":\"T. Mohsenin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f59d6d012c8ca3430de742c5e6dd634d39074014\",\"title\":\"Guiding Safe Reinforcement Learning Policies Using Structured Language Constraints\",\"url\":\"https://www.semanticscholar.org/paper/f59d6d012c8ca3430de742c5e6dd634d39074014\",\"venue\":\"SafeAI@AAAI\",\"year\":2020},{\"arxivId\":\"2008.06924\",\"authors\":[{\"authorId\":\"48207454\",\"name\":\"Li Zhou\"},{\"authorId\":\"50044599\",\"name\":\"Kevin Small\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9f21200b3557e5b3cb02e179b50019a1513b728e\",\"title\":\"Inverse Reinforcement Learning with Natural Language Goals\",\"url\":\"https://www.semanticscholar.org/paper/9f21200b3557e5b3cb02e179b50019a1513b728e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2004.07200\",\"authors\":[{\"authorId\":\"152612745\",\"name\":\"Tianshi Cao\"},{\"authorId\":\"71563016\",\"name\":\"Jingkang Wang\"},{\"authorId\":\"51052215\",\"name\":\"Yining Zhang\"},{\"authorId\":\"39981216\",\"name\":\"S. Manivasagam\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"416e02fa848e02fe0f030c61eb0c786b0c66db5d\",\"title\":\"BabyAI++: Towards Grounded-Language Learning beyond Memorization\",\"url\":\"https://www.semanticscholar.org/paper/416e02fa848e02fe0f030c61eb0c786b0c66db5d\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.07648\",\"authors\":[{\"authorId\":\"32245472\",\"name\":\"Corey Lynch\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8095bdd5861d1dbe43b77997bc0dbc2fd51acb93\",\"title\":\"Grounding Language in Play\",\"url\":\"https://www.semanticscholar.org/paper/8095bdd5861d1dbe43b77997bc0dbc2fd51acb93\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.14715\",\"authors\":[{\"authorId\":\"1976174397\",\"name\":\"Theodore R. Sumers\"},{\"authorId\":\"2543534\",\"name\":\"M. Ho\"},{\"authorId\":\"1881518\",\"name\":\"R. D. Hawkins\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"1799860\",\"name\":\"T. Griffiths\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aaeef30c77c6a10f7a4d1cc2778de393c1021995\",\"title\":\"Learning Rewards from Linguistic Feedback\",\"url\":\"https://www.semanticscholar.org/paper/aaeef30c77c6a10f7a4d1cc2778de393c1021995\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.01723\",\"authors\":[{\"authorId\":\"103596213\",\"name\":\"Kolby Nottingham\"},{\"authorId\":\"48347382\",\"name\":\"Anand Balakrishnan\"},{\"authorId\":\"1681722\",\"name\":\"J. Deshmukh\"},{\"authorId\":\"1381294482\",\"name\":\"Connor Christopherson\"},{\"authorId\":\"30585164\",\"name\":\"D. Wingate\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bb48fb892b87554817da4321dfd74b315fb11320\",\"title\":\"Using Logical Specifications of Objectives in Multi-Objective Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bb48fb892b87554817da4321dfd74b315fb11320\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52248191\",\"name\":\"Byu ScholarsArchive\"},{\"authorId\":\"103596213\",\"name\":\"Kolby Nottingham\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26cc8b3e34f38f809026cc3c20c8b2b3a752478d\",\"title\":\"Using Logical Specifications for Multi-Objective Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/26cc8b3e34f38f809026cc3c20c8b2b3a752478d\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1906.03926\",\"authors\":[{\"authorId\":\"1818756\",\"name\":\"Jelena Luketina\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"},{\"authorId\":\"2620211\",\"name\":\"Tim Rockt\\u00e4schel\"}],\"doi\":\"10.24963/ijcai.2019/880\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50\",\"title\":\"A Survey of Reinforcement Learning Informed by Natural Language\",\"url\":\"https://www.semanticscholar.org/paper/7dc156eb9d84ae8fd521ecac5ccc5b5426a42b50\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":\"2004.13657\",\"authors\":[{\"authorId\":\"1660812666\",\"name\":\"Katya Kudashkina\"},{\"authorId\":\"46660962\",\"name\":\"Valliappa Chockalingam\"},{\"authorId\":\"144639556\",\"name\":\"Graham W. Taylor\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"35763a64b605f37b6de74c776c57dc964416eb18\",\"title\":\"Sample-Efficient Model-based Actor-Critic for an Interactive Dialogue Task\",\"url\":\"https://www.semanticscholar.org/paper/35763a64b605f37b6de74c776c57dc964416eb18\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1405531452\",\"name\":\"D. Radev\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"cd57c6e1409ff5b7100147e630cb78fe25e1a112\",\"title\":\"CPSC 490 : Senior Project Proposal\",\"url\":\"https://www.semanticscholar.org/paper/cd57c6e1409ff5b7100147e630cb78fe25e1a112\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2029676598\",\"name\":\"Muddasar Naeem\"},{\"authorId\":\"38616786\",\"name\":\"Syed Tahir Hussain Rizvi\"},{\"authorId\":\"3072035\",\"name\":\"A. Coronato\"}],\"doi\":\"10.1109/ACCESS.2020.3038605\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"886144bdf52254bca44d3bd38bc971a5220ce288\",\"title\":\"A Gentle Introduction to Reinforcement Learning and its Application in Different Fields\",\"url\":\"https://www.semanticscholar.org/paper/886144bdf52254bca44d3bd38bc971a5220ce288\",\"venue\":\"IEEE Access\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50632861\",\"name\":\"R. Freedman\"},{\"authorId\":\"40947489\",\"name\":\"Rohin Shah\"},{\"authorId\":\"2745001\",\"name\":\"Anca D. Dragan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"de599e0fed4795bd63f1d2a81c7607ff19ab618b\",\"title\":\"Choice Set Misspecification in Reward Inference\",\"url\":\"https://www.semanticscholar.org/paper/de599e0fed4795bd63f1d2a81c7607ff19ab618b\",\"venue\":\"AISafety@IJCAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"151484341\",\"name\":\"Aidin Shiri\"},{\"authorId\":\"1564619335\",\"name\":\"Arnab Neelim Mazumder\"},{\"authorId\":\"1925495273\",\"name\":\"Bharat Prakash\"},{\"authorId\":\"1820794562\",\"name\":\"Nitheesh Kumar Manjunath\"},{\"authorId\":\"1747542\",\"name\":\"H. Homayoun\"},{\"authorId\":\"1798722\",\"name\":\"Avesta Sasan\"},{\"authorId\":\"3436871\",\"name\":\"Nicholas R. Waytowich\"},{\"authorId\":\"1751099751\",\"name\":\"T. Mohsenin\"}],\"doi\":\"10.1145/3386263.3407652\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9740305d2d2bc7d57ec778f87b2d581f7c22b42f\",\"title\":\"Energy-Efficient Hardware for Language Guided Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9740305d2d2bc7d57ec778f87b2d581f7c22b42f\",\"venue\":\"ACM Great Lakes Symposium on VLSI\",\"year\":2020},{\"arxivId\":\"2007.15543\",\"authors\":[{\"authorId\":\"38774604\",\"name\":\"Prasoon Goyal\"},{\"authorId\":\"2791038\",\"name\":\"S. Niekum\"},{\"authorId\":\"1797655\",\"name\":\"R. Mooney\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffc765ceffc4d3c5c8aa47897dcc79c46438f146\",\"title\":\"PixL2R: Guiding Reinforcement Learning Using Natural Language by Mapping Pixels to Rewards\",\"url\":\"https://www.semanticscholar.org/paper/ffc765ceffc4d3c5c8aa47897dcc79c46438f146\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":70350059,\"doi\":\"10.24963/ijcai.2019/331\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":5,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"0fa1c75a452a046e11e775eb6120051c696d9366\",\"references\":[{\"arxivId\":\"1705.02364\",\"authors\":[{\"authorId\":\"2480903\",\"name\":\"Alexis Conneau\"},{\"authorId\":\"1743722\",\"name\":\"Douwe Kiela\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"2934336\",\"name\":\"Lo\\u00efc Barrault\"},{\"authorId\":\"1713934\",\"name\":\"Antoine Bordes\"}],\"doi\":\"10.18653/v1/D17-1070\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c\",\"title\":\"Supervised Learning of Universal Sentence Representations from Natural Language Inference Data\",\"url\":\"https://www.semanticscholar.org/paper/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Stevan Harnad. The symbol grounding problem\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Physica D: Nonlinear Phenomena\",\"url\":\"\",\"venue\":\"42(1-3):335\\u2013346,\",\"year\":1990},{\"arxivId\":\"1705.04304\",\"authors\":[{\"authorId\":\"2896063\",\"name\":\"Romain Paulus\"},{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"032274e57f7d8b456bd255fe76b909b2c1d7458e\",\"title\":\"A Deep Reinforced Model for Abstractive Summarization\",\"url\":\"https://www.semanticscholar.org/paper/032274e57f7d8b456bd255fe76b909b2c1d7458e\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":\"1606.01541\",\"authors\":[{\"authorId\":\"5183779\",\"name\":\"J. Li\"},{\"authorId\":\"145768639\",\"name\":\"Will Monroe\"},{\"authorId\":\"1863425\",\"name\":\"Alan Ritter\"},{\"authorId\":\"1746807\",\"name\":\"Dan Jurafsky\"},{\"authorId\":\"1947267\",\"name\":\"Michel Galley\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"}],\"doi\":\"10.18653/v1/D16-1127\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"1298dae5751fb06184f6b067d1503bde8037bdb7\",\"title\":\"Deep Reinforcement Learning for Dialogue Generation\",\"url\":\"https://www.semanticscholar.org/paper/1298dae5751fb06184f6b067d1503bde8037bdb7\",\"venue\":\"EMNLP\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Russell Kaplan\"},{\"authorId\":null,\"name\":\"Christopher Sauer\"},{\"authorId\":null,\"name\":\"Alexander Sosa\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Association for Computational Linguistics . [ Harnad , 1990 ] Stevan Harnad . The symbol grounding prob\",\"url\":\"\",\"venue\":\"Physica D : Nonlinear Phenomena\",\"year\":null},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"114607071\",\"name\":\"Gregory Kuhlmann and Peter Stone and Raymond J. Mooney and Shavlik\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"dd2d5ddc0399e0b87c339ebea4042ef2ad6f0317\",\"title\":\"Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in RoboCup Soccer\",\"url\":\"https://www.semanticscholar.org/paper/dd2d5ddc0399e0b87c339ebea4042ef2ad6f0317\",\"venue\":\"AAAI 2004\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143845796\",\"name\":\"Jeffrey Pennington\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"},{\"authorId\":\"144783904\",\"name\":\"Christopher D. Manning\"}],\"doi\":\"10.3115/v1/D14-1162\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"title\":\"Glove: Global Vectors for Word Representation\",\"url\":\"https://www.semanticscholar.org/paper/f37e1b62a767a307c046404ca96bc140b3e68cb5\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1704.08795\",\"authors\":[{\"authorId\":\"31498163\",\"name\":\"Dipendra Kumar Misra\"},{\"authorId\":\"144162125\",\"name\":\"J. Langford\"},{\"authorId\":\"3167681\",\"name\":\"Yoav Artzi\"}],\"doi\":\"10.18653/v1/D17-1106\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"bc9f3c466c6f6b386f4ef1195853d498cf3c182e\",\"title\":\"Mapping Instructions and Visual Observations to Actions with Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bc9f3c466c6f6b386f4ef1195853d498cf3c182e\",\"venue\":\"EMNLP\",\"year\":2017},{\"arxivId\":\"1412.6980\",\"authors\":[{\"authorId\":\"1726807\",\"name\":\"Diederik P. Kingma\"},{\"authorId\":\"2503659\",\"name\":\"Jimmy Ba\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"title\":\"Adam: A Method for Stochastic Optimization\",\"url\":\"https://www.semanticscholar.org/paper/a6cb366736791bcccc5c8639de5a8f9636bf87e8\",\"venue\":\"ICLR\",\"year\":2015},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1406.1078\",\"authors\":[{\"authorId\":\"1979489\",\"name\":\"Kyunghyun Cho\"},{\"authorId\":\"3158246\",\"name\":\"B. V. Merrienboer\"},{\"authorId\":\"1854385\",\"name\":\"\\u00c7aglar G\\u00fcl\\u00e7ehre\"},{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"2076086\",\"name\":\"Fethi Bougares\"},{\"authorId\":\"144518416\",\"name\":\"Holger Schwenk\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":\"10.3115/v1/D14-1179\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0b544dfe355a5070b60986319a3f51fb45d1348e\",\"title\":\"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\",\"url\":\"https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e\",\"venue\":\"EMNLP\",\"year\":2014},{\"arxivId\":\"1707.08817\",\"authors\":[{\"authorId\":\"7515048\",\"name\":\"Matej Vecer\\u00edk\"},{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"36881095\",\"name\":\"Jonathan Scholz\"},{\"authorId\":\"2555447\",\"name\":\"F. Wang\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"8282805\",\"name\":\"Thomas Roth\\u00f6rl\"},{\"authorId\":\"46534085\",\"name\":\"T. Lampe\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1bead9000a719cb258bac7320228055aee650d2c\",\"title\":\"Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards\",\"url\":\"https://www.semanticscholar.org/paper/1bead9000a719cb258bac7320228055aee650d2c\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152876233\",\"name\":\"E. C. Williams\"},{\"authorId\":\"2646714\",\"name\":\"Nakul Gopalan\"},{\"authorId\":\"22246383\",\"name\":\"M. Rhee\"},{\"authorId\":\"2913681\",\"name\":\"Stefanie Tellex\"}],\"doi\":\"10.1109/ICRA.2018.8460937\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6609eed6e39ccb6ee8761bb895218b1b384b23ae\",\"title\":\"Learning to Parse Natural Language to Grounded Reward Functions with Weak Supervision\",\"url\":\"https://www.semanticscholar.org/paper/6609eed6e39ccb6ee8761bb895218b1b384b23ae\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34699434\",\"name\":\"A. Ng\"},{\"authorId\":\"1868677\",\"name\":\"D. Harada\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"94066dc12fe31e96af7557838159bde598cb4f10\",\"title\":\"Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/94066dc12fe31e96af7557838159bde598cb4f10\",\"venue\":\"ICML\",\"year\":1999},{\"arxivId\":\"1811.10092\",\"authors\":[{\"authorId\":\"48631993\",\"name\":\"Xin Eric Wang\"},{\"authorId\":\"1788124\",\"name\":\"Qiuyuan Huang\"},{\"authorId\":\"1709797\",\"name\":\"A. \\u00c7elikyilmaz\"},{\"authorId\":\"1800422\",\"name\":\"Jianfeng Gao\"},{\"authorId\":\"19178763\",\"name\":\"Dinghan Shen\"},{\"authorId\":\"1706938\",\"name\":\"Y. Wang\"},{\"authorId\":\"1682479\",\"name\":\"William Yang Wang\"},{\"authorId\":\"39089563\",\"name\":\"Lei Zhang\"}],\"doi\":\"10.1109/CVPR.2019.00679\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c66b8e508718f4b7f14829e5c2cde0add31d2693\",\"title\":\"Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation\",\"url\":\"https://www.semanticscholar.org/paper/c66b8e508718f4b7f14829e5c2cde0add31d2693\",\"venue\":\"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"year\":2019},{\"arxivId\":\"1704.05539\",\"authors\":[{\"authorId\":\"31121905\",\"name\":\"R. Kaplan\"},{\"authorId\":\"2514884\",\"name\":\"C. Sauer\"},{\"authorId\":\"31825173\",\"name\":\"A. Sosa\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4248b1c782d1e3e3b53a5126ea269518af92c68a\",\"title\":\"Beating Atari with Natural Language Guided Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4248b1c782d1e3e3b53a5126ea269518af92c68a\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ilya Kostrikov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Pytorch implementations of reinforcement learning algorithms\",\"url\":\"\",\"venue\":\"https://github.com/ ikostrikov/pytorch-a2c-ppo-acktr,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"145783676\",\"name\":\"Felix Hill\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"145148319\",\"name\":\"E. Hughes\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"71b152f65fd9967ec39f1e1f359ad0d99be1bab2\",\"title\":\"Learning to Follow Language Instructions with Adversarial Reward Induction\",\"url\":\"https://www.semanticscholar.org/paper/71b152f65fd9967ec39f1e1f359ad0d99be1bab2\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Harnad\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Stevan Harnad. The symbol grounding problem\",\"url\":\"\",\"venue\":\"Physica D: Nonlinear Phenomena\",\"year\":1990},{\"arxivId\":\"1704.06616\",\"authors\":[{\"authorId\":\"34544888\",\"name\":\"Dilip Arumugam\"},{\"authorId\":\"10737060\",\"name\":\"Siddharth Karamcheti\"},{\"authorId\":\"2646714\",\"name\":\"Nakul Gopalan\"},{\"authorId\":\"145307121\",\"name\":\"L. Wong\"},{\"authorId\":\"2913681\",\"name\":\"Stefanie Tellex\"}],\"doi\":\"10.15607/RSS.2017.XIII.056\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e21c7e5565b92079686fe852047f1310d54fac1\",\"title\":\"Accurately and Efficiently Interpreting Human-Robot Instructions of Varying Granularities\",\"url\":\"https://www.semanticscholar.org/paper/2e21c7e5565b92079686fe852047f1310d54fac1\",\"venue\":\"Robotics: Science and Systems\",\"year\":2017},{\"arxivId\":\"1401.5390\",\"authors\":[{\"authorId\":\"1741598\",\"name\":\"S. R. K. Branavan\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":\"10.1613/jair.3484\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d\",\"title\":\"Learning to Win by Reading Manuals in a Monte-Carlo Framework\",\"url\":\"https://www.semanticscholar.org/paper/99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d\",\"venue\":\"ACL\",\"year\":2011},{\"arxivId\":\"1705.10998\",\"authors\":[{\"authorId\":\"30525721\",\"name\":\"V. Kurin\"},{\"authorId\":\"2388416\",\"name\":\"Sebastian Nowozin\"},{\"authorId\":\"145186674\",\"name\":\"Katja Hofmann\"},{\"authorId\":\"39611591\",\"name\":\"Lucas Beyer\"},{\"authorId\":\"1789756\",\"name\":\"B. Leibe\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ab8efc8243b6c374374d407f25d38af41071830\",\"title\":\"The Atari Grand Challenge Dataset\",\"url\":\"https://www.semanticscholar.org/paper/9ab8efc8243b6c374374d407f25d38af41071830\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1806.01946\",\"authors\":[{\"authorId\":\"3335364\",\"name\":\"Dzmitry Bahdanau\"},{\"authorId\":\"145783676\",\"name\":\"Felix Hill\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"37591038\",\"name\":\"Edward Hughes\"},{\"authorId\":\"1410997201\",\"name\":\"S. A. Hosseini\"},{\"authorId\":\"143967473\",\"name\":\"P. Kohli\"},{\"authorId\":\"1864353\",\"name\":\"Edward Grefenstette\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4a4b71ff918ca8eeffa5dfe66be2db7fcc1291da\",\"title\":\"Learning to Understand Goal Specifications by Modelling Reward\",\"url\":\"https://www.semanticscholar.org/paper/4a4b71ff918ca8eeffa5dfe66be2db7fcc1291da\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49433803\",\"name\":\"S. Harnad\"}],\"doi\":\"10.1016/0167-2789(90)90087-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0945b4f7eb330f9469e70341a4160ddc45ba751b\",\"title\":\"The symbol grounding problem\",\"url\":\"https://www.semanticscholar.org/paper/0945b4f7eb330f9469e70341a4160ddc45ba751b\",\"venue\":\"\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1741598\",\"name\":\"S. R. K. Branavan\"},{\"authorId\":\"1684887\",\"name\":\"Nate Kushman\"},{\"authorId\":\"49986267\",\"name\":\"Tao Lei\"},{\"authorId\":\"1741283\",\"name\":\"R. Barzilay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2aa73c3f04536ed9d7c24db7d1090de897dec86c\",\"title\":\"Learning High-Level Planning from Text\",\"url\":\"https://www.semanticscholar.org/paper/2aa73c3f04536ed9d7c24db7d1090de897dec86c\",\"venue\":\"ACL\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"SRK Branavan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"David Silver\",\"url\":\"\",\"venue\":\"and Regina Barzilay. Learning to win by reading manuals in a monte-carlo framework. Journal of Artificial Intelligence Research, 43:661\\u2013704\",\"year\":2012},{\"arxivId\":\"1711.00106\",\"authors\":[{\"authorId\":\"2228109\",\"name\":\"Caiming Xiong\"},{\"authorId\":\"3428769\",\"name\":\"Victor Zhong\"},{\"authorId\":\"2166511\",\"name\":\"R. Socher\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a\",\"title\":\"DCN+: Mixed Objective and Deep Residual Coattention for Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/0342f5bf7f7b8f26ff4380846f9e577ae6fdd88a\",\"venue\":\"ICLR\",\"year\":2018}],\"title\":\"Using Natural Language for Reward Shaping in Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Natural language\",\"topicId\":\"1911\",\"url\":\"https://www.semanticscholar.org/topic/1911\"},{\"topic\":\"Traffic shaping\",\"topicId\":\"61867\",\"url\":\"https://www.semanticscholar.org/topic/61867\"},{\"topic\":\"Noise shaping\",\"topicId\":\"135480\",\"url\":\"https://www.semanticscholar.org/topic/135480\"},{\"topic\":\"Atari\",\"topicId\":\"20108\",\"url\":\"https://www.semanticscholar.org/topic/20108\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Map\",\"topicId\":\"2392\",\"url\":\"https://www.semanticscholar.org/topic/2392\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Command & Conquer:Yuri's Revenge\",\"topicId\":\"3603040\",\"url\":\"https://www.semanticscholar.org/topic/3603040\"}],\"url\":\"https://www.semanticscholar.org/paper/0fa1c75a452a046e11e775eb6120051c696d9366\",\"venue\":\"IJCAI\",\"year\":2019}\n"