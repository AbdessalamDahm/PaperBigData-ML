"{\"abstract\":\"Learning rational behaviors in First-person-shooter (FPS) games is a challenging task for Reinforcement Learning (RL) with the primary difficulties of huge action space and insufficient exploration. To address this, we propose a hierarchical agent based on combined options with intrinsic rewards to drive exploration. Specifically, we present a hierarchical model that works in a manager-worker fashion over two levels of hierarchy. The highlevel manager learns a policy over options, and the low-level workers, motivated by intrinsic reward, learn to execute the options. Performance is further improved with environmental signals appropriately harnessed. Extensive experiments demonstrate that our trained bot significantly outperforms the alternative RL-based models on FPS games requiring maze solving and combat skills, etc. Notably, we achieved first place in VDAIC 2018 Track(1)1.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"48257167\",\"name\":\"Shihong Song\",\"url\":\"https://www.semanticscholar.org/author/48257167\"},{\"authorId\":\"49056493\",\"name\":\"J. Weng\",\"url\":\"https://www.semanticscholar.org/author/49056493\"},{\"authorId\":null,\"name\":\"Hang Su\",\"url\":null},{\"authorId\":\"143848636\",\"name\":\"Dong Yan\",\"url\":\"https://www.semanticscholar.org/author/143848636\"},{\"authorId\":\"19256172\",\"name\":\"Haosheng Zou\",\"url\":\"https://www.semanticscholar.org/author/19256172\"},{\"authorId\":\"145254044\",\"name\":\"Jun Zhu\",\"url\":\"https://www.semanticscholar.org/author/145254044\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2011.02669\",\"authors\":[{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2209431\",\"name\":\"Weixun Wang\"},{\"authorId\":\"50982491\",\"name\":\"Hangtian Jia\"},{\"authorId\":null,\"name\":\"Yixiang Wang\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"1684705122\",\"name\":\"Feng Wu\"},{\"authorId\":\"153645633\",\"name\":\"Changjie Fan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d12ab6fc45f953a4bec640da267a913d7ad5289b\",\"title\":\"Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping\",\"url\":\"https://www.semanticscholar.org/paper/d12ab6fc45f953a4bec640da267a913d7ad5289b\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2001.00127\",\"authors\":[{\"authorId\":\"143613433\",\"name\":\"K. Jiang\"},{\"authorId\":\"12836918\",\"name\":\"X. Qin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3406d877b7b9faed2c791a7d5b869d8f2b927e91\",\"title\":\"Reinforcement Learning with Goal-Distance Gradient\",\"url\":\"https://www.semanticscholar.org/paper/3406d877b7b9faed2c791a7d5b869d8f2b927e91\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199466379,\"doi\":\"10.24963/ijcai.2019/482\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"5678aa198b35b672d5829e48c1d1bb0dec2ac4ad\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Prafulla Dhariwal\"},{\"authorId\":null,\"name\":\"Christopher Hesse\"},{\"authorId\":null,\"name\":\"Oleg Klimov\"},{\"authorId\":null,\"name\":\"Alex Nichol\"},{\"authorId\":null,\"name\":\"Matthias Plappert\"},{\"authorId\":null,\"name\":\"Alec Radford\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Szymon Sidor\"},{\"authorId\":null,\"name\":\"Yuhuai Wu\"},{\"authorId\":null,\"name\":\"Peter Zhokhov\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Openai baselines\",\"url\":\"\",\"venue\":\"https://github.com/openai/baselines,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9aa1d909544fd9ffe061b84a90eb344ac303e6d9\",\"title\":\"The MAXQ Method for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9aa1d909544fd9ffe061b84a90eb344ac303e6d9\",\"venue\":\"ICML\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98264506\",\"name\":\"Yuxin Wu\"},{\"authorId\":\"39402399\",\"name\":\"Yuandong Tian\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"add7b8b65355d5408a1ffb93a94b0ae688806bc4\",\"title\":\"Training Agent for First-Person Shooter Game with Actor-Critic Curriculum Learning\",\"url\":\"https://www.semanticscholar.org/paper/add7b8b65355d5408a1ffb93a94b0ae688806bc4\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1707.03902\",\"authors\":[{\"authorId\":\"22255447\",\"name\":\"Samuel Alvernaz\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/CIG.2017.8080408\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"60710f275494da521426a8c1af952b4451e062e2\",\"title\":\"Autoencoder-augmented neuroevolution for visual doom playing\",\"url\":\"https://www.semanticscholar.org/paper/60710f275494da521426a8c1af952b4451e062e2\",\"venue\":\"2017 IEEE Conference on Computational Intelligence and Games (CIG)\",\"year\":2017},{\"arxivId\":\"1612.00380\",\"authors\":[{\"authorId\":\"47829695\",\"name\":\"Shehroze Bhatti\"},{\"authorId\":\"3050846\",\"name\":\"Alban Desmaison\"},{\"authorId\":\"3336488\",\"name\":\"O. Miksik\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"145809603\",\"name\":\"N. Siddharth\"},{\"authorId\":\"143635540\",\"name\":\"P. Torr\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"16e1f2669b9a9ecb71ea4e0ed581e2c24cb55f79\",\"title\":\"Playing Doom with SLAM-Augmented Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/16e1f2669b9a9ecb71ea4e0ed581e2c24cb55f79\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1609.05140\",\"authors\":[{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15b26d8cb35d7e795c8832fe08794224ee1e9f84\",\"title\":\"The Option-Critic Architecture\",\"url\":\"https://www.semanticscholar.org/paper/15b26d8cb35d7e795c8832fe08794224ee1e9f84\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48878113\",\"name\":\"Michael Cook\"},{\"authorId\":\"1687610\",\"name\":\"S. Colton\"}],\"doi\":\"10.1109/CIG.2011.6032019\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9dfb2100cbaf57b4f52cb2ad83063612385d6b72\",\"title\":\"Multi-faceted evolution of simple arcade games\",\"url\":\"https://www.semanticscholar.org/paper/9dfb2100cbaf57b4f52cb2ad83063612385d6b72\",\"venue\":\"2011 IEEE Conference on Computational Intelligence and Games (CIG'11)\",\"year\":2011},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2788230\",\"name\":\"N. V. Hoorn\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"},{\"authorId\":\"145341374\",\"name\":\"J. Schmidhuber\"}],\"doi\":\"10.1109/CIG.2009.5286463\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8ce74809b58692c1582f3bbf86d7a37288cfa0fc\",\"title\":\"Hierarchical controller learning in a First-Person Shooter\",\"url\":\"https://www.semanticscholar.org/paper/8ce74809b58692c1582f3bbf86d7a37288cfa0fc\",\"venue\":\"2009 IEEE Symposium on Computational Intelligence and Games\",\"year\":2009},{\"arxivId\":\"1701.07274\",\"authors\":[{\"authorId\":\"2276894\",\"name\":\"Yuxi Li\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"9f1e9e56d80146766bc2316efbc54d8b770a23df\",\"title\":\"Deep Reinforcement Learning: An Overview\",\"url\":\"https://www.semanticscholar.org/paper/9f1e9e56d80146766bc2316efbc54d8b770a23df\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1606.02396\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"3422751\",\"name\":\"Simanta Gautam\"},{\"authorId\":\"1831199\",\"name\":\"S. Gershman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"10a4992ece5baea79326a8878a6244eeacbc6af5\",\"title\":\"Deep Successor Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/10a4992ece5baea79326a8878a6244eeacbc6af5\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1609.05521\",\"authors\":[{\"authorId\":\"1830914\",\"name\":\"Guillaume Lample\"},{\"authorId\":\"2328602\",\"name\":\"Devendra Singh Chaplot\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e0b65d3839e3bf703d156b524d7db7a5e10a2623\",\"title\":\"Playing FPS Games with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e0b65d3839e3bf703d156b524d7db7a5e10a2623\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1804.02767\",\"authors\":[{\"authorId\":\"40497777\",\"name\":\"Joseph Redmon\"},{\"authorId\":\"143787583\",\"name\":\"Ali Farhadi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e4845fb1e624965d4f036d7fd32e8dcdd2408148\",\"title\":\"YOLOv3: An Incremental Improvement\",\"url\":\"https://www.semanticscholar.org/paper/e4845fb1e624965d4f036d7fd32e8dcdd2408148\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Prafulla Dhariwal\"},{\"authorId\":null,\"name\":\"Christopher Hesse\"},{\"authorId\":null,\"name\":\"Oleg Klimov\"},{\"authorId\":null,\"name\":\"Alex Nichol\"},{\"authorId\":null,\"name\":\"Matthias Plappert\"},{\"authorId\":null,\"name\":\"Alec Radford\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Szymon Sidor\"},{\"authorId\":null,\"name\":\"Yuhuai Wu\"},{\"authorId\":null,\"name\":\"Peter Zhokhov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Openai baselines\",\"url\":\"\",\"venue\":\"https://github.com/openai/baselines,\",\"year\":2017},{\"arxivId\":\"1605.02097\",\"authors\":[{\"authorId\":\"3407787\",\"name\":\"Michal Kempka\"},{\"authorId\":\"3407043\",\"name\":\"Marek Wydmuch\"},{\"authorId\":\"3407668\",\"name\":\"Grzegorz Runc\"},{\"authorId\":\"3407634\",\"name\":\"Jakub Toczek\"},{\"authorId\":\"2146303\",\"name\":\"Wojciech Ja\\u015bkowski\"}],\"doi\":\"10.1109/CIG.2016.7860433\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a473f545318325ba23b7a6b477485d29777ba873\",\"title\":\"ViZDoom: A Doom-based AI research platform for visual reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/a473f545318325ba23b7a6b477485d29777ba873\",\"venue\":\"2016 IEEE Conference on Computational Intelligence and Games (CIG)\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrey Kolishchak\"}],\"doi\":null,\"intent\":[\"background\",\"result\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Doomnet\",\"url\":\"\",\"venue\":\"https:// github.com/akolishchak/doom-net-pytorch,\",\"year\":2018},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":\"1705.05363\",\"authors\":[{\"authorId\":\"38236002\",\"name\":\"Deepak Pathak\"},{\"authorId\":\"33932184\",\"name\":\"Pulkit Agrawal\"},{\"authorId\":\"1763086\",\"name\":\"Alexei A. Efros\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"}],\"doi\":\"10.1109/CVPRW.2017.70\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"225ab689f41cef1dc18237ef5dab059a49950abf\",\"title\":\"Curiosity-Driven Exploration by Self-Supervised Prediction\",\"url\":\"https://www.semanticscholar.org/paper/225ab689f41cef1dc18237ef5dab059a49950abf\",\"venue\":\"2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)\",\"year\":2017},{\"arxivId\":\"1611.01779\",\"authors\":[{\"authorId\":\"2841331\",\"name\":\"A. Dosovitskiy\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c25f50c7451fa72c562e21e3b11e416b11f74c8\",\"title\":\"Learning to Act by Predicting the Future\",\"url\":\"https://www.semanticscholar.org/paper/4c25f50c7451fa72c562e21e3b11e416b11f74c8\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1790646\",\"name\":\"P. Dayan\"},{\"authorId\":\"1695689\",\"name\":\"Geoffrey E. Hinton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1678bd32846b1aded5b1e80a617170812e80f562\",\"title\":\"Feudal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1678bd32846b1aded5b1e80a617170812e80f562\",\"venue\":\"NIPS\",\"year\":1992},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9932998\",\"name\":\"Shiyu Huang\"},{\"authorId\":\"144904238\",\"name\":\"H. Su\"},{\"authorId\":\"145254044\",\"name\":\"Jun Zhu\"},{\"authorId\":\"145358507\",\"name\":\"Ting Chen\"}],\"doi\":\"10.1609/AAAI.V33I01.3301954\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"67bb04ca3bb249d6e3daa01b050eb656863c2422\",\"title\":\"Combo-Action: Training Agent For FPS Game with Auxiliary Tasks\",\"url\":\"https://www.semanticscholar.org/paper/67bb04ca3bb249d6e3daa01b050eb656863c2422\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":\"1703.01161\",\"authors\":[{\"authorId\":\"9948791\",\"name\":\"A. S. Vezhnevets\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"049c6e5736313374c6e594c34b9be89a3a09dced\",\"title\":\"FeUdal Networks for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/049c6e5736313374c6e594c34b9be89a3a09dced\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Dong Yan\"},{\"authorId\":null,\"name\":\"Shiyu Huang\"},{\"authorId\":null,\"name\":\"Hang Su\"},{\"authorId\":null,\"name\":\"Jun Zhu\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b7bf66821aa6a0afb7d27f6079a4e8fdd04fd8bb\",\"title\":\"Learning to Assign Credit in Reinforcement Learning by Incorporating Abstract Relations\",\"url\":\"https://www.semanticscholar.org/paper/b7bf66821aa6a0afb7d27f6079a4e8fdd04fd8bb\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3414570\",\"name\":\"N. Mishra\"},{\"authorId\":\"22222033\",\"name\":\"Mostafa Rohaninejad\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7e9c1e0d247b20a0683f4797d9ea248c3b53d424\",\"title\":\"A Simple Neural Attentive Meta-Learner\",\"url\":\"https://www.semanticscholar.org/paper/7e9c1e0d247b20a0683f4797d9ea248c3b53d424\",\"venue\":\"ICLR\",\"year\":2018}],\"title\":\"Playing FPS Games With Environment-Aware Hierarchical Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Floating Point Systems\",\"topicId\":\"1988580\",\"url\":\"https://www.semanticscholar.org/topic/1988580\"}],\"url\":\"https://www.semanticscholar.org/paper/5678aa198b35b672d5829e48c1d1bb0dec2ac4ad\",\"venue\":\"IJCAI\",\"year\":2019}\n"