"{\"abstract\":\"Deep Q-Network (DQN) is an algorithm that achieves human-level performance in complex domains like Atari games. One of the important elements of DQN is its use of a target network, which is necessary to stabilize learning. We argue that using a target network is incompatible with online reinforcement learning, and it is possible to achieve faster and more stable learning without a target network when we use Mellowmax, an alternative softmax operator. We derive novel properties of Mellowmax, and empirically show that the combination of DQN and Mellowmax, but without a target network, outperforms DQN with a target network.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"48388776\",\"name\":\"Seungchan Kim\",\"url\":\"https://www.semanticscholar.org/author/48388776\"},{\"authorId\":\"7981071\",\"name\":\"Kavosh Asadi\",\"url\":\"https://www.semanticscholar.org/author/7981071\"},{\"authorId\":\"13958114\",\"name\":\"M. Littman\",\"url\":\"https://www.semanticscholar.org/author/13958114\"},{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\",\"url\":\"https://www.semanticscholar.org/author/1765407\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"48388776\",\"name\":\"Seungchan Kim\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"bd933323dacfaa0e243bd28d81eca28f253688fe\",\"title\":\"Adaptive Temperature Tuning for Mellowmax in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/bd933323dacfaa0e243bd28d81eca28f253688fe\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2012.09456\",\"authors\":[{\"authorId\":\"15042674\",\"name\":\"Yaozhong Gan\"},{\"authorId\":\"1812651\",\"name\":\"Zhengqian Zhang\"},{\"authorId\":\"2248421\",\"name\":\"Xiaoyang Tan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bf591da059e692cb5c2f1d6130e22fcb2fde6f54\",\"title\":\"Stabilizing Q Learning Via Soft Mellowmax Operator\",\"url\":\"https://www.semanticscholar.org/paper/bf591da059e692cb5c2f1d6130e22fcb2fde6f54\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"71231791\",\"name\":\"S. Ohnishi\"},{\"authorId\":\"1773761\",\"name\":\"E. Uchibe\"},{\"authorId\":\"1452988350\",\"name\":\"Yotaro Yamaguchi\"},{\"authorId\":\"51516944\",\"name\":\"Kosuke Nakanishi\"},{\"authorId\":\"2048202\",\"name\":\"Yuji Yasui\"},{\"authorId\":\"145516720\",\"name\":\"S. Ishii\"}],\"doi\":\"10.3389/fnbot.2019.00103\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f1ba4d23cbd05b0e62171a95965d1cc0d2a30dca\",\"title\":\"Constrained Deep Q-Learning Gradually Approaching Ordinary Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/f1ba4d23cbd05b0e62171a95965d1cc0d2a30dca\",\"venue\":\"Front. Neurorobot.\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1740844783\",\"name\":\"Xiaodong Nian\"},{\"authorId\":\"2897833\",\"name\":\"Athirai Aravazhi Irissappane\"},{\"authorId\":\"1917202\",\"name\":\"Diederik M. Roijers\"}],\"doi\":\"10.5555/3398761.3398870\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f25a69a981384a71f16d3312784e80a3a6369538\",\"title\":\"DCRAC: Deep Conditioned Recurrent Actor-Critic for Multi-Objective Partially Observable Environments\",\"url\":\"https://www.semanticscholar.org/paper/f25a69a981384a71f16d3312784e80a3a6369538\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"1911.09615\",\"authors\":[{\"authorId\":\"1422034120\",\"name\":\"Marta Sarrico\"},{\"authorId\":\"68972911\",\"name\":\"Kai Arulkumaran\"},{\"authorId\":\"52108401\",\"name\":\"A. Agostinelli\"},{\"authorId\":\"16326904\",\"name\":\"Pierre H. Richemond\"},{\"authorId\":\"2815535\",\"name\":\"A. Bharath\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d8bd56621f695b997a036553913368fee7b9ca57\",\"title\":\"Sample-Efficient Reinforcement Learning with Maximum Entropy Mellowmax Episodic Control\",\"url\":\"https://www.semanticscholar.org/paper/d8bd56621f695b997a036553913368fee7b9ca57\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"988e11eb5c5cd0f524acda9880092a3425907d08\",\"title\":\"Self-Guided and Self-Regularized Actor-Critic\",\"url\":\"https://www.semanticscholar.org/paper/988e11eb5c5cd0f524acda9880092a3425907d08\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.01870\",\"authors\":[{\"authorId\":\"2026957819\",\"name\":\"Weiming Liu\"},{\"authorId\":\"2025994844\",\"name\":\"Bin Li\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"281bfa43b956fc6a6ca1aaf7510d73ba5a364d77\",\"title\":\"Model-free Neural Counterfactual Regret Minimization with Bootstrap Learning\",\"url\":\"https://www.semanticscholar.org/paper/281bfa43b956fc6a6ca1aaf7510d73ba5a364d77\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.08068\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"1582220935\",\"name\":\"Kirby Banman\"},{\"authorId\":\"114860989\",\"name\":\"M. White\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ba61b36c3534297129a3c55c71ad96e2acf223f2\",\"title\":\"Leaky Tiling Activations: A Simple Approach to Learning Sparse Representations Online\",\"url\":\"https://www.semanticscholar.org/paper/ba61b36c3534297129a3c55c71ad96e2acf223f2\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48388776\",\"name\":\"Seungchan Kim\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"555fb742d67468328913a3d259ae22c3381e5fce\",\"title\":\"Adaptive Tuning of Temperature in Mellowmax using Meta-Gradients\",\"url\":\"https://www.semanticscholar.org/paper/555fb742d67468328913a3d259ae22c3381e5fce\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2009.08973\",\"authors\":[{\"authorId\":\"143901333\",\"name\":\"Lin Shao\"},{\"authorId\":\"151266759\",\"name\":\"Yifan You\"},{\"authorId\":\"3235234\",\"name\":\"Mengyuan Yan\"},{\"authorId\":\"3422968\",\"name\":\"Q. Sun\"},{\"authorId\":\"1775407\",\"name\":\"Jeannette Bohg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"75a8ce445889ff8130014bcea7705cd55e596a40\",\"title\":\"GRAC: Self-Guided and Self-Regularized Actor-Critic\",\"url\":\"https://www.semanticscholar.org/paper/75a8ce445889ff8130014bcea7705cd55e596a40\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"103e9dab81291a825d4a4c7c6027536af42cd607\",\"title\":\"Deep Tile Coder: an Efficient Sparse Representation Learning Approach with applications in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/103e9dab81291a825d4a4c7c6027536af42cd607\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2008.10861\",\"authors\":[{\"authorId\":\"1573300451\",\"name\":\"Taisuke Kobayashi\"},{\"authorId\":\"1515577860\",\"name\":\"Wendyam Eric Lionel Ilboudo\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5172ce1ca89d007c560389d0ea2f4756911509bb\",\"title\":\"t-Soft Update of Target Network for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5172ce1ca89d007c560389d0ea2f4756911509bb\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.01100\",\"authors\":[{\"authorId\":\"1659982436\",\"name\":\"Rong Zhu\"},{\"authorId\":\"2535094\",\"name\":\"Mattia Rigotti\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cf73d19bdb6d78929678faac4badfb6b5341b6f3\",\"title\":\"Self-correcting Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/cf73d19bdb6d78929678faac4badfb6b5341b6f3\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":197638361,\"doi\":\"10.24963/ijcai.2019/379\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":3,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"5a7326adb18303cb05500330ddc8a0bb1cf2d5c2\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"144445940\",\"name\":\"B. Dai\"},{\"authorId\":\"47291134\",\"name\":\"Albert Eaton Shaw\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"145942106\",\"name\":\"Lin Xiao\"},{\"authorId\":\"2903347\",\"name\":\"Niao He\"},{\"authorId\":\"46270580\",\"name\":\"Z. Liu\"},{\"authorId\":\"1720246\",\"name\":\"Jianshu Chen\"},{\"authorId\":\"1779453\",\"name\":\"L. Song\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1bd66197d692581f4ac507107184ca9110b61d7d\",\"title\":\"SBEED: Convergent Reinforcement Learning with Nonlinear Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/1bd66197d692581f4ac507107184ca9110b61d7d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39698837\",\"name\":\"D. Stahl\"},{\"authorId\":\"145839708\",\"name\":\"P. Wilson\"}],\"doi\":\"10.1016/0167-2681(94)90103-1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"90db3c531308d8599bba9eec81757f5652d64b13\",\"title\":\"Experimental evidence on players' models of other players\",\"url\":\"https://www.semanticscholar.org/paper/90db3c531308d8599bba9eec81757f5652d64b13\",\"venue\":\"\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aysel Safak. Statistical Analysis of the Power Sum of Mu Components\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"IEEE Transactions on Vehicular Technology\",\"url\":\"\",\"venue\":\"42(1):58\\u201361,\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sutton Seijen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Harm van Seijen and Rich Sutton\",\"url\":\"\",\"venue\":\"Proceedings of the International Conference on Machine Learning\",\"year\":2014},{\"arxivId\":\"1705.07798\",\"authors\":[{\"authorId\":\"1741549\",\"name\":\"Gergely Neu\"},{\"authorId\":\"143808510\",\"name\":\"A. Jonsson\"},{\"authorId\":\"145810673\",\"name\":\"V. G\\u00f3mez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e7d1e21409a90e66106722506aeb434ee7a18f3\",\"title\":\"A unified view of entropy-regularized Markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/2e7d1e21409a90e66106722506aeb434ee7a18f3\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3466704\",\"name\":\"Gavin Adrian Rummery\"},{\"authorId\":\"1697360\",\"name\":\"M. Niranjan\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a09464f26e18a25a948baaa736270bfb84b5e12\",\"title\":\"On-line Q-learning using connectionist systems\",\"url\":\"https://www.semanticscholar.org/paper/7a09464f26e18a25a948baaa736270bfb84b5e12\",\"venue\":\"\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"1766844\",\"name\":\"Eric Wiewiora\"},{\"authorId\":\"152677062\",\"name\":\"J. Langford\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1145/1143844.1143955\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"187f3f984e6f375178f41827ab90c4e748773fa7\",\"title\":\"PAC model-free reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/187f3f984e6f375178f41827ab90c4e748773fa7\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":\"1512.08562\",\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"3314041\",\"name\":\"Ari Pakman\"},{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a026fd65af4ba3575e64174de56fee093fa3330\",\"title\":\"Taming the Noise in Reinforcement Learning via Soft Updates\",\"url\":\"https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1843103\",\"name\":\"Stephen P. Boyd\"},{\"authorId\":\"2014414\",\"name\":\"L. Vandenberghe\"}],\"doi\":\"10.1017/CBO9780511804441\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4f607f03272e4d62708f5b2441355f9e005cb452\",\"title\":\"Convex Optimization\",\"url\":\"https://www.semanticscholar.org/paper/4f607f03272e4d62708f5b2441355f9e005cb452\",\"venue\":\"IEEE Transactions on Automatic Control\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1703952\",\"name\":\"Zhanxing Zhu\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f95c08125dedd20e292f3eb1052b5a3aaba7d845\",\"title\":\"Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/f95c08125dedd20e292f3eb1052b5a3aaba7d845\",\"venue\":\"AAAI 2016\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21456301\",\"name\":\"R. Bellman\"}],\"doi\":\"10.1512/IUMJ.1957.6.56038\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bff20fb30adad8d1c173963089df5fc9664304f0\",\"title\":\"A Markovian Decision Process\",\"url\":\"https://www.semanticscholar.org/paper/bff20fb30adad8d1c173963089df5fc9664304f0\",\"venue\":\"\",\"year\":1957},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hasselt\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hado van Hasselt. Double Q-learning\",\"url\":\"\",\"venue\":\"Advances in Neural Information Processing Systems 23\",\"year\":2010},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":\"1511.06581\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"title\":\"Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47217343\",\"name\":\"A. \\u015eafak\"}],\"doi\":\"10.1109/25.192387\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b393038356f62e6ba8a6d8ee7f07d09b769798f\",\"title\":\"Statistical analysis of the power sum of multiple correlated log-normal components\",\"url\":\"https://www.semanticscholar.org/paper/3b393038356f62e6ba8a6d8ee7f07d09b769798f\",\"venue\":\"\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Bellman. A Markovian Decision Process\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Journal of Mathematics and Mechanics\",\"url\":\"\",\"venue\":\"6(5),\",\"year\":1957},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Dearden\"},{\"authorId\":null,\"name\":\"Nir Friedman\"},{\"authorId\":null,\"name\":\"Stuart Russell. Bayesian Q-learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI/IAAI\",\"url\":\"\",\"venue\":\"pages 761\\u2013768,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50056360\",\"name\":\"William W. Cohen\"},{\"authorId\":\"100655694\",\"name\":\"A. Moore\"}],\"doi\":\"10.1145/1143844\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"title\":\"Proceedings of the 23rd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/c4607387ee863d5c5e5dc9f8adfbe7930508e286\",\"venue\":\"ICML 2008\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"},{\"authorId\":\"153059475\",\"name\":\"S. Becker\"},{\"authorId\":\"1405497839\",\"name\":\"Z. G. Eds\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"89558a43b3b0a24cd0fdb6d5c2283112493af3a0\",\"title\":\"In Advances in Neural Information Processing Systems 15\",\"url\":\"https://www.semanticscholar.org/paper/89558a43b3b0a24cd0fdb6d5c2283112493af3a0\",\"venue\":\"NIPS 1991\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matteo Hessel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Hado van Hasselt , Marc Lanctot , and Nando de Freitas . Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"\",\"venue\":\"Proceedings of the International Conference on Machine Learning\",\"year\":null},{\"arxivId\":\"1702.08892\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"title\":\"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"}],\"doi\":\"10.1145/1273496\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"title\":\"Proceedings of the 24th international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/e4e220c78c6f6f8ee18a133f1c81b26df3b6e149\",\"venue\":\"ICML 2007\",\"year\":2007},{\"arxivId\":\"1704.00805\",\"authors\":[{\"authorId\":\"23296189\",\"name\":\"B. Gao\"},{\"authorId\":\"145274326\",\"name\":\"L. Pavel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"59af974724b9e4af90dafa1695e76c81784567be\",\"title\":\"On the Properties of the Softmax Function with Application in Game Theory and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/59af974724b9e4af90dafa1695e76c81784567be\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"327950ac8d59054c4b4a37f901ac9bdbb8168087\",\"title\":\"A Generalized Reinforcement-Learning Model: Convergence and Applications\",\"url\":\"https://www.semanticscholar.org/paper/327950ac8d59054c4b4a37f901ac9bdbb8168087\",\"venue\":\"ICML\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2184331\",\"name\":\"R. Rosenfeld\"}],\"doi\":\"10.1016/j.otohns.2009.05.016\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"794c364f77621bee08b7985c51ef5c9169f4442c\",\"title\":\"Nature\",\"url\":\"https://www.semanticscholar.org/paper/794c364f77621bee08b7985c51ef5c9169f4442c\",\"venue\":\"Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1748153\",\"name\":\"H. V. Seijen\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f9979d1d2e4e47d58df5e4ce3aef84a46e997352\",\"title\":\"True Online TD(lambda)\",\"url\":\"https://www.semanticscholar.org/paper/f9979d1d2e4e47d58df5e4ce3aef84a46e997352\",\"venue\":\"ICML\",\"year\":2014}],\"title\":\"DeepMellow: Removing the Need for a Target Network in Deep Q-Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Atari\",\"topicId\":\"20108\",\"url\":\"https://www.semanticscholar.org/topic/20108\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Deep learning\",\"topicId\":\"2762\",\"url\":\"https://www.semanticscholar.org/topic/2762\"},{\"topic\":\"Open research\",\"topicId\":\"1298\",\"url\":\"https://www.semanticscholar.org/topic/1298\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Instability\",\"topicId\":\"4779\",\"url\":\"https://www.semanticscholar.org/topic/4779\"},{\"topic\":\"Softmax function\",\"topicId\":\"966784\",\"url\":\"https://www.semanticscholar.org/topic/966784\"}],\"url\":\"https://www.semanticscholar.org/paper/5a7326adb18303cb05500330ddc8a0bb1cf2d5c2\",\"venue\":\"IJCAI\",\"year\":2019}\n"