"{\"abstract\":\"Maximum entropy deep reinforcement learning (RL) methods have been demonstrated on a range of challenging continuous tasks. However, existing methods either suffer from severe instability when training on large off-policy data or cannot scale to tasks with very high state and action dimensionality such as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann policy set for non-optimal soft value function is not persuasive enough. In this paper, we first derive soft policy gradient based on entropy regularized expected reward objective for RL with continuous actions. Then, we present an off-policy actor-critic, model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG) by combining soft policy gradient with soft Bellman equation. To ensure stable learning while eliminating the need of two separate critics for soft value functions, we leverage double sampling approach to making the soft Bellman equation tractable. The experimental results demonstrate that our method outperforms in performance over off-policy prior methods.\",\"arxivId\":\"1909.03198\",\"authors\":[{\"authorId\":\"145033252\",\"name\":\"Wenjie Shi\",\"url\":\"https://www.semanticscholar.org/author/145033252\"},{\"authorId\":\"1760750\",\"name\":\"Shiji Song\",\"url\":\"https://www.semanticscholar.org/author/1760750\"},{\"authorId\":\"145253556\",\"name\":\"C. Wu\",\"url\":\"https://www.semanticscholar.org/author/145253556\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1811414728\",\"name\":\"Haoqiang Chen\"},{\"authorId\":\"7894799\",\"name\":\"Y. Liu\"},{\"authorId\":\"1811484589\",\"name\":\"Zongtan Zhou\"},{\"authorId\":\"70213632\",\"name\":\"D. Hu\"},{\"authorId\":\"145269712\",\"name\":\"M. Zhang\"}],\"doi\":\"10.1007/s10489-020-01755-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a52dea8e998c8f1ba4efa72490ac5f6af374c11b\",\"title\":\"GAMA: Graph Attention Multi-agent reinforcement learning algorithm for cooperation\",\"url\":\"https://www.semanticscholar.org/paper/a52dea8e998c8f1ba4efa72490ac5f6af374c11b\",\"venue\":\"Applied Intelligence\",\"year\":2020},{\"arxivId\":\"2011.04118\",\"authors\":[{\"authorId\":\"1403851982\",\"name\":\"Pamela Carreno-Medrano\"},{\"authorId\":\"32501423\",\"name\":\"S. Smith\"},{\"authorId\":\"1961423991\",\"name\":\"Dana Kuli\\u0107\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a6705f95c7fb9a2ccff7f9af70d5f1f96bf65ec8\",\"title\":\"Joint Estimation of Expertise and Reward Preferences From Human Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/a6705f95c7fb9a2ccff7f9af70d5f1f96bf65ec8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1909.03245\",\"authors\":[{\"authorId\":\"145033252\",\"name\":\"Wenjie Shi\"},{\"authorId\":\"1760750\",\"name\":\"Shiji Song\"},{\"authorId\":\"49498683\",\"name\":\"Hui Wu\"},{\"authorId\":\"50040990\",\"name\":\"Ya-Chu Hsu\"},{\"authorId\":\"49763108\",\"name\":\"Cheng Wu\"},{\"authorId\":\"143983679\",\"name\":\"Gao Huang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5fa138429015335d1ec648994082cecda4e0ad25\",\"title\":\"Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5fa138429015335d1ec648994082cecda4e0ad25\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2005.08844\",\"authors\":[{\"authorId\":\"144058368\",\"name\":\"Dong-Hoon Lee\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"450694857fc644e674dc05d3f3b207b3ff590ef7\",\"title\":\"Entropy-Augmented Entropy-Regularized Reinforcement Learning and a Continuous Path from Policy Gradient to Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/450694857fc644e674dc05d3f3b207b3ff590ef7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.07069\",\"authors\":[{\"authorId\":\"145033252\",\"name\":\"Wenjie Shi\"},{\"authorId\":\"143983679\",\"name\":\"Gao Huang\"},{\"authorId\":\"1760750\",\"name\":\"Shiji Song\"},{\"authorId\":\"9314027\",\"name\":\"Zhuo-yuan Wang\"},{\"authorId\":\"51486843\",\"name\":\"Tingyu Lin\"},{\"authorId\":\"145253556\",\"name\":\"C. Wu\"}],\"doi\":\"10.1109/TPAMI.2020.3037898\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"76b3c3f29f4efc510c4c9d871dd7e05dd262cb7f\",\"title\":\"Self-Supervised Discovering of Interpretable Features for Reinforcement Learning.\",\"url\":\"https://www.semanticscholar.org/paper/76b3c3f29f4efc510c4c9d871dd7e05dd262cb7f\",\"venue\":\"IEEE transactions on pattern analysis and machine intelligence\",\"year\":2020},{\"arxivId\":\"2006.09646\",\"authors\":[{\"authorId\":\"19236104\",\"name\":\"A. Srivastava\"},{\"authorId\":\"1770735\",\"name\":\"Srinivasa M. Salapaka\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2a766630cbe5c1469f21b7bf2fef06eab8602afc\",\"title\":\"Parameterized MDPs and Reinforcement Learning Problems - A Maximum Entropy Principle Based Framework\",\"url\":\"https://www.semanticscholar.org/paper/2a766630cbe5c1469f21b7bf2fef06eab8602afc\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46365371\",\"name\":\"Junta Wu\"},{\"authorId\":\"3312590\",\"name\":\"Huiyun Li\"}],\"doi\":\"10.1155/2020/4275623\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dcc02065f3f51a6bc4117adc431801e3be8a2362\",\"title\":\"Deep Ensemble Reinforcement Learning with Multiple Deep Deterministic Policy Gradient Algorithm\",\"url\":\"https://www.semanticscholar.org/paper/dcc02065f3f51a6bc4117adc431801e3be8a2362\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2011.00583\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2000281109\",\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"title\":\"An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199466294,\"doi\":\"10.24963/ijcai.2019/475\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"30a136e860f105604eba58dc880b9253c5f3202c\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492009633\",\"name\":\"Patrick J. Roa\"}],\"doi\":\"10.1023/A:1017153816538\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e6d789ee714d29c9b5156ba9d61b2170d7a315f\",\"title\":\"Volume 8\",\"url\":\"https://www.semanticscholar.org/paper/8e6d789ee714d29c9b5156ba9d61b2170d7a315f\",\"venue\":\"\",\"year\":1998},{\"arxivId\":\"1709.06560\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33690ff21ef1efb576410e656f2e60c89d0307d6\",\"title\":\"Deep Reinforcement Learning that Matters\",\"url\":\"https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A Rusu\"},{\"authorId\":null,\"name\":\"Joel Veness\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"End - to - end training of deep visuomotor policies\",\"url\":\"\",\"venue\":\"Journal of Machine Learning Research\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389654226\",\"name\":\"Brendan O'Donoghue\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5592edb72f1b0dea936ff8baa5ce81fd136b6f59\",\"title\":\"Combining policy gradient and Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/5592edb72f1b0dea936ff8baa5ce81fd136b6f59\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"title\":\"Deterministic Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/687d0e59d5c35f022ce4638b3e3a6142068efc94\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"},{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"144021446\",\"name\":\"Anind K. Dey\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"11b6bdfe36c48b11367b27187da11d95892f0361\",\"title\":\"Maximum Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/11b6bdfe36c48b11367b27187da11d95892f0361\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":\"1504.00702\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"46881670\",\"name\":\"Chelsea Finn\"},{\"authorId\":\"1753210\",\"name\":\"Trevor Darrell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b6b8a1b80891c96c28cc6340267b58186157e536\",\"title\":\"End-to-End Training of Deep Visuomotor Policies\",\"url\":\"https://www.semanticscholar.org/paper/b6b8a1b80891c96c28cc6340267b58186157e536\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143683893\",\"name\":\"S. Bhatnagar\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"144975865\",\"name\":\"Mark Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1a74fef3639f99a67fb90460091b05f0916dd054\",\"title\":\"Incremental Natural Actor-Critic Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/1a74fef3639f99a67fb90460091b05f0916dd054\",\"venue\":\"NIPS\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786249\",\"name\":\"D. Bertsekas\"},{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":\"10.1007/978-0-387-30164-8_588\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"81b2a749e6f1e21809b3c7cdb12cc33037e66055\",\"title\":\"Neuro-Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/81b2a749e6f1e21809b3c7cdb12cc33037e66055\",\"venue\":\"Encyclopedia of Machine Learning\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Satinder P Singh\"},{\"authorId\":null,\"name\":\"Tommi Jaakkola\"},{\"authorId\":null,\"name\":\"Michael I Jordan. Learning without state-estimation in par processes\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Machine Learning Proceedings 1994\",\"url\":\"\",\"venue\":\"pages 284\\u2013292. Elsevier,\",\"year\":1994},{\"arxivId\":\"1512.08562\",\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"3314041\",\"name\":\"Ari Pakman\"},{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4a026fd65af4ba3575e64174de56fee093fa3330\",\"title\":\"Taming the Noise in Reinforcement Learning via Soft Updates\",\"url\":\"https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1611.01626\",\"authors\":[{\"authorId\":\"1389654226\",\"name\":\"Brendan O'Donoghue\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c40dd8f235aabe6efbb93c59c0536adf491f9ead\",\"title\":\"PGQ: Combining policy gradient and Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/c40dd8f235aabe6efbb93c59c0536adf491f9ead\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Emanuel Todorov. General duality between optimal control\"},{\"authorId\":null,\"name\":\"estimation. In Decision\"},{\"authorId\":null,\"name\":\"Control\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"2008\",\"url\":\"\",\"venue\":\"CDC 2008. 47th IEEE Conference on, pages 4286\\u2013 4292. IEEE,\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Toussaint. Robot trajectory optimization using app inference\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 26th annual international conference on machine learning\",\"url\":\"\",\"venue\":\"pages 1049\\u20131056. ACM,\",\"year\":2009},{\"arxivId\":\"1506.02632\",\"authors\":[{\"authorId\":\"1401932095\",\"name\":\"A. PrashanthL.\"},{\"authorId\":\"144898849\",\"name\":\"Cheng Jie\"},{\"authorId\":\"1716042\",\"name\":\"M. Fu\"},{\"authorId\":\"33758038\",\"name\":\"S. Marcus\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1c36a38f9cd2f257cea352ff98d815c0060f1bb0\",\"title\":\"Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and Control\",\"url\":\"https://www.semanticscholar.org/paper/1c36a38f9cd2f257cea352ff98d815c0060f1bb0\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1707.01891\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"bc8eb0c66f8977d3b23dedb247607a8af2360859\",\"title\":\"Trust-PCL: An Off-Policy Trust Region Method for Continuous Control\",\"url\":\"https://www.semanticscholar.org/paper/bc8eb0c66f8977d3b23dedb247607a8af2360859\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Chelsea Finn\"},{\"authorId\":null,\"name\":\"Trevor Darrell\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. End-to-end training of deep visuomotor policies\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):1334\\u20131373,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"35132120\",\"name\":\"T. Jaakkola\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"}],\"doi\":\"10.1016/b978-1-55860-335-6.50042-8\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a579d06ac278e14948f67748cd651e4eb617ae4e\",\"title\":\"Learning Without State-Estimation in Partially Observable Markovian Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/a579d06ac278e14948f67748cd651e4eb617ae4e\",\"venue\":\"ICML\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144981762\",\"name\":\"D. Wang\"},{\"authorId\":\"72549128\",\"name\":\"G. B. Yang\"},{\"authorId\":\"2267865\",\"name\":\"M. Donath\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"68f21296c9ae0cd1660a4e469dec5db35b6c8920\",\"title\":\"American Control Conference\",\"url\":\"https://www.semanticscholar.org/paper/68f21296c9ae0cd1660a4e469dec5db35b6c8920\",\"venue\":\"\",\"year\":1993},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas Degris\"},{\"authorId\":null,\"name\":\"Patrick M Pilarski\"},{\"authorId\":null,\"name\":\"Richard S Sutton. Model-free reinforcement learning with c practice\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In American Control Conference (ACC)\",\"url\":\"\",\"venue\":\"2012, pages 2177\\u20132182. IEEE,\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"LA Prashanth\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Cheng Jie\",\"url\":\"\",\"venue\":\"Michael Fu, Steve Marcus, and Csaba Szepesv\\u00e1ri. Cumulative prospect theory meets reinforcement learning: Prediction and control. In International Conference on Machine Learning, pages 1406\\u20131415\",\"year\":2016},{\"arxivId\":\"1702.08892\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"title\":\"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2278681\",\"name\":\"S. Shortreed\"},{\"authorId\":\"32734155\",\"name\":\"Eric B. Laber\"},{\"authorId\":\"1690162\",\"name\":\"D. Lizotte\"},{\"authorId\":\"6456546\",\"name\":\"T. Stroup\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144180010\",\"name\":\"S. Murphy\"}],\"doi\":\"10.1007/s10994-010-5229-0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8acc06ebabba26b831787c08cba3c9ab7caee850\",\"title\":\"Informing sequential clinical decision-making through\\u00a0reinforcement learning: an empirical study\",\"url\":\"https://www.semanticscholar.org/paper/8acc06ebabba26b831787c08cba3c9ab7caee850\",\"venue\":\"Machine Learning\",\"year\":2010},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1702.08165\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"title\":\"Reinforcement Learning with Deep Energy-Based Policies\",\"url\":\"https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1109/ACC.2012.6315022\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa\",\"title\":\"Model-Free reinforcement learning with continuous action in practice\",\"url\":\"https://www.semanticscholar.org/paper/a75fe5ebd7bc2f0271b8a0742b70f29bbc4be3fa\",\"venue\":\"2012 American Control Conference (ACC)\",\"year\":2012},{\"arxivId\":\"1611.02247\",\"authors\":[{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1744700\",\"name\":\"Zoubin Ghahramani\"},{\"authorId\":\"145369890\",\"name\":\"R. Turner\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.17863/CAM.21294\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"524513b6f4ddca331c33bcc70a9f677fa240cfa3\",\"title\":\"Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic\",\"url\":\"https://www.semanticscholar.org/paper/524513b6f4ddca331c33bcc70a9f677fa240cfa3\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1704.06440\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"d0352057e2b99f65f8b5244a0b912026c86d7b21\",\"title\":\"Equivalence Between Policy Gradients and Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/d0352057e2b99f65f8b5244a0b912026c86d7b21\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"}],\"doi\":\"10.1109/CDC.2008.4739438\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1df88f9f0e599b79b1fef6d1bc8dd46271844a0a\",\"title\":\"General duality between optimal control and estimation\",\"url\":\"https://www.semanticscholar.org/paper/1df88f9f0e599b79b1fef6d1bc8dd46271844a0a\",\"venue\":\"2008 47th IEEE Conference on Decision and Control\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"}],\"doi\":\"10.1145/1553374.1553508\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c\",\"title\":\"Robot trajectory optimization using approximate inference\",\"url\":\"https://www.semanticscholar.org/paper/7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016}],\"title\":\"Soft Policy Gradient Method for Maximum Entropy Deep Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Gradient method\",\"topicId\":\"61331\",\"url\":\"https://www.semanticscholar.org/topic/61331\"}],\"url\":\"https://www.semanticscholar.org/paper/30a136e860f105604eba58dc880b9253c5f3202c\",\"venue\":\"IJCAI\",\"year\":2019}\n"