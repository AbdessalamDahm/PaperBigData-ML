"{\"abstract\":\"Game AI is of great importance as games are simulations of reality. Recent research on game AI has shown much progress in various kinds of games, such as console games, board games and MOBA games. However, the exploration in RTS games remains a challenge for their huge state space, imperfect information, sparse rewards and various strategies. Besides, the typical card-based RTS games have complex card features and are still lacking solutions. We present a deep model SEAT (selectionattention) to play card-based RTS games. The SEAT model includes two parts, a selection part for card choice and an attention part for card usage, and it learns from scratch via deep reinforcement learning. Comprehensive experiments are performed on Clash Royale1, a popular mobile card-based RTS game. Empirical results show that the SEAT model agent makes it to reach a high winning rate against rule-based agents and decision-tree-based agent.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"46999304\",\"name\":\"T. Liu\",\"url\":\"https://www.semanticscholar.org/author/46999304\"},{\"authorId\":\"3186970\",\"name\":\"Zijie Zheng\",\"url\":\"https://www.semanticscholar.org/author/3186970\"},{\"authorId\":\"2571460\",\"name\":\"Hongchang Li\",\"url\":\"https://www.semanticscholar.org/author/2571460\"},{\"authorId\":\"145987447\",\"name\":\"K. Bian\",\"url\":\"https://www.semanticscholar.org/author/145987447\"},{\"authorId\":\"38336053\",\"name\":\"L. Song\",\"url\":\"https://www.semanticscholar.org/author/38336053\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2012.03204\",\"authors\":[{\"authorId\":\"50982491\",\"name\":\"Hangtian Jia\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"81185915\",\"name\":\"Chunxu Ren\"},{\"authorId\":\"80892810\",\"name\":\"Tangjie Lv\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":\"1797369\",\"name\":\"C. Zhang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8922d5cd990903e594bcd4de96c6251361919c70\",\"title\":\"Fever Basketball: A Complex, Flexible, and Asynchronized Sports Game Environment for Multi-agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8922d5cd990903e594bcd4de96c6251361919c70\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.12186\",\"authors\":[{\"authorId\":\"22169323\",\"name\":\"Rinu Boney\"},{\"authorId\":\"145096481\",\"name\":\"A. Ilin\"},{\"authorId\":\"1776374\",\"name\":\"Juho Kannala\"},{\"authorId\":\"2040380033\",\"name\":\"Jarno Seppanen\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e445caf0773b8a3d0def135ab4467d362b5ef404\",\"title\":\"Learning to Play Imperfect-Information Games by Imitating an Oracle Planner\",\"url\":\"https://www.semanticscholar.org/paper/e445caf0773b8a3d0def135ab4467d362b5ef404\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50982491\",\"name\":\"Hangtian Jia\"},{\"authorId\":\"81185915\",\"name\":\"Chunxu Ren\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"152829349\",\"name\":\"Yingfeng Chen\"},{\"authorId\":\"80892810\",\"name\":\"Tangjie Lv\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\"},{\"authorId\":\"31190626\",\"name\":\"Hongyao Tang\"},{\"authorId\":\"48692513\",\"name\":\"J. Hao\"}],\"doi\":\"10.5555/3398761.3399011\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1d0e22458e1dd3021ee27555d83bcaa007c029b3\",\"title\":\"Mastering Basketball With Deep Reinforcement Learning: An Integrated Curriculum Training Approach\",\"url\":\"https://www.semanticscholar.org/paper/1d0e22458e1dd3021ee27555d83bcaa007c029b3\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"2001.00127\",\"authors\":[{\"authorId\":\"143613433\",\"name\":\"K. Jiang\"},{\"authorId\":\"12836918\",\"name\":\"X. Qin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3406d877b7b9faed2c791a7d5b869d8f2b927e91\",\"title\":\"Reinforcement Learning with Goal-Distance Gradient\",\"url\":\"https://www.semanticscholar.org/paper/3406d877b7b9faed2c791a7d5b869d8f2b927e91\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199465984,\"doi\":\"10.24963/ijcai.2019/631\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"a8f26f38e0b08e0b1891202d7287ea534333fc57\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"38637273\",\"name\":\"Hsiu-Chin Lin\"},{\"authorId\":\"20930108\",\"name\":\"P. Ray\"},{\"authorId\":\"39600588\",\"name\":\"M. Howard\"}],\"doi\":\"10.1109/icra33291.2017\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0aba478ab135e3b72b38dc9dd3e4608b213ba53\",\"title\":\"The 2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"url\":\"https://www.semanticscholar.org/paper/e0aba478ab135e3b72b38dc9dd3e4608b213ba53\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47969823\",\"name\":\"Nicholas Watters\"},{\"authorId\":\"2944502\",\"name\":\"Daniel Zoran\"},{\"authorId\":\"143947744\",\"name\":\"T. Weber\"},{\"authorId\":\"2019153\",\"name\":\"P. Battaglia\"},{\"authorId\":\"1996134\",\"name\":\"Razvan Pascanu\"},{\"authorId\":\"2844530\",\"name\":\"Andrea Tacchetti\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"213bdfd1ff527f4ce1c298f6f116a4b4240d7425\",\"title\":\"Visual Interaction Networks: Learning a Physics Simulator from Video\",\"url\":\"https://www.semanticscholar.org/paper/213bdfd1ff527f4ce1c298f6f116a4b4240d7425\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"9261516\",\"name\":\"Corey Clark\"},{\"authorId\":\"25635173\",\"name\":\"Anthony Fleshner\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba31501c8bb266a030f7607130b77720a1e2fb22\",\"title\":\"Fast Random Genetic Search for Large-Scale RTS Combat Scenarios\",\"url\":\"https://www.semanticscholar.org/paper/ba31501c8bb266a030f7607130b77720a1e2fb22\",\"venue\":\"AIIDE\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":\"10.1109/TG.2018.2809901\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"49290e08e8e8069d3b7d35064914d07783e2f5da\",\"title\":\"IEEE Transactions on Games: A Leading Journal for Games Research\",\"url\":\"https://www.semanticscholar.org/paper/49290e08e8e8069d3b7d35064914d07783e2f5da\",\"venue\":\"IEEE Trans. Games\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2914912\",\"name\":\"Nicolas A. Barriga\"},{\"authorId\":\"50183289\",\"name\":\"M. Stanescu\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":\"10.1109/TCIAIG.2017.2717902\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"40e774c5132d907f987e5af0cdbb9ab06e9b3d65\",\"title\":\"Game Tree Search Based on Nondeterministic Action Scripts in Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/40e774c5132d907f987e5af0cdbb9ab06e9b3d65\",\"venue\":\"IEEE Transactions on Games\",\"year\":2018},{\"arxivId\":\"1812.00054\",\"authors\":[{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"2401865\",\"name\":\"Jonas Gehring\"},{\"authorId\":\"52008119\",\"name\":\"Daniel Gant\"},{\"authorId\":\"51994279\",\"name\":\"Vegard Mella\"},{\"authorId\":\"2182694\",\"name\":\"Vasil Khalidov\"},{\"authorId\":\"3422899\",\"name\":\"Nicolas Carion\"},{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"30b12b0976c8172c0276a9bc23634e4c41a37cae\",\"title\":\"Forward Modeling for Partial Observation Strategy Games - A StarCraft Defogger\",\"url\":\"https://www.semanticscholar.org/paper/30b12b0976c8172c0276a9bc23634e4c41a37cae\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Ai Magazine\",\"url\":\"\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Timothy P. Lillicrap\"},{\"authorId\":null,\"name\":\"Jonathan J. Hunt\"},{\"authorId\":null,\"name\":\"Alexander Pritzel\"},{\"authorId\":null,\"name\":\"Nicolas Heess\"},{\"authorId\":null,\"name\":\"Tom Erez\"},{\"authorId\":null,\"name\":\"Yuval Tassa\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Daan Wierstra. Continuous control with deep reinforceme learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1509.02971,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Daniel Jiang\"},{\"authorId\":null,\"name\":\"Emmanuel Ekwedike\"},{\"authorId\":null,\"name\":\"Han Liu. Feedback-based tree search for reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 2289\\u20132298,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Joseph Redmon\"},{\"authorId\":null,\"name\":\"Ali Farhadi\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Yolo9000: better\",\"url\":\"\",\"venue\":\"faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7263\\u20137271,\",\"year\":2017},{\"arxivId\":\"1805.05935\",\"authors\":[{\"authorId\":\"1859529\",\"name\":\"Daniel R. Jiang\"},{\"authorId\":\"3315009\",\"name\":\"E. Ekwedike\"},{\"authorId\":\"89749055\",\"name\":\"H. Liu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0d38a46c74bdf86548644678c3cc0b5c07c51b58\",\"title\":\"Feedback-Based Tree Search for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0d38a46c74bdf86548644678c3cc0b5c07c51b58\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1609.02993\",\"authors\":[{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"},{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ddbeb2a1cf1a8a43beb7a775c3cdb080718f69a1\",\"title\":\"Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ddbeb2a1cf1a8a43beb7a775c3cdb080718f69a1\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1610.00633\",\"authors\":[{\"authorId\":\"2046135\",\"name\":\"Shixiang Gu\"},{\"authorId\":\"29891985\",\"name\":\"Ethan Holly\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.1109/ICRA.2017.7989385\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e37b999f0c96d7136db07b0185b837d5decd599a\",\"title\":\"Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates\",\"url\":\"https://www.semanticscholar.org/paper/e37b999f0c96d7136db07b0185b837d5decd599a\",\"venue\":\"2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38814203\",\"name\":\"Alberto Uriarte\"},{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"482a543abe7bdda9d81181f7d7cc764598471efa\",\"title\":\"Improving Terrain Analysis and Applications to RTS Game AI\",\"url\":\"https://www.semanticscholar.org/paper/482a543abe7bdda9d81181f7d7cc764598471efa\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1772287\",\"name\":\"Abdallah Saffidine\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"168c6eb518d4db46ad49912672c3bf5ea5b66b8a\",\"title\":\"Fast Heuristic Search for RTS Game Combat Scenarios\",\"url\":\"https://www.semanticscholar.org/paper/168c6eb518d4db46ad49912672c3bf5ea5b66b8a\",\"venue\":\"AIIDE\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1722671\",\"name\":\"S. Onta\\u00f1\\u00f3n\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"48dd3079dfc3d3b7dcf49b64970b8b10a6d8151b\",\"title\":\"Adversarial Hierarchical-Task Network Planning for Complex Real-Time Games\",\"url\":\"https://www.semanticscholar.org/paper/48dd3079dfc3d3b7dcf49b64970b8b10a6d8151b\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":\"1609.05521\",\"authors\":[{\"authorId\":\"1830914\",\"name\":\"Guillaume Lample\"},{\"authorId\":\"2328602\",\"name\":\"Devendra Singh Chaplot\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0b65d3839e3bf703d156b524d7db7a5e10a2623\",\"title\":\"Playing FPS Games with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e0b65d3839e3bf703d156b524d7db7a5e10a2623\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gabriel Synnaeve\"},{\"authorId\":null,\"name\":\"Pierre Bessiere. A bayesian model for rts units control a starcraft\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 2011 IEEE Conference on Computational Intelligence and Games (CIG\\u201911)\",\"url\":\"\",\"venue\":\"pages 190\\u2013196. IEEE,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Chelsea Finn\"},{\"authorId\":null,\"name\":\"Trevor Darrell\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. End-to-end training of deep visuomotor policies\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"17(1):1334\\u20131373,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ramakanth Pasunuru\"},{\"authorId\":null,\"name\":\"Mohit Bansal. Reinforced video captioning with entailmen rewards\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\",\"url\":\"\",\"venue\":\"pages 979\\u2013985,\",\"year\":2017},{\"arxivId\":\"1512.02325\",\"authors\":[{\"authorId\":\"46641573\",\"name\":\"W. Liu\"},{\"authorId\":\"1838674\",\"name\":\"Dragomir Anguelov\"},{\"authorId\":\"1761978\",\"name\":\"D. Erhan\"},{\"authorId\":\"2574060\",\"name\":\"Christian Szegedy\"},{\"authorId\":\"144828948\",\"name\":\"S. Reed\"},{\"authorId\":\"2667317\",\"name\":\"Cheng-Yang Fu\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"}],\"doi\":\"10.1007/978-3-319-46448-0_2\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0\",\"title\":\"SSD: Single Shot MultiBox Detector\",\"url\":\"https://www.semanticscholar.org/paper/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0\",\"venue\":\"ECCV\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49336604\",\"name\":\"Wenhui Wang\"},{\"authorId\":\"144610884\",\"name\":\"Nan Yang\"},{\"authorId\":\"49807919\",\"name\":\"Furu Wei\"},{\"authorId\":\"39488576\",\"name\":\"Baobao Chang\"},{\"authorId\":\"143849609\",\"name\":\"M. Zhou\"}],\"doi\":\"10.18653/v1/P17-1018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f\",\"title\":\"Gated Self-Matching Networks for Reading Comprehension and Question Answering\",\"url\":\"https://www.semanticscholar.org/paper/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f\",\"venue\":\"ACL\",\"year\":2017}],\"title\":\"Playing Card-Based RTS Games with Deep Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/a8f26f38e0b08e0b1891202d7287ea534333fc57\",\"venue\":\"IJCAI\",\"year\":2019}\n"