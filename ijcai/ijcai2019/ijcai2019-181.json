"{\"abstract\":\"The task of real-time combat game is to coordinate multiple units to defeat their enemies controlled by the given opponent in a real-time combat scenario. It is difficult to design a high-level Artificial Intelligence (AI) program for such a task due to its extremely large state-action space and real-time requirements. This paper formulates this task as a collective decentralized partially observable Markov decision process, and designs a Deep Decentralized Policy Network (DDPN) to model the polices. To train DDPN effectively, a novel two-stage learning algorithm is proposed which combines imitation learning from opponent and reinforcement learning by no-regret dynamics. Extensive experimental results on various combat scenarios indicate that proposed method can defeat different opponent models and significantly outperforms many state-of-the-art approaches.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2200852\",\"name\":\"Peixi Peng\",\"url\":\"https://www.semanticscholar.org/author/2200852\"},{\"authorId\":\"1757173\",\"name\":\"Junliang Xing\",\"url\":\"https://www.semanticscholar.org/author/1757173\"},{\"authorId\":\"50260540\",\"name\":\"L. Cao\",\"url\":\"https://www.semanticscholar.org/author/50260540\"},{\"authorId\":\"2448517\",\"name\":\"Lisen Mu\",\"url\":\"https://www.semanticscholar.org/author/2448517\"},{\"authorId\":\"48908475\",\"name\":\"C. Huang\",\"url\":\"https://www.semanticscholar.org/author/48908475\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":199465995,\"doi\":\"10.24963/ijcai.2019/181\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"bbe4c6dd398780207b381c6e12900724fb98b7ee\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tim Roughgarden. Twenty Lectures on Algorithmic Game Theory\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 230\\u2013247\",\"url\":\"\",\"venue\":\"Cambridge University Press, 1st edition,\",\"year\":2016},{\"arxivId\":\"1810.06339\",\"authors\":[{\"authorId\":\"2276894\",\"name\":\"Yuxi Li\"}],\"doi\":\"10.1201/9781351006620-6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"title\":\"Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1111.0062\",\"authors\":[{\"authorId\":\"1799949\",\"name\":\"Frans A. Oliehoek\"},{\"authorId\":\"1723205\",\"name\":\"M. Spaan\"},{\"authorId\":\"31651045\",\"name\":\"N. Vlassis\"}],\"doi\":\"10.1613/jair.2447\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"19ee8bd6b7ba5ddefa723a018271002705018815\",\"title\":\"Optimal and Approximate Q-value Functions for Decentralized POMDPs\",\"url\":\"https://www.semanticscholar.org/paper/19ee8bd6b7ba5ddefa723a018271002705018815\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1807.09936\",\"authors\":[{\"authorId\":\"51453887\",\"name\":\"Jiaming Song\"},{\"authorId\":\"40046694\",\"name\":\"H. Ren\"},{\"authorId\":\"1779671\",\"name\":\"D. Sadigh\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"aeeecd282e9a7065f37ca2246ca711264493041c\",\"title\":\"Multi-Agent Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/aeeecd282e9a7065f37ca2246ca711264493041c\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1706.02275\",\"authors\":[{\"authorId\":\"2054294\",\"name\":\"Ryan Lowe\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2080746\",\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"title\":\"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\"url\":\"https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yue Hu\"},{\"authorId\":null,\"name\":\"Juntao Li\"},{\"authorId\":\"50079147\",\"name\":\"X. Li\"},{\"authorId\":\"46452405\",\"name\":\"Gang Pan\"},{\"authorId\":\"2285442\",\"name\":\"M. Xu\"}],\"doi\":\"10.24963/ijcai.2018/204\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3416cb7c3462cfc52782c26900e912c46a2bd1c9\",\"title\":\"Knowledge-Guided Agent-Tactic-Aware Learning for StarCraft Micromanagement\",\"url\":\"https://www.semanticscholar.org/paper/3416cb7c3462cfc52782c26900e912c46a2bd1c9\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"367f2c63a6f6a10b3b64b8729d601e69337ee3cc\",\"title\":\"Rectifier Nonlinearities Improve Neural Network Acoustic Models\",\"url\":\"https://www.semanticscholar.org/paper/367f2c63a6f6a10b3b64b8729d601e69337ee3cc\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35147280\",\"name\":\"D. T. Nguyen\"},{\"authorId\":\"40305195\",\"name\":\"Akshat Kumar\"},{\"authorId\":\"1725253\",\"name\":\"H. C. Lau\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"3ecc35641a8d104210f70d932a37c30040cc9eff\",\"title\":\"Credit Assignment For Collective Multiagent RL With Global Rewards\",\"url\":\"https://www.semanticscholar.org/paper/3ecc35641a8d104210f70d932a37c30040cc9eff\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10213745\",\"name\":\"M. Pollack\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"27c593d8b1a3b6d51d33e13a0fc75052dd921775\",\"title\":\"Journal of Artificial Intelligence Research: Preface\",\"url\":\"https://www.semanticscholar.org/paper/27c593d8b1a3b6d51d33e13a0fc75052dd921775\",\"venue\":\"\",\"year\":2001},{\"arxivId\":\"1409.0575\",\"authors\":[{\"authorId\":\"2192178\",\"name\":\"Olga Russakovsky\"},{\"authorId\":\"48550120\",\"name\":\"J. Deng\"},{\"authorId\":\"71309570\",\"name\":\"H. Su\"},{\"authorId\":\"2285165\",\"name\":\"J. Krause\"},{\"authorId\":\"145031342\",\"name\":\"S. Satheesh\"},{\"authorId\":\"145423516\",\"name\":\"S. Ma\"},{\"authorId\":\"3109481\",\"name\":\"Zhiheng Huang\"},{\"authorId\":\"2354728\",\"name\":\"A. Karpathy\"},{\"authorId\":\"2556428\",\"name\":\"A. Khosla\"},{\"authorId\":\"145879842\",\"name\":\"Michael S. Bernstein\"},{\"authorId\":\"39668247\",\"name\":\"A. Berg\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"}],\"doi\":\"10.1007/s11263-015-0816-y\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"title\":\"ImageNet Large Scale Visual Recognition Challenge\",\"url\":\"https://www.semanticscholar.org/paper/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd\",\"venue\":\"International Journal of Computer Vision\",\"year\":2015},{\"arxivId\":\"1712.07305\",\"authors\":[{\"authorId\":\"46601892\",\"name\":\"Xiangyu Kong\"},{\"authorId\":\"1894653\",\"name\":\"B. Xin\"},{\"authorId\":\"32324034\",\"name\":\"Fangchen Liu\"},{\"authorId\":\"36637369\",\"name\":\"Y. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"fd1ed642617c362acd072e70cf8c8ea229430b42\",\"title\":\"Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/fd1ed642617c362acd072e70cf8c8ea229430b42\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":\"1011.0686\",\"authors\":[{\"authorId\":\"1700433\",\"name\":\"S. Ross\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"title\":\"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\",\"url\":\"https://www.semanticscholar.org/paper/79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"venue\":\"AISTATS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":\"10.1109/CIG.2013.6633643\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"f78a522c99cd3056dbfbbeae9d3410f08ab12027\",\"title\":\"Portfolio greedy search and simulation for large-scale combat in starcraft\",\"url\":\"https://www.semanticscholar.org/paper/f78a522c99cd3056dbfbbeae9d3410f08ab12027\",\"venue\":\"2013 IEEE Conference on Computational Inteligence in Games (CIG)\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yuandong Tian\"},{\"authorId\":null,\"name\":\"Qucheng Gong\"},{\"authorId\":null,\"name\":\"Wenling Shang\"},{\"authorId\":null,\"name\":\"Yuxin Wu\"},{\"authorId\":null,\"name\":\"C. Lawrence Zitnick\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Elf: An extensive\",\"url\":\"\",\"venue\":\"lightweight and flexible research platform for real-time strategy games. In NIPS,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1492009633\",\"name\":\"Patrick J. Roa\"}],\"doi\":\"10.1023/A:1017153816538\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8e6d789ee714d29c9b5156ba9d61b2170d7a315f\",\"title\":\"Volume 8\",\"url\":\"https://www.semanticscholar.org/paper/8e6d789ee714d29c9b5156ba9d61b2170d7a315f\",\"venue\":\"\",\"year\":1998},{\"arxivId\":\"1605.07736\",\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"50295c19e177480ba3599300de1ab837cc62b08c\",\"title\":\"Learning Multiagent Communication with Backpropagation\",\"url\":\"https://www.semanticscholar.org/paper/50295c19e177480ba3599300de1ab837cc62b08c\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143772943\",\"name\":\"T. Hester\"},{\"authorId\":\"7515048\",\"name\":\"Matej Vecer\\u00edk\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"48257711\",\"name\":\"Dan Horgan\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2533110\",\"name\":\"A. Sendonaris\"},{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1387885286\",\"name\":\"Gabriel Dulac-Arnold\"},{\"authorId\":\"70495322\",\"name\":\"J. Agapiou\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e3b0ea7209731c47b582215c6c67f9c691ad9863\",\"title\":\"Deep Q-learning From Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/e3b0ea7209731c47b582215c6c67f9c691ad9863\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D Silver\"},{\"authorId\":null,\"name\":\"A. Huang\"},{\"authorId\":null,\"name\":\"C. J. Maddison\"},{\"authorId\":null,\"name\":\"A Guez\"},{\"authorId\":null,\"name\":\"L Sifre\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"den Driessche G Van\",\"url\":\"\",\"venue\":\"J Schrittwieser, I Antonoglou, V Panneershelvam, and M Lanctot. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\\u2013489\",\"year\":2016},{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1703.01030\",\"authors\":[{\"authorId\":\"144426657\",\"name\":\"Wen Sun\"},{\"authorId\":\"1978198\",\"name\":\"A. Venkatraman\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"3288815\",\"name\":\"B. Boots\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"72a3da7491ebc09e319167dba4b2cdb1d285bcdc\",\"title\":\"Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction\",\"url\":\"https://www.semanticscholar.org/paper/72a3da7491ebc09e319167dba4b2cdb1d285bcdc\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1606.03476\",\"authors\":[{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"title\":\"Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145847467\",\"name\":\"D. Pomerleau\"}],\"doi\":\"10.1162/neco.1991.3.1.88\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8d652a1980e743c7c85ff6066409ea1e3be4d685\",\"title\":\"Efficient Training of Artificial Neural Networks for Autonomous Navigation\",\"url\":\"https://www.semanticscholar.org/paper/8d652a1980e743c7c85ff6066409ea1e3be4d685\",\"venue\":\"Neural Computation\",\"year\":1991},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46636184\",\"name\":\"P. Peng\"},{\"authorId\":\"145001851\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2324546\",\"name\":\"Levi H. S. Lelis\"}],\"doi\":\"10.24963/ijcai.2017/522\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"99ecef89ee0d8d74564a94b4ccc55f5acc1a3bef\",\"title\":\"Stratified Strategy Selection for Unit Control in Real-Time Strategy Games\",\"url\":\"https://www.semanticscholar.org/paper/99ecef89ee0d8d74564a94b4ccc55f5acc1a3bef\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1995717\",\"name\":\"Bingyi Kang\"},{\"authorId\":\"2750647\",\"name\":\"Zequn Jie\"},{\"authorId\":\"33221685\",\"name\":\"Jiashi Feng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c979efe1f0a8b0b343ea332368e5b51dc153c522\",\"title\":\"Policy Optimization with Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/c979efe1f0a8b0b343ea332368e5b51dc153c522\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1811.03516\",\"authors\":[{\"authorId\":\"3424488\",\"name\":\"F. Behbahani\"},{\"authorId\":\"3402736\",\"name\":\"K. Shiarlis\"},{\"authorId\":\"1683647\",\"name\":\"X. Chen\"},{\"authorId\":\"30525721\",\"name\":\"V. Kurin\"},{\"authorId\":\"77651840\",\"name\":\"Sudhanshu Kasewa\"},{\"authorId\":\"48360838\",\"name\":\"C. Stirbu\"},{\"authorId\":\"145113035\",\"name\":\"J. Gomes\"},{\"authorId\":\"20471246\",\"name\":\"Supratik Paul\"},{\"authorId\":\"1799949\",\"name\":\"Frans A. Oliehoek\"},{\"authorId\":\"2945930\",\"name\":\"J. Messias\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":\"10.1109/ICRA.2019.8794412\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c1bd2ab99a33c70c41333203ed9aa94eab7da35\",\"title\":\"Learning From Demonstration in the Wild\",\"url\":\"https://www.semanticscholar.org/paper/1c1bd2ab99a33c70c41333203ed9aa94eab7da35\",\"venue\":\"2019 International Conference on Robotics and Automation (ICRA)\",\"year\":2019},{\"arxivId\":\"1705.08926\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"2b292ff89d808fba10579871591a22f1649cd039\",\"title\":\"Counterfactual Multi-Agent Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/2b292ff89d808fba10579871591a22f1649cd039\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1609.02993\",\"authors\":[{\"authorId\":\"1746841\",\"name\":\"Nicolas Usunier\"},{\"authorId\":\"2282478\",\"name\":\"Gabriel Synnaeve\"},{\"authorId\":\"3370429\",\"name\":\"Zeming Lin\"},{\"authorId\":\"2127604\",\"name\":\"Soumith Chintala\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ddbeb2a1cf1a8a43beb7a775c3cdb080718f69a1\",\"title\":\"Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ddbeb2a1cf1a8a43beb7a775c3cdb080718f69a1\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"153118271\",\"name\":\"D. Churchill\"},{\"authorId\":\"1772287\",\"name\":\"Abdallah Saffidine\"},{\"authorId\":\"1799228\",\"name\":\"M. Buro\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"168c6eb518d4db46ad49912672c3bf5ea5b66b8a\",\"title\":\"Fast Heuristic Search for RTS Game Combat Scenarios\",\"url\":\"https://www.semanticscholar.org/paper/168c6eb518d4db46ad49912672c3bf5ea5b66b8a\",\"venue\":\"AIIDE\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143794407\",\"name\":\"X. Hao\"},{\"authorId\":\"8273966\",\"name\":\"Guigang Zhang\"},{\"authorId\":\"144153753\",\"name\":\"Shang Ma\"}],\"doi\":\"10.1142/S1793351X16500045\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f8d648c52edf74e41b0996128aa536e13cc7e82\",\"title\":\"Deep Learning\",\"url\":\"https://www.semanticscholar.org/paper/4f8d648c52edf74e41b0996128aa536e13cc7e82\",\"venue\":\"Int. J. Semantic Comput.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2593137\",\"name\":\"Nicholas J. Butko\"}],\"doi\":\"10.4324/9780203773178\",\"intent\":[\"background\",\"result\"],\"isInfluential\":false,\"paperId\":\"62a92a6d75aa00ec2f91dfd19b90ef91afc53602\",\"title\":\"Active perception\",\"url\":\"https://www.semanticscholar.org/paper/62a92a6d75aa00ec2f91dfd19b90ef91afc53602\",\"venue\":\"\",\"year\":2010},{\"arxivId\":\"1711.08101\",\"authors\":[{\"authorId\":\"30121835\",\"name\":\"Rubens O. Moraes\"},{\"authorId\":\"2324546\",\"name\":\"Levi H. S. Lelis\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2a5aa7394e85b381961f0f965587744061de9d90\",\"title\":\"Asymmetric Action Abstractions for Multi-Unit Control in Adversarial Real-Time Games\",\"url\":\"https://www.semanticscholar.org/paper/2a5aa7394e85b381961f0f965587744061de9d90\",\"venue\":\"AAAI\",\"year\":2018}],\"title\":\"Learning Deep Decentralized Policy Network by Collective Rewards for Real-Time Combat Game\",\"topics\":[{\"topic\":\"Real-time transcription\",\"topicId\":\"763488\",\"url\":\"https://www.semanticscholar.org/topic/763488\"}],\"url\":\"https://www.semanticscholar.org/paper/bbe4c6dd398780207b381c6e12900724fb98b7ee\",\"venue\":\"IJCAI\",\"year\":2019}\n"