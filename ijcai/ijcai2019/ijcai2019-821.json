"{\"abstract\":\"Standard reinforcement learning methods aim to master one way of solving a task whereas there may exist multiple near-optimal policies. Being able to identify this collection of near-optimal policies can allow a domain expert to efficiently explore the space of reasonable solutions. Unfortunately, existing approaches that quantify uncertainty over policies are not ultimately relevant to finding policies with qualitatively distinct behaviors. In this work, we formalize the difference between policies as a difference between the distribution of trajectories induced by each policy, which encourages diversity with respect to both state visitation and action choices. We derive a gradient-based optimization technique that can be combined with existing policy gradient methods to now identify diverse collections of well-performing policies. We demonstrate our approach on benchmarks and a healthcare task.\",\"arxivId\":\"1906.00088\",\"authors\":[{\"authorId\":\"145453110\",\"name\":\"M. A. Masood\",\"url\":\"https://www.semanticscholar.org/author/145453110\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\",\"url\":\"https://www.semanticscholar.org/author/1388372395\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2005.04301\",\"authors\":[{\"authorId\":\"2596338\",\"name\":\"Mingyu Lu\"},{\"authorId\":\"13258377\",\"name\":\"Zachary Shahn\"},{\"authorId\":\"51172394\",\"name\":\"Daby Sow\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\"},{\"authorId\":\"46260197\",\"name\":\"Li-wei Lehman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"400f39f346233cfa0b40e63d2fa8acec2e19d10c\",\"title\":\"Is Deep Reinforcement Learning Ready for Practical Applications in Healthcare? A Sensitivity Analysis of Duel-DDQN for Hemodynamic Management in Sepsis Patients\",\"url\":\"https://www.semanticscholar.org/paper/400f39f346233cfa0b40e63d2fa8acec2e19d10c\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2010.11486\",\"authors\":[{\"authorId\":\"38590757\",\"name\":\"Aneta Neumann\"},{\"authorId\":\"3011711\",\"name\":\"Jakob Bossek\"},{\"authorId\":\"153588341\",\"name\":\"F. Neumann\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ebfd9e4cb65a60f17fdeeb4ca20b3336848ac90a\",\"title\":\"Computing Diverse Sets of Solutions for Monotone Submodular Optimisation Problems\",\"url\":\"https://www.semanticscholar.org/paper/ebfd9e4cb65a60f17fdeeb4ca20b3336848ac90a\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.04207\",\"authors\":[{\"authorId\":\"144738857\",\"name\":\"L. Pan\"},{\"authorId\":\"144994208\",\"name\":\"Qingpeng Cai\"},{\"authorId\":\"2349295\",\"name\":\"Longbo Huang\"}],\"doi\":\"10.5555/3398761.3398878\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"81edfc11f2207d96ceb78133b052802e44fdc85c\",\"title\":\"Multi-Path Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/81edfc11f2207d96ceb78133b052802e44fdc85c\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"2001.03224\",\"authors\":[{\"authorId\":\"2585470\",\"name\":\"J. Futoma\"},{\"authorId\":\"145453110\",\"name\":\"M. A. Masood\"},{\"authorId\":\"1412069139\",\"name\":\"Finale Doshi-Velez\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"739b97715a08a85548eb1748f0aa662258a6edd0\",\"title\":\"Identifying Distinct, Effective Treatments for Acute Hypotension with SODA-RL: Safely Optimized Diverse Accurate Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/739b97715a08a85548eb1748f0aa662258a6edd0\",\"venue\":\"AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Translational Science\",\"year\":2020},{\"arxivId\":\"2006.07781\",\"authors\":[{\"authorId\":\"46216016\",\"name\":\"Zhenghao Peng\"},{\"authorId\":\"144990601\",\"name\":\"Hao Sun\"},{\"authorId\":\"145291669\",\"name\":\"B. Zhou\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fca28a48e3a01b9ad4c269511d9d23ff22ccd051\",\"title\":\"Non-local Policy Optimization via Diversity-regularized Collaborative Exploration\",\"url\":\"https://www.semanticscholar.org/paper/fca28a48e3a01b9ad4c269511d9d23ff22ccd051\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145453110\",\"name\":\"M. A. Masood\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"88c8e55decb1b33cff02c7de973a990cbb07a823\",\"title\":\"Algorithms for Discovering Collections of High-Quality and Diverse Solutions, With Applications to Bayesian Non-Negative Matrix Factorization and Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/88c8e55decb1b33cff02c7de973a990cbb07a823\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2007.12678\",\"authors\":[{\"authorId\":\"134763925\",\"name\":\"Shengpu Tang\"},{\"authorId\":\"32646210\",\"name\":\"Aditya Modi\"},{\"authorId\":\"4829952\",\"name\":\"M. Sjoding\"},{\"authorId\":\"38556322\",\"name\":\"J. Wiens\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e731aaf59a525fe3fd5b0a0c257fb5272d5decad\",\"title\":\"Clinician-in-the-Loop Decision Making: Reinforcement Learning with Near-Optimal Set-Valued Policies\",\"url\":\"https://www.semanticscholar.org/paper/e731aaf59a525fe3fd5b0a0c257fb5272d5decad\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2596338\",\"name\":\"Mingyu Lu\"},{\"authorId\":\"13258377\",\"name\":\"Zachary Shahn\"},{\"authorId\":\"51172394\",\"name\":\"Daby Sow\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\"},{\"authorId\":\"46260197\",\"name\":\"Li-wei Lehman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6f8fa65580621fdc7006c544536c27d7d7d5c26f\",\"title\":\"Is Deep Reinforcement Learning Ready for Practical Applications in Healthcare? A Sensitivity Analysis of Duel-DDQN for Sepsis Treatment\",\"url\":\"https://www.semanticscholar.org/paper/6f8fa65580621fdc7006c544536c27d7d7d5c26f\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":173991177,\"doi\":\"10.24963/ijcai.2019/821\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"0afedce86320fbb798b161e88584c93caaf6d5a8\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"48533852\",\"name\":\"M. Smith\"},{\"authorId\":\"47662867\",\"name\":\"H. V. Hoof\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"809f951c77b5a39e2a9d556e9cf9938de87f2393\",\"title\":\"An Inference-Based Policy Gradient Method for Learning Options\",\"url\":\"https://www.semanticscholar.org/paper/809f951c77b5a39e2a9d556e9cf9938de87f2393\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1802.04564\",\"authors\":[{\"authorId\":\"33317877\",\"name\":\"Zhang-Wei Hong\"},{\"authorId\":\"48441912\",\"name\":\"Tzu-Yun Shann\"},{\"authorId\":\"5550866\",\"name\":\"Shih-Yang Su\"},{\"authorId\":\"30595482\",\"name\":\"Y. Chang\"},{\"authorId\":\"49010592\",\"name\":\"Chun-Yi Lee\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3c3093f5f8e9601637612fcfb8f160f116fa30e4\",\"title\":\"Diversity-Driven Exploration Strategy for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3c3093f5f8e9601637612fcfb8f160f116fa30e4\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":\"1609.04436\",\"authors\":[{\"authorId\":\"103809454\",\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"}],\"doi\":\"10.1561/2200000049\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0858fb6efb0e7ef549db94813b9d6f896073d60a\",\"title\":\"Bayesian Reinforcement Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/0858fb6efb0e7ef549db94813b9d6f896073d60a\",\"venue\":\"Found. Trends Mach. Learn.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Komorowski\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"alents, following the preprocessing\",\"url\":\"\",\"venue\":\"\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":\"0805.2368\",\"authors\":[{\"authorId\":\"1708497\",\"name\":\"A. Gretton\"},{\"authorId\":\"1704422\",\"name\":\"K. Borgwardt\"},{\"authorId\":\"1733256\",\"name\":\"Malte J. Rasch\"},{\"authorId\":\"1707625\",\"name\":\"B. Sch\\u00f6lkopf\"},{\"authorId\":\"46234526\",\"name\":\"Alex Smola\"}],\"doi\":\"10.7551/mitpress/7503.003.0069\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9bca4d7b932e0854c3325f1578cfd17341dd8ea8\",\"title\":\"A Kernel Method for the Two-Sample-Problem\",\"url\":\"https://www.semanticscholar.org/paper/9bca4d7b932e0854c3325f1578cfd17341dd8ea8\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. Y. Chen\"},{\"authorId\":null,\"name\":\"L. Mackey\"},{\"authorId\":null,\"name\":\"J. Gorham\"},{\"authorId\":null,\"name\":\"F.-X. Briol\"},{\"authorId\":null,\"name\":\"C. J. Oates. Stein Points\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ArXiv e-prints\",\"url\":\"\",\"venue\":\"March\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28972636\",\"name\":\"Alistair E. W. Johnson\"},{\"authorId\":\"3531389\",\"name\":\"D. Stone\"},{\"authorId\":\"1827828\",\"name\":\"L. A. Celi\"},{\"authorId\":\"40541154\",\"name\":\"Tom J. Pollard\"}],\"doi\":\"10.1093/jamia/ocx084\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"36d7fd4bead962f0fda912a66b8eb858751bf979\",\"title\":\"The MIMIC Code Repository: enabling reproducibility in critical care research\",\"url\":\"https://www.semanticscholar.org/paper/36d7fd4bead962f0fda912a66b8eb858751bf979\",\"venue\":\"J. Am. Medical Informatics Assoc.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mahdi Milani Fard\"},{\"authorId\":null,\"name\":\"Joelle Pineau. Non-deterministic policies in markovian de Artif\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Intell\",\"url\":\"\",\"venue\":\"Res.(JAIR), 40:1\\u201324,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"ea59331b65129d8ac47bf159a0d0f43831738a14\",\"title\":\"Safe Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ea59331b65129d8ac47bf159a0d0f43831738a14\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40432075\",\"name\":\"J. L. Vincent\"},{\"authorId\":\"152412503\",\"name\":\"R. Moreno\"},{\"authorId\":\"50265267\",\"name\":\"J. Takala\"},{\"authorId\":\"21246562\",\"name\":\"S. Willatts\"},{\"authorId\":\"75105400\",\"name\":\"A. D. Mendonca\"},{\"authorId\":\"2264322\",\"name\":\"H. Bruining\"},{\"authorId\":\"152238831\",\"name\":\"C. Reinhart\"},{\"authorId\":\"2164133\",\"name\":\"P. Suter\"},{\"authorId\":\"31766779\",\"name\":\"L. G. Thijs\"}],\"doi\":\"10.1007/BF01709751\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"359ae2261d4a478e5d79e95a01a36054badb0b1b\",\"title\":\"The SOFA (Sepsis-related Organ Failure Assessment) score to describe organ dysfunction/failure\",\"url\":\"https://www.semanticscholar.org/paper/359ae2261d4a478e5d79e95a01a36054badb0b1b\",\"venue\":\"Intensive Care Medicine\",\"year\":2005},{\"arxivId\":\"1802.08331\",\"authors\":[{\"authorId\":\"145509577\",\"name\":\"A. Cohen\"},{\"authorId\":\"47785308\",\"name\":\"L. Yu\"},{\"authorId\":\"144766657\",\"name\":\"R. Wright\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"a033cd88249be20b1457d5b580375995414c3cfe\",\"title\":\"Diverse Exploration for Fast and Safe Policy Improvement\",\"url\":\"https://www.semanticscholar.org/paper/a033cd88249be20b1457d5b580375995414c3cfe\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":null,\"name\":\"Shie Mannor\"},{\"authorId\":null,\"name\":\"Joelle Pineau\"},{\"authorId\":null,\"name\":\"Aviv Tamar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bayesian reinforcement learning: A survey. Foundations and Trends R in Machine Learning\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144886843\",\"name\":\"Richard Lathe\"}],\"doi\":\"10.1038/332676B0\",\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"title\":\"Phd by thesis\",\"url\":\"https://www.semanticscholar.org/paper/6ec27fba80de3b9c52ef6ac4eaa9f59821aefb4b\",\"venue\":\"Nature\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shirin Sohrabi\"},{\"authorId\":null,\"name\":\"Anton V Riabov\"},{\"authorId\":null,\"name\":\"Octavian Udrea\"},{\"authorId\":null,\"name\":\"Oktie Hassanzadeh. Finding diverse highquality plans for generation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In ECAI\",\"url\":\"\",\"venue\":\"pages 1581\\u20131582,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sascha Lange\"},{\"authorId\":null,\"name\":\"Thomas Gabel\"},{\"authorId\":null,\"name\":\"Martin Riedmiller. Batch reinforcement learning. In Reinf learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 45\\u201373\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2012},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":\"1703.01260\",\"authors\":[{\"authorId\":\"2550764\",\"name\":\"Justin Fu\"},{\"authorId\":\"1388383230\",\"name\":\"John D. Co-Reyes\"},{\"authorId\":\"152198491\",\"name\":\"Sergey Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"850d78496304829d16d14701e4d81692f088f47d\",\"title\":\"EX2: Exploration with Exemplar Models for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/850d78496304829d16d14701e4d81692f088f47d\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144444290\",\"name\":\"B. Srivastava\"},{\"authorId\":\"34852029\",\"name\":\"T. Nguyen\"},{\"authorId\":\"2633661\",\"name\":\"A. Gerevini\"},{\"authorId\":\"1740315\",\"name\":\"S. Kambhampati\"},{\"authorId\":\"30779314\",\"name\":\"M. B. Do\"},{\"authorId\":\"1801054\",\"name\":\"I. Serina\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"52ce40c7e985ad48f81888ff0decbe5786e502cc\",\"title\":\"Domain Independent Approaches for Finding Diverse Plans\",\"url\":\"https://www.semanticscholar.org/paper/52ce40c7e985ad48f81888ff0decbe5786e502cc\",\"venue\":\"IJCAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50557081\",\"name\":\"C. Liu\"},{\"authorId\":\"145813732\",\"name\":\"X. Xu\"},{\"authorId\":\"46570618\",\"name\":\"D. Hu\"}],\"doi\":\"10.1109/TSMC.2014.2358639\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"df2da95303bedf417c76fa8439844d671fb056da\",\"title\":\"Multiobjective Reinforcement Learning: A Comprehensive Overview\",\"url\":\"https://www.semanticscholar.org/paper/df2da95303bedf417c76fa8439844d671fb056da\",\"venue\":\"IEEE Transactions on Systems, Man, and Cybernetics: Systems\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145348788\",\"name\":\"M. Ghassemi\"},{\"authorId\":\"1700653\",\"name\":\"M. Wu\"},{\"authorId\":\"2169240\",\"name\":\"M. Hughes\"},{\"authorId\":\"1679873\",\"name\":\"Peter Szolovits\"},{\"authorId\":\"1388372395\",\"name\":\"Finale Doshi-Velez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"05140f3598aba12389b7f9cae26a8005f8f55937\",\"title\":\"Predicting intervention onset in the ICU with switching state space models\",\"url\":\"https://www.semanticscholar.org/paper/05140f3598aba12389b7f9cae26a8005f8f55937\",\"venue\":\"CRI\",\"year\":2017},{\"arxivId\":\"1706.10295\",\"authors\":[{\"authorId\":\"39067762\",\"name\":\"Meire Fortunato\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"1808897\",\"name\":\"B. Piot\"},{\"authorId\":\"10698483\",\"name\":\"Jacob Menick\"},{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"123588356\",\"name\":\"Vlad Mnih\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"},{\"authorId\":\"79608109\",\"name\":\"O. Pietquin\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4cd76f8353f0c4852cc432fc0e7a5f2b91ae6ce5\",\"title\":\"Noisy Networks for Exploration\",\"url\":\"https://www.semanticscholar.org/paper/4cd76f8353f0c4852cc432fc0e7a5f2b91ae6ce5\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew Smith\"},{\"authorId\":null,\"name\":\"Herke Hoof\"},{\"authorId\":null,\"name\":\"Joelle Pineau. An inference-based policy gradient method options\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 4710\\u20134719,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Vladlen Koltun. Guided policy search\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1\\u20139,\",\"year\":2013},{\"arxivId\":\"1902.03633\",\"authors\":[{\"authorId\":\"145509577\",\"name\":\"A. Cohen\"},{\"authorId\":\"40113970\",\"name\":\"Xingye Qiao\"},{\"authorId\":\"47785308\",\"name\":\"L. Yu\"},{\"authorId\":\"69942350\",\"name\":\"Elliot Way\"},{\"authorId\":\"3337932\",\"name\":\"Xiangrong Tong\"}],\"doi\":\"10.1609/aaai.v33i01.33013404\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"593e03ca76fbe2b7500c8c30a1ade3d73dbf1f02\",\"title\":\"Diverse Exploration via Conjugate Policies for Policy Gradient Methods\",\"url\":\"https://www.semanticscholar.org/paper/593e03ca76fbe2b7500c8c30a1ade3d73dbf1f02\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"145231047\",\"name\":\"V. Koltun\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"244539f454800697ed663326b7cfba337ca0c2ec\",\"title\":\"Guided Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/244539f454800697ed663326b7cfba337ca0c2ec\",\"venue\":\"ICML\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"W. Y. Chen\"},{\"authorId\":null,\"name\":\"L. Mackey\"},{\"authorId\":null,\"name\":\"J. Gorham\"},{\"authorId\":null,\"name\":\"F.-X\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Briol, and C\",\"url\":\"\",\"venue\":\"J. Oates. Stein Points. ArXiv e-prints,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"},{\"authorId\":\"144518313\",\"name\":\"M. Lopes\"}],\"doi\":\"10.1109/ICRA.2017.7989464\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"7760a3060688d5c01099d7aae2c503daa5d0116c\",\"title\":\"Multi-bound tree search for logic-geometric programming in cooperative manipulation domains\",\"url\":\"https://www.semanticscholar.org/paper/7760a3060688d5c01099d7aae2c503daa5d0116c\",\"venue\":\"2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"39799304\",\"name\":\"Joel Lehman\"},{\"authorId\":\"1846883\",\"name\":\"K. Stanley\"}],\"doi\":\"10.1162/EVCO_a_00025\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0de77eceda6308618132204b28755ac1e63648c5\",\"title\":\"Abandoning Objectives: Evolution Through the Search for Novelty Alone\",\"url\":\"https://www.semanticscholar.org/paper/0de77eceda6308618132204b28755ac1e63648c5\",\"venue\":\"Evolutionary Computation\",\"year\":2011},{\"arxivId\":\"1805.10309\",\"authors\":[{\"authorId\":\"3366663\",\"name\":\"Tanmay Gangwani\"},{\"authorId\":\"47362455\",\"name\":\"Q. Liu\"},{\"authorId\":\"144439558\",\"name\":\"J. Peng\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"653bdfb3c35621ee04ee5d5253dc7e3a422d69e1\",\"title\":\"Learning Self-Imitating Diverse Policies\",\"url\":\"https://www.semanticscholar.org/paper/653bdfb3c35621ee04ee5d5253dc7e3a422d69e1\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":\"1401.3871\",\"authors\":[{\"authorId\":\"2852410\",\"name\":\"Mahdi Milani Fard\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":\"10.1613/jair.3175\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"612a609eeb41fecd8e5d554d3a79c82c7e797822\",\"title\":\"Non-Deterministic Policies in Markovian Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/612a609eeb41fecd8e5d554d3a79c82c7e797822\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2011},{\"arxivId\":\"1702.08165\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"4990833\",\"name\":\"Haoran Tang\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"title\":\"Reinforcement Learning with Deep Energy-Based Policies\",\"url\":\"https://www.semanticscholar.org/paper/9172cd6c253edf7c3a1568e03577db20648ad0c4\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1708497\",\"name\":\"A. Gretton\"},{\"authorId\":\"3313411\",\"name\":\"Bharath K. Sriperumbudur\"},{\"authorId\":\"1698032\",\"name\":\"D. Sejdinovic\"},{\"authorId\":\"1679743\",\"name\":\"Heiko Strathmann\"},{\"authorId\":\"3261943\",\"name\":\"S. Balakrishnan\"},{\"authorId\":\"1704699\",\"name\":\"M. Pontil\"},{\"authorId\":\"1693668\",\"name\":\"K. Fukumizu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7151066eb54f4e6b43bdd12aec566a9a912d744e\",\"title\":\"Optimal kernel choice for large-scale two-sample tests\",\"url\":\"https://www.semanticscholar.org/paper/7151066eb54f4e6b43bdd12aec566a9a912d744e\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3313411\",\"name\":\"Bharath K. Sriperumbudur\"},{\"authorId\":\"1693668\",\"name\":\"K. Fukumizu\"},{\"authorId\":\"1708497\",\"name\":\"A. Gretton\"},{\"authorId\":\"1707625\",\"name\":\"B. Sch\\u00f6lkopf\"},{\"authorId\":\"1725533\",\"name\":\"G. Lanckriet\"}],\"doi\":\"10.1109/ISIT.2010.5513626\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"028c96800f665ea4c7a93370d10788d92cf5a351\",\"title\":\"Non-parametric estimation of integral probability metrics\",\"url\":\"https://www.semanticscholar.org/paper/028c96800f665ea4c7a93370d10788d92cf5a351\",\"venue\":\"2010 IEEE International Symposium on Information Theory\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144202584\",\"name\":\"S. Lange\"},{\"authorId\":\"2777657\",\"name\":\"T. Gabel\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":\"10.1007/978-3-642-27645-3_2\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"5c65d095600d6c647426fa3bc45031b208882d5f\",\"title\":\"Batch Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5c65d095600d6c647426fa3bc45031b208882d5f\",\"venue\":\"Reinforcement Learning\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"6204450\",\"name\":\"M. Komorowski\"},{\"authorId\":\"143605744\",\"name\":\"L. Celi\"},{\"authorId\":\"4337300\",\"name\":\"O. Badawi\"},{\"authorId\":\"31699747\",\"name\":\"A. Gordon\"},{\"authorId\":\"35321363\",\"name\":\"Aldo A. Faisal\"}],\"doi\":\"10.1038/s41591-018-0213-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0e0d2fa9c5c99d16110d16c5fcf56614258497b8\",\"title\":\"The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care\",\"url\":\"https://www.semanticscholar.org/paper/0e0d2fa9c5c99d16110d16c5fcf56614258497b8\",\"venue\":\"Nature Medicine\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1821852\",\"name\":\"Shirin Sohrabi\"},{\"authorId\":\"145316333\",\"name\":\"A. Riabov\"},{\"authorId\":\"2493008\",\"name\":\"O. Udrea\"},{\"authorId\":\"1728091\",\"name\":\"Oktie Hassanzadeh\"}],\"doi\":\"10.3233/978-1-61499-672-9-1581\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"51e78aa2041d595b3871a49f4b92be725199e73d\",\"title\":\"Finding Diverse High-Quality Plans for Hypothesis Generation\",\"url\":\"https://www.semanticscholar.org/paper/51e78aa2041d595b3871a49f4b92be725199e73d\",\"venue\":\"ECAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Biplav Srivastava\"},{\"authorId\":null,\"name\":\"Tuan Anh Nguyen\"},{\"authorId\":null,\"name\":\"Alfonso Gerevini\"},{\"authorId\":null,\"name\":\"Subbarao Kambhampati\"},{\"authorId\":null,\"name\":\"Minh Binh Do\"},{\"authorId\":null,\"name\":\"Ivan Serina. Domain independent approaches for finding plans\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 2016\\u20132022,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"122879821\",\"name\":\"J. L. Gall\"},{\"authorId\":\"152683697\",\"name\":\"S. Lemeshow\"},{\"authorId\":\"153510935\",\"name\":\"F. Saulnier\"}],\"doi\":\"10.1001/JAMA.270.24.2957\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aeb4d84bf62d00bc958519e67aceef928e59c92a\",\"title\":\"A new Simplified Acute Physiology Score (SAPS II) based on a European/North American multicenter study\",\"url\":\"https://www.semanticscholar.org/paper/aeb4d84bf62d00bc958519e67aceef928e59c92a\",\"venue\":\"\",\"year\":1993},{\"arxivId\":\"1802.06070\",\"authors\":[{\"authorId\":\"8140754\",\"name\":\"Benjamin Eysenbach\"},{\"authorId\":\"144150283\",\"name\":\"A. Gupta\"},{\"authorId\":\"46920727\",\"name\":\"J. Ibarz\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5b01eaef54a653ba03ddd5a978690380fbc19bfc\",\"title\":\"Diversity is All You Need: Learning Skills without a Reward Function\",\"url\":\"https://www.semanticscholar.org/paper/5b01eaef54a653ba03ddd5a978690380fbc19bfc\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Justin Fu\"},{\"authorId\":null,\"name\":\"John Co-Reyes\"},{\"authorId\":null,\"name\":\"Sergey Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Ex 2 : Exploration with exemplar models for deep reinforcement learn\",\"url\":\"\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Bellemare\"},{\"authorId\":null,\"name\":\"Sriram Srinivasan\"},{\"authorId\":null,\"name\":\"Georg Ostrovski\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"David Saxton\"},{\"authorId\":null,\"name\":\"Remi Munos\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Unifying countbased exploration and intrinsic motivation\",\"url\":\"\",\"venue\":\"In Advances in Neural Information Processing Systems,\",\"year\":2016},{\"arxivId\":\"1704.02399\",\"authors\":[{\"authorId\":\"47909037\",\"name\":\"Yang Liu\"},{\"authorId\":\"3377142\",\"name\":\"Prajit Ramachandran\"},{\"authorId\":\"47362268\",\"name\":\"Qiang Liu\"},{\"authorId\":\"144439558\",\"name\":\"J. Peng\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3167b590e47b08828555938d3126fde1bb3c038e\",\"title\":\"Stein Variational Policy Gradient\",\"url\":\"https://www.semanticscholar.org/paper/3167b590e47b08828555938d3126fde1bb3c038e\",\"venue\":\"UAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34949407\",\"name\":\"A. Johnson\"},{\"authorId\":\"145959703\",\"name\":\"A. Kramer\"},{\"authorId\":\"2384471\",\"name\":\"G. Clifford\"}],\"doi\":\"10.1097/CCM.0b013e31828a24fe\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"09b574d8f4e1044d0ec77a71e48d760cd637d969\",\"title\":\"A New Severity of Illness Scale Using a Subset of Acute Physiology and Chronic Health Evaluation Data Elements Shows Comparable Predictive Accuracy*\",\"url\":\"https://www.semanticscholar.org/paper/09b574d8f4e1044d0ec77a71e48d760cd637d969\",\"venue\":\"Critical care medicine\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Muhammad A Masood\"},{\"authorId\":null,\"name\":\"Finale Doshi-Velez\"}],\"doi\":null,\"intent\":[\"result\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Diversity-inducing policy gradient: Using maximum mean discrepancy to find a set of diverse policies\",\"url\":\"\",\"venue\":\"arXiv preprint arXiv:1906.00088,\",\"year\":2019}],\"title\":\"Diversity-Inducing Policy Gradient: Using Maximum Mean Discrepancy to Find a Set of Diverse Policies\",\"topics\":[{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Policy\",\"topicId\":\"60001\",\"url\":\"https://www.semanticscholar.org/topic/60001\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"},{\"topic\":\"Subject-matter expert\",\"topicId\":\"146407\",\"url\":\"https://www.semanticscholar.org/topic/146407\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Discrepancy function\",\"topicId\":\"1146712\",\"url\":\"https://www.semanticscholar.org/topic/1146712\"},{\"topic\":\"Behavior\",\"topicId\":\"3332\",\"url\":\"https://www.semanticscholar.org/topic/3332\"},{\"topic\":\"Collections (publication)\",\"topicId\":\"23835\",\"url\":\"https://www.semanticscholar.org/topic/23835\"},{\"topic\":\"Solutions\",\"topicId\":\"28500\",\"url\":\"https://www.semanticscholar.org/topic/28500\"}],\"url\":\"https://www.semanticscholar.org/paper/0afedce86320fbb798b161e88584c93caaf6d5a8\",\"venue\":\"IJCAI\",\"year\":2019}\n"