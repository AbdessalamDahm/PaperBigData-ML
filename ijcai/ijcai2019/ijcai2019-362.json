"{\"abstract\":\"While off-policy temporal difference (TD) methods have widely been used in reinforcement learning due to their efficiency and simple implementation, their Bayesian counterparts have not been utilized as frequently. One reason is that the non-linear max operation in the Bellman optimality equation makes it difficult to define conjugate distributions over the value functions. In this paper, we introduce a novel Bayesian approach to off-policy TD methods, called as ADFQ, which updates beliefs on state-action values, Q, through an online Bayesian inference method known as Assumed Density Filtering. We formulate an efficient closed-form solution for the value update by approximately estimating analytic parameters of the posterior of the Q-beliefs. Uncertainty measures in the beliefs not only are used in exploration but also provide a natural regularization for the value update considering all next available actions. ADFQ converges to Q-learning as the uncertainty measures of the Q-beliefs decrease and improves common drawbacks of other Bayesian RL algorithms such as computational complexity. We extend ADFQ with a neural network. Our empirical results demonstrate that ADFQ outperforms comparable algorithms on various Atari 2600 games, with drastic improvements in highly stochastic domains or domains with a large action space.\",\"arxivId\":\"1712.03333\",\"authors\":[{\"authorId\":\"123561112\",\"name\":\"Heejin Jeong\",\"url\":\"https://www.semanticscholar.org/author/123561112\"},{\"authorId\":\"5371391\",\"name\":\"C. Zhang\",\"url\":\"https://www.semanticscholar.org/author/5371391\"},{\"authorId\":\"143770945\",\"name\":\"George J. Pappas\",\"url\":\"https://www.semanticscholar.org/author/143770945\"},{\"authorId\":\"145675444\",\"name\":\"D. Lee\",\"url\":\"https://www.semanticscholar.org/author/145675444\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2006.10190\",\"authors\":[{\"authorId\":\"47634800\",\"name\":\"Hee-Jin Jeong\"},{\"authorId\":\"34112189\",\"name\":\"H. Hassani\"},{\"authorId\":\"103001682\",\"name\":\"M. Morari\"},{\"authorId\":\"145675444\",\"name\":\"D. Lee\"},{\"authorId\":\"143770945\",\"name\":\"George J. Pappas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b53355d8303b18e10a1936013ebad9dd7e0a4140\",\"title\":\"Learning to Track Dynamic Targets in Partially Known Environments\",\"url\":\"https://www.semanticscholar.org/paper/b53355d8303b18e10a1936013ebad9dd7e0a4140\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1910.10754\",\"authors\":[{\"authorId\":\"123561112\",\"name\":\"Heejin Jeong\"},{\"authorId\":\"2366739\",\"name\":\"Brent Schlotfeldt\"},{\"authorId\":\"34112189\",\"name\":\"H. Hassani\"},{\"authorId\":\"144746954\",\"name\":\"M. Morari\"},{\"authorId\":\"145675444\",\"name\":\"D. Lee\"},{\"authorId\":\"143770945\",\"name\":\"George J. Pappas\"}],\"doi\":\"10.1109/IROS40897.2019.8968173\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"493769a50248a1318379c11f902157acca8969e0\",\"title\":\"Learning Q-network for Active Information Acquisition\",\"url\":\"https://www.semanticscholar.org/paper/493769a50248a1318379c11f902157acca8969e0\",\"venue\":\"2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2019}],\"corpusId\":47016764,\"doi\":\"10.24963/ijcai.2019/362\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"2f354bc1030d9c7c749ac28f750885954c07ef62\",\"references\":[{\"arxivId\":\"1301.6690\",\"authors\":[{\"authorId\":\"143654598\",\"name\":\"R. Dearden\"},{\"authorId\":\"50785579\",\"name\":\"N. Friedman\"},{\"authorId\":\"144980509\",\"name\":\"D. Andre\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"30a8d1e9c37969034e4823332a05de8536e4fded\",\"title\":\"Model based Bayesian Exploration\",\"url\":\"https://www.semanticscholar.org/paper/30a8d1e9c37969034e4823332a05de8536e4fded\",\"venue\":\"UAI\",\"year\":1999},{\"arxivId\":\"1402.0635\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"},{\"authorId\":\"145254492\",\"name\":\"Z. Wen\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1389772b8a0f9c7fc43057f9da41a7d0ebf0308b\",\"title\":\"Generalization and Exploration via Randomized Value Functions\",\"url\":\"https://www.semanticscholar.org/paper/1389772b8a0f9c7fc43057f9da41a7d0ebf0308b\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":\"1312.5602\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2319a491378867c7049b3da055c5df60e1671158\",\"title\":\"Playing Atari with Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/2319a491378867c7049b3da055c5df60e1671158\",\"venue\":\"ArXiv\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":\"10.1162/153244303768966102\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"4c1b2433b8205e8dc489000b7788466ca5ca14ea\",\"title\":\"On the Convergence of Optimistic Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/4c1b2433b8205e8dc489000b7788466ca5ca14ea\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":\"1205.3109\",\"authors\":[{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1790646\",\"name\":\"P. Dayan\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"918e8781344e7eee60a4f6bf883c858205a917d0\",\"title\":\"Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based Search\",\"url\":\"https://www.semanticscholar.org/paper/918e8781344e7eee60a4f6bf883c858205a917d0\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc G. Bellemare\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Will Dabney\",\"url\":\"\",\"venue\":\", and R\\u00e9mi Munos. A distributional perspective on reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"G. Brockman\"},{\"authorId\":null,\"name\":\"V. Cheung\"},{\"authorId\":null,\"name\":\"L. Pettersson\"},{\"authorId\":null,\"name\":\"J. Schneider\"},{\"authorId\":null,\"name\":\"J. Schulman\"},{\"authorId\":null,\"name\":\"J. Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and W\",\"url\":\"\",\"venue\":\"Zaremba. Openai gym\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peter S. Maybeck\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", and Andrew W . Moore . Reinforcement learning : A survey\",\"url\":\"\",\"venue\":\"Journal of Artificial Intelligence Research\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49450967\",\"name\":\"F. Wang\"}],\"doi\":\"10.1109/JAS.2014.7004611\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"90fdc5072ce70b00d40c94b3fffbfe300bc9a4a1\",\"title\":\"Welcome to IEEE/CAA Journal of Automatica Sinica\",\"url\":\"https://www.semanticscholar.org/paper/90fdc5072ce70b00d40c94b3fffbfe300bc9a4a1\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1733356\",\"name\":\"G. Chowdhary\"},{\"authorId\":\"47842115\",\"name\":\"M. Liu\"},{\"authorId\":\"144926179\",\"name\":\"T. Walsh\"},{\"authorId\":\"50309523\",\"name\":\"J. How\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"26bb4fb12341db61784d5edc4c9b249f79f8210f\",\"title\":\"Off-policy reinforcement learning with Gaussian processes Citation\",\"url\":\"https://www.semanticscholar.org/paper/26bb4fb12341db61784d5edc4c9b249f79f8210f\",\"venue\":\"\",\"year\":2014},{\"arxivId\":\"1802.04412\",\"authors\":[{\"authorId\":\"3371922\",\"name\":\"Kamyar Azizzadenesheli\"},{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"},{\"authorId\":\"2047844\",\"name\":\"Anima Anandkumar\"}],\"doi\":\"10.1109/ITA.2018.8503252\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f5eefd759da6e85feb55134f5ad7b1f4af8ee3d\",\"title\":\"Efficient Exploration Through Bayesian Deep Q-Networks\",\"url\":\"https://www.semanticscholar.org/paper/3f5eefd759da6e85feb55134f5ad7b1f4af8ee3d\",\"venue\":\"2018 Information Theory and Applications Workshop (ITA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1691022\",\"name\":\"M. Opper\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"199e1fe8cf3646f6352532dec35017d0a48d357b\",\"title\":\"A Bayesian Approach to Online Learning\",\"url\":\"https://www.semanticscholar.org/paper/199e1fe8cf3646f6352532dec35017d0a48d357b\",\"venue\":\"\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"}],\"doi\":\"10.1184/R1/6720692.V1\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a65434d43ffa6554eaf14b728780919ad4f33eb\",\"title\":\"Modeling purposeful adaptive behavior with the principle of maximum causal entropy\",\"url\":\"https://www.semanticscholar.org/paper/2a65434d43ffa6554eaf14b728780919ad4f33eb\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1666470833\",\"name\":\"Lohmarer Stadtgeschichte\"}],\"doi\":\"10.1515/9783111548050-034\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"83829d31cd18317ce12a8a0fcd71126b817cb96e\",\"title\":\"W\",\"url\":\"https://www.semanticscholar.org/paper/83829d31cd18317ce12a8a0fcd71126b817cb96e\",\"venue\":\"Therapielexikon Dermatologie und Allergologie\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1709.05380\",\"authors\":[{\"authorId\":\"1389654226\",\"name\":\"Brendan O'Donoghue\"},{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bdf6572b67a6c5d8aacd39e1826db2c5c8f85716\",\"title\":\"The Uncertainty Bellman Equation and Exploration\",\"url\":\"https://www.semanticscholar.org/paper/bdf6572b67a6c5d8aacd39e1826db2c5c8f85716\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47191491\",\"name\":\"M. O. Duff\"},{\"authorId\":\"9070053\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"6932937c3ac9e6e42c78f0e214445d017486542f\",\"title\":\"Optimal learning: computational procedures for bayes-adaptive markov decision processes\",\"url\":\"https://www.semanticscholar.org/paper/6932937c3ac9e6e42c78f0e214445d017486542f\",\"venue\":\"\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"D Brian\"}],\"doi\":null,\"intent\":[\"result\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Ziebart\",\"url\":\"\",\"venue\":\"Modeling purposeful adaptive behavior with the principle of maximum causal entropy.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"cs/9605103\",\"authors\":[{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"1760402\",\"name\":\"A. Moore\"}],\"doi\":\"10.1613/jair.301\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"12d1d070a53d4084d88a77b8b143bad51c40c38f\",\"title\":\"Reinforcement Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/12d1d070a53d4084d88a77b8b143bad51c40c38f\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"P. Dhariwal\"},{\"authorId\":null,\"name\":\"C. Hesse\"},{\"authorId\":null,\"name\":\"O. Klimov\"},{\"authorId\":null,\"name\":\"A. Nichol\"},{\"authorId\":null,\"name\":\"M. Plappert\"},{\"authorId\":null,\"name\":\"A. Radford\"},{\"authorId\":null,\"name\":\"J. Schulman\"},{\"authorId\":null,\"name\":\"S. Sidor\"},{\"authorId\":null,\"name\":\"Y. Wu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Openai baselines\",\"url\":\"\",\"venue\":\"https://github.com/openai/baselines\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49478313\",\"name\":\"L. Lin\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"54c4cf3a8168c1b70f91cf78a3dc98b671935492\",\"title\":\"Reinforcement learning for robots using neural networks\",\"url\":\"https://www.semanticscholar.org/paper/54c4cf3a8168c1b70f91cf78a3dc98b671935492\",\"venue\":\"\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthieu Geist\"},{\"authorId\":null,\"name\":\"Olivier Pietquin. Kalman temporal differences\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"39:483\\u2013532,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hado V. Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bellemare , Tom Stepleton , and R\\u00e9mi Munos . Q ( \\u03bb ) with off - policy corrections\",\"url\":\"\",\"venue\":\"International Conference on Algorithmic Learning Theory\",\"year\":2016},{\"arxivId\":\"1602.04621\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"1723876\",\"name\":\"Charles Blundell\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b63e34276aa98d5345efa7fe09bb06d8a9d8f52\",\"title\":\"Deep Exploration via Bootstrapped DQN\",\"url\":\"https://www.semanticscholar.org/paper/4b63e34276aa98d5345efa7fe09bb06d8a9d8f52\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1511.06581\",\"authors\":[{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"1737568\",\"name\":\"N. D. Freitas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"title\":\"Dueling Network Architectures for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4c05d7caa357148f0bbd61720bdd35f0bc05eb81\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Leslie P. Kaelbling\"},{\"authorId\":null,\"name\":\"Michael L. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Hasselt . Double q - learning\",\"url\":\"\",\"venue\":\"\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Greg Brockman\"},{\"authorId\":null,\"name\":\"Vicki Cheung\"},{\"authorId\":null,\"name\":\"Ludwig Pettersson\"},{\"authorId\":null,\"name\":\"Jonas Schneider\"},{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Jie Tang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Wojciech Zaremba\",\"url\":\"\",\"venue\":\"Openai gym,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Peter S. Maybeck. Stochastic models\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"estimation and control\",\"url\":\"\",\"venue\":\"Academic Press, chapter 12.7,\",\"year\":1982},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2057050\",\"name\":\"Y. Engel\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1766683\",\"name\":\"R. Meir\"}],\"doi\":\"10.1145/1102351.1102377\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ba4bcd8c4ebb1427d335c23d50249d370778ae49\",\"title\":\"Reinforcement learning with Gaussian processes\",\"url\":\"https://www.semanticscholar.org/paper/ba4bcd8c4ebb1427d335c23d50249d370778ae49\",\"venue\":\"ICML '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2057050\",\"name\":\"Y. Engel\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1766683\",\"name\":\"R. Meir\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"5f7d2efca150cc63ea4e6d25035c8f2430c6d803\",\"title\":\"Bayes Meets Bellman: The Gaussian Process Approach to Temporal Difference Learning\",\"url\":\"https://www.semanticscholar.org/paper/5f7d2efca150cc63ea4e6d25035c8f2430c6d803\",\"venue\":\"ICML\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Richard Dearden\"},{\"authorId\":null,\"name\":\"Nir Friedman\"},{\"authorId\":null,\"name\":\"Stuart Russell. Bayesian q-learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI/IAAI\",\"url\":\"\",\"venue\":\"pages 761\\u2013768,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthieu Geist\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Shie Mannor , and Ron Meir . Reinforcement learning with gaussian processes\",\"url\":\"\",\"venue\":\"Proceedings of the 22 nd International Conference on Machine Learning\",\"year\":null},{\"arxivId\":\"1301.7362\",\"authors\":[{\"authorId\":\"1681881\",\"name\":\"Xavier Boyen\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"92aea50331c19fe9716d3a9a02e26704afe24d88\",\"title\":\"Tractable Inference for Complex Stochastic Processes\",\"url\":\"https://www.semanticscholar.org/paper/92aea50331c19fe9716d3a9a02e26704afe24d88\",\"venue\":\"UAI\",\"year\":1998},{\"arxivId\":\"1406.3270\",\"authors\":[{\"authorId\":\"1737555\",\"name\":\"M. Geist\"},{\"authorId\":\"1721354\",\"name\":\"Olivier Pietquin\"}],\"doi\":\"10.1613/jair.3077\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"782c039c30f90551830ce453846870bc0ec1b5b2\",\"title\":\"Kalman Temporal Differences\",\"url\":\"https://www.semanticscholar.org/paper/782c039c30f90551830ce453846870bc0ec1b5b2\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4562073\",\"name\":\"Chris Watkins\"},{\"authorId\":\"46902274\",\"name\":\"P. Dayan\"}],\"doi\":\"10.1007/BF00992698\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"03b7e51c52084ac1db5118342a00b5fbcfc587aa\",\"title\":\"Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/03b7e51c52084ac1db5118342a00b5fbcfc587aa\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":\"1602.04951\",\"authors\":[{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"3382781\",\"name\":\"Tom Stepleton\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"}],\"doi\":\"10.1007/978-3-319-46379-7_21\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"12d959f3fa23ac6ecf06e58e3b545d4f5f4df12e\",\"title\":\"Q(\\u03bb) with Off-Policy Corrections\",\"url\":\"https://www.semanticscholar.org/paper/12d959f3fa23ac6ecf06e58e3b545d4f5f4df12e\",\"venue\":\"ALT\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1784072\",\"name\":\"M. Lagoudakis\"},{\"authorId\":\"145726861\",\"name\":\"R. Parr\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b750a17921d32936425e05f8b00b96569e2fc5a6\",\"title\":\"Least-Squares Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/b750a17921d32936425e05f8b00b96569e2fc5a6\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49972550\",\"name\":\"N. E. Manos\"}],\"doi\":\"10.1007/978-1-4939-7131-2_101271\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5d3b9c46c1e01bdc8d387208157d2875f66982fe\",\"title\":\"Stochastic Models\",\"url\":\"https://www.semanticscholar.org/paper/5d3b9c46c1e01bdc8d387208157d2875f66982fe\",\"venue\":\"Encyclopedia of Social Network Analysis and Mining. 2nd Ed.\",\"year\":1960},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143654598\",\"name\":\"R. Dearden\"},{\"authorId\":\"50785579\",\"name\":\"N. Friedman\"},{\"authorId\":\"145107462\",\"name\":\"S. Russell\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b90b5b0cf05bc63f768f55322381f5cfbee6ce1c\",\"title\":\"Bayesian Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/b90b5b0cf05bc63f768f55322381f5cfbee6ce1c\",\"venue\":\"AAAI/IAAI\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1733356\",\"name\":\"G. Chowdhary\"},{\"authorId\":\"47842115\",\"name\":\"M. Liu\"},{\"authorId\":\"2338458\",\"name\":\"Robert C. Grande\"},{\"authorId\":\"144926179\",\"name\":\"T. Walsh\"},{\"authorId\":\"1713935\",\"name\":\"J. How\"},{\"authorId\":\"145006559\",\"name\":\"L. Carin\"}],\"doi\":\"10.1109/JAS.2014.7004680\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ed4418987b6020cccb73d45926fbb336eeb54ed8\",\"title\":\"Off-policy reinforcement learning with Gaussian processes\",\"url\":\"https://www.semanticscholar.org/paper/ed4418987b6020cccb73d45926fbb336eeb54ed8\",\"venue\":\"IEEE/CAA Journal of Automatica Sinica\",\"year\":2014},{\"arxivId\":\"1511.05952\",\"authors\":[{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"34660073\",\"name\":\"John Quan\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"title\":\"Prioritized Experience Replay\",\"url\":\"https://www.semanticscholar.org/paper/c6170fa90d3b2efede5a2e1660cb23e1c824f2ca\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"1609.04436\",\"authors\":[{\"authorId\":\"103809454\",\"name\":\"Mohammad Ghavamzadeh\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"}],\"doi\":\"10.1561/2200000049\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0858fb6efb0e7ef549db94813b9d6f896073d60a\",\"title\":\"Bayesian Reinforcement Learning: A Survey\",\"url\":\"https://www.semanticscholar.org/paper/0858fb6efb0e7ef549db94813b9d6f896073d60a\",\"venue\":\"Found. Trends Mach. Learn.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144301306\",\"name\":\"W. Thompson\"}],\"doi\":\"10.1093/BIOMET/25.3-4.285\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ee2cd1d17f833d3c157a1016a778c7c22af555a2\",\"title\":\"ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES\",\"url\":\"https://www.semanticscholar.org/paper/ee2cd1d17f833d3c157a1016a778c7c22af555a2\",\"venue\":\"\",\"year\":1933},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693549\",\"name\":\"S. Dzeroski\"},{\"authorId\":\"1740042\",\"name\":\"L. D. Raedt\"},{\"authorId\":\"145057418\",\"name\":\"S. Wrobel\"}],\"doi\":\"10.1145/1102351\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"df976ef18596067c4f06b714edd3e3a70b1a0fd7\",\"title\":\"Proceedings of the 22nd international conference on Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/df976ef18596067c4f06b714edd3e3a70b1a0fd7\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"2057050\",\"name\":\"Y. Engel\"}],\"doi\":\"10.7551/mitpress/7503.003.0062\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"63afef5c22277fb4bf1bc1d185f7d71c9a34e557\",\"title\":\"Bayesian Policy Gradient Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/63afef5c22277fb4bf1bc1d185f7d71c9a34e557\",\"venue\":\"NIPS\",\"year\":2006},{\"arxivId\":\"1306.0940\",\"authors\":[{\"authorId\":\"2561924\",\"name\":\"Ian Osband\"},{\"authorId\":\"145751896\",\"name\":\"D. Russo\"},{\"authorId\":\"1731282\",\"name\":\"Benjamin Van Roy\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"789783016fb708abbc061790612ebe91273c05d3\",\"title\":\"(More) Efficient Reinforcement Learning via Posterior Sampling\",\"url\":\"https://www.semanticscholar.org/paper/789783016fb708abbc061790612ebe91273c05d3\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2948478\",\"name\":\"M. Strens\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"48cce5ee49facf75eeb12832c387452424b645dd\",\"title\":\"A Bayesian Framework for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/48cce5ee49facf75eeb12832c387452424b645dd\",\"venue\":\"ICML\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"644a079073969a92674f69483c4a85679d066545\",\"title\":\"Double Q-learning\",\"url\":\"https://www.semanticscholar.org/paper/644a079073969a92674f69483c4a85679d066545\",\"venue\":\"NIPS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1807041\",\"name\":\"P. Poupart\"},{\"authorId\":\"31651045\",\"name\":\"N. Vlassis\"},{\"authorId\":\"145803385\",\"name\":\"J. Hoey\"},{\"authorId\":\"144389412\",\"name\":\"Kevin Regan\"}],\"doi\":\"10.1145/1143844.1143932\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"40e4d0eaff2ce7538ecff773bda326388a87b515\",\"title\":\"An analytic solution to discrete Bayesian reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/40e4d0eaff2ce7538ecff773bda326388a87b515\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John N. Tsitsiklis. On the convergence of optimistic po iteration\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"3:59\\u201372,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"},{\"authorId\":\"1770007\",\"name\":\"M. Goldszmidt\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e650a013eda74d78384910630dc0bf145d9c971b\",\"title\":\"Proceedings of the 16th Conference on Uncertainty in Artificial Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/e650a013eda74d78384910630dc0bf145d9c971b\",\"venue\":\"\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"M. Ghavamzadeh\"},{\"authorId\":null,\"name\":\"Y. Engel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Bayesian policy gradient algorithm\",\"url\":\"\",\"venue\":\"Advances in Neural Information Processing Systems (NIPS), pages 457\\u2013464\",\"year\":2006}],\"title\":\"Assumed Density Filtering Q-learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Temporal difference learning\",\"topicId\":\"102033\",\"url\":\"https://www.semanticscholar.org/topic/102033\"},{\"topic\":\"Computational complexity theory\",\"topicId\":\"1133\",\"url\":\"https://www.semanticscholar.org/topic/1133\"},{\"topic\":\"Artificial neural network\",\"topicId\":\"6213\",\"url\":\"https://www.semanticscholar.org/topic/6213\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Atari\",\"topicId\":\"20108\",\"url\":\"https://www.semanticscholar.org/topic/20108\"},{\"topic\":\"Nonlinear system\",\"topicId\":\"5329\",\"url\":\"https://www.semanticscholar.org/topic/5329\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Matrix regularization\",\"topicId\":\"1280835\",\"url\":\"https://www.semanticscholar.org/topic/1280835\"}],\"url\":\"https://www.semanticscholar.org/paper/2f354bc1030d9c7c749ac28f750885954c07ef62\",\"venue\":\"IJCAI\",\"year\":2019}\n"