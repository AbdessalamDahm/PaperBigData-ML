"{\"abstract\":\"Many real-world problems, such as robot control and soccer game, are naturally modeled as sparse-interaction multi-agent systems. Reutilizing single-agent knowledge in multi-agent systems with sparse interactions can greatly accelerate the multi-agent learning process. Previous works rely on bisimulation metric to define Markov decision process (MDP) similarity for controlling knowledge transfer. However, bisimulation metric is costly to compute and is not suitable for highdimensional state space problems. In this work, we propose more scalable transfer learning methods based on a novel MDP similarity concept. We start by defining the MDP similarity based on the N -step return (NSR) values of an MDP. Then, we propose two knowledge transfer methods based on deep neural networks called direct value function transfer and NSR-based value function transfer. We conduct experiments in image-based grid world, multi-agent particle environment (MPE) and Ms. Pac-Man game. The results indicate that the proposed methods can significantly accelerate multi-agent reinforcement learning and meanwhile get better asymptotic performance.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"143711913\",\"name\":\"Y. Liu\",\"url\":\"https://www.semanticscholar.org/author/143711913\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\",\"url\":\"https://www.semanticscholar.org/author/1776850\"},{\"authorId\":\"49658113\",\"name\":\"Yang Gao\",\"url\":\"https://www.semanticscholar.org/author/49658113\"},{\"authorId\":\"2519427\",\"name\":\"Yingfeng Chen\",\"url\":\"https://www.semanticscholar.org/author/2519427\"},{\"authorId\":\"3120655\",\"name\":\"Changjie Fan\",\"url\":\"https://www.semanticscholar.org/author/3120655\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2003.08839\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"1412004702\",\"name\":\"Jakob Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f5f13c6f5c659754086a7faa51d6bc60f577cd1\",\"title\":\"Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/3f5f13c6f5c659754086a7faa51d6bc60f577cd1\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2020},{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143891653\",\"name\":\"Hao Jiang\"},{\"authorId\":\"117855573\",\"name\":\"Dian-xi Shi\"},{\"authorId\":\"144406347\",\"name\":\"C. Xue\"},{\"authorId\":null,\"name\":\"Yajie Wang\"},{\"authorId\":\"2038462380\",\"name\":\"Gongju Wang\"},{\"authorId\":\"32041155\",\"name\":\"Y. Zhang\"}],\"doi\":\"10.1109/SMC42975.2020.9282974\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ac14711a4c6dbd21c3bb48b9f510a9ef321a09b0\",\"title\":\"GHGC: Goal-based Hierarchical Group Communication in Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ac14711a4c6dbd21c3bb48b9f510a9ef321a09b0\",\"venue\":\"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\",\"year\":2020},{\"arxivId\":\"1911.10715\",\"authors\":[{\"authorId\":\"93006732\",\"name\":\"Y. Liu\"},{\"authorId\":\"2209431\",\"name\":\"Weixun Wang\"},{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"2476326\",\"name\":\"X. Chen\"},{\"authorId\":\"100787817\",\"name\":\"Yang Gao\"}],\"doi\":\"10.1609/AAAI.V34I05.6211\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e5b6919ea708977f59b2992055a1546155a1445b\",\"title\":\"Multi-Agent Game Abstraction via Graph Attention Neural Network\",\"url\":\"https://www.semanticscholar.org/paper/e5b6919ea708977f59b2992055a1546155a1445b\",\"venue\":\"AAAI\",\"year\":2020}],\"corpusId\":199465751,\"doi\":\"10.24963/ijcai.2019/65\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"2a2a010175fa23dc2ec0993c1c1be979dd95954c\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"145125979\",\"name\":\"Francisco S. Melo\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":\"10.1145/1558109.1558118\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1dab9935a4e79bdfcc8ebdf89a3ce27839b6c0b6\",\"title\":\"Learning of coordination: exploiting sparse interactions in multiagent systems\",\"url\":\"https://www.semanticscholar.org/paper/1dab9935a4e79bdfcc8ebdf89a3ce27839b6c0b6\",\"venue\":\"AAMAS\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50185605\",\"name\":\"Jinhua Song\"},{\"authorId\":\"145644823\",\"name\":\"Y. Gao\"},{\"authorId\":\"40021281\",\"name\":\"H. Wang\"},{\"authorId\":\"143706345\",\"name\":\"Bo An\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"84d9cf32975989a48931e651df13af6d1b7857ca\",\"title\":\"Measuring the Distance Between Finite Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/84d9cf32975989a48931e651df13af6d1b7857ca\",\"venue\":\"AAMAS\",\"year\":2016},{\"arxivId\":\"1706.05296\",\"authors\":[{\"authorId\":\"1814162\",\"name\":\"Peter Sunehag\"},{\"authorId\":\"3276293\",\"name\":\"G. Lever\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"2873921\",\"name\":\"Nicolas Sonnerat\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c4e824a574d396803cf4677b7d0ad4e28ad54804\",\"title\":\"Value-Decomposition Networks For Cooperative Multi-Agent Learning\",\"url\":\"https://www.semanticscholar.org/paper/c4e824a574d396803cf4677b7d0ad4e28ad54804\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":\"1706.02275\",\"authors\":[{\"authorId\":\"2054294\",\"name\":\"Ryan Lowe\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2080746\",\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"title\":\"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\"url\":\"https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Francisco\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Melo and Manuela M . Veloso . Decentralized MDPs with sparse interactions\",\"url\":\"\",\"venue\":\"Proceedings of the 8 th International Conference on Autonomous Agents and Multiagent Systems\",\"year\":null},{\"arxivId\":\"1705.08926\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2b292ff89d808fba10579871591a22f1649cd039\",\"title\":\"Counterfactual Multi-Agent Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/2b292ff89d808fba10579871591a22f1649cd039\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Francisco S. Melo\"},{\"authorId\":null,\"name\":\"Manuela M. Veloso. Decentralized MDPs with sparse interactions\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Artifitial Intelligence\",\"url\":\"\",\"venue\":\"175(11):1757\\u20131789,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50004012\",\"name\":\"Christopher Meek\"},{\"authorId\":\"5747829\",\"name\":\"M. Chickering\"},{\"authorId\":\"1691828\",\"name\":\"Joseph Y. Halpern\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b540f1cfceadc57c9b4d59bb9e130f570d91d807\",\"title\":\"Proceedings of the 20th conference on Uncertainty in artificial intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b540f1cfceadc57c9b4d59bb9e130f570d91d807\",\"venue\":\"UAI 2004\",\"year\":2004},{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1776850\",\"name\":\"Yujing Hu\"},{\"authorId\":\"145644823\",\"name\":\"Y. Gao\"},{\"authorId\":\"143706345\",\"name\":\"Bo An\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"09f1e3ea9fd7ec33a5d3b9b4f52f44b08a6980f6\",\"title\":\"Learning in Multi-agent Systems with Sparse Interactions by Knowledge Transfer and Game Abstraction\",\"url\":\"https://www.semanticscholar.org/paper/09f1e3ea9fd7ec33a5d3b9b4f52f44b08a6980f6\",\"venue\":\"AAMAS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730156\",\"name\":\"Carlos Guestrin\"},{\"authorId\":\"2262405\",\"name\":\"Shobha Venkataraman\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1cc469e191aeb9cb0daec83d7e6289fafa5879c0\",\"title\":\"Context-specific multiagent coordination and planning with factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/1cc469e191aeb9cb0daec83d7e6289fafa5879c0\",\"venue\":\"AAAI/IAAI\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"52628164\",\"name\":\"\\u9e7f\\u5cf6 \\u4e45\\u55e3\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"eb98386286f7003b4745ae16564c54d7bbcaabaa\",\"title\":\"The 21st International Conference on Machine Learning (ICML) 2004 \\u53c2\\u52a0\\u5831\\u544a\",\"url\":\"https://www.semanticscholar.org/paper/eb98386286f7003b4745ae16564c54d7bbcaabaa\",\"venue\":\"\",\"year\":2005},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jelle R. Kok\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Pieter Jan\\u2019t Hoen\",\"url\":\"\",\"venue\":\"Bram Bakker, and Nikos A. Vlassis. Utile coordination: Learning interdependencies among cooperative agents. In Proceedings of the 2005 IEEE Symposium on Computational Intelligence and Games (CIG05), pages 29\\u201336,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1605.07736\",\"authors\":[{\"authorId\":\"2265067\",\"name\":\"Sainbayar Sukhbaatar\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"50295c19e177480ba3599300de1ab837cc62b08c\",\"title\":\"Learning Multiagent Communication with Backpropagation\",\"url\":\"https://www.semanticscholar.org/paper/50295c19e177480ba3599300de1ab837cc62b08c\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145489534\",\"name\":\"Phillip Taylor\"},{\"authorId\":\"144482645\",\"name\":\"N. Griffiths\"},{\"authorId\":\"33473374\",\"name\":\"L. Barakat\"},{\"authorId\":\"145116379\",\"name\":\"S. Miles\"}],\"doi\":\"10.5555/3237383\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0d075226bf44899d8d86abb8b07a2e83c18fd65d\",\"title\":\"Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems\",\"url\":\"https://www.semanticscholar.org/paper/0d075226bf44899d8d86abb8b07a2e83c18fd65d\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Yann-Micha\\u00ebl De Hauwere\"},{\"authorId\":null,\"name\":\"Peter Vrancx\"},{\"authorId\":null,\"name\":\"Ann Now\\u00e9. Solving sparse delayed coordination problems learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In International Workshop on Adaptive and Learning Agents (ALA 2011)\",\"url\":\"\",\"venue\":\"pages 114\\u2013133,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jelle R. Kok\"},{\"authorId\":null,\"name\":\"Pieter Jan\\u2019t Hoen\"},{\"authorId\":null,\"name\":\"Bram Bakker\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Nikos A . Vlassis . Sparse cooperative Q - learning\",\"url\":\"\",\"venue\":\"Proceedings of the 21 st International Conference on Machine Learning ( ICML 2004 )\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46636184\",\"name\":\"P. Peng\"},{\"authorId\":\"145001851\",\"name\":\"Quan Yuan\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"50369253\",\"name\":\"Zhenkun Tang\"},{\"authorId\":\"50468018\",\"name\":\"Haitao Long\"},{\"authorId\":null,\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"title\":\"Multiagent Bidirectionally-Coordinated Nets for Learning to Play StarCraft Combat Games\",\"url\":\"https://www.semanticscholar.org/paper/94e10392b982b9ea8dad258cd331c6b145a7ef4d\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Guestrin\"},{\"authorId\":null,\"name\":\"Shobha Venkataraman\"},{\"authorId\":null,\"name\":\"Daphne Koller. Context-specific multiagent coordination\"},{\"authorId\":null,\"name\":\"planning with factored MDPs\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 18th National Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 253\\u2013259,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50881207\",\"name\":\"Barteld Kooi\"},{\"authorId\":\"2512398\",\"name\":\"H. V. Ditmarsch\"},{\"authorId\":\"1706100\",\"name\":\"W. Hoek\"}],\"doi\":\"10.5555/3306127\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"682646e296ad9e559edf75e7f222b0a825273d54\",\"title\":\"Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems\",\"url\":\"https://www.semanticscholar.org/paper/682646e296ad9e559edf75e7f222b0a825273d54\",\"venue\":\"AAMAS 2011\",\"year\":2011},{\"arxivId\":\"1602.01783\",\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"36045539\",\"name\":\"Adri\\u00e0 Puigdom\\u00e8nech Badia\"},{\"authorId\":\"145687827\",\"name\":\"M. Mirza\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"3367786\",\"name\":\"T. Harley\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"title\":\"Asynchronous Methods for Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/69e76e16740ed69f4dc55361a3d319ac2f1293dd\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Volodymyr Mnih\"},{\"authorId\":null,\"name\":\"Koray Kavukcuoglu\"},{\"authorId\":null,\"name\":\"David Silver\"},{\"authorId\":null,\"name\":\"Andrei A Rusu\"},{\"authorId\":null,\"name\":\"Joel Veness\"},{\"authorId\":null,\"name\":\"Marc G Belle-mare\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\", Alex Graves , Martin Riedmiller , Andreas K Fidje - land , Georg Ostrovski , et al . Human - level control through deep reinforcement learning\",\"url\":\"\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398777358\",\"name\":\"J. Hern\\u00e1ndez-Orallo\"},{\"authorId\":\"47840704\",\"name\":\"Peter A. Flach\"},{\"authorId\":\"144134367\",\"name\":\"C. Ferri\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"488a3f9092093d4f52b25df4342ec79f0327a325\",\"title\":\"Proceedings of the 28th International Conference on Machine Learning\",\"url\":\"https://www.semanticscholar.org/paper/488a3f9092093d4f52b25df4342ec79f0327a325\",\"venue\":\"\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1718764\",\"name\":\"C. Yu\"},{\"authorId\":\"34994053\",\"name\":\"M. Zhang\"},{\"authorId\":\"2058048\",\"name\":\"Fenghui Ren\"},{\"authorId\":\"145104980\",\"name\":\"G. Tan\"}],\"doi\":\"10.1109/TCYB.2014.2387277\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a1d48193d5b911870da327bbbee2a74e8c36ccae\",\"title\":\"Multiagent Learning of Coordination in Loosely Coupled Multiagent Systems\",\"url\":\"https://www.semanticscholar.org/paper/a1d48193d5b911870da327bbbee2a74e8c36ccae\",\"venue\":\"IEEE Transactions on Cybernetics\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"4562073\",\"name\":\"Chris Watkins\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"5c8bb027eb65b6d250a22e9b6db22853a552ac81\",\"title\":\"Learning from delayed rewards\",\"url\":\"https://www.semanticscholar.org/paper/5c8bb027eb65b6d250a22e9b6db22853a552ac81\",\"venue\":\"\",\"year\":1989}],\"title\":\"Value Function Transfer for Deep Multi-Agent Reinforcement Learning Based on N-Step Returns\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"}],\"url\":\"https://www.semanticscholar.org/paper/2a2a010175fa23dc2ec0993c1c1be979dd95954c\",\"venue\":\"IJCAI\",\"year\":2019}\n"