"{\"abstract\":\"Reinforcement Learning (RL) deals with problems that can be modeled as a Markov Decision Process (MDP) where the transition function is unknown. In situations where an arbitrary policy \\u03c0 is already in execution and the experiences with the environment were recorded in a batch D, an RL algorithm can use D to compute a new policy \\u03c0\\u2032. However, the policy computed by traditional RL algorithms might have worse performance compared to \\u03c0. Our goal is to develop safe RL algorithms, where the agent has a high confidence that the performance of \\u03c0\\u2032 is better than the performance of \\u03c0 given D. To develop sample-efficient and safe RL algorithms we combine ideas from exploration strategies in RL with a safe policy improvement method. 1 Model-based Exploration in RL To find the optimal policy quickly, the R-max algorithm [Brafman and Tennenholtz, 2002] incentivizes the agent to explore unknown parts of the environment in early stages of the learning process. To do so, it keeps track of a set of stateaction pairs considered known: Km = {(s, a) \\u2208 S \\u00d7A | n(s, a) \\u2265 m} , (1) where n(s, a) is the number of times the agent has applied action a in state s, and m is a threshold to consider a stateaction pair known. Often, the state space S can be represented by a set of state factors X = {X1, \\u00b7 \\u00b7 \\u00b7 , X|X|} where each factor has a domain Dom(Xi). When these factors are highly independent, a Factored MDP (FMDP) can compactly represent an MDP, using a dependence function D : S \\u00d7 A\\u00d7X \\u2192 I that indicates the commonalities among different factors, where I is a set of dependency identifiers [Strehl, 2007]. The probabilistic transition function can be compactly represented: T (s\\u2032 | s, a) = |X| \\u220f i=1 P (si | D(s, a,Xi)), where si is the value of Xi in the next state s \\u2032. The factored R-max algorithm is a direct extension of Rmax for FMDPs [Guestrin et al., 2003]. It maintains an estimate of each transition component distribution and decides which state-action pairs are known or not according to these estimates. This algorithm only considers as known parts of the environment where the estimate of all transition components have been experienced enough times. In particular, given a minimum number of samples for each factor ~ m = \\u3008m1, . . . ,m|X|\\u3009 and the counters of each transition component n(j), the set of known state-action pairs is constructed as follows: K~ m = {(s, a) \\u2208 S \\u00d7A | \\u2200Xi : n(D(s, a,Xi)) \\u2265 mi}. (2) 2 Safe Policy Improvement Safe Policy Improvement (SPI) addresses the question of how to compute a new policy \\u03c0 that outperforms the behavior policy \\u03c0b with high confidence 1 \\u2212 \\u03b4, given a batch of previous interactions D and an admissible error \\u03b6. The SPI by Baseline Bootstrapping (SPIBB) framework is a model-based approach that guarantees safety by bootstrapping unknown parts of the approximated model with the behavior policy \\u03c0b [Laroche et al., 2019]. Formally, the set of bootstrapped state-action pairs Bm is the complement of the set Km (1). This way, the SPIBB algorithms guarantee to perform at least as well as the behavior policy and do not rely on a safety test, in contrast to other SPI algorithms. The policy-based \\u03a0b-SPIBB algorithm attributes the same probability to bootstrapped pairs as the behavior policy, which restricts the policy space to \\u03a0b = {\\u03c0 | \\u03c0(s, a) = \\u03c0b(s, a) : \\u2200\\u03c0 \\u2208 \\u03a0, \\u2200(s, a) \\u2208 Bm}. (3) Laroche et al. [2019] prove that if m = 2 2 log |S||A|2|S| \\u03b4 then the \\u03a0b-SPIBB algorithm is safe, where is a bound on the L1 distance between the estimated transition function and the true transition function, that depends on the precision parameter \\u03b6. The \\u03a0b-SPIBB algorithm can change the policy if a subset of the state-action pairs is well known, therefore it ca be less conservative than other SPI algorithms. Nevertheless, when the problem is described by a set of factors, m grows exponentially in the number of factors. In the next section we show that, by taking in account the independence between features, it is possible to exploit the factored representation of the problem using a minimum number of samples that is only polynomial in the number of parameters of the FMDP. Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)\",\"arxivId\":null,\"authors\":[{\"authorId\":\"51893920\",\"name\":\"Thiago D. Sim\\u00e3o\",\"url\":\"https://www.semanticscholar.org/author/51893920\"}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":199466295,\"doi\":\"10.24963/ijcai.2019/919\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"2f2e8f138e2f870663c63d338e32af139c41bf78\",\"references\":[{\"arxivId\":\"1106.1822\",\"authors\":[{\"authorId\":\"1730156\",\"name\":\"Carlos Guestrin\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"},{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"2262405\",\"name\":\"Shobha Venkataraman\"}],\"doi\":\"10.1613/jair.1000\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2430b4748c4ffe8782ae4763d327ce48f3655639\",\"title\":\"Efficient Solution Algorithms for Factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/2430b4748c4ffe8782ae4763d327ce48f3655639\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"1681946\",\"name\":\"Carlos Diuk\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"15ba832dbb1882b2c516e06cb0f8f002d16bc4cf\",\"title\":\"Efficient Structure Learning in Factored-State MDPs\",\"url\":\"https://www.semanticscholar.org/paper/15ba832dbb1882b2c516e06cb0f8f002d16bc4cf\",\"venue\":\"AAAI\",\"year\":2007},{\"arxivId\":\"1712.06924\",\"authors\":[{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"31480268\",\"name\":\"P. Trichelair\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7eb0db941fbf19857631ec1c907f2888ea308779\",\"title\":\"Safe Policy Improvement with Baseline Bootstrapping\",\"url\":\"https://www.semanticscholar.org/paper/7eb0db941fbf19857631ec1c907f2888ea308779\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30563418\",\"name\":\"A.L. Strehl\"}],\"doi\":\"10.1109/ADPRL.2007.368176\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2a7cb4ceb55f09daa6dd70cb13e2ae9e5f4994ce\",\"title\":\"Model-Based Reinforcement Learning in Factored-State MDPs\",\"url\":\"https://www.semanticscholar.org/paper/2a7cb4ceb55f09daa6dd70cb13e2ae9e5f4994ce\",\"venue\":\"2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning\",\"year\":2007},{\"arxivId\":\"1607.03842\",\"authors\":[{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"145630605\",\"name\":\"M. Petrik\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"title\":\"Safe Policy Improvement by Minimizing Robust Baseline Regret\",\"url\":\"https://www.semanticscholar.org/paper/bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681946\",\"name\":\"Carlos Diuk\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"1700606\",\"name\":\"B. Leffler\"}],\"doi\":\"10.1145/1553374.1553406\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6316daea76f71804a0f3cd1e150d4df77a1bf517\",\"title\":\"The adaptive k-meteorologists problem and its application to structure learning and feature selection in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/6316daea76f71804a0f3cd1e150d4df77a1bf517\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51893920\",\"name\":\"Thiago D. Sim\\u00e3o\"},{\"authorId\":\"1723205\",\"name\":\"M. Spaan\"}],\"doi\":\"10.1609/AAAI.V33I01.33014967\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"7f6094b905bf79af6ea27462ef8d1d7318f70503\",\"title\":\"Safe Policy Improvement with Baseline Bootstrapping in Factored Environments\",\"url\":\"https://www.semanticscholar.org/paper/7f6094b905bf79af6ea27462ef8d1d7318f70503\",\"venue\":\"AAAI\",\"year\":2019}],\"title\":\"Safe and Sample-Efficient Reinforcement Learning Algorithms for Factored Environments\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"}],\"url\":\"https://www.semanticscholar.org/paper/2f2e8f138e2f870663c63d338e32af139c41bf78\",\"venue\":\"IJCAI\",\"year\":2019}\n"