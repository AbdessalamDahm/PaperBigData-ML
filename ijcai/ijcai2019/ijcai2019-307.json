"{\"abstract\":\"Automatically generating videos according to the given text is a highly challenging task, where visual quality and semantic consistency with text are two critical issues. In existing methods, when generating a specific frame, the information in those frames generated before is not fully exploited. And an effective way to measure the semantic consistency between videos and given text remains to be established. To address these issues, we present a novel Introspective Recurrent Convolutional GAN (IRC-GAN) approach. First, we propose a recurrent transconvolutional generator, where LSTM cells are integrated with 2D transconvolutional layers. As 2D transconvolutional layers put more emphasis on the details of each frame than 3D ones, our generator takes both the definition of each video frame and temporal coherence across the whole video into consideration, and thus can generate videos with better visual quality. Second, we propose mutual-information introspection to semantically align the generated video to text. Unlike other methods simply judging whether the video and the text match or not, we further take mutual information to concretely measure the semantic consistency. In this way, our model is able to introspect the semantic distance between the generated video and the corresponding text, and try to minimize it to boost the semantic consistency. We conduct experiments on 3 datasets and compare with state-ofthe-art methods. Experimental results demonstrate the effectiveness of our IRC-GAN to generate plausible videos from given text.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"151500851\",\"name\":\"K. Deng\",\"url\":\"https://www.semanticscholar.org/author/151500851\"},{\"authorId\":\"115706403\",\"name\":\"Tianyi Fei\",\"url\":\"https://www.semanticscholar.org/author/115706403\"},{\"authorId\":\"100599451\",\"name\":\"Xin Huang\",\"url\":\"https://www.semanticscholar.org/author/100599451\"},{\"authorId\":\"1704081\",\"name\":\"Y. Peng\",\"url\":\"https://www.semanticscholar.org/author/1704081\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2009.08616\",\"authors\":[{\"authorId\":\"152253423\",\"name\":\"Y. Yu\"},{\"authorId\":\"152364635\",\"name\":\"A. Srivastava\"},{\"authorId\":\"1753278\",\"name\":\"R. Shah\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f7626913b8247f2b33ca79afef548dc765b030ba\",\"title\":\"Conditional Hybrid GAN for Sequence Generation\",\"url\":\"https://www.semanticscholar.org/paper/f7626913b8247f2b33ca79afef548dc765b030ba\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.03182\",\"authors\":[{\"authorId\":\"2046142\",\"name\":\"S. Han\"},{\"authorId\":\"32545338\",\"name\":\"Siqu Long\"},{\"authorId\":\"39209233\",\"name\":\"Siwen Luo\"},{\"authorId\":\"1990752926\",\"name\":\"Kunze Wang\"},{\"authorId\":\"48422087\",\"name\":\"Josiah Poon\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ce03885afb09eee11dfcea7eadd59153f2f9633b\",\"title\":\"VICTR: Visual Information Captured Text Representation for Text-to-Image Multimodal Tasks\",\"url\":\"https://www.semanticscholar.org/paper/ce03885afb09eee11dfcea7eadd59153f2f9633b\",\"venue\":\"COLING\",\"year\":2020},{\"arxivId\":\"2011.11221\",\"authors\":[{\"authorId\":\"1643985055\",\"name\":\"Xianjin Chao\"},{\"authorId\":\"83163597\",\"name\":\"Yanrui Bin\"},{\"authorId\":\"2061528\",\"name\":\"Wenqing Chu\"},{\"authorId\":\"1491776992\",\"name\":\"Xuan Cao\"},{\"authorId\":\"83103152\",\"name\":\"Yanhao Ge\"},{\"authorId\":\"1978245\",\"name\":\"Chengjie Wang\"},{\"authorId\":\"49298244\",\"name\":\"Jilin Li\"},{\"authorId\":\"1835006\",\"name\":\"Feiyue Huang\"},{\"authorId\":\"39109691\",\"name\":\"H. Leung\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5a0be8828ada98ed3f91415ebd48638d02377a56\",\"title\":\"Adversarial Refinement Network for Human Motion Prediction\",\"url\":\"https://www.semanticscholar.org/paper/5a0be8828ada98ed3f91415ebd48638d02377a56\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":199466318,\"doi\":\"10.24963/ijcai.2019/307\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"cb7ec8735c831fe34ef709c16c9569a1f1021aab\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jingkuan Song\"},{\"authorId\":null,\"name\":\"Jingqiu Zhang\"},{\"authorId\":null,\"name\":\"Lianli Gao\"},{\"authorId\":null,\"name\":\"Xianglong Liu\"},{\"authorId\":null,\"name\":\"Heng Tao Shen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Mike Schuster and Kuldip K Paliwal . Bidirectional recurrent neural networks\",\"url\":\"\",\"venue\":\"IEEE Transactions on Signal Processing\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Masaki Saito\"},{\"authorId\":null,\"name\":\"Eiichi Matsumoto\"},{\"authorId\":null,\"name\":\"Shunta Saito. Temporal generative adversarial nets with s clipping\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the IEEE International Conference on Computer Vision\",\"url\":\"\",\"venue\":\"pages 2830\\u2013 2839,\",\"year\":2017},{\"arxivId\":\"1804.08264\",\"authors\":[{\"authorId\":\"3202968\",\"name\":\"Yingwei Pan\"},{\"authorId\":\"3430743\",\"name\":\"Zhaofan Qiu\"},{\"authorId\":\"145690248\",\"name\":\"Ting Yao\"},{\"authorId\":\"7179232\",\"name\":\"H. Li\"},{\"authorId\":\"144025741\",\"name\":\"T. Mei\"}],\"doi\":\"10.1145/3123266.3127905\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"883224c3b28b0563a393746066738f52e6fcc70d\",\"title\":\"To Create What You Tell: Generating Videos from Captions\",\"url\":\"https://www.semanticscholar.org/paper/883224c3b28b0563a393746066738f52e6fcc70d\",\"venue\":\"ACM Multimedia\",\"year\":2017},{\"arxivId\":\"1707.04993\",\"authors\":[{\"authorId\":\"145582202\",\"name\":\"S. Tulyakov\"},{\"authorId\":\"39793900\",\"name\":\"Ming-Yu Liu\"},{\"authorId\":\"144434220\",\"name\":\"X. Yang\"},{\"authorId\":\"1690538\",\"name\":\"J. Kautz\"}],\"doi\":\"10.1109/CVPR.2018.00165\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"title\":\"MoCoGAN: Decomposing Motion and Content for Video Generation\",\"url\":\"https://www.semanticscholar.org/paper/e76edb86f270c3a77ed9f5a1e1b305461f36f96f\",\"venue\":\"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":\"1804.10073\",\"authors\":[{\"authorId\":\"2439211\",\"name\":\"Chenrui Zhang\"},{\"authorId\":\"143753918\",\"name\":\"Y. Peng\"}],\"doi\":\"10.24963/ijcai.2018/157\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"090873e83594fc7396fb81b64f4ec38a05151304\",\"title\":\"Visual Data Synthesis via GAN for Zero-Shot Video Classification\",\"url\":\"https://www.semanticscholar.org/paper/090873e83594fc7396fb81b64f4ec38a05151304\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jingkuan Song\"},{\"authorId\":null,\"name\":\"Jingqiu Zhang\"},{\"authorId\":null,\"name\":\"Lianli Gao\"},{\"authorId\":null,\"name\":\"Xianglong Liu\"},{\"authorId\":null,\"name\":\"Heng Tao Shen. Dual conditional gans for face aging\"},{\"authorId\":null,\"name\":\"rejuvenation\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In IJCAI\",\"url\":\"\",\"venue\":\"pages 899\\u2013 905,\",\"year\":2018},{\"arxivId\":\"1807.01659\",\"authors\":[{\"authorId\":\"51056524\",\"name\":\"Guang-Yuan Hao\"},{\"authorId\":\"14878299\",\"name\":\"Hong-Xing Yu\"},{\"authorId\":\"3333315\",\"name\":\"W. Zheng\"}],\"doi\":\"10.24963/ijcai.2018/306\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fafc99f9e1e545c99bcda1cd7f6f49e51f2bb124\",\"title\":\"MIXGAN: Learning Concepts from Different Domains for Mixture Generation\",\"url\":\"https://www.semanticscholar.org/paper/fafc99f9e1e545c99bcda1cd7f6f49e51f2bb124\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"FaceCerts Darko Kirovski\"},{\"authorId\":\"1698689\",\"name\":\"N. Jojic\"}],\"doi\":\"10.1109/tsp.2004.824755\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a37293c27cd7cc31f0e6a3b7171cc826285def8e\",\"title\":\"Ieee Transactions on Signal Processing: Supplement on Secure Media 1 Facecerts Ieee Transactions on Signal Processing: Supplement on Secure Media 2\",\"url\":\"https://www.semanticscholar.org/paper/a37293c27cd7cc31f0e6a3b7171cc826285def8e\",\"venue\":\"\",\"year\":2003},{\"arxivId\":\"1602.05110\",\"authors\":[{\"authorId\":\"2903841\",\"name\":\"D. Im\"},{\"authorId\":\"32821535\",\"name\":\"C. D. Kim\"},{\"authorId\":\"145079150\",\"name\":\"Hui Jiang\"},{\"authorId\":\"1710604\",\"name\":\"R. Memisevic\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"32c09d2933a4638b034343f9be20544dacf6031f\",\"title\":\"Generating images with recurrent adversarial networks\",\"url\":\"https://www.semanticscholar.org/paper/32c09d2933a4638b034343f9be20544dacf6031f\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1612.03242\",\"authors\":[{\"authorId\":\"120811666\",\"name\":\"Han Zhang\"},{\"authorId\":\"145017761\",\"name\":\"Tao Xu\"},{\"authorId\":\"47893312\",\"name\":\"Hongsheng Li\"}],\"doi\":\"10.1109/ICCV.2017.629\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921\",\"title\":\"StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks\",\"url\":\"https://www.semanticscholar.org/paper/ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921\",\"venue\":\"2017 IEEE International Conference on Computer Vision (ICCV)\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Matthew D Zeiler\"},{\"authorId\":null,\"name\":\"Dilip Krishnan\"},{\"authorId\":null,\"name\":\"Graham W Taylor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Rob Fergus\",\"url\":\"\",\"venue\":\"Deconvolutional networks.\",\"year\":2010},{\"arxivId\":\"1511.01432\",\"authors\":[{\"authorId\":\"2555924\",\"name\":\"Andrew M. Dai\"},{\"authorId\":\"2827616\",\"name\":\"Quoc V. Le\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4aa9f5150b46320f534de4747a2dd0cd7f3fe292\",\"title\":\"Semi-supervised Sequence Learning\",\"url\":\"https://www.semanticscholar.org/paper/4aa9f5150b46320f534de4747a2dd0cd7f3fe292\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":\"1805.02456\",\"authors\":[{\"authorId\":\"34443348\",\"name\":\"Xudong Mao\"},{\"authorId\":\"1930238\",\"name\":\"Qing Li\"}],\"doi\":\"10.24963/ijcai.2018/354\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"72887cb810616b5696814cd14b7e892af742d9b2\",\"title\":\"Unpaired Multi-Domain Image Generation via Regularized Conditional GANs\",\"url\":\"https://www.semanticscholar.org/paper/72887cb810616b5696814cd14b7e892af742d9b2\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1606.03657\",\"authors\":[{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"},{\"authorId\":\"144581158\",\"name\":\"Yan Duan\"},{\"authorId\":\"3127100\",\"name\":\"Rein Houthooft\"},{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"35da0a2001eea88486a5de677ab97868c93d0824\",\"title\":\"InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\",\"url\":\"https://www.semanticscholar.org/paper/35da0a2001eea88486a5de677ab97868c93d0824\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Chenrui Zhang\"},{\"authorId\":null,\"name\":\"Yuxin Peng. Stacking vae\"},{\"authorId\":null,\"name\":\"gan for context-aware text-to-image generation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 2018 IEEE Fourth International Conference on Multimedia Big Data (BigMM)\",\"url\":\"\",\"venue\":\"pages 1\\u20135. IEEE,\",\"year\":2018},{\"arxivId\":\"1806.11191\",\"authors\":[{\"authorId\":\"6812347\",\"name\":\"Yu Tian\"},{\"authorId\":\"144152346\",\"name\":\"Xi Peng\"},{\"authorId\":\"48096253\",\"name\":\"Long Zhao\"},{\"authorId\":\"1753384\",\"name\":\"S. Zhang\"},{\"authorId\":\"1711560\",\"name\":\"D. Metaxas\"}],\"doi\":\"10.24963/ijcai.2018/131\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8f89d6ef6c9e3ee4883595f94b38ab01d05d7b21\",\"title\":\"CR-GAN: Learning Complete Representations for Multi-view Generation\",\"url\":\"https://www.semanticscholar.org/paper/8f89d6ef6c9e3ee4883595f94b38ab01d05d7b21\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":\"1805.02481\",\"authors\":[{\"authorId\":\"145966306\",\"name\":\"D. Park\"},{\"authorId\":\"41020567\",\"name\":\"Seungjoo Yoo\"},{\"authorId\":\"41019737\",\"name\":\"Hyojin Bahng\"},{\"authorId\":\"1795455\",\"name\":\"J. Choo\"},{\"authorId\":\"5166698\",\"name\":\"N. Park\"}],\"doi\":\"10.24963/ijcai.2018/122\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2d03d53580b6119d826d60d085c54a0ab9aaa9f3\",\"title\":\"MEGAN: Mixture of Experts of Generative Adversarial Networks for Multimodal Image Generation\",\"url\":\"https://www.semanticscholar.org/paper/2d03d53580b6119d826d60d085c54a0ab9aaa9f3\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Gaurav Mittal\"},{\"authorId\":null,\"name\":\"Tanya Marwah\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and Vineeth N Balasubramanian\",\"url\":\"\",\"venue\":\"Sync-draw: Automatic video generation using deep recurrent attentive architectures.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3739416\",\"name\":\"Y. Ni\"},{\"authorId\":\"145144398\",\"name\":\"Dandan Song\"},{\"authorId\":null,\"name\":\"Xi Zhang\"},{\"authorId\":\"46477167\",\"name\":\"H. Wu\"},{\"authorId\":\"3000498\",\"name\":\"Lejian Liao\"}],\"doi\":\"10.24963/ijcai.2018/359\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"204857ccc30dc338bbf96769712bf49dfd7c2592\",\"title\":\"CAGAN: Consistent Adversarial Training Enhanced GANs\",\"url\":\"https://www.semanticscholar.org/paper/204857ccc30dc338bbf96769712bf49dfd7c2592\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2346105\",\"name\":\"Jingkuan Song\"},{\"authorId\":\"3145905\",\"name\":\"Jingqiu Zhang\"},{\"authorId\":\"2671321\",\"name\":\"L. Gao\"},{\"authorId\":\"6820648\",\"name\":\"Xianglong Liu\"},{\"authorId\":\"1724393\",\"name\":\"H. Shen\"}],\"doi\":\"10.24963/ijcai.2018/125\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"93e962f8886eae13b02ad2aa98bdedfbd7e68709\",\"title\":\"Dual Conditional GANs for Face Aging and Rejuvenation\",\"url\":\"https://www.semanticscholar.org/paper/93e962f8886eae13b02ad2aa98bdedfbd7e68709\",\"venue\":\"IJCAI\",\"year\":2018}],\"title\":\"IRC-GAN: Introspective Recurrent Convolutional GAN for Text-to-video Generation\",\"topics\":[{\"topic\":\"Internet Relay Chat\",\"topicId\":\"38563\",\"url\":\"https://www.semanticscholar.org/topic/38563\"}],\"url\":\"https://www.semanticscholar.org/paper/cb7ec8735c831fe34ef709c16c9569a1f1021aab\",\"venue\":\"IJCAI\",\"year\":2019}\n"