"{\"abstract\":\"Drawing an inspiration from behavioral studies of human decision making, we propose here a general parametric framework for a reinforcement learning problem, which extends the standard Q-learning approach to incorporate a two-stream framework of reward processing with biases biologically associated with several neurological and psychiatric conditions, including Parkinson's and Alzheimer's diseases, attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain. For AI community, the development of agents that react differently to different types of rewards can enable us to understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems. Moreover, from the behavioral modeling perspective, our parametric framework can be viewed as a first step towards a unifying computational model capturing reward processing abnormalities across multiple mental conditions and user preferences in long-term recommendation systems.\",\"arxivId\":\"1906.12350\",\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\",\"url\":\"https://www.semanticscholar.org/author/5320216\"},{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\",\"url\":\"https://www.semanticscholar.org/author/1744675\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\",\"url\":\"https://www.semanticscholar.org/author/40193335\"}],\"citationVelocity\":4,\"citations\":[{\"arxivId\":\"2007.06368\",\"authors\":[{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"148155763\",\"name\":\"Sohini Upadhyay\"},{\"authorId\":\"2216967\",\"name\":\"Y. Khazaeni\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e71fdea2ebc9a90596a9add3232c511f9bb6ed3f\",\"title\":\"Contextual Bandit with Missing Rewards\",\"url\":\"https://www.semanticscholar.org/paper/e71fdea2ebc9a90596a9add3232c511f9bb6ed3f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.06580\",\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\"},{\"authorId\":\"1397329374\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"caafdd82583add2fe17cb362897498e9cd547bc7\",\"title\":\"Online Learning in Iterated Prisoner's Dilemma to Mimic Human Behavior\",\"url\":\"https://www.semanticscholar.org/paper/caafdd82583add2fe17cb362897498e9cd547bc7\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.11286\",\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\"},{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"40159880\",\"name\":\"Jenna Reinen\"},{\"authorId\":\"113766340\",\"name\":\"I. Rish\"}],\"doi\":\"10.5555/3398761.3398850\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"093553f203257cffa6a8509cada9d70ed6501499\",\"title\":\"A Story of Two Streams: Reinforcement Learning Models from Human Behavior and Neuropsychiatry\",\"url\":\"https://www.semanticscholar.org/paper/093553f203257cffa6a8509cada9d70ed6501499\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36589297\",\"name\":\"M. Gallagher\"},{\"authorId\":\"3193106\",\"name\":\"Nour Moustafa\"},{\"authorId\":\"2409710\",\"name\":\"E. Lakshika\"}],\"doi\":\"10.1007/978-3-030-64984-5\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f9aff7c7559cc4844a3f965e3b29f4b233c9a34d\",\"title\":\"AI 2020: Advances in Artificial Intelligence: 33rd Australasian Joint Conference, AI 2020, Canberra, ACT, Australia, November 29\\u201330, 2020, Proceedings\",\"url\":\"https://www.semanticscholar.org/paper/f9aff7c7559cc4844a3f965e3b29f4b233c9a34d\",\"venue\":\"Australasian Conference on Artificial Intelligence\",\"year\":2020},{\"arxivId\":\"2010.11413\",\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\"},{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5f5ba116378bce22bb4f36f5ffa9b41cd1da409e\",\"title\":\"Predicting Human Decision Making in Psychological Tasks with Recurrent Neural Networks\",\"url\":\"https://www.semanticscholar.org/paper/5f5ba116378bce22bb4f36f5ffa9b41cd1da409e\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.08457\",\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\"}],\"doi\":\"10.1007/978-3-030-64984-5_32\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a0bd78ba375590d753b82cf93de3c83e0bf33424\",\"title\":\"Online Semi-Supervised Learning in Contextual Bandits with Episodic Reward\",\"url\":\"https://www.semanticscholar.org/paper/a0bd78ba375590d753b82cf93de3c83e0bf33424\",\"venue\":\"Australasian Conference on Artificial Intelligence\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1466474372\",\"name\":\"Katleen Blanchet\"},{\"authorId\":\"46691521\",\"name\":\"A. Bouzeghoub\"},{\"authorId\":\"3002437\",\"name\":\"Selma Kchir\"},{\"authorId\":\"50345041\",\"name\":\"O. Lebec\"}],\"doi\":\"10.1109/SMC42975.2020.9283469\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5fbe945942c891fefce7902b2b2834637450753e\",\"title\":\"How to Guide Humans Towards Skills Improvement in Physical Human-Robot Collaboration Using Reinforcement Learning?\",\"url\":\"https://www.semanticscholar.org/paper/5fbe945942c891fefce7902b2b2834637450753e\",\"venue\":\"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\"},{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"1600524653\",\"name\":\"Jenna Reinen\"},{\"authorId\":\"113766340\",\"name\":\"I. Rish\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"224d766e3e7778929b0474ff4d0c59484bd437ab\",\"title\":\"Unified Models of Human Behavioral Agents in Bandits, Contextual Bandits and RL\",\"url\":\"https://www.semanticscholar.org/paper/224d766e3e7778929b0474ff4d0c59484bd437ab\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2005.04544\",\"authors\":[{\"authorId\":\"5320216\",\"name\":\"Baihan Lin\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\"},{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"1600524653\",\"name\":\"Jenna Reinen\"},{\"authorId\":\"1933655979\",\"name\":\"Irina Rish\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"f6b40720bdde9dc8e4fa2b84c75e970549bf1d74\",\"title\":\"An Empirical Study of Human Behavioral Agents in Bandits, Contextual Bandits and Reinforcement Learning.\",\"url\":\"https://www.semanticscholar.org/paper/f6b40720bdde9dc8e4fa2b84c75e970549bf1d74\",\"venue\":\"\",\"year\":2020}],\"corpusId\":195750844,\"doi\":\"10.24963/ijcai.2019/913\",\"fieldsOfStudy\":[\"Computer Science\",\"Biology\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"a1b80c4070f1344c766468fd50529f39764e3d3f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1015330.1015430\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"title\":\"Apprenticeship learning via inverse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49363606\",\"name\":\"M. Frank\"},{\"authorId\":\"7019179\",\"name\":\"L. Seeberger\"},{\"authorId\":\"1405345070\",\"name\":\"R. O'Reilly\"}],\"doi\":\"10.1126/SCIENCE.1102941\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"764fd3bc54709bc77c35df7d081538809065bd19\",\"title\":\"By Carrot or by Stick: Cognitive Reinforcement Learning in Parkinsonism\",\"url\":\"https://www.semanticscholar.org/paper/764fd3bc54709bc77c35df7d081538809065bd19\",\"venue\":\"Science\",\"year\":2004},{\"arxivId\":\"0912.3995\",\"authors\":[{\"authorId\":\"2157110\",\"name\":\"N. Srinivas\"},{\"authorId\":\"145343838\",\"name\":\"Andreas Krause\"},{\"authorId\":\"144695232\",\"name\":\"Sham M. Kakade\"},{\"authorId\":\"1680574\",\"name\":\"Matthias W. Seeger\"}],\"doi\":\"10.1109/TIT.2011.2182033\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0c8413ab8de0c1b8f2e86402b8d737d94371610f\",\"title\":\"Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design\",\"url\":\"https://www.semanticscholar.org/paper/0c8413ab8de0c1b8f2e86402b8d737d94371610f\",\"venue\":\"ICML\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145776101\",\"name\":\"A. Redish\"},{\"authorId\":\"35542888\",\"name\":\"S. Jensen\"},{\"authorId\":\"49158070\",\"name\":\"A. Johnson\"},{\"authorId\":\"1422832631\",\"name\":\"Zeb L. Kurth-Nelson\"}],\"doi\":\"10.1037/0033-295X.114.3.784\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"670c954c0f151564b6c0caa4a6dbc8ee010bb8ba\",\"title\":\"Reconciling reinforcement learning models with behavioral extinction and renewal: implications for addiction, relapse, and problem gambling.\",\"url\":\"https://www.semanticscholar.org/paper/670c954c0f151564b6c0caa4a6dbc8ee010bb8ba\",\"venue\":\"Psychological review\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2354198\",\"name\":\"D. Perry\"},{\"authorId\":\"3185619\",\"name\":\"J. Kramer\"}],\"doi\":\"10.1080/13554794.2013.873063\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"aece5c3104e31fc9d3a06dc9c9092479f74e5e05\",\"title\":\"Reward processing in neurodegenerative disease\",\"url\":\"https://www.semanticscholar.org/paper/aece5c3104e31fc9d3a06dc9c9092479f74e5e05\",\"venue\":\"Neurocase\",\"year\":2015},{\"arxivId\":\"1706.02897\",\"authors\":[{\"authorId\":\"1744675\",\"name\":\"Djallel Bouneffouf\"},{\"authorId\":\"2109771\",\"name\":\"Irina Rish\"},{\"authorId\":\"40193335\",\"name\":\"G. Cecchi\"}],\"doi\":\"10.1007/978-3-319-63703-7_22\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"0779e7c1aae692104e3ad402b66d89df53c31f60\",\"title\":\"Bandit Models of Human Behavior: Reward Processing in Mental Disorders\",\"url\":\"https://www.semanticscholar.org/paper/0779e7c1aae692104e3ad402b66d89df53c31f60\",\"venue\":\"AGI\",\"year\":2017}],\"title\":\"Split Q Learning: Reinforcement Learning with Two-Stream Rewards\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Q-learning\",\"topicId\":\"17301\",\"url\":\"https://www.semanticscholar.org/topic/17301\"},{\"topic\":\"Computational model\",\"topicId\":\"6579\",\"url\":\"https://www.semanticscholar.org/topic/6579\"},{\"topic\":\"Recommender system\",\"topicId\":\"12428\",\"url\":\"https://www.semanticscholar.org/topic/12428\"},{\"topic\":\"Auditory processing disorder\",\"topicId\":\"394106\",\"url\":\"https://www.semanticscholar.org/topic/394106\"},{\"topic\":\"Modeling perspective\",\"topicId\":\"304584\",\"url\":\"https://www.semanticscholar.org/topic/304584\"},{\"topic\":\"Behavioral modeling\",\"topicId\":\"84856\",\"url\":\"https://www.semanticscholar.org/topic/84856\"},{\"topic\":\"User (computing)\",\"topicId\":\"38845\",\"url\":\"https://www.semanticscholar.org/topic/38845\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Computation\",\"topicId\":\"339\",\"url\":\"https://www.semanticscholar.org/topic/339\"}],\"url\":\"https://www.semanticscholar.org/paper/a1b80c4070f1344c766468fd50529f39764e3d3f\",\"venue\":\"IJCAI\",\"year\":2019}\n"