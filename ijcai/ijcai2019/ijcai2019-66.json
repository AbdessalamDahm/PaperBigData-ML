"{\"abstract\":\"In this paper, we present exploitability descent, a new algorithm to compute approximate equilibria in two-player zero-sum extensive-form games with imperfect information, by direct policy optimization against worst-case opponents. We prove that when following this optimization, the exploitability of a player's strategy converges asymptotically to zero, and hence when both players employ this optimization, the joint policies converge to a Nash equilibrium. Unlike fictitious play (XFP) and counterfactual regret minimization (CFR), our convergence result pertains to the policies being optimized rather than the average policies. Our experiments demonstrate convergence rates comparable to XFP and CFR in four benchmark games in the tabular case. Using function approximation, we find that our algorithm outperforms the tabular version in two of the games, which, to the best of our knowledge, is the first such result in imperfect information games among this class of algorithms.\",\"arxivId\":\"1903.05614\",\"authors\":[{\"authorId\":\"49860549\",\"name\":\"Edward Lockhart\",\"url\":\"https://www.semanticscholar.org/author/49860549\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\",\"url\":\"https://www.semanticscholar.org/author/1975889\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\",\"url\":\"https://www.semanticscholar.org/author/3422031\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\",\"url\":\"https://www.semanticscholar.org/author/143783339\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\",\"url\":\"https://www.semanticscholar.org/author/2551974\"},{\"authorId\":\"2196718\",\"name\":\"Finbarr Timbers\",\"url\":\"https://www.semanticscholar.org/author/2196718\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\",\"url\":\"https://www.semanticscholar.org/author/2274623\"}],\"citationVelocity\":9,\"citations\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"47785961\",\"name\":\"J. Li\"},{\"authorId\":\"10764638\",\"name\":\"Yichi Zhou\"},{\"authorId\":\"32453998\",\"name\":\"Tongzheng Ren\"},{\"authorId\":\"1557387379\",\"name\":\"Jun Zhu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7d421c8bb0666d492f39d8323a0ff759b3b3815e\",\"title\":\"Exploration Analysis in Finite-Horizon Turn-based Stochastic Games\",\"url\":\"https://www.semanticscholar.org/paper/7d421c8bb0666d492f39d8323a0ff759b3b3815e\",\"venue\":\"UAI\",\"year\":2020},{\"arxivId\":\"1812.10607\",\"authors\":[{\"authorId\":null,\"name\":\"Hui Li\"},{\"authorId\":\"47703529\",\"name\":\"Kailiang Hu\"},{\"authorId\":\"66415006\",\"name\":\"Zhibang Ge\"},{\"authorId\":\"143872550\",\"name\":\"Tao Jiang\"},{\"authorId\":\"40612590\",\"name\":\"Yuan Qi\"},{\"authorId\":\"1779453\",\"name\":\"L. Song\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"b7a30e04aeb0ee7bb1a739d94c5dd785d00f3b25\",\"title\":\"Double Neural Counterfactual Regret Minimization\",\"url\":\"https://www.semanticscholar.org/paper/b7a30e04aeb0ee7bb1a739d94c5dd785d00f3b25\",\"venue\":\"ICLR\",\"year\":2020},{\"arxivId\":\"1911.11928\",\"authors\":[{\"authorId\":\"36567789\",\"name\":\"R. Qin\"},{\"authorId\":\"1432234123\",\"name\":\"Jing-Cheng Pang\"},{\"authorId\":\"144705629\",\"name\":\"Yang Yu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0c60f8f72b59548367ceccb2dbafb558fb28f9cb\",\"title\":\"Improving Fictitious Play Reinforcement Learning with Expanding Models\",\"url\":\"https://www.semanticscholar.org/paper/0c60f8f72b59548367ceccb2dbafb558fb28f9cb\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1897926\",\"name\":\"D. Hennes\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":\"51135177\",\"name\":\"Paavo Parmas\"},{\"authorId\":\"1400818648\",\"name\":\"Edgar A. Du\\u00e9\\u00f1ez-Guzm\\u00e1n\"},{\"authorId\":\"50772704\",\"name\":\"K. Tuyls\"}],\"doi\":\"10.5555/3398761.3398822\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4f7cfa0add7b20e2043373dc17604cc9cd17ff23\",\"title\":\"Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/4f7cfa0add7b20e2043373dc17604cc9cd17ff23\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"1912.02967\",\"authors\":[{\"authorId\":\"1414056326\",\"name\":\"Ryan D'Orazio\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"30782010\",\"name\":\"J. R. Wright\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":\"10.5555/3398761.3398805\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6064fc4dd0480b3609441f05410127cb3ee4a437\",\"title\":\"Alternative Function Approximation Parameterizations for Solving Games: An Analysis of f-Regression Counterfactual Regret Minimization\",\"url\":\"https://www.semanticscholar.org/paper/6064fc4dd0480b3609441f05410127cb3ee4a437\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"1911.07960\",\"authors\":[{\"authorId\":\"1757357\",\"name\":\"Tristan Cazenave\"},{\"authorId\":\"2003545\",\"name\":\"V. Ventos\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6c4b3079ee1ee36117d7fead733e606411362f4b\",\"title\":\"The {\\\\alpha}{\\\\mu} Search Algorithm for the Game of Bridge.\",\"url\":\"https://www.semanticscholar.org/paper/6c4b3079ee1ee36117d7fead733e606411362f4b\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2011.00583\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2000281109\",\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"title\":\"An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30660476\",\"name\":\"Jesse Clifton\"},{\"authorId\":null,\"name\":\"Maxime Rich\\u00e9\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7b552922a6423c3b8ad62bb18dc2d0a6eaf823de\",\"title\":\"Towards Cooperative Learning Equilibrium in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7b552922a6423c3b8ad62bb18dc2d0a6eaf823de\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3806712\",\"name\":\"C. Daskalakis\"},{\"authorId\":\"26198391\",\"name\":\"Dylan J. Foster\"},{\"authorId\":\"3348246\",\"name\":\"Noah Golowich\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d4f046956bc52715225ad2f190317703a2abf2a\",\"title\":\"Independent Policy Gradient Methods for Competitive Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8d4f046956bc52715225ad2f190317703a2abf2a\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2004.10638\",\"authors\":[{\"authorId\":\"144940197\",\"name\":\"O. Petrova\"},{\"authorId\":\"1977282\",\"name\":\"K. Durkota\"},{\"authorId\":\"1395049877\",\"name\":\"Galina Alperovich\"},{\"authorId\":\"1749949\",\"name\":\"K. Hor\\u00e1k\"},{\"authorId\":\"73195875\",\"name\":\"Micha\\u0142 Najman\"},{\"authorId\":\"1807287\",\"name\":\"B. Bosansk\\u00fd\"},{\"authorId\":\"1759154\",\"name\":\"V. Lis\\u00fd\"}],\"doi\":\"10.5555/3398761.3399044\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"03fdf61c7d7aeeca06f572399ca455a2c4e69bf6\",\"title\":\"Discovering Imperfectly Observable Adversarial Actions using Anomaly Detection\",\"url\":\"https://www.semanticscholar.org/paper/03fdf61c7d7aeeca06f572399ca455a2c4e69bf6\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":null,\"authors\":[],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"1faf0db13482d3adbf91abd8f9b74c45194a7750\",\"title\":\"DOUBLE NEURAL COUNTERFACTUAL REGRET MINI-\",\"url\":\"https://www.semanticscholar.org/paper/1faf0db13482d3adbf91abd8f9b74c45194a7750\",\"venue\":\"\",\"year\":2019},{\"arxivId\":\"2006.13085\",\"authors\":[{\"authorId\":\"38921168\",\"name\":\"Nelson Vadori\"},{\"authorId\":\"27411264\",\"name\":\"S. Ganesh\"},{\"authorId\":\"9391384\",\"name\":\"P. Reddy\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f6085c63a908dfbdf53e46fe80e2871f5a8df03b\",\"title\":\"Calibration of Shared Equilibria in General Sum Partially Observable Markov Games\",\"url\":\"https://www.semanticscholar.org/paper/f6085c63a908dfbdf53e46fe80e2871f5a8df03b\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bc. Michal Najman\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"fa412335490cf139beb60f29b8ab28ed85fc436d\",\"title\":\"Adversarial Machine Learning for Detecting Malicious Behavior in Network Security\",\"url\":\"https://www.semanticscholar.org/paper/fa412335490cf139beb60f29b8ab28ed85fc436d\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"30660476\",\"name\":\"Jesse Clifton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3c2f93571aee45ffddf69b091bd390ff10000961\",\"title\":\"Towards Cooperative Learning Equilibrium in Reinforcement Learning (WORKING DRAFT)\",\"url\":\"https://www.semanticscholar.org/paper/3c2f93571aee45ffddf69b091bd390ff10000961\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"2012.01870\",\"authors\":[{\"authorId\":\"2026957819\",\"name\":\"Weiming Liu\"},{\"authorId\":\"2025994844\",\"name\":\"Bin Li\"},{\"authorId\":\"1810053\",\"name\":\"J. Togelius\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"281bfa43b956fc6a6ca1aaf7510d73ba5a364d77\",\"title\":\"Model-free Neural Counterfactual Regret Minimization with Bootstrap Learning\",\"url\":\"https://www.semanticscholar.org/paper/281bfa43b956fc6a6ca1aaf7510d73ba5a364d77\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1007/s10458-019-09421-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"390364c3986d05ad71a40a967b9cc12aa30e4305\",\"title\":\"A survey and critique of multiagent deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/390364c3986d05ad71a40a967b9cc12aa30e4305\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2019},{\"arxivId\":\"2008.12234\",\"authors\":[{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2196718\",\"name\":\"Finbarr Timbers\"},{\"authorId\":\"152705817\",\"name\":\"M. Schmid\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":\"50599082\",\"name\":\"J. Schultz\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"},{\"authorId\":\"50772704\",\"name\":\"K. Tuyls\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c53bd6d868efc98fb60fe7baef39069d0fd55349\",\"title\":\"The Advantage Regret-Matching Actor-Critic\",\"url\":\"https://www.semanticscholar.org/paper/c53bd6d868efc98fb60fe7baef39069d0fd55349\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1906.00190\",\"authors\":[{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"1897926\",\"name\":\"D. Hennes\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"2949e41168873b03ab40e929ef51eba65d8d073e\",\"title\":\"Neural Replicator Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/2949e41168873b03ab40e929ef51eba65d8d073e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1810.05587\",\"authors\":[{\"authorId\":\"1400326437\",\"name\":\"Pablo Hernandez-Leal\"},{\"authorId\":\"35224631\",\"name\":\"Bilal Kartal\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"}],\"doi\":\"10.1007/s10458-019-09421-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b\",\"title\":\"Is multiagent deep reinforcement learning the answer or the question? A brief survey\",\"url\":\"https://www.semanticscholar.org/paper/3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2006.04635\",\"authors\":[{\"authorId\":\"52442941\",\"name\":\"T. Anthony\"},{\"authorId\":\"3241600\",\"name\":\"Tom Eccles\"},{\"authorId\":\"46342035\",\"name\":\"A. Tacchetti\"},{\"authorId\":\"50458432\",\"name\":\"J\\u00e1nos Kram\\u00e1r\"},{\"authorId\":\"8616871\",\"name\":\"I. Gemp\"},{\"authorId\":\"2841850\",\"name\":\"T. Hudson\"},{\"authorId\":\"1739350714\",\"name\":\"Nicolas Porcel\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"145867382\",\"name\":\"R. Everett\"},{\"authorId\":\"41065036\",\"name\":\"S. Singh\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"1698412\",\"name\":\"Yoram Bachrach\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"91c9b84e03667599d4c443b4ca688f37c469e857\",\"title\":\"Learning to Play No-Press Diplomacy with Best Response Policy Iteration\",\"url\":\"https://www.semanticscholar.org/paper/91c9b84e03667599d4c443b4ca688f37c469e857\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":\"144845452\",\"name\":\"M. Rowland\"},{\"authorId\":\"1419267454\",\"name\":\"Bart De Vylder\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"2196718\",\"name\":\"Finbarr Timbers\"},{\"authorId\":\"1897926\",\"name\":\"D. Hennes\"},{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"},{\"authorId\":\"37666967\",\"name\":\"Mohammad Gheshlaghi Azar\"},{\"authorId\":\"49860549\",\"name\":\"Edward Lockhart\"},{\"authorId\":\"50772704\",\"name\":\"K. Tuyls\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"563e891d21bf35187ac4e91721d22a4ba67907c1\",\"title\":\"Fast computation of Nash Equilibria in Imperfect Information Games\",\"url\":\"https://www.semanticscholar.org/paper/563e891d21bf35187ac4e91721d22a4ba67907c1\",\"venue\":\"ICML\",\"year\":2020},{\"arxivId\":\"1908.09453\",\"authors\":[{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"49860549\",\"name\":\"Edward Lockhart\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"148155754\",\"name\":\"Satyaki Upadhyay\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"152383695\",\"name\":\"Sriram Srinivasan\"},{\"authorId\":\"2196718\",\"name\":\"Finbarr Timbers\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"},{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"1897926\",\"name\":\"D. Hennes\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"47398134\",\"name\":\"P. Muller\"},{\"authorId\":\"23988602\",\"name\":\"Timo Ewalds\"},{\"authorId\":\"48627702\",\"name\":\"R. Faulkner\"},{\"authorId\":\"50458432\",\"name\":\"J\\u00e1nos Kram\\u00e1r\"},{\"authorId\":\"1419267454\",\"name\":\"Bart De Vylder\"},{\"authorId\":\"4125424\",\"name\":\"Brennan Saeta\"},{\"authorId\":\"35199668\",\"name\":\"J. Bradbury\"},{\"authorId\":\"1399191196\",\"name\":\"David Ding\"},{\"authorId\":\"148016269\",\"name\":\"Sebastian Borgeaud\"},{\"authorId\":\"40227832\",\"name\":\"Matthew Lai\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"144437624\",\"name\":\"T. Anthony\"},{\"authorId\":\"37591038\",\"name\":\"Edward Hughes\"},{\"authorId\":\"1841008\",\"name\":\"Ivo Danihelka\"},{\"authorId\":\"1411583481\",\"name\":\"Jonah Ryan-Davis\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"232bd009964a9bbf4757227aae94d1a139350e04\",\"title\":\"OpenSpiel: A Framework for Reinforcement Learning in Games\",\"url\":\"https://www.semanticscholar.org/paper/232bd009964a9bbf4757227aae94d1a139350e04\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2009.14521\",\"authors\":[{\"authorId\":\"1976182900\",\"name\":\"David Milec\"},{\"authorId\":\"3050965\",\"name\":\"J. Cern\\u00fd\"},{\"authorId\":\"1759154\",\"name\":\"V. Lis\\u00fd\"},{\"authorId\":\"1931762645\",\"name\":\"Bo An\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f7f11cc2a13313c44cefa16310ee90ba6350305f\",\"title\":\"Complexity and Algorithms for Exploiting Quantal Opponents in Large Two-Player Games\",\"url\":\"https://www.semanticscholar.org/paper/f7f11cc2a13313c44cefa16310ee90ba6350305f\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.08456\",\"authors\":[{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"143783339\",\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":\"3093004\",\"name\":\"Shayegan Omidshafiei\"},{\"authorId\":\"144845452\",\"name\":\"M. Rowland\"},{\"authorId\":\"145981974\",\"name\":\"Pedro A. Ortega\"},{\"authorId\":\"2625574\",\"name\":\"Neil Burch\"},{\"authorId\":\"52442941\",\"name\":\"T. Anthony\"},{\"authorId\":\"1722983\",\"name\":\"D. Balduzzi\"},{\"authorId\":\"1419267454\",\"name\":\"Bart De Vylder\"},{\"authorId\":\"1787822\",\"name\":\"G. Piliouras\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"50772704\",\"name\":\"K. Tuyls\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"af111ed91c177a14d403fce77e1c9c2fb99b0061\",\"title\":\"From Poincar\\u00e9 Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization\",\"url\":\"https://www.semanticscholar.org/paper/af111ed91c177a14d403fce77e1c9c2fb99b0061\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1911.10635\",\"authors\":[{\"authorId\":\"1776230\",\"name\":\"K. Zhang\"},{\"authorId\":\"150358650\",\"name\":\"Zhuoran Yang\"},{\"authorId\":\"39765157\",\"name\":\"T. Ba\\u015far\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"54d4a221db5a91a2487b1610374843fafff5a23d\",\"title\":\"Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/54d4a221db5a91a2487b1610374843fafff5a23d\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1910.01706\",\"authors\":[{\"authorId\":\"1414056326\",\"name\":\"Ryan D'Orazio\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"30782010\",\"name\":\"J. R. Wright\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3d49d51d2a62e45d92de9f615528d142db3c2ffa\",\"title\":\"Bounds for Approximate Regret-Matching Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/3d49d51d2a62e45d92de9f615528d142db3c2ffa\",\"venue\":\"ArXiv\",\"year\":2019}],\"corpusId\":76665732,\"doi\":\"10.24963/ijcai.2019/66\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":3,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"97395e484abd5701e483bcd3512627cd94dc109f\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"2685623\",\"name\":\"Noam Brown\"},{\"authorId\":\"2211504\",\"name\":\"Christian Kroer\"},{\"authorId\":\"145714168\",\"name\":\"T. Sandholm\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"abbb6c7e3b135ec49a8575ac8b4847fc4d9087a2\",\"title\":\"Dynamic Thresholding and Pruning for Regret Minimization\",\"url\":\"https://www.semanticscholar.org/paper/abbb6c7e3b135ec49a8575ac8b4847fc4d9087a2\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1736370\",\"name\":\"D. Koller\"},{\"authorId\":\"1706816\",\"name\":\"N. Megiddo\"},{\"authorId\":\"34843093\",\"name\":\"B. Stengel\"}],\"doi\":\"10.1145/195058.195451\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d7232f004eac8dd8cd76d10fa20258cbed86e7cb\",\"title\":\"Fast algorithms for finding randomized strategies in game trees\",\"url\":\"https://www.semanticscholar.org/paper/d7232f004eac8dd8cd76d10fa20258cbed86e7cb\",\"venue\":\"STOC '94\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1701353\",\"name\":\"Y. Shoham\"},{\"authorId\":\"1388404060\",\"name\":\"Kevin Leyton-Brown\"}],\"doi\":\"10.5860/choice.46-5662\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"96eaf9be3336df5bf07b04886a6faed8a19180c7\",\"title\":\"Multiagent Systems - Algorithmic, Game-Theoretic, and Logical Foundations\",\"url\":\"https://www.semanticscholar.org/paper/96eaf9be3336df5bf07b04886a6faed8a19180c7\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47648030\",\"name\":\"Takuya Kon-no\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ac48b2fb6d3efcca5ba7d59114161767cb4d5008\",\"title\":\"TRANSACTIONS OF THE AMERICAN MATHEMATICAL SOCIETY\",\"url\":\"https://www.semanticscholar.org/paper/ac48b2fb6d3efcca5ba7d59114161767cb4d5008\",\"venue\":\"\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Southey et al\"},{\"authorId\":null,\"name\":\"2005 Finnegan Southey\"},{\"authorId\":null,\"name\":\"Michael Bowling\"},{\"authorId\":null,\"name\":\"Bryce Larson\"},{\"authorId\":null,\"name\":\"Carmelo Piccione\"},{\"authorId\":null,\"name\":\"Neil Burch\"},{\"authorId\":null,\"name\":\"Darse Billings\"},{\"authorId\":null,\"name\":\"Chris Rayner. Bayes\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"bluff: Opponent modelling in poker\",\"url\":\"\",\"venue\":\"In Proceedings of the Twenty-First Conference on Uncertaintyin Artificial Intelligence (UAI),\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2685623\",\"name\":\"Noam Brown\"},{\"authorId\":\"145714168\",\"name\":\"T. Sandholm\"}],\"doi\":\"10.1126/science.aao1733\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"eaabb78d0bc44ed132e4d077e9486c86a9e4cda9\",\"title\":\"Superhuman AI for heads-up no-limit poker: Libratus beats top professionals\",\"url\":\"https://www.semanticscholar.org/paper/eaabb78d0bc44ed132e4d077e9486c86a9e4cda9\",\"venue\":\"Science\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Edward Lockhart\"},{\"authorId\":null,\"name\":\"Marc Lanctot\"},{\"authorId\":null,\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":null,\"name\":\"Jean-Baptiste Lespiau\"},{\"authorId\":null,\"name\":\"Dustin Morrill\"},{\"authorId\":null,\"name\":\"Finbarr Timbers\"},{\"authorId\":null,\"name\":\"Karl Tuyls. Computing approximate equilibria in sequent descent\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1903.05614,\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1681530\",\"name\":\"Michael Bradley Johanson\"},{\"authorId\":\"2294262\",\"name\":\"N. Bard\"},{\"authorId\":\"2625574\",\"name\":\"Neil Burch\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"baef5425987c2bc52b42fda09bf9337cec12c57b\",\"title\":\"Finding Optimal Abstract Strategies in Extensive-Form Games\",\"url\":\"https://www.semanticscholar.org/paper/baef5425987c2bc52b42fda09bf9337cec12c57b\",\"venue\":\"AAAI\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"36359915\",\"name\":\"M. Panella\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ff15528cbb9c47bb2324d0299c76bf331994882\",\"title\":\"Associate Editor of the Journal of Computer and System Sciences\",\"url\":\"https://www.semanticscholar.org/paper/9ff15528cbb9c47bb2324d0299c76bf331994882\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"e1f153c6df86d1ca8ecb9561daddfe7a54f901e7\",\"title\":\"Online Convex Programming and Generalized Infinitesimal Gradient Ascent\",\"url\":\"https://www.semanticscholar.org/paper/e1f153c6df86d1ca8ecb9561daddfe7a54f901e7\",\"venue\":\"ICML\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Y. Shoham\"},{\"authorId\":null,\"name\":\"K. Leyton-Brown\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Multiagent Systems: Algorithmic\",\"url\":\"\",\"venue\":\"Game-Theoretic, and Logical Foundations. Cambridge University Press\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Elad Hazan. Introduction to online convex optimization\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Foundations and Trends in Optimization\",\"url\":\"\",\"venue\":\"2(3\\u20134):157\\u2013325,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"H. W. Kuhn\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Simplified two-person Poker\",\"url\":\"\",\"venue\":\"Contributions to the Theory of Games, 1:97\\u2013103\",\"year\":1950},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144556841\",\"name\":\"S. Hoda\"},{\"authorId\":\"144720671\",\"name\":\"A. Gilpin\"},{\"authorId\":\"145055007\",\"name\":\"J. Pe\\u00f1a\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"e07950c4f9e44158b082b7a235631bf159f3acef\",\"title\":\"A GRADIENT-BASED APPROACH FOR COMPUTING NASH EQUILIBRIA OF LARGE SEQUENTIAL GAMES\",\"url\":\"https://www.semanticscholar.org/paper/e07950c4f9e44158b082b7a235631bf159f3acef\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Kevin Waugh\"},{\"authorId\":null,\"name\":\"J. Andrew Bagnell. A unified view of large-scale zero computation\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1411.5007,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Shalev-Shwartz\"},{\"authorId\":null,\"name\":\"others\"},{\"authorId\":null,\"name\":\"2012 Shai Shalev-Shwartz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Online learning and online convex optimization. Foundations and Trends R\",\"url\":\"\",\"venue\":\"\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1435508274\",\"name\":\"A. News\"},{\"authorId\":\"1999089\",\"name\":\"E. Lu\"},{\"authorId\":\"3028232\",\"name\":\"Minmin Zhou\"},{\"authorId\":null,\"name\":\"Rong Mocsai\"},{\"authorId\":\"145950435\",\"name\":\"A. Myers\"},{\"authorId\":\"1741991003\",\"name\":\"Erin E. Huang\"},{\"authorId\":\"153309963\",\"name\":\"B. Jackson\"},{\"authorId\":\"153501574\",\"name\":\"D. Ferrari\"},{\"authorId\":\"46448509\",\"name\":\"V. Tybulewicz\"},{\"authorId\":null,\"name\":\"Victor Lowell\"},{\"authorId\":\"11048382\",\"name\":\"C. Lepore\"},{\"authorId\":\"116515431\",\"name\":\"J. Koretzky\"},{\"authorId\":\"46292705\",\"name\":\"Gary Kahn\"},{\"authorId\":null,\"name\":\"Mark L\"},{\"authorId\":\"143807167\",\"name\":\"F. Achard\"},{\"authorId\":\"1491458517\",\"name\":\"H. D. Eva\"},{\"authorId\":null,\"name\":\"Ernst-Detlef See Also Schulze\"},{\"authorId\":\"5329613\",\"name\":\"J. Acharya\"},{\"authorId\":\"49185789\",\"name\":\"U. Acharya\"},{\"authorId\":\"49185789\",\"name\":\"U. Acharya\"},{\"authorId\":\"6077599\",\"name\":\"S. Patel\"},{\"authorId\":\"6398536\",\"name\":\"E. Koundakjian\"},{\"authorId\":\"48913846\",\"name\":\"K. Nagashima\"},{\"authorId\":\"87719226\",\"name\":\"X. Han\"},{\"authorId\":\"5329613\",\"name\":\"J. Acharya\"},{\"authorId\":\"144861008\",\"name\":\"D. Adams\"},{\"authorId\":null,\"name\":\"Jonathan C And Horton\"},{\"authorId\":null,\"name\":\"Blood\"},{\"authorId\":\"49185181\",\"name\":\"M. Adams\"},{\"authorId\":\"46775174\",\"name\":\"M. Mcvey\"},{\"authorId\":\"5909720\",\"name\":\"J. Sekelsky\"},{\"authorId\":\"52569394\",\"name\":\"J. Adamson\"},{\"authorId\":\"47920829\",\"name\":\"G. G. Kochendoerfer\"},{\"authorId\":\"7745713\",\"name\":\"A. W. Adeleke\"},{\"authorId\":null,\"name\":\"A See Kamdem-Toham\"},{\"authorId\":\"3111940\",\"name\":\"A. Aderem\"},{\"authorId\":\"152676651\",\"name\":\"C. Picard\"},{\"authorId\":\"115759360\",\"name\":\"Aeschlimann\"},{\"authorId\":\"4858956\",\"name\":\"G. Haug\"},{\"authorId\":\"2332181\",\"name\":\"G. S. Agarwal\"},{\"authorId\":\"50460883\",\"name\":\"M. Scully\"},{\"authorId\":\"144699300\",\"name\":\"H. Aguilaniu\"},{\"authorId\":\"46832542\",\"name\":\"L. Gustafsson\"},{\"authorId\":\"5667177\",\"name\":\"M. Rigoulet\"},{\"authorId\":\"145009924\",\"name\":\"T. Nystr\\u00f6m\"},{\"authorId\":null,\"name\":\"Asymmetric Inheri\"},{\"authorId\":\"46954724\",\"name\":\"Ferhaan Ahmad\"},{\"authorId\":\"6434784\",\"name\":\"J. P. Schmitt\"},{\"authorId\":\"153372043\",\"name\":\"Misako Aida\"},{\"authorId\":\"144767879\",\"name\":\"Salai C. Ammal\"},{\"authorId\":\"46324591\",\"name\":\"J. Aizenberg\"},{\"authorId\":\"3052861\",\"name\":\"D. Muller\"},{\"authorId\":\"117756927\",\"name\":\"John L. Grazul\"},{\"authorId\":\"145165160\",\"name\":\"D. R. Hamann\"},{\"authorId\":\"97839749\",\"name\":\"J. Ajioka\"},{\"authorId\":\"133799849\",\"name\":\"C. Su\"},{\"authorId\":\"50767520\",\"name\":\"A. Akella\"},{\"authorId\":\"38948478\",\"name\":\"M. Alam\"},{\"authorId\":\"47411788\",\"name\":\"F. Gao\"},{\"authorId\":\"4464233\",\"name\":\"A. Alatas\"},{\"authorId\":\"50169977\",\"name\":\"H. Sinn\"},{\"authorId\":\"7480073\",\"name\":\"Titus V. Albu\"},{\"authorId\":\"8204857\",\"name\":\"P. S. Zuev\"},{\"authorId\":\"1402325831\",\"name\":\"M. Al-Dayeh\"},{\"authorId\":\"117860354\",\"name\":\"J. Dwyer\"},{\"authorId\":\"114606050\",\"name\":\"A. Al-ghonaium\"},{\"authorId\":null,\"name\":\"Sami See Al-Hajjar\"},{\"authorId\":\"1421934622\",\"name\":\"S. Al-Jumaah\"},{\"authorId\":\"46591015\",\"name\":\"A. Allakhverdov\"},{\"authorId\":\"144263358\",\"name\":\"V. Pokrovsky\"},{\"authorId\":\"114745157\",\"name\":\"Allen\"},{\"authorId\":\"9441807\",\"name\":\"A. Brown\"},{\"authorId\":\"1475801484\",\"name\":\"J. H. Allen\"},{\"authorId\":\"9441807\",\"name\":\"A. Brown\"},{\"authorId\":null,\"name\":\"James H Gillooly\"},{\"authorId\":\"1443579396\",\"name\":\"James\"}],\"doi\":\"10.5406/j.ctvpj7hjj.15\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"3e0e337b477e0c226da00eae03fd29882275a469\",\"title\":\"What are \\u201c A \\u201d and \\u201c B \\u201d ?\",\"url\":\"https://www.semanticscholar.org/paper/3e0e337b477e0c226da00eae03fd29882275a469\",\"venue\":\"\",\"year\":null},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37183181\",\"name\":\"J. Heinrich\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"8b20f103c1f20074fa35bd8fc41983964283acac\",\"title\":\"Fictitious Self-Play in Extensive-Form Games\",\"url\":\"https://www.semanticscholar.org/paper/8b20f103c1f20074fa35bd8fc41983964283acac\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1389955537\",\"name\":\"S. Shalev-Shwartz\"}],\"doi\":\"10.1561/2200000018\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"bcce96a2a074448953fc61a29a84afbdfc8db55a\",\"title\":\"Online Learning and Online Convex Optimization\",\"url\":\"https://www.semanticscholar.org/paper/bcce96a2a074448953fc61a29a84afbdfc8db55a\",\"venue\":\"Found. Trends Mach. Learn.\",\"year\":2012},{\"arxivId\":\"1207.1411\",\"authors\":[{\"authorId\":\"2716445\",\"name\":\"F. Southey\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"},{\"authorId\":\"40186977\",\"name\":\"B. Larson\"},{\"authorId\":\"3248202\",\"name\":\"C. Piccione\"},{\"authorId\":\"2625574\",\"name\":\"Neil Burch\"},{\"authorId\":\"39261667\",\"name\":\"D. Billings\"},{\"authorId\":\"144309647\",\"name\":\"D. Rayner\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"24bac83f244cbbff359a271b2fce7cab9ef3d34b\",\"title\":\"Bayes? Bluff: Opponent Modelling in Poker\",\"url\":\"https://www.semanticscholar.org/paper/24bac83f244cbbff359a271b2fce7cab9ef3d34b\",\"venue\":\"UAI\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2063343\",\"name\":\"F. H. Clarke\"}],\"doi\":\"10.1090/S0002-9947-1975-0367131-6\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"99ee1cc66f8f2daf824f56434fc79d363fb589f1\",\"title\":\"Generalized gradients and applications\",\"url\":\"https://www.semanticscholar.org/paper/99ee1cc66f8f2daf824f56434fc79d363fb589f1\",\"venue\":\"\",\"year\":1975},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Noam Brown\"},{\"authorId\":null,\"name\":\"Adam Lerer\"},{\"authorId\":null,\"name\":\"Sam Gross\"},{\"authorId\":null,\"name\":\"Tuomas Sandholm. Deep counterfactual regret minimization\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1811.00164,\",\"year\":2018},{\"arxivId\":\"1411.7974\",\"authors\":[{\"authorId\":\"144514513\",\"name\":\"K. Waugh\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"28386ad2b37548a480ff43b94fef33ac132f20b4\",\"title\":\"Solving Games with Functional Regret Estimation\",\"url\":\"https://www.semanticscholar.org/paper/28386ad2b37548a480ff43b94fef33ac132f20b4\",\"venue\":\"AAAI Workshop: Computer Poker and Imperfect Information\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144237049\",\"name\":\"S. Hart\"},{\"authorId\":\"1399551631\",\"name\":\"A. Mas-Colell\"}],\"doi\":\"10.1111/1468-0262.00153\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2cd2df06d488565063e0600ff840d293be2eaf31\",\"title\":\"A Simple Adaptive Procedure Leading to Correlated Equilibrium\",\"url\":\"https://www.semanticscholar.org/paper/2cd2df06d488565063e0600ff840d293be2eaf31\",\"venue\":\"\",\"year\":1997},{\"arxivId\":null,\"authors\":[{\"authorId\":\"98045118\",\"name\":\"J. Darzentas\"}],\"doi\":\"10.1057/jors.1984.92\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"29f8b7112fb4660a64df8ac34a5d85c119019efe\",\"title\":\"Problem Complexity and Method Efficiency in Optimization\",\"url\":\"https://www.semanticscholar.org/paper/29f8b7112fb4660a64df8ac34a5d85c119019efe\",\"venue\":\"\",\"year\":1983},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Amir Beck\"},{\"authorId\":null,\"name\":\"Marc Teboulle. Mirror descent\"},{\"authorId\":null,\"name\":\"nonlinear projected subgradient methods for convex optimizat Letters\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"31(3):167\\u2013175\",\"url\":\"\",\"venue\":\"May\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jie Tang\"},{\"authorId\":null,\"name\":\"Keiran Paster\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Equilibrium finding via asymmetric self-play reinforcement learning\",\"url\":\"\",\"venue\":\"Deep Reinforcement Learning Workshop NeurIPS 2018,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"49501697\",\"name\":\"P. Gupta\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e34304787c1f747b6d6afeb3a243b4c7303895c\",\"title\":\"Linear programming and theory of games\",\"url\":\"https://www.semanticscholar.org/paper/3e34304787c1f747b6d6afeb3a243b4c7303895c\",\"venue\":\"\",\"year\":1979},{\"arxivId\":\"1810.09026\",\"authors\":[{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"90fac6e666cfbb2c7071610139ae65ef49066505\",\"title\":\"Actor-Critic Policy Optimization in Partially Observable Multiagent Environments\",\"url\":\"https://www.semanticscholar.org/paper/90fac6e666cfbb2c7071610139ae65ef49066505\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1703537\",\"name\":\"Y. Freund\"},{\"authorId\":\"1716301\",\"name\":\"R. Schapire\"}],\"doi\":\"10.1007/3-540-59119-2_166\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ccf5208521cb8c35f50ee8873df89294b8ed7292\",\"title\":\"A decision-theoretic generalization of on-line learning and an application to boosting\",\"url\":\"https://www.semanticscholar.org/paper/ccf5208521cb8c35f50ee8873df89294b8ed7292\",\"venue\":\"EuroCOLT\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144756643\",\"name\":\"P. Glynn\"},{\"authorId\":\"115292111\",\"name\":\"P. L'Ecuyer\"}],\"doi\":\"10.1017/S0001867800047789\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4caabff8b795d5d7334b1ff0d55d4aab366fa6ca\",\"title\":\"Likelihood ratio gradient estimation for stochastic recursions\",\"url\":\"https://www.semanticscholar.org/paper/4caabff8b795d5d7334b1ff0d55d4aab366fa6ca\",\"venue\":\"\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145557817\",\"name\":\"Jonathan Rubin\"},{\"authorId\":\"1809241\",\"name\":\"I. Watson\"}],\"doi\":\"10.1016/j.artint.2010.12.005\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"43f8e76de51b81396faacbd61c91a5ac03fab9f3\",\"title\":\"Computer poker: A review\",\"url\":\"https://www.semanticscholar.org/paper/43f8e76de51b81396faacbd61c91a5ac03fab9f3\",\"venue\":\"Artif. Intell.\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Johannes Heinrich\"},{\"authorId\":null,\"name\":\"David Silver. Deep reinforcement learning from self-play games\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1603.01121,\",\"year\":2016},{\"arxivId\":\"1106.0665\",\"authors\":[{\"authorId\":\"143731181\",\"name\":\"J. Baxter\"},{\"authorId\":\"1745169\",\"name\":\"P. Bartlett\"}],\"doi\":\"10.1613/jair.806\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"085fb3acabcbf80ef1bf47daec50d246475b072b\",\"title\":\"Infinite-Horizon Policy-Gradient Estimation\",\"url\":\"https://www.semanticscholar.org/paper/085fb3acabcbf80ef1bf47daec50d246475b072b\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40410858\",\"name\":\"R. J. Williams\"}],\"doi\":\"10.1007/BF00992696\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"title\":\"Simple statistical gradient-following algorithms for connectionist reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/4c915c1eecb217c123a36dc6d3ce52d12c742614\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"123242656\",\"name\":\"A. Hanks\"}],\"doi\":\"10.1300/J237v07n02_09\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"90315d5929b1f147f565bc18e0a01d7fdfeb2e74\",\"title\":\"Canada\",\"url\":\"https://www.semanticscholar.org/paper/90315d5929b1f147f565bc18e0a01d7fdfeb2e74\",\"venue\":\"\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2449382\",\"name\":\"T. Hubert\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"40227832\",\"name\":\"Matthew Lai\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"34838386\",\"name\":\"K. Simonyan\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1126/science.aar6404\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608\",\"title\":\"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play\",\"url\":\"https://www.semanticscholar.org/paper/f9717d29840f4d8f1cc19d1b1e80c5d12ec40608\",\"venue\":\"Science\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ronald J. Williams. Simple statistical gradient-following Learning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"8(3):229\\u2013256\",\"url\":\"\",\"venue\":\"May\",\"year\":1992},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"145689002\",\"name\":\"David A. McAllester\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144830983\",\"name\":\"Y. Mansour\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"a20f0ce0616def7cc9a87446c228906cd5da093b\",\"title\":\"Policy Gradient Methods for Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/a20f0ce0616def7cc9a87446c228906cd5da093b\",\"venue\":\"NIPS\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Bowling et al\"},{\"authorId\":null,\"name\":\"2015 Michael Bowling\"},{\"authorId\":null,\"name\":\"Neil Burch\"},{\"authorId\":null,\"name\":\"Michael Johanson\"},{\"authorId\":null,\"name\":\"Oskari Tammelin\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Heads-up Limit\",\"url\":\"\",\"venue\":\"\",\"year\":2015},{\"arxivId\":\"1711.00832\",\"authors\":[{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"3133079\",\"name\":\"V. Zambaldi\"},{\"authorId\":\"2203658\",\"name\":\"A. Gruslys\"},{\"authorId\":\"2672644\",\"name\":\"A. Lazaridou\"},{\"authorId\":\"2274623\",\"name\":\"K. Tuyls\"},{\"authorId\":\"3422031\",\"name\":\"Julien P\\u00e9rolat\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b\",\"title\":\"A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b0d8165eecf2aa04a85e701d0c6bb4edd4b3811b\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"},{\"authorId\":\"2625574\",\"name\":\"Neil Burch\"},{\"authorId\":\"1681530\",\"name\":\"Michael Bradley Johanson\"},{\"authorId\":\"3065811\",\"name\":\"Oskari Tammelin\"}],\"doi\":\"10.1126/science.1259433\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f45d9c0e2f3f96f97deda4fbb438b5a6e49d0dc\",\"title\":\"Heads-up limit hold\\u2019em poker is solved\",\"url\":\"https://www.semanticscholar.org/paper/0f45d9c0e2f3f96f97deda4fbb438b5a6e49d0dc\",\"venue\":\"Science\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1843103\",\"name\":\"Stephen P. Boyd\"},{\"authorId\":\"2014414\",\"name\":\"L. Vandenberghe\"}],\"doi\":\"10.1017/CBO9780511804441\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4f607f03272e4d62708f5b2441355f9e005cb452\",\"title\":\"Convex Optimization\",\"url\":\"https://www.semanticscholar.org/paper/4f607f03272e4d62708f5b2441355f9e005cb452\",\"venue\":\"IEEE Transactions on Automatic Control\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"40204991\",\"name\":\"A. Beck\"},{\"authorId\":\"1727609\",\"name\":\"M. Teboulle\"}],\"doi\":\"10.1016/S0167-6377(02)00231-6\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"4f0df7ed89d9a5bb5bafa5727ccdc0b6e2fb463d\",\"title\":\"Mirror descent and nonlinear projected subgradient methods for convex optimization\",\"url\":\"https://www.semanticscholar.org/paper/4f0df7ed89d9a5bb5bafa5727ccdc0b6e2fb463d\",\"venue\":\"Oper. Res. Lett.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jonathan Baxter\"},{\"authorId\":null,\"name\":\"Peter L Bartlett. Infinite-horizon policy-gradient estimation\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"Journal of Artificial Intelligence Research\",\"url\":\"\",\"venue\":\"15:319\\u2013350,\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Hoda\"},{\"authorId\":null,\"name\":\"A. Gilpin\"},{\"authorId\":null,\"name\":\"J. Pe na. A gradient-based approach for computing Nas games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Optimization Online\",\"url\":\"\",\"venue\":\"July\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Silver et al\"},{\"authorId\":null,\"name\":\"2018 David Silver\"},{\"authorId\":null,\"name\":\"Thomas Hubert\"},{\"authorId\":null,\"name\":\"Julian Schrittwieser\"},{\"authorId\":null,\"name\":\"Ioannis Antonoglou\"},{\"authorId\":null,\"name\":\"Matthew Lai\"},{\"authorId\":null,\"name\":\"Arthur Guez\"},{\"authorId\":null,\"name\":\"Marc Lanctot\"},{\"authorId\":null,\"name\":\"Laurent Sifre\"},{\"authorId\":null,\"name\":\"Dharshan Kumaran\"},{\"authorId\":null,\"name\":\"Thore Graepel\"},{\"authorId\":null,\"name\":\"Timothy Lillicrap\"},{\"authorId\":null,\"name\":\"Karen Simonyan\"},{\"authorId\":null,\"name\":\"Demis Hassabis\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A general reinforcement learning\",\"url\":\"\",\"venue\":\"\",\"year\":2018},{\"arxivId\":\"1701.01724\",\"authors\":[{\"authorId\":\"8801705\",\"name\":\"Matej Moravc\\u00edk\"},{\"authorId\":\"46828293\",\"name\":\"Martina Schmid\"},{\"authorId\":\"2625574\",\"name\":\"Neil Burch\"},{\"authorId\":\"1759154\",\"name\":\"V. Lis\\u00fd\"},{\"authorId\":\"2551974\",\"name\":\"Dustin Morrill\"},{\"authorId\":\"2294262\",\"name\":\"N. Bard\"},{\"authorId\":\"48112534\",\"name\":\"Trevor Davis\"},{\"authorId\":\"144514513\",\"name\":\"K. Waugh\"},{\"authorId\":\"1681530\",\"name\":\"Michael Bradley Johanson\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":\"10.1126/science.aam6960\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a2155552ca5afb784a3c1d67a5bcbd4e688b6e05\",\"title\":\"DeepStack: Expert-level artificial intelligence in heads-up no-limit poker\",\"url\":\"https://www.semanticscholar.org/paper/a2155552ca5afb784a3c1d67a5bcbd4e688b6e05\",\"venue\":\"Science\",\"year\":2017},{\"arxivId\":\"1909.05207\",\"authors\":[{\"authorId\":\"34840427\",\"name\":\"Elad Hazan\"}],\"doi\":\"10.1561/2400000013\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d4e61eb8656b6b1aedd8f32c7eef4ae807d162ed\",\"title\":\"Introduction to Online Convex Optimization\",\"url\":\"https://www.semanticscholar.org/paper/d4e61eb8656b6b1aedd8f32c7eef4ae807d162ed\",\"venue\":\"Found. Trends Optim.\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"143903370\",\"name\":\"Murray Campbell\"},{\"authorId\":\"32498609\",\"name\":\"A. J. Hoane\"},{\"authorId\":\"144844988\",\"name\":\"Feng-hsiung Hsu\"}],\"doi\":\"10.1016/S0004-3702(01)00129-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3e4bc1aa55c752918ae99b1a125f6adef61afad2\",\"title\":\"Deep Blue\",\"url\":\"https://www.semanticscholar.org/paper/3e4bc1aa55c752918ae99b1a125f6adef61afad2\",\"venue\":\"Artif. Intell.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"8195063\",\"name\":\"Martin Zinkevich\"},{\"authorId\":\"1681530\",\"name\":\"Michael Bradley Johanson\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"},{\"authorId\":\"3248202\",\"name\":\"C. Piccione\"}],\"doi\":\"10.7939/R3Q23R282\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"db20a76a702be3ad4e7cebf7eeb7f1898827cebd\",\"title\":\"Regret Minimization in Games with Incomplete Information\",\"url\":\"https://www.semanticscholar.org/paper/db20a76a702be3ad4e7cebf7eeb7f1898827cebd\",\"venue\":\"NIPS\",\"year\":2007},{\"arxivId\":\"1811.00164\",\"authors\":[{\"authorId\":\"2685623\",\"name\":\"Noam Brown\"},{\"authorId\":\"1977806\",\"name\":\"A. Lerer\"},{\"authorId\":\"39793298\",\"name\":\"S. Gross\"},{\"authorId\":\"145714168\",\"name\":\"T. Sandholm\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"42c01c027db1e1d9846f13b73e0846bcd3852e65\",\"title\":\"Deep Counterfactual Regret Minimization\",\"url\":\"https://www.semanticscholar.org/paper/42c01c027db1e1d9846f13b73e0846bcd3852e65\",\"venue\":\"ICML\",\"year\":2019}],\"title\":\"Computing Approximate Equilibria in Sequential Adversarial Games by Exploitability Descent\",\"topics\":[{\"topic\":\"Nash equilibrium\",\"topicId\":\"17596\",\"url\":\"https://www.semanticscholar.org/topic/17596\"},{\"topic\":\"Mathematical optimization\",\"topicId\":\"89\",\"url\":\"https://www.semanticscholar.org/topic/89\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"},{\"topic\":\"Table (information)\",\"topicId\":\"200\",\"url\":\"https://www.semanticscholar.org/topic/200\"},{\"topic\":\"XFP transceiver\",\"topicId\":\"1425387\",\"url\":\"https://www.semanticscholar.org/topic/1425387\"},{\"topic\":\"Descent\",\"topicId\":\"200769\",\"url\":\"https://www.semanticscholar.org/topic/200769\"},{\"topic\":\"Counterfactual conditional\",\"topicId\":\"69178\",\"url\":\"https://www.semanticscholar.org/topic/69178\"},{\"topic\":\"Best, worst and average case\",\"topicId\":\"8359\",\"url\":\"https://www.semanticscholar.org/topic/8359\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Converge\",\"topicId\":\"205534\",\"url\":\"https://www.semanticscholar.org/topic/205534\"},{\"topic\":\"Benchmark (computing)\",\"topicId\":\"1374\",\"url\":\"https://www.semanticscholar.org/topic/1374\"}],\"url\":\"https://www.semanticscholar.org/paper/97395e484abd5701e483bcd3512627cd94dc109f\",\"venue\":\"IJCAI\",\"year\":2019}\n"