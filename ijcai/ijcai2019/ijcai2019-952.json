"{\"abstract\":\"Allocating resources to customers in the customer service is a difficult problem, because designing an optimal strategy to achieve an optimal trade-off between available resources and customers\\u2019 satisfaction is non-trivial. In this paper, we formalize the customer routing problem, and propose a novel framework based on deep reinforcement learning (RL) to address this problem. To make it more practical, a demo is provided to show and compare different models, which visualizes all decision process, and in particular, the system shows how the optimal strategy is reached. Besides, our demo system also ships with a variety of models that users can choose based on their needs.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"2045991\",\"name\":\"Chong Long\",\"url\":\"https://www.semanticscholar.org/author/2045991\"},{\"authorId\":\"49293133\",\"name\":\"Zining Liu\",\"url\":\"https://www.semanticscholar.org/author/49293133\"},{\"authorId\":\"40505029\",\"name\":\"Xiaolu Lu\",\"url\":\"https://www.semanticscholar.org/author/40505029\"},{\"authorId\":\"40604082\",\"name\":\"Zehong Hu\",\"url\":\"https://www.semanticscholar.org/author/40604082\"},{\"authorId\":null,\"name\":\"Yafang Wang\",\"url\":null}],\"citationVelocity\":0,\"citations\":[],\"corpusId\":199465717,\"doi\":\"10.24963/ijcai.2019/952\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"49c16923999b14fe230c05f76996a3f7515b75c0\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ziyu Wang\"},{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"Matteo Hessel\"},{\"authorId\":null,\"name\":\"Hado van Hasselt\"},{\"authorId\":null,\"name\":\"Marc Lanctot\"},{\"authorId\":null,\"name\":\"Nando de Freitas. Dueling network architectures for deep Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML\",\"url\":\"\",\"venue\":\"pages 1995\\u20132003,\",\"year\":2016},{\"arxivId\":\"1509.06461\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"title\":\"Deep Reinforcement Learning with Double Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/3b9732bb07dc99bde5e1f9f75251c6ea5039373e\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tom Schaul\"},{\"authorId\":null,\"name\":\"John Quan\"},{\"authorId\":null,\"name\":\"Ioannis Antonoglou\"},{\"authorId\":null,\"name\":\"David Silver. Prioritized experience replay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1511.05952,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hado van Hasselt\"},{\"authorId\":null,\"name\":\"Arthur Guez\"},{\"authorId\":null,\"name\":\"David Silver. Deep reinforcement learning with double Q- Proc\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AAAI\",\"url\":\"\",\"venue\":\"pages 2094\\u20132100,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ionel-Alexandru Hosu\"},{\"authorId\":null,\"name\":\"Traian Rebedea. Playing atari games with deep reinforceme learning\"},{\"authorId\":null,\"name\":\"human checkpoint replay\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1607.05077,\",\"year\":2016}],\"title\":\"CRSRL: Customer Routing System Using Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Routing\",\"topicId\":\"1048\",\"url\":\"https://www.semanticscholar.org/topic/1048\"}],\"url\":\"https://www.semanticscholar.org/paper/49c16923999b14fe230c05f76996a3f7515b75c0\",\"venue\":\"IJCAI\",\"year\":2019}\n"