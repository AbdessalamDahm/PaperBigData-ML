"{\"abstract\":\"Reinforcement learning agents can learn to solve sequential decision tasks by interacting with the environment. Human knowledge of how to solve these tasks can be incorporated using imitation learning, where the agent learns to imitate human demonstrated decisions. However, human guidance is not limited to the demonstrations. Other types of guidance could be more suitable for certain tasks and require less human effort. This survey provides a high-level overview of five recent learning frameworks that primarily rely on human guidance other than conventional, step-by-step action demonstrations. We review the motivation, assumption, and implementation of each framework. We then discuss possible future research directions.\",\"arxivId\":\"1909.09906\",\"authors\":[{\"authorId\":\"2657185\",\"name\":\"Ruohan Zhang\",\"url\":\"https://www.semanticscholar.org/author/2657185\"},{\"authorId\":\"46221670\",\"name\":\"F. Torabi\",\"url\":\"https://www.semanticscholar.org/author/46221670\"},{\"authorId\":\"144190085\",\"name\":\"L. Guan\",\"url\":\"https://www.semanticscholar.org/author/144190085\"},{\"authorId\":\"1691804\",\"name\":\"D. Ballard\",\"url\":\"https://www.semanticscholar.org/author/1691804\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\",\"url\":\"https://www.semanticscholar.org/author/144848112\"}],\"citationVelocity\":5,\"citations\":[{\"arxivId\":\"1910.09986\",\"authors\":[{\"authorId\":\"2816836\",\"name\":\"Haodi Zhang\"},{\"authorId\":\"100680094\",\"name\":\"Zihang Gao\"},{\"authorId\":\"32066669\",\"name\":\"Yi Zhou\"},{\"authorId\":\"145140331\",\"name\":\"Hao Zhang\"},{\"authorId\":\"8584850\",\"name\":\"Kaishun Wu\"},{\"authorId\":\"144125597\",\"name\":\"F. Lin\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"7a141e2ba1360f50ad5ba88842f7da88c93a38f7\",\"title\":\"Faster and Safer Training by Embedding High-Level Knowledge into Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/7a141e2ba1360f50ad5ba88842f7da88c93a38f7\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"1903.06754\",\"authors\":[{\"authorId\":\"2657185\",\"name\":\"Ruohan Zhang\"},{\"authorId\":\"2619658\",\"name\":\"Zhuode Liu\"},{\"authorId\":\"144190085\",\"name\":\"L. Guan\"},{\"authorId\":\"13800723\",\"name\":\"L. Zhang\"},{\"authorId\":\"2848854\",\"name\":\"M. Hayhoe\"},{\"authorId\":\"1691804\",\"name\":\"D. Ballard\"}],\"doi\":\"10.1609/AAAI.V34I04.6161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d9dea72f5a7ca7a6e928eb4dca3962c24df6ca50\",\"title\":\"Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset\",\"url\":\"https://www.semanticscholar.org/paper/d9dea72f5a7ca7a6e928eb4dca3962c24df6ca50\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"2007.10504\",\"authors\":[{\"authorId\":\"4078204\",\"name\":\"Jonathan Chung\"},{\"authorId\":\"47869270\",\"name\":\"Anna Luo\"},{\"authorId\":\"1825740739\",\"name\":\"Xavier Raffin\"},{\"authorId\":\"29890574\",\"name\":\"S. Perry\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3bdb464777dad270ad1c80426614af16c08dd361\",\"title\":\"Battlesnake Challenge: A Multi-agent Reinforcement Learning Playground with Human-in-the-loop\",\"url\":\"https://www.semanticscholar.org/paper/3bdb464777dad270ad1c80426614af16c08dd361\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2012.06733\",\"authors\":[{\"authorId\":\"49686756\",\"name\":\"Ajay Mandlekar\"},{\"authorId\":\"2068265\",\"name\":\"Danfei Xu\"},{\"authorId\":\"1382655067\",\"name\":\"Roberto Mart\\u00edn-Mart\\u00edn\"},{\"authorId\":\"2117748\",\"name\":\"Yuke Zhu\"},{\"authorId\":\"48004138\",\"name\":\"Li Fei-Fei\"},{\"authorId\":\"1702137\",\"name\":\"S. Savarese\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d7f5aeb06f37bbaf6d47d560db2162665c7bdac6\",\"title\":\"Human-in-the-Loop Imitation Learning using Remote Teleoperation\",\"url\":\"https://www.semanticscholar.org/paper/d7f5aeb06f37bbaf6d47d560db2162665c7bdac6\",\"venue\":\"\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1466474372\",\"name\":\"Katleen Blanchet\"},{\"authorId\":\"46691521\",\"name\":\"A. Bouzeghoub\"},{\"authorId\":\"3002437\",\"name\":\"Selma Kchir\"},{\"authorId\":\"50345041\",\"name\":\"O. Lebec\"}],\"doi\":\"10.1109/SMC42975.2020.9283469\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5fbe945942c891fefce7902b2b2834637450753e\",\"title\":\"How to Guide Humans Towards Skills Improvement in Physical Human-Robot Collaboration Using Reinforcement Learning?\",\"url\":\"https://www.semanticscholar.org/paper/5fbe945942c891fefce7902b2b2834637450753e\",\"venue\":\"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\",\"year\":2020},{\"arxivId\":\"2001.06781\",\"authors\":[{\"authorId\":\"2797515\",\"name\":\"Baicen Xiao\"},{\"authorId\":\"153083396\",\"name\":\"Qifan Lu\"},{\"authorId\":\"31306695\",\"name\":\"B. Ramasubramanian\"},{\"authorId\":\"50502989\",\"name\":\"A. Clark\"},{\"authorId\":\"47932194\",\"name\":\"L. Bushnell\"},{\"authorId\":\"144786412\",\"name\":\"R. Poovendran\"}],\"doi\":\"10.5555/3398761.3398935\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"14b7d4a34ba869e106d82d658e2973163a11eb12\",\"title\":\"FRESH: Interactive Reward Shaping in High-Dimensional State Spaces using Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/14b7d4a34ba869e106d82d658e2973163a11eb12\",\"venue\":\"AAMAS\",\"year\":2020},{\"arxivId\":\"1902.06007\",\"authors\":[{\"authorId\":\"49915485\",\"name\":\"Andrew Silva\"},{\"authorId\":\"145223968\",\"name\":\"M. Gombolay\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a1432df7eb3b1dee996b5d633c76ac9989f533e\",\"title\":\"ProLoNets: Neural-encoding Human Experts' Domain Knowledge to Warm Start Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/4a1432df7eb3b1dee996b5d633c76ac9989f533e\",\"venue\":\"ArXiv\",\"year\":2019},{\"arxivId\":\"2010.15942\",\"authors\":[{\"authorId\":\"2657185\",\"name\":\"Ruohan Zhang\"},{\"authorId\":\"1720831208\",\"name\":\"Bo Liu\"},{\"authorId\":\"1778450\",\"name\":\"Y. Zhu\"},{\"authorId\":\"1807482896\",\"name\":\"Sihang Guo\"},{\"authorId\":\"2848854\",\"name\":\"M. Hayhoe\"},{\"authorId\":\"1691804\",\"name\":\"D. Ballard\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"0af86c8fe633e71aeb38561165609bc14d699290\",\"title\":\"Human versus Machine Attention in Deep Reinforcement Learning Tasks\",\"url\":\"https://www.semanticscholar.org/paper/0af86c8fe633e71aeb38561165609bc14d699290\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.07358\",\"authors\":[{\"authorId\":\"5315428\",\"name\":\"Benjamin J. Newman\"},{\"authorId\":\"33892852\",\"name\":\"Kevin Carlberg\"},{\"authorId\":\"2142628\",\"name\":\"R. Desai\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"db1a1e2001bc0470fa8e21d6e2e6cee65356bb81\",\"title\":\"Optimal Assistance for Object-Rearrangement Tasks in Augmented Reality\",\"url\":\"https://www.semanticscholar.org/paper/db1a1e2001bc0470fa8e21d6e2e6cee65356bb81\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2002.12500\",\"authors\":[{\"authorId\":\"10978611\",\"name\":\"Akanksha Saran\"},{\"authorId\":\"2657185\",\"name\":\"Ruohan Zhang\"},{\"authorId\":\"32775309\",\"name\":\"Elaine Schaertl Short\"},{\"authorId\":\"2791038\",\"name\":\"S. Niekum\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"a078029566d58c4efcef2b36523af794922cbf41\",\"title\":\"Efficiently Guiding Imitation Learning Algorithms with Human Gaze\",\"url\":\"https://www.semanticscholar.org/paper/a078029566d58c4efcef2b36523af794922cbf41\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2006.16498\",\"authors\":[{\"authorId\":\"145548210\",\"name\":\"D. Xu\"},{\"authorId\":\"1776245\",\"name\":\"Mohit Agarwal\"},{\"authorId\":\"1730720\",\"name\":\"F. Fekri\"},{\"authorId\":\"145896661\",\"name\":\"R. Sivakumar\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"82827af87d95747097cb94dd7e246c8dcdc5efa0\",\"title\":\"Accelerating Reinforcement Learning Agent with EEG-based Implicit Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/82827af87d95747097cb94dd7e246c8dcdc5efa0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"1809.02627\",\"authors\":[{\"authorId\":\"7174267\",\"name\":\"Arthur Juliani\"},{\"authorId\":\"51266557\",\"name\":\"Vincent-Pierre Berges\"},{\"authorId\":\"80170837\",\"name\":\"Esh Vckay\"},{\"authorId\":\"143792910\",\"name\":\"Yuan Gao\"},{\"authorId\":\"51450774\",\"name\":\"Hunter Henry\"},{\"authorId\":\"49354909\",\"name\":\"M. Mattar\"},{\"authorId\":\"51438954\",\"name\":\"D. Lange\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"252482d733d67230843fe99bf60427285f0ad8e8\",\"title\":\"Unity: A General Platform for Intelligent Agents\",\"url\":\"https://www.semanticscholar.org/paper/252482d733d67230843fe99bf60427285f0ad8e8\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"2006.11645\",\"authors\":[{\"authorId\":\"11844404\",\"name\":\"Tsung-Yen Yang\"},{\"authorId\":\"30193159\",\"name\":\"J. Rosca\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"1693135\",\"name\":\"P. Ramadge\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"050e5216dd134726f3b11282520b078084fb47c3\",\"title\":\"Accelerating Safe Reinforcement Learning with Constraint-mismatched Policies\",\"url\":\"https://www.semanticscholar.org/paper/050e5216dd134726f3b11282520b078084fb47c3\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2003.05012\",\"authors\":[{\"authorId\":\"144177520\",\"name\":\"S. Milani\"},{\"authorId\":\"34887814\",\"name\":\"Nicholay Topin\"},{\"authorId\":\"103681415\",\"name\":\"Brandon Houghton\"},{\"authorId\":\"39121861\",\"name\":\"William H. Guss\"},{\"authorId\":\"24178944\",\"name\":\"S. Mohanty\"},{\"authorId\":\"12965493\",\"name\":\"K. Nakata\"},{\"authorId\":\"49519592\",\"name\":\"Oriol Vinyals\"},{\"authorId\":\"1596824164\",\"name\":\"Noboru Sean Kuno\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e7fa675a5975dc57402513d950e786f2af7a0b45\",\"title\":\"Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e7fa675a5975dc57402513d950e786f2af7a0b45\",\"venue\":\"Proceedings of Machine Learning Research\",\"year\":2019},{\"arxivId\":\"2006.14804\",\"authors\":[{\"authorId\":\"144190085\",\"name\":\"L. Guan\"},{\"authorId\":\"151500192\",\"name\":\"M. Verma\"},{\"authorId\":\"1740315\",\"name\":\"S. Kambhampati\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"6e3273ea2cac5ebbf1479227c254d1d5d5fd0979\",\"title\":\"Explanation Augmented Feedback in Human-in-the-Loop Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/6e3273ea2cac5ebbf1479227c254d1d5d5fd0979\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2009.13649\",\"authors\":[{\"authorId\":\"7332443\",\"name\":\"Yuchen Cui\"},{\"authorId\":\"1490776695\",\"name\":\"Qiping Zhang\"},{\"authorId\":\"49642155\",\"name\":\"A. Allievi\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"},{\"authorId\":\"2791038\",\"name\":\"S. Niekum\"},{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"055843de1b8ffd9bcb4c29df593a0c9c56280a14\",\"title\":\"The EMPATHIC Framework for Task Learning from Implicit Human Feedback\",\"url\":\"https://www.semanticscholar.org/paper/055843de1b8ffd9bcb4c29df593a0c9c56280a14\",\"venue\":\"ArXiv\",\"year\":2020}],\"corpusId\":189805612,\"doi\":\"10.24963/ijcai.2019/884\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":1,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"9bd453ea1e4655311cb1172f7a188cb9c5ee367d\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1735414\",\"name\":\"T. Knasel\"}],\"doi\":\"10.1016/0921-8890(88)90002-4\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edd77f310393f521669b209cbb6828fb45a8485d\",\"title\":\"Robotics and autonomous systems\",\"url\":\"https://www.semanticscholar.org/paper/edd77f310393f521669b209cbb6828fb45a8485d\",\"venue\":\"Robotics Auton. Syst.\",\"year\":1988},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144389295\",\"name\":\"C. Wirth\"},{\"authorId\":\"1747752\",\"name\":\"J. F\\u00fcrnkranz\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"f009a64f87841b79b1e2ea1748366270455ef9c0\",\"title\":\"Model-Free Preference-Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/f009a64f87841b79b1e2ea1748366270455ef9c0\",\"venue\":\"AAAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2212554\",\"name\":\"Ofra Amir\"},{\"authorId\":\"1783184\",\"name\":\"Ece Kamar\"},{\"authorId\":\"6247481\",\"name\":\"A. Kolobov\"},{\"authorId\":\"1692242\",\"name\":\"B. Grosz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bebd4f1259a8db086faeabf05b282cb8912d44f3\",\"title\":\"Interactive Teaching Strategies for Agent Training\",\"url\":\"https://www.semanticscholar.org/paper/bebd4f1259a8db086faeabf05b282cb8912d44f3\",\"venue\":\"IJCAI\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1738814\",\"name\":\"Y. Li\"},{\"authorId\":\"46331912\",\"name\":\"M. Liu\"},{\"authorId\":\"144177248\",\"name\":\"James M. Rehg\"}],\"doi\":\"10.1007/978-3-030-01228-1_38\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"fa1723b216b1f41b085b62b450b7b0bd9f2fd281\",\"title\":\"In the Eye of Beholder: Joint Learning of Gaze and Actions in First Person Video\",\"url\":\"https://www.semanticscholar.org/paper/fa1723b216b1f41b085b62b450b7b0bd9f2fd281\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"5886094\",\"name\":\"P. Cochat\"},{\"authorId\":\"13267685\",\"name\":\"L. Vaucoret\"},{\"authorId\":\"31455512\",\"name\":\"J. Sarles\"}],\"doi\":\"10.1016/j.arcped.2012.01.013\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"10d85561e4aafc516d10064f30dff05b41f70afe\",\"title\":\"[Et al].\",\"url\":\"https://www.semanticscholar.org/paper/10d85561e4aafc516d10064f30dff05b41f70afe\",\"venue\":\"Archives de pediatrie : organe officiel de la Societe francaise de pediatrie\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"},{\"authorId\":\"3564227\",\"name\":\"C. Shelton\"},{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/375735.376334\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"109186b81a0575936297ae3a0ff41491124a4bf2\",\"title\":\"A social reinforcement learning agent\",\"url\":\"https://www.semanticscholar.org/paper/109186b81a0575936297ae3a0ff41491124a4bf2\",\"venue\":\"AGENTS '01\",\"year\":2001},{\"arxivId\":\"1810.11748\",\"authors\":[{\"authorId\":\"83976651\",\"name\":\"Riku Arakawa\"},{\"authorId\":\"3456592\",\"name\":\"S. Kobayashi\"},{\"authorId\":\"40939427\",\"name\":\"Y. Unno\"},{\"authorId\":\"3229899\",\"name\":\"Yuta Tsuboi\"},{\"authorId\":\"35647224\",\"name\":\"S. Maeda\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"31ed72a18ed8a1008786130af9f1d61761cff4f3\",\"title\":\"DQN-TAMER: Human-in-the-Loop Reinforcement Learning with Intractable Feedback\",\"url\":\"https://www.semanticscholar.org/paper/31ed72a18ed8a1008786130af9f1d61761cff4f3\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1707.05173\",\"authors\":[{\"authorId\":\"144444610\",\"name\":\"William Saunders\"},{\"authorId\":\"144864359\",\"name\":\"Girish Sastry\"},{\"authorId\":\"2214496\",\"name\":\"Andreas Stuhlm\\u00fcller\"},{\"authorId\":\"47107786\",\"name\":\"Owain Evans\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"30ff82cebce6fdc2957043c4085a426414474d78\",\"title\":\"Trial without Error: Towards Safe Reinforcement Learning via Human Intervention\",\"url\":\"https://www.semanticscholar.org/paper/30ff82cebce6fdc2957043c4085a426414474d78\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":\"1707.02201\",\"authors\":[{\"authorId\":\"1879232\",\"name\":\"J. Merel\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"22216833\",\"name\":\"TB Dhruva\"},{\"authorId\":\"144999731\",\"name\":\"S. Srinivasan\"},{\"authorId\":\"144083287\",\"name\":\"Jay Lemmon\"},{\"authorId\":\"47197117\",\"name\":\"Ziyu Wang\"},{\"authorId\":\"89504302\",\"name\":\"G. Wayne\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"e6e01f580c973d91f6445d839389f9f2d5efc78e\",\"title\":\"Learning human behaviors from motion capture by adversarial imitation\",\"url\":\"https://www.semanticscholar.org/paper/e6e01f580c973d91f6445d839389f9f2d5efc78e\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144389295\",\"name\":\"C. Wirth\"},{\"authorId\":\"1688702\",\"name\":\"R. Akrour\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"},{\"authorId\":\"1747752\",\"name\":\"J. F\\u00fcrnkranz\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"84082634110fcedaaa32632f6cc16a034eedb2a0\",\"title\":\"A Survey of Preference-Based Reinforcement Learning Methods\",\"url\":\"https://www.semanticscholar.org/paper/84082634110fcedaaa32632f6cc16a034eedb2a0\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2017},{\"arxivId\":\"1805.01954\",\"authors\":[{\"authorId\":\"46221670\",\"name\":\"F. Torabi\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.24963/ijcai.2018/687\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"cc2fb12eaa4dae74c5de0799b29624b5c585c43b\",\"title\":\"Behavioral Cloning from Observation\",\"url\":\"https://www.semanticscholar.org/paper/cc2fb12eaa4dae74c5de0799b29624b5c585c43b\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152300037\",\"name\":\"J. Young\"}],\"doi\":\"10.1038/230260b0\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b6cea23b4a07b66e3122252f47f9cc715b469cf9\",\"title\":\"Machine Intelligence\",\"url\":\"https://www.semanticscholar.org/paper/b6cea23b4a07b66e3122252f47f9cc715b469cf9\",\"venue\":\"Nature\",\"year\":1971},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Charles Isbell\"},{\"authorId\":null,\"name\":\"Christian R Shelton\"},{\"authorId\":null,\"name\":\"Michael Kearns\"},{\"authorId\":null,\"name\":\"Satinder Singh\"},{\"authorId\":null,\"name\":\"Peter Stone. A social reinforcement learning agent. In P agents\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 377\\u2013384\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Riad Akrour\"},{\"authorId\":null,\"name\":\"Marc Schoenauer\"},{\"authorId\":null,\"name\":\"Mich\\u00e8le Sebag\"},{\"authorId\":null,\"name\":\"Jean-Christophe Souplet. Programming by feedback. In International Learning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 32\",\"url\":\"\",\"venue\":\"pages 1503\\u20131511. JMLR. org,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"47139824\",\"name\":\"A. Fitzgibbon\"},{\"authorId\":\"1742208\",\"name\":\"M. Pollefeys\"},{\"authorId\":\"1681236\",\"name\":\"L. Gool\"},{\"authorId\":\"1688869\",\"name\":\"Andrew Zisserman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"19d6e925ed9643c28981edd233d074b6e3a793c6\",\"title\":\"European conference on computer vision (ECCV)\",\"url\":\"https://www.semanticscholar.org/paper/19d6e925ed9643c28981edd233d074b6e3a793c6\",\"venue\":\"eccv 2006\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"MacGlashan et al\"},{\"authorId\":null,\"name\":\"2017 James MacGlashan\"},{\"authorId\":null,\"name\":\"Mark K Ho\"},{\"authorId\":null,\"name\":\"Robert Loftin\"},{\"authorId\":null,\"name\":\"Bei Peng\"},{\"authorId\":null,\"name\":\"Guan Wang\"},{\"authorId\":null,\"name\":\"David L Roberts\"},{\"authorId\":null,\"name\":\"Matthew E Taylor\"},{\"authorId\":null,\"name\":\"Michael L Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Interactive learning from policydependent human feedback\",\"url\":\"\",\"venue\":\"In Proceedings of the 34th International Conference on Machine Learning-Volume\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1838206.1838208\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"accb4b7a1e670ec1be3d5a2e784b8f524ff8b303\",\"title\":\"Combining manual feedback with subsequent MDP reward signals for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/accb4b7a1e670ec1be3d5a2e784b8f524ff8b303\",\"venue\":\"AAMAS\",\"year\":2010},{\"arxivId\":\"1805.07914\",\"authors\":[{\"authorId\":\"48779623\",\"name\":\"A. Edwards\"},{\"authorId\":\"34594615\",\"name\":\"Himanshu Sahni\"},{\"authorId\":\"3403061\",\"name\":\"Yannick Schroecker\"},{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"395ea8a62d84c8dd85a8dadfc3043cf2228e38c5\",\"title\":\"Imitating Latent Policies from Observation\",\"url\":\"https://www.semanticscholar.org/paper/395ea8a62d84c8dd85a8dadfc3043cf2228e38c5\",\"venue\":\"ICML\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"St\\u00e9phane Ross\"},{\"authorId\":null,\"name\":\"Geoffrey J Gordon\"},{\"authorId\":null,\"name\":\"Drew Bagnell. A reduction of imitation learning\"},{\"authorId\":null,\"name\":\"structured prediction to no-regret online learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Artificial Intelligence and Statistics\",\"url\":\"\",\"venue\":\"pages 627\\u2013635,\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1701.04079\",\"authors\":[{\"authorId\":\"152422014\",\"name\":\"David Abel\"},{\"authorId\":\"3373139\",\"name\":\"J. Salvatier\"},{\"authorId\":\"2214496\",\"name\":\"Andreas Stuhlm\\u00fcller\"},{\"authorId\":\"47107786\",\"name\":\"Owain Evans\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e245ef09fe9e4f56d0fa7f75257298588f4d0392\",\"title\":\"Agent-Agnostic Human-in-the-Loop Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/e245ef09fe9e4f56d0fa7f75257298588f4d0392\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"144507887\",\"name\":\"M. R. Dawson\"},{\"authorId\":\"49491434\",\"name\":\"Thomas Degris\"},{\"authorId\":\"3252520\",\"name\":\"F. Fahimi\"},{\"authorId\":\"145853353\",\"name\":\"J. Carey\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1109/ICORR.2011.5975338\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"153dcdb9ffbd61f7f90cae0d44c154b3c0146359\",\"title\":\"Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/153dcdb9ffbd61f7f90cae0d44c154b3c0146359\",\"venue\":\"2011 IEEE International Conference on Rehabilitation Robotics\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"34699434\",\"name\":\"A. Ng\"}],\"doi\":\"10.1145/1015330.1015430\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"title\":\"Apprenticeship learning via inverse reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/f65020fc3b1692d7989e099d6b6e698be5a50a93\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2543534\",\"name\":\"M. Ho\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"2053350\",\"name\":\"F. Cushman\"},{\"authorId\":\"2494174\",\"name\":\"Joseph L. Austerweil\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f07062e8a8befecb50022d9f432c52dbfaf4c872\",\"title\":\"Showing versus doing: Teaching by demonstration\",\"url\":\"https://www.semanticscholar.org/paper/f07062e8a8befecb50022d9f432c52dbfaf4c872\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1597735.1597738\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"256c3bd45ab7452bb51721eb25d3367bb654225e\",\"title\":\"Interactively shaping agents via human reinforcement: the TAMER framework\",\"url\":\"https://www.semanticscholar.org/paper/256c3bd45ab7452bb51721eb25d3367bb654225e\",\"venue\":\"K-CAP '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Andrew Y Ng. Apprenticeship learning via inverse reinforc learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"page 1\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"21027373\",\"name\":\"S. Schaal\"}],\"doi\":\"10.1016/S1364-6613(99)01327-3\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f69e05a32fd1541bb41d981bdb013366c9150a85\",\"title\":\"Is imitation learning the route to humanoid robots?\",\"url\":\"https://www.semanticscholar.org/paper/f69e05a32fd1541bb41d981bdb013366c9150a85\",\"venue\":\"Trends in Cognitive Sciences\",\"year\":1999},{\"arxivId\":\"1905.09335\",\"authors\":[{\"authorId\":\"46221670\",\"name\":\"F. Torabi\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.24963/ijcai.2019/497\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1706cc1c2433275fc326967da8318790378da850\",\"title\":\"Imitation Learning from Video by Leveraging Proprioception\",\"url\":\"https://www.semanticscholar.org/paper/1706cc1c2433275fc326967da8318790378da850\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Debidatta Dwibedi\"},{\"authorId\":null,\"name\":\"Jonathan Tompson\"},{\"authorId\":null,\"name\":\"Corey Lynch\"},{\"authorId\":null,\"name\":\"Pierre Sermanet. Learning actionable representations from observations\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"url\":\"\",\"venue\":\"pages 1577\\u20131584. IEEE,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1885349\",\"name\":\"Aja Huang\"},{\"authorId\":\"2772217\",\"name\":\"Chris J. Maddison\"},{\"authorId\":\"35099444\",\"name\":\"A. Guez\"},{\"authorId\":\"2175946\",\"name\":\"L. Sifre\"},{\"authorId\":\"47568983\",\"name\":\"George van den Driessche\"},{\"authorId\":\"4337102\",\"name\":\"Julian Schrittwieser\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"2749418\",\"name\":\"Vedavyas Panneershelvam\"},{\"authorId\":\"1975889\",\"name\":\"Marc Lanctot\"},{\"authorId\":\"48373216\",\"name\":\"S. Dieleman\"},{\"authorId\":\"2401609\",\"name\":\"Dominik Grewe\"},{\"authorId\":\"4111313\",\"name\":\"John Nham\"},{\"authorId\":\"2583391\",\"name\":\"Nal Kalchbrenner\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"},{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"40662181\",\"name\":\"M. Leach\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"1686971\",\"name\":\"T. Graepel\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature16961\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"846aedd869a00c09b40f1f1f35673cb22bc87490\",\"title\":\"Mastering the game of Go with deep neural networks and tree search\",\"url\":\"https://www.semanticscholar.org/paper/846aedd869a00c09b40f1f1f35673cb22bc87490\",\"venue\":\"Nature\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"32182645\",\"name\":\"R. Loftin\"},{\"authorId\":\"2323268\",\"name\":\"B. Peng\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"39286677\",\"name\":\"Matthew E. Taylor\"},{\"authorId\":\"145522949\",\"name\":\"Jeff Huang\"},{\"authorId\":\"145630067\",\"name\":\"D. Roberts\"}],\"doi\":\"10.1007/s10458-015-9283-7\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8497e28e813e4d81f46be0c7908b5427f4fdbccd\",\"title\":\"Learning behaviors via human-delivered discrete feedback: modeling implicit feedback strategies to speed up learning\",\"url\":\"https://www.semanticscholar.org/paper/8497e28e813e4d81f46be0c7908b5427f4fdbccd\",\"venue\":\"Autonomous Agents and Multi-Agent Systems\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Faraz Torabi\"},{\"authorId\":null,\"name\":\"Garrett Warnell\"},{\"authorId\":null,\"name\":\"Peter Stone. Behavioral cloning from observation\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 27th International Joint Conference on Artificial Intelligence\",\"url\":\"\",\"venue\":\"pages 4950\\u20134957. AAAI Press,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144056302\",\"name\":\"A. Wilson\"},{\"authorId\":\"145841336\",\"name\":\"A. Fern\"},{\"authorId\":\"1729906\",\"name\":\"P. Tadepalli\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b3af2e367d7297775c71fa9a61b0b49fb888bc38\",\"title\":\"A Bayesian Approach for Policy Learning from Trajectory Preference Queries\",\"url\":\"https://www.semanticscholar.org/paper/b3af2e367d7297775c71fa9a61b0b49fb888bc38\",\"venue\":\"NIPS\",\"year\":2012},{\"arxivId\":\"1703.01703\",\"authors\":[{\"authorId\":\"3275284\",\"name\":\"Bradly C. Stadie\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1701686\",\"name\":\"Ilya Sutskever\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"2e1a1b9c2e8feeb31c6855292859bf94101e8382\",\"title\":\"Third-Person Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/2e1a1b9c2e8feeb31c6855292859bf94101e8382\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":\"1707.03374\",\"authors\":[{\"authorId\":\"49421394\",\"name\":\"Yuxuan Liu\"},{\"authorId\":\"144150283\",\"name\":\"A. Gupta\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.1109/ICRA.2018.8462901\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"77fa0239b9b074e7b62ca3798b8abf6fa3823f80\",\"title\":\"Imitation from Observation: Learning to Imitate Behaviors from Raw Video via Context Translation\",\"url\":\"https://www.semanticscholar.org/paper/77fa0239b9b074e7b62ca3798b8abf6fa3823f80\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":\"1706.03741\",\"authors\":[{\"authorId\":\"29848635\",\"name\":\"Paul F. Christiano\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"31035595\",\"name\":\"T. Brown\"},{\"authorId\":\"26890260\",\"name\":\"Miljan Martic\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd\",\"title\":\"Deep Reinforcement Learning from Human Preferences\",\"url\":\"https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1802.07606\",\"authors\":[{\"authorId\":\"3378188\",\"name\":\"Luisa M. Zintgraf\"},{\"authorId\":\"1917202\",\"name\":\"Diederik M. Roijers\"},{\"authorId\":\"35817159\",\"name\":\"Sjoerd Linders\"},{\"authorId\":\"1689001\",\"name\":\"C. Jonker\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c63afcf8b9fc708100c2143e41abc2b363d43e11\",\"title\":\"Ordered Preference Elicitation Strategies for Supporting Multi-Objective Decision Making\",\"url\":\"https://www.semanticscholar.org/paper/c63afcf8b9fc708100c2143e41abc2b363d43e11\",\"venue\":\"AAMAS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"119820926\",\"name\":\"A. Hussein\"},{\"authorId\":\"1698684\",\"name\":\"M. Gaber\"},{\"authorId\":\"1807106\",\"name\":\"Eyad Elyan\"},{\"authorId\":\"8683625\",\"name\":\"C. Jayne\"}],\"doi\":\"10.1145/3054912\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"269be66378a4314445a781b1bf577c5d61b93b71\",\"title\":\"Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/269be66378a4314445a781b1bf577c5d61b93b71\",\"venue\":\"ACM Comput. Surv.\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1836885\",\"name\":\"Brenna Argall\"},{\"authorId\":\"144753437\",\"name\":\"S. Chernova\"},{\"authorId\":\"1956361\",\"name\":\"M. Veloso\"},{\"authorId\":\"1699032\",\"name\":\"B. Browning\"}],\"doi\":\"10.1016/j.robot.2008.10.024\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"title\":\"A survey of robot learning from demonstration\",\"url\":\"https://www.semanticscholar.org/paper/4e5dfb0b1e54412e799eb0e86d552956cc3a5f54\",\"venue\":\"Robotics Auton. Syst.\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1403905269\",\"name\":\"A. C. Tenorio-Gonz\\u00e1lez\"},{\"authorId\":\"34970419\",\"name\":\"E. Morales\"},{\"authorId\":\"1710020\",\"name\":\"L. Pineda\"}],\"doi\":\"10.1007/978-3-642-16952-6_49\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"58d9d477ed46389bbabf0edde66e77621d9e8034\",\"title\":\"Dynamic Reward Shaping: Training a Robot by Voice\",\"url\":\"https://www.semanticscholar.org/paper/58d9d477ed46389bbabf0edde66e77621d9e8034\",\"venue\":\"IBERAMIA\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Christian Wirth\"},{\"authorId\":null,\"name\":\"Riad Akrour\"},{\"authorId\":null,\"name\":\"Gerhard Neumann\"},{\"authorId\":null,\"name\":\"Johannes F\\u00fcrnkranz. A survey of preference-based reinforcem methods\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"The Journal of Machine Learning Research\",\"url\":\"\",\"venue\":\"18(1):4945\\u20134990,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Argall et al\"},{\"authorId\":null,\"name\":\"2009 Brenna D Argall\"},{\"authorId\":null,\"name\":\"Sonia Chernova\"},{\"authorId\":null,\"name\":\"Manuela Veloso\"},{\"authorId\":null,\"name\":\"Brett Browning\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"A survey of robot learning\",\"url\":\"\",\"venue\":\"\",\"year\":2009},{\"arxivId\":\"1606.03476\",\"authors\":[{\"authorId\":\"2126278\",\"name\":\"Jonathan Ho\"},{\"authorId\":\"2490652\",\"name\":\"S. Ermon\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"title\":\"Generative Adversarial Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/4ab53de69372ec2cd2d90c126b6a100165dc8ed1\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":\"1611.01796\",\"authors\":[{\"authorId\":\"2112400\",\"name\":\"Jacob Andreas\"},{\"authorId\":\"38666915\",\"name\":\"D. Klein\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7\",\"title\":\"Modular Multitask Reinforcement Learning with Policy Sketches\",\"url\":\"https://www.semanticscholar.org/paper/3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1011.0686\",\"authors\":[{\"authorId\":\"1700433\",\"name\":\"S. Ross\"},{\"authorId\":\"21889436\",\"name\":\"G. Gordon\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"title\":\"A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning\",\"url\":\"https://www.semanticscholar.org/paper/79ab3c49903ec8cb339437ccf5cf998607fc313e\",\"venue\":\"AISTATS\",\"year\":2011},{\"arxivId\":\"1903.06754\",\"authors\":[{\"authorId\":\"2657185\",\"name\":\"Ruohan Zhang\"},{\"authorId\":\"2619658\",\"name\":\"Zhuode Liu\"},{\"authorId\":\"144190085\",\"name\":\"L. Guan\"},{\"authorId\":\"13800723\",\"name\":\"L. Zhang\"},{\"authorId\":\"2848854\",\"name\":\"M. Hayhoe\"},{\"authorId\":\"1691804\",\"name\":\"D. Ballard\"}],\"doi\":\"10.1609/AAAI.V34I04.6161\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d9dea72f5a7ca7a6e928eb4dca3962c24df6ca50\",\"title\":\"Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset\",\"url\":\"https://www.semanticscholar.org/paper/d9dea72f5a7ca7a6e928eb4dca3962c24df6ca50\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1207.4708\",\"authors\":[{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"2294249\",\"name\":\"Yavar Naddaf\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":\"10.1613/jair.3912\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f82e4ff4f003581330338aaae71f60316e58dd26\",\"title\":\"The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/f82e4ff4f003581330338aaae71f60316e58dd26\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38637273\",\"name\":\"Hsiu-Chin Lin\"},{\"authorId\":\"20930108\",\"name\":\"P. Ray\"},{\"authorId\":\"39600588\",\"name\":\"M. Howard\"}],\"doi\":\"10.1109/ICRA.2017.7989039\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4a28fd84ba4faeb2ede0e28dec85cf5dbf0119f0\",\"title\":\"Learning task constraints in operational space formulation\",\"url\":\"https://www.semanticscholar.org/paper/4a28fd84ba4faeb2ede0e28dec85cf5dbf0119f0\",\"venue\":\"2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2017},{\"arxivId\":\"1709.06560\",\"authors\":[{\"authorId\":\"40068904\",\"name\":\"Peter Henderson\"},{\"authorId\":\"18014232\",\"name\":\"R. Islam\"},{\"authorId\":\"143902541\",\"name\":\"Philip Bachman\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"2462512\",\"name\":\"D. Meger\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"33690ff21ef1efb576410e656f2e60c89d0307d6\",\"title\":\"Deep Reinforcement Learning that Matters\",\"url\":\"https://www.semanticscholar.org/paper/33690ff21ef1efb576410e656f2e60c89d0307d6\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51312865\",\"name\":\"Robert Pinsler\"},{\"authorId\":\"1688702\",\"name\":\"R. Akrour\"},{\"authorId\":\"40229316\",\"name\":\"Takayuki Osa\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"}],\"doi\":\"10.1109/ICRA.2018.8460907\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"ed83227d6922bfecc0dd0bd15b09cf2dc818018d\",\"title\":\"Sample and Feedback Efficient Hierarchical Reinforcement Learning from Human Preferences\",\"url\":\"https://www.semanticscholar.org/paper/ed83227d6922bfecc0dd0bd15b09cf2dc818018d\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1784820\",\"name\":\"A. Taylor\"}],\"doi\":\"10.1145/1518701.1519022\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"20efcd00528a0b7d3bf28c2da0950c8a0c7bebe6\",\"title\":\"Machine intelligence\",\"url\":\"https://www.semanticscholar.org/paper/20efcd00528a0b7d3bf28c2da0950c8a0c7bebe6\",\"venue\":\"CHI\",\"year\":2009},{\"arxivId\":\"1811.06521\",\"authors\":[{\"authorId\":\"6675568\",\"name\":\"Borja Ibarz\"},{\"authorId\":\"2990741\",\"name\":\"J. Leike\"},{\"authorId\":\"3408089\",\"name\":\"Tobias Pohlen\"},{\"authorId\":\"145659929\",\"name\":\"Geoffrey Irving\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"2698777\",\"name\":\"Dario Amodei\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"325e1b7cec684c22bb3c2cf65205c77eaf55114f\",\"title\":\"Reward learning from human preferences and demonstrations in Atari\",\"url\":\"https://www.semanticscholar.org/paper/325e1b7cec684c22bb3c2cf65205c77eaf55114f\",\"venue\":\"NeurIPS\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ofra Amir\"},{\"authorId\":null,\"name\":\"Ece Kamar\"},{\"authorId\":null,\"name\":\"Andrey Kolobov\"},{\"authorId\":null,\"name\":\"Barbara J Grosz. Interactive teaching strategies for agent Intelligence\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 804\\u2013811\",\"url\":\"\",\"venue\":\"AAAI Press,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"38637273\",\"name\":\"Hsiu-Chin Lin\"},{\"authorId\":\"20930108\",\"name\":\"P. Ray\"},{\"authorId\":\"39600588\",\"name\":\"M. Howard\"}],\"doi\":\"10.1109/icra33291.2017\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e0aba478ab135e3b72b38dc9dd3e4608b213ba53\",\"title\":\"The 2017 IEEE International Conference on Robotics and Automation (ICRA)\",\"url\":\"https://www.semanticscholar.org/paper/e0aba478ab135e3b72b38dc9dd3e4608b213ba53\",\"venue\":\"\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1398396696\",\"name\":\"R. Busa-Fekete\"},{\"authorId\":\"2206023\",\"name\":\"Bal\\u00e1zs Sz\\u00f6r\\u00e9nyi\"},{\"authorId\":\"144834415\",\"name\":\"Paul Weng\"},{\"authorId\":\"145832570\",\"name\":\"Weiwei Cheng\"},{\"authorId\":\"1691955\",\"name\":\"E. H\\u00fcllermeier\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dc29bcdda1e3525283b768bb9cbdc1906dd25466\",\"title\":\"Preference-based Evolutionary Direct Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/dc29bcdda1e3525283b768bb9cbdc1906dd25466\",\"venue\":\"\",\"year\":2013},{\"arxivId\":null,\"authors\":[{\"authorId\":\"46221670\",\"name\":\"F. Torabi\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"92bfa173e091c0340497fb8c1143a1df3c6f1dfd\",\"title\":\"Adversarial Imitation Learning from State-only Demonstrations\",\"url\":\"https://www.semanticscholar.org/paper/92bfa173e091c0340497fb8c1143a1df3c6f1dfd\",\"venue\":\"AAMAS\",\"year\":2019},{\"arxivId\":\"1704.06888\",\"authors\":[{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"},{\"authorId\":\"32245472\",\"name\":\"Corey Lynch\"},{\"authorId\":\"2527420\",\"name\":\"Yevgen Chebotar\"},{\"authorId\":\"2726592\",\"name\":\"Jasmine Hsu\"},{\"authorId\":\"145116380\",\"name\":\"Eric Jang\"},{\"authorId\":\"1745219\",\"name\":\"S. Schaal\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.1109/ICRA.2018.8462891\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"2adae2da173b9dd720c8bcac0250a90a7f1ec697\",\"title\":\"Time-Contrastive Networks: Self-Supervised Learning from Video\",\"url\":\"https://www.semanticscholar.org/paper/2adae2da173b9dd720c8bcac0250a90a7f1ec697\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jacob Andreas\"},{\"authorId\":null,\"name\":\"Dan Klein\"},{\"authorId\":null,\"name\":\"Sergey Levine. Modular multitask reinforcement learning w sketches\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the 34th International Conference on Machine Learning-Volume 70\",\"url\":\"\",\"venue\":\"pages 166\\u2013175. JMLR. org,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3228082\",\"name\":\"Shane Griffith\"},{\"authorId\":\"145362925\",\"name\":\"K. Subramanian\"},{\"authorId\":\"36881095\",\"name\":\"Jonathan Scholz\"},{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"},{\"authorId\":\"1682788\",\"name\":\"A. Thomaz\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"a7c31b6203949bd8ee6db0ebd4e075b980e9569f\",\"title\":\"Policy Shaping: Integrating Human Feedback with Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/a7c31b6203949bd8ee6db0ebd4e075b980e9569f\",\"venue\":\"NIPS\",\"year\":2013},{\"arxivId\":\"1807.06158\",\"authors\":[{\"authorId\":\"46221670\",\"name\":\"F. Torabi\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1f1f8330cddf1f4bf7bd73478223e5c02b69a1ff\",\"title\":\"Generative Adversarial Imitation from Observation\",\"url\":\"https://www.semanticscholar.org/paper/1f1f8330cddf1f4bf7bd73478223e5c02b69a1ff\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":\"1905.13566\",\"authors\":[{\"authorId\":\"46221670\",\"name\":\"F. Torabi\"},{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.24963/ijcai.2019/882\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"cbcbdb44d9d4ad7bb6bf4e9104653aa7623a17c5\",\"title\":\"Recent Advances in Imitation Learning from Observation\",\"url\":\"https://www.semanticscholar.org/paper/cbcbdb44d9d4ad7bb6bf4e9104653aa7623a17c5\",\"venue\":\"IJCAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Hoang Le\"},{\"authorId\":null,\"name\":\"Nan Jiang\"},{\"authorId\":null,\"name\":\"Alekh Agarwal\"},{\"authorId\":null,\"name\":\"Miroslav Dudik\"},{\"authorId\":null,\"name\":\"Yisong Yue\"},{\"authorId\":null,\"name\":\"Hal Daum\\u00e9. Hierarchical imitation\"},{\"authorId\":null,\"name\":\"reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 2923\\u20132932,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144832491\",\"name\":\"E. Todorov\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"}],\"doi\":\"10.1109/IROS.2012.6386109\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"title\":\"MuJoCo: A physics engine for model-based control\",\"url\":\"https://www.semanticscholar.org/paper/b354ee518bfc1ac0d8ac447eece9edb69e92eae1\",\"venue\":\"2012 IEEE/RSJ International Conference on Intelligent Robots and Systems\",\"year\":2012},{\"arxivId\":\"1703.02949\",\"authors\":[{\"authorId\":\"144150283\",\"name\":\"A. Gupta\"},{\"authorId\":\"144373380\",\"name\":\"C. Devin\"},{\"authorId\":\"49421394\",\"name\":\"Yuxuan Liu\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"b9f8a1a9a5aec3dcdd155c4594f25274f6418e11\",\"title\":\"Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b9f8a1a9a5aec3dcdd155c4594f25274f6418e11\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1977663\",\"name\":\"Thomas Cederborg\"},{\"authorId\":\"3141078\",\"name\":\"Ishaan Grover\"},{\"authorId\":\"1787816\",\"name\":\"C. Isbell\"},{\"authorId\":\"1682788\",\"name\":\"A. Thomaz\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"d4ade7b90b9de4c49face0b0866b1b4445bb98f2\",\"title\":\"Policy Shaping with Human Teachers\",\"url\":\"https://www.semanticscholar.org/paper/d4ade7b90b9de4c49face0b0866b1b4445bb98f2\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1682788\",\"name\":\"A. Thomaz\"},{\"authorId\":\"1711777\",\"name\":\"C. Breazeal\"}],\"doi\":\"10.1016/j.artint.2007.09.009\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"c86ba8c7b306e5927e8667011ce40402750e0b52\",\"title\":\"Teachable robots: Understanding human teaching behavior to build more effective robot learners\",\"url\":\"https://www.semanticscholar.org/paper/c86ba8c7b306e5927e8667011ce40402750e0b52\",\"venue\":\"Artif. Intell.\",\"year\":2008},{\"arxivId\":\"1903.04110\",\"authors\":[{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"3307026\",\"name\":\"S. Chang\"},{\"authorId\":\"2482533\",\"name\":\"Mo Yu\"},{\"authorId\":\"1699108\",\"name\":\"G. Tesauro\"},{\"authorId\":\"143903370\",\"name\":\"Murray Campbell\"}],\"doi\":\"10.1609/AAAI.V33I01.33013739\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"15704ce8121737e3eb109a210d4b72e2fe1a0ad7\",\"title\":\"Hybrid Reinforcement Learning with Expert State Sequences\",\"url\":\"https://www.semanticscholar.org/paper/15704ce8121737e3eb109a210d4b72e2fe1a0ad7\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144288136\",\"name\":\"W. B. Knox\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"31936551c1f219abd7e202a1775c9b6d756912bb\",\"title\":\"Reinforcement learning from simultaneous human and MDP reward\",\"url\":\"https://www.semanticscholar.org/paper/31936551c1f219abd7e202a1775c9b6d756912bb\",\"venue\":\"AAMAS\",\"year\":2012},{\"arxivId\":\"1808.00928\",\"authors\":[{\"authorId\":\"2420123\",\"name\":\"D. Dwibedi\"},{\"authorId\":\"2704494\",\"name\":\"J. Tompson\"},{\"authorId\":\"32245472\",\"name\":\"Corey Lynch\"},{\"authorId\":\"3142556\",\"name\":\"Pierre Sermanet\"}],\"doi\":\"10.1109/IROS.2018.8593951\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"93adca9ce6f4a0fab9ea027c90b4df828cfa10d7\",\"title\":\"Learning Actionable Representations from Visual Observations\",\"url\":\"https://www.semanticscholar.org/paper/93adca9ce6f4a0fab9ea027c90b4df828cfa10d7\",\"venue\":\"2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1747752\",\"name\":\"J. F\\u00fcrnkranz\"},{\"authorId\":\"1691955\",\"name\":\"E. H\\u00fcllermeier\"},{\"authorId\":\"49767752\",\"name\":\"W. Cheng\"},{\"authorId\":\"2829854\",\"name\":\"Sang-Hyeun Park\"}],\"doi\":\"10.1007/s10994-012-5313-8\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"93a71e6168839a05609fd6d579e783762d8eca4d\",\"title\":\"Preference-based reinforcement learning: a formal framework and a policy iteration algorithm\",\"url\":\"https://www.semanticscholar.org/paper/93a71e6168839a05609fd6d579e783762d8eca4d\",\"venue\":\"Machine Learning\",\"year\":2012},{\"arxivId\":\"1705.03854\",\"authors\":[{\"authorId\":\"38772386\",\"name\":\"A. Palazzi\"},{\"authorId\":\"3309130\",\"name\":\"Davide Abati\"},{\"authorId\":\"2175529\",\"name\":\"Simone Calderara\"},{\"authorId\":\"2059900\",\"name\":\"Francesco Solera\"},{\"authorId\":\"1741922\",\"name\":\"R. Cucchiara\"}],\"doi\":\"10.1109/TPAMI.2018.2845370\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"914e98db74f29fc608106ff438edde58965037c5\",\"title\":\"Predicting the Driver's Focus of Attention: The DR(eye)VE Project\",\"url\":\"https://www.semanticscholar.org/paper/914e98db74f29fc608106ff438edde58965037c5\",\"venue\":\"IEEE Transactions on Pattern Analysis and Machine Intelligence\",\"year\":2019},{\"arxivId\":\"1811.06711\",\"authors\":[{\"authorId\":\"40229316\",\"name\":\"Takayuki Osa\"},{\"authorId\":\"34906504\",\"name\":\"J. Pajarinen\"},{\"authorId\":\"26599977\",\"name\":\"G. Neumann\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"145197867\",\"name\":\"Jan Peters\"}],\"doi\":\"10.1561/2300000053\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8d6adaa16ed0af9935a1130a305c85e8bdf8780d\",\"title\":\"An Algorithmic Perspective on Imitation Learning\",\"url\":\"https://www.semanticscholar.org/paper/8d6adaa16ed0af9935a1130a305c85e8bdf8780d\",\"venue\":\"Found. Trends Robotics\",\"year\":2018},{\"arxivId\":\"1709.10163\",\"authors\":[{\"authorId\":\"1938253\",\"name\":\"Garrett Warnell\"},{\"authorId\":\"3436871\",\"name\":\"Nicholas R. Waytowich\"},{\"authorId\":\"2194602\",\"name\":\"V. Lawhern\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"abcf11a9af3d83f85c5fbfffc5901d416ca7a73f\",\"title\":\"Deep TAMER: Interactive Agent Shaping in High-Dimensional State Spaces\",\"url\":\"https://www.semanticscholar.org/paper/abcf11a9af3d83f85c5fbfffc5901d416ca7a73f\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1806.03960\",\"authors\":[{\"authorId\":\"2657185\",\"name\":\"Ruohan Zhang\"},{\"authorId\":\"2619658\",\"name\":\"Zhuode Liu\"},{\"authorId\":\"13800723\",\"name\":\"L. Zhang\"},{\"authorId\":\"51002225\",\"name\":\"Jake A. Whritner\"},{\"authorId\":\"47136793\",\"name\":\"K. Muller\"},{\"authorId\":\"2848854\",\"name\":\"M. Hayhoe\"},{\"authorId\":\"1691804\",\"name\":\"D. Ballard\"}],\"doi\":\"10.1007/978-3-030-01252-6_41\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"170643ea1794c4285a491bcea7dede2d35806726\",\"title\":\"AGIL: Learning Attention from Human for Visuomotor Tasks\",\"url\":\"https://www.semanticscholar.org/paper/170643ea1794c4285a491bcea7dede2d35806726\",\"venue\":\"ECCV\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999}],\"title\":\"Leveraging Human Guidance for Deep Reinforcement Learning Tasks\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"High- and low-level\",\"topicId\":\"33507\",\"url\":\"https://www.semanticscholar.org/topic/33507\"},{\"topic\":\"Interaction\",\"topicId\":\"72\",\"url\":\"https://www.semanticscholar.org/topic/72\"},{\"topic\":\"Learning Disorders\",\"topicId\":\"196018\",\"url\":\"https://www.semanticscholar.org/topic/196018\"}],\"url\":\"https://www.semanticscholar.org/paper/9bd453ea1e4655311cb1172f7a188cb9c5ee367d\",\"venue\":\"IJCAI\",\"year\":2019}\n"