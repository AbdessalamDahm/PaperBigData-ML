"{\"abstract\":\"Distribution and sample models are two popular model choices in model-based reinforcement learning (MBRL). However, learning these models can be intractable, particularly when the state and action spaces are large. Expectation models, on the other hand, are relatively easier to learn due to their compactness and have also been widely used for deterministic environments. For stochastic environments, it is not obvious how expectation models can be used for planning as they only partially characterize a distribution. In this paper, we propose a sound way of using approximate expectation models for MBRL. In particular, we 1) show that planning with an expectation model is equivalent to planning with a distribution model if the state value function is linear in state features, 2) analyze two common parametrization choices for approximating the expectation: linear and non-linear expectation models, 3) propose a sound model-based policy evaluation algorithm and present its convergence results, and 4) empirically demonstrate the effectiveness of the proposed planning algorithm.\",\"arxivId\":\"1904.01191\",\"authors\":[{\"authorId\":\"144704982\",\"name\":\"Yi Wan\",\"url\":\"https://www.semanticscholar.org/author/144704982\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\",\"url\":\"https://www.semanticscholar.org/author/145602215\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\",\"url\":\"https://www.semanticscholar.org/author/145240145\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\",\"url\":\"https://www.semanticscholar.org/author/144542337\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\",\"url\":\"https://www.semanticscholar.org/author/1699645\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1906.05243\",\"authors\":[{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"},{\"authorId\":\"39357484\",\"name\":\"Matteo Hessel\"},{\"authorId\":\"9958912\",\"name\":\"J. Aslanides\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"22ade45a75c1ce8ae63feae8d5381316493cefe8\",\"title\":\"When to use parametric models in reinforcement learning?\",\"url\":\"https://www.semanticscholar.org/paper/22ade45a75c1ce8ae63feae8d5381316493cefe8\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"2002.02829\",\"authors\":[{\"authorId\":\"11856260\",\"name\":\"Z. Hou\"},{\"authorId\":\"51237736\",\"name\":\"Kuangen Zhang\"},{\"authorId\":\"1879714\",\"name\":\"Youchuan Wan\"},{\"authorId\":\"48108799\",\"name\":\"Dongyu Li\"},{\"authorId\":\"153219145\",\"name\":\"Chenglong Fu\"},{\"authorId\":\"46492997\",\"name\":\"Haoyong Yu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"26a819ccc6ae088aed7a62f47fdbe1b840807daf\",\"title\":\"Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP)\",\"url\":\"https://www.semanticscholar.org/paper/26a819ccc6ae088aed7a62f47fdbe1b840807daf\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"31720152\",\"name\":\"Dong Li\"},{\"authorId\":\"119573347\",\"name\":\"Dongbin Zhao\"},{\"authorId\":\"3342918\",\"name\":\"Qichao Zhang\"}],\"doi\":\"10.1109/SSCI44817.2019.9003029\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"11d9530de666d912bbc3b33e53a80b0e0f35e895\",\"title\":\"Reinforcement Learning based Lane Change Decision-Making with Imaginary Sampling\",\"url\":\"https://www.semanticscholar.org/paper/11d9530de666d912bbc3b33e53a80b0e0f35e895\",\"venue\":\"2019 IEEE Symposium Series on Computational Intelligence (SSCI)\",\"year\":2019},{\"arxivId\":\"2010.13685\",\"authors\":[{\"authorId\":\"2003609520\",\"name\":\"Veronica Chelu\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"a64277b0ffb2c47450e7b22406e83c8ff5ce8247\",\"title\":\"Forethought and Hindsight in Credit Assignment\",\"url\":\"https://www.semanticscholar.org/paper/a64277b0ffb2c47450e7b22406e83c8ff5ce8247\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2009.13579\",\"authors\":[{\"authorId\":\"117899652\",\"name\":\"Ruo Yu Tao\"},{\"authorId\":\"1389921282\",\"name\":\"Vincent Fran\\u00e7ois-Lavet\"},{\"authorId\":\"145134886\",\"name\":\"Joelle Pineau\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d4d86f4a632d899ba5f511c1d0e7a7fbd44288c8\",\"title\":\"Novelty Search in representational space for sample efficient exploration\",\"url\":\"https://www.semanticscholar.org/paper/d4d86f4a632d899ba5f511c1d0e7a7fbd44288c8\",\"venue\":\"NeurIPS\",\"year\":2020}],\"corpusId\":91184649,\"doi\":\"10.24963/ijcai.2019/506\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"5d014e75d60340a952101b8cbc0e440cc8580873\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":\"1806.04624\",\"authors\":[{\"authorId\":\"7303313\",\"name\":\"Yangchen Pan\"},{\"authorId\":\"145602215\",\"name\":\"M. Zaheer\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"145690938\",\"name\":\"Andrew Patterson\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":\"10.24963/ijcai.2018/666\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"title\":\"Organizing Experience: a Deeper Look at Replay Mechanisms for Sample-Based Planning in Continuous State Domains\",\"url\":\"https://www.semanticscholar.org/paper/ba7a309fcc8dd361bddd27662fdfd68294e58b80\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":\"1707.08316\",\"authors\":[{\"authorId\":\"2324770\",\"name\":\"Lei Le\"},{\"authorId\":\"2188358\",\"name\":\"R. Kumaraswamy\"},{\"authorId\":\"144542337\",\"name\":\"Martha White\"}],\"doi\":\"10.24963/ijcai.2017/287\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"77e0c0d01acc69950e403cc7767fbbb5bd7b8430\",\"title\":\"Learning Sparse Representations in Reinforcement Learning with Sparse Coding\",\"url\":\"https://www.semanticscholar.org/paper/77e0c0d01acc69950e403cc7767fbbb5bd7b8430\",\"venue\":\"IJCAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"R\\u00e9mi Coulom. Efficient selectivity\"},{\"authorId\":null,\"name\":\"backup operators in monte-carlo tree search. In Internati computers\"},{\"authorId\":null,\"name\":\"games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 72\\u201383\",\"url\":\"\",\"venue\":\"Springer,\",\"year\":2006},{\"arxivId\":\"1611.07078\",\"authors\":[{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"1684887\",\"name\":\"Nate Kushman\"},{\"authorId\":\"145186674\",\"name\":\"Katja Hofmann\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"716776b39660f9e13859fa79790eb416d826fff3\",\"title\":\"A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/716776b39660f9e13859fa79790eb416d826fff3\",\"venue\":\"ICLR 2016\",\"year\":2016},{\"arxivId\":\"1507.08750\",\"authors\":[{\"authorId\":\"2894414\",\"name\":\"Junhyuk Oh\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"1697141\",\"name\":\"H. Lee\"},{\"authorId\":\"46328485\",\"name\":\"R. L. Lewis\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"e4257bc131c36504a04382290cbc27ca8bb27813\",\"title\":\"Action-Conditional Video Prediction using Deep Networks in Atari Games\",\"url\":\"https://www.semanticscholar.org/paper/e4257bc131c36504a04382290cbc27ca8bb27813\",\"venue\":\"NIPS\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1797222\",\"name\":\"H. Maei\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"143683893\",\"name\":\"S. Bhatnagar\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"1766844\",\"name\":\"Eric Wiewiora\"}],\"doi\":\"10.1145/1553374.1553501\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a97ba611613d6ee20ec441a15e18cab9d4ebd3e6\",\"title\":\"Fast gradient-descent methods for temporal-difference learning with linear function approximation\",\"url\":\"https://www.semanticscholar.org/paper/a97ba611613d6ee20ec441a15e18cab9d4ebd3e6\",\"venue\":\"ICML '09\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"2946888\",\"name\":\"M. Delp\"},{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"1780797\",\"name\":\"P. Pilarski\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"50e9a441f56124b7b969e6537b66469a0e1aa707\",\"title\":\"Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction\",\"url\":\"https://www.semanticscholar.org/paper/50e9a441f56124b7b969e6537b66469a0e1aa707\",\"venue\":\"AAMAS\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2670689\",\"name\":\"J. Meyer\"},{\"authorId\":\"2221945\",\"name\":\"A. Guillot\"}],\"doi\":\"10.4135/9781412950565.n6\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"3dd614ec69ca6ea73d7d096752d6daf4b4efc896\",\"title\":\"Adaptive Behavior\",\"url\":\"https://www.semanticscholar.org/paper/3dd614ec69ca6ea73d7d096752d6daf4b4efc896\",\"venue\":\"\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1786249\",\"name\":\"D. Bertsekas\"},{\"authorId\":\"144224173\",\"name\":\"J. Tsitsiklis\"}],\"doi\":\"10.1007/978-0-387-30164-8_588\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"81b2a749e6f1e21809b3c7cdb12cc33037e66055\",\"title\":\"Neuro-Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/81b2a749e6f1e21809b3c7cdb12cc33037e66055\",\"venue\":\"Encyclopedia of Machine Learning\",\"year\":1996},{\"arxivId\":\"1611.05397\",\"authors\":[{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"144792148\",\"name\":\"W. Czarnecki\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"1700356\",\"name\":\"Joel Z. Leibo\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"d7bd6e3addd8bc8e2e154048300eea15f030ed33\",\"title\":\"Reinforcement Learning with Unsupervised Auxiliary Tasks\",\"url\":\"https://www.semanticscholar.org/paper/d7bd6e3addd8bc8e2e154048300eea15f030ed33\",\"venue\":\"ICLR\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3119801\",\"name\":\"Xavier Glorot\"},{\"authorId\":\"1751762\",\"name\":\"Yoshua Bengio\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b71ac1e9fb49420d13e084ac67254a0bbd40f83f\",\"title\":\"Understanding the difficulty of training deep feedforward neural networks\",\"url\":\"https://www.semanticscholar.org/paper/b71ac1e9fb49420d13e084ac67254a0bbd40f83f\",\"venue\":\"AISTATS\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2136886\",\"name\":\"V. Borkar\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d889403623f3e4717b233fc4ed19718f950e3428\",\"title\":\"Stochastic Approximation: A Dynamical Systems Viewpoint\",\"url\":\"https://www.semanticscholar.org/paper/d889403623f3e4717b233fc4ed19718f950e3428\",\"venue\":\"\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"35367799\",\"name\":\"A. White\"}],\"doi\":\"10.7939/R3FF3M75H\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"38170c31740636c8970c472c01917a3aea25394a\",\"title\":\"DEVELOPING A PREDICTIVE APPROACH TO KNOWLEDGE\",\"url\":\"https://www.semanticscholar.org/paper/38170c31740636c8970c472c01917a3aea25394a\",\"venue\":\"\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760402\",\"name\":\"A. Moore\"},{\"authorId\":\"8483722\",\"name\":\"C. Atkeson\"}],\"doi\":\"10.1007/BF00993104\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"74921bc8762812345b7010746faa22988b85252e\",\"title\":\"Prioritized sweeping: Reinforcement learning with less data and less time\",\"url\":\"https://www.semanticscholar.org/paper/74921bc8762812345b7010746faa22988b85252e\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Wesley Chung\"},{\"authorId\":null,\"name\":\"Somjit Nath\"},{\"authorId\":null,\"name\":\"Ajin Joseph\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"and Martha White\",\"url\":\"\",\"venue\":\"Two-timescale networks for nonlinear value function approximation.\",\"year\":2018},{\"arxivId\":\"1206.3285\",\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"},{\"authorId\":\"1979505\",\"name\":\"A. Geramifard\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"background\",\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"eaf557c627d441deb0b4b5e033a3a4f5518f4715\",\"title\":\"Dyna-Style Planning with Linear Function Approximation and Prioritized Sweeping\",\"url\":\"https://www.semanticscholar.org/paper/eaf557c627d441deb0b4b5e033a3a4f5518f4715\",\"venue\":\"UAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2261881\",\"name\":\"M. Deisenroth\"},{\"authorId\":\"3472959\",\"name\":\"C. Rasmussen\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"60b7d47758a71978e74edff6dd8dea4d9c791d7a\",\"title\":\"PILCO: A Model-Based and Data-Efficient Approach to Policy Search\",\"url\":\"https://www.semanticscholar.org/paper/60b7d47758a71978e74edff6dd8dea4d9c791d7a\",\"venue\":\"ICML\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"cbeb58496711887bf563ad7b0a2860fd46bcd725\",\"title\":\"Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding\",\"url\":\"https://www.semanticscholar.org/paper/cbeb58496711887bf563ad7b0a2860fd46bcd725\",\"venue\":\"NIPS\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Xavier Glorot\"},{\"authorId\":null,\"name\":\"Yoshua Bengio. Understanding the difficulty of training d networks\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In Proceedings of the thirteenth international conference on artificial intelligence and statistics\",\"url\":\"\",\"venue\":\"pages 249\\u2013256,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1795401\",\"name\":\"Sarah Osentoski\"},{\"authorId\":\"143640165\",\"name\":\"P. S. Thomas\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"c29b7798b6839b84b87b4910f6263ee5a89f9279\",\"title\":\"Value Function Approximation in Reinforcement Learning Using the Fourier Basis\",\"url\":\"https://www.semanticscholar.org/paper/c29b7798b6839b84b87b4910f6263ee5a89f9279\",\"venue\":\"AAAI\",\"year\":2011},{\"arxivId\":null,\"authors\":[{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"2189083\",\"name\":\"G. Taylor\"},{\"authorId\":\"1409211035\",\"name\":\"Christopher Painter-Wakefield\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":\"10.1145/1390156.1390251\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"504f7b5cb348a29f7192b877627f87f9f9c72590\",\"title\":\"An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/504f7b5cb348a29f7192b877627f87f9f9c72590\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Sina Ghiassian\"},{\"authorId\":null,\"name\":\"Andrew Patterson\"},{\"authorId\":null,\"name\":\"Martha White\"},{\"authorId\":null,\"name\":\"Richard S. Sutton\"},{\"authorId\":null,\"name\":\"Adam White. Online off-policy prediction\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1811.02597,\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1844179\",\"name\":\"L. Baird\"}],\"doi\":\"10.1016/b978-1-55860-377-6.50013-x\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"f518bffb712a298bff18248c67f6fc0181018ae6\",\"title\":\"Residual Algorithms: Reinforcement Learning with Function Approximation\",\"url\":\"https://www.semanticscholar.org/paper/f518bffb712a298bff18248c67f6fc0181018ae6\",\"venue\":\"ICML\",\"year\":1995},{\"arxivId\":\"1112.1133\",\"authors\":[{\"authorId\":\"3321484\",\"name\":\"Joseph Modayil\"},{\"authorId\":\"145240145\",\"name\":\"Adam White\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1177/1059712313511648\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e02686db023498ddd6cefa3c66ac39618400a989\",\"title\":\"Multi-timescale nexting in a reinforcement learning robot\",\"url\":\"https://www.semanticscholar.org/paper/e02686db023498ddd6cefa3c66ac39618400a989\",\"venue\":\"Adapt. Behav.\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"52cc45c96a5c94c9c3464a4466ebdf1eb5f2a9b3\",\"title\":\"Integrated Modeling and Control Based on Reinforcement Learning and Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/52cc45c96a5c94c9c3464a4466ebdf1eb5f2a9b3\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Huizhen Yu. Convergence of least squares temporal differen conditions\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 1207\\u20131214,\",\"year\":2010}],\"title\":\"Planning with Expectation Models\",\"topics\":[{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Nonlinear system\",\"topicId\":\"5329\",\"url\":\"https://www.semanticscholar.org/topic/5329\"},{\"topic\":\"Consistency model\",\"topicId\":\"110252\",\"url\":\"https://www.semanticscholar.org/topic/110252\"},{\"topic\":\"Feedback\",\"topicId\":\"242\",\"url\":\"https://www.semanticscholar.org/topic/242\"},{\"topic\":\"Gradient\",\"topicId\":\"3221\",\"url\":\"https://www.semanticscholar.org/topic/3221\"},{\"topic\":\"Automated planning and scheduling\",\"topicId\":\"3649\",\"url\":\"https://www.semanticscholar.org/topic/3649\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"}],\"url\":\"https://www.semanticscholar.org/paper/5d014e75d60340a952101b8cbc0e440cc8580873\",\"venue\":\"IJCAI\",\"year\":2019}\n"