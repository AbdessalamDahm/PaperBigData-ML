"{\"abstract\":\"In a single-agent setting, reinforcement learning (RL) tasks can be cast into an inference problem by introducing a binary random variable o, which stands for the \\\"optimality\\\". In this paper, we redefine the binary random variable o in multi-agent setting and formalize multi-agent reinforcement learning (MARL) as probabilistic inference. We derive a variational lower bound of the likelihood of achieving the optimality and name it as Regularized Opponent Model with Maximum Entropy Objective (ROMMEO). From ROMMEO, we present a novel perspective on opponent modeling and show how it can improve the performance of training agents theoretically and empirically in cooperative games. To optimize ROMMEO, we first introduce a tabular Q-iteration method ROMMEO-Q with proof of convergence. We extend the exact algorithm to complex environments by proposing an approximate version, ROMMEO-AC. We evaluate these two algorithms on the challenging iterated matrix game and differential game respectively and show that they can outperform strong MARL baselines.\",\"arxivId\":\"1905.08087\",\"authors\":[{\"authorId\":\"152307789\",\"name\":\"Zheng Tian\",\"url\":\"https://www.semanticscholar.org/author/152307789\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\",\"url\":\"https://www.semanticscholar.org/author/50531782\"},{\"authorId\":\"8200689\",\"name\":\"Zhichen Gong\",\"url\":\"https://www.semanticscholar.org/author/8200689\"},{\"authorId\":\"1383122034\",\"name\":\"Faiz Punakkath\",\"url\":\"https://www.semanticscholar.org/author/1383122034\"},{\"authorId\":\"9399556\",\"name\":\"Shihao Zou\",\"url\":\"https://www.semanticscholar.org/author/9399556\"},{\"authorId\":\"48094081\",\"name\":\"J. Wang\",\"url\":\"https://www.semanticscholar.org/author/48094081\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2007.05271\",\"authors\":[{\"authorId\":\"7281978\",\"name\":\"Pier Giuseppe Sessa\"},{\"authorId\":\"1764328\",\"name\":\"Ilija Bogunovic\"},{\"authorId\":\"1731966\",\"name\":\"M. Kamgarpour\"},{\"authorId\":\"153243248\",\"name\":\"A. Krause\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"1f5d2ef2cdf7bae1abf409abcf7c085d7a4d3524\",\"title\":\"Learning to Play Sequential Games versus Unknown Opponents\",\"url\":\"https://www.semanticscholar.org/paper/1f5d2ef2cdf7bae1abf409abcf7c085d7a4d3524\",\"venue\":\"NeurIPS\",\"year\":2020},{\"arxivId\":\"2012.14228\",\"authors\":[{\"authorId\":\"49140611\",\"name\":\"Minne Li\"},{\"authorId\":\"145788064\",\"name\":\"M. Yang\"},{\"authorId\":\"39853706\",\"name\":\"Furui Liu\"},{\"authorId\":\"48259019\",\"name\":\"Xu Chen\"},{\"authorId\":\"2827164\",\"name\":\"Zhitang Chen\"},{\"authorId\":\"48093926\",\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"29badc077cb8101beec67f36bc1b9a53e46124e3\",\"title\":\"Causal World Models by Unsupervised Deconfounding of Physical Dynamics\",\"url\":\"https://www.semanticscholar.org/paper/29badc077cb8101beec67f36bc1b9a53e46124e3\",\"venue\":\"\",\"year\":2020},{\"arxivId\":\"1910.08285\",\"authors\":[{\"authorId\":\"49140611\",\"name\":\"Minne Li\"},{\"authorId\":\"2466694\",\"name\":\"Lisheng Wu\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"},{\"authorId\":\"49605774\",\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"1e0c224181ca656528163d7d4d73119ca962e06a\",\"title\":\"Multi-View Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/1e0c224181ca656528163d7d4d73119ca962e06a\",\"venue\":\"NeurIPS\",\"year\":2019},{\"arxivId\":\"1909.03510\",\"authors\":[{\"authorId\":\"49724536\",\"name\":\"Haifeng Zhang\"},{\"authorId\":\"47483274\",\"name\":\"W. Chen\"},{\"authorId\":\"153857894\",\"name\":\"Zeren Huang\"},{\"authorId\":\"36069327\",\"name\":\"Minne Li\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"48094081\",\"name\":\"J. Wang\"}],\"doi\":\"10.1609/AAAI.V34I05.6226\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"88cfdf2cd0162591e4c20077a68a8d0f66f82351\",\"title\":\"Bi-level Actor-Critic for Multi-agent Coordination\",\"url\":\"https://www.semanticscholar.org/paper/88cfdf2cd0162591e4c20077a68a8d0f66f82351\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":\"1901.09216\",\"authors\":[{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"46584512\",\"name\":\"J. Wang\"}],\"doi\":\"10.24963/ijcai.2020/58\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dd34893d012cf9c391e5af72a51d6bf8a220b2b3\",\"title\":\"Modelling Bounded Rationality in Multi-Agent Interactions by Generalized Recursive Reasoning\",\"url\":\"https://www.semanticscholar.org/paper/dd34893d012cf9c391e5af72a51d6bf8a220b2b3\",\"venue\":\"IJCAI\",\"year\":2020},{\"arxivId\":\"2011.00583\",\"authors\":[{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"2000281109\",\"name\":\"Jun Wang\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"title\":\"An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective\",\"url\":\"https://www.semanticscholar.org/paper/c3662e9176a7ad90020bdd025c179c5925d0b5b0\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2010.09776\",\"authors\":[{\"authorId\":\"1993256824\",\"name\":\"Ming Zhou\"},{\"authorId\":\"114810856\",\"name\":\"Jun Luo\"},{\"authorId\":\"1999944087\",\"name\":\"Julian Villela\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"32603054\",\"name\":\"D. Rusu\"},{\"authorId\":\"96790509\",\"name\":\"Jiayu Miao\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"1999737580\",\"name\":\"Montgomery Alban\"},{\"authorId\":\"19169929\",\"name\":\"Iman Fadakar\"},{\"authorId\":\"1557392267\",\"name\":\"Zheng Chen\"},{\"authorId\":\"1999880087\",\"name\":\"Aurora Chongxi Huang\"},{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"2929483\",\"name\":\"Kimia Hassanzadeh\"},{\"authorId\":\"1701335\",\"name\":\"D. Graves\"},{\"authorId\":\"144230587\",\"name\":\"D. Chen\"},{\"authorId\":\"1999326708\",\"name\":\"Zhengbang Zhu\"},{\"authorId\":\"71726663\",\"name\":\"N. Nguyen\"},{\"authorId\":\"1409732846\",\"name\":\"M. El-Sayed\"},{\"authorId\":\"1704229853\",\"name\":\"Kun Shao\"},{\"authorId\":\"67101359\",\"name\":\"S. Ahilan\"},{\"authorId\":\"1999534203\",\"name\":\"Baokuan Zhang\"},{\"authorId\":\"25531084\",\"name\":\"J. Wu\"},{\"authorId\":\"1999608314\",\"name\":\"Zhengang Fu\"},{\"authorId\":\"34562757\",\"name\":\"K. Rezaee\"},{\"authorId\":\"1443780838\",\"name\":\"Peyman Yadmellat\"},{\"authorId\":\"145367501\",\"name\":\"M. Rohani\"},{\"authorId\":\"1999879303\",\"name\":\"Nicolas Perez Nieves\"},{\"authorId\":\"48530375\",\"name\":\"Yihan Ni\"},{\"authorId\":\"150291826\",\"name\":\"Seyedershad Banijamali\"},{\"authorId\":\"1999769896\",\"name\":\"Alexander Cowen Rivers\"},{\"authorId\":\"50689654\",\"name\":\"Z. Tian\"},{\"authorId\":\"1751630928\",\"name\":\"Daniel Palenicek\"},{\"authorId\":\"46257744\",\"name\":\"H. Ammar\"},{\"authorId\":\"1419462918\",\"name\":\"Hongbo Zhang\"},{\"authorId\":\"2032869\",\"name\":\"W. Liu\"},{\"authorId\":\"40513470\",\"name\":\"Jianye Hao\"},{\"authorId\":\"49606473\",\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"2f91886bde87ab6916dce237e7417f3e247edec8\",\"title\":\"SMARTS: Scalable Multi-Agent Reinforcement Learning Training School for Autonomous Driving\",\"url\":\"https://www.semanticscholar.org/paper/2f91886bde87ab6916dce237e7417f3e247edec8\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":\"2001.03415\",\"authors\":[{\"authorId\":\"7830538\",\"name\":\"M. Liu\"},{\"authorId\":\"92660691\",\"name\":\"M. Zhou\"},{\"authorId\":\"8031058\",\"name\":\"W. Zhang\"},{\"authorId\":\"8773733\",\"name\":\"Y. Zhuang\"},{\"authorId\":\"48094081\",\"name\":\"J. Wang\"},{\"authorId\":\"2032869\",\"name\":\"W. Liu\"},{\"authorId\":\"1811427\",\"name\":\"Y. Yu\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0a7f78c9f736d0d6731ecc8bc67fb5a31bddcb30\",\"title\":\"Multi-Agent Interactions Modeling with Correlated Policies\",\"url\":\"https://www.semanticscholar.org/paper/0a7f78c9f736d0d6731ecc8bc67fb5a31bddcb30\",\"venue\":\"ICLR\",\"year\":2020}],\"corpusId\":159041998,\"doi\":\"10.24963/IJCAI.2019/85\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"6dd89f30a5a0e8c5404f23d05c27ef32149b9179\",\"references\":[{\"arxivId\":\"1803.11485\",\"authors\":[{\"authorId\":\"36054740\",\"name\":\"Tabish Rashid\"},{\"authorId\":\"49089678\",\"name\":\"Mikayel Samvelyan\"},{\"authorId\":\"47542438\",\"name\":\"C. S. Witt\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"ffc211476f2e40e79466ffc198c919a97da3bb76\",\"title\":\"QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ffc211476f2e40e79466ffc198c919a97da3bb76\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3169479\",\"name\":\"G. Chalkiadakis\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":\"10.1145/860575.860689\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dac78569eced221e2eab6ad375f9e33aea944bc8\",\"title\":\"Coordination in multiagent reinforcement learning: a Bayesian approach\",\"url\":\"https://www.semanticscholar.org/paper/dac78569eced221e2eab6ad375f9e33aea944bc8\",\"venue\":\"AAMAS '03\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1753269\",\"name\":\"Brian D. Ziebart\"},{\"authorId\":\"34961461\",\"name\":\"Andrew L. Maas\"},{\"authorId\":\"1756566\",\"name\":\"J. Bagnell\"},{\"authorId\":\"144021446\",\"name\":\"Anind K. Dey\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"11b6bdfe36c48b11367b27187da11d95892f0361\",\"title\":\"Maximum Entropy Inverse Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/11b6bdfe36c48b11367b27187da11d95892f0361\",\"venue\":\"AAAI\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"48057653\",\"name\":\"O. Bagasra\"}],\"doi\":\"10.1073/PNAS.95.17.10344-D\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"title\":\"PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES.\",\"url\":\"https://www.semanticscholar.org/paper/edeff2cfd3f850e76e82dd4aa450607ffee4b432\",\"venue\":\"Science\",\"year\":1915},{\"arxivId\":\"1702.08892\",\"authors\":[{\"authorId\":\"7624658\",\"name\":\"Ofir Nachum\"},{\"authorId\":\"144739074\",\"name\":\"Mohammad Norouzi\"},{\"authorId\":\"36303818\",\"name\":\"Kelvin Xu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"title\":\"Bridging the Gap Between Value and Policy Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/96a067e188f1c89db9faea1fea2314a15ae51bbc\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1805.00909\",\"authors\":[{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba\",\"title\":\"Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review\",\"url\":\"https://www.semanticscholar.org/paper/6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Roy Fox\"},{\"authorId\":null,\"name\":\"Ari Pakman\"},{\"authorId\":null,\"name\":\"Naftali Tishby. Taming the Noise in Reinforcement Learning Updates\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1512.08562, December\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jakob Foerster\"},{\"authorId\":null,\"name\":\"Gregory Farquhar\"},{\"authorId\":null,\"name\":\"Triantafyllos Afouras\"},{\"authorId\":null,\"name\":\"Nantas Nardelli\"},{\"authorId\":null,\"name\":\"Shimon Whiteson. Counterfactual Multi-Agent Policy Gradients\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1705.08926, May\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"He He\"},{\"authorId\":null,\"name\":\"Jordan Boyd-Graber\"},{\"authorId\":null,\"name\":\"Kevin Kwok\"},{\"authorId\":null,\"name\":\"Hal Daum\\u00e9 III. Opponent modeling in deep reinforcement learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1804\\u20131813,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Xi Chen\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Equivalence Between Policy Gradients\"},{\"authorId\":null,\"name\":\"Soft QLearning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1704.06440, April\",\"year\":2017},{\"arxivId\":\"1802.07740\",\"authors\":[{\"authorId\":\"3422052\",\"name\":\"Neil C. Rabinowitz\"},{\"authorId\":\"3054009\",\"name\":\"Frank Perbet\"},{\"authorId\":\"143745193\",\"name\":\"H. Song\"},{\"authorId\":\"40313479\",\"name\":\"C. Zhang\"},{\"authorId\":\"143648071\",\"name\":\"S. Eslami\"},{\"authorId\":\"46378362\",\"name\":\"M. Botvinick\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"856d5dcba4772328b8fb784494e3d41d39669b0d\",\"title\":\"Machine Theory of Mind\",\"url\":\"https://www.semanticscholar.org/paper/856d5dcba4772328b8fb784494e3d41d39669b0d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":\"1512.08562\",\"authors\":[{\"authorId\":\"145609073\",\"name\":\"R. Fox\"},{\"authorId\":\"3314041\",\"name\":\"Ari Pakman\"},{\"authorId\":\"1777660\",\"name\":\"Naftali Tishby\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"4a026fd65af4ba3575e64174de56fee093fa3330\",\"title\":\"Taming the Noise in Reinforcement Learning via Soft Updates\",\"url\":\"https://www.semanticscholar.org/paper/4a026fd65af4ba3575e64174de56fee093fa3330\",\"venue\":\"UAI\",\"year\":2016},{\"arxivId\":\"1804.09817\",\"authors\":[{\"authorId\":\"34765120\",\"name\":\"E. Wei\"},{\"authorId\":\"40617392\",\"name\":\"D. Wicke\"},{\"authorId\":\"2434084\",\"name\":\"David Freelan\"},{\"authorId\":\"1706276\",\"name\":\"S. Luke\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"cfbdca4d494fc2a50c25d69486c15b195b42b989\",\"title\":\"Multiagent Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/cfbdca4d494fc2a50c25d69486c15b195b42b989\",\"venue\":\"AAAI Spring Symposia\",\"year\":2018},{\"arxivId\":\"1802.09640\",\"authors\":[{\"authorId\":\"48647153\",\"name\":\"Roberta Raileanu\"},{\"authorId\":\"40081727\",\"name\":\"Emily L. Denton\"},{\"authorId\":\"3149531\",\"name\":\"Arthur Szlam\"},{\"authorId\":\"2276554\",\"name\":\"R. Fergus\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5634469ad355e69e92a4967b4663ed9ec4c42751\",\"title\":\"Modeling Others using Oneself in Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/5634469ad355e69e92a4967b4663ed9ec4c42751\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Toussaint. Robot trajectory optimization using app inference\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML \\u201909\",\"url\":\"\",\"venue\":\"pages 1049\\u20131056, New York, NY, USA,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Evangelos Theodorou\"},{\"authorId\":null,\"name\":\"Jonas Buchli\"},{\"authorId\":null,\"name\":\"Stefan Schaal. A generalized path integral control approa Res.\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"11\",\"url\":\"\",\"venue\":\"December\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tuomas Haarnoja\"},{\"authorId\":null,\"name\":\"Vitchyr Pong\"},{\"authorId\":null,\"name\":\"Aurick Zhou\"},{\"authorId\":null,\"name\":\"Murtaza Dalal\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Sergey Levine. Composable Deep Reinforcement Learning for Manipulation\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1803.06773, March\",\"year\":2018},{\"arxivId\":\"1901.09207\",\"authors\":[{\"authorId\":\"50531782\",\"name\":\"Ying Wen\"},{\"authorId\":\"49307876\",\"name\":\"Y. Yang\"},{\"authorId\":\"48846956\",\"name\":\"R. Luo\"},{\"authorId\":null,\"name\":\"Jun Wang\"},{\"authorId\":\"144282470\",\"name\":\"W. Pan\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"47a5219cb2ae1ee1c446fc4166e37fe75d91f0b1\",\"title\":\"Probabilistic Recursive Reasoning for Multi-Agent Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/47a5219cb2ae1ee1c446fc4166e37fe75d91f0b1\",\"venue\":\"ICLR\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Tuomas Haarnoja\"},{\"authorId\":null,\"name\":\"Haoran Tang\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Sergey Levine. Reinforcement learning with deep energy-ba policies\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1702.08165,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Marc Toussaint\"},{\"authorId\":null,\"name\":\"Amos Storkey. Probabilistic inference for solving discrete\"},{\"authorId\":null,\"name\":\"continuous state markov decision processes\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"ICML \\u201906\",\"url\":\"\",\"venue\":\"pages 945\\u2013952, New York, NY, USA,\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Caroline Claus\"},{\"authorId\":null,\"name\":\"Craig Boutilier. The dynamics of reinforcement learning systems\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AAAI \\u201998/IAAI \\u201998\",\"url\":\"\",\"venue\":\"Menlo Park, CA, USA,\",\"year\":1998},{\"arxivId\":\"1704.06440\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"41192764\",\"name\":\"Xi Chen\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"d0352057e2b99f65f8b5244a0b912026c86d7b21\",\"title\":\"Equivalence Between Policy Gradients and Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/d0352057e2b99f65f8b5244a0b912026c86d7b21\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael Bowling\"},{\"authorId\":null,\"name\":\"Manuela Veloso. Rational\"},{\"authorId\":null,\"name\":\"convergent learning in stochastic games. In IJCAI\"},{\"authorId\":null,\"name\":\"San Francisco\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CA\",\"url\":\"\",\"venue\":\"USA,\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2236434\",\"name\":\"K. Rawlik\"},{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"},{\"authorId\":\"144575699\",\"name\":\"S. Vijayakumar\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9ee2c730c765d6429c3fa7770d1b5f9a2e74535c\",\"title\":\"On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)\",\"url\":\"https://www.semanticscholar.org/paper/9ee2c730c765d6429c3fa7770d1b5f9a2e74535c\",\"venue\":\"IJCAI\",\"year\":2013},{\"arxivId\":\"1706.02275\",\"authors\":[{\"authorId\":\"2054294\",\"name\":\"Ryan Lowe\"},{\"authorId\":\"31613801\",\"name\":\"Yi Wu\"},{\"authorId\":\"3025260\",\"name\":\"A. Tamar\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"2080746\",\"name\":\"Igor Mordatch\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"title\":\"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\",\"url\":\"https://www.semanticscholar.org/paper/7c3ece1ba41c415d7e81cfa5ca33a8de66efd434\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1609.05559\",\"authors\":[{\"authorId\":\"91070223\",\"name\":\"He He\"},{\"authorId\":\"1389036863\",\"name\":\"Jordan L. Boyd-Graber\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5\",\"title\":\"Opponent Modeling in Deep Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/aa8e4263ef59d095dc0f87fb0dae19b441bfa6c5\",\"venue\":\"ICML\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144771028\",\"name\":\"C. Claus\"},{\"authorId\":\"145646162\",\"name\":\"Craig Boutilier\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"38d35d5581e58dca4e9458501e65c1f85ca754d5\",\"title\":\"The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems\",\"url\":\"https://www.semanticscholar.org/paper/38d35d5581e58dca4e9458501e65c1f85ca754d5\",\"venue\":\"AAAI/IAAI\",\"year\":1998},{\"arxivId\":\"1803.06773\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"144401061\",\"name\":\"Vitchyr H. Pong\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"35904540\",\"name\":\"Murtaza Dalal\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":\"10.1109/ICRA.2018.8460756\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"dee85f0ed3571d7b591e23000848c584242186ef\",\"title\":\"Composable Deep Reinforcement Learning for Robotic Manipulation\",\"url\":\"https://www.semanticscholar.org/paper/dee85f0ed3571d7b591e23000848c584242186ef\",\"venue\":\"2018 IEEE International Conference on Robotics and Automation (ICRA)\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10295260\",\"name\":\"N. Stanietsky\"},{\"authorId\":\"122651067\",\"name\":\"H. \\u0160imi\\u0107\"},{\"authorId\":\"3888282\",\"name\":\"J. Arapovi\\u0107\"},{\"authorId\":\"46403067\",\"name\":\"Amir Toporik\"},{\"authorId\":\"144555680\",\"name\":\"O. Levy\"},{\"authorId\":\"18814437\",\"name\":\"A. Novik\"},{\"authorId\":\"4660267\",\"name\":\"Z. Levine\"},{\"authorId\":\"11537061\",\"name\":\"Meirav Beiman\"},{\"authorId\":\"3502722\",\"name\":\"L. Dassa\"},{\"authorId\":\"3714556\",\"name\":\"H. Achdout\"},{\"authorId\":\"1397179736\",\"name\":\"Noam Stern-Ginossar\"},{\"authorId\":\"16241718\",\"name\":\"Pinhas Tsukerman\"},{\"authorId\":\"4700253\",\"name\":\"S. Jonji\\u0107\"},{\"authorId\":\"3851561\",\"name\":\"O. Mandelboim\"}],\"doi\":\"10.1073/pnas.0903474106\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"87c3c6b98f3cfa972cae3164eabadfd01c1a35c3\",\"title\":\"The interaction of TIGIT with PVR and PVRL2 inhibits human NK cell cytotoxicity\",\"url\":\"https://www.semanticscholar.org/paper/87c3c6b98f3cfa972cae3164eabadfd01c1a35c3\",\"venue\":\"Proceedings of the National Academy of Sciences\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2236434\",\"name\":\"K. Rawlik\"},{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"},{\"authorId\":\"144575699\",\"name\":\"S. Vijayakumar\"}],\"doi\":\"10.15607/RSS.2012.VIII.045\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"4b8f52da1aa977de0ad3ede54b36730cfbf700fd\",\"title\":\"On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference\",\"url\":\"https://www.semanticscholar.org/paper/4b8f52da1aa977de0ad3ede54b36730cfbf700fd\",\"venue\":\"Robotics: Science and Systems\",\"year\":2012},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"George W. Brown. Iterative solution of games by fictitiou play\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAPA\",\"url\":\"\",\"venue\":\"New York,\",\"year\":1951},{\"arxivId\":null,\"authors\":[],\"doi\":\"10.1016/0361-9230(86)90152-8\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0928edcc212554987f7c6d0ea7368f7dcc031232\",\"title\":\"Editors\",\"url\":\"https://www.semanticscholar.org/paper/0928edcc212554987f7c6d0ea7368f7dcc031232\",\"venue\":\"Brain Research Bulletin\",\"year\":1986},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":\"physics/0505066\",\"authors\":[{\"authorId\":\"1792269\",\"name\":\"H. Kappen\"}],\"doi\":\"10.1088/1742-5468/2005/11/P11011\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"b107acedcf1953ff498ff459f915845962c47674\",\"title\":\"Path integrals and symmetry breaking for optimal control theory\",\"url\":\"https://www.semanticscholar.org/paper/b107acedcf1953ff498ff459f915845962c47674\",\"venue\":\"\",\"year\":2005},{\"arxivId\":\"1810.04444\",\"authors\":[{\"authorId\":\"50689654\",\"name\":\"Z. Tian\"},{\"authorId\":\"9399556\",\"name\":\"Shihao Zou\"},{\"authorId\":\"47349087\",\"name\":\"I. Davies\"},{\"authorId\":\"80460077\",\"name\":\"Tim Warr\"},{\"authorId\":\"2466694\",\"name\":\"Lisheng Wu\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"},{\"authorId\":\"48093926\",\"name\":\"J. Wang\"}],\"doi\":\"10.1609/AAAI.V34I05.6217\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"0f647032afbca1b3b948a4ba71c55b93f319e8e4\",\"title\":\"Learning to Communicate Implicitly by Actions\",\"url\":\"https://www.semanticscholar.org/paper/0f647032afbca1b3b948a4ba71c55b93f319e8e4\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Florensa\"},{\"authorId\":null,\"name\":\"Yan Duan\"},{\"authorId\":null,\"name\":\"Pieter Abbeel. Stochastic Neural Networks for Hierarchica Learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"April\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Spiros Kapetanakis\"},{\"authorId\":null,\"name\":\"Daniel Kudenko. Reinforcement learning of coordination in Intelligence\"},{\"authorId\":null,\"name\":\"Menlo Park\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CA\",\"url\":\"\",\"venue\":\"USA,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Abbas Abdolmaleki\"},{\"authorId\":null,\"name\":\"Jost Tobias Springenberg\"},{\"authorId\":null,\"name\":\"Yuval Tassa\"},{\"authorId\":null,\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":null,\"name\":\"Nicolas Heess\"},{\"authorId\":null,\"name\":\"Martin A. Riedmiller. Maximum a posteriori policy optimisation\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1806.06920,\",\"year\":2018},{\"arxivId\":\"1802.03216\",\"authors\":[{\"authorId\":\"1399315491\",\"name\":\"J. Grau-Moya\"},{\"authorId\":\"2505365\",\"name\":\"Felix Leibfried\"},{\"authorId\":\"1398842047\",\"name\":\"Haitham Bou-Ammar\"}],\"doi\":\"10.24963/ijcai.2018/37\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae\",\"title\":\"Balancing Two-Player Stochastic Games with Soft Q-Learning\",\"url\":\"https://www.semanticscholar.org/paper/8f2f2d8cba5bf44bb0c10fd53adf25228655e8ae\",\"venue\":\"IJCAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Aaron Wilson\"},{\"authorId\":null,\"name\":\"Alan Fern\"},{\"authorId\":null,\"name\":\"Prasad Tadepalli. Bayesian policy search for multi-agent discovery\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"AAAI\\u201910\",\"url\":\"\",\"venue\":\"pages 624\\u2013629. AAAI Press,\",\"year\":2010},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ryan Lowe\"},{\"authorId\":null,\"name\":\"Yi Wu\"},{\"authorId\":null,\"name\":\"Aviv Tamar\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Igor Mordatch. Multi-Agent Actor-Critic for Mixed Coope Environments\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1706.02275, June\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144918851\",\"name\":\"Marc Toussaint\"},{\"authorId\":\"1728216\",\"name\":\"A. Storkey\"}],\"doi\":\"10.1145/1143844.1143963\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"65f048e0420d11d099d7130fc16e4bd6e3ad88b1\",\"title\":\"Probabilistic inference for solving discrete and continuous state Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/65f048e0420d11d099d7130fc16e4bd6e3ad88b1\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Roberta Raileanu\"},{\"authorId\":null,\"name\":\"Emily Denton\"},{\"authorId\":null,\"name\":\"Arthur Szlam\"},{\"authorId\":null,\"name\":\"Rob Fergus. Modeling others using oneself in multiagen learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1802.09640,\",\"year\":2018},{\"arxivId\":\"1705.08926\",\"authors\":[{\"authorId\":\"145356667\",\"name\":\"Jakob N. Foerster\"},{\"authorId\":\"38698094\",\"name\":\"Gregory Farquhar\"},{\"authorId\":\"2285516\",\"name\":\"Triantafyllos Afouras\"},{\"authorId\":\"39683441\",\"name\":\"Nantas Nardelli\"},{\"authorId\":\"1766767\",\"name\":\"S. Whiteson\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2b292ff89d808fba10579871591a22f1649cd039\",\"title\":\"Counterfactual Multi-Agent Policy Gradients\",\"url\":\"https://www.semanticscholar.org/paper/2b292ff89d808fba10579871591a22f1649cd039\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":\"1801.01290\",\"authors\":[{\"authorId\":\"2587648\",\"name\":\"T. Haarnoja\"},{\"authorId\":\"35499972\",\"name\":\"Aurick Zhou\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"811df72e210e20de99719539505da54762a11c6d\",\"title\":\"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor\",\"url\":\"https://www.semanticscholar.org/paper/811df72e210e20de99719539505da54762a11c6d\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Ofir Nachum\"},{\"authorId\":null,\"name\":\"Mohammad Norouzi\"},{\"authorId\":null,\"name\":\"Kelvin Xu\"},{\"authorId\":null,\"name\":\"Dale Schuurmans. Bridging the Gap Between Value\"},{\"authorId\":null,\"name\":\"Policy Based Reinforcement Learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1702.08892, February\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Rudolf Kalman. A new approach to linear filtering\"},{\"authorId\":null,\"name\":\"prediction problems. Transactions of the ASME - Journal of ba Engineering\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"82:35\\u201345\",\"url\":\"\",\"venue\":\"01\",\"year\":1960},{\"arxivId\":null,\"authors\":[{\"authorId\":\"50689654\",\"name\":\"Z. Tian\"},{\"authorId\":\"9399556\",\"name\":\"Shihao Zou\"},{\"authorId\":\"80460077\",\"name\":\"Tim Warr\"},{\"authorId\":\"2466694\",\"name\":\"Lisheng Wu\"},{\"authorId\":\"48094908\",\"name\":\"J. Wang\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"2a2f9b13bdfe9bfbd448fe67edfb76fb99b282ad\",\"title\":\"Learning Multi-agent Implicit Communication Through Actions: A Case Study in Contract Bridge, a Collaborative Imperfect-Information Game\",\"url\":\"https://www.semanticscholar.org/paper/2a2f9b13bdfe9bfbd448fe67edfb76fb99b282ad\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1387138786\",\"name\":\"Marc M\\u00e9zard\"},{\"authorId\":\"145710121\",\"name\":\"F. Alcar\\u00e1z\"}],\"doi\":\"10.1088/1742-5468\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0f681b9acc7cf914b5b9281bdfc8087d95c4cafe\",\"title\":\"Journal of Statistical Mechanics: theory and experiment\",\"url\":\"https://www.semanticscholar.org/paper/0f681b9acc7cf914b5b9281bdfc8087d95c4cafe\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Johannes Heinrich\"},{\"authorId\":null,\"name\":\"David Silver. Deep reinforcement learning from self-play games\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"CoRR\",\"url\":\"\",\"venue\":\"abs/1603.01121,\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Jordi Grau-Moya\"},{\"authorId\":null,\"name\":\"Felix Leibfried\"},{\"authorId\":null,\"name\":\"Haitham Bou-Ammar. Balancing Two-Player Stochastic Games w Q-Learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"arXiv e-prints\",\"url\":\"\",\"venue\":\"page arXiv:1802.03216, February\",\"year\":2018}],\"title\":\"A Regularized Opponent Model with Maximum Entropy Objective\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Binary data\",\"topicId\":\"46329\",\"url\":\"https://www.semanticscholar.org/topic/46329\"},{\"topic\":\"Multi-agent system\",\"topicId\":\"3830\",\"url\":\"https://www.semanticscholar.org/topic/3830\"},{\"topic\":\"Baseline (configuration management)\",\"topicId\":\"3403\",\"url\":\"https://www.semanticscholar.org/topic/3403\"},{\"topic\":\"Exact algorithm\",\"topicId\":\"81325\",\"url\":\"https://www.semanticscholar.org/topic/81325\"},{\"topic\":\"Approximation algorithm\",\"topicId\":\"87\",\"url\":\"https://www.semanticscholar.org/topic/87\"},{\"topic\":\"Table (information)\",\"topicId\":\"200\",\"url\":\"https://www.semanticscholar.org/topic/200\"},{\"topic\":\"Landweber iteration\",\"topicId\":\"267370\",\"url\":\"https://www.semanticscholar.org/topic/267370\"},{\"topic\":\"Calculus of variations\",\"topicId\":\"34187\",\"url\":\"https://www.semanticscholar.org/topic/34187\"}],\"url\":\"https://www.semanticscholar.org/paper/6dd89f30a5a0e8c5404f23d05c27ef32149b9179\",\"venue\":\"IJCAI\",\"year\":2019}\n"