"{\"abstract\":\"We investigate how Safe Policy Improvement (SPI) algorithms can exploit the structure of factored Markov decision processes when such structure is unknown a priori. To facilitate the application of reinforcement learning in the real world, SPI provides probabilistic guarantees that policy changes in a running process will improve the performance of this process. However, current SPI algorithms have requirements that might be impractical, such as: (i) availability of a large amount of historical data, or (ii) prior knowledge of the underlying structure. To overcome these limitations we enhance a Factored SPI (FSPI) algorithm with different structure learning methods. The resulting algorithms need fewer samples to improve the policy and require weaker prior knowledge assumptions. In well-factorized domains, the proposed algorithms improve performance significantly compared to a flat SPI algorithm, demonstrating a sample complexity closer to an FSPI algorithm that knows the structure. This indicates that the combination of FSPI and structure learning algorithms is a promising solution to real-world problems involving many variables.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"51893920\",\"name\":\"Thiago D. Sim\\u00e3o\",\"url\":\"https://www.semanticscholar.org/author/51893920\"},{\"authorId\":\"1723205\",\"name\":\"M. Spaan\",\"url\":\"https://www.semanticscholar.org/author/1723205\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1909.05236\",\"authors\":[{\"authorId\":\"51893920\",\"name\":\"Thiago D. Sim\\u00e3o\"},{\"authorId\":\"144100820\",\"name\":\"R. Laroche\"},{\"authorId\":\"153223281\",\"name\":\"R\\u00e9mi Tachet des Combes\"}],\"doi\":\"10.5555/3398761.3398908\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"6ccc13a2916348c7795484665fcd67edc07c5d45\",\"title\":\"Safe Policy Improvement with an Estimated Baseline Policy\",\"url\":\"https://www.semanticscholar.org/paper/6ccc13a2916348c7795484665fcd67edc07c5d45\",\"venue\":\"AAMAS\",\"year\":2020}],\"corpusId\":199466438,\"doi\":\"10.24963/ijcai.2019/479\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"c47f864bdef551522008b6696c0981ac8c62924e\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Philip S. Thomas\"},{\"authorId\":null,\"name\":\"Georgios Theocharous\"},{\"authorId\":null,\"name\":\"Mohammad Ghavamzadeh. High Confidence Policy Improvement. I Learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 2380\\u20132388\",\"url\":\"\",\"venue\":\"JMLR.org,\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Craig Boutilier\"},{\"authorId\":null,\"name\":\"Richard Dearden\"},{\"authorId\":null,\"name\":\"Mois\\u00e9s Goldszmidt. Exploiting Structure in Policy Constru Intelligence\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 1104\\u20131113\",\"url\":\"\",\"venue\":\"Morgan Kaufmann,\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas G Dietterich. The MAXQ Method for Hierarchical Rei Learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"volume 98\",\"url\":\"\",\"venue\":\"pages 118\\u2013126,\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Assaf Hallak\"},{\"authorId\":null,\"name\":\"Fran\\u00e7ois Schnitzler\"},{\"authorId\":null,\"name\":\"Timothy Arthur Mann\"},{\"authorId\":null,\"name\":\"Shie Mannor. Off-policy Modelbased Learning under Unkno Learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"pages 711\\u2013719\",\"url\":\"\",\"venue\":\"JMLR.org,\",\"year\":2015},{\"arxivId\":\"1106.1822\",\"authors\":[{\"authorId\":\"1730156\",\"name\":\"Carlos Guestrin\"},{\"authorId\":\"1736370\",\"name\":\"D. Koller\"},{\"authorId\":\"145726861\",\"name\":\"R. Parr\"},{\"authorId\":\"2262405\",\"name\":\"Shobha Venkataraman\"}],\"doi\":\"10.1613/jair.1000\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"2430b4748c4ffe8782ae4763d327ce48f3655639\",\"title\":\"Efficient Solution Algorithms for Factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/2430b4748c4ffe8782ae4763d327ce48f3655639\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2003},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1730156\",\"name\":\"Carlos Guestrin\"},{\"authorId\":\"2228643\",\"name\":\"Relu Patrascu\"},{\"authorId\":\"1714772\",\"name\":\"Dale Schuurmans\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"8678d49a82020773dfe1e8b5fa789aa927ffcdcb\",\"title\":\"Algorithm-Directed Exploration for Model-Based Reinforcement Learning in Factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/8678d49a82020773dfe1e8b5fa789aa927ffcdcb\",\"venue\":\"ICML\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Diuk\"},{\"authorId\":null,\"name\":\"Lihong Li\"},{\"authorId\":null,\"name\":\"Bethany R. Leffler. The Adaptive k-meteorologists Problem\"},{\"authorId\":null,\"name\":\"Its Application to Structure Learning\"},{\"authorId\":null,\"name\":\"Feature Selection in Reinforcement Learning. In Proc. of I Learning\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 249\\u2013256\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2009},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"I Ronen\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Brafman and Moshe Tennenholtz . R - MAX - A General Polynomial Time Algorithm for Near - Optimal Reinforcement Learn\",\"url\":\"\",\"venue\":\"Journal of Machine Learning Research\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"S. Philip\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"Thomas , Georgios Theocharous , and Mohammad Ghavamzadeh . High Confidence Policy Improvement\",\"url\":\"\",\"venue\":\"Model - Based Reinforcement Learning in Factored - State MDPs . In 2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1804488\",\"name\":\"T. Degris\"},{\"authorId\":\"3211142\",\"name\":\"Olivier Sigaud\"},{\"authorId\":\"1686119\",\"name\":\"P. Wuillemin\"}],\"doi\":\"10.1145/1143844.1143877\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"4dfa8530ca5c8152af5de6cc091052acb24d8b28\",\"title\":\"Learning the structure of Factored Markov Decision Processes in reinforcement learning problems\",\"url\":\"https://www.semanticscholar.org/paper/4dfa8530ca5c8152af5de6cc091052acb24d8b28\",\"venue\":\"ICML '06\",\"year\":2006},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"background\"],\"isInfluential\":true,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"152175853\",\"name\":\"L. Goddard\"}],\"doi\":\"10.1038/222304c0\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"60ec5ddae05190537c72974b8f93beb46b8db857\",\"title\":\"Operations Research\",\"url\":\"https://www.semanticscholar.org/paper/60ec5ddae05190537c72974b8f93beb46b8db857\",\"venue\":\"Nature\",\"year\":1969},{\"arxivId\":\"1607.03842\",\"authors\":[{\"authorId\":\"1678622\",\"name\":\"M. Ghavamzadeh\"},{\"authorId\":\"145630605\",\"name\":\"M. Petrik\"},{\"authorId\":\"1819830\",\"name\":\"Yinlam Chow\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"title\":\"Safe Policy Improvement by Minimizing Robust Baseline Regret\",\"url\":\"https://www.semanticscholar.org/paper/bd58d3265263a87159a3f7ed3a5e4c887c5c0792\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"9aa1d909544fd9ffe061b84a90eb344ac303e6d9\",\"title\":\"The MAXQ Method for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/9aa1d909544fd9ffe061b84a90eb344ac303e6d9\",\"venue\":\"ICML\",\"year\":1998},{\"arxivId\":null,\"authors\":[{\"authorId\":\"51893920\",\"name\":\"Thiago D. Sim\\u00e3o\"},{\"authorId\":\"1723205\",\"name\":\"M. Spaan\"}],\"doi\":\"10.1609/AAAI.V33I01.33014967\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":false,\"paperId\":\"7f6094b905bf79af6ea27462ef8d1d7318f70503\",\"title\":\"Safe Policy Improvement with Baseline Bootstrapping in Factored Environments\",\"url\":\"https://www.semanticscholar.org/paper/7f6094b905bf79af6ea27462ef8d1d7318f70503\",\"venue\":\"AAAI\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Carlos Guestrin\"},{\"authorId\":null,\"name\":\"Relu Patrascu\"},{\"authorId\":null,\"name\":\"Dale Schuurmans. Algorithm-Directed Exploration for Mod Learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 235\\u2013242\",\"url\":\"\",\"venue\":\"Morgan Kaufmann,\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1770217\",\"name\":\"D. Chakraborty\"}],\"doi\":\"10.1007/978-3-319-02606-0_7\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"9a3d63ebc6fcc9f33a88d130fd6c8e9c188603bf\",\"title\":\"Structure Learning in Factored MDPs\",\"url\":\"https://www.semanticscholar.org/paper/9a3d63ebc6fcc9f33a88d130fd6c8e9c188603bf\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1927205\",\"name\":\"D. Liu\"},{\"authorId\":\"1708654\",\"name\":\"R. Munos\"},{\"authorId\":\"144568218\",\"name\":\"J. Si\"},{\"authorId\":\"145033828\",\"name\":\"D. Wunsch\"}],\"doi\":\"10.1109/ADPRL.2007.368158\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"9729d9e5db7fc3dfb69dd82afe2e02fe7e4a3fa6\",\"title\":\"2007 IEEE international symposium on approximate dynamic programming and reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/9729d9e5db7fc3dfb69dd82afe2e02fe7e4a3fa6\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2014574318\",\"name\":\"Hang Li\"},{\"authorId\":\"1400659302\",\"name\":\"Dinh Phung\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"b401747b5e93b99585f5fe367d34750a5d7da030\",\"title\":\"Journal of Machine Learning Research: Preface\",\"url\":\"https://www.semanticscholar.org/paper/b401747b5e93b99585f5fe367d34750a5d7da030\",\"venue\":\"\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10418917\",\"name\":\"J. Garcia\"},{\"authorId\":\"143901279\",\"name\":\"F. Fern\\u00e1ndez\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"c0f2c4104ef6e36bb67022001179887e6600d24d\",\"title\":\"A comprehensive survey on safe reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/c0f2c4104ef6e36bb67022001179887e6600d24d\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Alexander L. Strehl\"},{\"authorId\":null,\"name\":\"Carlos Diuk\"},{\"authorId\":null,\"name\":\"Michael L. Littman. Efficient Structure Learning in Factor Intelligence\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 645\\u2013650\",\"url\":\"\",\"venue\":\"AAAI Press,\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990806\",\"name\":\"Alexander L. Strehl\"},{\"authorId\":\"1681946\",\"name\":\"Carlos Diuk\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"15ba832dbb1882b2c516e06cb0f8f002d16bc4cf\",\"title\":\"Efficient Structure Learning in Factored-State MDPs\",\"url\":\"https://www.semanticscholar.org/paper/15ba832dbb1882b2c516e06cb0f8f002d16bc4cf\",\"venue\":\"AAAI\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":\"10213745\",\"name\":\"M. Pollack\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"27c593d8b1a3b6d51d33e13a0fc75052dd921775\",\"title\":\"Journal of Artificial Intelligence Research: Preface\",\"url\":\"https://www.semanticscholar.org/paper/27c593d8b1a3b6d51d33e13a0fc75052dd921775\",\"venue\":\"\",\"year\":2001},{\"arxivId\":null,\"authors\":[{\"authorId\":\"28929337\",\"name\":\"L. Li\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"144926179\",\"name\":\"T. Walsh\"}],\"doi\":\"10.1145/1390156.1390228\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"36d12719d0a077983d9106608c35e5f5364eaa06\",\"title\":\"Knows what it knows: a framework for self-aware learning\",\"url\":\"https://www.semanticscholar.org/paper/36d12719d0a077983d9106608c35e5f5364eaa06\",\"venue\":\"ICML\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":\"7433481\",\"name\":\"Onur Teymur\"},{\"authorId\":\"90421119\",\"name\":\"Kostas Zygalakis\"},{\"authorId\":\"2476875\",\"name\":\"B. Calderhead\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"a4fd9f07f40e8935139edfdac3b8cf64c7a4e351\",\"title\":\"Advances in Neural Information Processing Systems 29\",\"url\":\"https://www.semanticscholar.org/paper/a4fd9f07f40e8935139edfdac3b8cf64c7a4e351\",\"venue\":\"\",\"year\":2016}],\"title\":\"Structure Learning for Safe Policy Improvement\",\"topics\":[{\"topic\":\"Algorithm\",\"topicId\":\"305\",\"url\":\"https://www.semanticscholar.org/topic/305\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Decision tree\",\"topicId\":\"20672\",\"url\":\"https://www.semanticscholar.org/topic/20672\"},{\"topic\":\"Markov decision process\",\"topicId\":\"2556\",\"url\":\"https://www.semanticscholar.org/topic/2556\"},{\"topic\":\"Machine learning\",\"topicId\":\"168\",\"url\":\"https://www.semanticscholar.org/topic/168\"},{\"topic\":\"Sample complexity\",\"topicId\":\"112440\",\"url\":\"https://www.semanticscholar.org/topic/112440\"},{\"topic\":\"Requirement\",\"topicId\":\"136\",\"url\":\"https://www.semanticscholar.org/topic/136\"},{\"topic\":\"Markov chain\",\"topicId\":\"5418\",\"url\":\"https://www.semanticscholar.org/topic/5418\"}],\"url\":\"https://www.semanticscholar.org/paper/c47f864bdef551522008b6696c0981ac8c62924e\",\"venue\":\"IJCAI\",\"year\":2019}\n"