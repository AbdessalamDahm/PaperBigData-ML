"{\"abstract\":\"The options framework in reinforcement learning models the notion of a skill or a temporally extended sequence of actions. The discovery of a reusable set of skills has typically entailed building options, that navigate to bottleneck states. This work adopts a complementary approach, where we attempt to discover options that navigate to landmark states. These states are prototypical representatives of well-connected regions and can hence access the associated region with relative ease. In this work, we propose Successor Options, which leverages Successor Representations to build a model of the state space. The intra-option policies are learnt using a novel pseudo-reward and the model scales to high-dimensional spaces easily. Additionally, we also propose an Incremental Successor Options model that iterates between constructing Successor Representations and building options, which is useful when robust Successor Representations cannot be built solely from primitive actions. We demonstrate the efficacy of our approach on a collection of grid-worlds, and on the high-dimensional robotic control environment of Fetch.\",\"arxivId\":\"1905.05731\",\"authors\":[{\"authorId\":\"10081629\",\"name\":\"R. Ramesh\",\"url\":\"https://www.semanticscholar.org/author/10081629\"},{\"authorId\":\"119439163\",\"name\":\"M. Tomar\",\"url\":\"https://www.semanticscholar.org/author/119439163\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\",\"url\":\"https://www.semanticscholar.org/author/1723632\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"2009.14108\",\"authors\":[{\"authorId\":\"1973667445\",\"name\":\"Vihang P. Patil\"},{\"authorId\":\"66798949\",\"name\":\"M. Hofmarcher\"},{\"authorId\":\"1974372841\",\"name\":\"Marius-Constantin Dinu\"},{\"authorId\":\"2874052\",\"name\":\"M. Dorfer\"},{\"authorId\":\"102984313\",\"name\":\"P. Blies\"},{\"authorId\":\"78843496\",\"name\":\"Johannes Brandstetter\"},{\"authorId\":\"1409265458\",\"name\":\"Jose A. Arjona-Medina\"},{\"authorId\":\"3308557\",\"name\":\"S. Hochreiter\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"91ce8091c1dd08b9db40fb0f6adfcbadd5698eb9\",\"title\":\"Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution\",\"url\":\"https://www.semanticscholar.org/paper/91ce8091c1dd08b9db40fb0f6adfcbadd5698eb9\",\"venue\":\"ArXiv\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1419480565\",\"name\":\"Daan Zeeuwe\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"bd26c5677f2b2a9fc7ff7e0dd0d640fc6db5d322\",\"title\":\"On the Emergence of Biologically-Plausible Representation in Recurrent Neural Networks for Navigation\",\"url\":\"https://www.semanticscholar.org/paper/bd26c5677f2b2a9fc7ff7e0dd0d640fc6db5d322\",\"venue\":\"\",\"year\":2019},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2400282\",\"name\":\"S. Sreedharan\"},{\"authorId\":\"152735970\",\"name\":\"S. Srivastava\"},{\"authorId\":\"1740315\",\"name\":\"S. Kambhampati\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"3a143586dfb230297e940919d73d961d42b415d2\",\"title\":\"TLdR: Policy Summarization for Factored SSP Problems Using Temporal Abstractions\",\"url\":\"https://www.semanticscholar.org/paper/3a143586dfb230297e940919d73d961d42b415d2\",\"venue\":\"ICAPS\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1990422\",\"name\":\"I. Momennejad\"}],\"doi\":\"10.1016/j.cobeha.2020.02.017\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"82728deb259100a927d8c14ccba62707837d2fbb\",\"title\":\"Learning Structures: Predictive Representations, Replay, and Generalization\",\"url\":\"https://www.semanticscholar.org/paper/82728deb259100a927d8c14ccba62707837d2fbb\",\"venue\":\"Current Opinion in Behavioral Sciences\",\"year\":2020}],\"corpusId\":153312771,\"doi\":\"10.24963/ijcai.2019/458\",\"fieldsOfStudy\":[\"Computer Science\",\"Mathematics\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":true,\"paperId\":\"4b51277eac12939867ec04a81bdbc756f61ec9ea\",\"references\":[{\"arxivId\":\"1609.05140\",\"authors\":[{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"40638357\",\"name\":\"J. Harb\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"15b26d8cb35d7e795c8832fe08794224ee1e9f84\",\"title\":\"The Option-Critic Architecture\",\"url\":\"https://www.semanticscholar.org/paper/15b26d8cb35d7e795c8832fe08794224ee1e9f84\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":\"1802.09464\",\"authors\":[{\"authorId\":\"3407285\",\"name\":\"Matthias Plappert\"},{\"authorId\":\"2206490\",\"name\":\"Marcin Andrychowicz\"},{\"authorId\":\"6672240\",\"name\":\"Alex Ray\"},{\"authorId\":\"39593364\",\"name\":\"Bob McGrew\"},{\"authorId\":\"40566201\",\"name\":\"Bowen Baker\"},{\"authorId\":\"46818887\",\"name\":\"Glenn Powell\"},{\"authorId\":\"145540310\",\"name\":\"J. Schneider\"},{\"authorId\":\"48104547\",\"name\":\"Josh Tobin\"},{\"authorId\":\"36045639\",\"name\":\"Maciek Chociej\"},{\"authorId\":\"2930640\",\"name\":\"P. Welinder\"},{\"authorId\":\"48021747\",\"name\":\"V. Kumar\"},{\"authorId\":\"2563432\",\"name\":\"W. Zaremba\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ce1c28ca2f52a42c6e60d792cd71ba894abc47d5\",\"title\":\"Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research\",\"url\":\"https://www.semanticscholar.org/paper/ce1c28ca2f52a42c6e60d792cd71ba894abc47d5\",\"venue\":\"ArXiv\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1727849\",\"name\":\"S. Hanson\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"69d7086300e7f5322c06f2f242a565b3a182efb5\",\"title\":\"In Advances in Neural Information Processing Systems\",\"url\":\"https://www.semanticscholar.org/paper/69d7086300e7f5322c06f2f242a565b3a182efb5\",\"venue\":\"NIPS 1990\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pierre-Luc Bacon\"},{\"authorId\":null,\"name\":\"Jean Harb\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and Doina Precup\",\"url\":\"\",\"venue\":\"The option-critic architecture.\",\"year\":2017},{\"arxivId\":\"1509.02971\",\"authors\":[{\"authorId\":\"2542999\",\"name\":\"T. Lillicrap\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1863250\",\"name\":\"A. Pritzel\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"1968210\",\"name\":\"T. Erez\"},{\"authorId\":\"2109481\",\"name\":\"Y. Tassa\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"024006d4c2a89f7acacc6e4438d156525b60a98f\",\"title\":\"Continuous control with deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/024006d4c2a89f7acacc6e4438d156525b60a98f\",\"venue\":\"ICLR\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Andrew W Moore\"},{\"authorId\":null,\"name\":\"L Baird\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"and LP Kaelbling\",\"url\":\"\",\"venue\":\"Multi-value-functions: E cient automatic action hierarchies for multiple goal mdps.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[],\"isInfluential\":true,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":\"1703.00956\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"8423cc50c18d68f797adaa4f571f5e4efbe325a5\",\"title\":\"A Laplacian Framework for Option Discovery in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8423cc50c18d68f797adaa4f571f5e4efbe325a5\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":\"1707.06347\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"143909660\",\"name\":\"F. Wolski\"},{\"authorId\":\"6515819\",\"name\":\"Prafulla Dhariwal\"},{\"authorId\":\"38909097\",\"name\":\"A. Radford\"},{\"authorId\":\"144538754\",\"name\":\"O. Klimov\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"title\":\"Proximal Policy Optimization Algorithms\",\"url\":\"https://www.semanticscholar.org/paper/dce6f9d4017b1785979e7520fd0834ef8cf02f4b\",\"venue\":\"ArXiv\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"\\u00d6zg\\u00fcr \\u015eim\\u015fek\"},{\"authorId\":null,\"name\":\"Alicia P. Wolfe\"},{\"authorId\":null,\"name\":\"Andrew G. Barto. Identifying useful subgoals in reinforce partitioning\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"pages 816\\u2013823\",\"url\":\"\",\"venue\":\"ACM Press,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"\\u00d6zg\\u00fcr \\u015eim\\u015fek\"},{\"authorId\":null,\"name\":\"Alicia P. Wolfe\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"[ \\u015eim\\u015fek and Barto , 2009 ] \\u00d6zg\\u00fcr \\u015eim\\u015fek and Andrew G Barto . Skill characterization based on betweenness\",\"url\":\"\",\"venue\":\"Advances in neural information processing systems\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2299685\",\"name\":\"A. McGovern\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"03dbb37d2373500115735ed69871e94c7f6b2c5e\",\"title\":\"Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density\",\"url\":\"https://www.semanticscholar.org/paper/03dbb37d2373500115735ed69871e94c7f6b2c5e\",\"venue\":\"ICML\",\"year\":2001},{\"arxivId\":\"1606.02396\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"3422751\",\"name\":\"Simanta Gautam\"},{\"authorId\":\"1831199\",\"name\":\"S. Gershman\"}],\"doi\":null,\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"10a4992ece5baea79326a8878a6244eeacbc6af5\",\"title\":\"Deep Successor Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/10a4992ece5baea79326a8878a6244eeacbc6af5\",\"venue\":\"ArXiv\",\"year\":2016},{\"arxivId\":\"1807.11622\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"143913104\",\"name\":\"Michael H. Bowling\"}],\"doi\":\"10.1609/AAAI.V34I04.5955\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"9f67b3edc67a35c884bd532a5e73fa3a7f3660d8\",\"title\":\"Count-Based Exploration with the Successor Representation\",\"url\":\"https://www.semanticscholar.org/paper/9f67b3edc67a35c884bd532a5e73fa3a7f3660d8\",\"venue\":\"AAAI\",\"year\":2020},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1790646\",\"name\":\"P. Dayan\"}],\"doi\":\"10.1162/neco.1993.5.4.613\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"c2e8806f0bd1d504bcb395ef1f6fe509a023a048\",\"title\":\"Improving Generalization for Temporal Difference Learning: The Successor Representation\",\"url\":\"https://www.semanticscholar.org/paper/c2e8806f0bd1d504bcb395ef1f6fe509a023a048\",\"venue\":\"Neural Computation\",\"year\":1993},{\"arxivId\":\"1605.05359\",\"authors\":[{\"authorId\":\"2943530\",\"name\":\"Aravind S. Lakshminarayanan\"},{\"authorId\":\"32858189\",\"name\":\"R. Krishnamurthy\"},{\"authorId\":\"15045198\",\"name\":\"P. Kumar\"},{\"authorId\":\"1723632\",\"name\":\"Balaraman Ravindran\"}],\"doi\":null,\"intent\":[\"result\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"773347ec0eaaa485493feb72096640557510692f\",\"title\":\"Option Discovery in Hierarchical Reinforcement Learning using Spatio-Temporal Clustering.\",\"url\":\"https://www.semanticscholar.org/paper/773347ec0eaaa485493feb72096640557510692f\",\"venue\":\"\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1760402\",\"name\":\"A. Moore\"},{\"authorId\":\"1844179\",\"name\":\"L. Baird\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"d37861f68c9b680e0aadf62d797fe6a979c756ec\",\"title\":\"Multi-Value-Functions: Efficient Automatic Action Hierarchies for Multiple Goal MDPs\",\"url\":\"https://www.semanticscholar.org/paper/d37861f68c9b680e0aadf62d797fe6a979c756ec\",\"venue\":\"IJCAI\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"John Schulman\"},{\"authorId\":null,\"name\":\"Sergey Levine\"},{\"authorId\":null,\"name\":\"Pieter Abbeel\"},{\"authorId\":null,\"name\":\"Michael Jordan\"},{\"authorId\":null,\"name\":\"Philipp Moritz. Trust region policy optimization\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In International Conference on Machine Learning\",\"url\":\"\",\"venue\":\"pages 1889\\u20131897,\",\"year\":2015},{\"arxivId\":\"1606.05312\",\"authors\":[{\"authorId\":\"143999673\",\"name\":\"Andr\\u00e9 Barreto\"},{\"authorId\":\"2605877\",\"name\":\"W. Dabney\"},{\"authorId\":\"118538000\",\"name\":\"R\\u00e9mi Munos\"},{\"authorId\":\"2323922\",\"name\":\"J. Hunt\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"7634925\",\"name\":\"H. V. Hasselt\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d8686b657b61a37da351af2952aabd8b281de408\",\"title\":\"Successor Features for Transfer in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d8686b657b61a37da351af2952aabd8b281de408\",\"venue\":\"NIPS\",\"year\":2017},{\"arxivId\":\"1710.11089\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1986463\",\"name\":\"C. Rosenbaum\"},{\"authorId\":\"1955964\",\"name\":\"Xiaoxiao Guo\"},{\"authorId\":\"46331912\",\"name\":\"M. Liu\"},{\"authorId\":\"1699108\",\"name\":\"G. Tesauro\"},{\"authorId\":\"143903370\",\"name\":\"Murray Campbell\"}],\"doi\":null,\"intent\":[\"result\",\"background\",\"methodology\"],\"isInfluential\":true,\"paperId\":\"58fb60c5592224901a26dd84220a2f3332c1fcf5\",\"title\":\"Eigenoption Discovery through the Deep Successor Representation\",\"url\":\"https://www.semanticscholar.org/paper/58fb60c5592224901a26dd84220a2f3332c1fcf5\",\"venue\":\"ICLR\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2510156\",\"name\":\"\\u00d6. Simsek\"},{\"authorId\":\"1759756\",\"name\":\"Alicia P. Wolfe\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1145/1102351.1102454\",\"intent\":[\"background\",\"methodology\"],\"isInfluential\":false,\"paperId\":\"1c59bfa0e8654ebea94277064f82062875cae8b6\",\"title\":\"Identifying useful subgoals in reinforcement learning by local graph partitioning\",\"url\":\"https://www.semanticscholar.org/paper/1c59bfa0e8654ebea94277064f82062875cae8b6\",\"venue\":\"ICML '05\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2510156\",\"name\":\"\\u00d6. Simsek\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1145/1015330.1015353\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"fd4de76fadddd1cc9a91cea954200c8d656e1dbb\",\"title\":\"Using relative novelty to identify useful temporal abstractions in reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/fd4de76fadddd1cc9a91cea954200c8d656e1dbb\",\"venue\":\"ICML '04\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1684547\",\"name\":\"I. Menache\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"},{\"authorId\":\"1742179\",\"name\":\"N. Shimkin\"}],\"doi\":\"10.1007/3-540-36755-1_25\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"dca9444e1c69eee36c0be04703d71114a762c84a\",\"title\":\"Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/dca9444e1c69eee36c0be04703d71114a762c84a\",\"venue\":\"ECML\",\"year\":2002},{\"arxivId\":\"1604.06057\",\"authors\":[{\"authorId\":\"1954876\",\"name\":\"Tejas D. Kulkarni\"},{\"authorId\":\"144958935\",\"name\":\"Karthik Narasimhan\"},{\"authorId\":\"3231182\",\"name\":\"A. Saeedi\"},{\"authorId\":\"1763295\",\"name\":\"J. Tenenbaum\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d37620e6f8fe678a43e12930743281cd8cca6a66\",\"title\":\"Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation\",\"url\":\"https://www.semanticscholar.org/paper/d37620e6f8fe678a43e12930743281cd8cca6a66\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2510156\",\"name\":\"\\u00d6. Simsek\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"0be90056a9b4f0434c05e640647f4de9ae32b1c5\",\"title\":\"Skill Characterization Based on Betweenness\",\"url\":\"https://www.semanticscholar.org/paper/0be90056a9b4f0434c05e640647f4de9ae32b1c5\",\"venue\":\"NIPS\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"\\u00d6zg\\u00fcr \\u015eim\\u015fek\"},{\"authorId\":null,\"name\":\"Andrew G Barto. Using relative novelty to identify useful learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"page 95\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3255983\",\"name\":\"V. Mnih\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"1392331736\",\"name\":\"Andrei A. Rusu\"},{\"authorId\":\"144056327\",\"name\":\"J. Veness\"},{\"authorId\":\"1397980088\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1753223\",\"name\":\"A. Graves\"},{\"authorId\":\"3137672\",\"name\":\"Martin A. Riedmiller\"},{\"authorId\":\"1397979864\",\"name\":\"Andreas K. Fidjeland\"},{\"authorId\":\"2273072\",\"name\":\"Georg Ostrovski\"},{\"authorId\":\"145386761\",\"name\":\"S. Petersen\"},{\"authorId\":\"48878752\",\"name\":\"C. Beattie\"},{\"authorId\":\"49813280\",\"name\":\"A. Sadik\"},{\"authorId\":\"2460849\",\"name\":\"Ioannis Antonoglou\"},{\"authorId\":\"153907173\",\"name\":\"H. King\"},{\"authorId\":\"2106164\",\"name\":\"D. Kumaran\"},{\"authorId\":\"1688276\",\"name\":\"Daan Wierstra\"},{\"authorId\":\"34313265\",\"name\":\"S. Legg\"},{\"authorId\":\"48987704\",\"name\":\"Demis Hassabis\"}],\"doi\":\"10.1038/nature14236\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"title\":\"Human-level control through deep reinforcement learning\",\"url\":\"https://www.semanticscholar.org/paper/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\",\"venue\":\"Nature\",\"year\":2015},{\"arxivId\":\"1502.05477\",\"authors\":[{\"authorId\":\"47971768\",\"name\":\"John Schulman\"},{\"authorId\":\"1736651\",\"name\":\"S. Levine\"},{\"authorId\":\"1689992\",\"name\":\"P. Abbeel\"},{\"authorId\":\"1694621\",\"name\":\"Michael I. Jordan\"},{\"authorId\":\"29912342\",\"name\":\"P. Moritz\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"66cdc28dc084af6507e979767755e99fe0b46b39\",\"title\":\"Trust Region Policy Optimization\",\"url\":\"https://www.semanticscholar.org/paper/66cdc28dc084af6507e979767755e99fe0b46b39\",\"venue\":\"ICML\",\"year\":2015},{\"arxivId\":\"1703.01161\",\"authors\":[{\"authorId\":\"9948791\",\"name\":\"A. S. Vezhnevets\"},{\"authorId\":\"2217144\",\"name\":\"Simon Osindero\"},{\"authorId\":\"1725157\",\"name\":\"T. Schaul\"},{\"authorId\":\"2801204\",\"name\":\"N. Heess\"},{\"authorId\":\"3093886\",\"name\":\"Max Jaderberg\"},{\"authorId\":\"145824029\",\"name\":\"D. Silver\"},{\"authorId\":\"2645384\",\"name\":\"K. Kavukcuoglu\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"049c6e5736313374c6e594c34b9be89a3a09dced\",\"title\":\"FeUdal Networks for Hierarchical Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/049c6e5736313374c6e594c34b9be89a3a09dced\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144165008\",\"name\":\"D. Arthur\"},{\"authorId\":\"1749789\",\"name\":\"Sergei Vassilvitskii\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"5e0c61b7ee4a2de183a197f32c5013ad109531fa\",\"title\":\"k-means++: the advantages of careful seeding\",\"url\":\"https://www.semanticscholar.org/paper/5e0c61b7ee4a2de183a197f32c5013ad109531fa\",\"venue\":\"SODA '07\",\"year\":2007}],\"title\":\"Successor Options: An Option Discovery Framework for Reinforcement Learning\",\"topics\":[{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"State space\",\"topicId\":\"6115\",\"url\":\"https://www.semanticscholar.org/topic/6115\"},{\"topic\":\"Robot\",\"topicId\":\"6657\",\"url\":\"https://www.semanticscholar.org/topic/6657\"},{\"topic\":\"Pseudo brand of pseudoephedrine\",\"topicId\":\"9801110\",\"url\":\"https://www.semanticscholar.org/topic/9801110\"},{\"topic\":\"Increment\",\"topicId\":\"38961\",\"url\":\"https://www.semanticscholar.org/topic/38961\"},{\"topic\":\"Policy\",\"topicId\":\"60001\",\"url\":\"https://www.semanticscholar.org/topic/60001\"}],\"url\":\"https://www.semanticscholar.org/paper/4b51277eac12939867ec04a81bdbc756f61ec9ea\",\"venue\":\"IJCAI\",\"year\":2019}\n"