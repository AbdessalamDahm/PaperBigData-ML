"{\"abstract\":\"Effective options can make reinforcement learning easier by enhancing an agent\\u2019s ability to both explore in a targeted manner and plan further into the future. However, learning an appropriate model of an option\\u2019s dynamics in hard, requiring estimating a highly parameterized probability distribution. This paper introduces and motivates the ExpectedLength Model (ELM) for options, an alternate model for transition dynamics. We prove ELM is a (biased) estimator of the traditional MultiTime Model (MTM), but provide a non-vacuous bound on their deviation. We further prove that, in stochastic shortest path problems, ELM induces a value function that is sufficiently similar to the one induced by MTM, and is thus capable of supporting near-optimal behavior. We explore the practical utility of this option model experimentally, finding consistent support for the thesis that ELM is a suitable replacement for MTM. In some cases, we find ELM leads to more sample efficient learning, especially when options are arranged in a hierarchy.\",\"arxivId\":null,\"authors\":[{\"authorId\":\"152422014\",\"name\":\"David Abel\",\"url\":\"https://www.semanticscholar.org/author/152422014\"},{\"authorId\":\"144214684\",\"name\":\"J. Winder\",\"url\":\"https://www.semanticscholar.org/author/144214684\"},{\"authorId\":\"144980202\",\"name\":\"M. desJardins\",\"url\":\"https://www.semanticscholar.org/author/144980202\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\",\"url\":\"https://www.semanticscholar.org/author/144885169\"}],\"citationVelocity\":0,\"citations\":[{\"arxivId\":\"1912.07544\",\"authors\":[{\"authorId\":\"144214684\",\"name\":\"J. Winder\"},{\"authorId\":\"144177520\",\"name\":\"S. Milani\"},{\"authorId\":\"52209217\",\"name\":\"Matthew Landen\"},{\"authorId\":\"1466551011\",\"name\":\"Erebus Oh\"},{\"authorId\":\"1468844871\",\"name\":\"Shane Parr\"},{\"authorId\":\"50460087\",\"name\":\"S. Squire\"},{\"authorId\":\"144980202\",\"name\":\"M. desJardins\"},{\"authorId\":\"2674440\",\"name\":\"Cynthia Matuszek\"}],\"doi\":\"10.1609/AAAI.V34I06.6555\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ff50006801efdbcd38163bc87c761210506987f4\",\"title\":\"Planning with Abstract Learned Models While Learning Transferable Subtasks\",\"url\":\"https://www.semanticscholar.org/paper/ff50006801efdbcd38163bc87c761210506987f4\",\"venue\":\"AAAI\",\"year\":2020}],\"corpusId\":199466466,\"doi\":\"10.24963/ijcai.2019/270\",\"fieldsOfStudy\":[\"Computer Science\"],\"influentialCitationCount\":0,\"is_open_access\":true,\"is_publisher_licensed\":false,\"paperId\":\"2f58c0c89e33021c9834115a41db07c1a18f9133\",\"references\":[{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1016/S0004-3702(99)00052-1\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"title\":\"Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\",\"venue\":\"Artif. Intell.\",\"year\":1999},{\"arxivId\":\"1703.08667\",\"authors\":[{\"authorId\":\"9937287\",\"name\":\"Ronan Fruit\"},{\"authorId\":\"3254390\",\"name\":\"A. Lazaric\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"f0a074177409db16cf3919434b44ce1b6590871b\",\"title\":\"Exploration-Exploitation in MDPs with Options\",\"url\":\"https://www.semanticscholar.org/paper/f0a074177409db16cf3919434b44ce1b6590871b\",\"venue\":\"AISTATS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"34887814\",\"name\":\"Nicholay Topin\"},{\"authorId\":\"2547396\",\"name\":\"Nicholas Haltmeyer\"},{\"authorId\":\"50460087\",\"name\":\"S. Squire\"},{\"authorId\":\"144214684\",\"name\":\"J. Winder\"},{\"authorId\":\"144980202\",\"name\":\"M. desJardins\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"b8a3c1c0ebe113ffb61b0066aa91ef03701e5550\",\"title\":\"Portable Option Discovery for Automated Learning Transfer in Object-Oriented Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/b8a3c1c0ebe113ffb61b0066aa91ef03701e5550\",\"venue\":\"IJCAI\",\"year\":2015},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Erik Talvitie. Self-correcting models for model-based r learning\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 2597\\u20132603,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Satinder P Singh\"},{\"authorId\":null,\"name\":\"Andrew G Barto\"},{\"authorId\":null,\"name\":\"Nuttapong Chentanez. Intrinsically motivated reinforcement learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 1281\\u20131288,\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"3187297\",\"name\":\"Daniel J. Mankowitz\"},{\"authorId\":\"2554720\",\"name\":\"Timothy A. Mann\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"5ce4016826d84e1d06f177177b54ad02ecd53e7a\",\"title\":\"Time-regularized interrupting options\",\"url\":\"https://www.semanticscholar.org/paper/5ce4016826d84e1d06f177177b54ad02ecd53e7a\",\"venue\":\"ICML 2014\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Thomas G Dietterich. Hierarchical reinforcement learning decomposition\"}],\"doi\":null,\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"\",\"title\":\"JAIR\",\"url\":\"\",\"venue\":\"13:227\\u2013303,\",\"year\":2000},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1745716\",\"name\":\"Nicholas K. Jong\"},{\"authorId\":\"144848112\",\"name\":\"P. Stone\"}],\"doi\":\"10.1145/1390156.1390211\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"dd1d2a2d70b0fcbadd83106cb3c092d3aa4ae9ea\",\"title\":\"Hierarchical model-based reinforcement learning: R-max + MAXQ\",\"url\":\"https://www.semanticscholar.org/paper/dd1d2a2d70b0fcbadd83106cb3c092d3aa4ae9ea\",\"venue\":\"ICML '08\",\"year\":2008},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Emma Brunskill\"},{\"authorId\":null,\"name\":\"Lihong Li. PAC-inspired option discovery in lifelong rein learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 316\\u2013324,\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1680506\",\"name\":\"R. Brafman\"},{\"authorId\":\"1708847\",\"name\":\"Moshe Tennenholtz\"}],\"doi\":\"10.1162/153244303765208377\",\"intent\":[\"methodology\",\"background\"],\"isInfluential\":true,\"paperId\":\"c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"title\":\"R-MAX - A General Polynomial Time Algorithm for Near-Optimal Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c5fa00d361e9e4d4344235ad4e354459f3f24e1e\",\"venue\":\"J. Mach. Learn. Res.\",\"year\":2002},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Timothy A Mann\"},{\"authorId\":null,\"name\":\"Shie Mannor. The advantage of planning with options\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"RLDM 2013\",\"url\":\"\",\"venue\":\"page 9,\",\"year\":2013},{\"arxivId\":\"1612.06018\",\"authors\":[{\"authorId\":\"1701322\",\"name\":\"Erik Talvitie\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"ea22d6190b1ae38fefd391e3d6c48d8807d72626\",\"title\":\"Self-Correcting Models for Model-Based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/ea22d6190b1ae38fefd391e3d6c48d8807d72626\",\"venue\":\"AAAI\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":\"10.1023/A:1022633531479\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"title\":\"Learning to Predict by the Methods of Temporal Differences\",\"url\":\"https://www.semanticscholar.org/paper/a91635f8d0e7fb804efd1c38d9c24ee952ba7076\",\"venue\":\"Machine Learning\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":\"81338045\",\"name\":\"M. Kearns\"},{\"authorId\":\"1699868\",\"name\":\"Satinder Singh\"}],\"doi\":\"10.1023/A:1017984413808\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"dc649486b881e672eea6546da48c46e1f98daf32\",\"title\":\"Near-Optimal Reinforcement Learning in Polynomial Time\",\"url\":\"https://www.semanticscholar.org/paper/dc649486b881e672eea6546da48c46e1f98daf32\",\"venue\":\"Machine Learning\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"37814588\",\"name\":\"M. Puterman\"}],\"doi\":\"10.1002/9780470316887\",\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"title\":\"Markov Decision Processes: Discrete Stochastic Dynamic Programming\",\"url\":\"https://www.semanticscholar.org/paper/8090121ad488b4af27bc59bf91b62e9c6a6f49c6\",\"venue\":\"Wiley Series in Probability and Statistics\",\"year\":1994},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"b6826f2a475f360eeb1bcbf3002df44b8fd91d96\",\"title\":\"Multi-Time Models for Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/b6826f2a475f360eeb1bcbf3002df44b8fd91d96\",\"venue\":\"\",\"year\":2007},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Doina Precup\"},{\"authorId\":null,\"name\":\"Richard S Sutton. Multi-time models for temporally abstrac planning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 1050\\u20131056,\",\"year\":1998},{\"arxivId\":\"1703.00956\",\"authors\":[{\"authorId\":\"40066857\",\"name\":\"Marlos C. Machado\"},{\"authorId\":\"1792298\",\"name\":\"Marc G. Bellemare\"},{\"authorId\":\"1687780\",\"name\":\"Michael Bowling\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"8423cc50c18d68f797adaa4f571f5e4efbe325a5\",\"title\":\"A Laplacian Framework for Option Discovery in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/8423cc50c18d68f797adaa4f571f5e4efbe325a5\",\"venue\":\"ICML\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2554720\",\"name\":\"Timothy A. Mann\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"ea2b8c200d4b045803e65620242603edb9a9bf01\",\"title\":\"Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations\",\"url\":\"https://www.semanticscholar.org/paper/ea2b8c200d4b045803e65620242603edb9a9bf01\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"},{\"authorId\":\"1388700951\",\"name\":\"Tomas Lozano-Perez\"}],\"doi\":\"10.1613/jair.5575\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"adfc4a14e319bd4a49b88452c74b03d7ae4400eb\",\"title\":\"From Skills to Symbols: Learning Symbolic Representations for Abstract High-Level Planning\",\"url\":\"https://www.semanticscholar.org/paper/adfc4a14e319bd4a49b88452c74b03d7ae4400eb\",\"venue\":\"J. Artif. Intell. Res.\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1765407\",\"name\":\"G. Konidaris\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"16050a256dd6add1e9187e8c4f5c30c85f342fd8\",\"title\":\"Building Portable Options: Skill Transfer in Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/16050a256dd6add1e9187e8c4f5c30c85f342fd8\",\"venue\":\"IJCAI\",\"year\":2007},{\"arxivId\":\"1602.03351\",\"authors\":[{\"authorId\":\"3187297\",\"name\":\"Daniel J. Mankowitz\"},{\"authorId\":\"2554720\",\"name\":\"Timothy A. Mann\"},{\"authorId\":\"1712535\",\"name\":\"Shie Mannor\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"edff4138184b4f37590649b9aadc9a627942a5d0\",\"title\":\"Adaptive Skills Adaptive Partitions (ASAP)\",\"url\":\"https://www.semanticscholar.org/paper/edff4138184b4f37590649b9aadc9a627942a5d0\",\"venue\":\"NIPS\",\"year\":2016},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1682823\",\"name\":\"P. Tseng\"}],\"doi\":\"10.1016/0167-6377(90)90022-W\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"9d8bed554b0f08880fafadfa6d66e90f783cc9a7\",\"title\":\"Solving H-horizon, stationary Markov decision problems in time proportional to log(H)\",\"url\":\"https://www.semanticscholar.org/paper/9d8bed554b0f08880fafadfa6d66e90f783cc9a7\",\"venue\":\"\",\"year\":1990},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1693725234\",\"name\":\"Twoweek Loan COpy\"},{\"authorId\":\"72115799\",\"name\":\"G. Chew\"},{\"authorId\":\"5333438\",\"name\":\"Stanley Mandelstam\"},{\"authorId\":\"48014963\",\"name\":\"P. Noyes\"}],\"doi\":\"10.1177/019263653902308210\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"d51134eb0793870209fed5e28a9de510921e846d\",\"title\":\"University of California\",\"url\":\"https://www.semanticscholar.org/paper/d51134eb0793870209fed5e28a9de510921e846d\",\"venue\":\"The American journal of dental science\",\"year\":1886},{\"arxivId\":\"1711.03817\",\"authors\":[{\"authorId\":\"3134710\",\"name\":\"A. Harutyunyan\"},{\"authorId\":\"2528631\",\"name\":\"Peter Vrancx\"},{\"authorId\":\"145180695\",\"name\":\"P. Bacon\"},{\"authorId\":\"144368601\",\"name\":\"Doina Precup\"},{\"authorId\":\"144336828\",\"name\":\"A. Now\\u00e9\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"9975ca72a2507d5de379501a72a430ef31d6218b\",\"title\":\"Learning with Options that Terminate Off-Policy\",\"url\":\"https://www.semanticscholar.org/paper/9975ca72a2507d5de379501a72a430ef31d6218b\",\"venue\":\"AAAI\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1617826387\",\"name\":\"Lan Ma\"},{\"authorId\":\"1618302531\",\"name\":\"Pingya Luo\"},{\"authorId\":\"1621465387\",\"name\":\"Yi He\"},{\"authorId\":\"1623974684\",\"name\":\"Liyun Zhang\"},{\"authorId\":\"1617840894\",\"name\":\"Yi Fan\"},{\"authorId\":\"1618119585\",\"name\":\"Zhenju Jiang\"}],\"doi\":\"10.1515/9783111413426-022\",\"intent\":[\"methodology\"],\"isInfluential\":true,\"paperId\":\"1452a8c334630ac1b6dfd8a0981f52ffdd8a0fd3\",\"title\":\"U\",\"url\":\"https://www.semanticscholar.org/paper/1452a8c334630ac1b6dfd8a0981f52ffdd8a0fd3\",\"venue\":\"Edinburgh Medical and Surgical Journal\",\"year\":1824},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Michael L Littman\"},{\"authorId\":null,\"name\":\"Thomas L Dean\"},{\"authorId\":null,\"name\":\"Leslie Pack Kaelbling. On the complexity of solving Marko problems\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In UAI\",\"url\":\"\",\"venue\":\"pages 394\\u2013402,\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Martha White. Unifying task specification in reinforcemen learning\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In ICML\",\"url\":\"\",\"venue\":\"pages 3742\\u20133750,\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2646714\",\"name\":\"Nakul Gopalan\"},{\"authorId\":\"144980202\",\"name\":\"M. desJardins\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"2700008\",\"name\":\"J. MacGlashan\"},{\"authorId\":\"50460087\",\"name\":\"S. Squire\"},{\"authorId\":\"2913681\",\"name\":\"Stefanie Tellex\"},{\"authorId\":\"144214684\",\"name\":\"J. Winder\"},{\"authorId\":\"145307121\",\"name\":\"L. Wong\"}],\"doi\":null,\"intent\":[\"methodology\"],\"isInfluential\":false,\"paperId\":\"90426c8d12d9df32322821c7d8f980c9a5c53675\",\"title\":\"Planning with Abstract Markov Decision Processes\",\"url\":\"https://www.semanticscholar.org/paper/90426c8d12d9df32322821c7d8f980c9a5c53675\",\"venue\":\"ICAPS\",\"year\":2017},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144299726\",\"name\":\"Thomas G. Dietterich\"}],\"doi\":\"10.1145/242224.242229\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"aab43c9c33af00b718cf2ae374b861d49862a563\",\"title\":\"Machine learning\",\"url\":\"https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563\",\"venue\":\"CSUR\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"Pierre-Luc Bacon\"},{\"authorId\":null,\"name\":\"Jean Harb\"},{\"authorId\":null,\"name\":\"Doina Precup. The option-critic architecture\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In AAAI\",\"url\":\"\",\"venue\":\"pages 1726\\u20131734,\",\"year\":2017},{\"arxivId\":\"1302.4971\",\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"39971338\",\"name\":\"T. Dean\"},{\"authorId\":\"1709512\",\"name\":\"L. Kaelbling\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"7ae91735c6e5f4ab44bfa95bd144663f057d1935\",\"title\":\"On the Complexity of Solving Markov Decision Problems\",\"url\":\"https://www.semanticscholar.org/paper/7ae91735c6e5f4ab44bfa95bd144663f057d1935\",\"venue\":\"UAI\",\"year\":1995},{\"arxivId\":null,\"authors\":[{\"authorId\":\"144885169\",\"name\":\"M. Littman\"},{\"authorId\":\"40868287\",\"name\":\"Csaba Szepesvari\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"327950ac8d59054c4b4a37f901ac9bdbb8168087\",\"title\":\"A Generalized Reinforcement-Learning Model: Convergence and Applications\",\"url\":\"https://www.semanticscholar.org/paper/327950ac8d59054c4b4a37f901ac9bdbb8168087\",\"venue\":\"ICML\",\"year\":1996},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1699645\",\"name\":\"R. Sutton\"},{\"authorId\":\"1730590\",\"name\":\"A. Barto\"}],\"doi\":\"10.1109/TNN.1998.712192\",\"intent\":[],\"isInfluential\":false,\"paperId\":\"97efafdb4a3942ab3efba53ded7413199f79c054\",\"title\":\"Reinforcement Learning: An Introduction\",\"url\":\"https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054\",\"venue\":\"IEEE Transactions on Neural Networks\",\"year\":2005},{\"arxivId\":\"1804.07193\",\"authors\":[{\"authorId\":\"7981071\",\"name\":\"Kavosh Asadi\"},{\"authorId\":\"31498163\",\"name\":\"Dipendra Kumar Misra\"},{\"authorId\":\"144885169\",\"name\":\"M. Littman\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"c4f529934b6f22aa38e014e295a9737daa6e7db5\",\"title\":\"Lipschitz Continuity in Model-based Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/c4f529934b6f22aa38e014e295a9737daa6e7db5\",\"venue\":\"ICML\",\"year\":2018},{\"arxivId\":null,\"authors\":[{\"authorId\":\"2563117\",\"name\":\"Emma Brunskill\"},{\"authorId\":\"28929337\",\"name\":\"L. Li\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f\",\"title\":\"PAC-inspired Option Discovery in Lifelong Reinforcement Learning\",\"url\":\"https://www.semanticscholar.org/paper/d163ae2ae7ee2b7991ab017113f13f54fc5c8c5f\",\"venue\":\"ICML\",\"year\":2014},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"\\u00d6zg\\u00fcr \\u015eim\\u015fek\"},{\"authorId\":null,\"name\":\"Andrew G Barto. Using relative novelty to identify useful ICML\"}],\"doi\":null,\"intent\":[],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"page 95\",\"url\":\"\",\"venue\":\"ACM,\",\"year\":2004},{\"arxivId\":null,\"authors\":[{\"authorId\":\"1446962550\",\"name\":\"Dock Bumpers\"},{\"authorId\":\"1446961266\",\"name\":\"Support Ledgers\"}],\"doi\":\"10.1023/A:1017189329742\",\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"title\":\"Volume 2\",\"url\":\"https://www.semanticscholar.org/paper/e113be5f2d8458877ad64b8ac0023c332dc16c0e\",\"venue\":\"Proceedings of the Ninth International Conference on Computer Supported Cooperative Work in Design, 2005.\",\"year\":2005},{\"arxivId\":null,\"authors\":[{\"authorId\":null,\"name\":\"George Konidaris\"},{\"authorId\":null,\"name\":\"Andrew G Barto. Skill discovery in continuous reinforceme chaining\"}],\"doi\":null,\"intent\":[\"background\"],\"isInfluential\":false,\"paperId\":\"\",\"title\":\"In NeurIPS\",\"url\":\"\",\"venue\":\"pages 1015\\u20131023,\",\"year\":2009}],\"title\":\"The Expected-Length Model of Options\",\"topics\":[{\"topic\":\"Midwoofer-tweeter-midwoofer\",\"topicId\":\"2840565\",\"url\":\"https://www.semanticscholar.org/topic/2840565\"},{\"topic\":\"Reinforcement learning\",\"topicId\":\"2557\",\"url\":\"https://www.semanticscholar.org/topic/2557\"},{\"topic\":\"Approximation\",\"topicId\":\"3247\",\"url\":\"https://www.semanticscholar.org/topic/3247\"},{\"topic\":\"Experiment\",\"topicId\":\"378\",\"url\":\"https://www.semanticscholar.org/topic/378\"},{\"topic\":\"Stochastic process\",\"topicId\":\"21302\",\"url\":\"https://www.semanticscholar.org/topic/21302\"},{\"topic\":\"Shortest path problem\",\"topicId\":\"28585\",\"url\":\"https://www.semanticscholar.org/topic/28585\"},{\"topic\":\"Bellman equation\",\"topicId\":\"65628\",\"url\":\"https://www.semanticscholar.org/topic/65628\"},{\"topic\":\"Overhead (computing)\",\"topicId\":\"4163\",\"url\":\"https://www.semanticscholar.org/topic/4163\"},{\"topic\":\"Stochastic gradient descent\",\"topicId\":\"202796\",\"url\":\"https://www.semanticscholar.org/topic/202796\"},{\"topic\":\"Elm\",\"topicId\":\"154580\",\"url\":\"https://www.semanticscholar.org/topic/154580\"}],\"url\":\"https://www.semanticscholar.org/paper/2f58c0c89e33021c9834115a41db07c1a18f9133\",\"venue\":\"IJCAI\",\"year\":2019}\n"